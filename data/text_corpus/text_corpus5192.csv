index,text
25960,environmental modelling practitioners are now seeking to move forwards together and build standards and technologies with more universal applicability and integration with so many overlapping environmental modelling technologies and infrastructures being offered and with so many relevant supporting technologies contributing on the periphery there is considerable scope to articulate and utilise underlying concepts which draw them together as such this paper offers the spatio temporal modelling taxonomy a simple taxonomy for describing the spatio temporal structure of environmental numerical modelling data used as input to or produced as output from environmental numerical models the taxonomy is motivated from common spatial structures a set of feature types to describe observed environmental data and the implementation of the openmi integrated modelling standard it serves as a natural evolution of terminology that is in common use in environmental numerical modelling and is designed to strike the right balance between complexity and utility it implements a structured theoretical framework whilst being essentially practical in nature to apply to the real world facing numerical modellers and those seeking to integrate environmental numerical models and the data supporting them keywords spatio temporal feature type environmental modelling data structure taxonomy vocabulary software availability an xml encoding of the full spatio temporal modelling taxonomy is given in the spatio temporal modelling taxonomy github repository https github com qharpham spatio temporal modelling taxonomy 1 introduction it was suggested by bruce tuckman 1965 that groups go through a developmental sequence characterised by four stages forming storming norming and performing during the forming stage group members are being introduced to one another as they gradually form the team whilst still acting fairly independently as they begin to approach the task together the storming stage sees opinions voiced personality clashes and if the group is to be productive the resolution of differences of direction and emphasis then the group beings to settle down and perform as a coherent team if a similar idea could be applied to entire scientific disciplines then the relatively young discipline of integrated environmental modelling iem see for example meiburg 2008 laniak et al 2013 sutherland et al 2014 could be said to be entering the storming stage integrated environmental modelling seeks to link together potentially disparate data sources and numerical models to better understand the interconnectivity of environmental processes thereby improving the accuracy of simulations the objective is to bring more structure standardisation and formality into a set of previously ad hoc and compartmentalised numerical modelling use cases inevitably this activity spans a number of different numerical modelling disciplines which need to work together one single use case can cover a wide variety of modelling and data management approaches for example studying flash flooding can require implementation of meteorological models to simulate rainfall hydrological models to simulate drainage hydraulic models to simulate open channel flow overtopping and flood spreading and impact models to simulate damage to property and infrastructure and hazard to people danovaro et al 2014 indeed d agostino et al 2014 demonstrate a distributed infrastructure for implementing such a numerical modelling chain using high performance computing for meteorological models grid computing for hydrological models and cloud computing for hydraulic and impact models groups from different disciplines have made considerable progress in producing technical infrastructures and environments to facilitate practitioners integrated environmental modelling activities examples include the community surface dynamics modeling system csdms peckham 2013 earth system modelling framework esmf hill et al 2004 collins et al 2005 frames whelan et al 2014 oasis valcke 2013 craig et al 2017 opengms chen et al 2019 and the open modelling interface openmi fluidearth harpham et al 2014 2019 buahin and horsburgh 2018 gregersen et al 2007 such environments are usually based around methods to couple numerical models together and to handle the provision of the necessary supporting data many other related technologies also offer relevant functionality including handling workflow steps uncertainty in model results and the portrayal of results the status of these activities would certainly correspond to tuckman s forming stage with so many overlapping technologies having been developed in recent years and so many relevant supporting technologies contributing on the periphery the situation could be described as being a little chaotic laniak et al 2013 lay out a vision and roadmap for the future of integrated environmental modelling which requires that the global community of iem stakeholders transcend social and organizational boundaries and pursue greater levels of collaboration sutherland et al 2014 propose six topics of consolidation metadata for data and models supporting information software as a service linking or interface technologies diagnostic or reasoning tools and the portrayal and understanding of integrated modelling that is demonstrating the structure verification and validation of the linked composition they emphasise the importance of open software architectures open source software and a community that values openness and the sharing of models and data consolidation efforts such as these indicate a desire to move forwards together to build standards and technologies with more universal applicability technologies which would support this move inevitably pertain most closely to the use cases that drove their creation and often unknown to their creators make assumptions that do not hold for the practitioners in adjacent domains this is entirely understandable since all groups of numerical modellers will create tools and technologies which solve the problems immediate to them and they should be heralded for offering their technologies to others for example those for whom performance is a critical issue will introduce technologies to facilitate faster solutions e g craig et al 2005 those for whom flexibility is a critical issue will introduce technologies to facilitate flexible solutions e g harpham et al 2019 moreover doubt has been cast over some aspects of the relative gain in attempting usage of tools intended to increase productivity due to the typically long learning curve iwanaga 2018 any productivity gained by a tool designed to manage a complex task can be hard won and only attained by a user with considerable patience and perseverance however examples such as harpham et al 2019 and danovaro et al 2014 illustrate the underlying potential of these technologies and all of these factors together bear the hallmarks of a group moving into tuckman s storming stage attempts to bring together and characterise iem trends and technologies such as laniak et al 2013 and sutherland et al 2014 tend to refer to characteristics which are offered as common topics suitable for collaborative development for example these two agree on the need for standards and metadata to publish environmental numerical models with a commonly agreed descriptive framework such as that offered by harpham and danovaro 2015 such a framework will inevitably refer to the spatio temporal nature of the data which is required as input to the models and the data which is produced by the models as one of the key ingredients in moving iem forwards through and beyond tuckman s storming stage this paper offers a simple taxonomy for describing the spatio temporal nature of such environmental numerical modelling data with the concept and an initial arrangement first introduced in harpham 2019 this spatio temporal modelling taxonomy serves as a natural evolution of terminology that is in common use in environmental numerical modelling it structures formalises and extends this terminology to apply to data used in a wide variety of modelling domains and hence facilitates iem across these domains as well as within each of them it implements a structured theoretical framework whilst being essentially practical in nature to apply to the real world facing numerical modellers and those seeking to integrate environmental numerical models and the data supporting them 2 motivation three independent technical innovations help to motivate and shape the derivation of the taxonomy for describing the spatio temporal structure of the data supporting environmental numerical modelling common spatial structures the climate science modelling language csml and usage of adaptors in openmi 2 1 common spatial structures the common shapefile format for geographic information systems gis describes spatial vector features in terms of points polylines polygonal chains or linestrings and polygons the wikipedia definitions of these features are paraphrased as follows point a unique dimensionless location in euclidean space specified by an ordered pair of numbers for 2 dimensional space and a triplet for 3 dimensional space point geometry 2019 polyline polygonal chain linestring a polygonal chain is a connected series of line segments a curve specified by a sequence of point features vertices polygonal chain 2019 polygon a plane figure that is described by a finite number of straight line segments which are connected to form a closed circuit polygon 2019 including these well established constructs herring 2011 presents an opengis implementation standard for geographic information describing the relationships between these and other simple features some of which are constructed from point line linestring and polygon features in particular and as used in feature layers within the shapefile format multipoint multilinestring and multipolygon are given as a collections of points linestrings and polygons respectively 2 2 climate science modelling language the iso19156 observations and measurements o m model ogc observations and measurements 2013 iso19156 2011 offers a taxonomy of basic spatial sampling features sf samplingpoint sf samplingcurve sf samplingsurface and sf samplingsolid these are based on the shape types gm point gm curve gm surface and gm solid seventeen application sampling features such as a borehole and a shipstrack are mapped to each of the sampling features this provides a motivation for csml which offers a set of spatio temporal feature types intended to be a practical set describing measured environmental data which has been collected by deployed instruments woolf and lowe 2010 in addition to the spatial properties of the feature types a temporal dimension is brought in in version 3 0 of csml ten practical feature types are given nine are specialisations of the o m model and one is a direct derivation they are summarised in table 1 within csml a narrower variety of spatial structures than that given in the opengis implementation standard for geographic information are considered necessary to describe the data collected by measuring devices the feature types additionally incorporate a temporal aspect which points towards a set which would typically be required by environmental numerical modellers however the motivation for csml and indeed o m derives from measured observed data and so the full set of structures required for environmental numerical modelling are not represented 2 3 adaptors in openmi now more closely considering applications of these concepts within environmental numerical modelling the openmi version 2 0 standard harpham et al 2019 the most recent version of openmi enables the transfer of data between numerical models as they proceed through their respective time horizons the output data from one numerical model may be passed into a second numerical model along a pair of connected exchange items output to input this transfer of data can take place in both directions so that the numerical models have opportunity to influence each other version 2 0 of the standard breaks down the spatial aspect of numerical model inputs and outputs into familiar spatial constructs this is governed by the ispatialdefinition interface ogc openmi 2014 section 6 4 the element set of the spatial structure ielementset includes management of indexes counts and coordinates with each element enumerated into five types idbased point polyline polygon and polyhedron this specification allows the representation of geospatial objects used in environmental modelling but it does not offer additional topological specification such as cell adjacency common structures such as model grids are constructed from polygon elements to handle any differences between the outputs offered by one numerical model and the inputs required by another openmi uses adaptors ogc openmi 2014 section 6 10 these adaptors are independent from the numerical model components as illustrated in fig 1 for example harpham et al 2014 gives a grid to grid adaptor interpolating between two model grids belonging to different models in a single composition adaptors are independent entities and can be chained together to perform all of the required functions they can be re used as appropriate and focus on specific aspects for example transferring the data between two models can require temporal interpolation spatial interpolation and unit transformation different models in the composition can require different combinations of these this is illustrated in fig 2 due to the high degree of flexibility offered by openmi a wide variety of spatio temporal structures may be used in compositions of numerical models measured data sources such as live feeds coming from deployed instruments or archives of previously collected readings may also be included these can incorporate a variety of spatio temporal structures more typically adopted by measured data sources such as those offered by csml table 1 the independent adaptor concept allows for the creation of reusable adaptor components which since they are openmi compliant can exist independently of the numerical models which they serve in order to articulate their purpose the functionality of these adaptor components must be characterised in some way some adaptors may perform standard unit transformations metres to feet kilograms to pounds other adaptors may interpolate between different spatio temporal structures data pertaining to a single point but varying in time to data pertaining to a single polygon and varying in time it is clear that a taxonomy to describe the spatio temporal structures used by numerical models will both aid articulation of the functionality and requirements of individual models as well as facilitating the construction of the required functions and optimised file storage formats for connecting them together openmi refers to these functions as adaptors but similar constructs exist for all leading technologies which seek to integrate numerical models with an author list that includes representatives from openmi csdms esmf opengms as well as social scientists and gis experts chen et al 2020 make the observation that geographic models produce outputs that include a wide variety of spatial feature types such as grids points and meshes this is demonstrated in the csdms basic model interface bmi peckham et al 2013 which provides a set of seven grid types to describe the model data spatial structure scalar points vector unstructured structured quadrilateral rectilinear and uniform rectilinear another example is esmf hill et al 2004 collins et al 2005 which offers a set of associated grid types grid logically rectangular mesh unstructured locstream location stream a set of unconnected points however chen et al 2020 continue to make the observation that although the implementations of standards such as openmi offer low level flexibility in interpolating among feature types when implemented in different time step schemes and some discrete global grids have been developed to express grid nodes edges and cells in a uniform way to support spatial data organization more work is still required to make this truly generic practical and efficient this indicates that gains are to be made with a higher level representation independent of any implementing technology indeed there is no formally adopted vocabulary describing the full breadth of spatio temporal structures possible in environmental modelling motivated by common spatial structures csml and openmi adaptors the spatio temporal modelling taxonomy offered here seeks to describe the spatio temporal structure of all such data in a simple and accessible way 3 derivation the three motivations discussed above common spatial structures csml and openmi adopt an implementation of point polyline and polygon in some form the common spatial structures and openmi refer to these directly with grids constructed from polygons and csml implements points trajectories including a time dimension to the structure and grids we therefore begin with purely spatial structures in 2 dimensions with these three adopting the terminology point polyline and polygon as is established in gis using these spatial structures data in this case numerical modelling results or measurements can be stored as follows point as an attribute of the point defined by the ordered pair of its coordinates polyline as an attribute of the point features defining the polyline as an attribute of each line segment defining the polyline perhaps attributed to its centroid or as an attribute of the entire polyline polygon as an attribute of the point features defining the polygon as an attribute of each line segment making the border of the polygon perhaps attributed to its centroid or as an attribute of the entire polygon perhaps attributed to its centroid or its area this is illustrated in fig 3 now adopting terminology used by herring 2011 we refer to collections of points polylines and polygons by adding the prefix multi as multipoint multipolyline and multipolygon it is noted that herring uses multilinestring but here we adopt multipolyline due to the usage of polyline in gis and to enable an alliteration this is shown in fig 4 the motivation above and derivation so far come primarily from vector data however the underlying representation of any of these feature types can also be achieved through a raster the intention is to derive a taxonomy which describes the spatio temporal structure of the environmental modelling data but which also allows it to be stored in a form most applicable to each use case a commonly used special case of multipoint is a pointcloud point cloud 2020 a usually vast set of points in space typically produced by scanning processes referring to collections of polygons as a multipolygon encompasses additional complexity such as overlapping polygons and cut outs where the data held can be a combination of the two polygons e g a doughnut shape where the data can refer to only the outer ring if more regular spatial structures are used then two special cases of the multipolygon structure are a grid very commonly stored in either vector or raster form where the polygons are regular rectangles with shared vertices and a mesh where the polygons have shared vertices but can be of different shapes and sizes see for example grid classification 2020 we now add in a time dimension to these six feature types excluding the special cases for now by following the csml terminology for measured data adding the suffix series which is also commonly adopted amongst practitioners thus data attributed to a single point which varies in time and not space is referred to as a pointseries as adopted in csml data attributed to a single polyline which varies in time and not space is referred to as a polylineseries and data attributed to a single polygon which varies in time and not space is referred to as a polygonseries time is divided into discrete steps with fixed or variable length this is illustrated in fig 5 it is not necessary for a data value to exist at each time instant data attributed to a collection of fixed points where the data for each of the points varies in time is therefore referred to as a multipointseries and similarly multipolylineseries and multipolygonseries as in fig 6 the spatial structure of a grid gives rise to the special case of a gridseries and the spatial structure of a mesh gives rise to the special case of a meshseries csml allows data attributed to a point to move in both time and space by the creation of a track or trajectory an observation along a discrete path varying in time and space such data can be collected by the example given of a ship taking measurements as it moves the spatial structure of the point is allowed to change location with time and the data associated with the point can also vary with time adopting the shorter suffix track to describe this effect and specifying the point as the spatial feature we describe this as a pointtrack similarly we define polylinetrack and polygontrack where single polylines and polygons can potentially change their location and geometry with time in addition to the data attributed to them this is illustrated in fig 7 a summary of the complete taxonomy is given in table 2 with the definitions listed in table 3 and a class hierarchy diagram in fig 8 4 examples and use cases the spatio temporal modelling taxonomy has been derived from a set of principles and has a logical structure and symmetry however the overriding objective is for it to relate to real world situations and be of practical use it may be pleasing in theory but is it applicable in practice indeed a simpler or a more complex and holistic taxonomy may be possible but any such structured approach must be appropriately balanced between utility and completeness its very existence not least any complexity therein must earn its place in practical application so is a structured vocabulary like this useful and applicable especially in light of the number of generally accepted albeit not always formally defined terms for describing these structures accordingly a variety of examples are given here to illustrate the breadth of relevance of this taxonomy for a selection of use cases this set of examples covers the entire taxonomy each representing a real and practical use case most of which are used commercially 4 1 wave overtopping of sea defences the european overtopping manual pullen et al 2007 presents a set of methods and formulae for calculating the volume of water which overtops stretches of coastline at points represented by different cross shore profiles fig 9 shows an online calculation tool where the user has selected to perform empirical calculations for a composite slope armoured with rubble the tool returns the mean overtopping discharge in the bottom left corner this model output is an example of a point if it were to be applied to a number of points along a coastal stretch at the same time instant or interval then this model output would be a represented by a multipoint polyline or perhaps multipolyline with the data referring to the vertexes if it were to be applied to a single point with the wave conditions changing over a set of timesteps then the output would be represented by a pointseries 4 2 forecasting dengue fever cases in vietnam the d moss dengue fever forecasting system hofmann et al 2018 colón gonzález et al 2019 produces an early warning of numbers of cases of dengue fever driven by earth observation eo data from satellites and seasonal meteorological forecasts the system leverages the relationship between dengue fever and hydrometeorological phenomena such as air temperature and water availability if enough history of each of these phenomena can be obtained then these statistical relationships can be established and exploited to forecast the number of dengue fever cases the first implementation of the d moss system was for vietnam the history of dengue cases are collected by health workers across the country each case is attributed to the geographic region district where it was reported the cases recorded for each district are then aggregated up to province level where each district belongs to a single province each province is made up of about ten districts the districts and indeed the provinces of vietnam could be described as a mulltipolygon see fig 10 therefore both the dataset of dengue fever cases in vietnam and the forecast could each be described as a multipolygonseries a dataset representing cases for the whole of vietnam is of course a single polygon with cases varying in time as a polygonseries in addition to the multipolygon and multipolygonseries a variety of other spatio temporal structures are featured in the information architecture the eo data collected which provides both the history and the regular update of the hydro meteorological data is represented as a gridseries requiring a mapping between this gridseries and the associated multipolygon representing the provinces the el niño index is a scalar quantity associated with an area of ocean and so can be considered as a polygonseries or pointseries related to say the centroid of the polygon the water availability modelling component is calibrated by stream flow measurements these are taken by flow meters at points in the country s river catchments as such they form a multipointseries compared to the hydro meteorological parameters land use type is an almost static parameter which has a high impact on cases of dengue fever this is also obtained from satellite sources and from its original grid raster formulation is post processed into a multipolygon fig 11 shows an example raster data image describing land cover elevation and soil type data also has a grid structure population is given as a multipolygonseries 4 3 modelling groundwater river flow interaction in the river thames catchment harpham et al 2019 includes a brief description of the thames integrated model an integrated model composition of the river thames catchment using the openmi version 2 0 standard a river model mcrouter is linked to two groundwater models zoomq3d jackson and spink 2004 and bgsgw mansour et al 2013 as part of this complex composition of models this link allows the transfer of water from the groundwater system into the river channel and vice versa the river model outputs are represented by a multipolylineseries and both groundwater models are each represented by a different gridseries fig 12 illustrates these two spatio temporal structures and the required mapping between them in order to exchange the data between the two numerical models as such the transfer of water is described by this multipolylineseries gridseries mapping each single point from the multipolylineseries structure where data is produced by the river model is mapped to a set of grid nodes from the gridseries structure of each groundwater model 4 4 comparing wave flume experiments with equivalent numerical models the hydralab programme coordinated activities from a large community of hydraulic laboratories in europe the latest hydralab project included a preliminary community standardisation of data from experiments conducted by those laboratories see for example harpham et al 2018 the standard data package formulated by this work was used in additional experiments to test the interface between physical modelling in the laboratory and numerical modelling the physical modelling element was a wave flume experiment conducted at hr wallingford s fast flow facility 1 1 http www hrwallingford com facilities fast flow facility wave height and velocity data was collected by sensors at five locations along the flume after post processing the resultant data products were offered as pointseries datasets respective not to a global or regional coordinate system but to their x y position within the flume dimensions this is as represented by the csml for observed measured data in totality the dataset was represented as a multipointseries one numerical model version of these conditions was simulated using the swan wave model booij et al 1999 swan simulating waves nearshore is a state of the art 3rd generation spectral wave model which simulates the generation and propagation of random directional waves the swan model was set up in stationary 1 dimensional mode to simulate the flume with the model domain spanning the positions of the five sensors in the flume the post processed results of tests from the experiment are given in fig 13 these results are presented as a polylineseries where the polyline vertices are the positions of the sensors with wave height values changing with time the flume experiment was also duplicated using the funwave model see for example bruno et al 2009 this model was set up as a quasi 1d model with 3 nodes across the width of the flume at the positions of the five sensors as such the representation of the results dataset is a multipolylineseries with the vertexes of each of the five polylines being located at the nodes going across the flume at the positions of each of the five sensors along the flume a third numerical model version of the physical model experiment was performed using the openfoam software package 2 2 http www openfoam com openfoam is a general purpose open source 2 dimensional computational fluid dynamic cfd numerical model see for example cuomo et al 2013 richardson et al 2013 dimakopoulos et al 2014 2016 a single model mesh was used with a uniform background mesh resolution with a characteristic edge length of 0 0175 m in the wave generation absorption zones as well as in the actual flume mesh refinement was relaxed along the x direction toward the relaxation zone according to the methodology in dimakopoulos et al 2016 the mesh structure has approximately 750 000 cells the results from this model are given as a multipolygonseries a meshseries with the positions of the five sensors plotted onto it example snapshots from this meshseries showing the free surface evolution and the velocity field at simulation time t 20s and t 60s are presented in fig 14 4 5 thunderstorm tracking with cb tram cumulonimbus tracking and monitoring cb tram zinner et al 2008 simulates the position of thunderstorms these are portrayed with a set of images which plot the thunderstorm areas as polygons which change shape and position with timesteps experimental output from this model is shown in fig 15 the area shown is the western mediterranean with coastline marked on the image in light green with italy in the top right several thunderstorm cells appear within a northward flow of moist air over this area given with a red border as the nowcast indicates that each progresses north they change shape and position and are marked at the next timestep 1 h later with a dashed red and white border a large cell is located just over the city of marseille in the region of genoa further east along the coastline cb tram marks the thunderstorm cell over an area which was affected by heavy flooding during this weather event this model output is an example of a multipolygontrack with each single polygon represented by a polygontrack 4 6 modelling shoreline evolution beachplan is a shoreline evolution model used to simulate changes in the plan shape of a non cohesive sand or gravel beach over a period of time ozasa and brampton 1980 the beach is discretised into a number of cross shore profiles with a constant beach gradient offshore wave conditions are transformed to the breaker line and a formula is used to calculate the longshore sediment transport rate at each profile caused by wave breaking the model is suited to gently curving beaches with parallel depth contours and simulates the position on one single beach contour as such the typical output from beachplan can be represented as a polylinetrack as illustrated in fig 16 and see also kemp et al 2007 kemp and brampton 2013 also show similar model results but given in probabilistic terms for timesteps that is for a timestep different possible shoreline positions are given maximum shoreline position mean shoreline position and minimum shoreline position this spatio temporal structure is a multipolylinetrack as illustrated in fig 17 also sutherland et al 2013 demonstrates usage of beachplan re engineered as an openmi composition of its component functions exchanging data across the interfaces between them 4 7 creating bathymetry datasets bathymetry datasets are often required for hydrodynamic models particularly in shallow water see for example the novel deformation experiment in harpham et al 2014 such datasets are often compiled by interpolating and stitching together marine surveys when post processed from the raw instrument data such surveys are presented as a pointcloud fig 18 illustrates how different bathymetry surfaces appear when constructed from such surveys 4 8 post processing of ensemble wave forecasts the wavesentry system harpham et al 2016 ingests live data streams in order to update forecasts of wave conditions in areas of open sea it serves as a post processor to existing ensemble wave forecasts ensemble members may be created by perturbing instances of the same model to produce a selection of different model results the ensemble members e g saetra and bidlot 2004 cao et al 2009 behrens 2015 by building collections of different models e g durrant et al 2009 or with a combination of the two e g alves et al 2013 where such an ensemble wave forecast exists it will produce forecasts periodically say once or twice each day between runs of the forecast live wave data can be collected from instruments deployed on various devices this live data can be used to update the ensemble forecast to produce a more accurate result when the forecast is produced each ensemble member is considered equally likely to be closest to observed conditions that is with equal weight as measured data is collected a set of ensemble members will emerge as a closer match to these conditions than the other ensemble members the weighting of each can be updated accordingly to result in an amendment to the overall forecast fig 19 illustrates the different data types which input to this process the original wave forecast model results are given as a multipolygonseries a gridseries the measured data sources which are input into the wavesentry system also follow spatio temporal structures data from sensors fixed in space such as those mounted on buoys are structured as a pointseries data from sensors mounted on vessels or satellites are each structured as a pointtrack or a multipointtrack for a set of sensors considered together the post processed wave forecast carries the same spatio temporal structure as the original forecast a multipolygonseries 5 discussion and conclusions this narrative has so far shown that a taxonomy built around a common pattern see fig 8 can be created based on established principles and formalising commonly used terms it has also been demonstrated to be relevant to a wide variety of environmental modelling use cases this is not surprising since the vast majority if not all environmental modelling data can be said to be applicable in some way to both time and space basing the taxonomy around spatio temporal structures will inevitably find relevance moreover the motivations from basic gis concepts the csml definitions for observed data and the emerging need for adaptors in an environmental modelling standard openmi give the taxonomy the necessary broad set of reference points for its application the utility of the taxonomy is gained by allowing practitioners a compact and simple vocabulary to represent the spatio temporal aspects of their data described in its entirety by the information in table 3 in their own domains these practitioners will design build and implement file standards and data store structures applicable to their own datasets many of these exist already netcdf ogc netcdf 2011 is well established and optimised for grid and gridseries structures timeseriesml ogc timeseriesml for pointseries data las las file format 2019 for multipoint pointcloud datasets more bespoke filetypes like selafin as used in the telemac mascaret modelling suite 3 3 http www opentelemac org index php also exist for mesh and meshseries data the shapefile format is of course a common standard for point multipoint polyline multipolyline polygon and multipolygon datasets general re use of each of these would be greatly facilitated by clearer structured communication about their scope and applicability to certain spatio temporal structures as would an understanding of their weaknesses and inapplicability to others moreover re use of applications dedicated to processing spatio temporal datasets is facilitated through greater understanding of their relevance to specific types of spatio temporal data an application designed to process one type of spatio temporal dataset e g a pointcloud is of little value to a practitioner wishing to process another type e g a pointseries however another pointcloud user may be able to make direct use of such an application for example clipping interpolation formatting and other post processing of grid and gridseries earth observation data from satellites is common to almost all applications which use this source of data but is of little use to a user looking at pointtracks moreover adaptations between commonly used spatio temporal structures can find a high degree of re use by practitioners to this end the openmi standard was published with a set of adaptors between environmental modelling input and output datasets incorporating some of the most common of these see https sourceforge net projects openmi the taxonomy incorporates a degree of complexity considered necessary to be of practical use as standards like openmi develop the taxonomy offers a route to wider more universal standardisation and interoperability direct adoption in standards and frameworks offers a first step to achieving this however limitations do exist for certain use cases in spatial terms it has only been defined to cover 2 dimensions many of the spatio temporal feature type structures translate directly and intuitively into 3 dimensions but others do not those based around the basic point could equally be represented in 3 dimensions as two simply by adding a third dimension to the location of the point s indeed the pointcloud is almost always 3 dimensional the structures based around the basic polyline are similar in this regard with each of its vertices represented in 3 dimensions rather than two conceptually feature type structures based around the polygon are similar where polygons become polyhedrons but this increases the complexity since the faces of the polyhedrons are themselves polygons the taxonomy could be extended to incorporate this if required also the taxonomy does not detail lower level topological details such as line direction which can be useful for example when modelling flow and cell adjacency which can be useful for example for efficient processing of multipolygon structures the descriptions in table 3 could be expanded to include such optional attributes however it is submitted that this is best incorporated into the lower level data structure definitions the concept of an adaptive mesh 4 4 https en wikipedia org wiki adaptive mesh refinement that is a mesh which changes structure in time in the number of the polygons making up the mesh as well as in their shape and size is not represented in the taxonomy as it stands the closest structure is that of a multipolygontrack which allows the polygons to change shape and position with time but this structure does not allow the number of polygons to change and certainly not at the rapid rate that they typically appear and disappear in an adaptive mesh the adaptive mesh is a common use case and so the term adaptivemesh could be added to the taxonomy to cover this certain of the spatio temporal feature types represented in the taxonomy are also often provided in duplicate sets for example meteorological forecasts such as glosea5 maclachlan et al 2014 are produced as ensembles that is say 42 possible results each represented in the same identical gridseries structure since each ensemble member in its own right is a valid gridseries it is questionable whether any real value is added by acknowledging the presence of the entire ensemble however if this is required the suffix ensemble could be added to any of the spatio temporal feature types in the taxonomy for example glosea5 forecast outputs could be said to be a gridseriesensemble structure overall the spatio temporal modelling taxonomy tries to find the balance between complexity and utility it is offered to practitioners particularly those trying to navigate tuckman s storming stage as more integration is required in environmental modelling declaration of competing interest i confirm that there is no conflict of interest concerning this publication acknowledgements this work is a further development of phd studies undertaken at and funded by hr wallingford as an associated research centre of the open university harpham 2019 much of the understanding and examples used in this paper originate from the development and application of the openmi 2 0 standard this is one of the results of over a decade of research by a wide variety of individuals the projects include harmonit ec contract evk1 ct 2001 00090 and openmi life grant agreement number life06 env uk 000409 drihm ec grant agreement 283568 and drihm2us ec grant agreement 313122 fluidearth hr wallingford the dengue fever forecasting use case is an example of more recent incorporated research into spatio temporal structures which has been conducted as part of the uksa ipp d moss project building and implementing a dengue fever forecasting system for vietnam https www gov uk government news new projects see uk space firms tackle southeast asian challenges the physical modelling interface use case was conducted as part of the hydralab project which received funding from the european union s horizon 2020 research and innovation programme under grant agreement no 654110 hydralab plus appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104810 
25960,environmental modelling practitioners are now seeking to move forwards together and build standards and technologies with more universal applicability and integration with so many overlapping environmental modelling technologies and infrastructures being offered and with so many relevant supporting technologies contributing on the periphery there is considerable scope to articulate and utilise underlying concepts which draw them together as such this paper offers the spatio temporal modelling taxonomy a simple taxonomy for describing the spatio temporal structure of environmental numerical modelling data used as input to or produced as output from environmental numerical models the taxonomy is motivated from common spatial structures a set of feature types to describe observed environmental data and the implementation of the openmi integrated modelling standard it serves as a natural evolution of terminology that is in common use in environmental numerical modelling and is designed to strike the right balance between complexity and utility it implements a structured theoretical framework whilst being essentially practical in nature to apply to the real world facing numerical modellers and those seeking to integrate environmental numerical models and the data supporting them keywords spatio temporal feature type environmental modelling data structure taxonomy vocabulary software availability an xml encoding of the full spatio temporal modelling taxonomy is given in the spatio temporal modelling taxonomy github repository https github com qharpham spatio temporal modelling taxonomy 1 introduction it was suggested by bruce tuckman 1965 that groups go through a developmental sequence characterised by four stages forming storming norming and performing during the forming stage group members are being introduced to one another as they gradually form the team whilst still acting fairly independently as they begin to approach the task together the storming stage sees opinions voiced personality clashes and if the group is to be productive the resolution of differences of direction and emphasis then the group beings to settle down and perform as a coherent team if a similar idea could be applied to entire scientific disciplines then the relatively young discipline of integrated environmental modelling iem see for example meiburg 2008 laniak et al 2013 sutherland et al 2014 could be said to be entering the storming stage integrated environmental modelling seeks to link together potentially disparate data sources and numerical models to better understand the interconnectivity of environmental processes thereby improving the accuracy of simulations the objective is to bring more structure standardisation and formality into a set of previously ad hoc and compartmentalised numerical modelling use cases inevitably this activity spans a number of different numerical modelling disciplines which need to work together one single use case can cover a wide variety of modelling and data management approaches for example studying flash flooding can require implementation of meteorological models to simulate rainfall hydrological models to simulate drainage hydraulic models to simulate open channel flow overtopping and flood spreading and impact models to simulate damage to property and infrastructure and hazard to people danovaro et al 2014 indeed d agostino et al 2014 demonstrate a distributed infrastructure for implementing such a numerical modelling chain using high performance computing for meteorological models grid computing for hydrological models and cloud computing for hydraulic and impact models groups from different disciplines have made considerable progress in producing technical infrastructures and environments to facilitate practitioners integrated environmental modelling activities examples include the community surface dynamics modeling system csdms peckham 2013 earth system modelling framework esmf hill et al 2004 collins et al 2005 frames whelan et al 2014 oasis valcke 2013 craig et al 2017 opengms chen et al 2019 and the open modelling interface openmi fluidearth harpham et al 2014 2019 buahin and horsburgh 2018 gregersen et al 2007 such environments are usually based around methods to couple numerical models together and to handle the provision of the necessary supporting data many other related technologies also offer relevant functionality including handling workflow steps uncertainty in model results and the portrayal of results the status of these activities would certainly correspond to tuckman s forming stage with so many overlapping technologies having been developed in recent years and so many relevant supporting technologies contributing on the periphery the situation could be described as being a little chaotic laniak et al 2013 lay out a vision and roadmap for the future of integrated environmental modelling which requires that the global community of iem stakeholders transcend social and organizational boundaries and pursue greater levels of collaboration sutherland et al 2014 propose six topics of consolidation metadata for data and models supporting information software as a service linking or interface technologies diagnostic or reasoning tools and the portrayal and understanding of integrated modelling that is demonstrating the structure verification and validation of the linked composition they emphasise the importance of open software architectures open source software and a community that values openness and the sharing of models and data consolidation efforts such as these indicate a desire to move forwards together to build standards and technologies with more universal applicability technologies which would support this move inevitably pertain most closely to the use cases that drove their creation and often unknown to their creators make assumptions that do not hold for the practitioners in adjacent domains this is entirely understandable since all groups of numerical modellers will create tools and technologies which solve the problems immediate to them and they should be heralded for offering their technologies to others for example those for whom performance is a critical issue will introduce technologies to facilitate faster solutions e g craig et al 2005 those for whom flexibility is a critical issue will introduce technologies to facilitate flexible solutions e g harpham et al 2019 moreover doubt has been cast over some aspects of the relative gain in attempting usage of tools intended to increase productivity due to the typically long learning curve iwanaga 2018 any productivity gained by a tool designed to manage a complex task can be hard won and only attained by a user with considerable patience and perseverance however examples such as harpham et al 2019 and danovaro et al 2014 illustrate the underlying potential of these technologies and all of these factors together bear the hallmarks of a group moving into tuckman s storming stage attempts to bring together and characterise iem trends and technologies such as laniak et al 2013 and sutherland et al 2014 tend to refer to characteristics which are offered as common topics suitable for collaborative development for example these two agree on the need for standards and metadata to publish environmental numerical models with a commonly agreed descriptive framework such as that offered by harpham and danovaro 2015 such a framework will inevitably refer to the spatio temporal nature of the data which is required as input to the models and the data which is produced by the models as one of the key ingredients in moving iem forwards through and beyond tuckman s storming stage this paper offers a simple taxonomy for describing the spatio temporal nature of such environmental numerical modelling data with the concept and an initial arrangement first introduced in harpham 2019 this spatio temporal modelling taxonomy serves as a natural evolution of terminology that is in common use in environmental numerical modelling it structures formalises and extends this terminology to apply to data used in a wide variety of modelling domains and hence facilitates iem across these domains as well as within each of them it implements a structured theoretical framework whilst being essentially practical in nature to apply to the real world facing numerical modellers and those seeking to integrate environmental numerical models and the data supporting them 2 motivation three independent technical innovations help to motivate and shape the derivation of the taxonomy for describing the spatio temporal structure of the data supporting environmental numerical modelling common spatial structures the climate science modelling language csml and usage of adaptors in openmi 2 1 common spatial structures the common shapefile format for geographic information systems gis describes spatial vector features in terms of points polylines polygonal chains or linestrings and polygons the wikipedia definitions of these features are paraphrased as follows point a unique dimensionless location in euclidean space specified by an ordered pair of numbers for 2 dimensional space and a triplet for 3 dimensional space point geometry 2019 polyline polygonal chain linestring a polygonal chain is a connected series of line segments a curve specified by a sequence of point features vertices polygonal chain 2019 polygon a plane figure that is described by a finite number of straight line segments which are connected to form a closed circuit polygon 2019 including these well established constructs herring 2011 presents an opengis implementation standard for geographic information describing the relationships between these and other simple features some of which are constructed from point line linestring and polygon features in particular and as used in feature layers within the shapefile format multipoint multilinestring and multipolygon are given as a collections of points linestrings and polygons respectively 2 2 climate science modelling language the iso19156 observations and measurements o m model ogc observations and measurements 2013 iso19156 2011 offers a taxonomy of basic spatial sampling features sf samplingpoint sf samplingcurve sf samplingsurface and sf samplingsolid these are based on the shape types gm point gm curve gm surface and gm solid seventeen application sampling features such as a borehole and a shipstrack are mapped to each of the sampling features this provides a motivation for csml which offers a set of spatio temporal feature types intended to be a practical set describing measured environmental data which has been collected by deployed instruments woolf and lowe 2010 in addition to the spatial properties of the feature types a temporal dimension is brought in in version 3 0 of csml ten practical feature types are given nine are specialisations of the o m model and one is a direct derivation they are summarised in table 1 within csml a narrower variety of spatial structures than that given in the opengis implementation standard for geographic information are considered necessary to describe the data collected by measuring devices the feature types additionally incorporate a temporal aspect which points towards a set which would typically be required by environmental numerical modellers however the motivation for csml and indeed o m derives from measured observed data and so the full set of structures required for environmental numerical modelling are not represented 2 3 adaptors in openmi now more closely considering applications of these concepts within environmental numerical modelling the openmi version 2 0 standard harpham et al 2019 the most recent version of openmi enables the transfer of data between numerical models as they proceed through their respective time horizons the output data from one numerical model may be passed into a second numerical model along a pair of connected exchange items output to input this transfer of data can take place in both directions so that the numerical models have opportunity to influence each other version 2 0 of the standard breaks down the spatial aspect of numerical model inputs and outputs into familiar spatial constructs this is governed by the ispatialdefinition interface ogc openmi 2014 section 6 4 the element set of the spatial structure ielementset includes management of indexes counts and coordinates with each element enumerated into five types idbased point polyline polygon and polyhedron this specification allows the representation of geospatial objects used in environmental modelling but it does not offer additional topological specification such as cell adjacency common structures such as model grids are constructed from polygon elements to handle any differences between the outputs offered by one numerical model and the inputs required by another openmi uses adaptors ogc openmi 2014 section 6 10 these adaptors are independent from the numerical model components as illustrated in fig 1 for example harpham et al 2014 gives a grid to grid adaptor interpolating between two model grids belonging to different models in a single composition adaptors are independent entities and can be chained together to perform all of the required functions they can be re used as appropriate and focus on specific aspects for example transferring the data between two models can require temporal interpolation spatial interpolation and unit transformation different models in the composition can require different combinations of these this is illustrated in fig 2 due to the high degree of flexibility offered by openmi a wide variety of spatio temporal structures may be used in compositions of numerical models measured data sources such as live feeds coming from deployed instruments or archives of previously collected readings may also be included these can incorporate a variety of spatio temporal structures more typically adopted by measured data sources such as those offered by csml table 1 the independent adaptor concept allows for the creation of reusable adaptor components which since they are openmi compliant can exist independently of the numerical models which they serve in order to articulate their purpose the functionality of these adaptor components must be characterised in some way some adaptors may perform standard unit transformations metres to feet kilograms to pounds other adaptors may interpolate between different spatio temporal structures data pertaining to a single point but varying in time to data pertaining to a single polygon and varying in time it is clear that a taxonomy to describe the spatio temporal structures used by numerical models will both aid articulation of the functionality and requirements of individual models as well as facilitating the construction of the required functions and optimised file storage formats for connecting them together openmi refers to these functions as adaptors but similar constructs exist for all leading technologies which seek to integrate numerical models with an author list that includes representatives from openmi csdms esmf opengms as well as social scientists and gis experts chen et al 2020 make the observation that geographic models produce outputs that include a wide variety of spatial feature types such as grids points and meshes this is demonstrated in the csdms basic model interface bmi peckham et al 2013 which provides a set of seven grid types to describe the model data spatial structure scalar points vector unstructured structured quadrilateral rectilinear and uniform rectilinear another example is esmf hill et al 2004 collins et al 2005 which offers a set of associated grid types grid logically rectangular mesh unstructured locstream location stream a set of unconnected points however chen et al 2020 continue to make the observation that although the implementations of standards such as openmi offer low level flexibility in interpolating among feature types when implemented in different time step schemes and some discrete global grids have been developed to express grid nodes edges and cells in a uniform way to support spatial data organization more work is still required to make this truly generic practical and efficient this indicates that gains are to be made with a higher level representation independent of any implementing technology indeed there is no formally adopted vocabulary describing the full breadth of spatio temporal structures possible in environmental modelling motivated by common spatial structures csml and openmi adaptors the spatio temporal modelling taxonomy offered here seeks to describe the spatio temporal structure of all such data in a simple and accessible way 3 derivation the three motivations discussed above common spatial structures csml and openmi adopt an implementation of point polyline and polygon in some form the common spatial structures and openmi refer to these directly with grids constructed from polygons and csml implements points trajectories including a time dimension to the structure and grids we therefore begin with purely spatial structures in 2 dimensions with these three adopting the terminology point polyline and polygon as is established in gis using these spatial structures data in this case numerical modelling results or measurements can be stored as follows point as an attribute of the point defined by the ordered pair of its coordinates polyline as an attribute of the point features defining the polyline as an attribute of each line segment defining the polyline perhaps attributed to its centroid or as an attribute of the entire polyline polygon as an attribute of the point features defining the polygon as an attribute of each line segment making the border of the polygon perhaps attributed to its centroid or as an attribute of the entire polygon perhaps attributed to its centroid or its area this is illustrated in fig 3 now adopting terminology used by herring 2011 we refer to collections of points polylines and polygons by adding the prefix multi as multipoint multipolyline and multipolygon it is noted that herring uses multilinestring but here we adopt multipolyline due to the usage of polyline in gis and to enable an alliteration this is shown in fig 4 the motivation above and derivation so far come primarily from vector data however the underlying representation of any of these feature types can also be achieved through a raster the intention is to derive a taxonomy which describes the spatio temporal structure of the environmental modelling data but which also allows it to be stored in a form most applicable to each use case a commonly used special case of multipoint is a pointcloud point cloud 2020 a usually vast set of points in space typically produced by scanning processes referring to collections of polygons as a multipolygon encompasses additional complexity such as overlapping polygons and cut outs where the data held can be a combination of the two polygons e g a doughnut shape where the data can refer to only the outer ring if more regular spatial structures are used then two special cases of the multipolygon structure are a grid very commonly stored in either vector or raster form where the polygons are regular rectangles with shared vertices and a mesh where the polygons have shared vertices but can be of different shapes and sizes see for example grid classification 2020 we now add in a time dimension to these six feature types excluding the special cases for now by following the csml terminology for measured data adding the suffix series which is also commonly adopted amongst practitioners thus data attributed to a single point which varies in time and not space is referred to as a pointseries as adopted in csml data attributed to a single polyline which varies in time and not space is referred to as a polylineseries and data attributed to a single polygon which varies in time and not space is referred to as a polygonseries time is divided into discrete steps with fixed or variable length this is illustrated in fig 5 it is not necessary for a data value to exist at each time instant data attributed to a collection of fixed points where the data for each of the points varies in time is therefore referred to as a multipointseries and similarly multipolylineseries and multipolygonseries as in fig 6 the spatial structure of a grid gives rise to the special case of a gridseries and the spatial structure of a mesh gives rise to the special case of a meshseries csml allows data attributed to a point to move in both time and space by the creation of a track or trajectory an observation along a discrete path varying in time and space such data can be collected by the example given of a ship taking measurements as it moves the spatial structure of the point is allowed to change location with time and the data associated with the point can also vary with time adopting the shorter suffix track to describe this effect and specifying the point as the spatial feature we describe this as a pointtrack similarly we define polylinetrack and polygontrack where single polylines and polygons can potentially change their location and geometry with time in addition to the data attributed to them this is illustrated in fig 7 a summary of the complete taxonomy is given in table 2 with the definitions listed in table 3 and a class hierarchy diagram in fig 8 4 examples and use cases the spatio temporal modelling taxonomy has been derived from a set of principles and has a logical structure and symmetry however the overriding objective is for it to relate to real world situations and be of practical use it may be pleasing in theory but is it applicable in practice indeed a simpler or a more complex and holistic taxonomy may be possible but any such structured approach must be appropriately balanced between utility and completeness its very existence not least any complexity therein must earn its place in practical application so is a structured vocabulary like this useful and applicable especially in light of the number of generally accepted albeit not always formally defined terms for describing these structures accordingly a variety of examples are given here to illustrate the breadth of relevance of this taxonomy for a selection of use cases this set of examples covers the entire taxonomy each representing a real and practical use case most of which are used commercially 4 1 wave overtopping of sea defences the european overtopping manual pullen et al 2007 presents a set of methods and formulae for calculating the volume of water which overtops stretches of coastline at points represented by different cross shore profiles fig 9 shows an online calculation tool where the user has selected to perform empirical calculations for a composite slope armoured with rubble the tool returns the mean overtopping discharge in the bottom left corner this model output is an example of a point if it were to be applied to a number of points along a coastal stretch at the same time instant or interval then this model output would be a represented by a multipoint polyline or perhaps multipolyline with the data referring to the vertexes if it were to be applied to a single point with the wave conditions changing over a set of timesteps then the output would be represented by a pointseries 4 2 forecasting dengue fever cases in vietnam the d moss dengue fever forecasting system hofmann et al 2018 colón gonzález et al 2019 produces an early warning of numbers of cases of dengue fever driven by earth observation eo data from satellites and seasonal meteorological forecasts the system leverages the relationship between dengue fever and hydrometeorological phenomena such as air temperature and water availability if enough history of each of these phenomena can be obtained then these statistical relationships can be established and exploited to forecast the number of dengue fever cases the first implementation of the d moss system was for vietnam the history of dengue cases are collected by health workers across the country each case is attributed to the geographic region district where it was reported the cases recorded for each district are then aggregated up to province level where each district belongs to a single province each province is made up of about ten districts the districts and indeed the provinces of vietnam could be described as a mulltipolygon see fig 10 therefore both the dataset of dengue fever cases in vietnam and the forecast could each be described as a multipolygonseries a dataset representing cases for the whole of vietnam is of course a single polygon with cases varying in time as a polygonseries in addition to the multipolygon and multipolygonseries a variety of other spatio temporal structures are featured in the information architecture the eo data collected which provides both the history and the regular update of the hydro meteorological data is represented as a gridseries requiring a mapping between this gridseries and the associated multipolygon representing the provinces the el niño index is a scalar quantity associated with an area of ocean and so can be considered as a polygonseries or pointseries related to say the centroid of the polygon the water availability modelling component is calibrated by stream flow measurements these are taken by flow meters at points in the country s river catchments as such they form a multipointseries compared to the hydro meteorological parameters land use type is an almost static parameter which has a high impact on cases of dengue fever this is also obtained from satellite sources and from its original grid raster formulation is post processed into a multipolygon fig 11 shows an example raster data image describing land cover elevation and soil type data also has a grid structure population is given as a multipolygonseries 4 3 modelling groundwater river flow interaction in the river thames catchment harpham et al 2019 includes a brief description of the thames integrated model an integrated model composition of the river thames catchment using the openmi version 2 0 standard a river model mcrouter is linked to two groundwater models zoomq3d jackson and spink 2004 and bgsgw mansour et al 2013 as part of this complex composition of models this link allows the transfer of water from the groundwater system into the river channel and vice versa the river model outputs are represented by a multipolylineseries and both groundwater models are each represented by a different gridseries fig 12 illustrates these two spatio temporal structures and the required mapping between them in order to exchange the data between the two numerical models as such the transfer of water is described by this multipolylineseries gridseries mapping each single point from the multipolylineseries structure where data is produced by the river model is mapped to a set of grid nodes from the gridseries structure of each groundwater model 4 4 comparing wave flume experiments with equivalent numerical models the hydralab programme coordinated activities from a large community of hydraulic laboratories in europe the latest hydralab project included a preliminary community standardisation of data from experiments conducted by those laboratories see for example harpham et al 2018 the standard data package formulated by this work was used in additional experiments to test the interface between physical modelling in the laboratory and numerical modelling the physical modelling element was a wave flume experiment conducted at hr wallingford s fast flow facility 1 1 http www hrwallingford com facilities fast flow facility wave height and velocity data was collected by sensors at five locations along the flume after post processing the resultant data products were offered as pointseries datasets respective not to a global or regional coordinate system but to their x y position within the flume dimensions this is as represented by the csml for observed measured data in totality the dataset was represented as a multipointseries one numerical model version of these conditions was simulated using the swan wave model booij et al 1999 swan simulating waves nearshore is a state of the art 3rd generation spectral wave model which simulates the generation and propagation of random directional waves the swan model was set up in stationary 1 dimensional mode to simulate the flume with the model domain spanning the positions of the five sensors in the flume the post processed results of tests from the experiment are given in fig 13 these results are presented as a polylineseries where the polyline vertices are the positions of the sensors with wave height values changing with time the flume experiment was also duplicated using the funwave model see for example bruno et al 2009 this model was set up as a quasi 1d model with 3 nodes across the width of the flume at the positions of the five sensors as such the representation of the results dataset is a multipolylineseries with the vertexes of each of the five polylines being located at the nodes going across the flume at the positions of each of the five sensors along the flume a third numerical model version of the physical model experiment was performed using the openfoam software package 2 2 http www openfoam com openfoam is a general purpose open source 2 dimensional computational fluid dynamic cfd numerical model see for example cuomo et al 2013 richardson et al 2013 dimakopoulos et al 2014 2016 a single model mesh was used with a uniform background mesh resolution with a characteristic edge length of 0 0175 m in the wave generation absorption zones as well as in the actual flume mesh refinement was relaxed along the x direction toward the relaxation zone according to the methodology in dimakopoulos et al 2016 the mesh structure has approximately 750 000 cells the results from this model are given as a multipolygonseries a meshseries with the positions of the five sensors plotted onto it example snapshots from this meshseries showing the free surface evolution and the velocity field at simulation time t 20s and t 60s are presented in fig 14 4 5 thunderstorm tracking with cb tram cumulonimbus tracking and monitoring cb tram zinner et al 2008 simulates the position of thunderstorms these are portrayed with a set of images which plot the thunderstorm areas as polygons which change shape and position with timesteps experimental output from this model is shown in fig 15 the area shown is the western mediterranean with coastline marked on the image in light green with italy in the top right several thunderstorm cells appear within a northward flow of moist air over this area given with a red border as the nowcast indicates that each progresses north they change shape and position and are marked at the next timestep 1 h later with a dashed red and white border a large cell is located just over the city of marseille in the region of genoa further east along the coastline cb tram marks the thunderstorm cell over an area which was affected by heavy flooding during this weather event this model output is an example of a multipolygontrack with each single polygon represented by a polygontrack 4 6 modelling shoreline evolution beachplan is a shoreline evolution model used to simulate changes in the plan shape of a non cohesive sand or gravel beach over a period of time ozasa and brampton 1980 the beach is discretised into a number of cross shore profiles with a constant beach gradient offshore wave conditions are transformed to the breaker line and a formula is used to calculate the longshore sediment transport rate at each profile caused by wave breaking the model is suited to gently curving beaches with parallel depth contours and simulates the position on one single beach contour as such the typical output from beachplan can be represented as a polylinetrack as illustrated in fig 16 and see also kemp et al 2007 kemp and brampton 2013 also show similar model results but given in probabilistic terms for timesteps that is for a timestep different possible shoreline positions are given maximum shoreline position mean shoreline position and minimum shoreline position this spatio temporal structure is a multipolylinetrack as illustrated in fig 17 also sutherland et al 2013 demonstrates usage of beachplan re engineered as an openmi composition of its component functions exchanging data across the interfaces between them 4 7 creating bathymetry datasets bathymetry datasets are often required for hydrodynamic models particularly in shallow water see for example the novel deformation experiment in harpham et al 2014 such datasets are often compiled by interpolating and stitching together marine surveys when post processed from the raw instrument data such surveys are presented as a pointcloud fig 18 illustrates how different bathymetry surfaces appear when constructed from such surveys 4 8 post processing of ensemble wave forecasts the wavesentry system harpham et al 2016 ingests live data streams in order to update forecasts of wave conditions in areas of open sea it serves as a post processor to existing ensemble wave forecasts ensemble members may be created by perturbing instances of the same model to produce a selection of different model results the ensemble members e g saetra and bidlot 2004 cao et al 2009 behrens 2015 by building collections of different models e g durrant et al 2009 or with a combination of the two e g alves et al 2013 where such an ensemble wave forecast exists it will produce forecasts periodically say once or twice each day between runs of the forecast live wave data can be collected from instruments deployed on various devices this live data can be used to update the ensemble forecast to produce a more accurate result when the forecast is produced each ensemble member is considered equally likely to be closest to observed conditions that is with equal weight as measured data is collected a set of ensemble members will emerge as a closer match to these conditions than the other ensemble members the weighting of each can be updated accordingly to result in an amendment to the overall forecast fig 19 illustrates the different data types which input to this process the original wave forecast model results are given as a multipolygonseries a gridseries the measured data sources which are input into the wavesentry system also follow spatio temporal structures data from sensors fixed in space such as those mounted on buoys are structured as a pointseries data from sensors mounted on vessels or satellites are each structured as a pointtrack or a multipointtrack for a set of sensors considered together the post processed wave forecast carries the same spatio temporal structure as the original forecast a multipolygonseries 5 discussion and conclusions this narrative has so far shown that a taxonomy built around a common pattern see fig 8 can be created based on established principles and formalising commonly used terms it has also been demonstrated to be relevant to a wide variety of environmental modelling use cases this is not surprising since the vast majority if not all environmental modelling data can be said to be applicable in some way to both time and space basing the taxonomy around spatio temporal structures will inevitably find relevance moreover the motivations from basic gis concepts the csml definitions for observed data and the emerging need for adaptors in an environmental modelling standard openmi give the taxonomy the necessary broad set of reference points for its application the utility of the taxonomy is gained by allowing practitioners a compact and simple vocabulary to represent the spatio temporal aspects of their data described in its entirety by the information in table 3 in their own domains these practitioners will design build and implement file standards and data store structures applicable to their own datasets many of these exist already netcdf ogc netcdf 2011 is well established and optimised for grid and gridseries structures timeseriesml ogc timeseriesml for pointseries data las las file format 2019 for multipoint pointcloud datasets more bespoke filetypes like selafin as used in the telemac mascaret modelling suite 3 3 http www opentelemac org index php also exist for mesh and meshseries data the shapefile format is of course a common standard for point multipoint polyline multipolyline polygon and multipolygon datasets general re use of each of these would be greatly facilitated by clearer structured communication about their scope and applicability to certain spatio temporal structures as would an understanding of their weaknesses and inapplicability to others moreover re use of applications dedicated to processing spatio temporal datasets is facilitated through greater understanding of their relevance to specific types of spatio temporal data an application designed to process one type of spatio temporal dataset e g a pointcloud is of little value to a practitioner wishing to process another type e g a pointseries however another pointcloud user may be able to make direct use of such an application for example clipping interpolation formatting and other post processing of grid and gridseries earth observation data from satellites is common to almost all applications which use this source of data but is of little use to a user looking at pointtracks moreover adaptations between commonly used spatio temporal structures can find a high degree of re use by practitioners to this end the openmi standard was published with a set of adaptors between environmental modelling input and output datasets incorporating some of the most common of these see https sourceforge net projects openmi the taxonomy incorporates a degree of complexity considered necessary to be of practical use as standards like openmi develop the taxonomy offers a route to wider more universal standardisation and interoperability direct adoption in standards and frameworks offers a first step to achieving this however limitations do exist for certain use cases in spatial terms it has only been defined to cover 2 dimensions many of the spatio temporal feature type structures translate directly and intuitively into 3 dimensions but others do not those based around the basic point could equally be represented in 3 dimensions as two simply by adding a third dimension to the location of the point s indeed the pointcloud is almost always 3 dimensional the structures based around the basic polyline are similar in this regard with each of its vertices represented in 3 dimensions rather than two conceptually feature type structures based around the polygon are similar where polygons become polyhedrons but this increases the complexity since the faces of the polyhedrons are themselves polygons the taxonomy could be extended to incorporate this if required also the taxonomy does not detail lower level topological details such as line direction which can be useful for example when modelling flow and cell adjacency which can be useful for example for efficient processing of multipolygon structures the descriptions in table 3 could be expanded to include such optional attributes however it is submitted that this is best incorporated into the lower level data structure definitions the concept of an adaptive mesh 4 4 https en wikipedia org wiki adaptive mesh refinement that is a mesh which changes structure in time in the number of the polygons making up the mesh as well as in their shape and size is not represented in the taxonomy as it stands the closest structure is that of a multipolygontrack which allows the polygons to change shape and position with time but this structure does not allow the number of polygons to change and certainly not at the rapid rate that they typically appear and disappear in an adaptive mesh the adaptive mesh is a common use case and so the term adaptivemesh could be added to the taxonomy to cover this certain of the spatio temporal feature types represented in the taxonomy are also often provided in duplicate sets for example meteorological forecasts such as glosea5 maclachlan et al 2014 are produced as ensembles that is say 42 possible results each represented in the same identical gridseries structure since each ensemble member in its own right is a valid gridseries it is questionable whether any real value is added by acknowledging the presence of the entire ensemble however if this is required the suffix ensemble could be added to any of the spatio temporal feature types in the taxonomy for example glosea5 forecast outputs could be said to be a gridseriesensemble structure overall the spatio temporal modelling taxonomy tries to find the balance between complexity and utility it is offered to practitioners particularly those trying to navigate tuckman s storming stage as more integration is required in environmental modelling declaration of competing interest i confirm that there is no conflict of interest concerning this publication acknowledgements this work is a further development of phd studies undertaken at and funded by hr wallingford as an associated research centre of the open university harpham 2019 much of the understanding and examples used in this paper originate from the development and application of the openmi 2 0 standard this is one of the results of over a decade of research by a wide variety of individuals the projects include harmonit ec contract evk1 ct 2001 00090 and openmi life grant agreement number life06 env uk 000409 drihm ec grant agreement 283568 and drihm2us ec grant agreement 313122 fluidearth hr wallingford the dengue fever forecasting use case is an example of more recent incorporated research into spatio temporal structures which has been conducted as part of the uksa ipp d moss project building and implementing a dengue fever forecasting system for vietnam https www gov uk government news new projects see uk space firms tackle southeast asian challenges the physical modelling interface use case was conducted as part of the hydralab project which received funding from the european union s horizon 2020 research and innovation programme under grant agreement no 654110 hydralab plus appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104810 
25961,we introduce simple g a simplified international model of agricultural prices land use and the environment gridded version which is a novel tool for evaluating sustainability policies in a global context while factoring in local heterogeneity in land and water resources and natural ecosystem services this multi scale model can provide boundary conditions for local decision makers as well as capturing feedback from local policies to national and global scales to illustrate its value in environmental analysis we provide two applications of the model first we quantify the local stresses on land and water resources due to global changes in population income and productivity second we quantify the global impacts of local policy responses and adaptations to water scarcity graphical abstract image 1 keywords sustainability agriculture environmental stresses multiscale modelling global change water scarcity land use 1 introduction the world faces significant sustainability challenges in the decades ahead united nations 2019 growing populations and rising incomes are placing unprecedented stresses on the planetary boundaries with the world s land and water resources at growing risk steffen et al 2015 the challenge posed in making such assessments is that sustainability stresses do not respect disciplinary boundaries furthermore while the stresses are often highly localized the drivers of these stresses are global and the local responses can feedback to national and global outcomes for this reason assessment of the underlying risks as well as potential solutions is typically undertaken with a suite of models using complex approaches that often preclude replication and use by researchers outside the core group obersteiner et al 2016 springmann et al 2018 up to this point there have been just a few open source bottom up economic environmental modelling framework capable of analyzing global sustainability at the resolution of individual grid cells lotze campen et al 2008 valin et al 2013 there is clearly a tradeoff between complexity and accessibility models used in teaching and academic research are generally simpler than those developed by national and international labs and research institutions having a relatively simple global grid resolving sustainability framework that can be also run in cloud will allow wider participation in sustainability discussions and can facilitate greater crowd sourcing of new ideas data and parameters to enrich the representation of local stresses policies and adaptations this paper introduces such a modelling framework simple g a simplified international model of agricultural prices land use and the environment gridded version the simple g framework allows for analysis of the interplay between economic and environmental systems taking account of the actions of local agricultural producers pertaining to land and water use within the context of regional and global commodity markets this model integrates economic theories with environmental sciences to analyze the biophysical and economic impacts at different geospatial scales the economic supply of land and water takes account of local institutions biophysical characteristics sustainability criteria along with maximum available resources as a consequence heterogeneous local constraints lead to different rates of change in land and water use on the demand side growth in income and population lead to changes in food consumption baskets and changes in agricultural trade patterns integrating the human and earth system analysis within a global economic framework is a challenging task and often focuses on one way linkages such as those used in down scaling regional results to a grid cell level reilly et al 2012 it has also been common practice to extrapolate from sophisticated grid level analysis to national scale by assuming that the share of production or land use is unchanging schlenker and roberts 2009 bridging local national and global scales within a single framework is challenging yet it is essential if we wish to bring into consideration the behavior of local decision makers within the context of global sustainability analysis in the simple g framework laid out here these decisions are made endogenously considering local biophysical characteristics and institutions as well as nationally determined input prices and globally determined commodity prices despite computational advances model solution time remains another major challenge for integrated frameworks particularly those utilized by individual researchers without access to high performance computing at national labs and major research institutions with simple g we introduce a solution strategy that dramatically reduces computing time permitting individuals to solve a version of simple g with a million grid cells in a matter of minutes on a desktop computer furthermore by implementing simple g on one of the nsf funded hubzero sites geohub we have made the model along with visualization software readily available to any user with access to a web browser this greatly expands access to multi scale modelling of sustainability challenges at the interface of agriculture and the environment it should also accelerate the development of new and improved data bases and representations of local institutions and other constraints within this framework to illustrate the versatility of simple g in sustainability analysis we highlight an implementation of this framework wherein the us has been broken out in detail 5 arc minutes while other regions are aggregated previous applications have disaggregated the globe uniformly 30 arc minutes liu et al 2017 we undertake two experiments aimed at highlighting two different types of analyses that can be undertaken with simple g in the first we investigate the contribution of global changes in population technology and income to changes in gridded us water and land use by mid century it includes global demand shocks as well as local supply responses this highlights locations most vulnerable to land and water stresses furthermore we tie these stresses to individual global change drivers including for example population growth in africa or income growth in asia by linking local environmental stresses in the us to global change drivers we underscore the essence of 21st century global sustainability challenges in a second application of the simple g framework we focus on the local to global feedbacks associated with sustainability policies in this case we begin with the projections made in the first experiment but now we overlay a location specific sustainability policy in particular we do not allow irrigation withdrawals to increase from the present day in those grid cells where withdrawals currently exceed recharge rates we then explore how these sustainability restrictions alter global prices production consumption and trade the remainder of the paper is organized as follows section two provides an overview of the model section three introduces the diverse information used in the construction of the database and parameters for simple g us section four describes the software and implementation of this model section five explores the two experiments mentioned above and the final section provides further discussion and conclusions 2 model the simple g model is based on simple a simplified international model of agricultural prices land use and the environment baldos and hertel 2013 this is a partial equilibrium agricultural trade model that has been validated for the study of long run sustainability and food security baldos and hertel 2014 we extend the simple model to include gridded biophysical and economic relationships hence the name simple g this model is multi scale in other words it simultaneously solves for outcomes at the level of tens of thousands of grid cells within a region at the same time regional and global market equilibria are also enforced this allows simple g to explicitly incorporate local heterogeneity in climate soils water and regulatory institutions while also capturing global change drivers and feedbacks for local adaptations to national and international markets at each grid cell land and water resources comprise the linkage to the environment and natural ecosystems we model economically motivated changes in land use as well as changes in water withdrawals which reflect differential resource availability and constraints fig 1 summarizes the main demand and supply components of the simple g model the model solves for equilibrium quantities and prices for land and nonland inputs as well as for irrigation water and crop outputs equilibrium water withdrawals are endogenously determined at each grid cell assuming a grid cell specific shadow price for water within the grid cell crop prices are permitted to vary by region based on the extent of domestic market segmentation from the world market 2 1 global socioeconomic determinants of crop demand at the regional scale the consumption of different commodities is a function of population income prices and biofuel demands prices are determined endogenously as a function of supply and demand while population and income changes are exogenous to the model with increases in per capita income driving diet changes population income and biofuels production can be specified to follow long run growth scenarios such as the shared socioeconomic pathways ssps or other global economic projections within this framework global food and agricultural markets link the changes in population income and diets to gridded crop production and associated stresses on land and water resources one of the best understood patterns of economic development is engel s law which states that as per capita income rises the share of income devoted to food will fall clements and chen 1996 simple g captures this relationship by allowing the income elasticity of demand for food ε i y the propensity to spend incremental income on food to evolve with per capita income y based on the estimated parameters α i y and β i y and similarly for the price elasticity of food demand ε i p 1 ε i y α i y β i y ln y 2 ε i p α i p β i p ln y equations 1 and 2 are indexed by type of food demand i simple g distinguishes between direct consumption of crops and indirect consumption through either livestock product consumption or processed food consumption this results in the following equations describing the evolution of per capita demands for each type of food product 1 1 the astute reader will ask why there are no cross price effects in this demand equation the answer is that simple g models only aggregate crop demand if we were considering disaggregated crop products we would need to account for cross price effects while not integrable into an underlying utility or expenditure function this demand system allows for the evolution of price and income elasticities with per capita income in a manner which has been documented by international cross section studies of food demand muhammad et al 2011 this has proven essential to the long run validation of the simple model baldos and hertel 2013 3 q i ε i p p i ε i y y total demand for crops in a given region comes from four sources the direct crop demand is found by multiplying per capita demand by population in each region then we sum the total direct demand for crops in final consumption together with the indirect demands in livestock food processing and biofuel sectors the demand for crops in biofuel production is a derived demand that we assume to be exogenously determined by government mandates the livestock and food processing sectors demands for crops are endogenous and modelled using constant elasticity of substitution ces production functions that combine the raw crop input with other inputs used in livestock or processed food production the mathematical representation of these ces functions is developed in the next section 2 2 gridded crop production is the result of economic optimization crop production is the result of representative producers maximization of profits subject to technology prices policies and resource constraints the crop production technologies both rainfed and irrigated production in each grid cell allow for substitution between nitrogen fertilizer water land and other inputs the latter is an aggregate of capital labor other chemicals energy etc the particular mix of inputs employed in a grid cell depends on relative prices government policies and production possibilities output levels expand or contract in order to ensure zero pure economic profits over the long run thus unlike downscaling approaches the spatial pattern of production is endogenously determined crop producers within a given grid cell are price takers as they are assumed to have no market power the equilibration of supply and demand for crops occurs at the level of market regions within the market regions in simple crop demands are an aggregate of the four end uses described above demands may be satisfied from either domestic or global markets depending on relative prices this follows the method of armington 1969 which results in imperfect substitution between domestic and foreign products symmetrically on the supply side producers transform their products imperfectly between domestic and global markets this permits us to calibrate the model to the observed data in which similar products are both imported and exported from the same country we consider a nested ces structure as shown in fig 2 in each ces nest two inputs are combined to produce a composite product using the following specification of technology 4 q a φ n q n ρ φ o q o ρ 1 ρ where σ 1 1 ρ and ρ 1 where q shows the quantity of output or input a is the technical efficiency φ is the ces parameter ρ is related to substitution elasticity σ is substitution elasticity n is an index for nitrogen fertilizer and o shows other agricultural inputs each ces nest comprises three key behavioral equations which result from our assumptions of cost minimization coupled with free entry and exit from these activities in keeping with the model condensation and nonlinear solution strategy described in section 4 we write these equations in linearized percentage change form dixon 1982 the following three equations pertain to the top level nest in which nitrogen fertilizer n and other inputs o are combined in variable proportions to produced aggregate crop output 5 p a j θ j p j a j agricultural entry exit zero profits 6 q n a n q a σ p n a n p a demand for nitrogen fertilizer 7 q o a o q a σ p o a o p a demand for other inputs here lower case variables denote percentage changes in levels variables i e p 100 d p p is the percentage change in crop price and a 100 d a a is the percentage change in total factor productivity the variables p j q j a j denote the percentage changes in input j s price quantity and factor augmenting productivity and θ j is the share of that input in total costs equation 5 is the consequence of our assumption of unrestricted entry and exit from the crops sector if output price rises with unchanged technology and input prices then there will be excess profits in the sector this will attract new entrants or encourage the expansion of existing producers which will drive up input prices and drive down output prices until zero pure economic profits are restored manipulation of equation 5 7 yields the following equivalent quantity based expression of this condition dual to 5 which we will use in the model to facilitate our condensation strategy described in section 4 8 q a j θ j q j a j equations 6 and 7 are the derived demand conditions for inputs thus the percentage change in demand for nitrogen fertilizer a key source of non point water pollution from agriculture depends on changes in technology α α n changes in total crop output q and changes in the price of nitrogen fertilizer p n relative to an index of all input costs p in section 3 below we will discuss how the elasticity of substitution between nitrogen fertilizer and other inputs σ can be calibrated to reproduce grid cell and practice specific agronomic characteristics of crop production it is evident from equation 6 that a large substitution elasticity will result in a much greater response to e g a tax on fertilizer use in crop production therefore σ is a key parameter in sustainability analysis returning to the production tree in fig 2 we see that the other inputs in equation 7 are a composite of water land and the remaining inputs once again there are three equations analogous to 5 7 describing the substitution possibilities at this level in the production tree see appendix this is followed by a ces nest combining land and irrigation water if crop output is strictly proportional to irrigation water delivered then the elasticity of substitution between land and irrigation water is zero on the other hand if a reduction in water delivered to the crop does not go hand in hand with a proportionate reduction in output then this elasticity is greater than zero and it captures the potential for deficit irrigation i e achieving the same output level with less water but more land the next ces nest in fig 2 combines irrigation water and irrigation capital the associated elasticity of substitution at the bottom of this production tree describes the potential for conserving irrigation water through investments in e g drip irrigation to replace sprinkler or canal based irrigation capital once again this is a key sustainability parameter which will be discussed below in section 3 the final ces nest in fig 2 combines surface and groundwater to create an irrigation water composite the rationale for this nest is that surface and groundwater extraction often co exists in a given grid cell despite differences in cost the two sources of water offer farmers different characteristics groundwater for example is available on demand and largely independent of current weather conditions 2 3 nitrogen fertilizer and nitrate leaching as noted above nitrogen fertilizer use is determined endogenously in the model considering relative prices technology substitution possibilities and overall output level the potential for nitrogen land substitution is grid cell and activity specific and is obtained from agronomic yield functions as described in section 3 the price of nitrogen fertilizer is determined at the regional level through a market clearing condition wherein regional supply equals demand which is in turn determined by aggregating nitrogen use across all grid cells and practices nitrate leaching functions are quadratic in form and are practice and grid specific see section 3 2 4 local water withdrawals and irrigation irrigation water is another focal point of simple g irrigation water supply and demand are endogenously determined for each grid cell however they are linked to exogenous environmental factors for example heat stress may increase the water requirement of crops grown in a grid cell or a drought may reduce the environmental supply of water hydrological dynamics are not directly modelled and are treated exogenously however simple g can be readily paired with a hydrological model to shed light e g on the economic consequences of changing basin level water scarcity or inter basin transfers of water liu et al 2017 water withdrawals are endogenously determined through the interaction of supply constraints and irrigation demand for crop production demand for water depends on the irrigation area production levels technology and relative prices this includes likely adaptation channels and adjustment mechanisms we consider change in irrigation extension haqiqi and hertel 2019 location of crop production change in irrigation technology change in water intensity and trade haqiqi and hertel 2016 water supply at each grid cell is limited by hydrological constraints fig 3 illustrates two examples this form of water supply function is slowly increasing at the beginning up to a and then rapidly increasing after b when approaching the asymptote c withdrawal of water is constrained by maximum water available in each grid cell after subtracting non agricultural water use the supply elasticity of water ε s varies by grid cell it depends on the ratio of water extracted relative to the sustainable extraction level r and calibrated parameters ω1 ω2 ω3 we assume a three parameter fréchet function for water supply 9 ε s ω 1 r ω 2 ω 3 where r is calculated as the ratio of annual withdrawal to annual groundwater recharge or as the ratio of annual withdrawal to annual available surface water we calibrate this supply function separately for surface water and groundwater at each grid cell based on economic and hydrologic information including the annual water withdrawal for crop irrigation sustainable extraction level of water by source and the estimated value of water 2 5 gridded land use total cropland divided into rainfed and irrigated practices and the associated land rents are endogenously determined in the model land rents are grid cell specific and depend on local biophysical characteristics prices as well as technologies available to each production unit allocation of land to rainfed and irrigated production is determined according to their relative returns land rental this is determined endogenously for each grid cell assuming a constant elasticity of transformation function ahmed et al 2008 the key parameter in this function is the elasticity of transformation between irrigated and rainfed cropland this elasticity measures the responsiveness of the rainfed irrigated crop mix ratio to changes in relative returns a larger elasticity value indicates an easier transformation of cropland between irrigated and rainfed categories in the case of land conversion from rainfed to irrigated cropping this is heavily influenced by water law which varies by the locality in the us the land coverage and land use details vary among the models depending on the application the simple g us model is concentrated on irrigated versus rainfed cropland production it does not consider pasturelands but does include the cropland that produces fodder crops this will be the feed input to the livestock production in the model in simple g h us wetlands are also included in the model and there is a cet structure to model the wetland cropland conversions in simple g brazil rather than distinguishing rainfed and irrigated crops we focus on the distinction between pastures and cropland at the grid cell level 2 6 climate climate is exogenous in simple g however the consequences of climate change for land and water use as well as food security may be explored by linking exogenous climate change to key variables in the model this includes total factor productivity labor or land productivity and land and water availability for example excess heat stress may affect yields of irrigated and rainfed crops climate change may affect water availability global warming may reduce labor capacity change the water requirements of crops and alter the suitability of cropland the climate information and biophysical characteristics are embedded implicitly in the benchmark database the benchmark model includes information on land water nitrogen fertilizer crop yields and crop production we have included exogenous variables to link the change in climate conditions to the change in water land yields productivity etc for example simulation of a hydrological model can capture the impact of climate on surface water availability for irrigation this information can be transferred to the model as a shock to the supply of surface water for irrigation in other words the outcomes of other studies can be translated into appropriate variables in the simple g model for example liu et al 2017 employ outputs of the water balance model wbm to shock water availability in simple g many different biophysical models can be used to inform simple g simulations as another example haqiqi et al 2018 employ the yield response function from a statistical estimation combined with nasa nex gddp to inform the future shocks on corn soy yields this has been used in another study to investigate the impacts on market volatilities mcclain and hertel 2019 3 benchmark data and parameters simple g requires benchmark gridded data for key economic and biophysical variables describing the crop economy in initial equilibrium this includes gridded cropland use crop production nitrate leaching and water use in the supplementary materials see figure s1 we describe the workflow for constructing the benchmark data for a us focused version of simple g wherein we utilize gridded data for the us while employing regional information for other parts of the world there are also efforts underway to implement simple g for china and brazil and the initial application of simple g was undertaken at the global scale albeit at coarser resolution liu et al 2017 for simple g us crop production is at the level of geo referenced grid cell units at 5 arcmin resolution squares of side 9 26 km at the equator we add gridded information for us crop production covering both irrigated and non irrigated practices and including the value and quantity of crop output land use nitrogen fertilizer input water and aggregated other inputs table 1 summarizes the main parameters and their sources here we illustrate two examples to show the spatial heterogeneity of the simple g fig 4 shows the ratio of groundwater extraction to recharge that is used in estimation of groundwater supply elasticity the red areas in this figure have a high ratio of withdrawal to recharge in these grid cells the expansion of irrigation is more costly compared to grid cells with a lower ratio in other words given a similar increase in crop prices expansion is expected to be more rapid in areas with a lower ratio holding all other factors constant the simple g h us also includes wetland that requires a transformation elasticity between cropland and wetland loduca et al 2020 fig 5 illustrates the value of this parameter over the us estimated based on potential restorable wetland area the supplementary materials include more details about the parameters 4 software the simple g model and database are prepared and solved with the gempack modelling suite horridge et al 2018 this software package is specifically designed for the solution of large scale economic equilibrium models with numerous markets and agents the database files can readily store multi scale and multi dimensional variables other attractive features of this software are discussed below however the unique advantage of gempack in the context of multi scale modelling is the capability to condense the model and later backsolve for key endogenous variables 4 1 condensation solution times can be substantial for an equilibrium model with many equations and with complex interconnections between the unknown variables e g the market responds to farmer decisions even as the farmers respond to market outcomes 2 2 researchers have designed different algorithms to reduce the solution time most algorithms iterate between two phases a linear algebra phase which solves a first order approximation to the non linear equation system and a formula phase which updates variable values and re computes coefficients of the linear system in gempack solution time for the linear phase rises with the square or cube of the number of equations while time for the formula phase tends to increase only linearly a typical simple g application might distinguish 2 million grid cells and 7 regions for each grid cell a system of about 20 equations some shown above determines crop output of that grid cell given grid level exogenous settings and the price of output which is the same for all cells within a given region so these grid level equations may number about 40 million for each region other equations add up grid cell output to obtain total crop supply or inter relate region level prices and quantities there might be 100 such equations per region or 700 in total hence the overwhelming majority of equations are at grid cell level a linear system of 20 million equations is impossibly slow to solve and might require enormous amounts of ram we need to greatly reduce the number of equations by substitution a k a condensation for example we could rewrite equation 6 above as the grid index is omitted 6 q n q a a n σ p n a n p a demand for nitrogen fertilizer then we could replace each occurrence of q n in other equations by q a a n σ p n a n p a and drop equation 6 from the system so reducing its size by 2 million equations after the linear system was solved we could use equation 6 to recover or backsolve for values of q n such techniques are often used by modelers who manually perform such substitutions in their model specification file the drawback is especially when a number of substitutions are performed that the necessary algebra is difficult and the remaining equations become extremely complicated and un transparent however gempack is able to do the algebra to perform such substitutions and the backsolves automatically reaping a performance gain while leaving the model specification tablo file in its original simpler uncondensed form in fact for simple g all equations at grid level are substituted out leaving a regional level linear system of modest 700 size such a system takes very little time to solve however the coefficients of the system involve calculations at grid level the time taken is proportional to the number of grid cells hence solution time increases only linearly as a function of the number of grid cells see fig 6 4 2 linearization gempack can automatically translate the original equation system into a linearized system reformulated as a system of first order partial differential equations alternatively the modeler can specify conveniently interpretable linearized forms of the underlying behavioral equations as in equation 5 7 clever representation of the model e g using equation 8 in place of 5 can facilitate condensation as well as more rapid solution of the model in our case we substitute out all of the variables with a grid cell index in simple g all of the cross grid cell interactions are transmitted through regional market prices once we know the regional crop nitrogen irrigation capital and other input prices we can backsolve for crop output input use land prices and the shadow price of irrigation water in each grid cell independently however since the model is non linear recall equation 1 the cost shares in equation 5 must be updated at each step in the solution process consequently the model is solved by multistep methods such as the euler method or gragg s modified midpoint method pearson 1991 the solution of a large system of linear equations is accomplished using sparse matrix techniques schiffmann and jerie 2019 richardson extrapolation is used to improve accuracy pearson 1991 this linearized approach has proven capable of solving very large non linear models e g one data point in fig 6 is a model with 8 million grid cells 4 3 decomposition in addition to these features gempack has some extensions which prove invaluable in simple g applications it provides a way to formulate inequality constraints or non differentiable equations as complementarities bach and pearson 1996 which can be important in sustainability analyses it also offers a technique to decompose changes in model variables due to several shocks into components due to each individual shock harrison et al 2000 we will illustrate this in the first application undertaken in the next section of the paper 4 4 web application on geohub the web app version of simple g permits users to simulate explore and visualize the results of simple g without installing the gempack program or any visualization software linux versions of gempack programs run on the geohub server the web app also includes pre solved experiments and demonstrations on how to run the model and analyze results based on the policy briefs presented at the 2018 conference on long run sustainability of us agriculture https mygeohub org groups glass npc2018 the latest version of the web app allows users to run their own experiments which could range from global projections on food production and food demand up to grid level analysis see fig 7 this is done by uploading key growth rates via text based command files improvements of the tool are ongoing to make it more user friendly and easy to use 5 two applications here we illustrate the usefulness of the simple g model through two applications since we use the us focused version of simple g these two applications focus on the us however similar applications of simple g in other regions are underway the first application evaluates the role of global drivers of local sustainability stresses within the continental us in the second we consider the feedback to national and global markets stemming from locally implemented sustainability policies on irrigation water use together these applications demonstrate the capacity of simple g to capture global to local to global interactions 5 1 global drivers of local sustainability stresses in the coming decades changes in population income and technology will alter the pattern of agricultural crop consumption production and international trade we expect that productivity growth will lead to higher yields thereby moderating the demand for scarce land and water resources on the other hand we expect the changes in population and income growth will create heightened sustainability pressures for projecting this footrace between supply and demand forward to mid century we take predicted changes in population income and total factor productivity as in baldos and hertel 2014 these are reported in fig 8 and are based on the business as usual shared socioeconomic pathway ssp2 o neill et al 2014 we also assume that historical agricultural productivity growth rates persist to mid century fuglie 2012 south asia and china are projected to have the greatest cumulative per capita income growth over this period rising by 641 and 607 respectively sub saharan africa is expected to experience the highest rate of population growth 139 in contrast east europe and japan and korea are expected to see declines in their populations by mid century the role of global change drivers in projected growth for us crop production by 2050 is shown in fig 9 exploiting the decomposition feature of gempack harrison et al 2000 this figure shows that one quarter of the projected us production expansion is due to demand growth in south asia and china alone overall growth in income and population outside the us is far more important in driving us crop production than growth within the us this is due to higher income growth rates in the developing and emerging economies coupled with higher income elasticities of demand 1 and higher rates of population growth in africa and other low income regions fig 10 shows the pattern of cropland expansion across the us over the projections period as a percentage change from 2010 this particular indicator of sustainability stress reveals that absent any policy interventions the greatest land use change stresses will arise in the marginal areas on the edges of the corn belt there is very little remaining land available for expansion in the heart of the corn belt these marginal regions are often environmentally sensitive and they are also the areas where the largest land use stresses arose during the 2008 2012 biofuels boom period lark et al 2015 these changes are based on the statistically estimated gridded land supply elasticities villoria and liu 2018 5 2 limiting unsustainable water withdrawals as seen in fig 5 above many locations in the western us suffer from excess groundwater withdrawals despite productivity improvements our projections suggest that this situation will become even worse under our business as usual baseline due to global growth in the demand for us crops fig 11 a here we examine the impacts of a counterfactual scenario in which we do not allow any increase in water withdrawals in locations showing withdrawals in excess of recharge in the base year of 2010 fig 11 b fig 12 shows the impact of this water sustainability policy on irrigated cropland area in the us as well as global changes in cropland and production owing to this policy while the aggregate impact of the water withdrawal restriction on us crop production and land use is less than 1 it nonetheless has a significant impact on the pattern of crop production and the irrigated area the reduction in us production is partially offset by increased production in other regions of the world with eu south america china and sub saharan africa offsetting 19 19 12 and 11 of the reduction respectively compared to the baseline us aggregate water withdrawals decrease by 1 82 irrigated area declines by 0 13 and rainfed area increases by 0 08 while this figure seems to be very small for the whole country there are significant impacts on many local communities compared to the baseline the irrigated area declines by as much as 17 7 in some grid cells and may increase by up to 5 7 in other grid cells the rainfed area may also decline by up to 5 7 and may increase by 163 1 in other grid cells as shown in fig 13 irrigation is reduced in locations facing the sustainability restriction in a few grid cells rainfed land is converted to irrigated land in response to water limits which involves improvement in irrigation efficiency not allowing the unsustainable grid cells to increase groundwater withdrawal reduces the irrigated area by up to 370 ha in some grid cells each grid cell can have 3500 7000 ha of cropland fig 13 while the rainfed cropland is projected to increase in most of the us in response to the water withdrawal restrictions the highest absolute increase in rainfed land arises in the locations with water withdrawal limits as land reverts from irrigated to rainfed production increases in rainfed land is also projected to be higher in the marginal area as a response to the higher crop prices as shown in fig 14 5 3 other applications of simple g while these two applications use the high resolution us version of simple g there are other applications of this framework designed to address different research questions as described in table 2 these models may have different production structures fig 2 different spatial focus different resolutions different crop coverage and some employ different modules in one such application of simple g changes in ecosystem services are linked to land use as well as the productivity of land loduca et al 2020 specify grid cell specific wetland expansion and habitat conservation measures in simple g for the chesapeake bay watershed in the united states hertel ramankutty and baldos 2014 use the gridded terrestrial carbon data base from west et al 2010 to deduce changes in terrestrial carbon emissions stemming from an african green revolution based on cropland changes in the simple model given its importance to the sustainability debate simple g has a nutrition module that allows users to assess the impact of changes in price and income on the prevalence and depth of undernutrition in developing countries baldos and hertel 2014 it follows the fao neiken 2003 approach modelling the distribution of caloric intake in a region using a log normal distribution when coupled with information about the mean and standard deviation of consumption as well as the minimum caloric intake it is possible to deduce the prevalence of undernutrition as well as the average caloric gap of those who are undernourished with this module simple g can be used to assess food utilization outcomes in addition to food production and food prices key metrics for food availability and food access this module allows for the assessment of a variety of important questions such as the impact of climate change on food security baldos et al 2020 baldos and hertel 2014 this nutrition extension of the model links the local resources mainly land and water to global food security the goal is to create opportunities for analyzing the trade off between global food security goals and local sustainability of land and water resources kabir et al 2019 a recent extension of the simple model also estimates food waste by incorporating an econometrically estimated relationship between per capita income and the share of food availability that is wasted lopez barrera and hertel 2020 the authors use this extended model to examine the impacts of freezing the share of food waste in different regions and at different future dates one early application of this model has treated the world as a uniformly distributed set of global grid cells liu et al 2017 that research focused on the impact of emerging water scarcity at global scale and was undertaken in conjunction with the global wbm vörösmarty et al 1998 the global gridded implementation was performed at a coarser resolution 30 arc minute grids and the economic demands for irrigation and the hydrological supplies net of non agricultural uses were reconciled at the level of nearly 1000 hydrological sub basins this enabled the authors to explore the implications of various adaptations to water scarcity including inter basin water transfers as well as increased integration of commodity markets another version of the model has been developed for questions regarding sustainability of water resources haqiqi et al 2018 in the agricultural production it considers not only land use but also water use and nitrate fertilizer application similar to the model described in this paper the production is modelled at 5 arc min grid cells over the us for the rest of the world the production is modelled at the level of 16 market regions in this version there are two levels of aggregate demand one aggregation is at sub national production regions for the us then the global demand is modelled at aggregated regional level the water withdrawal module is similar to this paper with different functional forms for water supply simple g is also flexible in terms of crop coverage while most versions have considered all crops aggregate another set of models have focused on corn soy composite one application is the analysis of water quality in the corn belt of the us liu et al 2018 as a large portion of the water pollution has been related to corn and soy cultivation it makes sense to focus on the specific responses of these crops for this application a nitrate leaching module has been developed with crop specific yield and nitrate leaching response this module is parametrized with the outputs of agro ibis agronomic model as described earlier the most recent version of simple g was developed for high resolution conservation studies that version of the framework includes a module on potentially restorable wetlands on agricultural lands from enviroatlas the model is solved at a much finer resolution 750 m over the continental us parametrization of grid cells exploits satellite data as well as reported county level information this includes a detailed wetland restoration and conservation with conservation reserve program crp and conservation reserve enhancement program crep considering wildlife habitat water quality and greenhouse gas emissions loduca et al 2020 these applications illustrate the flexibility of the simple g framework it is not just a single model rather it is a flexible way of looking at the world indeed there are two nsf funded efforts underway that are building high resolution versions of simple g focusing on china and brazil 6 discussion and conclusions simple g is by no means the first attempt to undertake global economic analysis of sustainability challenges at the interface of agriculture and the environment using a grid resolving approach to our knowledge the first such model was magpie lotze campen et al 2008 this is a global optimization model with the objective of minimizing the global cost of producing food to meet a pre specified level of demand it was developed at the potsdam institute for climate pik and is typically used in conjunction with a gridded dynamic vegetation model to look at issues related to land use change climate impacts on agriculture bioenergy and technology change among other issues magpie differs fundamentally from the approach developed in this paper simple g is an economic equilibrium model in which decentralized agents e g irrigated crop producers in a given grid cell food processors or consumers in a particular regions of the world interact through regional and global markets in the presence of policy distortions and barriers to trade the global equilibrium determined by simple g will not minimize total costs in this sense it aims to be predictive as opposed to normative indeed the presence of market imperfections means that global optimization models such as magpie must often place artificial constraints on the model in order to allow it to replicate observed patterns of production consumption and international trade more recently the globiom model has emerged on the global sustainability scene it is maintained at the international institute for applied systems analysis valin et al 2013 like magpie globiom is a recursive dynamic optimization model in addition to 18 major crops it has a livestock module and when linked to models of crop growth bioenergy forestry and fisheries it has been used to deal with a wide range of sustainability issues including deforestation water use and greenhouse gas emissions havlik et al 2013 leclère et al 2014 soterroni et al 2018 there are regional versions of the model focusing on the eu and brazil among others soterroni et al 2019 due to its large size and complexity the model is not solved at the individual grid cell level but rather it is solved for representative groupings of grid cells in short it is a very ambitious undertaking involving dozens of researchers and this work represents the cutting edge of global sustainability research with grid cell resolution as its name indicates the simple g framework introduced in this paper has more modest aspirations rather than continually extending this model to handle new issues this framework aims to be as simple as possible while capturing the essence of a given sustainability challenge if a different set of challenges emerges the idea is to build a different version of simple g rather than extending the original model to add another feature at the heart of this simplicity lies the fact that simple g always has just one composite crop albeit produced with different techniques and resource requirements both within and across grid cells in the application presented here the single crop was a composite of all crops and our analysis focused on the extensive margin of land and water use in agriculture however as noted in the preceding section this crop could also be a single crop such as maize or a maize soy composite such as in liu et al 2018 from an economic point of view this means that within the crop composite it is assumed that prices move in tandem a key economic condition for aggregation of products for the all crop composite this doesn t make sense in the short run but over decades it is likely the case that substitution both on the supply and the demand sides will force crop prices to move together and this representation may be preferred to one in which modest substitution across crops permits significant divergence in crop prices over the long run in part due to this long run orientation simple g is not run on an annual recursive basis unlike the aforementioned models rather it is treated as a one shot comparative static model e g starting in 2010 one might simulate the global crop economy in 2050 as is done in the applications above the outputs from simple g can also be linked to other spatially explicit models one such example is provided by sun et al 2020 who use the us corn soy version of simple g to explore the land use and water quality impacts of widespread biomass co firing in the midwestern united states this paper shows that while the region wide deterioration in water quality is likely to be modest a gridded analysis reveals the presence of dangerous hotspots these arise where coal plants coincide with areas of concentrated corn production and already high levels of nitrogen fertilizer application and leaching similarly linking simple g to spatial models of water use such as li et al 2019 could offer important insights while the restriction to a single composite crop and the one shot comparative static approach may seem like a great sacrifice it has yielded one very significant benefit namely facilitation of model parameterization and validation to date all of the validation efforts have been conducted using the non gridded version of simple which breaks the world into 15 aggregate regions and these have been quite informative in the first such paper baldos and hertel 2013 found that simple was able to re produce global changes in aggregate crop output cropland area yield and prices over the 1961 2006 period this was a significant breakthrough in the global land use change literature and was used as a basis for understanding why many models at the time appeared to be over predicting long run land use change in the 21st century baldos and hertel 2013 historical simulation of the simple model has also revealed areas where the model falls short in particular while the first version of simple closely followed global crop production it failed to reproduce the regional pattern of production changes over this period this led the authors to introduce market segmentation whereby individual consumers and producers in each region have differential access to world markets at the aggregate level this results in a constant elasticity of substitution between domestic and international goods on the demand side and a constant elasticity of transformation between domestic and international goods on the supply side the resulting segmented markets version of simple now the default approach performed much better at the regional level and also resulted in very different consequences for a number of key sustainability policies hertel and baldos 2016 future work with simple g will focus on its ability to reproduce historical patterns of land use change and irrigation intensities at the level of subnational regions and individual grid cells this will provide the necessary foundation for policy relevant multi scale modelling of future sustainability challenges software and data availability simple g web application and simple g us web application models and data are open source and available for download for offline use or in cloud computing at https mygeohub org resources simpleg and https mygeohub org tools simpleus the regional economic data is taken from the faostat fao 2014 and gtap aguiar et al 2019 as well as regional simple hertel and baldos 2016 consumer demand elasticities are based on usda reports muhammad et al 2011 cropland area is obtained from the usda cropland data layer han et al 2012 at 30 meter resolution and aggregated to 5 arc min irrigated cropland is from the moderate resolution imaging spectroradiometer modis irrigated agriculture dataset for the united states mirad us provided by usgs at 250 meter resolution brown and pervez 2014 and aggregated to 5 arc min gridded land supply elasticities for the us are based on econometric estimations villoria and liu 2018 transformation elasticity parameter is estimated considering water right jame et al 2017 we take the value of crop sold per acre from usda nass by county usda nass 2019 and use gcwm siebert and döll 2010 simulated yields to generate gridded yield nitrogen fertilizer application rates leaching parameters and substitution elasticity between nitrogen fertilizer and other inputs are obtained from agro ibis liu et al 2018 lark et al 2020 irrigation water withdrawal rates are estimated using usgs county level water use data maupin et al 2014 information about groundwater recharge is taken from the annual estimate of recharge reitz et al 2017 maximum surface water available at each grid cell is calculated after subtracting non agricultural water use from locally generated runoff wolock 2003 maximum available ground water available is determined with groundwater stock befus et al 2017 gleeson et al 2016 the groundwater supply elasticity of is determined based on the ratio of groundwater withdrawal to groundwater recharge gleeson et al 2016 reitz et al 2017 and estimated value of water haqiqi et al 2016 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge support from the united states department of agriculture afri grant 2019 67023 29679 economic foundations of long run agricultural sustainability the national science foundation infews grant 1855937 identifying sustainability solutions through global local global analysis of a coupled water agriculture bioenergy system and the united states department of energy office of science biological and environmental research program earth and environmental systems modelling multisector dynamics contract de sc0016162 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104805 
25961,we introduce simple g a simplified international model of agricultural prices land use and the environment gridded version which is a novel tool for evaluating sustainability policies in a global context while factoring in local heterogeneity in land and water resources and natural ecosystem services this multi scale model can provide boundary conditions for local decision makers as well as capturing feedback from local policies to national and global scales to illustrate its value in environmental analysis we provide two applications of the model first we quantify the local stresses on land and water resources due to global changes in population income and productivity second we quantify the global impacts of local policy responses and adaptations to water scarcity graphical abstract image 1 keywords sustainability agriculture environmental stresses multiscale modelling global change water scarcity land use 1 introduction the world faces significant sustainability challenges in the decades ahead united nations 2019 growing populations and rising incomes are placing unprecedented stresses on the planetary boundaries with the world s land and water resources at growing risk steffen et al 2015 the challenge posed in making such assessments is that sustainability stresses do not respect disciplinary boundaries furthermore while the stresses are often highly localized the drivers of these stresses are global and the local responses can feedback to national and global outcomes for this reason assessment of the underlying risks as well as potential solutions is typically undertaken with a suite of models using complex approaches that often preclude replication and use by researchers outside the core group obersteiner et al 2016 springmann et al 2018 up to this point there have been just a few open source bottom up economic environmental modelling framework capable of analyzing global sustainability at the resolution of individual grid cells lotze campen et al 2008 valin et al 2013 there is clearly a tradeoff between complexity and accessibility models used in teaching and academic research are generally simpler than those developed by national and international labs and research institutions having a relatively simple global grid resolving sustainability framework that can be also run in cloud will allow wider participation in sustainability discussions and can facilitate greater crowd sourcing of new ideas data and parameters to enrich the representation of local stresses policies and adaptations this paper introduces such a modelling framework simple g a simplified international model of agricultural prices land use and the environment gridded version the simple g framework allows for analysis of the interplay between economic and environmental systems taking account of the actions of local agricultural producers pertaining to land and water use within the context of regional and global commodity markets this model integrates economic theories with environmental sciences to analyze the biophysical and economic impacts at different geospatial scales the economic supply of land and water takes account of local institutions biophysical characteristics sustainability criteria along with maximum available resources as a consequence heterogeneous local constraints lead to different rates of change in land and water use on the demand side growth in income and population lead to changes in food consumption baskets and changes in agricultural trade patterns integrating the human and earth system analysis within a global economic framework is a challenging task and often focuses on one way linkages such as those used in down scaling regional results to a grid cell level reilly et al 2012 it has also been common practice to extrapolate from sophisticated grid level analysis to national scale by assuming that the share of production or land use is unchanging schlenker and roberts 2009 bridging local national and global scales within a single framework is challenging yet it is essential if we wish to bring into consideration the behavior of local decision makers within the context of global sustainability analysis in the simple g framework laid out here these decisions are made endogenously considering local biophysical characteristics and institutions as well as nationally determined input prices and globally determined commodity prices despite computational advances model solution time remains another major challenge for integrated frameworks particularly those utilized by individual researchers without access to high performance computing at national labs and major research institutions with simple g we introduce a solution strategy that dramatically reduces computing time permitting individuals to solve a version of simple g with a million grid cells in a matter of minutes on a desktop computer furthermore by implementing simple g on one of the nsf funded hubzero sites geohub we have made the model along with visualization software readily available to any user with access to a web browser this greatly expands access to multi scale modelling of sustainability challenges at the interface of agriculture and the environment it should also accelerate the development of new and improved data bases and representations of local institutions and other constraints within this framework to illustrate the versatility of simple g in sustainability analysis we highlight an implementation of this framework wherein the us has been broken out in detail 5 arc minutes while other regions are aggregated previous applications have disaggregated the globe uniformly 30 arc minutes liu et al 2017 we undertake two experiments aimed at highlighting two different types of analyses that can be undertaken with simple g in the first we investigate the contribution of global changes in population technology and income to changes in gridded us water and land use by mid century it includes global demand shocks as well as local supply responses this highlights locations most vulnerable to land and water stresses furthermore we tie these stresses to individual global change drivers including for example population growth in africa or income growth in asia by linking local environmental stresses in the us to global change drivers we underscore the essence of 21st century global sustainability challenges in a second application of the simple g framework we focus on the local to global feedbacks associated with sustainability policies in this case we begin with the projections made in the first experiment but now we overlay a location specific sustainability policy in particular we do not allow irrigation withdrawals to increase from the present day in those grid cells where withdrawals currently exceed recharge rates we then explore how these sustainability restrictions alter global prices production consumption and trade the remainder of the paper is organized as follows section two provides an overview of the model section three introduces the diverse information used in the construction of the database and parameters for simple g us section four describes the software and implementation of this model section five explores the two experiments mentioned above and the final section provides further discussion and conclusions 2 model the simple g model is based on simple a simplified international model of agricultural prices land use and the environment baldos and hertel 2013 this is a partial equilibrium agricultural trade model that has been validated for the study of long run sustainability and food security baldos and hertel 2014 we extend the simple model to include gridded biophysical and economic relationships hence the name simple g this model is multi scale in other words it simultaneously solves for outcomes at the level of tens of thousands of grid cells within a region at the same time regional and global market equilibria are also enforced this allows simple g to explicitly incorporate local heterogeneity in climate soils water and regulatory institutions while also capturing global change drivers and feedbacks for local adaptations to national and international markets at each grid cell land and water resources comprise the linkage to the environment and natural ecosystems we model economically motivated changes in land use as well as changes in water withdrawals which reflect differential resource availability and constraints fig 1 summarizes the main demand and supply components of the simple g model the model solves for equilibrium quantities and prices for land and nonland inputs as well as for irrigation water and crop outputs equilibrium water withdrawals are endogenously determined at each grid cell assuming a grid cell specific shadow price for water within the grid cell crop prices are permitted to vary by region based on the extent of domestic market segmentation from the world market 2 1 global socioeconomic determinants of crop demand at the regional scale the consumption of different commodities is a function of population income prices and biofuel demands prices are determined endogenously as a function of supply and demand while population and income changes are exogenous to the model with increases in per capita income driving diet changes population income and biofuels production can be specified to follow long run growth scenarios such as the shared socioeconomic pathways ssps or other global economic projections within this framework global food and agricultural markets link the changes in population income and diets to gridded crop production and associated stresses on land and water resources one of the best understood patterns of economic development is engel s law which states that as per capita income rises the share of income devoted to food will fall clements and chen 1996 simple g captures this relationship by allowing the income elasticity of demand for food ε i y the propensity to spend incremental income on food to evolve with per capita income y based on the estimated parameters α i y and β i y and similarly for the price elasticity of food demand ε i p 1 ε i y α i y β i y ln y 2 ε i p α i p β i p ln y equations 1 and 2 are indexed by type of food demand i simple g distinguishes between direct consumption of crops and indirect consumption through either livestock product consumption or processed food consumption this results in the following equations describing the evolution of per capita demands for each type of food product 1 1 the astute reader will ask why there are no cross price effects in this demand equation the answer is that simple g models only aggregate crop demand if we were considering disaggregated crop products we would need to account for cross price effects while not integrable into an underlying utility or expenditure function this demand system allows for the evolution of price and income elasticities with per capita income in a manner which has been documented by international cross section studies of food demand muhammad et al 2011 this has proven essential to the long run validation of the simple model baldos and hertel 2013 3 q i ε i p p i ε i y y total demand for crops in a given region comes from four sources the direct crop demand is found by multiplying per capita demand by population in each region then we sum the total direct demand for crops in final consumption together with the indirect demands in livestock food processing and biofuel sectors the demand for crops in biofuel production is a derived demand that we assume to be exogenously determined by government mandates the livestock and food processing sectors demands for crops are endogenous and modelled using constant elasticity of substitution ces production functions that combine the raw crop input with other inputs used in livestock or processed food production the mathematical representation of these ces functions is developed in the next section 2 2 gridded crop production is the result of economic optimization crop production is the result of representative producers maximization of profits subject to technology prices policies and resource constraints the crop production technologies both rainfed and irrigated production in each grid cell allow for substitution between nitrogen fertilizer water land and other inputs the latter is an aggregate of capital labor other chemicals energy etc the particular mix of inputs employed in a grid cell depends on relative prices government policies and production possibilities output levels expand or contract in order to ensure zero pure economic profits over the long run thus unlike downscaling approaches the spatial pattern of production is endogenously determined crop producers within a given grid cell are price takers as they are assumed to have no market power the equilibration of supply and demand for crops occurs at the level of market regions within the market regions in simple crop demands are an aggregate of the four end uses described above demands may be satisfied from either domestic or global markets depending on relative prices this follows the method of armington 1969 which results in imperfect substitution between domestic and foreign products symmetrically on the supply side producers transform their products imperfectly between domestic and global markets this permits us to calibrate the model to the observed data in which similar products are both imported and exported from the same country we consider a nested ces structure as shown in fig 2 in each ces nest two inputs are combined to produce a composite product using the following specification of technology 4 q a φ n q n ρ φ o q o ρ 1 ρ where σ 1 1 ρ and ρ 1 where q shows the quantity of output or input a is the technical efficiency φ is the ces parameter ρ is related to substitution elasticity σ is substitution elasticity n is an index for nitrogen fertilizer and o shows other agricultural inputs each ces nest comprises three key behavioral equations which result from our assumptions of cost minimization coupled with free entry and exit from these activities in keeping with the model condensation and nonlinear solution strategy described in section 4 we write these equations in linearized percentage change form dixon 1982 the following three equations pertain to the top level nest in which nitrogen fertilizer n and other inputs o are combined in variable proportions to produced aggregate crop output 5 p a j θ j p j a j agricultural entry exit zero profits 6 q n a n q a σ p n a n p a demand for nitrogen fertilizer 7 q o a o q a σ p o a o p a demand for other inputs here lower case variables denote percentage changes in levels variables i e p 100 d p p is the percentage change in crop price and a 100 d a a is the percentage change in total factor productivity the variables p j q j a j denote the percentage changes in input j s price quantity and factor augmenting productivity and θ j is the share of that input in total costs equation 5 is the consequence of our assumption of unrestricted entry and exit from the crops sector if output price rises with unchanged technology and input prices then there will be excess profits in the sector this will attract new entrants or encourage the expansion of existing producers which will drive up input prices and drive down output prices until zero pure economic profits are restored manipulation of equation 5 7 yields the following equivalent quantity based expression of this condition dual to 5 which we will use in the model to facilitate our condensation strategy described in section 4 8 q a j θ j q j a j equations 6 and 7 are the derived demand conditions for inputs thus the percentage change in demand for nitrogen fertilizer a key source of non point water pollution from agriculture depends on changes in technology α α n changes in total crop output q and changes in the price of nitrogen fertilizer p n relative to an index of all input costs p in section 3 below we will discuss how the elasticity of substitution between nitrogen fertilizer and other inputs σ can be calibrated to reproduce grid cell and practice specific agronomic characteristics of crop production it is evident from equation 6 that a large substitution elasticity will result in a much greater response to e g a tax on fertilizer use in crop production therefore σ is a key parameter in sustainability analysis returning to the production tree in fig 2 we see that the other inputs in equation 7 are a composite of water land and the remaining inputs once again there are three equations analogous to 5 7 describing the substitution possibilities at this level in the production tree see appendix this is followed by a ces nest combining land and irrigation water if crop output is strictly proportional to irrigation water delivered then the elasticity of substitution between land and irrigation water is zero on the other hand if a reduction in water delivered to the crop does not go hand in hand with a proportionate reduction in output then this elasticity is greater than zero and it captures the potential for deficit irrigation i e achieving the same output level with less water but more land the next ces nest in fig 2 combines irrigation water and irrigation capital the associated elasticity of substitution at the bottom of this production tree describes the potential for conserving irrigation water through investments in e g drip irrigation to replace sprinkler or canal based irrigation capital once again this is a key sustainability parameter which will be discussed below in section 3 the final ces nest in fig 2 combines surface and groundwater to create an irrigation water composite the rationale for this nest is that surface and groundwater extraction often co exists in a given grid cell despite differences in cost the two sources of water offer farmers different characteristics groundwater for example is available on demand and largely independent of current weather conditions 2 3 nitrogen fertilizer and nitrate leaching as noted above nitrogen fertilizer use is determined endogenously in the model considering relative prices technology substitution possibilities and overall output level the potential for nitrogen land substitution is grid cell and activity specific and is obtained from agronomic yield functions as described in section 3 the price of nitrogen fertilizer is determined at the regional level through a market clearing condition wherein regional supply equals demand which is in turn determined by aggregating nitrogen use across all grid cells and practices nitrate leaching functions are quadratic in form and are practice and grid specific see section 3 2 4 local water withdrawals and irrigation irrigation water is another focal point of simple g irrigation water supply and demand are endogenously determined for each grid cell however they are linked to exogenous environmental factors for example heat stress may increase the water requirement of crops grown in a grid cell or a drought may reduce the environmental supply of water hydrological dynamics are not directly modelled and are treated exogenously however simple g can be readily paired with a hydrological model to shed light e g on the economic consequences of changing basin level water scarcity or inter basin transfers of water liu et al 2017 water withdrawals are endogenously determined through the interaction of supply constraints and irrigation demand for crop production demand for water depends on the irrigation area production levels technology and relative prices this includes likely adaptation channels and adjustment mechanisms we consider change in irrigation extension haqiqi and hertel 2019 location of crop production change in irrigation technology change in water intensity and trade haqiqi and hertel 2016 water supply at each grid cell is limited by hydrological constraints fig 3 illustrates two examples this form of water supply function is slowly increasing at the beginning up to a and then rapidly increasing after b when approaching the asymptote c withdrawal of water is constrained by maximum water available in each grid cell after subtracting non agricultural water use the supply elasticity of water ε s varies by grid cell it depends on the ratio of water extracted relative to the sustainable extraction level r and calibrated parameters ω1 ω2 ω3 we assume a three parameter fréchet function for water supply 9 ε s ω 1 r ω 2 ω 3 where r is calculated as the ratio of annual withdrawal to annual groundwater recharge or as the ratio of annual withdrawal to annual available surface water we calibrate this supply function separately for surface water and groundwater at each grid cell based on economic and hydrologic information including the annual water withdrawal for crop irrigation sustainable extraction level of water by source and the estimated value of water 2 5 gridded land use total cropland divided into rainfed and irrigated practices and the associated land rents are endogenously determined in the model land rents are grid cell specific and depend on local biophysical characteristics prices as well as technologies available to each production unit allocation of land to rainfed and irrigated production is determined according to their relative returns land rental this is determined endogenously for each grid cell assuming a constant elasticity of transformation function ahmed et al 2008 the key parameter in this function is the elasticity of transformation between irrigated and rainfed cropland this elasticity measures the responsiveness of the rainfed irrigated crop mix ratio to changes in relative returns a larger elasticity value indicates an easier transformation of cropland between irrigated and rainfed categories in the case of land conversion from rainfed to irrigated cropping this is heavily influenced by water law which varies by the locality in the us the land coverage and land use details vary among the models depending on the application the simple g us model is concentrated on irrigated versus rainfed cropland production it does not consider pasturelands but does include the cropland that produces fodder crops this will be the feed input to the livestock production in the model in simple g h us wetlands are also included in the model and there is a cet structure to model the wetland cropland conversions in simple g brazil rather than distinguishing rainfed and irrigated crops we focus on the distinction between pastures and cropland at the grid cell level 2 6 climate climate is exogenous in simple g however the consequences of climate change for land and water use as well as food security may be explored by linking exogenous climate change to key variables in the model this includes total factor productivity labor or land productivity and land and water availability for example excess heat stress may affect yields of irrigated and rainfed crops climate change may affect water availability global warming may reduce labor capacity change the water requirements of crops and alter the suitability of cropland the climate information and biophysical characteristics are embedded implicitly in the benchmark database the benchmark model includes information on land water nitrogen fertilizer crop yields and crop production we have included exogenous variables to link the change in climate conditions to the change in water land yields productivity etc for example simulation of a hydrological model can capture the impact of climate on surface water availability for irrigation this information can be transferred to the model as a shock to the supply of surface water for irrigation in other words the outcomes of other studies can be translated into appropriate variables in the simple g model for example liu et al 2017 employ outputs of the water balance model wbm to shock water availability in simple g many different biophysical models can be used to inform simple g simulations as another example haqiqi et al 2018 employ the yield response function from a statistical estimation combined with nasa nex gddp to inform the future shocks on corn soy yields this has been used in another study to investigate the impacts on market volatilities mcclain and hertel 2019 3 benchmark data and parameters simple g requires benchmark gridded data for key economic and biophysical variables describing the crop economy in initial equilibrium this includes gridded cropland use crop production nitrate leaching and water use in the supplementary materials see figure s1 we describe the workflow for constructing the benchmark data for a us focused version of simple g wherein we utilize gridded data for the us while employing regional information for other parts of the world there are also efforts underway to implement simple g for china and brazil and the initial application of simple g was undertaken at the global scale albeit at coarser resolution liu et al 2017 for simple g us crop production is at the level of geo referenced grid cell units at 5 arcmin resolution squares of side 9 26 km at the equator we add gridded information for us crop production covering both irrigated and non irrigated practices and including the value and quantity of crop output land use nitrogen fertilizer input water and aggregated other inputs table 1 summarizes the main parameters and their sources here we illustrate two examples to show the spatial heterogeneity of the simple g fig 4 shows the ratio of groundwater extraction to recharge that is used in estimation of groundwater supply elasticity the red areas in this figure have a high ratio of withdrawal to recharge in these grid cells the expansion of irrigation is more costly compared to grid cells with a lower ratio in other words given a similar increase in crop prices expansion is expected to be more rapid in areas with a lower ratio holding all other factors constant the simple g h us also includes wetland that requires a transformation elasticity between cropland and wetland loduca et al 2020 fig 5 illustrates the value of this parameter over the us estimated based on potential restorable wetland area the supplementary materials include more details about the parameters 4 software the simple g model and database are prepared and solved with the gempack modelling suite horridge et al 2018 this software package is specifically designed for the solution of large scale economic equilibrium models with numerous markets and agents the database files can readily store multi scale and multi dimensional variables other attractive features of this software are discussed below however the unique advantage of gempack in the context of multi scale modelling is the capability to condense the model and later backsolve for key endogenous variables 4 1 condensation solution times can be substantial for an equilibrium model with many equations and with complex interconnections between the unknown variables e g the market responds to farmer decisions even as the farmers respond to market outcomes 2 2 researchers have designed different algorithms to reduce the solution time most algorithms iterate between two phases a linear algebra phase which solves a first order approximation to the non linear equation system and a formula phase which updates variable values and re computes coefficients of the linear system in gempack solution time for the linear phase rises with the square or cube of the number of equations while time for the formula phase tends to increase only linearly a typical simple g application might distinguish 2 million grid cells and 7 regions for each grid cell a system of about 20 equations some shown above determines crop output of that grid cell given grid level exogenous settings and the price of output which is the same for all cells within a given region so these grid level equations may number about 40 million for each region other equations add up grid cell output to obtain total crop supply or inter relate region level prices and quantities there might be 100 such equations per region or 700 in total hence the overwhelming majority of equations are at grid cell level a linear system of 20 million equations is impossibly slow to solve and might require enormous amounts of ram we need to greatly reduce the number of equations by substitution a k a condensation for example we could rewrite equation 6 above as the grid index is omitted 6 q n q a a n σ p n a n p a demand for nitrogen fertilizer then we could replace each occurrence of q n in other equations by q a a n σ p n a n p a and drop equation 6 from the system so reducing its size by 2 million equations after the linear system was solved we could use equation 6 to recover or backsolve for values of q n such techniques are often used by modelers who manually perform such substitutions in their model specification file the drawback is especially when a number of substitutions are performed that the necessary algebra is difficult and the remaining equations become extremely complicated and un transparent however gempack is able to do the algebra to perform such substitutions and the backsolves automatically reaping a performance gain while leaving the model specification tablo file in its original simpler uncondensed form in fact for simple g all equations at grid level are substituted out leaving a regional level linear system of modest 700 size such a system takes very little time to solve however the coefficients of the system involve calculations at grid level the time taken is proportional to the number of grid cells hence solution time increases only linearly as a function of the number of grid cells see fig 6 4 2 linearization gempack can automatically translate the original equation system into a linearized system reformulated as a system of first order partial differential equations alternatively the modeler can specify conveniently interpretable linearized forms of the underlying behavioral equations as in equation 5 7 clever representation of the model e g using equation 8 in place of 5 can facilitate condensation as well as more rapid solution of the model in our case we substitute out all of the variables with a grid cell index in simple g all of the cross grid cell interactions are transmitted through regional market prices once we know the regional crop nitrogen irrigation capital and other input prices we can backsolve for crop output input use land prices and the shadow price of irrigation water in each grid cell independently however since the model is non linear recall equation 1 the cost shares in equation 5 must be updated at each step in the solution process consequently the model is solved by multistep methods such as the euler method or gragg s modified midpoint method pearson 1991 the solution of a large system of linear equations is accomplished using sparse matrix techniques schiffmann and jerie 2019 richardson extrapolation is used to improve accuracy pearson 1991 this linearized approach has proven capable of solving very large non linear models e g one data point in fig 6 is a model with 8 million grid cells 4 3 decomposition in addition to these features gempack has some extensions which prove invaluable in simple g applications it provides a way to formulate inequality constraints or non differentiable equations as complementarities bach and pearson 1996 which can be important in sustainability analyses it also offers a technique to decompose changes in model variables due to several shocks into components due to each individual shock harrison et al 2000 we will illustrate this in the first application undertaken in the next section of the paper 4 4 web application on geohub the web app version of simple g permits users to simulate explore and visualize the results of simple g without installing the gempack program or any visualization software linux versions of gempack programs run on the geohub server the web app also includes pre solved experiments and demonstrations on how to run the model and analyze results based on the policy briefs presented at the 2018 conference on long run sustainability of us agriculture https mygeohub org groups glass npc2018 the latest version of the web app allows users to run their own experiments which could range from global projections on food production and food demand up to grid level analysis see fig 7 this is done by uploading key growth rates via text based command files improvements of the tool are ongoing to make it more user friendly and easy to use 5 two applications here we illustrate the usefulness of the simple g model through two applications since we use the us focused version of simple g these two applications focus on the us however similar applications of simple g in other regions are underway the first application evaluates the role of global drivers of local sustainability stresses within the continental us in the second we consider the feedback to national and global markets stemming from locally implemented sustainability policies on irrigation water use together these applications demonstrate the capacity of simple g to capture global to local to global interactions 5 1 global drivers of local sustainability stresses in the coming decades changes in population income and technology will alter the pattern of agricultural crop consumption production and international trade we expect that productivity growth will lead to higher yields thereby moderating the demand for scarce land and water resources on the other hand we expect the changes in population and income growth will create heightened sustainability pressures for projecting this footrace between supply and demand forward to mid century we take predicted changes in population income and total factor productivity as in baldos and hertel 2014 these are reported in fig 8 and are based on the business as usual shared socioeconomic pathway ssp2 o neill et al 2014 we also assume that historical agricultural productivity growth rates persist to mid century fuglie 2012 south asia and china are projected to have the greatest cumulative per capita income growth over this period rising by 641 and 607 respectively sub saharan africa is expected to experience the highest rate of population growth 139 in contrast east europe and japan and korea are expected to see declines in their populations by mid century the role of global change drivers in projected growth for us crop production by 2050 is shown in fig 9 exploiting the decomposition feature of gempack harrison et al 2000 this figure shows that one quarter of the projected us production expansion is due to demand growth in south asia and china alone overall growth in income and population outside the us is far more important in driving us crop production than growth within the us this is due to higher income growth rates in the developing and emerging economies coupled with higher income elasticities of demand 1 and higher rates of population growth in africa and other low income regions fig 10 shows the pattern of cropland expansion across the us over the projections period as a percentage change from 2010 this particular indicator of sustainability stress reveals that absent any policy interventions the greatest land use change stresses will arise in the marginal areas on the edges of the corn belt there is very little remaining land available for expansion in the heart of the corn belt these marginal regions are often environmentally sensitive and they are also the areas where the largest land use stresses arose during the 2008 2012 biofuels boom period lark et al 2015 these changes are based on the statistically estimated gridded land supply elasticities villoria and liu 2018 5 2 limiting unsustainable water withdrawals as seen in fig 5 above many locations in the western us suffer from excess groundwater withdrawals despite productivity improvements our projections suggest that this situation will become even worse under our business as usual baseline due to global growth in the demand for us crops fig 11 a here we examine the impacts of a counterfactual scenario in which we do not allow any increase in water withdrawals in locations showing withdrawals in excess of recharge in the base year of 2010 fig 11 b fig 12 shows the impact of this water sustainability policy on irrigated cropland area in the us as well as global changes in cropland and production owing to this policy while the aggregate impact of the water withdrawal restriction on us crop production and land use is less than 1 it nonetheless has a significant impact on the pattern of crop production and the irrigated area the reduction in us production is partially offset by increased production in other regions of the world with eu south america china and sub saharan africa offsetting 19 19 12 and 11 of the reduction respectively compared to the baseline us aggregate water withdrawals decrease by 1 82 irrigated area declines by 0 13 and rainfed area increases by 0 08 while this figure seems to be very small for the whole country there are significant impacts on many local communities compared to the baseline the irrigated area declines by as much as 17 7 in some grid cells and may increase by up to 5 7 in other grid cells the rainfed area may also decline by up to 5 7 and may increase by 163 1 in other grid cells as shown in fig 13 irrigation is reduced in locations facing the sustainability restriction in a few grid cells rainfed land is converted to irrigated land in response to water limits which involves improvement in irrigation efficiency not allowing the unsustainable grid cells to increase groundwater withdrawal reduces the irrigated area by up to 370 ha in some grid cells each grid cell can have 3500 7000 ha of cropland fig 13 while the rainfed cropland is projected to increase in most of the us in response to the water withdrawal restrictions the highest absolute increase in rainfed land arises in the locations with water withdrawal limits as land reverts from irrigated to rainfed production increases in rainfed land is also projected to be higher in the marginal area as a response to the higher crop prices as shown in fig 14 5 3 other applications of simple g while these two applications use the high resolution us version of simple g there are other applications of this framework designed to address different research questions as described in table 2 these models may have different production structures fig 2 different spatial focus different resolutions different crop coverage and some employ different modules in one such application of simple g changes in ecosystem services are linked to land use as well as the productivity of land loduca et al 2020 specify grid cell specific wetland expansion and habitat conservation measures in simple g for the chesapeake bay watershed in the united states hertel ramankutty and baldos 2014 use the gridded terrestrial carbon data base from west et al 2010 to deduce changes in terrestrial carbon emissions stemming from an african green revolution based on cropland changes in the simple model given its importance to the sustainability debate simple g has a nutrition module that allows users to assess the impact of changes in price and income on the prevalence and depth of undernutrition in developing countries baldos and hertel 2014 it follows the fao neiken 2003 approach modelling the distribution of caloric intake in a region using a log normal distribution when coupled with information about the mean and standard deviation of consumption as well as the minimum caloric intake it is possible to deduce the prevalence of undernutrition as well as the average caloric gap of those who are undernourished with this module simple g can be used to assess food utilization outcomes in addition to food production and food prices key metrics for food availability and food access this module allows for the assessment of a variety of important questions such as the impact of climate change on food security baldos et al 2020 baldos and hertel 2014 this nutrition extension of the model links the local resources mainly land and water to global food security the goal is to create opportunities for analyzing the trade off between global food security goals and local sustainability of land and water resources kabir et al 2019 a recent extension of the simple model also estimates food waste by incorporating an econometrically estimated relationship between per capita income and the share of food availability that is wasted lopez barrera and hertel 2020 the authors use this extended model to examine the impacts of freezing the share of food waste in different regions and at different future dates one early application of this model has treated the world as a uniformly distributed set of global grid cells liu et al 2017 that research focused on the impact of emerging water scarcity at global scale and was undertaken in conjunction with the global wbm vörösmarty et al 1998 the global gridded implementation was performed at a coarser resolution 30 arc minute grids and the economic demands for irrigation and the hydrological supplies net of non agricultural uses were reconciled at the level of nearly 1000 hydrological sub basins this enabled the authors to explore the implications of various adaptations to water scarcity including inter basin water transfers as well as increased integration of commodity markets another version of the model has been developed for questions regarding sustainability of water resources haqiqi et al 2018 in the agricultural production it considers not only land use but also water use and nitrate fertilizer application similar to the model described in this paper the production is modelled at 5 arc min grid cells over the us for the rest of the world the production is modelled at the level of 16 market regions in this version there are two levels of aggregate demand one aggregation is at sub national production regions for the us then the global demand is modelled at aggregated regional level the water withdrawal module is similar to this paper with different functional forms for water supply simple g is also flexible in terms of crop coverage while most versions have considered all crops aggregate another set of models have focused on corn soy composite one application is the analysis of water quality in the corn belt of the us liu et al 2018 as a large portion of the water pollution has been related to corn and soy cultivation it makes sense to focus on the specific responses of these crops for this application a nitrate leaching module has been developed with crop specific yield and nitrate leaching response this module is parametrized with the outputs of agro ibis agronomic model as described earlier the most recent version of simple g was developed for high resolution conservation studies that version of the framework includes a module on potentially restorable wetlands on agricultural lands from enviroatlas the model is solved at a much finer resolution 750 m over the continental us parametrization of grid cells exploits satellite data as well as reported county level information this includes a detailed wetland restoration and conservation with conservation reserve program crp and conservation reserve enhancement program crep considering wildlife habitat water quality and greenhouse gas emissions loduca et al 2020 these applications illustrate the flexibility of the simple g framework it is not just a single model rather it is a flexible way of looking at the world indeed there are two nsf funded efforts underway that are building high resolution versions of simple g focusing on china and brazil 6 discussion and conclusions simple g is by no means the first attempt to undertake global economic analysis of sustainability challenges at the interface of agriculture and the environment using a grid resolving approach to our knowledge the first such model was magpie lotze campen et al 2008 this is a global optimization model with the objective of minimizing the global cost of producing food to meet a pre specified level of demand it was developed at the potsdam institute for climate pik and is typically used in conjunction with a gridded dynamic vegetation model to look at issues related to land use change climate impacts on agriculture bioenergy and technology change among other issues magpie differs fundamentally from the approach developed in this paper simple g is an economic equilibrium model in which decentralized agents e g irrigated crop producers in a given grid cell food processors or consumers in a particular regions of the world interact through regional and global markets in the presence of policy distortions and barriers to trade the global equilibrium determined by simple g will not minimize total costs in this sense it aims to be predictive as opposed to normative indeed the presence of market imperfections means that global optimization models such as magpie must often place artificial constraints on the model in order to allow it to replicate observed patterns of production consumption and international trade more recently the globiom model has emerged on the global sustainability scene it is maintained at the international institute for applied systems analysis valin et al 2013 like magpie globiom is a recursive dynamic optimization model in addition to 18 major crops it has a livestock module and when linked to models of crop growth bioenergy forestry and fisheries it has been used to deal with a wide range of sustainability issues including deforestation water use and greenhouse gas emissions havlik et al 2013 leclère et al 2014 soterroni et al 2018 there are regional versions of the model focusing on the eu and brazil among others soterroni et al 2019 due to its large size and complexity the model is not solved at the individual grid cell level but rather it is solved for representative groupings of grid cells in short it is a very ambitious undertaking involving dozens of researchers and this work represents the cutting edge of global sustainability research with grid cell resolution as its name indicates the simple g framework introduced in this paper has more modest aspirations rather than continually extending this model to handle new issues this framework aims to be as simple as possible while capturing the essence of a given sustainability challenge if a different set of challenges emerges the idea is to build a different version of simple g rather than extending the original model to add another feature at the heart of this simplicity lies the fact that simple g always has just one composite crop albeit produced with different techniques and resource requirements both within and across grid cells in the application presented here the single crop was a composite of all crops and our analysis focused on the extensive margin of land and water use in agriculture however as noted in the preceding section this crop could also be a single crop such as maize or a maize soy composite such as in liu et al 2018 from an economic point of view this means that within the crop composite it is assumed that prices move in tandem a key economic condition for aggregation of products for the all crop composite this doesn t make sense in the short run but over decades it is likely the case that substitution both on the supply and the demand sides will force crop prices to move together and this representation may be preferred to one in which modest substitution across crops permits significant divergence in crop prices over the long run in part due to this long run orientation simple g is not run on an annual recursive basis unlike the aforementioned models rather it is treated as a one shot comparative static model e g starting in 2010 one might simulate the global crop economy in 2050 as is done in the applications above the outputs from simple g can also be linked to other spatially explicit models one such example is provided by sun et al 2020 who use the us corn soy version of simple g to explore the land use and water quality impacts of widespread biomass co firing in the midwestern united states this paper shows that while the region wide deterioration in water quality is likely to be modest a gridded analysis reveals the presence of dangerous hotspots these arise where coal plants coincide with areas of concentrated corn production and already high levels of nitrogen fertilizer application and leaching similarly linking simple g to spatial models of water use such as li et al 2019 could offer important insights while the restriction to a single composite crop and the one shot comparative static approach may seem like a great sacrifice it has yielded one very significant benefit namely facilitation of model parameterization and validation to date all of the validation efforts have been conducted using the non gridded version of simple which breaks the world into 15 aggregate regions and these have been quite informative in the first such paper baldos and hertel 2013 found that simple was able to re produce global changes in aggregate crop output cropland area yield and prices over the 1961 2006 period this was a significant breakthrough in the global land use change literature and was used as a basis for understanding why many models at the time appeared to be over predicting long run land use change in the 21st century baldos and hertel 2013 historical simulation of the simple model has also revealed areas where the model falls short in particular while the first version of simple closely followed global crop production it failed to reproduce the regional pattern of production changes over this period this led the authors to introduce market segmentation whereby individual consumers and producers in each region have differential access to world markets at the aggregate level this results in a constant elasticity of substitution between domestic and international goods on the demand side and a constant elasticity of transformation between domestic and international goods on the supply side the resulting segmented markets version of simple now the default approach performed much better at the regional level and also resulted in very different consequences for a number of key sustainability policies hertel and baldos 2016 future work with simple g will focus on its ability to reproduce historical patterns of land use change and irrigation intensities at the level of subnational regions and individual grid cells this will provide the necessary foundation for policy relevant multi scale modelling of future sustainability challenges software and data availability simple g web application and simple g us web application models and data are open source and available for download for offline use or in cloud computing at https mygeohub org resources simpleg and https mygeohub org tools simpleus the regional economic data is taken from the faostat fao 2014 and gtap aguiar et al 2019 as well as regional simple hertel and baldos 2016 consumer demand elasticities are based on usda reports muhammad et al 2011 cropland area is obtained from the usda cropland data layer han et al 2012 at 30 meter resolution and aggregated to 5 arc min irrigated cropland is from the moderate resolution imaging spectroradiometer modis irrigated agriculture dataset for the united states mirad us provided by usgs at 250 meter resolution brown and pervez 2014 and aggregated to 5 arc min gridded land supply elasticities for the us are based on econometric estimations villoria and liu 2018 transformation elasticity parameter is estimated considering water right jame et al 2017 we take the value of crop sold per acre from usda nass by county usda nass 2019 and use gcwm siebert and döll 2010 simulated yields to generate gridded yield nitrogen fertilizer application rates leaching parameters and substitution elasticity between nitrogen fertilizer and other inputs are obtained from agro ibis liu et al 2018 lark et al 2020 irrigation water withdrawal rates are estimated using usgs county level water use data maupin et al 2014 information about groundwater recharge is taken from the annual estimate of recharge reitz et al 2017 maximum surface water available at each grid cell is calculated after subtracting non agricultural water use from locally generated runoff wolock 2003 maximum available ground water available is determined with groundwater stock befus et al 2017 gleeson et al 2016 the groundwater supply elasticity of is determined based on the ratio of groundwater withdrawal to groundwater recharge gleeson et al 2016 reitz et al 2017 and estimated value of water haqiqi et al 2016 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge support from the united states department of agriculture afri grant 2019 67023 29679 economic foundations of long run agricultural sustainability the national science foundation infews grant 1855937 identifying sustainability solutions through global local global analysis of a coupled water agriculture bioenergy system and the united states department of energy office of science biological and environmental research program earth and environmental systems modelling multisector dynamics contract de sc0016162 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104805 
25962,the prediction of wildfire rate of spread and growth under high wind speeds and dry fuel moisture conditions is key to taking proactive actions to warn and in turn protect communities we used two datasets of wildfires spreading under critical fire weather conditions to evaluate an existing rule of thumb that equates the forward rate of fire spread to 10 of the average open wind speed the rule predicted the observed rates of fire spread with an overall mean absolute error of 1 7 km h 1 the absolute error magnitude was consistent across the range in observed rates of fire spread resulting in a reduction in percent error with an increase in spread rates mean absolute percent errors close to 20 were obtained for wildfires spreading faster than 2 0 km h 1 the implications of model errors in the forecasting of fire spread with respect to community warning and safety are discussed keywords crown fire fine dead fuel moisture content fire behaviour fire prediction fire propagation fire weather fuel type model error 1 introduction early and contemporaneous research into the effects of global warming on wildfire activity have forecasted an increase in the length of the fire seasons and the number of days of extreme fire danger when large fire spread events can occur e g ryan 1991 beer and williams 1995 barbero et al 2020 the events of the last two decades or so have corroborated these hypotheses with an increase in the number of highly destructive wildfires globally tedim et al 2020 associated with many of these events is the substantial loss of human life and property at unprecedented rates teague et al 2010 goldammer et al 2019 gee and anguiano 2020 despite the science of fire behaviour prediction being well established in many parts of the globe scott et al 2014 it is clear that even in countries with a notable investment in fire behaviour research and operational fire intelligence e g situational awareness fire spread forecasting etc fires can still surprise fire management and emergency response agencies and cause a large number of fatalities this is largely due to a combination of factors including the inability of these agencies to forecast a wildfire s propagation rapidly and effectively communicate the potential wildfire threat to the public and take the necessary actions to safeguard human lives wildfires driven by strong winds fig 1 a c that within a few hours of their ignition grow to a large size and quickly impact communities with little or no official warning are often associated with multiple fatalities cruz et al 2012 blanchi et al 2014 brewer and clements 2019 xanthopoulos and athanasiou 2019 for this type of fires many of the methods used by specialised fire behaviour analysts scott et al 2014 neale and may 2018 imbedded in incident management teams or coordination centres to generate fire spread predictions andrews et al 2007 giannaros et al 2019 neale and may 2020 might well fail to meet the necessary urgency associated with such events cruz and alexander 2019 developed a rule of thumb for obtaining a first approximation of a wildfire s forward rate of spread in conifer forests eucalypt forests and shrublands but not in grasslands the rule of thumb states that a wildfire rate of forward spread is approximately 10 of the average 10 m open wind speed the rule was found to be most appropriate for strong wind and dry fuel conditions i e both fine fuel moisture and overall long term landscape dryness associated with fast spreading wildfires where one would consider that time is of the essence for public warnings and evacuation notices the rule of thumb was considered to be appropriate for wildfires spreading over level to gently undulating topography and in cases where large wildfires advance across drainages with alternating upslope and downslope runs under extended drought conditions fuels in mountainous topography are readily able to burn regardless of the variations in terrain characteristics cheney 1981 when winds are suitably strong and accordingly sustained a high intensity wildfire can continue to spread for several hours burning out entire forested drainages and crossing mountain ridges that would normally be a barrier to fire growth rothermel 1991 one such spectacular event occurred on september 6 1988 when the canyon creek fire under the influence of a dry cold front advanced eastward across the continental divide fig 1a and out onto the plains of west central montana usa goens 1990 although the empirical basis of the rule of thumb makes it a reflection of the broad wildfire dataset used in its development no independent evaluation of its performance has been carried out up to now here we examine the robustness of this rule of thumb by evaluating its predictive ability against independent wildfire datasets 2 methods observations of forward rate of fire spread were compiled from two different databases this included that of harris et al 2011 and kilinc et al 2012 for southern australia wildfires and the bonfire global fire behaviour database fernandes et al 2020 the dataset extracted from harris et al 2011 and kilinc et al 2012 was limited to wildfires in eucalypt forests luke and mcarthur 1978 and included detailed information on fire size fuel characteristics and predictions from rate of fire spread models currently used in south eastern australia mcarthur 1967 cheney et al 2012 the dataset extracted from bonfire is broader containing data from wildfires in shrublands eucalypt forests and conifer forests but overall less detailed regarding fire size and fuel characteristics 2 1 southern australia database kilinc et al 2012 assembled a fire behaviour database of australian wildfires occurring in native eucalypt forests this dataset is largely based on the data collated by harris et al 2011 aimed at evaluating the effects of fire weather and fire behaviour on community impacts although some new fires were added to the database by kilinc et al 2012 the combined dataset from harris et al 2011 and kilinc et al 2012 used in the current study is given in table a1 of the appendix 2 1 1 fire spread isochrones and forward rate of fire spread spatial fire propagation data including isochrones of progression at given time intervals were derived from published e g cfa 1999 and unpublished wildfire case studies reports aerial imagery photography and infra red line scans and newspaper articles many of the documented wildfires occurred prior to the 1980s with information regarding the spatial attributes of these fires recorded in the form of paper maps these maps were digitised geometrically rectified and the data then processed through a spatial geographical information system gis database infra red line scans of fire perimeter location at given times obtained from the country fire authority cfa and department of sustainability and environment dse in the state of victoria were used to reconstruct some of the major fires between the years 2000 and 2007 accounts of fire progression typically came from direct observations made by experienced fire suppression personnel as recorded in logbooks and radio logs the database also included more detailed fire reconstruction information from recent wildfire events such as the 2009 black saturday fires teague et al 2010 cruz et al 2012 the forward rate of fire spread measurement r km h 1 for a given time interval is the maximum r for the interval determined as r d t where d represents the maximum distance km from the end point of one fire isochrone to the end point of the preceding isochrone and t is the time period h between isochrones for some bushfires no fire progression maps could be retrieved but r measurements were documented in the form of reports 2 1 2 weather data and fuel moisture content weather data was obtained from the australian bureau of meteorology s automatic weather station aws network records and from reports which present data taken from manual weather stations located closest to the fire incident base data included air temperature relative humidity rainfall history and mean 10 m open wind speed u 10 km h 1 and direction in certain instances weather observations were made some distance from the fire and thus the actual fire conditions may have been significantly different to that indicated by the weather observations for each wildfire spread period considered between two isochrones the corresponding weather data was taken as the average meteorological conditions over the time interval field measurements of fine dead fuel moisture content mc oven dry weight were not available for the wildfires analysed mc was in turn estimated from air temperature and relative humidity using models for eucalypt forests matthews et al 2010 2 1 3 vegetation and fuel data the area enclosed by each wildfire isochrone was classified as either dry eucalypt forest or wet eucalypt forest this information was derived from the original fire reports and state vegetation maps dry eucalypt forests and woodlands typically consist of multi aged stands of a mix of eucalypts and have an understorey dominated by hard leaved shrubs grasses sedges or bracken fern wet eucalypt forests typically consist of a tall eucalypt overstorey of multi aged and mixed species and a dense understorey of ferns soft broad leaved shrubs and small trees for wildfires in victoria spatially averaged fuel characteristics were determined from a database of ecological vegetation classes and associated age related fuel accumulation curves e g walker 1981 allowing fuel characteristics such as load and fuel hazard by understorey component hines et al 2010 to be estimated at a given site for any given year for other states these fuel characteristics were determined from reports and estimated through visual examination of photographs using the hines et al 2010 fuel hazard rating and gould et al 2007 fuel hazard score assessment guides 2 2 bonfire database fernandes et al 2020 surveyed peer reviewed articles grey literature and unpublished data on file to develop a global fire behaviour database encompassing information from experimental fires prescribed fires and wildfires in the current analysis we focused solely on the wildfire component of the database the following information was retrieved for each fire georeferenced location broad vegetation type i e conifer forest eucalypt forest or shrubland dominant species slope steepness observed rate of fire spread fire run duration air temperature relative humidity and u 10 when available fuel complex attributes e g fuel loads per size class and live or dead condition curing or fine dead fuel fuel depth or height fuel layer cover were also extracted given the diversity of sources the dataset is somewhat variable in its completeness as part of the post processing of data mc values were estimated from air temperature and relative humidity using vegetation fuel type specific models this included matthews et al 2010 for eucalypt forest the parameterization of this model for semi arid shrublands given in cruz et al 2010 anderson et al 2015 for temperate shrublands and conifer forest with a prominent shrub layer for other conifer forests we used the rothermel 1983 fuel moisture tables to ensure consistency with the data used by cruz and alexander 2019 for this vegetation fuel type the bonfire data used in the current study is given in table a2 of the appendix 2 3 imposed data constraints and reliability several criteria were imposed on the datasets selected for the analysis to ensure compatibility with the intended use of the 10 rule of thumb data used in the analyses originated from wildfire runs lasting at least 1 0 h but less than 6 0 h wildfires that were affected by frontal passage driven wind changes e g cruz et al 2012 that altered the dynamics of the fire propagation process blanchi et al 2014 were not considered as the rule of thumb is not applicable to these situations cruz and alexander 2019 we also restricted the analyses to wildfires meeting the high wind i e u 10 30 km h 1 and low fuel moisture mc 7 conditions where the rule of thumb has shown to be most applicable cruz and alexander 2019 none of the wildfires used in the original formulation of the 10 rule of thumb alexander and cruz 2006 cheney et al 2012 anderson et al 2015 were considered in the present analyses kilinc et al 2012 and fernandes et al 2020 assigned wildfire data reliability scores for weather fuel and fire behaviour characteristics as per cheney et al 2012 and cruz et al 2012 this is provided in tables a1 and a2 respectively interpretation of the reliability scores is given in table a3 2 4 predicted fire spread rate data we applied the 10 rule of thumb to the u 10 values in both datasets to produce estimates of r for each wildfire we also used the predictions from the mcarthur 1967 and cheney et al 2012 fire spread models in the southern australia dataset to understand how the rule of thumb predictions compare with those of current operational fire spread models under heightened fire spread potential conditions predictions for the mcarthur 1967 model relied on an overall understorey fuel load comprised of surface near surface elevated and bark fuels similar analysis was not pursued for the bonfire dataset due to the absence of the necessary fuel information needed to apply the appropriate fire spread models 2 5 statistics model error predictions were quantified using the following statistics mean absolute error mae mean bias error mbe mean absolute percent error mape root mean square error rmse and the ratio between the mbe and mae which we call the mean bias percent error mbpe willmott 1982 cruz et al 2018 the mae expressed in the same units as the original data is a quantity used to measure how close predictions are to the observed value as the name suggests the mae is an average of the absolute error the mbe describes the dispersion or spread of the residual distribution about the estimate of the mean a positive value indicates an over prediction trend while a negative value is an indication of an under prediction trend the mape is a very popular measure of the accuracy of a predictive model or system it represents the summed differences between the individual predicted versus observed values divided by the observed value and expressed as a percentage if a perfect fit is obtained then the mape is zero the mbpe provides a measure of the bias in relation to the mae expressed as a percentage the rmse represents the standard deviation of the residuals prediction errors residuals are a measure of how far from the line of perfect agreement the data points lie the rmse is a measure of how spread out these residuals are the analysis of the rule of thumb error was conducted separately for the southern australia and bonfire datasets despite certain commonalities namely data from wildfires in australian eucalypt forests the data were not pooled together due to the different data collection methods standards and overall data characteristics we analysed differences in fire environment variables for subsets of datasets through unpaired two sample tests the shapiro wilk test of normality was used to determine if variables were normally distributed for normally distributed variables student t tests were used for non normal distributions the non parametric wilcoxon rank sum test was used all statistical analysis was conducted using the software r r core team 2019 3 results 3 1 southern australia dataset the original southern australia database consisted of 183 fire observations in eucalypt forests the dataset selected for analysis was reduced to 61 wildfire observations after removing fire runs with a duration of less than 1 0 h and cases of post frontal passage type of fire propagation within those a total of 30 fire runs had an estimated mc 7 and a measured u 10 30 km h 1 for this subset the average rate of fire spread was 3 6 km h 1 spanning a range of 0 8 8 0 km h 1 table 1 provides the basic statistics for the subset of data used in this study the 10 rule of thumb predicted the dataset with a mae of 1 75 km h 1 and a mbe of 0 89 km h 1 table 2 fig 2 shows the rule of thumb predictions clustered around two different areas there is a group of fires n 10 propagating with a forward rate of spread less than 2 0 km h 1 that were characterized by a strong over prediction bias the remaining two thirds of the data with higher rates of spread had predictions that mostly fell within the 35 band the wildfires in the over predicted group were slower spreading the shapiro wilk test of normality showed mc u 10 fuel loads and fire front width in each subset of data i e slower spreading wildfires with r 2 0 km h 1 vs faster spreading wildfires with r 2 km h 1 to not be normally distributed the slower spreading wildfires were characterized by higher mc levels mostly 5 6 averaging 5 3 than the dataset average of 4 4 the wilcoxon rank sum test indicated mc to be significantly higher in the slower spreading group than in the faster spreading group p 0 008 the same test found no statistically significant differences for wind speeds and fuel loads in the two groups α 0 05 fire front width was larger in the faster spreading wildfires 4 3 km than in the slower spreading group of fires 1 6 km with the wilcoxon rank sum test finding the differences significant p 0 004 considering only the wildfires spreading with a r 2 0 km h 1 the use of the rule of thumb resulted in an mae of 1 04 km h 1 mape of 22 4 and a mbe of 0 24 km h 1 predictions of rate of fire spread with the mcarthur 1967 and cheney et al 2012 models yielded higher maes 2 12 and 2 19 km h 1 respectively than the rule of thumb albeit showing distinct trends fig 2 the mcarthur 1967 model largely under predicted the spread of the wildfires with an mbe 2 03 km h 1 of similar absolute magnitude to the mae table 2 whereas the cheney et al 2012 model yielded a negligible bias mbe of 0 02 km h 1 both of these models produced more accurate predictions for the slower spreading fires r 2 0 km h 1 but higher errors for the faster spreading ones fig 2b the cheney et al 2012 model over predicted the rate of advance of all wildfires in the slower spreading group fig 2b 3 2 bonfire dataset the bonfire database compiled by fernandes et al 2020 contained a total of 167 wildfire runs in non grass fuels with a run duration 1 0 h not classified as post frontal passage fire propagation nor included in the datasets used to derive the rule of thumb i e alexander and cruz 2006 cheney et al 2012 anderson et al 2015 and not present in the southern australia dataset within that dataset there were a total of 58 fire observations in conifer forests eucalypt forests and shrublands that met the criteria of the mc 7 and u 10 30 km h 1 the basic statistics for this dataset are given in table 3 including the distribution by geographic location the r for these datasets averaged 4 0 km h 1 with the distribution of this variable mc and u 10 not significantly different from the southern australia dataset tukey multiple comparison tests indicated no significant differences between the r in the three broad vegetation fuel types present in the dataset comparable results were obtained for u 10 but significant differences p 0 05 were observed for the mc the application of the 10 rule of thumb resulted in a mae of 1 70 km h 1 table 4 when considering the aggregate of the three vegetation fuel types a value comparable to the one obtained for the southern australia dataset the 10 rule of thumb predicted r values for the dataset with a mbe of 0 49 km h 1 or 29 of the mae table 4 overall the most accurate predictions were obtained for the shrubland subset mae 1 54 km h 1 mape 59 6 followed by the conifer forest subset mae 1 55 km h 1 mape 80 3 and then the eucalypt forest subset mae 1 95 km h 1 mape 89 3 table 4 as with the southern australia dataset the analysis shows a notable over prediction bias for wildfires propagating with rates of spread 2 0 km h 1 fig 3 a large proportion of the fires spreading with an r between 2 0 and 8 0 km h 1 were predicted within the 35 error prediction band the rule of thumb tended to under predict fires spreading with an r 7 5 km h 1 although errors for these fires were around the 35 threshold fig 3 4 discussion 4 1 fire spread prediction error the spread of a wildfire flame front comprises very dynamic phenomenon influenced by a number of variables and processes its prediction in an operational setting over a period of hours to days is fraught with uncertainty associated with the limitations of our understanding of the controlling processes difficulty in accurately estimating a model s input variables compounding effects of errors and the obvious difficulty in describing the chaotic nature of fluids in a turbulent and constantly changing environment albini 1976 catchpole et al 1993 report that measured rates of fire spread in replicated i e same environmental conditions laboratory experimental fires conducted under constant wind speed conditions agreed to within 20 fig 4 a this result can be seen as a benchmark with respect to fire spread variability spreading fires will exhibit notable variability even when burning in a controlled environment under homogeneous and identical conditions outdoor fires but especially wildfires will obviously be characterized by a higher variability given the transient and dynamic nature of boundary layer meteorology plus the spatial variability in fuel characteristics e g structure moisture and terrain e g slope steepness aspect or slope exposure the most accurate results obtained with empirical models in a field setting are for model predictions against the datasets used in their development with mapes varying between 20 and 35 cheney et al 1998 2012 fernandes 2001 fig 4b prediction errors naturally increase when the models are applied to independent experimental fire fig 4c or wildfire fig 4d datasets in these cases the mape has been characterized by an interquartile range varying from 50 to 70 for experimental fire datasets cruz and alexander 2013 and from 54 to 120 for wildfire data alexander and cruz 2006 cheney et al 2012 kilinc et al 2012 anderson et al 2015 the main reason for the error increase in the latter dataset is due to the large uncertainty in spread rate measurements model inputs and the natural unaccounted variability in the fire environment associated with large wildfire runs cruz and alexander 2013 2019 coen et al 2018 considering the spread of fires under critical fire weather conditions our analysis was based on a broader dataset n 88 than the original work by cruz and alexander 2019 that included only 24 wildfire runs out of 118 observations within the constraints of mc 7 and u 10 30 km h 1 the results obtained by the simple 10 rate of spread rule of thumb as analysed in the present study with the mape varying between 60 and 101 tables 2 and 4 is first and foremost indicative of the strong control wind speed alone exerts on landscape scale fire propagation under dry fuel conditions this despite the uncertainty in the use of measured wind speed data as being representative of the conditions driving a wildfire some distance away from the measurement location as observed in the analysis of cruz and alexander 2019 the percent error associated with the rule of thumb predictions decreases substantially with the increase in r the mape varied between 22 and 34 when considering wildfires that were propagating with an r 2 0 km h 1 the wildfires that are most dangerous from the point of view of community and fire fighter safety this error is on par with the error obtained by empirical based fire spread models when assessed against their original datasets with in situ accurate measurements of the fire environment e g cheney et al 1998 fernandes 2001 the results presented in figs 2 and 3 raise the question of why does the 10 rule of thumb work so well under certain conditions and not so well in others the analysis showed a substantial over prediction for wildfires observed to spread with an r 2 0 km h 1 independent of the dataset a similar over prediction bias for wildfires in this range of observed rate of spread was noted in the original analysis by cruz and alexander 2019 in the southern australia related dataset this bias was related to fireline width and time of day mostly early to mid afternoon fire runs this hints at the notion that early afternoon conditions reflect the fact that longer timelag fuels nelson 2001 are not yet fully available for combustion and the possible impact of effective suppression plucinski 2019a 2019b in some areas around a wildfire s perimeter could explain the smaller fireline width and the lower observed rates of fire spread relative to the 10 rule of thumb expectation unfortunately the bonfire dataset did not have the detail necessary to further explore this issue these results highlight some of the limitations of the 10 rule of thumb namely that its best accuracy might be restricted to lower mc levels than proposed by cruz and alexander 2019 in their analysis it was suggested that the rule of thumb worked best below an mc of 7 while the current analysis with a broader dataset indicates that the most accurate results are obtained with a mc level up to 5 the analysis of the bonfire related dataset identified an under prediction trend for wildfires spreading with an r 7 5 km h 1 there were eight fires in this group with seven under predictions six of them with under predictions varying between 29 and 38 and one of them with a 55 a closer review of these wildfires reveals a prevalence of documented long range spotting distances in most of them namely the 1983 deans marsh fire 10 km spotting rawson et al 1983 and the east trentham fire 10 12 km rawson et al 1983 storey et al 2020b in victoria australia the 1936 galatea creek fire in alberta canada 5 6 km fryer and johnson 1988 and the 1983 mount muirhead fire in south australia 15 20 km keeves and douglas 1983 long range spotting is a highly stochastic process linked to a number of variables such as the size of the active fire area fuel type s terrain roughness wind speed levels and wind exposure page et al 2018 storey et al 2020a the processes are heuristically understood with long range spotting associated with wind driven wildfires typically linked to fires accelerating in wind exposed upslope runs this results in localised increases in energy release an increase in the number of firebrands generated and pulses in upward momentum in a wildfire s plume kerr et al 1971 luke and mcarthur 1978 mccarthy et al 2018 these periodic pulses are able to transport firebrands higher into the wildfire s plume where upper levels winds can then maximise their downwind transport albini et al 2012 however not all upslope fire runs will lead to the long range spotting as observed in the wildfires mentioned above the results obtained seem to indicate that if the fire environment is conducive to the occurrence of long range spotting distances then the 10 rule of thumb will likely under predict the overall fire spread distance in our evaluation the level of under prediction was between 30 and 40 which is fairly acceptable given the uncertainty in the input variables and the stochasticity of the process although higher under prediction errors cannot be ruled out the fact that the 10 rule of thumb predicted well wildfires driven by winds of 60 70 km h 1 with gusts up to around 100 km h 1 and characterized by long range spotting with errors up to 40 is in itself a very interesting result 4 2 fire spread prediction error operational implications as simulations of fire spread models are used to support decision making during ongoing wildfires it is important that the users of such information understand the uncertainty in fire spread predictions either due to errors associated with inaccurate inputs or model limitations albini 1976 despite the complexity of wildfire phenomena the unknowns in our scientific understanding of fire behaviour and the chaos associated with a wildfire approaching the wui to the individuals making decisions regarding the safety of communities or fire fighters in the field the questions are very simple where is the wildfire at the moment and what is its rate of spread and intensity luke and mcarthur 1978 will the wildfire reach a particular community or pre defined trigger point and at what time will it do so cova et al 2005 ramirez et al 2019 the decision maker will need to understand the uncertainty and potential bias associated with the fire spread prediction to make better decisions and tailor the actions to be taken fig 5 summarises the impact of a rate of spread prediction error on a hypothetical example of a wildfire starting 6 0 km upwind of a community the example assumes that 0 5 h after ignition the wildfire is spreading at 3 0 km h 1 for the sake of simplicity the wildfire is assumed to have been detected the moment following ignition and that in the early stages of fire propagation i e during its build up phase the fire was spreading at the pseudo steady state rate of spread luke and mcarthur 1978 the example considers the wildfire to impact the community 2 0 h after ignition and that a fire behaviour analyst produced a forecast of fire spread 0 5 h after the ignition was detected and the local authorities act upon this information to immediately release a warning to the general public if a correct r prediction of 3 0 km h 1 is made i e 0 error and an evacuation warning is issued then the population has 1 5 h to act before the wildfire impacts the community over predictions will result in a reduction in the perceived time to impact i e the community will think that they have less time to act than in reality with an increase in an over prediction the predicted time to impact decreases to a level that might lead to a negative outcome area a in fig 5 e g emergency services decide that the time for a community to safely evacuate is too short and thus do not issue an evacuation warning this is clear for the 100 over prediction error a r prediction of 6 0 km h 1 where a predicted time to impact of 0 5 h might lead to a change from evacuation to a shelter in place warning such advice against evacuation when time does allow for its safe implementation can potentially put members of the general public at undue risk under prediction biases of wildfire rate of spread can also have a detrimental but distinct impact on the decision making process under predictions will result in an erroneous over estimate of the time to fire impact potentially removing the necessary sense of urgency area b in fig 5 for example the largest under prediction in fig 5 suggesting 6 0 h to impact might delay any warnings to the general public during a critical time period considering 1 0 h as a time scale relevant for community evacuation li et al 2019 errors up to 33 are not likely to have a detrimental impact on public safety it is errors above this threshold and in particular under prediction errors that can result in a lack of timely and appropriate warnings leading to the most detrimental consequences cheney 1981 teague et al 2010 4 3 performance of the 10 rule of thumb against five notable recent wildfire disasters the recent past is populated with some of the deadliest single wildfire events on record box 1 common features of these wildfires were for example a new ignition starting the deadly fire run with the exception of the 2017 arganil seia fire strong winds leading to fast spread rates impact into communities within a few hours of their ignition and communities not warned of the impending danger until it was too late to evacuate to safe areas e g xanthopoulos and athanasiou 2019 for most of these wildfires the lack of a formal warning to the communities prior to fire s impact was due in part to the lack of situational awareness and an under appreciation of the fire spread rate potential by civil protection emergency response agencies teague et al 2010 goldammer et al 2019 in complementing the analyses presented in the previous sections table 5 provides some additional details on the characteristics of the wildfires listed in box 1 a feature of the measured wind data reported in table 5 note the different standards for the measurement height above ground for these wildfires is the broad range in the average wind speeds for each of the fires fig 6 winds were measured at different locations near the vicinity of these wildfires or within the final fire perimeter weather stations situated on ridgelines or on windward slopes generally resulted in the upper range in wind speeds whereas stations located at lower elevations where the impacted communities typically were yielded a lower range in wind speeds these results highlight some of the inherent issues of predicting the spread of wildfires advancing across complex topography detailed wind studies such as those carried out by coen et al 2018 brewer and clements 2019 and lagouvardos et al 2019 for example further highlight the spatial heterogeneity in landscape scale winds and the complexities associated with the strong wind events linked to some of the catastrophic wildfires described in box 1 and table 5 this also calls attention to the fact that the wind speed data given in tables a1 and a2 should be seen as indicative not necessarily as a precise value representative of a large wildfire run the 10 rule of thumb yielded estimates that approximate the observed rates of spread of the wildfire disasters fig 6 the wind speed bar can be read as the 10 rule of thumb r prediction in the rate of fire spread axis label the application of the rule of thumb under predicted the spread rate for the 2009 kilmore east fire 26 error when considering the maximum wind speed and the overall rate of fire spread a result linked to the occurrence of long range spotting during this fire s major run cruz et al 2012 and over predicted the maximum spread rate for the 2017 tubbs fire 35 error which are clearly errors within an acceptable range for wildfire propagation prediction errors for the other wildfires contained in table 5 were smaller with the range in the predicted rate of spread based on the range in observed wind speeds overlapping the range in observed rate of fire spread fig 6 we need to emphasize that these results should be seen as qualitative and only as an illustration of the usefulness of the 10 rule of thumb as a first approximation in situations where there is no particular fire behaviour prediction know how or there is no time to apply more comprehensive and accepted fire behaviour prediction methods e g rothermel 1983 1991 plucinski et al 2017 taylor and alexander 2018 the quick usage of the rule of thumb in these situations when time is of the essence leaves more time for undertaking other time critical actions including informing the general population the strong control that wind speed exerts on the propagation of wildfire conflagrations as shown in the analysis of wildfire case studies highlights the importance of accurate wind forecasting coen et al 2018 lac et al 2018 lagouvardos et al 2019 and its fine scale spatial modelling wagenbrenner et al 2016 filippi et al 2018 especially in complex terrain to guide effective decision making and issuing of warnings during extreme burning conditions 4 4 on the wind speed effect on fire propagation the error analysis resulting from the evaluation of the 10 rule of thumb against independent data has provided additional clues as to the influence of wind speed on wildfire propagation rates we noted in section 4 2 that a group of the slower spreading fires i e r 2 0 km h 1 were found to be over predicted by the 10 rule of thumb although we did not apply the 10 rule to fires spreading with u 10 levels 30 km h 1 or mc 7 in the present study the analysis by cruz and alexander 2019 found an over prediction bias for wildfires spreading under these conditions in contrast the low percent errors produced for wildfires with observed r levels 2 0 km h 1 hints at the overarching control wind speed exerts on the forward speed of wildfires when fine dead fuels are critically dry and wind speeds are strong the fit statistics do not leave much room for other influences be it a forest or shrubland vegetation fuel type topographic effects or fire atmosphere interactions when wildfires are spreading under these conditions potter 2002 suggested that under strong wind conditions the convective plume tilt of a wildfire along with the transport of the plume condensation area downwind will lead to a decoupling between the advancing flame front of the fire at the surface and the plume above this will limit dynamic feedbacks such as downdrafts or return flows on the advancing flame front that may arise from the moisture latent heat release in the fire s plume in these situations wind speed and fuel dryness control a wildfire s spread rate the observed over prediction for the group of slower spreading wildfires mentioned above could arise from fire atmosphere interactions in situations with lower wind speeds the fire s convection column is more vertical than in the strong wind case and coupling between the upper levels of the plume and the surface can occur byram 1959 rothermel 1991 air entrainment due to vertical plume development in distinct layers of the atmosphere coupled with moisture condensation will feedback into a free burning fire as the vertical motions change the near fire surface winds potter 2002 these feedbacks can result in periodic or occasional strong downdrafts winds that can cause sudden changes in fire behaviour potter 2005 coen 2011 occasionally with possible life threatening consequences e g rothermel 1991 goens and andrews 1998 despite the occasional extreme flows associated with dynamic feedbacks the strong entrainment into the fire s plume and upward motions will when considering the time scales used in our study i e greater than 1 h result in overall lower horizontal winds at the surface this might possibly explain in part the observed fire spread rates being lower than expected based on the rule of thumb 5 conclusions we conducted an examination of the predictive ability of the cruz and alexander 2019 10 rule of thumb to estimate a wildfire s forward rate of fire spread using two independent datasets this simple rule of thumb aims to provide first approximations of wildfire propagation for situations where there is little or no time to apply more comprehensive and accepted fire behaviour prediction methods the rule of thumb was shown to work well with overall mapes between 80 and 100 comparable to other model evaluation studies based on wildfire data the analysis showed the rule of thumb to work best for fast spreading wildfires r 2 0 km h 1 for these cases the mape varied between 22 and 34 a result on par with error statistics obtained when evaluating empirical based fire spread models against the data used in their development our analysis showed that the range of conditions where the rule of thumb worked best is possibly more restrictive than originally thought we found the rule to be most reliable under strong wind u 10 30 km h 1 and dry fine fuel mc 5 rather than 7 conditions typically associated with fast spreading wildfires it is these types of fires that can surprise communities and emergency response agencies due to their high potential spread rates the 10 rule of thumb is relevant when landscape scale dryness is conducive to major wildfire outbreaks i e low moisture content levels for live fuels and dead fuels with long timelag response e g deep duff layers and large diameter coarse woody debris although the focus of the present work was to evaluate the 10 rule of thumb against wildfire data the analysis also provided insight into fundamental properties of fires burning under high fire spread potential despite the uncertainty and variability in the data the trends are clear wind speed has an overwhelmingly dominant effect on the spread rate of wildfires when fuels are dry and the wind is strong insight into the processes and variables with strong influence on large scale fire propagation can only arise from the analysis of wildfire data rather than experimental fires field or laboratory or relying solely on simulation modelling recent research into spot fire controls page et al 2018 storey et al 2020a rate of fire spread in mountain pine beetle killed stands perrakis et al 2014 and whole fire atmosphere processes dowdy et al 2017 brewer and clements 2019 mccarthy et al 2019 are examples of new insights into fire dynamics based on wildfire data that can help us better understand wildfire propagation and guide future fire behaviour modelling efforts advances and deployment of remote sensing technology better mapping of vegetation and its structure at landscape scales and more accurate measurement of weather variables have created opportunities to reduce the uncertainty associated with the quantification of wildfire propagation for research studies these data sourcing techniques will in turn lead to improvements of our understanding of fire dynamics model calibration and more accurate forecasting of wildfire propagation on a concluding note the evaluation of a fire behaviour model or guide constitutes a continuing practice watts 1987 in this regard we plan to continue to add to the southern australian and bonfire databases which will allow for the periodic evaluation of the 10 rule of thumb and other fire spread models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements pmf and âs contribution was carried out in the framework of the uidb 04033 2020 project funded by the portuguese foundation for science and technology fct and the bonfire ptdc aag maa 2656 2014 project funded by fct and the european regional development fund erdf through compete 2020 operational program for competitiveness and internationalization poci âs received support from the portuguese foundation for science and technology fct through ph d grant sfrh bd 132838 2017 funded by the ministry of science technology and higher education and by the european social fund operational program human capital within the 2014 2020 eu strategic framework the authors are indebted to andrew sullivan and matt plucinski csiro australia craig clements and matthew brewer san jose state university usa and stuart matthews nsw rfs australia for their helpful comments on earlier versions of this paper the assistance of gavril xanthopoulos and miltiadis athanasiou hellenic agricultural organization demeter greece and kostas lagouvardos national observatory of athens greece in the details surrounding the 2018 mati fire is hereby appreciated appendix table a1 date fire name fire run time interval weather conditions fuel loads fire behaviour characteristics and reliability scores for the southern australia wildfire dataset harris et al 2011 kilinc et al 2012 the weather data and in particular u 10 should be seen as indicative not a precise value for the fire run table a1 calendar date fire name fire spread time interval t c rh mc u 10 km h 1 r km h 1 fire width km surface fuel load kg m 2 total fuel load kg m 2 reliability weather fuel r a january 16 1962 daylesford 21 30 22 30 33 17 4 2 37 1 2 1 6 1 4 2 1 3 3 3 april 4 1978 gervasse 16 00 17 30 28 29 5 8 100 8 3 1 0 4 0 4 3 3 3 april 4 1978 maranup ford 11 00 12 00 28 29 5 8 40 5 4 2 0 0 4 0 4 3 3 3 february 16 1983 otways 16 00 17 00 41 4 2 5 44 4 5 3 7 1 7 3 0 3 3 4 february 16 1983 otways 17 00 18 00 41 5 2 6 44 3 7 5 6 1 6 3 0 3 3 4 february 16 1983 otways 18 00 19 30 41 5 2 6 39 5 10 0 1 6 3 0 3 3 4 february 16 1983 otways 20 00 21 00 41 5 2 6 47 3 7 11 3 1 1 2 4 3 3 4 february 16 1983 cockatoo 20 00 21 05 41 5 2 6 47 0 8 0 4 1 1 1 8 2 3 2 february 16 1983 mt lofty 13 50 15 10 40 10 3 3 41 3 4 1 4 0 7 1 2 3 4 3 january 14 1985 anakie 14 40 15 40 42 7 2 9 37 5 9 4 4 0 7 1 1 2 2 2 january 8 1994 springwood 13 00 15 30 34 20 4 6 41 3 1 1 2 0 2 7 3 4 5 january 8 1994 springwood 16 00 17 30 36 18 4 3 43 2 8 3 3 2 0 2 7 3 4 5 december 2 1998 linton 13 00 14 00 28 26 5 5 41 0 8 0 4 1 8 3 0 2 3 2 december 2 1998 linton 16 15 18 00 29 23 5 1 30 0 9 1 0 1 7 2 8 2 3 2 march 12 2006 riley road 15 00 17 39 36 9 3 2 33 2 5 1 1 1 7 2 8 3 3 3 december 14 2006 coopers creek 15 00 16 00 35 9 3 2 48 6 2 7 1 8 2 9 3 3 3 december 14 2006 coopers creek 16 00 18 00 27 25 5 4 46 6 1 14 5 1 8 2 9 3 3 3 january 21 2006 century track 16 30 18 30 41 18 4 2 39 4 4 2 8 1 7 2 8 3 3 3 february 7 2009 bunyip 13 00 14 00 43 10 3 2 46 4 7 2 0 1 0 2 3 2 3 3 february 7 2009 kilmore east 17 00 18 00 41 10 3 2 46 5 8 6 9 1 3 2 4 2 3 3 february 7 2009 white timber spur 13 30 15 00 25 31 6 1 61 1 7 1 1 1 5 1 9 3 3 3 february 7 2009 white timber spur 15 00 16 30 27 26 5 5 55 1 6 1 4 1 4 1 8 3 3 3 february 7 2009 white timber spur 16 30 17 30 27 24 5 2 46 6 7 1 8 1 4 1 8 3 3 3 february 7 2009 white timber spur 17 30 18 30 27 25 5 4 41 5 1 3 1 1 3 1 6 3 3 3 february 7 2009 white timber spur 18 30 19 30 26 26 5 5 52 4 9 3 9 1 2 1 6 3 3 3 february 7 2009 white timber spur 19 30 20 30 25 27 5 6 50 1 9 4 3 1 1 1 5 3 3 3 february 7 2009 white timber spur 20 30 22 00 25 29 5 9 48 1 4 5 2 1 1 1 5 3 3 3 february 6 2011 roleystone kelmscott 13 30 14 30 23 31 6 2 43 1 7 0 2 1 4 1 8 2 3 5 february 6 2011 roleystone kelmscott 14 30 15 30 24 29 5 9 39 1 4 0 6 1 4 1 8 2 3 5 february 1 2011 tostaree 13 00 16 00 39 13 3 6 31 3 8 1 5 1 9 3 0 3 3 3 asee table a3 table a2 date fire name and country vegetation type weather conditions rate of fire spread reliability scores and fuel loads for the bonfire dataset fernandes et al 2020 the weather data and in particular u 10 should be seen as indicative not a precise value for the fire run table a2 calendar date dd mm yyyy fire name country vegetation fuel type t c rh mc u 10 km h 1 r km h 1 fine fuel load kg m 2 reliability weather fuel r a source 11 8 2006 serra da ossa portugal forest eucalypt 34 15 4 0 37 2 5 1 5 2 2 2 unpublished data on file with p m fernandes 16 2 1983 belgrave australia forest eucalypt 42 5 2 6 52 6 6 1 9 2 3 2 keeves and douglas 1983 16 2 1983 belgrave australia forest eucalypt 41 7 2 9 50 3 5 1 4 2 3 2 keeves and douglas 1983 17 11 1962 longford australia forest eucalypt 32 7 17 4 3 48 1 4 1 2 1 3 2 mcarthur 1965 15 11 2002 redmond australia forest eucalypt 35 13 3 5 50 6 0 8 5 2 3 3 mccaw 2003 16 2 1983 east trentham macedon australia forest eucalypt 41 5 3 5 44 4 3 2 5 2 3 4 rawson et al 1983 16 2 1983 east trentham macedon australia forest eucalypt 41 7 3 5 70 11 2 2 2 2 3 4 rawson et al 1983 16 2 1983 east trentham macedon australia forest eucalypt 41 7 3 5 47 6 0 1 5 2 3 4 rawson et al 1983 16 2 1983 east trentham macedon australia forest eucalypt 41 7 3 5 33 1 3 2 3 2 3 4 rawson et al 1983 20 12 1974 rocky gully australia forest eucalypt 40 10 3 0 70 6 4 1 5 4 3 3 underwood et al 1985 4 4 1978 brunswick australia forest eucalypt na na 4 0 52 8 0 na 3 3 3 underwood et al 1985 16 2 1983 deans marsh australia forest eucalypt 40 11 3 0 70 10 0 na 2 4 3 rawson et al 1983 29 1 2009 delburn australia forest eucalypt 44 5 10 3 2 42 2 2 1 5 2 3 3 harris et al 2011 7 2 2009 maiden gully australia forest eucalypt 44 9 7 2 8 41 1 9 1 0 2 3 3 harris et al 2011 8 3 1990 millbrook road australia forest eucalypt 31 5 12 3 7 30 1 5 na 2 3 3 pratt 1990 20 1 1988 blackjack australia forest eucalypt 29 18 4 4 30 1 5 na 1 2 1 bartlett 1993 14 1 1962 dandenongs australia forest eucalypt 39 2 12 3 5 39 1 2 1 4 3 3 3 harris et al 2011 8 1 1969 maldon australia forest eucalypt 37 1 6 2 8 37 0 8 0 3 3 3 3 harris et al 2011 13 1 1939 black friday australia forest eucalypt 44 6 9 3 0 56 1 6 1 6 5 3 3 harris et al 2011 13 1 1939 colac australia forest eucalypt 42 2 9 3 1 56 4 9 1 3 3 3 3 sullivan 2004 13 1 1939 kyneton australia forest eucalypt 42 2 9 3 1 56 5 2 1 4 3 3 3 sullivan 2004 13 1 1939 tawong australia forest eucalypt 46 13 3 5 56 7 9 1 1 3 3 3 sullivan 2004 18 01 2003 mt stromlo australia forest conifer 37 10 6 0 37 3 5 na 1 1 1 gellie 2005 31 7 2001 las palomas spain forest conifer 28 21 4 0 57 2 6 0 7 1 1 1 rodríguez y silva and molina martinez 2012 2 2 1979 caroline australia forest conifer 37 17 6 0 46 4 8 2 5 2 3 3 billing 1980 17 6 2017 pedrogão grande portugal forest conifer 31 5 34 7 0 30 2 3 1 7 2 3 2 guerreiro et al 2017 26 2 1995 berwick forest new zealand forest conifer 34 5 10 6 0 30 0 5 3 0 1 3 1 fogarty et al 1997 26 2 1995 berwick forest new zealand forest conifer 33 5 14 6 0 32 0 9 3 0 1 3 1 fogarty et al 1997 26 2 1995 berwick forest new zealand forest conifer 33 16 6 0 33 2 3 3 0 1 3 1 fogarty et al 1997 21 7 2009 horta de sant joan spain forest conifer 38 11 6 0 62 4 9 na 2 3 1 graf 2010 16 2 1983 narraweena australia forest conifer 40 10 6 0 80 8 0 na 2 3 3 keeves and douglas 1983 16 2 1983 mount muirhead australia forest conifer 40 10 6 0 80 12 5 na 2 3 3 keeves and douglas 1983 6 9 1988 canyon creek usa forest conifer 27 15 6 0 55 6 2 2 8 4 3 2 goens 1990 bushey 1991 ward et al 1994 20 10 1991 east bay usa forest conifer 32 2 17 6 0 37 1 5 na 1 2 1 alexander 2002 nfpa 1992 15 10 2017 mata nacional de leiria portugal forest conifer 32 1 19 6 0 37 4 7 1 3 2 2 2 guerreiro et al 2018 3 8 1936 galatea creek canada forest conifer na na 7 0 55 7 8 na 2 3 1 fryer and johnson 1988 8 5 1987 wallace lake canada forest conifer 28 12 6 0 30 3 9 na 4 3 1 hirsch 1988 27 9 1994 beerburrum australia forest conifer 36 6 12 6 0 50 3 6 na 1 3 1 hunt et al 1995 6 11 1994 beerburrum australia forest conifer 24 14 6 0 38 1 6 na 1 3 1 hunt et al 1995 10 1 1987 lago puelo argentina forest conifer 30 20 7 0 30 1 7 na 4 2 2 sagarzazu and defossé 2009 10 1 1987 lago puelo argentina forest conifer 30 20 7 0 50 4 3 na 4 2 2 sagarzazu and defossé 2009 23 7 2018 mati greece forest conifer 38 17 6 0 44 2 6 na 2 na 2 xanthopoulos et al 2018 10 1 1987 lago puelo argentina forest conifer 30 20 7 0 45 2 3 na 4 2 2 sagarzazu and defossé 2009 27 07 2007 obejo spain shrubland 39 11 4 5 36 3 1 2 8 1 1 1 rodríguez y silva and molina martinez 2012 31 07 2001 sierra parda spain shrubland 33 18 5 9 33 4 1 2 8 1 1 1 rodríguez y silva and molina martinez 2012 18 07 2012 tavira portugal shrubland 21 24 6 0 40 2 0 1 7 2 3 2 viegas et al 2012 21 12 1989 fitzgerald river np australia shrubland 33 12 4 0 42 1 9 na 2 5 3 mccaw et al 1992 21 12 1989 fitzgerald river np australia shrubland 35 9 3 5 43 3 0 na 2 5 3 mccaw et al 1992 21 12 1989 fitzgerald river np australia shrubland 35 9 2 0 34 7 5 na 2 5 3 mccaw et al 1992 21 12 1989 fitzgerald river np australia shrubland 35 9 3 0 43 4 8 na 2 5 3 mccaw et al 1992 9 7 2013 picões portugal shrubland 34 15 5 5 35 4 0 na 2 3 2 viegas et al 2013 15 10 2017 mata nacional de leiria portugal shrubland 29 9 21 4 3 40 6 5 1 7 2 2 2 guerreiro et al 2018 15 10 2017 relva velha portugal shrubland 32 9 16 8 5 8 35 4 5 1 5 2 3 2 guerreiro et al 2018 19 9 2010 machine gun usa shrubland 32 6 2 9 37 2 6 na 2 3 1 frost 2015 22 12 1980 dimboola australia shrubland 35 7 16 6 1 40 2 0 1 0 3 3 3 harris et al 2011 7 1 1979 epuyn lake argentina shrubland 30 20 6 6 39 1 1 na 4 2 2 sagarzazu and defossé 2009 9 10 2017 tubbs usa shrubland 32 8 7 4 5 73 6 5 na 2 3 2 coen et al 2018 nauslar et al 2018 5 8 2018 perna da negra portugal shrubland 24 8 14 6 1 34 2 4 2 2 3 3 3 rego et al 2019 a see table a3 table a3 reliability rating for weather fuel and fire spread observations for wildfire case studies adapted from cheney et al 2012 and cruz et al 2012 table a3 rating weather fuel complex rate of spread 1 nearby 25 km meteorological station or direct measurements in the field with high quality instruments and or validated modelled wind field fuel characteristics inferred from a fuel age function developed for the particular fuel type and area direct timing of fire spread measurements i e infrared scans aerial observations observed reference points with photographs 2 meteorological station within 50 km of the fire with no local effects i e terrain vegetation on the wind field and or partially validated modelled wind field fuel characteristics inferred from a visual assessment or measurements of nearby unburnt forest reliable timing within 15 min of fire spread by field observations with general reference points 3 meteorological station within 50 km of the fire but there are local effects on the wind field or the data not representative of the fire area meteorological station 50 km of the fire reconstruction of wind speed for fire site unvalidated modelled wind field fuel characteristics inferred from a fuel age curve for a forest type of similar structure reconstruction of fire spread with numerous cross references 4 spot meteorological observation near the fire fuel characteristics typical of equilibrium level in the representative fuel type doubtful reconstruction of fire spread 5 distant meteorological observations at locations very different to fire site qualitative fuel type description anecdotal or conflicting reports of fire spread 
25962,the prediction of wildfire rate of spread and growth under high wind speeds and dry fuel moisture conditions is key to taking proactive actions to warn and in turn protect communities we used two datasets of wildfires spreading under critical fire weather conditions to evaluate an existing rule of thumb that equates the forward rate of fire spread to 10 of the average open wind speed the rule predicted the observed rates of fire spread with an overall mean absolute error of 1 7 km h 1 the absolute error magnitude was consistent across the range in observed rates of fire spread resulting in a reduction in percent error with an increase in spread rates mean absolute percent errors close to 20 were obtained for wildfires spreading faster than 2 0 km h 1 the implications of model errors in the forecasting of fire spread with respect to community warning and safety are discussed keywords crown fire fine dead fuel moisture content fire behaviour fire prediction fire propagation fire weather fuel type model error 1 introduction early and contemporaneous research into the effects of global warming on wildfire activity have forecasted an increase in the length of the fire seasons and the number of days of extreme fire danger when large fire spread events can occur e g ryan 1991 beer and williams 1995 barbero et al 2020 the events of the last two decades or so have corroborated these hypotheses with an increase in the number of highly destructive wildfires globally tedim et al 2020 associated with many of these events is the substantial loss of human life and property at unprecedented rates teague et al 2010 goldammer et al 2019 gee and anguiano 2020 despite the science of fire behaviour prediction being well established in many parts of the globe scott et al 2014 it is clear that even in countries with a notable investment in fire behaviour research and operational fire intelligence e g situational awareness fire spread forecasting etc fires can still surprise fire management and emergency response agencies and cause a large number of fatalities this is largely due to a combination of factors including the inability of these agencies to forecast a wildfire s propagation rapidly and effectively communicate the potential wildfire threat to the public and take the necessary actions to safeguard human lives wildfires driven by strong winds fig 1 a c that within a few hours of their ignition grow to a large size and quickly impact communities with little or no official warning are often associated with multiple fatalities cruz et al 2012 blanchi et al 2014 brewer and clements 2019 xanthopoulos and athanasiou 2019 for this type of fires many of the methods used by specialised fire behaviour analysts scott et al 2014 neale and may 2018 imbedded in incident management teams or coordination centres to generate fire spread predictions andrews et al 2007 giannaros et al 2019 neale and may 2020 might well fail to meet the necessary urgency associated with such events cruz and alexander 2019 developed a rule of thumb for obtaining a first approximation of a wildfire s forward rate of spread in conifer forests eucalypt forests and shrublands but not in grasslands the rule of thumb states that a wildfire rate of forward spread is approximately 10 of the average 10 m open wind speed the rule was found to be most appropriate for strong wind and dry fuel conditions i e both fine fuel moisture and overall long term landscape dryness associated with fast spreading wildfires where one would consider that time is of the essence for public warnings and evacuation notices the rule of thumb was considered to be appropriate for wildfires spreading over level to gently undulating topography and in cases where large wildfires advance across drainages with alternating upslope and downslope runs under extended drought conditions fuels in mountainous topography are readily able to burn regardless of the variations in terrain characteristics cheney 1981 when winds are suitably strong and accordingly sustained a high intensity wildfire can continue to spread for several hours burning out entire forested drainages and crossing mountain ridges that would normally be a barrier to fire growth rothermel 1991 one such spectacular event occurred on september 6 1988 when the canyon creek fire under the influence of a dry cold front advanced eastward across the continental divide fig 1a and out onto the plains of west central montana usa goens 1990 although the empirical basis of the rule of thumb makes it a reflection of the broad wildfire dataset used in its development no independent evaluation of its performance has been carried out up to now here we examine the robustness of this rule of thumb by evaluating its predictive ability against independent wildfire datasets 2 methods observations of forward rate of fire spread were compiled from two different databases this included that of harris et al 2011 and kilinc et al 2012 for southern australia wildfires and the bonfire global fire behaviour database fernandes et al 2020 the dataset extracted from harris et al 2011 and kilinc et al 2012 was limited to wildfires in eucalypt forests luke and mcarthur 1978 and included detailed information on fire size fuel characteristics and predictions from rate of fire spread models currently used in south eastern australia mcarthur 1967 cheney et al 2012 the dataset extracted from bonfire is broader containing data from wildfires in shrublands eucalypt forests and conifer forests but overall less detailed regarding fire size and fuel characteristics 2 1 southern australia database kilinc et al 2012 assembled a fire behaviour database of australian wildfires occurring in native eucalypt forests this dataset is largely based on the data collated by harris et al 2011 aimed at evaluating the effects of fire weather and fire behaviour on community impacts although some new fires were added to the database by kilinc et al 2012 the combined dataset from harris et al 2011 and kilinc et al 2012 used in the current study is given in table a1 of the appendix 2 1 1 fire spread isochrones and forward rate of fire spread spatial fire propagation data including isochrones of progression at given time intervals were derived from published e g cfa 1999 and unpublished wildfire case studies reports aerial imagery photography and infra red line scans and newspaper articles many of the documented wildfires occurred prior to the 1980s with information regarding the spatial attributes of these fires recorded in the form of paper maps these maps were digitised geometrically rectified and the data then processed through a spatial geographical information system gis database infra red line scans of fire perimeter location at given times obtained from the country fire authority cfa and department of sustainability and environment dse in the state of victoria were used to reconstruct some of the major fires between the years 2000 and 2007 accounts of fire progression typically came from direct observations made by experienced fire suppression personnel as recorded in logbooks and radio logs the database also included more detailed fire reconstruction information from recent wildfire events such as the 2009 black saturday fires teague et al 2010 cruz et al 2012 the forward rate of fire spread measurement r km h 1 for a given time interval is the maximum r for the interval determined as r d t where d represents the maximum distance km from the end point of one fire isochrone to the end point of the preceding isochrone and t is the time period h between isochrones for some bushfires no fire progression maps could be retrieved but r measurements were documented in the form of reports 2 1 2 weather data and fuel moisture content weather data was obtained from the australian bureau of meteorology s automatic weather station aws network records and from reports which present data taken from manual weather stations located closest to the fire incident base data included air temperature relative humidity rainfall history and mean 10 m open wind speed u 10 km h 1 and direction in certain instances weather observations were made some distance from the fire and thus the actual fire conditions may have been significantly different to that indicated by the weather observations for each wildfire spread period considered between two isochrones the corresponding weather data was taken as the average meteorological conditions over the time interval field measurements of fine dead fuel moisture content mc oven dry weight were not available for the wildfires analysed mc was in turn estimated from air temperature and relative humidity using models for eucalypt forests matthews et al 2010 2 1 3 vegetation and fuel data the area enclosed by each wildfire isochrone was classified as either dry eucalypt forest or wet eucalypt forest this information was derived from the original fire reports and state vegetation maps dry eucalypt forests and woodlands typically consist of multi aged stands of a mix of eucalypts and have an understorey dominated by hard leaved shrubs grasses sedges or bracken fern wet eucalypt forests typically consist of a tall eucalypt overstorey of multi aged and mixed species and a dense understorey of ferns soft broad leaved shrubs and small trees for wildfires in victoria spatially averaged fuel characteristics were determined from a database of ecological vegetation classes and associated age related fuel accumulation curves e g walker 1981 allowing fuel characteristics such as load and fuel hazard by understorey component hines et al 2010 to be estimated at a given site for any given year for other states these fuel characteristics were determined from reports and estimated through visual examination of photographs using the hines et al 2010 fuel hazard rating and gould et al 2007 fuel hazard score assessment guides 2 2 bonfire database fernandes et al 2020 surveyed peer reviewed articles grey literature and unpublished data on file to develop a global fire behaviour database encompassing information from experimental fires prescribed fires and wildfires in the current analysis we focused solely on the wildfire component of the database the following information was retrieved for each fire georeferenced location broad vegetation type i e conifer forest eucalypt forest or shrubland dominant species slope steepness observed rate of fire spread fire run duration air temperature relative humidity and u 10 when available fuel complex attributes e g fuel loads per size class and live or dead condition curing or fine dead fuel fuel depth or height fuel layer cover were also extracted given the diversity of sources the dataset is somewhat variable in its completeness as part of the post processing of data mc values were estimated from air temperature and relative humidity using vegetation fuel type specific models this included matthews et al 2010 for eucalypt forest the parameterization of this model for semi arid shrublands given in cruz et al 2010 anderson et al 2015 for temperate shrublands and conifer forest with a prominent shrub layer for other conifer forests we used the rothermel 1983 fuel moisture tables to ensure consistency with the data used by cruz and alexander 2019 for this vegetation fuel type the bonfire data used in the current study is given in table a2 of the appendix 2 3 imposed data constraints and reliability several criteria were imposed on the datasets selected for the analysis to ensure compatibility with the intended use of the 10 rule of thumb data used in the analyses originated from wildfire runs lasting at least 1 0 h but less than 6 0 h wildfires that were affected by frontal passage driven wind changes e g cruz et al 2012 that altered the dynamics of the fire propagation process blanchi et al 2014 were not considered as the rule of thumb is not applicable to these situations cruz and alexander 2019 we also restricted the analyses to wildfires meeting the high wind i e u 10 30 km h 1 and low fuel moisture mc 7 conditions where the rule of thumb has shown to be most applicable cruz and alexander 2019 none of the wildfires used in the original formulation of the 10 rule of thumb alexander and cruz 2006 cheney et al 2012 anderson et al 2015 were considered in the present analyses kilinc et al 2012 and fernandes et al 2020 assigned wildfire data reliability scores for weather fuel and fire behaviour characteristics as per cheney et al 2012 and cruz et al 2012 this is provided in tables a1 and a2 respectively interpretation of the reliability scores is given in table a3 2 4 predicted fire spread rate data we applied the 10 rule of thumb to the u 10 values in both datasets to produce estimates of r for each wildfire we also used the predictions from the mcarthur 1967 and cheney et al 2012 fire spread models in the southern australia dataset to understand how the rule of thumb predictions compare with those of current operational fire spread models under heightened fire spread potential conditions predictions for the mcarthur 1967 model relied on an overall understorey fuel load comprised of surface near surface elevated and bark fuels similar analysis was not pursued for the bonfire dataset due to the absence of the necessary fuel information needed to apply the appropriate fire spread models 2 5 statistics model error predictions were quantified using the following statistics mean absolute error mae mean bias error mbe mean absolute percent error mape root mean square error rmse and the ratio between the mbe and mae which we call the mean bias percent error mbpe willmott 1982 cruz et al 2018 the mae expressed in the same units as the original data is a quantity used to measure how close predictions are to the observed value as the name suggests the mae is an average of the absolute error the mbe describes the dispersion or spread of the residual distribution about the estimate of the mean a positive value indicates an over prediction trend while a negative value is an indication of an under prediction trend the mape is a very popular measure of the accuracy of a predictive model or system it represents the summed differences between the individual predicted versus observed values divided by the observed value and expressed as a percentage if a perfect fit is obtained then the mape is zero the mbpe provides a measure of the bias in relation to the mae expressed as a percentage the rmse represents the standard deviation of the residuals prediction errors residuals are a measure of how far from the line of perfect agreement the data points lie the rmse is a measure of how spread out these residuals are the analysis of the rule of thumb error was conducted separately for the southern australia and bonfire datasets despite certain commonalities namely data from wildfires in australian eucalypt forests the data were not pooled together due to the different data collection methods standards and overall data characteristics we analysed differences in fire environment variables for subsets of datasets through unpaired two sample tests the shapiro wilk test of normality was used to determine if variables were normally distributed for normally distributed variables student t tests were used for non normal distributions the non parametric wilcoxon rank sum test was used all statistical analysis was conducted using the software r r core team 2019 3 results 3 1 southern australia dataset the original southern australia database consisted of 183 fire observations in eucalypt forests the dataset selected for analysis was reduced to 61 wildfire observations after removing fire runs with a duration of less than 1 0 h and cases of post frontal passage type of fire propagation within those a total of 30 fire runs had an estimated mc 7 and a measured u 10 30 km h 1 for this subset the average rate of fire spread was 3 6 km h 1 spanning a range of 0 8 8 0 km h 1 table 1 provides the basic statistics for the subset of data used in this study the 10 rule of thumb predicted the dataset with a mae of 1 75 km h 1 and a mbe of 0 89 km h 1 table 2 fig 2 shows the rule of thumb predictions clustered around two different areas there is a group of fires n 10 propagating with a forward rate of spread less than 2 0 km h 1 that were characterized by a strong over prediction bias the remaining two thirds of the data with higher rates of spread had predictions that mostly fell within the 35 band the wildfires in the over predicted group were slower spreading the shapiro wilk test of normality showed mc u 10 fuel loads and fire front width in each subset of data i e slower spreading wildfires with r 2 0 km h 1 vs faster spreading wildfires with r 2 km h 1 to not be normally distributed the slower spreading wildfires were characterized by higher mc levels mostly 5 6 averaging 5 3 than the dataset average of 4 4 the wilcoxon rank sum test indicated mc to be significantly higher in the slower spreading group than in the faster spreading group p 0 008 the same test found no statistically significant differences for wind speeds and fuel loads in the two groups α 0 05 fire front width was larger in the faster spreading wildfires 4 3 km than in the slower spreading group of fires 1 6 km with the wilcoxon rank sum test finding the differences significant p 0 004 considering only the wildfires spreading with a r 2 0 km h 1 the use of the rule of thumb resulted in an mae of 1 04 km h 1 mape of 22 4 and a mbe of 0 24 km h 1 predictions of rate of fire spread with the mcarthur 1967 and cheney et al 2012 models yielded higher maes 2 12 and 2 19 km h 1 respectively than the rule of thumb albeit showing distinct trends fig 2 the mcarthur 1967 model largely under predicted the spread of the wildfires with an mbe 2 03 km h 1 of similar absolute magnitude to the mae table 2 whereas the cheney et al 2012 model yielded a negligible bias mbe of 0 02 km h 1 both of these models produced more accurate predictions for the slower spreading fires r 2 0 km h 1 but higher errors for the faster spreading ones fig 2b the cheney et al 2012 model over predicted the rate of advance of all wildfires in the slower spreading group fig 2b 3 2 bonfire dataset the bonfire database compiled by fernandes et al 2020 contained a total of 167 wildfire runs in non grass fuels with a run duration 1 0 h not classified as post frontal passage fire propagation nor included in the datasets used to derive the rule of thumb i e alexander and cruz 2006 cheney et al 2012 anderson et al 2015 and not present in the southern australia dataset within that dataset there were a total of 58 fire observations in conifer forests eucalypt forests and shrublands that met the criteria of the mc 7 and u 10 30 km h 1 the basic statistics for this dataset are given in table 3 including the distribution by geographic location the r for these datasets averaged 4 0 km h 1 with the distribution of this variable mc and u 10 not significantly different from the southern australia dataset tukey multiple comparison tests indicated no significant differences between the r in the three broad vegetation fuel types present in the dataset comparable results were obtained for u 10 but significant differences p 0 05 were observed for the mc the application of the 10 rule of thumb resulted in a mae of 1 70 km h 1 table 4 when considering the aggregate of the three vegetation fuel types a value comparable to the one obtained for the southern australia dataset the 10 rule of thumb predicted r values for the dataset with a mbe of 0 49 km h 1 or 29 of the mae table 4 overall the most accurate predictions were obtained for the shrubland subset mae 1 54 km h 1 mape 59 6 followed by the conifer forest subset mae 1 55 km h 1 mape 80 3 and then the eucalypt forest subset mae 1 95 km h 1 mape 89 3 table 4 as with the southern australia dataset the analysis shows a notable over prediction bias for wildfires propagating with rates of spread 2 0 km h 1 fig 3 a large proportion of the fires spreading with an r between 2 0 and 8 0 km h 1 were predicted within the 35 error prediction band the rule of thumb tended to under predict fires spreading with an r 7 5 km h 1 although errors for these fires were around the 35 threshold fig 3 4 discussion 4 1 fire spread prediction error the spread of a wildfire flame front comprises very dynamic phenomenon influenced by a number of variables and processes its prediction in an operational setting over a period of hours to days is fraught with uncertainty associated with the limitations of our understanding of the controlling processes difficulty in accurately estimating a model s input variables compounding effects of errors and the obvious difficulty in describing the chaotic nature of fluids in a turbulent and constantly changing environment albini 1976 catchpole et al 1993 report that measured rates of fire spread in replicated i e same environmental conditions laboratory experimental fires conducted under constant wind speed conditions agreed to within 20 fig 4 a this result can be seen as a benchmark with respect to fire spread variability spreading fires will exhibit notable variability even when burning in a controlled environment under homogeneous and identical conditions outdoor fires but especially wildfires will obviously be characterized by a higher variability given the transient and dynamic nature of boundary layer meteorology plus the spatial variability in fuel characteristics e g structure moisture and terrain e g slope steepness aspect or slope exposure the most accurate results obtained with empirical models in a field setting are for model predictions against the datasets used in their development with mapes varying between 20 and 35 cheney et al 1998 2012 fernandes 2001 fig 4b prediction errors naturally increase when the models are applied to independent experimental fire fig 4c or wildfire fig 4d datasets in these cases the mape has been characterized by an interquartile range varying from 50 to 70 for experimental fire datasets cruz and alexander 2013 and from 54 to 120 for wildfire data alexander and cruz 2006 cheney et al 2012 kilinc et al 2012 anderson et al 2015 the main reason for the error increase in the latter dataset is due to the large uncertainty in spread rate measurements model inputs and the natural unaccounted variability in the fire environment associated with large wildfire runs cruz and alexander 2013 2019 coen et al 2018 considering the spread of fires under critical fire weather conditions our analysis was based on a broader dataset n 88 than the original work by cruz and alexander 2019 that included only 24 wildfire runs out of 118 observations within the constraints of mc 7 and u 10 30 km h 1 the results obtained by the simple 10 rate of spread rule of thumb as analysed in the present study with the mape varying between 60 and 101 tables 2 and 4 is first and foremost indicative of the strong control wind speed alone exerts on landscape scale fire propagation under dry fuel conditions this despite the uncertainty in the use of measured wind speed data as being representative of the conditions driving a wildfire some distance away from the measurement location as observed in the analysis of cruz and alexander 2019 the percent error associated with the rule of thumb predictions decreases substantially with the increase in r the mape varied between 22 and 34 when considering wildfires that were propagating with an r 2 0 km h 1 the wildfires that are most dangerous from the point of view of community and fire fighter safety this error is on par with the error obtained by empirical based fire spread models when assessed against their original datasets with in situ accurate measurements of the fire environment e g cheney et al 1998 fernandes 2001 the results presented in figs 2 and 3 raise the question of why does the 10 rule of thumb work so well under certain conditions and not so well in others the analysis showed a substantial over prediction for wildfires observed to spread with an r 2 0 km h 1 independent of the dataset a similar over prediction bias for wildfires in this range of observed rate of spread was noted in the original analysis by cruz and alexander 2019 in the southern australia related dataset this bias was related to fireline width and time of day mostly early to mid afternoon fire runs this hints at the notion that early afternoon conditions reflect the fact that longer timelag fuels nelson 2001 are not yet fully available for combustion and the possible impact of effective suppression plucinski 2019a 2019b in some areas around a wildfire s perimeter could explain the smaller fireline width and the lower observed rates of fire spread relative to the 10 rule of thumb expectation unfortunately the bonfire dataset did not have the detail necessary to further explore this issue these results highlight some of the limitations of the 10 rule of thumb namely that its best accuracy might be restricted to lower mc levels than proposed by cruz and alexander 2019 in their analysis it was suggested that the rule of thumb worked best below an mc of 7 while the current analysis with a broader dataset indicates that the most accurate results are obtained with a mc level up to 5 the analysis of the bonfire related dataset identified an under prediction trend for wildfires spreading with an r 7 5 km h 1 there were eight fires in this group with seven under predictions six of them with under predictions varying between 29 and 38 and one of them with a 55 a closer review of these wildfires reveals a prevalence of documented long range spotting distances in most of them namely the 1983 deans marsh fire 10 km spotting rawson et al 1983 and the east trentham fire 10 12 km rawson et al 1983 storey et al 2020b in victoria australia the 1936 galatea creek fire in alberta canada 5 6 km fryer and johnson 1988 and the 1983 mount muirhead fire in south australia 15 20 km keeves and douglas 1983 long range spotting is a highly stochastic process linked to a number of variables such as the size of the active fire area fuel type s terrain roughness wind speed levels and wind exposure page et al 2018 storey et al 2020a the processes are heuristically understood with long range spotting associated with wind driven wildfires typically linked to fires accelerating in wind exposed upslope runs this results in localised increases in energy release an increase in the number of firebrands generated and pulses in upward momentum in a wildfire s plume kerr et al 1971 luke and mcarthur 1978 mccarthy et al 2018 these periodic pulses are able to transport firebrands higher into the wildfire s plume where upper levels winds can then maximise their downwind transport albini et al 2012 however not all upslope fire runs will lead to the long range spotting as observed in the wildfires mentioned above the results obtained seem to indicate that if the fire environment is conducive to the occurrence of long range spotting distances then the 10 rule of thumb will likely under predict the overall fire spread distance in our evaluation the level of under prediction was between 30 and 40 which is fairly acceptable given the uncertainty in the input variables and the stochasticity of the process although higher under prediction errors cannot be ruled out the fact that the 10 rule of thumb predicted well wildfires driven by winds of 60 70 km h 1 with gusts up to around 100 km h 1 and characterized by long range spotting with errors up to 40 is in itself a very interesting result 4 2 fire spread prediction error operational implications as simulations of fire spread models are used to support decision making during ongoing wildfires it is important that the users of such information understand the uncertainty in fire spread predictions either due to errors associated with inaccurate inputs or model limitations albini 1976 despite the complexity of wildfire phenomena the unknowns in our scientific understanding of fire behaviour and the chaos associated with a wildfire approaching the wui to the individuals making decisions regarding the safety of communities or fire fighters in the field the questions are very simple where is the wildfire at the moment and what is its rate of spread and intensity luke and mcarthur 1978 will the wildfire reach a particular community or pre defined trigger point and at what time will it do so cova et al 2005 ramirez et al 2019 the decision maker will need to understand the uncertainty and potential bias associated with the fire spread prediction to make better decisions and tailor the actions to be taken fig 5 summarises the impact of a rate of spread prediction error on a hypothetical example of a wildfire starting 6 0 km upwind of a community the example assumes that 0 5 h after ignition the wildfire is spreading at 3 0 km h 1 for the sake of simplicity the wildfire is assumed to have been detected the moment following ignition and that in the early stages of fire propagation i e during its build up phase the fire was spreading at the pseudo steady state rate of spread luke and mcarthur 1978 the example considers the wildfire to impact the community 2 0 h after ignition and that a fire behaviour analyst produced a forecast of fire spread 0 5 h after the ignition was detected and the local authorities act upon this information to immediately release a warning to the general public if a correct r prediction of 3 0 km h 1 is made i e 0 error and an evacuation warning is issued then the population has 1 5 h to act before the wildfire impacts the community over predictions will result in a reduction in the perceived time to impact i e the community will think that they have less time to act than in reality with an increase in an over prediction the predicted time to impact decreases to a level that might lead to a negative outcome area a in fig 5 e g emergency services decide that the time for a community to safely evacuate is too short and thus do not issue an evacuation warning this is clear for the 100 over prediction error a r prediction of 6 0 km h 1 where a predicted time to impact of 0 5 h might lead to a change from evacuation to a shelter in place warning such advice against evacuation when time does allow for its safe implementation can potentially put members of the general public at undue risk under prediction biases of wildfire rate of spread can also have a detrimental but distinct impact on the decision making process under predictions will result in an erroneous over estimate of the time to fire impact potentially removing the necessary sense of urgency area b in fig 5 for example the largest under prediction in fig 5 suggesting 6 0 h to impact might delay any warnings to the general public during a critical time period considering 1 0 h as a time scale relevant for community evacuation li et al 2019 errors up to 33 are not likely to have a detrimental impact on public safety it is errors above this threshold and in particular under prediction errors that can result in a lack of timely and appropriate warnings leading to the most detrimental consequences cheney 1981 teague et al 2010 4 3 performance of the 10 rule of thumb against five notable recent wildfire disasters the recent past is populated with some of the deadliest single wildfire events on record box 1 common features of these wildfires were for example a new ignition starting the deadly fire run with the exception of the 2017 arganil seia fire strong winds leading to fast spread rates impact into communities within a few hours of their ignition and communities not warned of the impending danger until it was too late to evacuate to safe areas e g xanthopoulos and athanasiou 2019 for most of these wildfires the lack of a formal warning to the communities prior to fire s impact was due in part to the lack of situational awareness and an under appreciation of the fire spread rate potential by civil protection emergency response agencies teague et al 2010 goldammer et al 2019 in complementing the analyses presented in the previous sections table 5 provides some additional details on the characteristics of the wildfires listed in box 1 a feature of the measured wind data reported in table 5 note the different standards for the measurement height above ground for these wildfires is the broad range in the average wind speeds for each of the fires fig 6 winds were measured at different locations near the vicinity of these wildfires or within the final fire perimeter weather stations situated on ridgelines or on windward slopes generally resulted in the upper range in wind speeds whereas stations located at lower elevations where the impacted communities typically were yielded a lower range in wind speeds these results highlight some of the inherent issues of predicting the spread of wildfires advancing across complex topography detailed wind studies such as those carried out by coen et al 2018 brewer and clements 2019 and lagouvardos et al 2019 for example further highlight the spatial heterogeneity in landscape scale winds and the complexities associated with the strong wind events linked to some of the catastrophic wildfires described in box 1 and table 5 this also calls attention to the fact that the wind speed data given in tables a1 and a2 should be seen as indicative not necessarily as a precise value representative of a large wildfire run the 10 rule of thumb yielded estimates that approximate the observed rates of spread of the wildfire disasters fig 6 the wind speed bar can be read as the 10 rule of thumb r prediction in the rate of fire spread axis label the application of the rule of thumb under predicted the spread rate for the 2009 kilmore east fire 26 error when considering the maximum wind speed and the overall rate of fire spread a result linked to the occurrence of long range spotting during this fire s major run cruz et al 2012 and over predicted the maximum spread rate for the 2017 tubbs fire 35 error which are clearly errors within an acceptable range for wildfire propagation prediction errors for the other wildfires contained in table 5 were smaller with the range in the predicted rate of spread based on the range in observed wind speeds overlapping the range in observed rate of fire spread fig 6 we need to emphasize that these results should be seen as qualitative and only as an illustration of the usefulness of the 10 rule of thumb as a first approximation in situations where there is no particular fire behaviour prediction know how or there is no time to apply more comprehensive and accepted fire behaviour prediction methods e g rothermel 1983 1991 plucinski et al 2017 taylor and alexander 2018 the quick usage of the rule of thumb in these situations when time is of the essence leaves more time for undertaking other time critical actions including informing the general population the strong control that wind speed exerts on the propagation of wildfire conflagrations as shown in the analysis of wildfire case studies highlights the importance of accurate wind forecasting coen et al 2018 lac et al 2018 lagouvardos et al 2019 and its fine scale spatial modelling wagenbrenner et al 2016 filippi et al 2018 especially in complex terrain to guide effective decision making and issuing of warnings during extreme burning conditions 4 4 on the wind speed effect on fire propagation the error analysis resulting from the evaluation of the 10 rule of thumb against independent data has provided additional clues as to the influence of wind speed on wildfire propagation rates we noted in section 4 2 that a group of the slower spreading fires i e r 2 0 km h 1 were found to be over predicted by the 10 rule of thumb although we did not apply the 10 rule to fires spreading with u 10 levels 30 km h 1 or mc 7 in the present study the analysis by cruz and alexander 2019 found an over prediction bias for wildfires spreading under these conditions in contrast the low percent errors produced for wildfires with observed r levels 2 0 km h 1 hints at the overarching control wind speed exerts on the forward speed of wildfires when fine dead fuels are critically dry and wind speeds are strong the fit statistics do not leave much room for other influences be it a forest or shrubland vegetation fuel type topographic effects or fire atmosphere interactions when wildfires are spreading under these conditions potter 2002 suggested that under strong wind conditions the convective plume tilt of a wildfire along with the transport of the plume condensation area downwind will lead to a decoupling between the advancing flame front of the fire at the surface and the plume above this will limit dynamic feedbacks such as downdrafts or return flows on the advancing flame front that may arise from the moisture latent heat release in the fire s plume in these situations wind speed and fuel dryness control a wildfire s spread rate the observed over prediction for the group of slower spreading wildfires mentioned above could arise from fire atmosphere interactions in situations with lower wind speeds the fire s convection column is more vertical than in the strong wind case and coupling between the upper levels of the plume and the surface can occur byram 1959 rothermel 1991 air entrainment due to vertical plume development in distinct layers of the atmosphere coupled with moisture condensation will feedback into a free burning fire as the vertical motions change the near fire surface winds potter 2002 these feedbacks can result in periodic or occasional strong downdrafts winds that can cause sudden changes in fire behaviour potter 2005 coen 2011 occasionally with possible life threatening consequences e g rothermel 1991 goens and andrews 1998 despite the occasional extreme flows associated with dynamic feedbacks the strong entrainment into the fire s plume and upward motions will when considering the time scales used in our study i e greater than 1 h result in overall lower horizontal winds at the surface this might possibly explain in part the observed fire spread rates being lower than expected based on the rule of thumb 5 conclusions we conducted an examination of the predictive ability of the cruz and alexander 2019 10 rule of thumb to estimate a wildfire s forward rate of fire spread using two independent datasets this simple rule of thumb aims to provide first approximations of wildfire propagation for situations where there is little or no time to apply more comprehensive and accepted fire behaviour prediction methods the rule of thumb was shown to work well with overall mapes between 80 and 100 comparable to other model evaluation studies based on wildfire data the analysis showed the rule of thumb to work best for fast spreading wildfires r 2 0 km h 1 for these cases the mape varied between 22 and 34 a result on par with error statistics obtained when evaluating empirical based fire spread models against the data used in their development our analysis showed that the range of conditions where the rule of thumb worked best is possibly more restrictive than originally thought we found the rule to be most reliable under strong wind u 10 30 km h 1 and dry fine fuel mc 5 rather than 7 conditions typically associated with fast spreading wildfires it is these types of fires that can surprise communities and emergency response agencies due to their high potential spread rates the 10 rule of thumb is relevant when landscape scale dryness is conducive to major wildfire outbreaks i e low moisture content levels for live fuels and dead fuels with long timelag response e g deep duff layers and large diameter coarse woody debris although the focus of the present work was to evaluate the 10 rule of thumb against wildfire data the analysis also provided insight into fundamental properties of fires burning under high fire spread potential despite the uncertainty and variability in the data the trends are clear wind speed has an overwhelmingly dominant effect on the spread rate of wildfires when fuels are dry and the wind is strong insight into the processes and variables with strong influence on large scale fire propagation can only arise from the analysis of wildfire data rather than experimental fires field or laboratory or relying solely on simulation modelling recent research into spot fire controls page et al 2018 storey et al 2020a rate of fire spread in mountain pine beetle killed stands perrakis et al 2014 and whole fire atmosphere processes dowdy et al 2017 brewer and clements 2019 mccarthy et al 2019 are examples of new insights into fire dynamics based on wildfire data that can help us better understand wildfire propagation and guide future fire behaviour modelling efforts advances and deployment of remote sensing technology better mapping of vegetation and its structure at landscape scales and more accurate measurement of weather variables have created opportunities to reduce the uncertainty associated with the quantification of wildfire propagation for research studies these data sourcing techniques will in turn lead to improvements of our understanding of fire dynamics model calibration and more accurate forecasting of wildfire propagation on a concluding note the evaluation of a fire behaviour model or guide constitutes a continuing practice watts 1987 in this regard we plan to continue to add to the southern australian and bonfire databases which will allow for the periodic evaluation of the 10 rule of thumb and other fire spread models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements pmf and âs contribution was carried out in the framework of the uidb 04033 2020 project funded by the portuguese foundation for science and technology fct and the bonfire ptdc aag maa 2656 2014 project funded by fct and the european regional development fund erdf through compete 2020 operational program for competitiveness and internationalization poci âs received support from the portuguese foundation for science and technology fct through ph d grant sfrh bd 132838 2017 funded by the ministry of science technology and higher education and by the european social fund operational program human capital within the 2014 2020 eu strategic framework the authors are indebted to andrew sullivan and matt plucinski csiro australia craig clements and matthew brewer san jose state university usa and stuart matthews nsw rfs australia for their helpful comments on earlier versions of this paper the assistance of gavril xanthopoulos and miltiadis athanasiou hellenic agricultural organization demeter greece and kostas lagouvardos national observatory of athens greece in the details surrounding the 2018 mati fire is hereby appreciated appendix table a1 date fire name fire run time interval weather conditions fuel loads fire behaviour characteristics and reliability scores for the southern australia wildfire dataset harris et al 2011 kilinc et al 2012 the weather data and in particular u 10 should be seen as indicative not a precise value for the fire run table a1 calendar date fire name fire spread time interval t c rh mc u 10 km h 1 r km h 1 fire width km surface fuel load kg m 2 total fuel load kg m 2 reliability weather fuel r a january 16 1962 daylesford 21 30 22 30 33 17 4 2 37 1 2 1 6 1 4 2 1 3 3 3 april 4 1978 gervasse 16 00 17 30 28 29 5 8 100 8 3 1 0 4 0 4 3 3 3 april 4 1978 maranup ford 11 00 12 00 28 29 5 8 40 5 4 2 0 0 4 0 4 3 3 3 february 16 1983 otways 16 00 17 00 41 4 2 5 44 4 5 3 7 1 7 3 0 3 3 4 february 16 1983 otways 17 00 18 00 41 5 2 6 44 3 7 5 6 1 6 3 0 3 3 4 february 16 1983 otways 18 00 19 30 41 5 2 6 39 5 10 0 1 6 3 0 3 3 4 february 16 1983 otways 20 00 21 00 41 5 2 6 47 3 7 11 3 1 1 2 4 3 3 4 february 16 1983 cockatoo 20 00 21 05 41 5 2 6 47 0 8 0 4 1 1 1 8 2 3 2 february 16 1983 mt lofty 13 50 15 10 40 10 3 3 41 3 4 1 4 0 7 1 2 3 4 3 january 14 1985 anakie 14 40 15 40 42 7 2 9 37 5 9 4 4 0 7 1 1 2 2 2 january 8 1994 springwood 13 00 15 30 34 20 4 6 41 3 1 1 2 0 2 7 3 4 5 january 8 1994 springwood 16 00 17 30 36 18 4 3 43 2 8 3 3 2 0 2 7 3 4 5 december 2 1998 linton 13 00 14 00 28 26 5 5 41 0 8 0 4 1 8 3 0 2 3 2 december 2 1998 linton 16 15 18 00 29 23 5 1 30 0 9 1 0 1 7 2 8 2 3 2 march 12 2006 riley road 15 00 17 39 36 9 3 2 33 2 5 1 1 1 7 2 8 3 3 3 december 14 2006 coopers creek 15 00 16 00 35 9 3 2 48 6 2 7 1 8 2 9 3 3 3 december 14 2006 coopers creek 16 00 18 00 27 25 5 4 46 6 1 14 5 1 8 2 9 3 3 3 january 21 2006 century track 16 30 18 30 41 18 4 2 39 4 4 2 8 1 7 2 8 3 3 3 february 7 2009 bunyip 13 00 14 00 43 10 3 2 46 4 7 2 0 1 0 2 3 2 3 3 february 7 2009 kilmore east 17 00 18 00 41 10 3 2 46 5 8 6 9 1 3 2 4 2 3 3 february 7 2009 white timber spur 13 30 15 00 25 31 6 1 61 1 7 1 1 1 5 1 9 3 3 3 february 7 2009 white timber spur 15 00 16 30 27 26 5 5 55 1 6 1 4 1 4 1 8 3 3 3 february 7 2009 white timber spur 16 30 17 30 27 24 5 2 46 6 7 1 8 1 4 1 8 3 3 3 february 7 2009 white timber spur 17 30 18 30 27 25 5 4 41 5 1 3 1 1 3 1 6 3 3 3 february 7 2009 white timber spur 18 30 19 30 26 26 5 5 52 4 9 3 9 1 2 1 6 3 3 3 february 7 2009 white timber spur 19 30 20 30 25 27 5 6 50 1 9 4 3 1 1 1 5 3 3 3 february 7 2009 white timber spur 20 30 22 00 25 29 5 9 48 1 4 5 2 1 1 1 5 3 3 3 february 6 2011 roleystone kelmscott 13 30 14 30 23 31 6 2 43 1 7 0 2 1 4 1 8 2 3 5 february 6 2011 roleystone kelmscott 14 30 15 30 24 29 5 9 39 1 4 0 6 1 4 1 8 2 3 5 february 1 2011 tostaree 13 00 16 00 39 13 3 6 31 3 8 1 5 1 9 3 0 3 3 3 asee table a3 table a2 date fire name and country vegetation type weather conditions rate of fire spread reliability scores and fuel loads for the bonfire dataset fernandes et al 2020 the weather data and in particular u 10 should be seen as indicative not a precise value for the fire run table a2 calendar date dd mm yyyy fire name country vegetation fuel type t c rh mc u 10 km h 1 r km h 1 fine fuel load kg m 2 reliability weather fuel r a source 11 8 2006 serra da ossa portugal forest eucalypt 34 15 4 0 37 2 5 1 5 2 2 2 unpublished data on file with p m fernandes 16 2 1983 belgrave australia forest eucalypt 42 5 2 6 52 6 6 1 9 2 3 2 keeves and douglas 1983 16 2 1983 belgrave australia forest eucalypt 41 7 2 9 50 3 5 1 4 2 3 2 keeves and douglas 1983 17 11 1962 longford australia forest eucalypt 32 7 17 4 3 48 1 4 1 2 1 3 2 mcarthur 1965 15 11 2002 redmond australia forest eucalypt 35 13 3 5 50 6 0 8 5 2 3 3 mccaw 2003 16 2 1983 east trentham macedon australia forest eucalypt 41 5 3 5 44 4 3 2 5 2 3 4 rawson et al 1983 16 2 1983 east trentham macedon australia forest eucalypt 41 7 3 5 70 11 2 2 2 2 3 4 rawson et al 1983 16 2 1983 east trentham macedon australia forest eucalypt 41 7 3 5 47 6 0 1 5 2 3 4 rawson et al 1983 16 2 1983 east trentham macedon australia forest eucalypt 41 7 3 5 33 1 3 2 3 2 3 4 rawson et al 1983 20 12 1974 rocky gully australia forest eucalypt 40 10 3 0 70 6 4 1 5 4 3 3 underwood et al 1985 4 4 1978 brunswick australia forest eucalypt na na 4 0 52 8 0 na 3 3 3 underwood et al 1985 16 2 1983 deans marsh australia forest eucalypt 40 11 3 0 70 10 0 na 2 4 3 rawson et al 1983 29 1 2009 delburn australia forest eucalypt 44 5 10 3 2 42 2 2 1 5 2 3 3 harris et al 2011 7 2 2009 maiden gully australia forest eucalypt 44 9 7 2 8 41 1 9 1 0 2 3 3 harris et al 2011 8 3 1990 millbrook road australia forest eucalypt 31 5 12 3 7 30 1 5 na 2 3 3 pratt 1990 20 1 1988 blackjack australia forest eucalypt 29 18 4 4 30 1 5 na 1 2 1 bartlett 1993 14 1 1962 dandenongs australia forest eucalypt 39 2 12 3 5 39 1 2 1 4 3 3 3 harris et al 2011 8 1 1969 maldon australia forest eucalypt 37 1 6 2 8 37 0 8 0 3 3 3 3 harris et al 2011 13 1 1939 black friday australia forest eucalypt 44 6 9 3 0 56 1 6 1 6 5 3 3 harris et al 2011 13 1 1939 colac australia forest eucalypt 42 2 9 3 1 56 4 9 1 3 3 3 3 sullivan 2004 13 1 1939 kyneton australia forest eucalypt 42 2 9 3 1 56 5 2 1 4 3 3 3 sullivan 2004 13 1 1939 tawong australia forest eucalypt 46 13 3 5 56 7 9 1 1 3 3 3 sullivan 2004 18 01 2003 mt stromlo australia forest conifer 37 10 6 0 37 3 5 na 1 1 1 gellie 2005 31 7 2001 las palomas spain forest conifer 28 21 4 0 57 2 6 0 7 1 1 1 rodríguez y silva and molina martinez 2012 2 2 1979 caroline australia forest conifer 37 17 6 0 46 4 8 2 5 2 3 3 billing 1980 17 6 2017 pedrogão grande portugal forest conifer 31 5 34 7 0 30 2 3 1 7 2 3 2 guerreiro et al 2017 26 2 1995 berwick forest new zealand forest conifer 34 5 10 6 0 30 0 5 3 0 1 3 1 fogarty et al 1997 26 2 1995 berwick forest new zealand forest conifer 33 5 14 6 0 32 0 9 3 0 1 3 1 fogarty et al 1997 26 2 1995 berwick forest new zealand forest conifer 33 16 6 0 33 2 3 3 0 1 3 1 fogarty et al 1997 21 7 2009 horta de sant joan spain forest conifer 38 11 6 0 62 4 9 na 2 3 1 graf 2010 16 2 1983 narraweena australia forest conifer 40 10 6 0 80 8 0 na 2 3 3 keeves and douglas 1983 16 2 1983 mount muirhead australia forest conifer 40 10 6 0 80 12 5 na 2 3 3 keeves and douglas 1983 6 9 1988 canyon creek usa forest conifer 27 15 6 0 55 6 2 2 8 4 3 2 goens 1990 bushey 1991 ward et al 1994 20 10 1991 east bay usa forest conifer 32 2 17 6 0 37 1 5 na 1 2 1 alexander 2002 nfpa 1992 15 10 2017 mata nacional de leiria portugal forest conifer 32 1 19 6 0 37 4 7 1 3 2 2 2 guerreiro et al 2018 3 8 1936 galatea creek canada forest conifer na na 7 0 55 7 8 na 2 3 1 fryer and johnson 1988 8 5 1987 wallace lake canada forest conifer 28 12 6 0 30 3 9 na 4 3 1 hirsch 1988 27 9 1994 beerburrum australia forest conifer 36 6 12 6 0 50 3 6 na 1 3 1 hunt et al 1995 6 11 1994 beerburrum australia forest conifer 24 14 6 0 38 1 6 na 1 3 1 hunt et al 1995 10 1 1987 lago puelo argentina forest conifer 30 20 7 0 30 1 7 na 4 2 2 sagarzazu and defossé 2009 10 1 1987 lago puelo argentina forest conifer 30 20 7 0 50 4 3 na 4 2 2 sagarzazu and defossé 2009 23 7 2018 mati greece forest conifer 38 17 6 0 44 2 6 na 2 na 2 xanthopoulos et al 2018 10 1 1987 lago puelo argentina forest conifer 30 20 7 0 45 2 3 na 4 2 2 sagarzazu and defossé 2009 27 07 2007 obejo spain shrubland 39 11 4 5 36 3 1 2 8 1 1 1 rodríguez y silva and molina martinez 2012 31 07 2001 sierra parda spain shrubland 33 18 5 9 33 4 1 2 8 1 1 1 rodríguez y silva and molina martinez 2012 18 07 2012 tavira portugal shrubland 21 24 6 0 40 2 0 1 7 2 3 2 viegas et al 2012 21 12 1989 fitzgerald river np australia shrubland 33 12 4 0 42 1 9 na 2 5 3 mccaw et al 1992 21 12 1989 fitzgerald river np australia shrubland 35 9 3 5 43 3 0 na 2 5 3 mccaw et al 1992 21 12 1989 fitzgerald river np australia shrubland 35 9 2 0 34 7 5 na 2 5 3 mccaw et al 1992 21 12 1989 fitzgerald river np australia shrubland 35 9 3 0 43 4 8 na 2 5 3 mccaw et al 1992 9 7 2013 picões portugal shrubland 34 15 5 5 35 4 0 na 2 3 2 viegas et al 2013 15 10 2017 mata nacional de leiria portugal shrubland 29 9 21 4 3 40 6 5 1 7 2 2 2 guerreiro et al 2018 15 10 2017 relva velha portugal shrubland 32 9 16 8 5 8 35 4 5 1 5 2 3 2 guerreiro et al 2018 19 9 2010 machine gun usa shrubland 32 6 2 9 37 2 6 na 2 3 1 frost 2015 22 12 1980 dimboola australia shrubland 35 7 16 6 1 40 2 0 1 0 3 3 3 harris et al 2011 7 1 1979 epuyn lake argentina shrubland 30 20 6 6 39 1 1 na 4 2 2 sagarzazu and defossé 2009 9 10 2017 tubbs usa shrubland 32 8 7 4 5 73 6 5 na 2 3 2 coen et al 2018 nauslar et al 2018 5 8 2018 perna da negra portugal shrubland 24 8 14 6 1 34 2 4 2 2 3 3 3 rego et al 2019 a see table a3 table a3 reliability rating for weather fuel and fire spread observations for wildfire case studies adapted from cheney et al 2012 and cruz et al 2012 table a3 rating weather fuel complex rate of spread 1 nearby 25 km meteorological station or direct measurements in the field with high quality instruments and or validated modelled wind field fuel characteristics inferred from a fuel age function developed for the particular fuel type and area direct timing of fire spread measurements i e infrared scans aerial observations observed reference points with photographs 2 meteorological station within 50 km of the fire with no local effects i e terrain vegetation on the wind field and or partially validated modelled wind field fuel characteristics inferred from a visual assessment or measurements of nearby unburnt forest reliable timing within 15 min of fire spread by field observations with general reference points 3 meteorological station within 50 km of the fire but there are local effects on the wind field or the data not representative of the fire area meteorological station 50 km of the fire reconstruction of wind speed for fire site unvalidated modelled wind field fuel characteristics inferred from a fuel age curve for a forest type of similar structure reconstruction of fire spread with numerous cross references 4 spot meteorological observation near the fire fuel characteristics typical of equilibrium level in the representative fuel type doubtful reconstruction of fire spread 5 distant meteorological observations at locations very different to fire site qualitative fuel type description anecdotal or conflicting reports of fire spread 
25963,in model studies a careful consideration of uncertainty is needed parameter and data uncertainty can be explored by sampling but model structure uncertainty is more challenging to capture since the underlying hypotheses of many models are not directly clear this study explores whether model structure can be inferred from model output we created a dendrogram family tree based on model structure using a modular modelling framework subsequently we created dendrograms based on model output we determined the correlation between both dendrograms to analysed if model structure families could be inferred from model output results from this experiment over 671 climate instances showed that the performance of the inference depends on the type of output evaluated and the climate however the performance of the inference is overall low implying that model structure cannot be inferred from model output these results demonstrate the need to further investigate opportunities to sample model space keywords model structure uncertainty modular modelling framework model genealogy signatures dendrogram sampling model space data availability the camels data set is publicly available it is provided by ncar doi 10 5065 d6mw2f4d the r package for fuse is also publicly available doi 10 5281 zenodo 14005 vitolo et al 2015 the r scripts for our results and analysis are available on request 1 introduction earth and environmental models are inevitably subject to uncertainty oreskes et al 1994 it is the challenging task for modellers to provide projections as accurate as possible while acknowledging that some uncertainty is irreducible weigel et al 2010 careful consideration of the uncertainty is needed to investigate if the model conclusions are robust not only because of socio economic impacts jakeman et al 2006 mcmillan et al 2018 uusitalo et al 2015 but also to enhance process understanding butts et al 2004 clark et al 2008 deser et al 2012 generally three main types of uncertainty are distinguished data uncertainty model structure uncertainty and parameter uncertainty which all consequently lead to output uncertainty bastin et al 2013 refsgaard et al 2007 dependent on the research field research question and the modelled location certain types of uncertainty will prevail deser et al 2012 for instance showed that uncertainty in the initial conditions of climate models has large influence on the final simulations clark et al 2008 showed for a hydrological application that in an arid catchment hydrological model structure has more influence on the simulations than in a wet catchment newman et al 2015 found that errors in the forcing data specifically precipitation and snow melt data could explain outliers in model performance in their tested catchments demonstrating the influence of input data uncertainty therefore a thorough estimation and quantification of all sources of uncertainty is required butts et al 2004 parameter uncertainty and data uncertainty can to a large extent be quantified rather straightforwardly by a combination of intelligent sampling and using brute computer force to create large ensembles quantification of model structure uncertainty is more challenging implementing different off the shelve models can be costly in terms of time and effort addor and melsen 2019 jakeman et al 2006 but also because of the question how do you sample model space theoretically model space could represent all plausible hypotheses of how a system could function if it was possible to sample models in model hypothesis space this would be congruent with the current method to treat parameter uncertainty but it is yet unknown how many plausible hypotheses potentially exist a more thorough understanding of which part of model space is sampled in a study is highly valuable to better understand which part of model structure uncertainty is captured to identify a range of different models that represent parts of model space and eventually to be able to select the most appropriate model for a particular research question a currently frequently used approach to capture model structural uncertainty is to employ model ensembles to sample model space abramowitz and gupta 2008 knutti et al 2013 these ensembles however can be biased when certain parts of the model structure are over represented abramowitz and gupta 2008 weigel et al 2010 since the individual models were not designed from the central idea that all uncertainty in model structure should be captured in other words it is unknown if the space of possible models has been sampled correctly refsgaard et al 2007 for instance in hydrology weiler and beven 2015 observed that a plethora of hydrological models exists which can be used to estimate model structure uncertainty but many hydrological models have very similar structures thereby providing an incomplete picture of the hydrological model space and thus of model structure uncertainty an additional challenge is that many decisions related to model implementation and model structure are not explicitly communicated babel et al 2019 knutti et al 2013 melsen et al 2019 renard et al 2010 wagener and kollat 2007 this hampers the identification of bias in model ensembles correctly sampling model space is relevant to estimate uncertainty and to identify which regions are explored but challenging because of biases in ensembles and the lack of transparency of underlying hypotheses and assumptions in models to tackle bias in the model ensemble model classification schemes could be employed past attempts for model classification in hydrology have not been numerous or extensive plate 2009 made a classification for flood management models the goal of this study however was to provide guidelines when to use which model not to provide guidance on how to capture structural uncertainty jajarmizadeh et al 2012 discuss several classifications of hydrological models these classifications are however all based on certain modelling approaches e g lumped versus distributed rather than on the underlying hypotheses to describe different processes furthermore jajarmizadeh et al 2012 acknowledged that their classification would probably depend on the person who made the classification mueller et al 2011 produces a more extensive evaluation and classification of global land evapotranspiration products from a range of different model types such approaches however are not common practice in hydrology on top of that the data and models are closely interlinked in the product in hydrology a sampling method to consistently relate model ensembles to quantifiable uncertainty is yet to be developed nearing and gupta 2018 an alternative to model classification is to evaluate model structures based on model output knutti et al 2013 and masson and knutti 2011 for instance used hierarchical clustering to show how the climate model intercomparison project 5 cmip5 generation of climate models are related hierarchical clustering is a method widely employed in biology that can be visualised in the form of dendrograms baake 1998 weinberger et al 2006 because of the use of dendrograms this methods is referred to as model genealogy for the ipcc climate projections based on the cmip5 ensemble it is assumed that the range of models correctly represents model structure uncertainty but the dendrograms of knutti et al 2013 and of masson and knutti 2011 show that model similarities can bias the output the cmip5 model genealogy based on model output however was not validated with the actual model code this is practically impossible with the highly complex cmip5 climate models therefore the question with this approach is if model output correctly reflects model structure can model structure families be inferred from model output modular modelling frameworks mmfs provide an excellent opportunity to explore this question mmfs are master templates for model generation they can provide an experimental set up to determine which component in model structure results in a certain difference in model output by having the ability to change the model components individually several mmfs are available within the field of hydrology such as mms leavesley et al 1996 superflex fenicia et al 2011 fuse clark et al 2008 summa clark et al 2015 and raven craig et al 2020 the mmf approach enables us to explore the question whether bias in model ensembles can be determined based on model output whereas in earlier studies the lack of detailed knowledge on model structure and implementation hampered the estimation of model structure bias directly mmfs provide a transparent way to assemble an ensemble of model structures of which their familiarity and similarity can be determined directly based on model code therefore by using mmfs we can investigate if model structure bias in an ensemble indeed can be determined based on model output similar to the approach by knutti et al 2013 in this study we investigate if the model genealogy approach determining model families based on model output is compatible with model families based on model structure defined using a hydrological mmf 2 methods we will employ the framework for understanding structural errors fuse clark et al 2008 to create an ensemble of models for which we can construct a dendrogram based on the model structure subsequently we run the ensemble and create a dendrogram based on the model output by comparing both dendrograms we can investigate if model families can be inferred from model outputs for a set of hydrological models using a large sample data set spanning a range of climate conditions camels newman et al 2015 addor et al 2017 allows to investigate a more general pattern in this inference finally we link the results to climate to explain the quality of the inference three climate characteristics based on knoben et al 2018 were considered aridity seasonality and snow fraction 2 1 model structures the first step was to select and classify several model structures the use of an mmf was crucial in order to have full control over changes in the model structure we selected the mmf fuse clark et al 2008 as implemented in r by vitolo et al 2015 to configure eight different model structures these eight models were chosen to sample a wide range within fuse and to create three distinguishably different families fuse was developed to analyse the difference in model simulations occurring due to different but equally probable representations of the real processes fuse only offers models based on the bucket based modelling philosophy thereby representing only a small part of all possible modelling hypotheses this mmf was selected because the model structures can easily be identified and compared which is necessary for this study the specific structures that are employed are presented in table 1 and visualised in fig 1 2 2 dendrogram based on model structures applying the kullback leibler divergence a dendrogram was constructed based on model structure the kullback leibler divergence is a method to illustrate the relative entropy between two or more entities masson and knutti 2011 relative entropy is a measure of uncertainty it reflects the loss of information if entity y would be used to approximate entity x weinberg 2016 the kullback leibler divergence consists of a distance matrix and hierarchical clustering the matrix is the input for hierarchical clustering hierarchical clustering clusters the two entities that are closest together then it clusters the two entities that are second closest together this can also be a previously formed cluster and a single entity or two previously formed clusters this process is repeated until all entities are included in one cluster several metrics and methods can be used for both the distance matrix and the hierarchical clustering r core team 2018 desgraupes 2013 in appendix d these different methods are presented in more detail all methods were employed for this study but we only present the results from the euclidean method as distance metric and the ward d2 method as hierarchical clustering as these methods are most commonly used kaufman and rousseeuw 2009 mendlik and gobiet 2016 wilcke and bärring 2016 the eight model structures table 1 were ordered based on a distance metric and hierarchical clustering in order to create a dendrogram however creating dendrograms based on nominal data the equations in the model code requires several steps since one equation is not necessarily higher or lower than the other we had to convert the equations in table 1 to a matrix without giving more weight to one equation than to the other we chose to translate table 1 into a binary matrix for all models table d 4 in appendix d in which each row represents one model eight rows in total each column refers to a different equation for instance for the upper layer three different equation were employed so this translates into three separate columns if the equation occurred in the model structure the assigned value to the column was 1 if the equation did not occur in the model structure the assigned value was 0 the same goes for the other processes except interflow and routing because these processes are represented with the same equations across all models in total this matrix contains eighteen columns using this approach all equations had an equal weight which means that models are most similar when they share the highest number of structural elements note that we do not account for the relative impact of each equation on the model output this resulted in the dendrogram as shown in fig 1 2 3 data and parameter sampling we forced the eight model structures with the daymet atmospheric forcing thornton et al 2012 of all 671 catchments that are available in the camels data set spanning a wide range of hydro climates a snow module is not included in fuse as implemented in r vitolo et al 2015 however the daymet forcing product in the camels data sets provides net water input this is a combination of precipitation and snow melt and as such already accounts for delays in input due to snow storage because our model experiments are synthetic rather than aimed at reproducing observations model performance and calibration were not considered instead we used a parameter ensemble of 100 parameter sets per model structure these parameter sets were sampled using latin hypercube sampling from the default parameter ranges that are provided for fuse see appendix b the sample size was tested as detailed in appendix c it was found that 100 samples was sufficient to capture variability in output introduced by parameters in general each data set contains data from 01 10 1980 till 31 12 2014 the first five hydrological years were used as spin up time we tested for the effect of initial conditions by inserting a heavy precipitation event at day one for several runs and compared this to runs without this event this test showed that the runs converged within one year confirming that five years as spin up time is sufficient from the remaining 29 hydrological years from 01 10 1985 onward the model output was used for comparison with the model structure families the final three months were not used either because these were part of a new hydrological year some forcings 69 out of 671 covered a shorter time period in this case the spin up was kept at five years but the analysed output was shortened 2 4 dendrograms based on signatures hydrologic signatures were calculated from the model output obtained for 100 parameter samples per model the ensemble mean hydrologic signatures were considered representative for the model structure in a similar manner as for the model structures with euclidean distance as distance metric and ward d2 for hierarchical clustering a dendrogram was constructed based on each signature since the data for the signatures are not nominal we did not have to convert the matrices but could define a dendrogram based on the signatures directly the signatures that we used to create the dendrograms are listed in table 2 although it always remains subjective to some extent these signatures were selected to capture the water balance i e different components of the water balance are considered discharge and evaporation and the dynamics i e timing of discharge and evaporation over the year surface runoff and runoff ratio furthermore addor et al 2018 showed that the mean half flow date of the discharge average discharge and runoff ratio can be well explained by climate since we did not calibrate the models we expect the climate signal to be relevant for model output the stream flow elasticity sfe the proportional change in mean annual stream flow divided by the proportional change in mean annual precipitation sankarasubramanian et al 2001 was selected because this is the hydrological equivalent to climate sensitivity signatures providing a relevant link to climate change literature the signatures indicated by subscript y are produced by taking the average per day of the year across all simulation years and parameter samples this results in 365 values in total the 29 th of february is left out the half time signatures indicated by subscript h are based on the day of the year when 50 of the total volume has passed this is determined for each year in the simulation averaged over the parameter samples the half time signatures commonly have 29 values as for most catchments 29 years of data are available model output dendrograms were created based on vectors per model structure for each of the eight signatures we also tested dendrograms that were based on combinations of signatures using the same distance metric and hierarchical clustering method in total 28 combinations of signatures were investigated some signatures might contain the same type of information and therefore not provide extra insights for the dendrogram to quantify this we computed the mutual information mi cover and thomas 1991 between all signatures sfe had relatively little mi with the other signatures the mi among the other signatures varied strongly between catchments and also between the models therefore we decided to not exclude any signatures beforehand 2 5 comparing dendrograms finally the dendrogram based on the model structures was compared to the dendrograms based on the signatures and combinations of signatures using r package dendextend galili 2015 the two dendrograms were connected by lines which show how the relation between the eight models changed or did not change see e g fig 2 panel b the difference between the dendrograms is quantified by the baker s gamma bg correlation coefficient this coefficient ranges from 1 to 1 where both 1 and 1 are synonymous with an exact match between the two dendrograms a value of 0 means that both dendrograms are completely incongruent with each other because of the symmetry in the coefficient values the absolute value of the coefficient was used to identify how successful the inference of the dendrograms based on the signatures was we defined a baseline scenario based on the complete unmodified model output discharge surface runoff and evaporation for each catchment the bg coefficient was determined on the complete output vectors the baseline is the median bg coefficients from all 671 climate instances because the influence of model structures on uncertainty can vary across climates clark et al 2008 mockler et al 2016 newman et al 2015 we also evaluated the bg coefficient over a climate gradient expressed with three climatic characteristics aridity seasonality and snow fraction based on knoben et al 2018 in summary we used eight models defined using fuse table 1 and fig 1 on which we based a dendrogram each model was forced with 671 climate instances obtained from the camels data set with a parameter ensemble of 100 parameter sets leading to a total of 536 800 model runs eight signatures were obtained per model structure and climate instance averaged across the 100 parameter samples table 2 dendrograms were made based on the signature vectors of the eight model structures subsequently the signature based dendrograms were compared with the model structure based dendrogram in total this procedure resulted in 671 bg coefficients for each individual signature or combination of signatures across a range of climate conditions 3 results and analysis first two selected cases are presented to demonstrate how the method works in more detail secondly the dendrogram based on model structures is compared to the dendrograms based on single and combined signatures over all 671 cases their similarity is expressed in the bg coefficient finally the influence of climate on the bg coefficients is discussed 3 1 cases two cases with a high and a low bg coefficient respectively are discussed in detail to demonstrate the method fig 2 we compared model outputs obtained from the eight different fuse models for one climate instance for each of these model outputs the half time discharge signature qh was calculated per year averaged across the 100 parameter samples resulting in 25 different values note that in most cases 29 years of data are available resulting in 29 different values the time series of qh can be seen in fig 2a these time series form the basis for the distance metric and hierarchical clustering to determine the dendrogram based on qh in this case the model structure families do not overlap the model output time series of qh within one model structure family are highly similar this raises the expectation that the qh signature based dendrogram is similar to the model structure dendrogram this is indeed the case visible in fig 2b no differences in the clustering occurs only the distances within and between the families differ this results in a bg coefficient of 0 999 which means that the two dendrograms are highly similar this corroborates that for this specific instance the model structure families could be inferred from model output based on qh model structure families can however not always successfully be inferred from the model output for the second case displayed in fig 2 the dendrograms based on the mean surface runoff signature sry could not replicate the dendrogram based on the model structures fig 2d the model output based on sry does not reflect the model structure in fig 2c it is visible that the model structure family bands overlap the model output within one model structure family is not necessarily most similar to one another this results in inconsistency between the signature based dendrogram and the model structure based dendrogram expressed in a bg coefficient of 0 084 this means that sry contains little information about the complete model structures sry might however contain more information about a single component of the model structure the surface runoff parameterisations within the models models 3 4 5 and 8 who share the same surface runoff parameterisation are clustered together the same applies to model 1 and 2 and 6 and 7 each pair has the same parameterisation for surface runoff these models are also ordered in the same cluster this demonstrates that the sensitivity of a signature for a certain process in the model determines whether model structure can successfully be inferred if a signature is mainly sensitive to a specific part of the model structure the families can only be inferred for this part these two cases display how our approach works a high bg coefficient means that model structure families can be inferred from model output in such cases bias in the chosen models of model ensembles can be determined without exactly knowing the underlying code of each model a low bg coefficient implies that model structure families cannot be inferred from model output analysis of model output is then not an appropriate method to determine bias in model ensembles however note that we focus on inferring complete model structures as the example with surface runoff sry demonstrated a low bg coefficient can also indicate that a certain signature only contains information from a certain process formulation in the model and therefore is only able to distinguish families based on this process formulation the cases also show that some signatures might provide more information about model structure than others qh is a seasonal and therefore perhaps more gradual characteristic whereas sry might be heavily influenced by individual precipitation events whether a clear distinction between model structures can be observed in certain signatures depends on the freedom a model has to respond in different ways to events that influence this signature this relates to the sensitivity of model output to each component of the model structure which is currently not accounted for in the model structure based dendrogram 3 2 bg coefficients across signatures to obtain a general picture of how well model structure families can be inferred from model output we investigated the bg coefficients over 671 different climate instances this is done both for the eight single signatures and for 28 combinations of signatures fig 3 provides a summary of the results panel a of fig 3 shows the range average and median bg coefficient for each single signature across all 671 climate instances also the baseline the median bg coefficient across all climates based on all model output is depicted although it could be expected that the baseline would have most information available because no averaged signatures were computed and therefore would obtain the highest bg coefficient it turns out that inference based on signatures generally outperforms the inference based on all model output this can indicate that the inference can benefit from selecting well defined signatures that focus on specific information to reduce the noisy signal that is present in the complete model output overall the bg coefficients are low implying that it is difficult to infer model structure families from model output with the chosen signatures there is no clear signal that timing based signatures indicated with an h perform better or worse than magnitude based signatures indicated with an y from the three evaluated fluxes discharge surface runoff and evaporation the discharge and the evaporation based signatures have the highest mean and median bg coefficient for qh the upper 25 of the bg coefficients ranges from just above 0 4 to about 1 with the runoff ratio the highest mean and median bg coefficients are obtained the low correlations based on sry and srh the surface runoff signatures can be understood from a process sensitivity point of view in all models the surface runoff is subtracted from the precipitation see appendix a accordingly the sry and srh results are dependent only on the surface runoff equation in each model additionally surface runoff depends on relative saturated area the maximum saturated area is sampled as parameter and therefore for some values in the range of this parameter surface runoff might be limited only three surface runoff equations are employed see table 1 and therefore it is more challenging to distinguish the eight separate models this leads to the insight that in order to infer model structure from model output the signature should contain information that is influenced by several components in the model structure combining the signatures does not increase the potential to infer model structure families from model output fig 3b in this figure eight examples are depicted which showcase the general pattern the boxplots for all combinations can be found in appendix e the combination of the signatures qh and srh results in a better median which increased by 0 04 compared to qh alone certain combinations of signatures give better results than the separate signatures e g qy ey shown in appendix e showing that combined they contain more information but for most combinations this is not the case e g qy sfe and qh sry all combinations with sfe returned the same distribution as the other signature on its own even thought both ey and rr have relatively high medians on their own in most combinations the median decreased every combination made with sry or srh is at the baseline or below yet the combination of srh with eh does have a higher range which signifies the influence of eh in general srh is the most influential signature because all combinations with srh are either around the baseline or substantially lower than what the second signature would achieve on its own meaning that srh blurs the relation to model structure for reasons explained above qm qh and ey have a positive influence on the inference of model structure families when combined with other signatures when eh rr and sfe are combined with other signatures they have a relatively neutral effect on the inference overall the half time signatures are more influential than the mean signatures this confirms gunkel et al 2015 s observation that timing is important to consider in model structures and model selection ideally we would like to relate the potential of combining signatures using their mutual information the less information signatures share the more information their combination contains which could enhance a successful inference it turned out however the mutual information between signatures was highly climate and model dependent and therefore difficult to directly relate to the potential of combining signatures for a higher bg coefficient an alternative strategy could be to conduct a process based sensitivity analysis and combine signatures that have contrasting process sensitivities however the results based on combined signatures shows that combining signatures does not automatically resolve the issue that some signatures are only sensitive to some parts of the model 3 3 relation to climate characteristics the bg coefficient was computed per signature for all 671 climate instances with different climate characteristics to analyse the influence of climate on model structure inference the bg coefficients were evaluated against three climate characteristics defined by knoben et al 2018 average moisture index as an expression of aridity note that 1 indicates highly arid in this definition and 1 highly humid seasonality in moisture index and snow fraction addor et al 2018 already showed that some signatures relate stronger to climate than others for example mean annual discharge runoff ratio and mean half flow date relate closely to the climate characteristics given that the signatures relate in different ways to climate it can be expected that the bg coefficients based on different signatures also respond differently over the investigated climate characteristics if the ability to infer model families from signatures depends on the model input climate we expect to see a strong relation between the bg coefficient based on one of the signatures and relevant climate characteristics fig 4 depicts the bg coefficients for each signature for the three climate characteristics a first order relation between climate characteristic and bg coefficient is tested with simple linear regression no strong linear relations are found between climate characteristic and bg coefficient for any of the signatures the strongest relation exists between average surface runoff year sry and aridity expressed as moisture index the less arid the region is the higher the bg coefficient based on sry but the bg coefficients are very low also in very humid regions indicating that inference of the whole model structure is not successful as discussed earlier both surface runoff signatures might still be excellent indicators for how the surface runoff components within each model structure are related for the discharge signatures qy and qh a division in bg coefficients is visible for qy for instance two layers can be distinguished one around a bg coefficient of 0 05 and the other at around 0 4 for qh the bg coefficients are divided over three layers given that these layers persist across the climate indicators climate at least with the tested indicators cannot explain this distinction for the evaporation signatures ey and eh aridity and seasonality seem to show some but a weak relation to bg coefficient the quality of the inference increases in more arid regions as in more arid regions the role of evaporation becomes more prominent this could be understood from a process point of view in the bg coefficients based on evaporation signatures again a division is visible similar as what was seen for the discharge signatures this division is not explained by these climate characteristics individually it could be that a combination of the tested climate characteristics or other climate characteristics not considered in this study can explain these divisions the bg coefficients based on the runoff ratio rr have a marginal trend with seasonality for high seasonality the bg coefficients based on rr seem to decrease the definition of seasonality employed here as defined by knoben et al 2018 is the seasonality in moisture index thus a high seasonality implies a high variation in moisture index this could potentially make the runoff ratio more noisy and as such blur the inference the bg coefficients based on stream flow elasticity sfe seem to be randomly organised over all three climate indicators in summary there seems to be some relation between the bg coefficient and climate but the trends are weak the graphs suggest that there is probably another variable that can explain the height of the bg coefficient given the different layers that can be distinguished for several signatures 4 discussion in this study we explored if model structure families could be inferred from model output this could potentially be a useful approach to determine biases in model ensembles without the need to dig into details of model code furthermore it could provide insights into which part of the model space is sampled with the model ensemble and as such is a measure for how well model structure uncertainty is represented since model space is infinitely large this would however always remain a relative estimate compared to other ensembles the results of this study show that the performance of inferring model structure from model output depends on the type of signature evaluated and the climate but overall it is proven difficult to infer complete model families from model output in this section we discuss two aspects in more detail our study setup also in relation to earlier work on model families and the implications of our results 4 1 study setup in this study we employed a novel combination of approaches to explore the potential of inferring model families from model output which eventually can be used to quantify model structure uncertainty there are three aspects of our study design that might have implications for our results the mmf choice the evaluated signatures and the statistical methods for hierarchical clustering mmf choice as mmf we employed the framework for understanding structural errors fuse this is only one of many mmfs that are available in hydrology these days craig et al 2020 knoben et al 2019 weber et al 2019 clark et al 2015 each mmf has a clear focus in terms of modelling philosophy and therefore only covers a limited part of model space the choice of the mmf does as such already determine which part of model space is explored the model components included in fuse are only representative for conceptual bucket based hydrological models focussing on the subsurface processes this limits the part of model space that can be sampled with fuse and is thus unrepresentative of the whole hydrological model space summa clark et al 2015 would have been more representative for the land surface part of model space and raven craig et al 2020 seems to be able to capture both conceptual and more physics based models a continuation of this work with a broader sample of model structures is therefore deemed highly valuable knutti et al 2013 performed the hierarchical clustering on the output of cmip5 climate models these models are much more complex than the suit of models employed in this study they validated their results by comparing some of the main building blocks of the climate models e g if they shared the same ocean model this was however not based on the actual model code leaving the option open that the same ocean model was incorporated differently in different climate models the limited range of model space covered by the fuse models in this study would probably be evaluated as one family compared to the cmip5 models open for discussion is how different models should be in order not to create a bias in the ensemble choice of signatures gunkel et al 2015 and gupta et al 2012 demonstrate that signatures can give insight into the capabilities and short comings of model structures which is in accordance with our results the choice of the signature to conduct the inference is very important some signatures could provide high bg coefficients discharge and evaporation based signatures while other signatures related to surface runoff gave consistently low bg coefficients many signatures are available and signatures should be carefully selected mcmillan et al 2017 however clear guidance on signature selection is still lacking addor et al 2018 our results demonstrate that the choice of the signatures should be based on to what extent they are influenced by or sensitive to multiple processes in the model structure knutti et al 2013 selected temperature and precipitation signatures to create their dendrograms for the cmip5 climate models for climate models these signatures are heavily influenced by interactions in the model so these signatures probably encompass the correct information to conduct the inference the greatest confidence in a signature can be obtained when the processes underlying a signature are well known and properly modelled knutti et al 2013 hence signature choice is an important aspect when inferring model structure families from model output statistical methods in this study we employed five different methods for both the distance metric and the hierarchical clustering we only presented the results of the euclidean distance metric and the ward d2 clustering method however the bg coefficient differed per combinations of statistical methods see appendix d in general three hierarchical clustering methods performed best complete ward d and ward d2 but this also related to the employed distance metric and the evaluated signature the euclidean distance metric and the ward d2 clustering method were representative of the general pattern because the euclidean method provided similar results as the manhattan and minkowski method the ward d2 method was one of the hierarchical clustering methods that performed consistently better this could however change if other signatures are employed the choice between the other distance metrics or hierarchical clustering methods depends on which signatures can be computed from the model output 4 2 on sampling model space whereas input data uncertainty can be derived from the accuracy of the measurement device or intercomparison of observation methods and parameter uncertainty can be approached by sampling parameter space within clearly defined parameter boundaries model structure uncertainty is more difficult to estimate and this has become even more challenging with increasing model complexity wagener and kollat 2007 ideally one would like to sample model space in the same way parameters can be sampled but model space could be infinitely large and actually a clear definition of what model space comprehends is lacking in this study we assumed model space as model hypothesis space where we consider the equations in hydrological models as hypotheses given the challenge of going through the codes of many different models we tested if model output could be representative of the underlying model hypotheses one could however also argue that model space is represented by model output space therefore a clarification is needed of what is meant by sampling model space does this concern model hypothesis space with different parameterisations that can lead to the same output or model output space with the output covering a wide range to have some idea of the uncertainty our argument to use model hypothesis space as the relevant space to sample is that this is more congruent with the way parameter uncertainty is treated and because model output space is the consequence of model hypothesis space combined with model parameter space and observation uncertainty in case of calibration making model hypothesis space a cleaner representation of model structure an implication of this viewpoint however is that our dendrogram based on model structures was solely based on the model equations with each equation receiving an equal weight some equations will however have relatively more influence on the model output than others which is represented by the model sensitivity the sensitivity of each modelling decision was not taken into account in the construction of the dendrograms and might partly explain the mismatch between model structure based dendrograms and signature based dendrograms yet it is challenging to account for sensitivity in the construction of model structure dendrograms firstly because it already requires analysis of model output and secondly because sensitivity will depend on parameter values the signature evaluated and the climate this would warrant detailed analysis of the separate model components and their influence on model results potentially resulting in flexible model structure families which can depend on the climate modelled catchment used signature and the modelling purpose one can also only be interested in one specific part of the model structure and choose the signature accordingly as the presented results showed the inference of complete model structure based on surface runoff signatures was not successful but it was possible to distinguish the three different surface runoff formulations among the 8 models using surface runoff based signatures as knutti et al 2013 explains model ensemble runs can be biased because some parameterisations are over represented in the models of the ensemble in their study it is assumed that they were able to infer model families from model output it is however also possible that different combinations of parameterisations lead to the same model output model equifinality this would distort the relation between the inferred model families and the model structure based families in our study we tested this inference and have to conclude that it is difficult to infer model structure from model output within our experimental set up of fuse models camels data set and chosen signatures therefore we might need other approaches to explore model space enemark et al 2019 provide a review of studies that explore several model concepts and suggest that for each conceptual model alternative models representing hypotheses that are mutually exclusive should be defined another potential approach could be a more comprehensive model classification based on this classification a sample of models could be selected as being representative for a part of model space 5 conclusion in this study we investigated if model structure families could be inferred from model output this could potentially provide useful information on how well model structure uncertainty is captured and whether any bias exists in the model ensemble we employed a novel combination of approaches to quantify the success of inferring model structure families from model output represented by signatures we show that some signatures do lead to successful inference discharge based signatures while others do not surface runoff based signatures combining signatures does in general not enhance the potential to infer model structure families from model output overall it was shown that it is challenging and in most cases impossible to infer model structure from model output for the part of model space bucket based hydrological models that we sampled therefore we have to continue searching for ways to capture model space in order to represent model structure uncertainty declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements for the statistical analysis we would like to thank joost van heerwaarden and ron wehrens for their expertise and time furthermore we would like to thank diana spieler for the insightful discussions we thank claudia vitolo and nans addor for their help in understanding fuse finally we thank nans addor wouter knoben and an anonymous reviewer for their thorough and constructive feedback appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2020 104817 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
25963,in model studies a careful consideration of uncertainty is needed parameter and data uncertainty can be explored by sampling but model structure uncertainty is more challenging to capture since the underlying hypotheses of many models are not directly clear this study explores whether model structure can be inferred from model output we created a dendrogram family tree based on model structure using a modular modelling framework subsequently we created dendrograms based on model output we determined the correlation between both dendrograms to analysed if model structure families could be inferred from model output results from this experiment over 671 climate instances showed that the performance of the inference depends on the type of output evaluated and the climate however the performance of the inference is overall low implying that model structure cannot be inferred from model output these results demonstrate the need to further investigate opportunities to sample model space keywords model structure uncertainty modular modelling framework model genealogy signatures dendrogram sampling model space data availability the camels data set is publicly available it is provided by ncar doi 10 5065 d6mw2f4d the r package for fuse is also publicly available doi 10 5281 zenodo 14005 vitolo et al 2015 the r scripts for our results and analysis are available on request 1 introduction earth and environmental models are inevitably subject to uncertainty oreskes et al 1994 it is the challenging task for modellers to provide projections as accurate as possible while acknowledging that some uncertainty is irreducible weigel et al 2010 careful consideration of the uncertainty is needed to investigate if the model conclusions are robust not only because of socio economic impacts jakeman et al 2006 mcmillan et al 2018 uusitalo et al 2015 but also to enhance process understanding butts et al 2004 clark et al 2008 deser et al 2012 generally three main types of uncertainty are distinguished data uncertainty model structure uncertainty and parameter uncertainty which all consequently lead to output uncertainty bastin et al 2013 refsgaard et al 2007 dependent on the research field research question and the modelled location certain types of uncertainty will prevail deser et al 2012 for instance showed that uncertainty in the initial conditions of climate models has large influence on the final simulations clark et al 2008 showed for a hydrological application that in an arid catchment hydrological model structure has more influence on the simulations than in a wet catchment newman et al 2015 found that errors in the forcing data specifically precipitation and snow melt data could explain outliers in model performance in their tested catchments demonstrating the influence of input data uncertainty therefore a thorough estimation and quantification of all sources of uncertainty is required butts et al 2004 parameter uncertainty and data uncertainty can to a large extent be quantified rather straightforwardly by a combination of intelligent sampling and using brute computer force to create large ensembles quantification of model structure uncertainty is more challenging implementing different off the shelve models can be costly in terms of time and effort addor and melsen 2019 jakeman et al 2006 but also because of the question how do you sample model space theoretically model space could represent all plausible hypotheses of how a system could function if it was possible to sample models in model hypothesis space this would be congruent with the current method to treat parameter uncertainty but it is yet unknown how many plausible hypotheses potentially exist a more thorough understanding of which part of model space is sampled in a study is highly valuable to better understand which part of model structure uncertainty is captured to identify a range of different models that represent parts of model space and eventually to be able to select the most appropriate model for a particular research question a currently frequently used approach to capture model structural uncertainty is to employ model ensembles to sample model space abramowitz and gupta 2008 knutti et al 2013 these ensembles however can be biased when certain parts of the model structure are over represented abramowitz and gupta 2008 weigel et al 2010 since the individual models were not designed from the central idea that all uncertainty in model structure should be captured in other words it is unknown if the space of possible models has been sampled correctly refsgaard et al 2007 for instance in hydrology weiler and beven 2015 observed that a plethora of hydrological models exists which can be used to estimate model structure uncertainty but many hydrological models have very similar structures thereby providing an incomplete picture of the hydrological model space and thus of model structure uncertainty an additional challenge is that many decisions related to model implementation and model structure are not explicitly communicated babel et al 2019 knutti et al 2013 melsen et al 2019 renard et al 2010 wagener and kollat 2007 this hampers the identification of bias in model ensembles correctly sampling model space is relevant to estimate uncertainty and to identify which regions are explored but challenging because of biases in ensembles and the lack of transparency of underlying hypotheses and assumptions in models to tackle bias in the model ensemble model classification schemes could be employed past attempts for model classification in hydrology have not been numerous or extensive plate 2009 made a classification for flood management models the goal of this study however was to provide guidelines when to use which model not to provide guidance on how to capture structural uncertainty jajarmizadeh et al 2012 discuss several classifications of hydrological models these classifications are however all based on certain modelling approaches e g lumped versus distributed rather than on the underlying hypotheses to describe different processes furthermore jajarmizadeh et al 2012 acknowledged that their classification would probably depend on the person who made the classification mueller et al 2011 produces a more extensive evaluation and classification of global land evapotranspiration products from a range of different model types such approaches however are not common practice in hydrology on top of that the data and models are closely interlinked in the product in hydrology a sampling method to consistently relate model ensembles to quantifiable uncertainty is yet to be developed nearing and gupta 2018 an alternative to model classification is to evaluate model structures based on model output knutti et al 2013 and masson and knutti 2011 for instance used hierarchical clustering to show how the climate model intercomparison project 5 cmip5 generation of climate models are related hierarchical clustering is a method widely employed in biology that can be visualised in the form of dendrograms baake 1998 weinberger et al 2006 because of the use of dendrograms this methods is referred to as model genealogy for the ipcc climate projections based on the cmip5 ensemble it is assumed that the range of models correctly represents model structure uncertainty but the dendrograms of knutti et al 2013 and of masson and knutti 2011 show that model similarities can bias the output the cmip5 model genealogy based on model output however was not validated with the actual model code this is practically impossible with the highly complex cmip5 climate models therefore the question with this approach is if model output correctly reflects model structure can model structure families be inferred from model output modular modelling frameworks mmfs provide an excellent opportunity to explore this question mmfs are master templates for model generation they can provide an experimental set up to determine which component in model structure results in a certain difference in model output by having the ability to change the model components individually several mmfs are available within the field of hydrology such as mms leavesley et al 1996 superflex fenicia et al 2011 fuse clark et al 2008 summa clark et al 2015 and raven craig et al 2020 the mmf approach enables us to explore the question whether bias in model ensembles can be determined based on model output whereas in earlier studies the lack of detailed knowledge on model structure and implementation hampered the estimation of model structure bias directly mmfs provide a transparent way to assemble an ensemble of model structures of which their familiarity and similarity can be determined directly based on model code therefore by using mmfs we can investigate if model structure bias in an ensemble indeed can be determined based on model output similar to the approach by knutti et al 2013 in this study we investigate if the model genealogy approach determining model families based on model output is compatible with model families based on model structure defined using a hydrological mmf 2 methods we will employ the framework for understanding structural errors fuse clark et al 2008 to create an ensemble of models for which we can construct a dendrogram based on the model structure subsequently we run the ensemble and create a dendrogram based on the model output by comparing both dendrograms we can investigate if model families can be inferred from model outputs for a set of hydrological models using a large sample data set spanning a range of climate conditions camels newman et al 2015 addor et al 2017 allows to investigate a more general pattern in this inference finally we link the results to climate to explain the quality of the inference three climate characteristics based on knoben et al 2018 were considered aridity seasonality and snow fraction 2 1 model structures the first step was to select and classify several model structures the use of an mmf was crucial in order to have full control over changes in the model structure we selected the mmf fuse clark et al 2008 as implemented in r by vitolo et al 2015 to configure eight different model structures these eight models were chosen to sample a wide range within fuse and to create three distinguishably different families fuse was developed to analyse the difference in model simulations occurring due to different but equally probable representations of the real processes fuse only offers models based on the bucket based modelling philosophy thereby representing only a small part of all possible modelling hypotheses this mmf was selected because the model structures can easily be identified and compared which is necessary for this study the specific structures that are employed are presented in table 1 and visualised in fig 1 2 2 dendrogram based on model structures applying the kullback leibler divergence a dendrogram was constructed based on model structure the kullback leibler divergence is a method to illustrate the relative entropy between two or more entities masson and knutti 2011 relative entropy is a measure of uncertainty it reflects the loss of information if entity y would be used to approximate entity x weinberg 2016 the kullback leibler divergence consists of a distance matrix and hierarchical clustering the matrix is the input for hierarchical clustering hierarchical clustering clusters the two entities that are closest together then it clusters the two entities that are second closest together this can also be a previously formed cluster and a single entity or two previously formed clusters this process is repeated until all entities are included in one cluster several metrics and methods can be used for both the distance matrix and the hierarchical clustering r core team 2018 desgraupes 2013 in appendix d these different methods are presented in more detail all methods were employed for this study but we only present the results from the euclidean method as distance metric and the ward d2 method as hierarchical clustering as these methods are most commonly used kaufman and rousseeuw 2009 mendlik and gobiet 2016 wilcke and bärring 2016 the eight model structures table 1 were ordered based on a distance metric and hierarchical clustering in order to create a dendrogram however creating dendrograms based on nominal data the equations in the model code requires several steps since one equation is not necessarily higher or lower than the other we had to convert the equations in table 1 to a matrix without giving more weight to one equation than to the other we chose to translate table 1 into a binary matrix for all models table d 4 in appendix d in which each row represents one model eight rows in total each column refers to a different equation for instance for the upper layer three different equation were employed so this translates into three separate columns if the equation occurred in the model structure the assigned value to the column was 1 if the equation did not occur in the model structure the assigned value was 0 the same goes for the other processes except interflow and routing because these processes are represented with the same equations across all models in total this matrix contains eighteen columns using this approach all equations had an equal weight which means that models are most similar when they share the highest number of structural elements note that we do not account for the relative impact of each equation on the model output this resulted in the dendrogram as shown in fig 1 2 3 data and parameter sampling we forced the eight model structures with the daymet atmospheric forcing thornton et al 2012 of all 671 catchments that are available in the camels data set spanning a wide range of hydro climates a snow module is not included in fuse as implemented in r vitolo et al 2015 however the daymet forcing product in the camels data sets provides net water input this is a combination of precipitation and snow melt and as such already accounts for delays in input due to snow storage because our model experiments are synthetic rather than aimed at reproducing observations model performance and calibration were not considered instead we used a parameter ensemble of 100 parameter sets per model structure these parameter sets were sampled using latin hypercube sampling from the default parameter ranges that are provided for fuse see appendix b the sample size was tested as detailed in appendix c it was found that 100 samples was sufficient to capture variability in output introduced by parameters in general each data set contains data from 01 10 1980 till 31 12 2014 the first five hydrological years were used as spin up time we tested for the effect of initial conditions by inserting a heavy precipitation event at day one for several runs and compared this to runs without this event this test showed that the runs converged within one year confirming that five years as spin up time is sufficient from the remaining 29 hydrological years from 01 10 1985 onward the model output was used for comparison with the model structure families the final three months were not used either because these were part of a new hydrological year some forcings 69 out of 671 covered a shorter time period in this case the spin up was kept at five years but the analysed output was shortened 2 4 dendrograms based on signatures hydrologic signatures were calculated from the model output obtained for 100 parameter samples per model the ensemble mean hydrologic signatures were considered representative for the model structure in a similar manner as for the model structures with euclidean distance as distance metric and ward d2 for hierarchical clustering a dendrogram was constructed based on each signature since the data for the signatures are not nominal we did not have to convert the matrices but could define a dendrogram based on the signatures directly the signatures that we used to create the dendrograms are listed in table 2 although it always remains subjective to some extent these signatures were selected to capture the water balance i e different components of the water balance are considered discharge and evaporation and the dynamics i e timing of discharge and evaporation over the year surface runoff and runoff ratio furthermore addor et al 2018 showed that the mean half flow date of the discharge average discharge and runoff ratio can be well explained by climate since we did not calibrate the models we expect the climate signal to be relevant for model output the stream flow elasticity sfe the proportional change in mean annual stream flow divided by the proportional change in mean annual precipitation sankarasubramanian et al 2001 was selected because this is the hydrological equivalent to climate sensitivity signatures providing a relevant link to climate change literature the signatures indicated by subscript y are produced by taking the average per day of the year across all simulation years and parameter samples this results in 365 values in total the 29 th of february is left out the half time signatures indicated by subscript h are based on the day of the year when 50 of the total volume has passed this is determined for each year in the simulation averaged over the parameter samples the half time signatures commonly have 29 values as for most catchments 29 years of data are available model output dendrograms were created based on vectors per model structure for each of the eight signatures we also tested dendrograms that were based on combinations of signatures using the same distance metric and hierarchical clustering method in total 28 combinations of signatures were investigated some signatures might contain the same type of information and therefore not provide extra insights for the dendrogram to quantify this we computed the mutual information mi cover and thomas 1991 between all signatures sfe had relatively little mi with the other signatures the mi among the other signatures varied strongly between catchments and also between the models therefore we decided to not exclude any signatures beforehand 2 5 comparing dendrograms finally the dendrogram based on the model structures was compared to the dendrograms based on the signatures and combinations of signatures using r package dendextend galili 2015 the two dendrograms were connected by lines which show how the relation between the eight models changed or did not change see e g fig 2 panel b the difference between the dendrograms is quantified by the baker s gamma bg correlation coefficient this coefficient ranges from 1 to 1 where both 1 and 1 are synonymous with an exact match between the two dendrograms a value of 0 means that both dendrograms are completely incongruent with each other because of the symmetry in the coefficient values the absolute value of the coefficient was used to identify how successful the inference of the dendrograms based on the signatures was we defined a baseline scenario based on the complete unmodified model output discharge surface runoff and evaporation for each catchment the bg coefficient was determined on the complete output vectors the baseline is the median bg coefficients from all 671 climate instances because the influence of model structures on uncertainty can vary across climates clark et al 2008 mockler et al 2016 newman et al 2015 we also evaluated the bg coefficient over a climate gradient expressed with three climatic characteristics aridity seasonality and snow fraction based on knoben et al 2018 in summary we used eight models defined using fuse table 1 and fig 1 on which we based a dendrogram each model was forced with 671 climate instances obtained from the camels data set with a parameter ensemble of 100 parameter sets leading to a total of 536 800 model runs eight signatures were obtained per model structure and climate instance averaged across the 100 parameter samples table 2 dendrograms were made based on the signature vectors of the eight model structures subsequently the signature based dendrograms were compared with the model structure based dendrogram in total this procedure resulted in 671 bg coefficients for each individual signature or combination of signatures across a range of climate conditions 3 results and analysis first two selected cases are presented to demonstrate how the method works in more detail secondly the dendrogram based on model structures is compared to the dendrograms based on single and combined signatures over all 671 cases their similarity is expressed in the bg coefficient finally the influence of climate on the bg coefficients is discussed 3 1 cases two cases with a high and a low bg coefficient respectively are discussed in detail to demonstrate the method fig 2 we compared model outputs obtained from the eight different fuse models for one climate instance for each of these model outputs the half time discharge signature qh was calculated per year averaged across the 100 parameter samples resulting in 25 different values note that in most cases 29 years of data are available resulting in 29 different values the time series of qh can be seen in fig 2a these time series form the basis for the distance metric and hierarchical clustering to determine the dendrogram based on qh in this case the model structure families do not overlap the model output time series of qh within one model structure family are highly similar this raises the expectation that the qh signature based dendrogram is similar to the model structure dendrogram this is indeed the case visible in fig 2b no differences in the clustering occurs only the distances within and between the families differ this results in a bg coefficient of 0 999 which means that the two dendrograms are highly similar this corroborates that for this specific instance the model structure families could be inferred from model output based on qh model structure families can however not always successfully be inferred from the model output for the second case displayed in fig 2 the dendrograms based on the mean surface runoff signature sry could not replicate the dendrogram based on the model structures fig 2d the model output based on sry does not reflect the model structure in fig 2c it is visible that the model structure family bands overlap the model output within one model structure family is not necessarily most similar to one another this results in inconsistency between the signature based dendrogram and the model structure based dendrogram expressed in a bg coefficient of 0 084 this means that sry contains little information about the complete model structures sry might however contain more information about a single component of the model structure the surface runoff parameterisations within the models models 3 4 5 and 8 who share the same surface runoff parameterisation are clustered together the same applies to model 1 and 2 and 6 and 7 each pair has the same parameterisation for surface runoff these models are also ordered in the same cluster this demonstrates that the sensitivity of a signature for a certain process in the model determines whether model structure can successfully be inferred if a signature is mainly sensitive to a specific part of the model structure the families can only be inferred for this part these two cases display how our approach works a high bg coefficient means that model structure families can be inferred from model output in such cases bias in the chosen models of model ensembles can be determined without exactly knowing the underlying code of each model a low bg coefficient implies that model structure families cannot be inferred from model output analysis of model output is then not an appropriate method to determine bias in model ensembles however note that we focus on inferring complete model structures as the example with surface runoff sry demonstrated a low bg coefficient can also indicate that a certain signature only contains information from a certain process formulation in the model and therefore is only able to distinguish families based on this process formulation the cases also show that some signatures might provide more information about model structure than others qh is a seasonal and therefore perhaps more gradual characteristic whereas sry might be heavily influenced by individual precipitation events whether a clear distinction between model structures can be observed in certain signatures depends on the freedom a model has to respond in different ways to events that influence this signature this relates to the sensitivity of model output to each component of the model structure which is currently not accounted for in the model structure based dendrogram 3 2 bg coefficients across signatures to obtain a general picture of how well model structure families can be inferred from model output we investigated the bg coefficients over 671 different climate instances this is done both for the eight single signatures and for 28 combinations of signatures fig 3 provides a summary of the results panel a of fig 3 shows the range average and median bg coefficient for each single signature across all 671 climate instances also the baseline the median bg coefficient across all climates based on all model output is depicted although it could be expected that the baseline would have most information available because no averaged signatures were computed and therefore would obtain the highest bg coefficient it turns out that inference based on signatures generally outperforms the inference based on all model output this can indicate that the inference can benefit from selecting well defined signatures that focus on specific information to reduce the noisy signal that is present in the complete model output overall the bg coefficients are low implying that it is difficult to infer model structure families from model output with the chosen signatures there is no clear signal that timing based signatures indicated with an h perform better or worse than magnitude based signatures indicated with an y from the three evaluated fluxes discharge surface runoff and evaporation the discharge and the evaporation based signatures have the highest mean and median bg coefficient for qh the upper 25 of the bg coefficients ranges from just above 0 4 to about 1 with the runoff ratio the highest mean and median bg coefficients are obtained the low correlations based on sry and srh the surface runoff signatures can be understood from a process sensitivity point of view in all models the surface runoff is subtracted from the precipitation see appendix a accordingly the sry and srh results are dependent only on the surface runoff equation in each model additionally surface runoff depends on relative saturated area the maximum saturated area is sampled as parameter and therefore for some values in the range of this parameter surface runoff might be limited only three surface runoff equations are employed see table 1 and therefore it is more challenging to distinguish the eight separate models this leads to the insight that in order to infer model structure from model output the signature should contain information that is influenced by several components in the model structure combining the signatures does not increase the potential to infer model structure families from model output fig 3b in this figure eight examples are depicted which showcase the general pattern the boxplots for all combinations can be found in appendix e the combination of the signatures qh and srh results in a better median which increased by 0 04 compared to qh alone certain combinations of signatures give better results than the separate signatures e g qy ey shown in appendix e showing that combined they contain more information but for most combinations this is not the case e g qy sfe and qh sry all combinations with sfe returned the same distribution as the other signature on its own even thought both ey and rr have relatively high medians on their own in most combinations the median decreased every combination made with sry or srh is at the baseline or below yet the combination of srh with eh does have a higher range which signifies the influence of eh in general srh is the most influential signature because all combinations with srh are either around the baseline or substantially lower than what the second signature would achieve on its own meaning that srh blurs the relation to model structure for reasons explained above qm qh and ey have a positive influence on the inference of model structure families when combined with other signatures when eh rr and sfe are combined with other signatures they have a relatively neutral effect on the inference overall the half time signatures are more influential than the mean signatures this confirms gunkel et al 2015 s observation that timing is important to consider in model structures and model selection ideally we would like to relate the potential of combining signatures using their mutual information the less information signatures share the more information their combination contains which could enhance a successful inference it turned out however the mutual information between signatures was highly climate and model dependent and therefore difficult to directly relate to the potential of combining signatures for a higher bg coefficient an alternative strategy could be to conduct a process based sensitivity analysis and combine signatures that have contrasting process sensitivities however the results based on combined signatures shows that combining signatures does not automatically resolve the issue that some signatures are only sensitive to some parts of the model 3 3 relation to climate characteristics the bg coefficient was computed per signature for all 671 climate instances with different climate characteristics to analyse the influence of climate on model structure inference the bg coefficients were evaluated against three climate characteristics defined by knoben et al 2018 average moisture index as an expression of aridity note that 1 indicates highly arid in this definition and 1 highly humid seasonality in moisture index and snow fraction addor et al 2018 already showed that some signatures relate stronger to climate than others for example mean annual discharge runoff ratio and mean half flow date relate closely to the climate characteristics given that the signatures relate in different ways to climate it can be expected that the bg coefficients based on different signatures also respond differently over the investigated climate characteristics if the ability to infer model families from signatures depends on the model input climate we expect to see a strong relation between the bg coefficient based on one of the signatures and relevant climate characteristics fig 4 depicts the bg coefficients for each signature for the three climate characteristics a first order relation between climate characteristic and bg coefficient is tested with simple linear regression no strong linear relations are found between climate characteristic and bg coefficient for any of the signatures the strongest relation exists between average surface runoff year sry and aridity expressed as moisture index the less arid the region is the higher the bg coefficient based on sry but the bg coefficients are very low also in very humid regions indicating that inference of the whole model structure is not successful as discussed earlier both surface runoff signatures might still be excellent indicators for how the surface runoff components within each model structure are related for the discharge signatures qy and qh a division in bg coefficients is visible for qy for instance two layers can be distinguished one around a bg coefficient of 0 05 and the other at around 0 4 for qh the bg coefficients are divided over three layers given that these layers persist across the climate indicators climate at least with the tested indicators cannot explain this distinction for the evaporation signatures ey and eh aridity and seasonality seem to show some but a weak relation to bg coefficient the quality of the inference increases in more arid regions as in more arid regions the role of evaporation becomes more prominent this could be understood from a process point of view in the bg coefficients based on evaporation signatures again a division is visible similar as what was seen for the discharge signatures this division is not explained by these climate characteristics individually it could be that a combination of the tested climate characteristics or other climate characteristics not considered in this study can explain these divisions the bg coefficients based on the runoff ratio rr have a marginal trend with seasonality for high seasonality the bg coefficients based on rr seem to decrease the definition of seasonality employed here as defined by knoben et al 2018 is the seasonality in moisture index thus a high seasonality implies a high variation in moisture index this could potentially make the runoff ratio more noisy and as such blur the inference the bg coefficients based on stream flow elasticity sfe seem to be randomly organised over all three climate indicators in summary there seems to be some relation between the bg coefficient and climate but the trends are weak the graphs suggest that there is probably another variable that can explain the height of the bg coefficient given the different layers that can be distinguished for several signatures 4 discussion in this study we explored if model structure families could be inferred from model output this could potentially be a useful approach to determine biases in model ensembles without the need to dig into details of model code furthermore it could provide insights into which part of the model space is sampled with the model ensemble and as such is a measure for how well model structure uncertainty is represented since model space is infinitely large this would however always remain a relative estimate compared to other ensembles the results of this study show that the performance of inferring model structure from model output depends on the type of signature evaluated and the climate but overall it is proven difficult to infer complete model families from model output in this section we discuss two aspects in more detail our study setup also in relation to earlier work on model families and the implications of our results 4 1 study setup in this study we employed a novel combination of approaches to explore the potential of inferring model families from model output which eventually can be used to quantify model structure uncertainty there are three aspects of our study design that might have implications for our results the mmf choice the evaluated signatures and the statistical methods for hierarchical clustering mmf choice as mmf we employed the framework for understanding structural errors fuse this is only one of many mmfs that are available in hydrology these days craig et al 2020 knoben et al 2019 weber et al 2019 clark et al 2015 each mmf has a clear focus in terms of modelling philosophy and therefore only covers a limited part of model space the choice of the mmf does as such already determine which part of model space is explored the model components included in fuse are only representative for conceptual bucket based hydrological models focussing on the subsurface processes this limits the part of model space that can be sampled with fuse and is thus unrepresentative of the whole hydrological model space summa clark et al 2015 would have been more representative for the land surface part of model space and raven craig et al 2020 seems to be able to capture both conceptual and more physics based models a continuation of this work with a broader sample of model structures is therefore deemed highly valuable knutti et al 2013 performed the hierarchical clustering on the output of cmip5 climate models these models are much more complex than the suit of models employed in this study they validated their results by comparing some of the main building blocks of the climate models e g if they shared the same ocean model this was however not based on the actual model code leaving the option open that the same ocean model was incorporated differently in different climate models the limited range of model space covered by the fuse models in this study would probably be evaluated as one family compared to the cmip5 models open for discussion is how different models should be in order not to create a bias in the ensemble choice of signatures gunkel et al 2015 and gupta et al 2012 demonstrate that signatures can give insight into the capabilities and short comings of model structures which is in accordance with our results the choice of the signature to conduct the inference is very important some signatures could provide high bg coefficients discharge and evaporation based signatures while other signatures related to surface runoff gave consistently low bg coefficients many signatures are available and signatures should be carefully selected mcmillan et al 2017 however clear guidance on signature selection is still lacking addor et al 2018 our results demonstrate that the choice of the signatures should be based on to what extent they are influenced by or sensitive to multiple processes in the model structure knutti et al 2013 selected temperature and precipitation signatures to create their dendrograms for the cmip5 climate models for climate models these signatures are heavily influenced by interactions in the model so these signatures probably encompass the correct information to conduct the inference the greatest confidence in a signature can be obtained when the processes underlying a signature are well known and properly modelled knutti et al 2013 hence signature choice is an important aspect when inferring model structure families from model output statistical methods in this study we employed five different methods for both the distance metric and the hierarchical clustering we only presented the results of the euclidean distance metric and the ward d2 clustering method however the bg coefficient differed per combinations of statistical methods see appendix d in general three hierarchical clustering methods performed best complete ward d and ward d2 but this also related to the employed distance metric and the evaluated signature the euclidean distance metric and the ward d2 clustering method were representative of the general pattern because the euclidean method provided similar results as the manhattan and minkowski method the ward d2 method was one of the hierarchical clustering methods that performed consistently better this could however change if other signatures are employed the choice between the other distance metrics or hierarchical clustering methods depends on which signatures can be computed from the model output 4 2 on sampling model space whereas input data uncertainty can be derived from the accuracy of the measurement device or intercomparison of observation methods and parameter uncertainty can be approached by sampling parameter space within clearly defined parameter boundaries model structure uncertainty is more difficult to estimate and this has become even more challenging with increasing model complexity wagener and kollat 2007 ideally one would like to sample model space in the same way parameters can be sampled but model space could be infinitely large and actually a clear definition of what model space comprehends is lacking in this study we assumed model space as model hypothesis space where we consider the equations in hydrological models as hypotheses given the challenge of going through the codes of many different models we tested if model output could be representative of the underlying model hypotheses one could however also argue that model space is represented by model output space therefore a clarification is needed of what is meant by sampling model space does this concern model hypothesis space with different parameterisations that can lead to the same output or model output space with the output covering a wide range to have some idea of the uncertainty our argument to use model hypothesis space as the relevant space to sample is that this is more congruent with the way parameter uncertainty is treated and because model output space is the consequence of model hypothesis space combined with model parameter space and observation uncertainty in case of calibration making model hypothesis space a cleaner representation of model structure an implication of this viewpoint however is that our dendrogram based on model structures was solely based on the model equations with each equation receiving an equal weight some equations will however have relatively more influence on the model output than others which is represented by the model sensitivity the sensitivity of each modelling decision was not taken into account in the construction of the dendrograms and might partly explain the mismatch between model structure based dendrograms and signature based dendrograms yet it is challenging to account for sensitivity in the construction of model structure dendrograms firstly because it already requires analysis of model output and secondly because sensitivity will depend on parameter values the signature evaluated and the climate this would warrant detailed analysis of the separate model components and their influence on model results potentially resulting in flexible model structure families which can depend on the climate modelled catchment used signature and the modelling purpose one can also only be interested in one specific part of the model structure and choose the signature accordingly as the presented results showed the inference of complete model structure based on surface runoff signatures was not successful but it was possible to distinguish the three different surface runoff formulations among the 8 models using surface runoff based signatures as knutti et al 2013 explains model ensemble runs can be biased because some parameterisations are over represented in the models of the ensemble in their study it is assumed that they were able to infer model families from model output it is however also possible that different combinations of parameterisations lead to the same model output model equifinality this would distort the relation between the inferred model families and the model structure based families in our study we tested this inference and have to conclude that it is difficult to infer model structure from model output within our experimental set up of fuse models camels data set and chosen signatures therefore we might need other approaches to explore model space enemark et al 2019 provide a review of studies that explore several model concepts and suggest that for each conceptual model alternative models representing hypotheses that are mutually exclusive should be defined another potential approach could be a more comprehensive model classification based on this classification a sample of models could be selected as being representative for a part of model space 5 conclusion in this study we investigated if model structure families could be inferred from model output this could potentially provide useful information on how well model structure uncertainty is captured and whether any bias exists in the model ensemble we employed a novel combination of approaches to quantify the success of inferring model structure families from model output represented by signatures we show that some signatures do lead to successful inference discharge based signatures while others do not surface runoff based signatures combining signatures does in general not enhance the potential to infer model structure families from model output overall it was shown that it is challenging and in most cases impossible to infer model structure from model output for the part of model space bucket based hydrological models that we sampled therefore we have to continue searching for ways to capture model space in order to represent model structure uncertainty declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements for the statistical analysis we would like to thank joost van heerwaarden and ron wehrens for their expertise and time furthermore we would like to thank diana spieler for the insightful discussions we thank claudia vitolo and nans addor for their help in understanding fuse finally we thank nans addor wouter knoben and an anonymous reviewer for their thorough and constructive feedback appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2020 104817 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
25964,successful data integration requires careful examination of data semantics a task that has often been approached with the use of ontologies however there are some barriers to build ontologies for data integration in complex domains such as the environmental one a relevant problem is the development of new ontologies disregarding previous knowledge resources such as reference models and vocabularies this paper addresses this challenge by proposing a systematic approach dubbed clear for the identification and selection of reusable artifacts for building ontologies with the purpose of research data integration clear follows some principles of the systematic literature reviews supporting the search for structured resources in the scientific literature we apply clear to the environmental domain a total of 543 publications were surveyed the results obtained provide a set of 75 structured resources for the environmental domain evaluated according domain coverage and some quality attributes e g proper documentation community acceptance keywords data integration environmental research data knowledge resources reuse systematic search ontology 1 introduction scientific research is often a data centric endeavor involving the systematic collection interpretation and evaluation of scientific data çaparlar and dönmez 2016 in several domains scientific research comprises i the interaction between many actors such as academic institutions government agencies private companies and independent research groups ii carrying out research activities such as observation and measurement and iii the use of various nomenclatures and classification schemes types of materials collected types of properties observed etc in these settings scientific data is produced from a variety of sources in different contexts and for a variety of purposes as a consequence such data is produced in heterogeneous forms given the high costs involved in producing scientific data e g for environmental data science gibert et al 2018 it is no surprise that significant gains can be obtained from data sharing reuse and integration uhlir and schröder 2007 data integration demands strategies to deal with data heterogeneity whether in terms of syntax schema or semantics lenzerini 2002 syntactic heterogeneity occurs mainly due to the use of different serialization formats and technologies schematic heterogeneity occurs when data sources use different schemas with different structures to represent the same information finally semantic heterogeneity is caused by divergent interpretations of data according to the different contexts in which the same data can be used the semantic aspect which is the focus of this paper has been frequently approached with the use of ontologies rajpathak and chougule 2011 as presented in cruz and xiao 2005 gruber 1991 ontologies can be used among other possibilities as global or shared conceptualization for data integration in this sense ontologies can promote data interoperability by providing a common semantic background for data interpretation supporting meaning negotiation in the last decades several ontologies have been built for this purpose in some success cases they have become reference models reused by a large community e g the gene ontology proposed by ashburner et al 2000 has had a significant impact in the sharing of scientific knowledge about the functions of genes in other cases they have failed to establish de facto shareability and consequently to support data interoperability this failure may have many reasons a relevant one surfaces when new ontologies are developed disregarding previous knowledge resources i e any type of artifact that represents knowledge about a domain including ontologies and other kinds of reference models and representation schemes this creates new interoperability problems ambiguities and inconsistencies among existing ontologies thus reuse has becoming a common concern in the ontology engineering area uschold et al 1998 bontas et al 2005 some ontology engineering methodologies describe specific activities to deal with reuse suárez figueroa et al 2012 falbo 2014 leung et al 2011 despite that some challenges still need to be tackled to promote reuse the neon methodology suárez figueroa et al 2012 for example proposes eight scenarios for building ontologies from the reuse of previous knowledge resources however such methodology provides only generic guidelines for the search and selection of reusable knowledge resources since no other ontology engineering methodology consulted provides a systematic method for accomplishing these activities we realized the need to propose an approach to do so in a systematic way the approach is dubbed clear conducting literature search for artifact reuse and is based on some practices of the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 the search in the scientific literature becomes the basis for the identification of knowledge resources that jointly cover the domain and exhibit properties considered desirable for reuse proper documentation community acceptance among others in general clear activities consist of i defining data integration requirements ii finding reusable knowledge resources on the domain of interest and iii selecting some of the identified knowledge resources to be reused in the development of ontology for data integration purposes as clear addresses specific ontology engineering activities it is designed to be used as a complement to existing ontology engineering methodologies we have applied clear to the water quality domain a total of 543 publications were surveyed the results obtained provide a set of 75 knowledge resources on this domain this set of knowledge resources make up a knowledge base on the domain to be reused whenever necessary this justifies the effort employed the proposed work is not automated in performing the systematic search for a domain for the first time this work is inserted in a project entitled an escience infrastructure for water quality management in the doce river basin called henceforth doce river project for brevity this project is concerned with the integration of water quality data produced by various sources to assess the impacts of the mining disaster that occurred in the city of mariana in brazil in 2015 when the fundão tailings dam broke contaminating the doce river basin the paper is further structured as follows section 2 presents some background knowledge that supports our investigation on the development of an approach to search and select reusable knowledge resources for the integration of scientific research data section 3 describes the clear approach section 4 discusses the results of the application of clear to the environmental research domain finally section 5 presents the final considerations 2 background in this section we review ontology engineering methodologies gaps of existing methodologies related to reuse and the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 required for the development of this work 2 1 ontology engineering methodologies ontology engineering is formally defined as the set of activities that concern the ontology development process the ontology life cycle and the methodologies tools and languages for building ontologies gómez pérez et al 2010 ontology engineering methodologies provide guidelines for the development management and maintenance of ontologies such methodologies decompose the ontology engineering process in a number of steps and recommend activities and tasks to be performed for each one in addition they define the roles of the individuals and organizations involved in the ontology engineering process in general domain experts provide knowledge with respect to the domain to be modeled ontology engineers or ontology developers have expertise in fields such as knowledge representation and development tools and users apply the ontology for a particular purpose simperl et al 2009 in gómez pérez et al 2010 the authors differentiate three types of activities within an ontology engineering process management development and support activities the first covers the organizational setting of the overall process in particular at pre development time a feasibility study examines if an ontology based application or the use of an ontology in a given context is the right way to solve the problem at hand the second type of activities refers to classical activities such as domain analysis conceptualization and implementation but also maintenance and use which are performed at post development time ontology support activities such as knowledge acquisition evaluation reuse and documentation are performed in parallel to the development activities simperl et al 2009 a distinction between ontology engineering methodologies takes into account the strategy adopted for building ontologies that is building from scratch or building from existing knowledge resources soares 2009 examples of methodologies that address building ontologies from scratch can be found in soares 2009 examples of methodologies that describe specific activities for addressing reuse are the neon methodology suárez figueroa et al 2012 the systematic approach for building ontologies sabio falbo 2014 and the methodology of integration oriented ontology development miod leung et al 2011 2 1 1 reuse related gaps reuse is pointed out as a promising approach to ontology engineering since it enables speeding up the ontology development process saving time and money poveda villalón et al 2010 and avoids the unnecessary proliferation of new ontologies however there is a lack of concern with search and selection of reusable knowledge resources by the reuse oriented ontology engineering methodologies this is shown in salamon et al 2018 in which a systematic mapping was performed to provide the current panorama of ontology integration approaches the results reveal some problems among them a lack of concern with search and selection of the ontologies to be integrated among the reuse oriented methodologies some focus on the identification and the integration of existing knowledge resources e g neon suárez figueroa et al 2012 sabio falbo 2014 and miod leung et al 2011 in general they propose steps for the specification of ontology requirements for the identification of the knowledge resources to be reused for the integration of the knowledge resources reengineering alignment merging etc and for the evaluation of the resulting ontology ontology requirements are specified mainly in the form of competency questions cqs i e questions writing in natural language that the ontology should be able to answer gruninger and fox 1995 in turn the terms whose definition could be reusable from existing knowledge resources are those appearing in the ontology requirements specification ontology developers can locate knowledge resources in ontology libraries domain related sites resources within organizations and general purpose search engines some reuse oriented methodologies focus only on the identification of relevant knowledge resources e g shiang et al 2018 and blanco et al 2011 in shiang et al 2018 the authors propose an ontology pattern classification scheme to allow the reuse of existing ontology knowledge for multiagent systems development in blanco et al 2011 a systematic literature review is carried out to obtain security ontologies these ontologies are compared according to the evaluation framework proposed in tello and gómez pérez 2004 making it possible to identify the key requirements that an integrated security ontology should have other reuse oriented methodologies focus only on the integration of two or more knowledge resources e g stoilos et al 2018 crow and shadbolt 2001 they assume that knowledge resources are identified in a previous step besides that there are some ontology engineering methodologies focused on data integration but which do not address the reuse of knowledge resources this is the case of the methodology for development on data integration ontodi yunianta et al 2019 it proposes specific steps to identify data sources to be integrated and to correct semantic inconsistences between them despite proposing activities to identify and integrate existing knowledge resources neon suárez figueroa et al 2012 sabio falbo 2014 and miod leung et al 2011 do not show how to perform the search and record the search results regarding knowledge resources selection miod suggests some evaluation criteria for example quality of documentation and language used to implement the resource but does not show how to assess these criteria neon applies a subjective evaluation criterion that is the consensus about the knowledge and terminology used by the resource sabio does not describe how knowledge resources are to be selected in relation to the methodologies shiang et al 2018 and blanco et al 2011 they search for specific types of knowledge resources ontology patterns or ontologies or in specific domains security in their turn the methodologies proposed in stoilos et al 2018 and crow and shadbolt 2001 do not address the search for reusable knowledge resources finally ontodi yunianta et al 2019 does not address a step related to reuse 2 2 systematic literature review as we have discussed in the previous section there is explicit support for reuse in ontology engineering methodologies however they provide only generic guidelines for reusable knowledge resources search and selection activities this justifies a more systematic approach to perform them we draw inspiration for such approach from the practices of the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 is one of the main mechanisms that support evidence based research this research paradigm has been advocated as a good practice for decision making or troubleshooting in many areas such as medicine economics and software engineering an slr is a secondary study method based on evaluating and interpreting all available research relevant to a particular research question topic area or phenomenon of interest and then on reporting the methodology used and the results obtained although an slr requires considerable effort to be implemented when compared to ad hoc literature reviews slrs are auditable more trustworthy and rigorous an slr has three phases planning conducting and reporting the review kitchenham and charters 2007 in the planning phase the first step is to identify the need for the review that is the reason the review is being carried out then the review protocol is developed a review protocol specifies the methods that will be used to perform a specific slr it must contain the research questions that the review aims to answer the strategy to search for primary studies including search terms search string and search engines the criteria and procedures for selecting studies the checklist and procedures for assessing the quality of studies the strategy for extracting data and the strategy for the synthesis of extracted data the protocol is refined in the following phases but must be defined in planning to make it less likely that the results of the literature will be biased and further to make search assumptions explicit in the conduction phase the search is performed and the primary studies are retrieved next the selection criteria are applied to identify the studies that provide direct evidence about the research questions then the quality of the selected studies related to the extent to which the studies minimize bias and maximize internal and external validity is evaluated finally some data are extracted from the selected studies and synthesized in tables so that the meta analysis i e statistical techniques aimed at integrating the results of the primary studies can be performed in the reporting phase the main report with final results is prepared and evaluated to verify if the search need has been met kitchenham and charters 2007 as a way to enhance the quality of the search snowballing can be performed wohlin 2014 snowballing refers to using the reference list of a study or the citations to the study to identify additional studies and therefore increase coverage of relevant literature using the references and the citations respectively is referred to as backward and forward snowballing the studies obtained from the snowballing are analyzed in the same way that the studies returned directly by the search in this work slr is useful because we are interested in searching for reusable knowledge resources on a scientific research domain however we aim to investigate scientific literature and technical papers to find available knowledge resources in the domain of interest thus the slr planning conducting and reporting activities need to be adapted to accommodate this characteristic this is the subject of clear as discussed in section 3 3 the clear approach clear conducting literature search for artifact reuse is a systematic approach to find and select reusable knowledge resources here called structured resources for building ontologies with the purpose of scientific research data integration by structured resources we mean those that represent knowledge through the use of formal specification of concepts relations and properties as ontologies and also other types of artifacts that capture semantic value for the concerned domain such as reference models representation schemas knowledge base schemas database schemas data exchange formats metadata standards vocabularies and thesauri the proposed approach adopts some practices of the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 more specifically publications in a given domain are analyzed as a strategy for finding structured resources available on that domain this aims to increase the scope of the search and reduce the bias promoting the identification of structured resources that jointly cover the domain and exhibit properties considered desirable for reuse proper documentation available representation and community acceptance as a result the set of retrieved structured resources make up a knowledge base on the domain to be reused whenever necessary this justifies the effort employed in performing the systematic search for a domain for the first time clear addresses specific ontology engineering activities consequently it is designed to be used as a complement to existing ontology engineering methodologies for example when used together with neon suárez figueroa et al 2012 clear activities correspond to and replace neon s specification of ontology requirements search for reusable knowledge resources assessment of candidate knowledge resources and selection of knowledge resources the overview of clear activities is presented in the sequel 3 1 overview of clear activities clear is structured in three cycles as shown in fig 1 the activities of cycle i aim at defining the data integration requirements and the scope of the ontology to be developed these requirements are necessary to perform the activities of the other two cycles the activities of cycle ii aim at systematically identifying structured resources candidates to be reused in the development of the ontology based on the requirements defined in cycle i once identified the structured resources can be selected to be reused which is the goal of cycle iii the three cycles are intended to be executed in an iterative fashion in the same way the activities of each cycle itself should be visited iteratively as knowledge about the domain is gathered and requirements are refined new structured resources are identified and should be considered for reuse 3 2 cycle i data integration requirements definition the data integration requirements definition cycle i is composed of three activities a integration questions definition b data sources selection and c domain aspects identification in the first activity a top down analysis of the integration requirements is made through the definition of integration questions iqs iqs are questions about the research domain that can only be answered through the integration of different data sources lenzerini 2002 that is because the contents of data are different and or complementary to each other or because different views of the same content must be contrasted in the second activity the data sources needed to address the iqs are selected by ontology engineers and domain experts in the third activity of this cycle a bottom up analysis of the integration requirements is done by studying the selected data sources the analysis of data sources iqs and domain standards norms national and international standards guides etc combined with the knowledge of domain experts allows the ontology engineers to identify the domain aspects domain aspects are subjects of the domain that can be treated in a modular way they must be enough to represent the universe of discourse they can be related to activities actors and roles description characterization of researched entities and so on they are used in cycle ii to support the systematic search for structured resources and in cycle iii to guide the selection of structured resources found in cycle ii 3 2 1 integration questions definition in this activity a top down analysis of the integration requirements is made through the definition of integration questions iqs driven by the needs of domain experts as iqs are answered from the integration of different data sources some candidate data sources to be integrated are known to domain experts prior to the application of clear these data sources serve as input to the definition of iqs in turn iqs support the selection of the set of data sources to be integrated as will be seen below iqs are also used in the definition of the domain aspects besides that in the joint use of clear with ontology engineering methodologies iqs are broken down into competency questions thus they are used to define the ontology scope and also for the evaluation of the developed ontology since clear is iterative it allows the refinement of iqs throughout the process which can be done by adding grouping uncoupling and updating actions table 1 shows the inputs outputs and actors of this activity 3 2 2 data sources selection from iqs it is possible to define the final set of data sources selecting those that provide appropriate data to answer iqs the selected data sources will be integrated with the support of the ontology to be developed from the reuse of the discovered structured resources the selection of data sources can be challenging considering that i data producers may be many researchers government entities non profit organizations industry and laboratories and sometimes unknown ii data can be difficult to find and obtain due to organizational barriers and iii data can be large heterogeneous and of varying quality table 2 shows the inputs outputs and actors of this activity 3 2 3 domain aspects identification in this activity the domain aspects are identified for this one can use general questions to characterize a scientific research that needs to consume integrated data examples of these questions are how is scientific research done where when what is researched who is the agent or principal and why is scientific research done similarly to iqs domain aspects can be refined continuously by adding grouping uncoupling or updating actions it is important to note that the analysis of the selected data sources elements provides significant knowledge for the identification of domain aspects this is because our ultimate goal is to find structured resources to be reused in the development of ontologies for the integration of these data sources however as mentioned before data sources content can be large heterogeneous and of varying quality therefore care must be taken when analyzing data sources to identify domain aspects this involves correlating different terms used to represent the same concept understanding the different granularities used to represent data and verifying the meaning of the absence of data when not justified this should be done with the support of the domain experts table 3 shows the inputs outputs and actors of this activity 3 3 cycle ii structured resources systematic search in clear the planning activity is called a systematic search configuration the conducting activity is divided into three b publications selection c structured resources identification and d snowballing the reporting activity is called e systematic search reporting they are performed by ontology engineers who are interested in finding structured resources to improve their work in systematic search configuration the strategy required to perform the search is defined steps such as the specification of the search goals and the definition of inclusion and exclusion criteria are executed in publications selection the systematic search for publications is performed the returned publications are analyzed and selected by applying the inclusion and exclusion criteria of publications after the publications selection the structured resources presented or mentioned by the selected publications are analyzed and selected by applying the inclusion and exclusion criteria of structured resources this is done in the structured resources identification activity to enhance the quality of the search the snowballing activity can be performed the snowballing technique wohlin 2014 can be applied to both publications and structured resources as a result of these activities we have the sets of identified and selected publications and structured resources finally in systematic search reporting the results of the systematic search are presented and evaluated to verify if the search goals were reached 3 3 1 systematic search configuration in systematic search configuration the following steps are executed specification of the search goals which concerns ultimately the identification of structured resources in the particular research domain selection of keywords to compose the search string elaboration of the search string selection of search engines definition of inclusion and exclusion criteria whose purpose is to select only publications and structured resources that meet the search goals definition of the publications selection procedure definition of the structured resources identification procedure and definition of the snowballing procedure in clear the selection of keywords reflects the dual nature of the search goals thus keywords represent not only the domain but also the types of structured resources to be found ontologies reference models database schemas etc in addition there are two different types of inclusion and exclusion criteria one for publications the other for structured resources the eight steps of this activity are explained below search goals specification in this first step the search goals are specified to guide systematic search activities they must be related to the structured resources to be searched keywords selection in this step the terms to compose the search string are selected once we are searching for structured resources on a specific domain we need to define some keywords related to structured resources and others related to the domain to make reference to structured resources terms such as ontology reference model vocabulary taxonomy and their related terms must be considered regarding the domain keywords that depict the domain itself the super domain i e a domain more generic than ours or the domain aspects should be used the domain related terms are obtained from discussions with domain experts glossaries prepared by them domain standards and domain aspects when they are used search string improvement the terms obtained in the previous step are organized in a search string this string should group the keywords into a logical expression typically using or and operators in clear the expression is formed by two main terms connected by and the first one selects publications concerned with structured resources and the second one selects domain specific publications each of these main terms is disjunctive in order to include alternative terms that are used to denote structured resources and to identify the research domain the search string is tested gradually including terms subsequently in the disjunctions in order to test whether they actually increase the search results and should be kept in the string search engines selection after the search string was constructed the search engines to be used need to be selected they include digital libraries specific journals and conference proceedings as recommended by kitchenham and charters 2007 checking search engines results against lists of already known primary studies called here control papers can be useful for selection of the search engines kitchenham and charters 2007 inclusion and exclusion criteria definition in this step the criteria to select inclusion or discard exclusion publications and structured resources obtained by the systematic search are defined then only those that directly reach the search goals are maintained for publications a general inclusion criteria recommended by clear is that the publications must present or mention structured resources about the domain or an aspect of it other inclusion criteria could be language journal authors setting participants or subjects research design sampling method and date of publication kitchenham and charters 2007 for structured resources an inclusion criteria proposed by clear is that they must address the domain or its aspects as exclusion criteria both for publications and structured resources we can check their availability that is publications and structured resources whose content is not fully available must be excluded publications selection procedure definition in this step the process to be followed for the publications selection is defined initially one must determine the scope of the search that is if the string terms will be searched only in title abstract or any part of the publications secondly one must define data to be registered about the studied publications and the form for example a spreadsheet to be used to record them regarding publications data it is necessary to register the year the title the authors and the source structured resources identification procedure definition in this step the process to be followed for the structured resources identification is defined one must define data to be registered about the studied structured resources and the form to be used to record them in relation to the structured resources data it is necessary to register the name the source the language used to build the resource such as ontology web language owl extensible markup language xml and unified modeling language uml the owner the description the key concepts the upper level ontology applicable only to ontologies the resources that reuse the structured resource the selected publications that present the structured resource and the selected publications that mention the structured resource snowballing procedure definition in this step the process to be followed for the snowballing application is defined in the case of publications it can be used in the same way as in the slr that is by checking the reference lists and citations of selected publications in the case of structured resources it selects structured resources that are reused by each one analyzed table 4 shows the inputs outputs and actors of the systematic search configuration 3 3 2 publications selection in this activity the process defined in publications selection procedure definition is performed the search engines are configured according to the search scope and some inclusion and exclusion criteria such as the publication language journal authors and date of publication then the search is performed the returned publications data are recorded in the publications form publications are analyzed and selected by applying the inclusion and exclusion criteria of publications table 5 shows the inputs outputs and actors of this activity 3 3 3 structured resources identification after the publications selection the process defined in structured resources identification procedure definition is performed the structured resources presented or mentioned by the selected publications are identified the structured resources data are recorded in the structured resources form structured resources are analyzed and selected by applying the inclusion and exclusion criteria of structured resources table 6 shows the inputs outputs and actors of this activity 3 3 4 snowballing in this activity the process defined in snowballing procedure definition is performed the new publications and structured resources data are recorded on the corresponding forms new publications and structured resources are analyzed and selected by applying the respective inclusion and exclusion criteria table 7 shows the inputs outputs and actors of this activity 3 3 5 systematic search reporting in this activity the results of the systematic search are presented and evaluated to verify if the search goals were reached this is done by analyzing including graphically some of the information collected about publications and structured resources such as the language used to build the resources the number of publications that mention the resources and the number of resources that reuse them this is useful in evaluating the quality attributes of the structured resources performed in cycle iii as it will be presented below table 8 shows the inputs outputs and actors of this activity 3 4 cycle iii structured resources selection the final structured resources selection cycle iii is composed of three activities a structured resources analysis b structured resources classification and c structured resources evaluation in the first activity the structured resources identified in cycle ii are assessed by verifying domain coverage and key quality attributes for reuse proper documentation available representation and community acceptance this allows the classification of the structured resources in the second activity finally in the third activity the best classified structured resources are evaluated according to their suitability for the representation of existing data as a final result we have the selected structured resources to be reused in addition we have a set of relevant structured resources in the research domain classified according to domain coverage and quality attributes 3 4 1 structured resources analysis domain coverage analysis domain coverage is analyzed based on the domain aspects this can be verified by checking whether or not a domain aspect is covered by structured resources or indicating a degree of coverage the domain coverage provides a relevant criterion for making decisions about structured resources reuse for example considering the first option it is verified that each structured resource covers a subset of the domain aspects set identified in cycle i thus if a domain aspect is covered by only one structured resource this contributes for deciding to select it for reuse on the other hand if the domain aspects covered by a structured resource are a subset of the domain aspects set covered by another resource this may indicate that the second is a better choice than the first in clear the domain coverage analysis is performed by means of a matrix as shown in table 9 each row of the matrix refers to a structured resource and each column refers to a domain aspect if a domain aspect is covered by a structured resource the corresponding cell of the matrix must be checked the domain aspects are grouped according to the questions that answer to characterize a scientific research the total of domain aspects covered and the total of domain aspects covered in each group by the structured resources are computed quality attributes analysis the quality analysis supports the choice of the structured resources since it differentiates resources that have similar domain coverage relevant quality attributes for reuse include reuse economic cost need to acquire a use license etc understandability effort e g quality of the documentation code clarity integration effort modularization language used etc and reliability e g development team reputation popularity suárez figueroa et al 2012 clear adopts the following quality attributes proper documentation available representation and community acceptance we have prioritized those attributes as they can be evaluated objectively as discussed in the sequel other attributes may be added if deemed appropriate proper documentation it refers to the availability of documentation to facilitate the understanding of structured resources concepts relationships and properties and as consequence to enable their proper use we check the availability of glossaries and examples of instantiation glossaries explain the meaning intended for the concepts that compose the structured resources examples of instantiation allow us to understand what is or is not an instance of concepts available representation it is related to the availability of a conceptual graphical model and the availability of a computational representation both of which are desirable the first one is because it promotes a clear and precise description of domain entities for the purposes of communication learning and problem solving through the creation of a conceptual model that describes the solution to a problem the second one is because it provides a machine readable implementation version of the structured resource we have used the language used to build the structured resources mapped in cycle ii to help in this analysis community acceptance this is about a structured resource being considered a domain standard this can be verified through metrics that show how well it is recognized and used by the community to assess how much a structured resource is recognized and reused by the community we use the number of publications that mention the structured resource and the number of resources that reuse it respectively we consider as mentioned or reused the resources that obtained at least 50 of the maximum number of mentions or reuse this is to disregard little mentioned or reused structured resources the quality attribute analysis is performed by means of a matrix as shown in table 10 each row of the matrix refers to a structured resource and each column refers to a quality attribute if a structured resource ranks positively in a quality attribute the corresponding cell in the matrix must be checked the quantity of quality attributes in which a structured resource is positively classified is calculated in the quality attributes score column table 11 shows the inputs outputs and actors of this activity 3 4 2 structured resources classification in this activity the structured resources are classified in each domain aspects group thus those most appropriate to treat the domain aspects of each group are identified for this a final score is computed based on the total of domain aspects covered in each group by the structured resources and their quality attributes score initially these values must be normalized in the 0 1 interval then the arithmetic or weighted average of the normalized values is calculated the structured resources are classified in each group according to this average table 12 shows the inputs outputs and actors of this activity 3 4 3 structured resources evaluation in this activity the best ranked structured resources in each aspects group are selected and evaluated to verify their suitability for the representation of different domain data this evaluation is performed trying to annotate each element of the data sources selected in cycle i with the concepts classes properties and instances made available by each structured resource as the structured resources are evaluated they are selected or discarded if discarded because they do not properly represent the elements of the target aspects group the next resources in the classification should be evaluated at the end of this activity we have a set of complementary structured resources to be reused in addition we have a set of relevant structured resources in the research domain classified according to domain coverage and quality attributes table 13 shows the inputs outputs and actors of this activity 4 applying clear to the water quality domain in this section we apply the clear approach to the water quality domain in the context of the doce river project the objective is to find structured resources to be reused in the development of an ontology for the integration of water quality data the work was carried out by two domain experts and two ontology engineers over a period of 2 months cycle i and cycle ii activities took approximately 2 weeks each and cycle iii activities took approximately 1 month the most time consuming step is the structured resources analysis in cycle iii as it is necessary to study each of the identified structured resources to verify the domain coverage and quality attributes the domain experts are researchers in the areas of geochemistry and aquatic biodiversity the ontology engineers already had knowledge about the water quality domain before applying the approach which reduced the time required to study publications and structured resources it is worth mentioning that the time required for applying clear depends directly on the number of publications and structured resources to be analyzed as well as the size of the structured resources and the quality of documentation available on them in turn the number of publications and structured resources to be analyzed is driven by the requirements specified in cycle i that is iqs more generic or specific data sources to be integrated and domain aspects 4 1 definition of the water quality data integration requirements in this section we present the application of the cycle i of clear to the water quality domain a key aspect of this cycle is the participation of domain experts who are knowledgeable of data semantics and who face themselves integration questions in their research activities 4 1 1 integration questions for the water quality domain a non exhaustive list of iqs defined by domain experts is shown in table 14 as one can observe these questions are related to the assessment of water quality at monitoring points along the doce river and its tributaries they concern not only the impacts of the disaster but also water quality in general these questions could be answered by analyzing the measurements of the physical chemical and biological properties of the water and sediment samples and the ecotoxicological essays carried out by different brazilian organizations 4 1 2 data sources to be integrated the data sources needed to address the iqs are provided by various brazilian governmental and non governmental organizations among the governmental ones there are those that cover the national territory and those that cover the states of minas gerais and espírito santo bathed by the doce river and impacted by the disaster the national governmental organizations selected are the national water agency ana ana 2019 the geological survey of brazil cprm cprm 2019 and the brazilian institute for the environment and renewable natural resources ibama ibama 2019 the state level governmental organizations selected are the water management institute of minas gerais igam igam 2019 and the institute of environment and water resources of espírito santo iema iema 2019 the non governmental organization selected is renova foundation renova 2019 that is the entity responsible for the mobilization to repair damages caused by the rupture of the fundão dam in mariana mg 4 1 3 water quality domain aspects from the iqs presented in table 14 it is possible to extract many domain aspects that answer the general questions used to characterize a scientific research some of them are water sampling water quality analysis water quality measurement and water quality monitoring how water quality properties parameters and meteorological aspects what location where and normative element why for example the normative element domain aspect which defines water quality and motivates water sampling water quality analysis etc was obtained from iq03 and iq11 iq03 mentions the applicable legislation for freshwater and iq11 mentions the metal levels thresholds adopted by environmental agencies table 15 was extracted from the weekly water quality bulletin 04 feb 2019 obtained at the renova foundation website renova 2019 for each element of this table we have identified a domain aspect provenance renova foundation geographical entities water courses chemical physical and biological properties of water presence of cyanobacteria electric conductivity dissolved oxygen and ph meteorological aspects rain of the period units of measurement μg l μs cm mg l and mm sensors used telemetric stations reference to norms 357 2005 conama resolution conama 2005 and compliance table 16 presents an analysis of data source elements in two of the data sources we considered ibama iema and igam for each data source element usually a column name in tabular data provided by a data source we have identified a domain aspect domain aspects group elements that deal with related concepts the identified domain aspects are provenance ibama iema or igam geographic coordinates altitude latitude etc geographical entities hydrographic basin sub basin water course among others location e g site county station temporal references date year etc sampling which encompasses other aspects such as sampling method inferred from the concept of sample type and material entity inferred from the concept of sample point category measurement which contain more specific aspects such as chemical physical and biological properties e g alkalinity of bicarbonates units of measurement mgcaco3 l and measurement agent data source as well as normative elements framing class of water course note that different data sources cover the same domain aspect with different representation schemes the analysis of the iqs the domain standards e g rice et al 2017 and the selected data sources elements resulted in the following list of the water quality domain aspects research activity sampling preparation measurement analysis monitoring sampling method preparation method measurement method analysis method and monitoring method how location geographic coordinates and geographic entity where material entity abiotic entity biotic entity properties chemical property physical property biological property unit of measurement and meteorological aspects what temporal references when agent sensor and provenance who normative elements why these aspects together establish the required coverage of the ontology to be developed 4 2 systematic search for structured resources on the water quality domain next we present the application of the cycle ii of clear to the water quality domain it consists in the systematic search for structured resources on this domain 4 2 1 configuring the systematic search the following search goal was formulated for the water quality domain find structured resources candidates to be reused in the development of ontologies for data integration in the water quality domain identify the structured resources the language in which they are represented the location where they are available the key concepts addressed by them and the resource owner among the keywords related to structured resources we have used ontology and vocabulary related terms so that publications containing structured vocabularies and taxonomies were also identified see table 17 for alternative terms with respect to the terms related to domain besides water quality itself and its alternative terms the super domain environmental quality was included to make it possible to carry out a wider search see table 18 the final string obtained is presented below ontology or vocabulary or reference model or knowledge base or schema or taxonomy or thesaurus and water quality or water resource or environmental quality or water evaluation or water analysis or water monitoring or water assessment or environmental resource or environmental evaluation or environmental analysis or environmental monitoring or environmental assessment or environment quality or environment resource or environment evaluation or environment analysis or environment monitoring or environment assessment the control papers cp used to aid in the selection of the search engines are listed in table 19 they were chosen based on a non systematic search campos et al 2018 in which it was possible to find publications that propose structured resources suited for the representation of the water quality domain we selected google scholar as the search engine for our systematic search because google scholar retrieves technical works in the domain of interest presented at domain specific conferences as well as scientific papers unlike other digital libraries engineering village scopus and ieee explore the google scholar search retrieves all three control papers the publications inclusion pic and exclusion criteria pec are shown in table 20 and the structured resources inclusion sric and exclusion criteria srec are shown in table 21 pic01 is directly related to the search goal pic02 is used to select only publications globally recognized and pec01 is used to discard unavailable publications sric01 is used to select only structured resources that address the water quality domain srec01 is used to discard structured resources that are also unavailable because they have been discontinued or because they have not been made available to broaden the scope of the search it was decided to apply snowballing on the reference lists and citations of the selected publications and on the structured resources reused by those selected 4 2 2 selecting publications in relation to the search scope we decided to look for the keywords in the paper title for pragmatic reasons in this case we note that even while searching the title the relevant publications were returned one way to verify that relevant publications have not been left out is to check if the systematic search returns publications found by previously non systematic searches we verify that the publications found by the non systematic search presented in campos et al 2018 which propose structured resources suited for the representation of the water quality domain were returned by the systematic search thus the search scope was configured in the google scholar besides that the option to search only publications written in english was checked in the google scholar to meet the inclusion criteria pic02 the systematic search was performed on the june 21st 2019 the publications returned were analyzed and selected by applying pic01 and pec01 in total 64 publications were obtained after applying the inclusion and exclusion criteria 18 were selected publication data can be found in the publications selection table of the dataset campos et al 2020 provided with this work 4 2 3 identifying structured resources the structured resources extracted from selected publications were analyzed and selected by applying sric01 and srec01 in total 57 structured resources were obtained after applying the inclusion and exclusion criteria 44 were selected structured resource data can be found in the structured resources identification table of the dataset campos et al 2020 4 2 4 applying snowballing the application of snowballing on the reference lists and citations of the selected publications resulted in 479 new publications after applying the publications inclusion and exclusion criteria to them 67 were selected for better organization new publications were listed in the new tables reference lists selection and citations selection with the same structure as the publications selection table of the dataset campos et al 2020 the analysis of the new publications resulted in 34 new structured resources after applying the structured resources inclusion and exclusion criteria to them 25 were selected in addition the application of snowballing on the resources reused by the 60 selected structured resources resulted in 22 new structured resources after applying the inclusion and exclusion criteria to them 6 were selected all structured resources were identified in structured resources identification table of the dataset campos et al 2020 at the end of the systematic search 85 publications were selected from a total of 543 analyzed publications also 75 structured resources were selected as candidates for reuse from a total of 113 identified structured resources the analysis of publications and structured resources was divided among ontology engineers which reviewed each other s work divergences in analysis were discussed and resolved in meetings 4 2 5 reporting the results of the systematic search as previously discussed the systematic search returned a total of 543 publications of which 85 15 7 were selected for presenting or mentioning structured resources about the water quality domain or part of it among the discarded publications 458 publications 346 publications 75 5 did not meet inclusion criteria pic01 15 3 3 did not meet inclusion criteria pic02 and 97 publications 21 2 met exclusion criteria pec01 this means that most publications were discarded because they did not present or mention a structured resource on the domain of interest that is they did not meet the systematic search goal regarding the structured resources a total of 113 structured resources were obtained counting those extracted from publications and those reused by other resources among them 75 were selected as candidates for reuse and 38 were discarded among the 38 structured resources discarded 20 52 6 did not meet inclusion criteria sric01 and 18 47 4 met exclusion criteria srec01 several links provided by publications were broken in some cases it was possible to find them elsewhere but in cases in which it was not possible structured resources were excluded according to srec01 with respect to data extracted about the selected structured resources we analyze the language used to build the resources the number of publications that mention these resources not including the papers that present them and the number of resources that reuse them such data is used in cycle iii to evaluate the quality attributes of the structured resources the key concepts treated by the structured resources are also used in cycle iii to verify the coverage of the domain by each of them regarding the language we have found certain convergence ontology web language owl is used by 38 9 of the structured resources found while schemas written in resource description framework rdf and extensible markup language xml have reached 22 2 only 8 3 use unified modeling language uml 6 5 use hypertext markup language html in this case structured links and 24 1 use other languages for this analysis see graph of fig 2 resources have been counted more than once according to the number of languages in which they are made available the language is used to verify the quality attributes related to the representation level of each structured resource in cycle iii the number of publications that mention a structured resource can be used to measure how well it is recognized by the community in cycle iii as shown in the graph of fig 3 two structured resources semantic sensor network ssn ontology compton et al 2012 and semantic web for earth and environmental terminology sweet ontologies raskin and pan 2005 are mentioned by fourteen publications one structured resource the observations and measurements o m conceptual model iso 2011 is mentioned by thirteen publications one resource the chemical entities of biological interest chebi ontology hastings et al 2016 is mentioned by ten publications two resources time ontology in owl owl time cox and little 2017 and quantity unit dimension and type qudt ontologies hodgson et al 2014 by nine publications and one resource water markup language waterml zaslavsky et al 2007 by five publications 18 7 of the resources are mentioned by three publications 25 3 of the resources are mentioned by two publications and 26 7 of the resources by one publication 20 0 of the structured resources were identified only from the publication that presents them or from the resources that reuse them they are not mentioned by other publications the number of resources that reuse a structured resource represents how much it is used by the community regarding the number of resources that reuse a structured resource the graph of fig 4 shows that one structured resource o m iso 2011 is reused by twelve resources one structured resource geography markup language gml iso 2007 is reused by eight resources one structured resource ssn compton et al 2012 is reused by seven resources one structured resource the standard geographic information geomatics iso tc 211 tom and roswell 2009 is reused by six resources one structured resource owl time cox and little 2017 is reused by five resources and one structured resource sweet raskin and pan 2005 is reused by four resources 2 7 of the structured resources are reused by three resources 8 0 are reused by two resources 34 6 are reused by one resource and 46 7 are not reused by any of the other selected resources in relation to the last two graphs we verify that the structured resources were mentioned or reused by groups different from those that created them in addition we disregard the publications that present the structured resources in the analysis performed in the graph of fig 3 this is to ensure that the structured resources are recognized and reused by the community and not just by the group that have created them 4 3 selection of the structured resources on the water quality domain in this section the application of the cycle iii of clear to the water quality domain is discussed 4 3 1 analyzing the structured resources table 22 shows the domain coverage analysis for the selected structured resources the complete analysis was recorded in the structured resources selection table of the dataset which includes citations to all of the resources campos et al 2020 in table 22 to improve the view of the domain coverage by groups the columns of the domain aspects that make up each group were painted with the same color the structured resources were ordered by the total of domain aspects covered by them from largest to smallest the structured resources positioned at the beginning of table 22 address a greater number of domain aspects than the others they deal with domain aspects contained in most groups tending to be more generic e g united states geological survey usgs thesaurus usgs 2019 infrastructure for spatial information in europe inspire inspire 2019 and sweet raskin and pan 2005 the structured resources positioned at the end cover a smaller number of domain aspects contained in one or two groups thus they tend to be more specific as examples we can mention geonames 2019 and geosparql perry and herring 2012 where owl time cox and little 2017 when and qudt hodgson et al 2014 and chebi hastings et al 2016 what we do not identify structured resources that cover only domain aspects of how who or why groups table 23 shows the quality attributes analysis for the selected structured resources the ordering used for table 22 was maintained to facilitate the identification of the structured resources and the comparison of the two tables this analysis was recorded in the structured resources selection table of the dataset campos et al 2020 from table 23 it can be verified that only two structured resources o m iso 2011 and ssn compton et al 2012 rank positively in all 6 quality attributes two structured resources qudt hodgson et al 2014 and owl time cox and little 2017 in 5 quality attributes 24 0 of the structured resources in 4 quality attributes 16 0 in 3 quality attributes 30 7 in 2 quality attributes and 24 0 in 1 quality attribute 45 3 of the structured resources rank positively in 3 or more quality attributes which favors the reuse of them 4 3 2 classifying the structured resources for the water quality domain we calculated the arithmetic average of the normalized values of domain aspects covered in each group by the structured resources and their quality attributes score to compute the final score the classification was recorded in the structured resources classification table of the dataset campos et al 2020 table 24 shows the ranking for the top 10 structured resources from each group in some cases the number of structured resources presented is greater than 10 because more resources were tied in the same position as one can observe some structured resources appear well classified in all or most of the aspects groups this is the case of inspire inspire 2019 well classified in the 6 groups iso tc 211 tom and roswell 2009 and united states geological survey hydrologic markup language usgshydroml bermudez and piasecki 2003 well classified into 5 groups and o m iso 2011 and sweet raskin and pan 2005 well classified into 4 groups 4 3 3 evaluating the structured resources we selected 75 elements from five data sources identified in cycle i to be annotated with the structured resources the data providers are ana ana 2019 ibama ibama 2019 and iema iema 2019 igam igam 2019 cprm cprm 2019 and renova foundation renova 2019 the first structured resource evaluated was the inspire inspire 2019 since it ranked well in all aspects groups in its evaluation 59 of the 75 data sources elements 78 7 were properly represented this number indicates that inspire is indeed an artifact to be reused it is important that 14 23 7 of the 59 data sources elements were represented by other structured resources reused by inspire 12 from o m iso 2011 and 2 from iso tc 2011 tom and roswell 2009 also confirming the good positioning of these resources about the other 16 concepts 21 3 they are relative to the physical chemical and biological properties used for water quality measurements we choose not to represent them with inspire because it treats them very generically to represent them we selected qudt hodgson et al 2014 and environment ontology envo envo 2019 well classified in the what group qudt represents each of the properties and units of measure used by the data sources envo represents the chemical entities it is also important to note that envo represents the chemical entities through chebi hastings et al 2016 another resource identified in cycle ii but not ranked so well in the what group because it is focused narrowly on chemical entities this evaluation is available in the structured resources evaluation table of the dataset campos et al 2020 table 25 shows part of this evaluation focusing on data elements presented in table 16 of this work table 25 contains the data source which indicates the provenance of data the data source element to be annotated the structured resource that provides the proper representation to the data source element and the structured resource concept property and instance that can be used to represent the data source element for example in the second row of igam we have the data source element hydrographic basin inspire provides the concept riverbasin with the property geographicalname to represent it another example can be seen in the last row of ibama iema that contains the element alkalinity of bicarbonates mgcaco3 l the instance concentration of the concept chemistryquantitykind of qudt is used to represent the chemical property the concept calcium carbonate of envo chebi is used to represent the chemical entity caco3 the instance milligram liter of the concept unit of qudt is used to represent the unit of measurement and the concept quantityvalue of qudt is used to represent the measured value for this chemical property in the evaluation performed we were able to represent all elements of the data sources identified in cycle i with 6 of the structured resources identified in cycle ii inspire o m iso tc 2011 qudt envo and chebi these resources are complementary to each other with inspire offering broad coverage of domain aspects and the other resources covering some aspects in depth 5 final considerations in this paper we have presented clear an approach inspired by systematic literature review practices to find reusable structured resources about a scientific research domain clear can be used with existing reuse oriented ontology engineering methodologies for example neon suárez figueroa et al 2012 and miod leung et al 2011 to support the search and selection of reusable knowledge resources clear cycle i corresponds to the activity of ontology requirements specification of ontology engineering methodologies in turn clear cycles ii and iii correspond to the knowledge resources identification the structured resources selected from the application of clear to a domain serve as input for the next activity of ontology engineering methodologies the integration of the reusable knowledge resources in addition the set of iqs identified can be used to evaluate the resulting ontology in the same way that cqs are used by neon and miod the main advantage of using clear is that it supports the identification of reusable knowledge resources in a systematic fashion which is not addressed by existing ontology engineering methodologies another advantage is that it proposes the evaluation of reusable knowledge resources based on objective quality attributes a feature not present in existing ontology engineering methodologies in addition clear is aligned to the needs of ontology building for the purpose of scientific research data integration with ontology requirements derived from iqs and data to be integrated a disadvantage of clear is the effort required for its application to a domain in the first iteration however once applied to a particular domain clear provides a set of evaluated and classified structured resources that can be reused whenever new needs about such domain arise we argue that this result justifies the effort employed it is important to state that the set of structured resources returned by applying clear to a given domain depends on the requirements specified in cycle i if iqs data sources and domain aspects are changed another set of structured resources can be obtained as result in any case to build ontologies that need to address similar domain aspects the same set of structured resources can be used even though iqs and data sources are different here we have reported the application of clear to the water quality domain we focused on finding structured resources to be reused for the integration of water quality data a set of 75 structured resources candidates to be reused were obtained these knowledge resources were analyzed according to the domain coverage and the quality attributes proper documentation available representation and community acceptance and classified based on this assessment in the evaluation performed 6 of the structured resources were able to jointly represent all elements of the data sources to be integrated these structured resources were selected to be reused in campos 2019 some of us report the use of clear together with neon to build an ontology for the water quality domain using these 6 structured resources as they differ from each other and cannot be integrated into their original format a foundational ontology was employed in the analysis and reengineering of them most of the concepts represented by the designed ontology 42 out of a total of 78 concepts i e 53 8 were reused from the knowledge resources selected this evidences the fruitfulness of clear in promoting reuse the set of 75 structured resources resulting from the application of clear to the water quality domain is available in campos et al 2020 and provides an important knowledge base that can be reused thus people who need to build ontologies for the water quality domain or environmental domain with similar domain aspects can consult it saving the effort and time required to perform the systematic search and the assessment of the structured resources on this domain in a previous work see campos et al 2018 we have conducted a non systematic search for structured resources about the water quality domain this search resulted in a set of 11 reusable knowledge resources some were already known to us others were obtained from the analysis of various publications that we could identify as can be seen the number of structured resources obtained from the application of clear is considerably higher than that obtained from the non systematic search it is important to mention that two of the knowledge resources identified by the non systematic search namely ontobio albuquerque et al 2015 and m opl barcellos et al 2014 were not returned by clear ontobio was published in portuguese therefore it does not meet inclusion criteria and m opl addresses a more general issue measurements in general not specifically targeted at the environmental quality domain when comparing the approaches we observe that the application of a systematic approach guides the search and broadens the scope of results moreover we realize that clear facilitates discovery of important initiatives and working groups in the field of interest among the difficulties encountered in performing this work we can mention the bureaucracy faced to obtain data to be integrated in many cases such data is not available online thus it was in many cases necessary to contact each provider for access another difficulty identified was the lack of documentation or examples of use of some reusable structured resources documentation and examples are essential for the activities of verifying domain coverage understanding the knowledge resources and aligning them with a foundational ontology if they are not available the effort to carry out these activities which is not small increases considerably finally as future work we can consider evaluating the degree of coverage of domain aspects not covered covered largely covered and fully covered rather than just whether or not they are covered by knowledge resources we can also look for new quality attributes to be evaluated for the classification and selection of existing knowledge resources besides that we can study the automation of some steps of clear to reduce the effort required to apply it as examples we can try to automate the application of the inclusion and exclusion criteria and the extraction of data from publications and structured resources we can also try automating the domain coverage analysis and the quality attributes analysis as these steps are the most time consuming and this would greatly reduce the effort of applying the approach declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is funded by the brazilian coordination for the improvement of higher education personnel capes grant number 23038 028816 2016 41 and the brazilian national council for scientific and technological development cnpq grant numbers 312123 2017 5 and 407235 2017 5 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104813 
25964,successful data integration requires careful examination of data semantics a task that has often been approached with the use of ontologies however there are some barriers to build ontologies for data integration in complex domains such as the environmental one a relevant problem is the development of new ontologies disregarding previous knowledge resources such as reference models and vocabularies this paper addresses this challenge by proposing a systematic approach dubbed clear for the identification and selection of reusable artifacts for building ontologies with the purpose of research data integration clear follows some principles of the systematic literature reviews supporting the search for structured resources in the scientific literature we apply clear to the environmental domain a total of 543 publications were surveyed the results obtained provide a set of 75 structured resources for the environmental domain evaluated according domain coverage and some quality attributes e g proper documentation community acceptance keywords data integration environmental research data knowledge resources reuse systematic search ontology 1 introduction scientific research is often a data centric endeavor involving the systematic collection interpretation and evaluation of scientific data çaparlar and dönmez 2016 in several domains scientific research comprises i the interaction between many actors such as academic institutions government agencies private companies and independent research groups ii carrying out research activities such as observation and measurement and iii the use of various nomenclatures and classification schemes types of materials collected types of properties observed etc in these settings scientific data is produced from a variety of sources in different contexts and for a variety of purposes as a consequence such data is produced in heterogeneous forms given the high costs involved in producing scientific data e g for environmental data science gibert et al 2018 it is no surprise that significant gains can be obtained from data sharing reuse and integration uhlir and schröder 2007 data integration demands strategies to deal with data heterogeneity whether in terms of syntax schema or semantics lenzerini 2002 syntactic heterogeneity occurs mainly due to the use of different serialization formats and technologies schematic heterogeneity occurs when data sources use different schemas with different structures to represent the same information finally semantic heterogeneity is caused by divergent interpretations of data according to the different contexts in which the same data can be used the semantic aspect which is the focus of this paper has been frequently approached with the use of ontologies rajpathak and chougule 2011 as presented in cruz and xiao 2005 gruber 1991 ontologies can be used among other possibilities as global or shared conceptualization for data integration in this sense ontologies can promote data interoperability by providing a common semantic background for data interpretation supporting meaning negotiation in the last decades several ontologies have been built for this purpose in some success cases they have become reference models reused by a large community e g the gene ontology proposed by ashburner et al 2000 has had a significant impact in the sharing of scientific knowledge about the functions of genes in other cases they have failed to establish de facto shareability and consequently to support data interoperability this failure may have many reasons a relevant one surfaces when new ontologies are developed disregarding previous knowledge resources i e any type of artifact that represents knowledge about a domain including ontologies and other kinds of reference models and representation schemes this creates new interoperability problems ambiguities and inconsistencies among existing ontologies thus reuse has becoming a common concern in the ontology engineering area uschold et al 1998 bontas et al 2005 some ontology engineering methodologies describe specific activities to deal with reuse suárez figueroa et al 2012 falbo 2014 leung et al 2011 despite that some challenges still need to be tackled to promote reuse the neon methodology suárez figueroa et al 2012 for example proposes eight scenarios for building ontologies from the reuse of previous knowledge resources however such methodology provides only generic guidelines for the search and selection of reusable knowledge resources since no other ontology engineering methodology consulted provides a systematic method for accomplishing these activities we realized the need to propose an approach to do so in a systematic way the approach is dubbed clear conducting literature search for artifact reuse and is based on some practices of the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 the search in the scientific literature becomes the basis for the identification of knowledge resources that jointly cover the domain and exhibit properties considered desirable for reuse proper documentation community acceptance among others in general clear activities consist of i defining data integration requirements ii finding reusable knowledge resources on the domain of interest and iii selecting some of the identified knowledge resources to be reused in the development of ontology for data integration purposes as clear addresses specific ontology engineering activities it is designed to be used as a complement to existing ontology engineering methodologies we have applied clear to the water quality domain a total of 543 publications were surveyed the results obtained provide a set of 75 knowledge resources on this domain this set of knowledge resources make up a knowledge base on the domain to be reused whenever necessary this justifies the effort employed the proposed work is not automated in performing the systematic search for a domain for the first time this work is inserted in a project entitled an escience infrastructure for water quality management in the doce river basin called henceforth doce river project for brevity this project is concerned with the integration of water quality data produced by various sources to assess the impacts of the mining disaster that occurred in the city of mariana in brazil in 2015 when the fundão tailings dam broke contaminating the doce river basin the paper is further structured as follows section 2 presents some background knowledge that supports our investigation on the development of an approach to search and select reusable knowledge resources for the integration of scientific research data section 3 describes the clear approach section 4 discusses the results of the application of clear to the environmental research domain finally section 5 presents the final considerations 2 background in this section we review ontology engineering methodologies gaps of existing methodologies related to reuse and the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 required for the development of this work 2 1 ontology engineering methodologies ontology engineering is formally defined as the set of activities that concern the ontology development process the ontology life cycle and the methodologies tools and languages for building ontologies gómez pérez et al 2010 ontology engineering methodologies provide guidelines for the development management and maintenance of ontologies such methodologies decompose the ontology engineering process in a number of steps and recommend activities and tasks to be performed for each one in addition they define the roles of the individuals and organizations involved in the ontology engineering process in general domain experts provide knowledge with respect to the domain to be modeled ontology engineers or ontology developers have expertise in fields such as knowledge representation and development tools and users apply the ontology for a particular purpose simperl et al 2009 in gómez pérez et al 2010 the authors differentiate three types of activities within an ontology engineering process management development and support activities the first covers the organizational setting of the overall process in particular at pre development time a feasibility study examines if an ontology based application or the use of an ontology in a given context is the right way to solve the problem at hand the second type of activities refers to classical activities such as domain analysis conceptualization and implementation but also maintenance and use which are performed at post development time ontology support activities such as knowledge acquisition evaluation reuse and documentation are performed in parallel to the development activities simperl et al 2009 a distinction between ontology engineering methodologies takes into account the strategy adopted for building ontologies that is building from scratch or building from existing knowledge resources soares 2009 examples of methodologies that address building ontologies from scratch can be found in soares 2009 examples of methodologies that describe specific activities for addressing reuse are the neon methodology suárez figueroa et al 2012 the systematic approach for building ontologies sabio falbo 2014 and the methodology of integration oriented ontology development miod leung et al 2011 2 1 1 reuse related gaps reuse is pointed out as a promising approach to ontology engineering since it enables speeding up the ontology development process saving time and money poveda villalón et al 2010 and avoids the unnecessary proliferation of new ontologies however there is a lack of concern with search and selection of reusable knowledge resources by the reuse oriented ontology engineering methodologies this is shown in salamon et al 2018 in which a systematic mapping was performed to provide the current panorama of ontology integration approaches the results reveal some problems among them a lack of concern with search and selection of the ontologies to be integrated among the reuse oriented methodologies some focus on the identification and the integration of existing knowledge resources e g neon suárez figueroa et al 2012 sabio falbo 2014 and miod leung et al 2011 in general they propose steps for the specification of ontology requirements for the identification of the knowledge resources to be reused for the integration of the knowledge resources reengineering alignment merging etc and for the evaluation of the resulting ontology ontology requirements are specified mainly in the form of competency questions cqs i e questions writing in natural language that the ontology should be able to answer gruninger and fox 1995 in turn the terms whose definition could be reusable from existing knowledge resources are those appearing in the ontology requirements specification ontology developers can locate knowledge resources in ontology libraries domain related sites resources within organizations and general purpose search engines some reuse oriented methodologies focus only on the identification of relevant knowledge resources e g shiang et al 2018 and blanco et al 2011 in shiang et al 2018 the authors propose an ontology pattern classification scheme to allow the reuse of existing ontology knowledge for multiagent systems development in blanco et al 2011 a systematic literature review is carried out to obtain security ontologies these ontologies are compared according to the evaluation framework proposed in tello and gómez pérez 2004 making it possible to identify the key requirements that an integrated security ontology should have other reuse oriented methodologies focus only on the integration of two or more knowledge resources e g stoilos et al 2018 crow and shadbolt 2001 they assume that knowledge resources are identified in a previous step besides that there are some ontology engineering methodologies focused on data integration but which do not address the reuse of knowledge resources this is the case of the methodology for development on data integration ontodi yunianta et al 2019 it proposes specific steps to identify data sources to be integrated and to correct semantic inconsistences between them despite proposing activities to identify and integrate existing knowledge resources neon suárez figueroa et al 2012 sabio falbo 2014 and miod leung et al 2011 do not show how to perform the search and record the search results regarding knowledge resources selection miod suggests some evaluation criteria for example quality of documentation and language used to implement the resource but does not show how to assess these criteria neon applies a subjective evaluation criterion that is the consensus about the knowledge and terminology used by the resource sabio does not describe how knowledge resources are to be selected in relation to the methodologies shiang et al 2018 and blanco et al 2011 they search for specific types of knowledge resources ontology patterns or ontologies or in specific domains security in their turn the methodologies proposed in stoilos et al 2018 and crow and shadbolt 2001 do not address the search for reusable knowledge resources finally ontodi yunianta et al 2019 does not address a step related to reuse 2 2 systematic literature review as we have discussed in the previous section there is explicit support for reuse in ontology engineering methodologies however they provide only generic guidelines for reusable knowledge resources search and selection activities this justifies a more systematic approach to perform them we draw inspiration for such approach from the practices of the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 is one of the main mechanisms that support evidence based research this research paradigm has been advocated as a good practice for decision making or troubleshooting in many areas such as medicine economics and software engineering an slr is a secondary study method based on evaluating and interpreting all available research relevant to a particular research question topic area or phenomenon of interest and then on reporting the methodology used and the results obtained although an slr requires considerable effort to be implemented when compared to ad hoc literature reviews slrs are auditable more trustworthy and rigorous an slr has three phases planning conducting and reporting the review kitchenham and charters 2007 in the planning phase the first step is to identify the need for the review that is the reason the review is being carried out then the review protocol is developed a review protocol specifies the methods that will be used to perform a specific slr it must contain the research questions that the review aims to answer the strategy to search for primary studies including search terms search string and search engines the criteria and procedures for selecting studies the checklist and procedures for assessing the quality of studies the strategy for extracting data and the strategy for the synthesis of extracted data the protocol is refined in the following phases but must be defined in planning to make it less likely that the results of the literature will be biased and further to make search assumptions explicit in the conduction phase the search is performed and the primary studies are retrieved next the selection criteria are applied to identify the studies that provide direct evidence about the research questions then the quality of the selected studies related to the extent to which the studies minimize bias and maximize internal and external validity is evaluated finally some data are extracted from the selected studies and synthesized in tables so that the meta analysis i e statistical techniques aimed at integrating the results of the primary studies can be performed in the reporting phase the main report with final results is prepared and evaluated to verify if the search need has been met kitchenham and charters 2007 as a way to enhance the quality of the search snowballing can be performed wohlin 2014 snowballing refers to using the reference list of a study or the citations to the study to identify additional studies and therefore increase coverage of relevant literature using the references and the citations respectively is referred to as backward and forward snowballing the studies obtained from the snowballing are analyzed in the same way that the studies returned directly by the search in this work slr is useful because we are interested in searching for reusable knowledge resources on a scientific research domain however we aim to investigate scientific literature and technical papers to find available knowledge resources in the domain of interest thus the slr planning conducting and reporting activities need to be adapted to accommodate this characteristic this is the subject of clear as discussed in section 3 3 the clear approach clear conducting literature search for artifact reuse is a systematic approach to find and select reusable knowledge resources here called structured resources for building ontologies with the purpose of scientific research data integration by structured resources we mean those that represent knowledge through the use of formal specification of concepts relations and properties as ontologies and also other types of artifacts that capture semantic value for the concerned domain such as reference models representation schemas knowledge base schemas database schemas data exchange formats metadata standards vocabularies and thesauri the proposed approach adopts some practices of the systematic literature review slr dyba et al 2005 kitchenham and charters 2007 more specifically publications in a given domain are analyzed as a strategy for finding structured resources available on that domain this aims to increase the scope of the search and reduce the bias promoting the identification of structured resources that jointly cover the domain and exhibit properties considered desirable for reuse proper documentation available representation and community acceptance as a result the set of retrieved structured resources make up a knowledge base on the domain to be reused whenever necessary this justifies the effort employed in performing the systematic search for a domain for the first time clear addresses specific ontology engineering activities consequently it is designed to be used as a complement to existing ontology engineering methodologies for example when used together with neon suárez figueroa et al 2012 clear activities correspond to and replace neon s specification of ontology requirements search for reusable knowledge resources assessment of candidate knowledge resources and selection of knowledge resources the overview of clear activities is presented in the sequel 3 1 overview of clear activities clear is structured in three cycles as shown in fig 1 the activities of cycle i aim at defining the data integration requirements and the scope of the ontology to be developed these requirements are necessary to perform the activities of the other two cycles the activities of cycle ii aim at systematically identifying structured resources candidates to be reused in the development of the ontology based on the requirements defined in cycle i once identified the structured resources can be selected to be reused which is the goal of cycle iii the three cycles are intended to be executed in an iterative fashion in the same way the activities of each cycle itself should be visited iteratively as knowledge about the domain is gathered and requirements are refined new structured resources are identified and should be considered for reuse 3 2 cycle i data integration requirements definition the data integration requirements definition cycle i is composed of three activities a integration questions definition b data sources selection and c domain aspects identification in the first activity a top down analysis of the integration requirements is made through the definition of integration questions iqs iqs are questions about the research domain that can only be answered through the integration of different data sources lenzerini 2002 that is because the contents of data are different and or complementary to each other or because different views of the same content must be contrasted in the second activity the data sources needed to address the iqs are selected by ontology engineers and domain experts in the third activity of this cycle a bottom up analysis of the integration requirements is done by studying the selected data sources the analysis of data sources iqs and domain standards norms national and international standards guides etc combined with the knowledge of domain experts allows the ontology engineers to identify the domain aspects domain aspects are subjects of the domain that can be treated in a modular way they must be enough to represent the universe of discourse they can be related to activities actors and roles description characterization of researched entities and so on they are used in cycle ii to support the systematic search for structured resources and in cycle iii to guide the selection of structured resources found in cycle ii 3 2 1 integration questions definition in this activity a top down analysis of the integration requirements is made through the definition of integration questions iqs driven by the needs of domain experts as iqs are answered from the integration of different data sources some candidate data sources to be integrated are known to domain experts prior to the application of clear these data sources serve as input to the definition of iqs in turn iqs support the selection of the set of data sources to be integrated as will be seen below iqs are also used in the definition of the domain aspects besides that in the joint use of clear with ontology engineering methodologies iqs are broken down into competency questions thus they are used to define the ontology scope and also for the evaluation of the developed ontology since clear is iterative it allows the refinement of iqs throughout the process which can be done by adding grouping uncoupling and updating actions table 1 shows the inputs outputs and actors of this activity 3 2 2 data sources selection from iqs it is possible to define the final set of data sources selecting those that provide appropriate data to answer iqs the selected data sources will be integrated with the support of the ontology to be developed from the reuse of the discovered structured resources the selection of data sources can be challenging considering that i data producers may be many researchers government entities non profit organizations industry and laboratories and sometimes unknown ii data can be difficult to find and obtain due to organizational barriers and iii data can be large heterogeneous and of varying quality table 2 shows the inputs outputs and actors of this activity 3 2 3 domain aspects identification in this activity the domain aspects are identified for this one can use general questions to characterize a scientific research that needs to consume integrated data examples of these questions are how is scientific research done where when what is researched who is the agent or principal and why is scientific research done similarly to iqs domain aspects can be refined continuously by adding grouping uncoupling or updating actions it is important to note that the analysis of the selected data sources elements provides significant knowledge for the identification of domain aspects this is because our ultimate goal is to find structured resources to be reused in the development of ontologies for the integration of these data sources however as mentioned before data sources content can be large heterogeneous and of varying quality therefore care must be taken when analyzing data sources to identify domain aspects this involves correlating different terms used to represent the same concept understanding the different granularities used to represent data and verifying the meaning of the absence of data when not justified this should be done with the support of the domain experts table 3 shows the inputs outputs and actors of this activity 3 3 cycle ii structured resources systematic search in clear the planning activity is called a systematic search configuration the conducting activity is divided into three b publications selection c structured resources identification and d snowballing the reporting activity is called e systematic search reporting they are performed by ontology engineers who are interested in finding structured resources to improve their work in systematic search configuration the strategy required to perform the search is defined steps such as the specification of the search goals and the definition of inclusion and exclusion criteria are executed in publications selection the systematic search for publications is performed the returned publications are analyzed and selected by applying the inclusion and exclusion criteria of publications after the publications selection the structured resources presented or mentioned by the selected publications are analyzed and selected by applying the inclusion and exclusion criteria of structured resources this is done in the structured resources identification activity to enhance the quality of the search the snowballing activity can be performed the snowballing technique wohlin 2014 can be applied to both publications and structured resources as a result of these activities we have the sets of identified and selected publications and structured resources finally in systematic search reporting the results of the systematic search are presented and evaluated to verify if the search goals were reached 3 3 1 systematic search configuration in systematic search configuration the following steps are executed specification of the search goals which concerns ultimately the identification of structured resources in the particular research domain selection of keywords to compose the search string elaboration of the search string selection of search engines definition of inclusion and exclusion criteria whose purpose is to select only publications and structured resources that meet the search goals definition of the publications selection procedure definition of the structured resources identification procedure and definition of the snowballing procedure in clear the selection of keywords reflects the dual nature of the search goals thus keywords represent not only the domain but also the types of structured resources to be found ontologies reference models database schemas etc in addition there are two different types of inclusion and exclusion criteria one for publications the other for structured resources the eight steps of this activity are explained below search goals specification in this first step the search goals are specified to guide systematic search activities they must be related to the structured resources to be searched keywords selection in this step the terms to compose the search string are selected once we are searching for structured resources on a specific domain we need to define some keywords related to structured resources and others related to the domain to make reference to structured resources terms such as ontology reference model vocabulary taxonomy and their related terms must be considered regarding the domain keywords that depict the domain itself the super domain i e a domain more generic than ours or the domain aspects should be used the domain related terms are obtained from discussions with domain experts glossaries prepared by them domain standards and domain aspects when they are used search string improvement the terms obtained in the previous step are organized in a search string this string should group the keywords into a logical expression typically using or and operators in clear the expression is formed by two main terms connected by and the first one selects publications concerned with structured resources and the second one selects domain specific publications each of these main terms is disjunctive in order to include alternative terms that are used to denote structured resources and to identify the research domain the search string is tested gradually including terms subsequently in the disjunctions in order to test whether they actually increase the search results and should be kept in the string search engines selection after the search string was constructed the search engines to be used need to be selected they include digital libraries specific journals and conference proceedings as recommended by kitchenham and charters 2007 checking search engines results against lists of already known primary studies called here control papers can be useful for selection of the search engines kitchenham and charters 2007 inclusion and exclusion criteria definition in this step the criteria to select inclusion or discard exclusion publications and structured resources obtained by the systematic search are defined then only those that directly reach the search goals are maintained for publications a general inclusion criteria recommended by clear is that the publications must present or mention structured resources about the domain or an aspect of it other inclusion criteria could be language journal authors setting participants or subjects research design sampling method and date of publication kitchenham and charters 2007 for structured resources an inclusion criteria proposed by clear is that they must address the domain or its aspects as exclusion criteria both for publications and structured resources we can check their availability that is publications and structured resources whose content is not fully available must be excluded publications selection procedure definition in this step the process to be followed for the publications selection is defined initially one must determine the scope of the search that is if the string terms will be searched only in title abstract or any part of the publications secondly one must define data to be registered about the studied publications and the form for example a spreadsheet to be used to record them regarding publications data it is necessary to register the year the title the authors and the source structured resources identification procedure definition in this step the process to be followed for the structured resources identification is defined one must define data to be registered about the studied structured resources and the form to be used to record them in relation to the structured resources data it is necessary to register the name the source the language used to build the resource such as ontology web language owl extensible markup language xml and unified modeling language uml the owner the description the key concepts the upper level ontology applicable only to ontologies the resources that reuse the structured resource the selected publications that present the structured resource and the selected publications that mention the structured resource snowballing procedure definition in this step the process to be followed for the snowballing application is defined in the case of publications it can be used in the same way as in the slr that is by checking the reference lists and citations of selected publications in the case of structured resources it selects structured resources that are reused by each one analyzed table 4 shows the inputs outputs and actors of the systematic search configuration 3 3 2 publications selection in this activity the process defined in publications selection procedure definition is performed the search engines are configured according to the search scope and some inclusion and exclusion criteria such as the publication language journal authors and date of publication then the search is performed the returned publications data are recorded in the publications form publications are analyzed and selected by applying the inclusion and exclusion criteria of publications table 5 shows the inputs outputs and actors of this activity 3 3 3 structured resources identification after the publications selection the process defined in structured resources identification procedure definition is performed the structured resources presented or mentioned by the selected publications are identified the structured resources data are recorded in the structured resources form structured resources are analyzed and selected by applying the inclusion and exclusion criteria of structured resources table 6 shows the inputs outputs and actors of this activity 3 3 4 snowballing in this activity the process defined in snowballing procedure definition is performed the new publications and structured resources data are recorded on the corresponding forms new publications and structured resources are analyzed and selected by applying the respective inclusion and exclusion criteria table 7 shows the inputs outputs and actors of this activity 3 3 5 systematic search reporting in this activity the results of the systematic search are presented and evaluated to verify if the search goals were reached this is done by analyzing including graphically some of the information collected about publications and structured resources such as the language used to build the resources the number of publications that mention the resources and the number of resources that reuse them this is useful in evaluating the quality attributes of the structured resources performed in cycle iii as it will be presented below table 8 shows the inputs outputs and actors of this activity 3 4 cycle iii structured resources selection the final structured resources selection cycle iii is composed of three activities a structured resources analysis b structured resources classification and c structured resources evaluation in the first activity the structured resources identified in cycle ii are assessed by verifying domain coverage and key quality attributes for reuse proper documentation available representation and community acceptance this allows the classification of the structured resources in the second activity finally in the third activity the best classified structured resources are evaluated according to their suitability for the representation of existing data as a final result we have the selected structured resources to be reused in addition we have a set of relevant structured resources in the research domain classified according to domain coverage and quality attributes 3 4 1 structured resources analysis domain coverage analysis domain coverage is analyzed based on the domain aspects this can be verified by checking whether or not a domain aspect is covered by structured resources or indicating a degree of coverage the domain coverage provides a relevant criterion for making decisions about structured resources reuse for example considering the first option it is verified that each structured resource covers a subset of the domain aspects set identified in cycle i thus if a domain aspect is covered by only one structured resource this contributes for deciding to select it for reuse on the other hand if the domain aspects covered by a structured resource are a subset of the domain aspects set covered by another resource this may indicate that the second is a better choice than the first in clear the domain coverage analysis is performed by means of a matrix as shown in table 9 each row of the matrix refers to a structured resource and each column refers to a domain aspect if a domain aspect is covered by a structured resource the corresponding cell of the matrix must be checked the domain aspects are grouped according to the questions that answer to characterize a scientific research the total of domain aspects covered and the total of domain aspects covered in each group by the structured resources are computed quality attributes analysis the quality analysis supports the choice of the structured resources since it differentiates resources that have similar domain coverage relevant quality attributes for reuse include reuse economic cost need to acquire a use license etc understandability effort e g quality of the documentation code clarity integration effort modularization language used etc and reliability e g development team reputation popularity suárez figueroa et al 2012 clear adopts the following quality attributes proper documentation available representation and community acceptance we have prioritized those attributes as they can be evaluated objectively as discussed in the sequel other attributes may be added if deemed appropriate proper documentation it refers to the availability of documentation to facilitate the understanding of structured resources concepts relationships and properties and as consequence to enable their proper use we check the availability of glossaries and examples of instantiation glossaries explain the meaning intended for the concepts that compose the structured resources examples of instantiation allow us to understand what is or is not an instance of concepts available representation it is related to the availability of a conceptual graphical model and the availability of a computational representation both of which are desirable the first one is because it promotes a clear and precise description of domain entities for the purposes of communication learning and problem solving through the creation of a conceptual model that describes the solution to a problem the second one is because it provides a machine readable implementation version of the structured resource we have used the language used to build the structured resources mapped in cycle ii to help in this analysis community acceptance this is about a structured resource being considered a domain standard this can be verified through metrics that show how well it is recognized and used by the community to assess how much a structured resource is recognized and reused by the community we use the number of publications that mention the structured resource and the number of resources that reuse it respectively we consider as mentioned or reused the resources that obtained at least 50 of the maximum number of mentions or reuse this is to disregard little mentioned or reused structured resources the quality attribute analysis is performed by means of a matrix as shown in table 10 each row of the matrix refers to a structured resource and each column refers to a quality attribute if a structured resource ranks positively in a quality attribute the corresponding cell in the matrix must be checked the quantity of quality attributes in which a structured resource is positively classified is calculated in the quality attributes score column table 11 shows the inputs outputs and actors of this activity 3 4 2 structured resources classification in this activity the structured resources are classified in each domain aspects group thus those most appropriate to treat the domain aspects of each group are identified for this a final score is computed based on the total of domain aspects covered in each group by the structured resources and their quality attributes score initially these values must be normalized in the 0 1 interval then the arithmetic or weighted average of the normalized values is calculated the structured resources are classified in each group according to this average table 12 shows the inputs outputs and actors of this activity 3 4 3 structured resources evaluation in this activity the best ranked structured resources in each aspects group are selected and evaluated to verify their suitability for the representation of different domain data this evaluation is performed trying to annotate each element of the data sources selected in cycle i with the concepts classes properties and instances made available by each structured resource as the structured resources are evaluated they are selected or discarded if discarded because they do not properly represent the elements of the target aspects group the next resources in the classification should be evaluated at the end of this activity we have a set of complementary structured resources to be reused in addition we have a set of relevant structured resources in the research domain classified according to domain coverage and quality attributes table 13 shows the inputs outputs and actors of this activity 4 applying clear to the water quality domain in this section we apply the clear approach to the water quality domain in the context of the doce river project the objective is to find structured resources to be reused in the development of an ontology for the integration of water quality data the work was carried out by two domain experts and two ontology engineers over a period of 2 months cycle i and cycle ii activities took approximately 2 weeks each and cycle iii activities took approximately 1 month the most time consuming step is the structured resources analysis in cycle iii as it is necessary to study each of the identified structured resources to verify the domain coverage and quality attributes the domain experts are researchers in the areas of geochemistry and aquatic biodiversity the ontology engineers already had knowledge about the water quality domain before applying the approach which reduced the time required to study publications and structured resources it is worth mentioning that the time required for applying clear depends directly on the number of publications and structured resources to be analyzed as well as the size of the structured resources and the quality of documentation available on them in turn the number of publications and structured resources to be analyzed is driven by the requirements specified in cycle i that is iqs more generic or specific data sources to be integrated and domain aspects 4 1 definition of the water quality data integration requirements in this section we present the application of the cycle i of clear to the water quality domain a key aspect of this cycle is the participation of domain experts who are knowledgeable of data semantics and who face themselves integration questions in their research activities 4 1 1 integration questions for the water quality domain a non exhaustive list of iqs defined by domain experts is shown in table 14 as one can observe these questions are related to the assessment of water quality at monitoring points along the doce river and its tributaries they concern not only the impacts of the disaster but also water quality in general these questions could be answered by analyzing the measurements of the physical chemical and biological properties of the water and sediment samples and the ecotoxicological essays carried out by different brazilian organizations 4 1 2 data sources to be integrated the data sources needed to address the iqs are provided by various brazilian governmental and non governmental organizations among the governmental ones there are those that cover the national territory and those that cover the states of minas gerais and espírito santo bathed by the doce river and impacted by the disaster the national governmental organizations selected are the national water agency ana ana 2019 the geological survey of brazil cprm cprm 2019 and the brazilian institute for the environment and renewable natural resources ibama ibama 2019 the state level governmental organizations selected are the water management institute of minas gerais igam igam 2019 and the institute of environment and water resources of espírito santo iema iema 2019 the non governmental organization selected is renova foundation renova 2019 that is the entity responsible for the mobilization to repair damages caused by the rupture of the fundão dam in mariana mg 4 1 3 water quality domain aspects from the iqs presented in table 14 it is possible to extract many domain aspects that answer the general questions used to characterize a scientific research some of them are water sampling water quality analysis water quality measurement and water quality monitoring how water quality properties parameters and meteorological aspects what location where and normative element why for example the normative element domain aspect which defines water quality and motivates water sampling water quality analysis etc was obtained from iq03 and iq11 iq03 mentions the applicable legislation for freshwater and iq11 mentions the metal levels thresholds adopted by environmental agencies table 15 was extracted from the weekly water quality bulletin 04 feb 2019 obtained at the renova foundation website renova 2019 for each element of this table we have identified a domain aspect provenance renova foundation geographical entities water courses chemical physical and biological properties of water presence of cyanobacteria electric conductivity dissolved oxygen and ph meteorological aspects rain of the period units of measurement μg l μs cm mg l and mm sensors used telemetric stations reference to norms 357 2005 conama resolution conama 2005 and compliance table 16 presents an analysis of data source elements in two of the data sources we considered ibama iema and igam for each data source element usually a column name in tabular data provided by a data source we have identified a domain aspect domain aspects group elements that deal with related concepts the identified domain aspects are provenance ibama iema or igam geographic coordinates altitude latitude etc geographical entities hydrographic basin sub basin water course among others location e g site county station temporal references date year etc sampling which encompasses other aspects such as sampling method inferred from the concept of sample type and material entity inferred from the concept of sample point category measurement which contain more specific aspects such as chemical physical and biological properties e g alkalinity of bicarbonates units of measurement mgcaco3 l and measurement agent data source as well as normative elements framing class of water course note that different data sources cover the same domain aspect with different representation schemes the analysis of the iqs the domain standards e g rice et al 2017 and the selected data sources elements resulted in the following list of the water quality domain aspects research activity sampling preparation measurement analysis monitoring sampling method preparation method measurement method analysis method and monitoring method how location geographic coordinates and geographic entity where material entity abiotic entity biotic entity properties chemical property physical property biological property unit of measurement and meteorological aspects what temporal references when agent sensor and provenance who normative elements why these aspects together establish the required coverage of the ontology to be developed 4 2 systematic search for structured resources on the water quality domain next we present the application of the cycle ii of clear to the water quality domain it consists in the systematic search for structured resources on this domain 4 2 1 configuring the systematic search the following search goal was formulated for the water quality domain find structured resources candidates to be reused in the development of ontologies for data integration in the water quality domain identify the structured resources the language in which they are represented the location where they are available the key concepts addressed by them and the resource owner among the keywords related to structured resources we have used ontology and vocabulary related terms so that publications containing structured vocabularies and taxonomies were also identified see table 17 for alternative terms with respect to the terms related to domain besides water quality itself and its alternative terms the super domain environmental quality was included to make it possible to carry out a wider search see table 18 the final string obtained is presented below ontology or vocabulary or reference model or knowledge base or schema or taxonomy or thesaurus and water quality or water resource or environmental quality or water evaluation or water analysis or water monitoring or water assessment or environmental resource or environmental evaluation or environmental analysis or environmental monitoring or environmental assessment or environment quality or environment resource or environment evaluation or environment analysis or environment monitoring or environment assessment the control papers cp used to aid in the selection of the search engines are listed in table 19 they were chosen based on a non systematic search campos et al 2018 in which it was possible to find publications that propose structured resources suited for the representation of the water quality domain we selected google scholar as the search engine for our systematic search because google scholar retrieves technical works in the domain of interest presented at domain specific conferences as well as scientific papers unlike other digital libraries engineering village scopus and ieee explore the google scholar search retrieves all three control papers the publications inclusion pic and exclusion criteria pec are shown in table 20 and the structured resources inclusion sric and exclusion criteria srec are shown in table 21 pic01 is directly related to the search goal pic02 is used to select only publications globally recognized and pec01 is used to discard unavailable publications sric01 is used to select only structured resources that address the water quality domain srec01 is used to discard structured resources that are also unavailable because they have been discontinued or because they have not been made available to broaden the scope of the search it was decided to apply snowballing on the reference lists and citations of the selected publications and on the structured resources reused by those selected 4 2 2 selecting publications in relation to the search scope we decided to look for the keywords in the paper title for pragmatic reasons in this case we note that even while searching the title the relevant publications were returned one way to verify that relevant publications have not been left out is to check if the systematic search returns publications found by previously non systematic searches we verify that the publications found by the non systematic search presented in campos et al 2018 which propose structured resources suited for the representation of the water quality domain were returned by the systematic search thus the search scope was configured in the google scholar besides that the option to search only publications written in english was checked in the google scholar to meet the inclusion criteria pic02 the systematic search was performed on the june 21st 2019 the publications returned were analyzed and selected by applying pic01 and pec01 in total 64 publications were obtained after applying the inclusion and exclusion criteria 18 were selected publication data can be found in the publications selection table of the dataset campos et al 2020 provided with this work 4 2 3 identifying structured resources the structured resources extracted from selected publications were analyzed and selected by applying sric01 and srec01 in total 57 structured resources were obtained after applying the inclusion and exclusion criteria 44 were selected structured resource data can be found in the structured resources identification table of the dataset campos et al 2020 4 2 4 applying snowballing the application of snowballing on the reference lists and citations of the selected publications resulted in 479 new publications after applying the publications inclusion and exclusion criteria to them 67 were selected for better organization new publications were listed in the new tables reference lists selection and citations selection with the same structure as the publications selection table of the dataset campos et al 2020 the analysis of the new publications resulted in 34 new structured resources after applying the structured resources inclusion and exclusion criteria to them 25 were selected in addition the application of snowballing on the resources reused by the 60 selected structured resources resulted in 22 new structured resources after applying the inclusion and exclusion criteria to them 6 were selected all structured resources were identified in structured resources identification table of the dataset campos et al 2020 at the end of the systematic search 85 publications were selected from a total of 543 analyzed publications also 75 structured resources were selected as candidates for reuse from a total of 113 identified structured resources the analysis of publications and structured resources was divided among ontology engineers which reviewed each other s work divergences in analysis were discussed and resolved in meetings 4 2 5 reporting the results of the systematic search as previously discussed the systematic search returned a total of 543 publications of which 85 15 7 were selected for presenting or mentioning structured resources about the water quality domain or part of it among the discarded publications 458 publications 346 publications 75 5 did not meet inclusion criteria pic01 15 3 3 did not meet inclusion criteria pic02 and 97 publications 21 2 met exclusion criteria pec01 this means that most publications were discarded because they did not present or mention a structured resource on the domain of interest that is they did not meet the systematic search goal regarding the structured resources a total of 113 structured resources were obtained counting those extracted from publications and those reused by other resources among them 75 were selected as candidates for reuse and 38 were discarded among the 38 structured resources discarded 20 52 6 did not meet inclusion criteria sric01 and 18 47 4 met exclusion criteria srec01 several links provided by publications were broken in some cases it was possible to find them elsewhere but in cases in which it was not possible structured resources were excluded according to srec01 with respect to data extracted about the selected structured resources we analyze the language used to build the resources the number of publications that mention these resources not including the papers that present them and the number of resources that reuse them such data is used in cycle iii to evaluate the quality attributes of the structured resources the key concepts treated by the structured resources are also used in cycle iii to verify the coverage of the domain by each of them regarding the language we have found certain convergence ontology web language owl is used by 38 9 of the structured resources found while schemas written in resource description framework rdf and extensible markup language xml have reached 22 2 only 8 3 use unified modeling language uml 6 5 use hypertext markup language html in this case structured links and 24 1 use other languages for this analysis see graph of fig 2 resources have been counted more than once according to the number of languages in which they are made available the language is used to verify the quality attributes related to the representation level of each structured resource in cycle iii the number of publications that mention a structured resource can be used to measure how well it is recognized by the community in cycle iii as shown in the graph of fig 3 two structured resources semantic sensor network ssn ontology compton et al 2012 and semantic web for earth and environmental terminology sweet ontologies raskin and pan 2005 are mentioned by fourteen publications one structured resource the observations and measurements o m conceptual model iso 2011 is mentioned by thirteen publications one resource the chemical entities of biological interest chebi ontology hastings et al 2016 is mentioned by ten publications two resources time ontology in owl owl time cox and little 2017 and quantity unit dimension and type qudt ontologies hodgson et al 2014 by nine publications and one resource water markup language waterml zaslavsky et al 2007 by five publications 18 7 of the resources are mentioned by three publications 25 3 of the resources are mentioned by two publications and 26 7 of the resources by one publication 20 0 of the structured resources were identified only from the publication that presents them or from the resources that reuse them they are not mentioned by other publications the number of resources that reuse a structured resource represents how much it is used by the community regarding the number of resources that reuse a structured resource the graph of fig 4 shows that one structured resource o m iso 2011 is reused by twelve resources one structured resource geography markup language gml iso 2007 is reused by eight resources one structured resource ssn compton et al 2012 is reused by seven resources one structured resource the standard geographic information geomatics iso tc 211 tom and roswell 2009 is reused by six resources one structured resource owl time cox and little 2017 is reused by five resources and one structured resource sweet raskin and pan 2005 is reused by four resources 2 7 of the structured resources are reused by three resources 8 0 are reused by two resources 34 6 are reused by one resource and 46 7 are not reused by any of the other selected resources in relation to the last two graphs we verify that the structured resources were mentioned or reused by groups different from those that created them in addition we disregard the publications that present the structured resources in the analysis performed in the graph of fig 3 this is to ensure that the structured resources are recognized and reused by the community and not just by the group that have created them 4 3 selection of the structured resources on the water quality domain in this section the application of the cycle iii of clear to the water quality domain is discussed 4 3 1 analyzing the structured resources table 22 shows the domain coverage analysis for the selected structured resources the complete analysis was recorded in the structured resources selection table of the dataset which includes citations to all of the resources campos et al 2020 in table 22 to improve the view of the domain coverage by groups the columns of the domain aspects that make up each group were painted with the same color the structured resources were ordered by the total of domain aspects covered by them from largest to smallest the structured resources positioned at the beginning of table 22 address a greater number of domain aspects than the others they deal with domain aspects contained in most groups tending to be more generic e g united states geological survey usgs thesaurus usgs 2019 infrastructure for spatial information in europe inspire inspire 2019 and sweet raskin and pan 2005 the structured resources positioned at the end cover a smaller number of domain aspects contained in one or two groups thus they tend to be more specific as examples we can mention geonames 2019 and geosparql perry and herring 2012 where owl time cox and little 2017 when and qudt hodgson et al 2014 and chebi hastings et al 2016 what we do not identify structured resources that cover only domain aspects of how who or why groups table 23 shows the quality attributes analysis for the selected structured resources the ordering used for table 22 was maintained to facilitate the identification of the structured resources and the comparison of the two tables this analysis was recorded in the structured resources selection table of the dataset campos et al 2020 from table 23 it can be verified that only two structured resources o m iso 2011 and ssn compton et al 2012 rank positively in all 6 quality attributes two structured resources qudt hodgson et al 2014 and owl time cox and little 2017 in 5 quality attributes 24 0 of the structured resources in 4 quality attributes 16 0 in 3 quality attributes 30 7 in 2 quality attributes and 24 0 in 1 quality attribute 45 3 of the structured resources rank positively in 3 or more quality attributes which favors the reuse of them 4 3 2 classifying the structured resources for the water quality domain we calculated the arithmetic average of the normalized values of domain aspects covered in each group by the structured resources and their quality attributes score to compute the final score the classification was recorded in the structured resources classification table of the dataset campos et al 2020 table 24 shows the ranking for the top 10 structured resources from each group in some cases the number of structured resources presented is greater than 10 because more resources were tied in the same position as one can observe some structured resources appear well classified in all or most of the aspects groups this is the case of inspire inspire 2019 well classified in the 6 groups iso tc 211 tom and roswell 2009 and united states geological survey hydrologic markup language usgshydroml bermudez and piasecki 2003 well classified into 5 groups and o m iso 2011 and sweet raskin and pan 2005 well classified into 4 groups 4 3 3 evaluating the structured resources we selected 75 elements from five data sources identified in cycle i to be annotated with the structured resources the data providers are ana ana 2019 ibama ibama 2019 and iema iema 2019 igam igam 2019 cprm cprm 2019 and renova foundation renova 2019 the first structured resource evaluated was the inspire inspire 2019 since it ranked well in all aspects groups in its evaluation 59 of the 75 data sources elements 78 7 were properly represented this number indicates that inspire is indeed an artifact to be reused it is important that 14 23 7 of the 59 data sources elements were represented by other structured resources reused by inspire 12 from o m iso 2011 and 2 from iso tc 2011 tom and roswell 2009 also confirming the good positioning of these resources about the other 16 concepts 21 3 they are relative to the physical chemical and biological properties used for water quality measurements we choose not to represent them with inspire because it treats them very generically to represent them we selected qudt hodgson et al 2014 and environment ontology envo envo 2019 well classified in the what group qudt represents each of the properties and units of measure used by the data sources envo represents the chemical entities it is also important to note that envo represents the chemical entities through chebi hastings et al 2016 another resource identified in cycle ii but not ranked so well in the what group because it is focused narrowly on chemical entities this evaluation is available in the structured resources evaluation table of the dataset campos et al 2020 table 25 shows part of this evaluation focusing on data elements presented in table 16 of this work table 25 contains the data source which indicates the provenance of data the data source element to be annotated the structured resource that provides the proper representation to the data source element and the structured resource concept property and instance that can be used to represent the data source element for example in the second row of igam we have the data source element hydrographic basin inspire provides the concept riverbasin with the property geographicalname to represent it another example can be seen in the last row of ibama iema that contains the element alkalinity of bicarbonates mgcaco3 l the instance concentration of the concept chemistryquantitykind of qudt is used to represent the chemical property the concept calcium carbonate of envo chebi is used to represent the chemical entity caco3 the instance milligram liter of the concept unit of qudt is used to represent the unit of measurement and the concept quantityvalue of qudt is used to represent the measured value for this chemical property in the evaluation performed we were able to represent all elements of the data sources identified in cycle i with 6 of the structured resources identified in cycle ii inspire o m iso tc 2011 qudt envo and chebi these resources are complementary to each other with inspire offering broad coverage of domain aspects and the other resources covering some aspects in depth 5 final considerations in this paper we have presented clear an approach inspired by systematic literature review practices to find reusable structured resources about a scientific research domain clear can be used with existing reuse oriented ontology engineering methodologies for example neon suárez figueroa et al 2012 and miod leung et al 2011 to support the search and selection of reusable knowledge resources clear cycle i corresponds to the activity of ontology requirements specification of ontology engineering methodologies in turn clear cycles ii and iii correspond to the knowledge resources identification the structured resources selected from the application of clear to a domain serve as input for the next activity of ontology engineering methodologies the integration of the reusable knowledge resources in addition the set of iqs identified can be used to evaluate the resulting ontology in the same way that cqs are used by neon and miod the main advantage of using clear is that it supports the identification of reusable knowledge resources in a systematic fashion which is not addressed by existing ontology engineering methodologies another advantage is that it proposes the evaluation of reusable knowledge resources based on objective quality attributes a feature not present in existing ontology engineering methodologies in addition clear is aligned to the needs of ontology building for the purpose of scientific research data integration with ontology requirements derived from iqs and data to be integrated a disadvantage of clear is the effort required for its application to a domain in the first iteration however once applied to a particular domain clear provides a set of evaluated and classified structured resources that can be reused whenever new needs about such domain arise we argue that this result justifies the effort employed it is important to state that the set of structured resources returned by applying clear to a given domain depends on the requirements specified in cycle i if iqs data sources and domain aspects are changed another set of structured resources can be obtained as result in any case to build ontologies that need to address similar domain aspects the same set of structured resources can be used even though iqs and data sources are different here we have reported the application of clear to the water quality domain we focused on finding structured resources to be reused for the integration of water quality data a set of 75 structured resources candidates to be reused were obtained these knowledge resources were analyzed according to the domain coverage and the quality attributes proper documentation available representation and community acceptance and classified based on this assessment in the evaluation performed 6 of the structured resources were able to jointly represent all elements of the data sources to be integrated these structured resources were selected to be reused in campos 2019 some of us report the use of clear together with neon to build an ontology for the water quality domain using these 6 structured resources as they differ from each other and cannot be integrated into their original format a foundational ontology was employed in the analysis and reengineering of them most of the concepts represented by the designed ontology 42 out of a total of 78 concepts i e 53 8 were reused from the knowledge resources selected this evidences the fruitfulness of clear in promoting reuse the set of 75 structured resources resulting from the application of clear to the water quality domain is available in campos et al 2020 and provides an important knowledge base that can be reused thus people who need to build ontologies for the water quality domain or environmental domain with similar domain aspects can consult it saving the effort and time required to perform the systematic search and the assessment of the structured resources on this domain in a previous work see campos et al 2018 we have conducted a non systematic search for structured resources about the water quality domain this search resulted in a set of 11 reusable knowledge resources some were already known to us others were obtained from the analysis of various publications that we could identify as can be seen the number of structured resources obtained from the application of clear is considerably higher than that obtained from the non systematic search it is important to mention that two of the knowledge resources identified by the non systematic search namely ontobio albuquerque et al 2015 and m opl barcellos et al 2014 were not returned by clear ontobio was published in portuguese therefore it does not meet inclusion criteria and m opl addresses a more general issue measurements in general not specifically targeted at the environmental quality domain when comparing the approaches we observe that the application of a systematic approach guides the search and broadens the scope of results moreover we realize that clear facilitates discovery of important initiatives and working groups in the field of interest among the difficulties encountered in performing this work we can mention the bureaucracy faced to obtain data to be integrated in many cases such data is not available online thus it was in many cases necessary to contact each provider for access another difficulty identified was the lack of documentation or examples of use of some reusable structured resources documentation and examples are essential for the activities of verifying domain coverage understanding the knowledge resources and aligning them with a foundational ontology if they are not available the effort to carry out these activities which is not small increases considerably finally as future work we can consider evaluating the degree of coverage of domain aspects not covered covered largely covered and fully covered rather than just whether or not they are covered by knowledge resources we can also look for new quality attributes to be evaluated for the classification and selection of existing knowledge resources besides that we can study the automation of some steps of clear to reduce the effort required to apply it as examples we can try to automate the application of the inclusion and exclusion criteria and the extraction of data from publications and structured resources we can also try automating the domain coverage analysis and the quality attributes analysis as these steps are the most time consuming and this would greatly reduce the effort of applying the approach declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is funded by the brazilian coordination for the improvement of higher education personnel capes grant number 23038 028816 2016 41 and the brazilian national council for scientific and technological development cnpq grant numbers 312123 2017 5 and 407235 2017 5 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104813 
