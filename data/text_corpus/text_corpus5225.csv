index,text
26125,we applied two approaches to model grassland yield and nitrogen n content the first was a series of regression equations the second was the century dynamic model the regression model was generated from data from eighty nine experimental sites across europe distinguishing between five climatic regions the century model was applied to six sites across these regions both approaches estimated mean grassland yields and n content reasonably well though the root mean squared error tended to be lower for the dynamic model the regression model achieved better correlations between observed and predicted values both models were more sensitive to uncertainties in weather than in soil properties with precipitation often accounting for the majority of model uncertainty the regression approach is applicable over large spatial scales but lacks precision making it suitable for considering general trends century is better applied at a local level where more detailed and specific analysis is required keywords grasslands yield nitrogen modelling 1 introduction effective grassland models allow researchers to evaluate different management strategies predict how the productivity and quality of grassland will change over time anticipate the consequences of climate change and generally gain a better understanding of grassland ecosystems different types of models have different ranges of applicability and effectiveness some are applicable over wide spatial scales while others are site specific some work well in certain regions but are less useful in other areas our research considers two very different approaches to modelling the first is an empirical model generated through stepwise regression on climatic locational and managerial variables and the second is a process based dynamic model namely century described by parton et al 1987 empirical pasture models may be site specific or they can be applied at a larger e g regional or national scale armstrong et al 1997 hurtado uria et al 2014 trnka et al 2006 these are simpler and therefore faster and less computationally demanding than process based models and require less input data qi et al 2017 compared the outputs of a process based model for the productivity of several grassland sites in the uk with those of an empirical meta model derived from the outputs of the same process based model while the empirical model accounted for less variation as would be expected it still produced sufficiently precise estimations of pasture yield there are disadvantages of empirical models unlike dynamic models they are restricted to a single output qi et al 2017 they are subject to issues with co linearity between predictor variables and they assume that past relationships will hold in the future lobell and burke 2010 they are also only applicable within the confines of the experiments which contributed to their development i e they cannot be used to predict grassland yield or quality under climate or management conditions different from those original experiments despite the drawbacks of this method it is still useful in determining trends in grassland responses to weather and management variation dynamic models simulate the different processes in a system looking at how the system changes over time they can be seen as being more biologically realistic than empirical models they are usually applied to a single site or several homogeneous sites and require a large number of inputs korhonen et al 2018 applied several different dynamic models to timothy grass swards in northern europe and canada and found that the more detailed the model the more accurate the results however highly detailed models require large amounts of input data making it difficult to apply more complex models to sites where only limited data is available the wide variety of grassland ecosystems also makes it difficult to develop a one size fits all model while models can be parametrised to individual sites there will always be areas where they function less well trnka et al 2006 a broad range of dynamic models exists for modelling grasslands as summarised by bellocchi et al 2013 and chang et al 2013 we chose to use the century model this is a tool for ecosystem analysis and can be applied to croplands forests and grasslands it has a focus on carbon nitrogen and water fluxes in the plant soil system and runs on a monthly time step it also allows for complex agricultural management practices metherell et al 1993 it was selected because the grassland part of the model is relatively simple and requires fewer inputs than many other dynamic grassland models it can be applied to a diverse range of grasslands and also because it has a relatively fast run time a daily version of century exists dailydaycent but this takes considerably longer to run and requires more input information having a relatively small number of inputs makes it easier to implement the model on a range of sites particularly as some sites have only very limited information available the main relevant inputs are grassland type temperature precipitation grassland management and soil properties century has predominantly been used to model soil carbon c and nitrogen n dynamics though parton et al 1993 used it to model plant production at several grassland sites around the world they found that the predictions were within 25 of the observations 60 of the time and that century produced slightly higher r2 values than empirical models century is designed to work on a wide range of ecosystems meaning that it can be applied throughout europe other modelling approaches such as ensemble modelling sándor et al 2017 and integrated assessment modelling rose 2014 were also considered however we wished to prioritise fast run times in order to be able to perform a detailed sensitivity analysis we also wanted to minimise the input information required so that we could apply the models to as many sites as possible the other approaches considered were not compatible with these goals in the present study we aim to evaluate the two modelling approaches one statistical and one dynamic in different climatic zones across europe for both permanent and temporary grasslands considering both yield dry matter production and n content these outputs were chosen due to their importance to grassland based livestock systems and also because while yield has been widely modelled with these methodologies n content has not no attempt has been made to develop regression equations to model grassland n content over large spatial scales similarly century has not generally been used to consider plant n content and so little is known about its effectiveness this research will address these gaps and determine if regression and or century are effective ways of modelling grassland n content we will also investigate the sensitivity of each model to input uncertainties and the circumstances under which each of the models performs best this will inform future grassland modelling work by enabling researchers to better evaluate their results when using similar models for predictive purposes such as looking at the effects of climate change or considering alternate management practices 2 methods 2 1 data both approaches required data from grassland experiments across europe to be included these experiments had to have recorded harvested plant dry matter and or n content over a period of at least three years the experimental data was assembled from published literature and through contacting experts and relevant institutions the locations of these experiments are shown in fig 1 the sites were divided into five geographic regions alpine atlantic continental northern and southern this regional classification is consistent with the climatic zones used by the intergovernmental panel on climate change ipcc sites were also divided into permanent and temporary grasslands permanent grasslands are dominated by one or more species of grass though may include many different plant types they have been used continuously as grassland for at least five years temporary grasslands are usually 100 grass or else a grass legume mixture and produce high yields they have been used as grassland for less than five years in making these divisions by region and grassland type we aimed to account for as much of the existing variation in grasslands as possible while still being able to group them in a manageable way furthermore more data was usually available for the temporary sites than the permanent ones in particular data on species composition so by separating the two types we were able to do a more detailed analysis of temporary grasslands than would otherwise have been possible the full list of sites can be found in appendix a monthly temperature and precipitation data for all sites was taken from the climatic research unit gridded dataset uea cru et al 2017 2 2 regression model to ensure that no single site dominated the analysis data from each experimental site was edited so that all those for a given region and grassland type contributed approximately the same number of data points each dataset was then divided into four quarters three quarters of the data from all datasets were used as input to a stepwise regression process in r r core team 2017 this was done separately for each grassland type and for both yield and n content resulting in the following equations yield permanent grassland yield t dm ha α0 αregion α1rainjfm α2rainamj α3rainja α4tempfm α5tempamj α6tempja α7rainjfm 2 α8rainamj 2 α9rainja 2 α10tempja 2 α11altitude α12cuts α13nf α14cuts2 α15nf2 α16nf rainjfm α17nf tempja applicable to the alpine atlantic continental and northern regions yield temporary grassland yield tdm ha β0 βregion β1rainjfm β2rainamj β3rainja β4tempjf β5tempma β6tempmj β7tempja β8rainjfm 2 β9rainamj 2 β10rainja 2 β11tempmj 2 β12tempja 2 β13altitude β14cuts β15legume β16nf β17altitude2 β18cuts2 β19legume2 β20nf2 β21nf rainja β22nf cuts applicable to the atlantic continental northern and southern regions n content permanent grassland n content kg ha γ0 γ1rainmarch γ2rainam γ3rainjja γ4tempjanuary γ5tempaugust γ6rainmarch 2 γ7rainjja 2 γ8altitude γ9cuts γ10cuts2 γ11nf γ12nf rainmarch γ13nf tempjanuary γ14nf tempaugust γ15nf cuts applicable to the continental region n content temporary grassland n content kg ha δ0 δregion δ1rainam δ2rainjja δ3tempjf δ4tempma δ5tempjja δ6rainam 2 δ7rainjja 2 δ8tempjf 2 δ9tempma 2 δ10tempjja 2 δ11altitude δ12cuts δ13legume δ14nf δ15altitude2 δ16cuts2 δ17legume2 δ18nf2 δ19nf tempma δ20nf cuts applicable to the atlantic continental and northern regions coefficients for these equations are listed in appendix b subscripts indicate months of the year for example rainam is total rainfall in april and may tempjja is average temperature in june july and august altitude is measured in metres cuts indicates the number of harvests per year legume is the percentage of nitrogen fixing plants at seeding for example 5 would be taken as 5 0 in the equation nf is the amount of nitrogen fertiliser used per year kg n ha these equations are only applicable to certain regions due to the availability of data for developing the equations the remaining quarter of the data was used for validation the process was then repeated a further three times with a different quarter being used for validation each time this permutational approach helps to prevent over fitting and allows standard errors of the resulting root mean squared errors rmses and correlations to be calculated 2 3 century model while the century model requires relatively little input information compared with many other dynamic ecosystem models it still requires certain site specific information and sufficient data for model parameterisation very few sites met all the necessary requirements six sites were eventually selected based on the availability of necessary information and also to ensure a range of sites from different regions and of different grassland types the selected sites are listed in table 1 the model was only applied to one temporary grassland site this was because temporary grassland experiments tended to be of much shorter duration and there was insufficient data to parameterise the model at the selected site hurley uk data from each of seven annual harvests was available rather than just an annual total harvested yield was measured at all sites but n content was only measured in four of the six experiments in order to optimally parameterise the century model the input parameters having the greatest effect on plant yield and n content were first identified this was done through a review of relevant literature necpálová et al 2015 rafique et al 2015 wang et al 2013 wu et al 2014 expert consultation and preliminary data analysis the sensitivity of the model to each suggested parameter was tested by checking how much the predicted yield and n content changed when the parameter was varied within a reasonable range the identified relevant parameters are shown in table 2 parameters representing the effects of temperature on growth ppdf 1 4 were often cited in the literature as being particularly relevant however it was found that including them in the optimisation process often led to over fitting and produced unrealistic predictions when the model was applied to anything other than the original experimental conditions instead reasonable values for these parameters were chosen based on preliminary model runs on the available data and century documentation for each site optimal values for the parameters were attained through markov chain monte carlo mcmc optimisation using the l bfgs b algorithm within the python scipy module jones et al 2001 the optimisation routine minimised the total error x where x s o i l c i y i n i y i r m s e p y o y o y for fertiliser treatment i n i r m s e p n o n o n for fertiliser treatment i rmse a b is the root mean squared error between a and b p y and p n are the model predictions for yield and plant n content o y and o n are the experimental observations for yield and plant n content o y and o n are the mean experimental observations for yield and plant n content soilc 100 gradient of total soil carbon at end of spin up period 3 a century simulation begins with a long spin up period which allows the system to stabilise before the experimental period begins by including the gradient of total soil carbon at the end of the spin up period as part of the error term we ensured that the parameter values chosen enable this stabilisation to be achieved this precise choice of gradient term was achieved through trial and error and is designed not to dominate the error term x while still achieving a sufficiently stable state the optimisation procedure was run for multiple management regimes e g varying fertiliser treatments mowing frequency grazing intensity etc depending on the availability of measured data simultaneously in order to obtain a single set of optimal parameters for each site applicable to all situations 2 4 model fit to assess the goodness of fit of the century model predicted and observed values for average yield and n content were compared and corresponding standard errors were evaluated in addition the rmse and correlation between predicted and observed yields and n content were calculated for both models and the rmse were divided into bias and variance terms 2 5 sensitivity analysis we looked at the sensitivity of the model predictions to uncertainty in different input parameters these are shown in table 3 along with ranges for their potential uncertainty based on fitton et al 2014 and gottschalk et al 2007 these parameters are prone to measurement errors or else were estimated from other sources rather than being measured on site and could lead to inaccuracies such errors have the potential to propagate through the models and influence the results by conducting a sensitivity analysis we determine how uncertainties in each input affect uncertainty in our modelled estimates for both models we calculated the contribution of each parameter as a percentage of the total uncertainty to do this we first calculated the standard deviation in the total uncertainty σ g when varying all parameters simultaneously within their uncertainty ranges this was done by running the model until σ g converged approximately 5000 runs with different combinations of parameters in each run the choice of parameter values was determined using latin hypercube sampling for reasons of computational efficiency which was implemented in python we repeated this process multiple times now keeping one parameter at its original value while allowing the others to vary this allowed us to calculate the standard deviation in the simulations with parameter i set to its original value σ i these values were used to calculate the contribution index c i for each parameter i as follows c i σ g σ i i 1 i m a x σ g σ i 100 where i max is the number of input parameters varied the higher the c i the greater the contribution of that parameter to the total uncertainty this methodology is based on that of gottschalk et al 2007 for the regression model we performed this process twice for each regression equation and each region once with the average fertiliser level from the experiments conducted in that region and once with no fertiliser the weather inputs were the monthly averages from the original experiments for the given region for century we performed this process for each fertiliser level used in the original experiments table 1 for the century model we also investigated the linearity of the uncertainty propagation for each parameter this was not necessary for the regression models since the linearity is obvious from the equations for each parameter we ran the model ten times setting the parameter to ten equally spaced steps within the uncertainty range while leaving the other parameters at their original values we then found the best fit regression using r between the change in yield or n content from the original prediction and the parameter value with terms of different orders for example for soil ph c h a n g e i n m o d e l p r e d i c t i o n α 0 α 1 p h α 2 p h 2 α 3 p h 3 α 4 p h 4 by comparing the r 2 values of this regression equation with an equivalent linear equation and by seeing which of the α i were statistically significant p 0 05 we could determine the linearity or non linearity of the model s response to uncertainty in a given parameter this was done for each of the five parameters and the analysis was performed separately for each site and fertiliser treatment this methodology is based on that of fitton et al 2014 and hastings et al 2010 3 results 3 1 regression model looking at the coefficients of the regression equations appendix b some trends become apparent for both yield and n content rainfall usually has a positive effect but when these terms are squared they are usually negative suggesting that exceptionally high rainfall decreases yield and n content higher spring temperatures lead to higher yields while higher winter temperatures lead to reduced n content and higher summer temperatures increase it more cuts per year implies high yields and n content but only up to a certain point with the cuts 2 term always being negative indicating that excessive harvests reduce yield and n content a similar effect was seen for legume percentage in temporary grasslands with both yield and n content increasing up to a certain threshold beyond which they begin to decrease the goodness of fit of the equations is evaluated in table 4 in all cases the fit was reasonably good with high correlations but also relatively high rmses though the latter were due entirely to variation rather than bias the equations for n content had better fit than those for yield having higher r2 values and correlations the models were usually similarly good for permanent and temporary grasslands though the rmses for permanent grasslands were slightly higher than those for temporary 3 2 century model the goodness of fit of the parameterised models is shown in table 5 the observed and predicted means were usually very close to one another as such the rmse tended to be dominated by variance rather than bias the correlations between predictions and observations showed more variation ranging from no correlation iceland to quite high correlation hurley it should also be noted that the standard errors of the predicted means were always less than those of the observed means for both yield and n content the predictions showed considerably less inter annual variation than there was in reality for yield the greatest discrepancies between observed and predicted means were in the atlantic region when fertiliser was used this region also had some of the highest rmses for permanent grasslands though many of the rmses were quite high two sites exhibited no correlation between observed and predicted yields these being the alpine site with fertiliser and the northern site without fertiliser for n content the model performed very well for the atlantic site though it is not clear if this is due to the region or due to it being the only temporary grassland in the analysis the model also performed well for the alpine site under the low fertiliser treatment the model was less successful at predicting n content in the continental and northern regions and was particularly poor in the northern region when no fertiliser was used where there was a large discrepancy between the predicted and observed means a high rmse and no correlation overall the dynamic model performed best in the atlantic region especially for the temporary grassland site and particularly poorly in the alpine region with high fertiliser use and the northern region with no fertiliser use 3 3 sensitivity analysis 3 3 1 regression model the sensitivity analysis results for the regression model are shown in table 6 there was no apparent difference in the variation of yield and n content between the fertiliser treatments when the input parameters were varied there was a much higher level of variation for southern temporary grasslands than in other regions while it appears that temporary grasslands exhibit more variation than permanent ones these are not comparable as the regression equations for permanent grasslands do not account for legume percentage and so this could not be varied uncertainty associated with precipitation measurements was by far the largest contributor to total uncertainty often accounting for more than 80 the exception was for yields of permanent grasslands in the continental region where temperature uncertainties had much more of an influence the contribution indices show that there was generally very little difference between the distribution of uncertainty in the fertilised and unfertilised cases though there were large differences in these distributions for yields of permanent grasslands in the atlantic and continental regions 3 3 2 century model the standard deviations of the total uncertainty σ g for each site are shown in fig 2 there was considerably more variation at the atlantic permanent site than at any of the others while for the atlantic temporary site the variation was very small the contribution indices for each site are shown in fig 3 overall the weather parameters made the greatest contribution to the total uncertainty with the soil parameters often contributing a negligible amount uncertainty in the yield results was usually due to the same input parameters as uncertainty in the n content results though the alpine site was a notable exception to this here the yield uncertainty was almost exclusively due to temperature variations 93 98 while for n content it was almost exclusively due to uncertainties in the precipitation amount 94 96 for the atlantic permanent and continental sites most of the uncertainty was due to potential precipitation errors 66 99 while for the northern region it was primarily due to potential temperature errors 51 88 results for the atlantic temporary and southern sites were more mixed with no one parameter dominating the uncertainty and with very different combinations of parameters making up the uncertainty for yield and n content and for the different fertiliser treatments though neither site was sensitive to variations in soil ph when each parameter was varied individually the results for yield and n content were very similar changing soil ph generally had very little effect on either yield or n content except at the atlantic permanent site where reducing soil ph led to large increases in both yield and n content 26 and 44 respectively varying the soil clay content also had little influence except at the southern european site where it did have an effect particularly for yield when no fertiliser was used ranging from a 7 increase with decreasing clay content to an 8 decrease with increasing clay content here the uncertainty propagation was linear when fertiliser was used but non linear without fertiliser varying soil bulk density led to some small changes in plant yield and n content again this was most noticeable at the southern site with no fertiliser 9 yield increase and 12 n content increase when bulk density is increased plant responses to uncertainty in bulk density were usually linear changing precipitation amounts had an effect at all sites and the uncertainty propagation was always non linear except for n content at the alpine site reductions in precipitation nearly always led to decreases in both yield and n content while increasing precipitation generally led to either increasing yields and n content or else very little change the strongest responses were at the atlantic permanent continental and southern sites the largest being a 42 decrease in n content at the atlantic permanent site with decreasing precipitation for temperature the results were very mixed there tended to be a greater response to changes in temperature under the no low fertiliser treatments though the direction of the response varied between the sites the uncertainty propagation was always linear at the northern site and always non linear at the continental and atlantic temporary sites but varied for the other locations full results can be found in the supplementary materials 4 discussion the present study set out to model the yield and n content of european grasslands using both a statistical regression and a dynamic model approach the models goodness of fit and sensitivity to input uncertainties were considered the results presented above address these objectives 4 1 regression model looking at the r2 values and the correlations for the regression equations there was a very good fit with the observed data also the standard errors of these measures were very low suggesting that the models were not over fitted however the rmses were relatively high likely due to the considerable amount of variation amongst the experimental sites and the large geographical regions involved it is not surprising that the equations for permanent grasslands produce higher rmses than those for temporary grasslands since permanent grasslands tend to be more variable and have a higher degree of plant species diversity and are therefore less predictable several previous studies have found difficulties with using a linear regression methodology to relate plant yields with weather conditions such as low signal to noise ratios lobell and burke 2010 large numbers of relevant variables and interactions of variables many of which were correlated with one another or were non linear and extreme climatic events having an influence lasting multiple years jenkinson et al 1994 these factors may also partly explain the high rmses though it is encouraging that there was no evidence of bias in the results suggesting that these regression equations can be a useful predictive tool albeit one which produces relatively large confidence intervals 4 2 century model for the century model there was more variance in the correlation coefficients than the error terms as the optimisation process minimised the rmse but did not look at correlation the hurley site had the largest discrepancies between predicted and observed annual totals this is likely because this experiment took place over a much shorter duration than the others so there were only four years of data to use for model parameterisation it is also the only temporary grassland site though without more temporary sites for comparison it is not clear if this has an influence on the fit of the model it is encouraging that the observed and predicted means were usually quite similar suggesting that while the model may struggle to capture inter annual variation it is producing the right value on average the instances where there was little to no correlation between predictions and observations sites in iceland switzerland with high fertiliser and germany with no fertiliser are more concerning while it is expected that the modelled results will not display the full range of inter annual variation because the model used monthly weather data rather than daily values it is hoped that they should pick up the general trends an absence of any correlation suggests that the model is not sufficiently capturing the effects of temperature and precipitation and these results should be treated with caution for the swiss site the high fertiliser treatment is very high 560 kg n ha 1a 1 and it may be that this is causing the model to allow grass growth to reach its maximum potential every year meaning it becomes relatively insensitive to weather parton et al 1993 found a similar result i e a lack of inter annual variation for some sites in ukraine and russia when using century to model grassland live biomass though for other sites the model was more effective the use of a model with a monthly time step rather than daily also means that the effect of rainfall distribution is not captured a plant will respond differently to exceptionally heavy rain on one day than it will to the same amount of rain over a longer period the use of a daily model would account for this and it would likely have a better fit than century though it would have a considerably longer run time while we considered using dailydaycent the daily version of century for this study the time it takes to run would have meant that such in depth parameterisation and sensitivity analysis would not have been possible the effectiveness of the century model varied considerably between the sites grassland types and fertiliser levels there are indications that it performed less well in the alpine and northern sites two of the more climatically extreme locations and better in the atlantic region where it is more temperate but it is difficult to draw a firm conclusion from such a small number of sites there is some evidence that dynamic crop models perform less well in mountainous areas or under stress conditions timsina and humphreys 2003 xiong et al 2008 so it may be that such models are generally more reliable in temperate regions 4 3 sensitivity analysis some general trends were apparent across the different sensitivity tests the level and distribution of the uncertainty was usually about the same for different fertiliser treatments this is consistent with the findings of fitton et al 2014 and suggests that there is no significant interaction between fertiliser use and the sensitivity of yield and n content to measurement uncertainties in terms of the linearity of the models responses the main causes of variation were uncertainties in precipitation and temperature measurements for both models the responses to these uncertainties were usually non linear for the regression model this is apparent from the equations this is logical since plants response to precipitation and temperature is non linear in general there being optimal values for growth beyond which plant performance will decrease the large effect of uncertainty in precipitation measurements is likely because errors in precipitation are cumulative if the measurements are wrong by 1 mm a day then they can be wrong by up to 30 mm a month for the regression equations multiple months are grouped together further multiplying the error this is not the case for errors in temperature measurements where an error of 1 c in daily measurements will lead to the same error in average monthly measurements for the regression model yield predictions for the southern region displayed a particularly high amount of variability when the inputs were varied and this was due almost exclusively to variations in the amount of precipitation this region had by far the lowest amount of rainfall suggesting that drier regions are more sensitive to uncertainties in rainfall measurements than wetter regions this is likely because soil water reserves are lower in such areas and thus a reduction in rainfall has more effect on plant growth than it would in wetter regions southern europe is predicted to become drier as a result of climate change ipcc 2013 suggesting that irrigation may become increasing necessary as these results suggest that water limitation is already an issue for the century model when looking at the parameters individually the largest changes occurred when precipitation was varied and precipitation also often dominated the total uncertainty when the parameters were allowed to fluctuate simultaneously the other major contributor to the uncertainty being temperature when we identified the parameters having the greatest influence on plant yield and n content for the purposes of model parameterisation table 2 many of these related to temperature and precipitation effects it is therefore consistent that the sensitivity analysis has shown that the model is more sensitive to weather parameters than soil properties plant production in the century model is constrained by temperature and moisture metherell et al 1993 which is likely why grass yields were so sensitive to variations in these parameters necpálová et al 2015 found a similar sensitivity of crop productivity to temperature and soil moisture when applying dailydaycent to a corn soybean cropping system this fits with areas where growth is typically limited by short growing seasons due to low temperatures i e alpine and northern regions having most of their sensitivity being due to uncertainties in temperature measurements while areas where growth is not temperature limited e g atlantic and continental regions were more affected by uncertainties in precipitation measures it is not clear why the atlantic permanent site exhibited such a large degree of uncertainty compared with the other sites though it is consistent with this site also having the largest rmses in its yield predictions table 5 this site does not experience such extreme climatic conditions as some of the others suggesting that this uncertainty may be due to some local property possibly relating to soil characteristics management practices or species composition it is possible that legumes in the plot are generating cyclical dynamics for which the model is not accounting a possible reason for the century model s lack of sensitivity to soil properties is that the soil pools are stabilised during the spin up period a shorter spin up time may lead to more uncertainty in contrast fitton et al 2014 found that crop yields are mostly sensitive to soil ph and not at all to uncertainties in precipitation or temperature however they use a variation on the contribution index formula which will tend to give opposite results suggesting that our findings are in agreement the results emphasise the need to ensure that weather measurements are as precise as possible especially for precipitation if at all possible data from on site weather stations should be used rather than larger scale estimates on the other hand estimations of soil parameters rather than direct measurements are acceptable as small errors have little effect on the results 4 4 model comparison overall there was a greater amount of uncertainty in the regression model predictions than those from the century model i e the standard deviation when the inputs were varied was higher for the regression model this is likely because the century model applies to a single site whereas the regression models are valid over a large geographic region meaning that they are considerably less precise similarly the rmses from the regression model were at the high end of the range of those produced by century on the other hand the correlations between observed and predicted values from the regression results were higher than those from century this suggests that the regression approach is better at modelling trends in the annual response of grassland yields and n content to temperature and precipitation since the correlations are so high but it is less precise at predicting absolute values due to the high sensitivity and large rmses in terms of the models utility the regression model is applicable over very large spatial scales making it particularly useful for considering general trends for example the impacts of climate change however because this model is purely statistical it cannot be used to extrapolate beyond the bounds of the experiments which were used in its development century is usually applied to a single site or multiple homogeneous sites which makes it more useful for local considerations such as alterations to management practices because it is process based extrapolation to consider alternative scenarios is possible to some extent applying the regression model to a single site would be problematic due to its imprecision while applying century to large spatial scales would require a huge amount of input data century and dailydaycent have been applied over large scales using a gridded approach e g del grosso et al 2009 but this leads to very approximate results and requires considerable effort to determine suitable input parameters the relative performance of the two models suggests that they each have their benefits and limitations and that users should carefully consider which approach is more appropriate for their needs author contributions m d c t a d p g p g b and e w designed the research m d performed the research and analysed the results g p and n f advised on century parameterisation m d and d h wrote code for the models m d wrote the paper all other authors provided feedback on the paper declaration of competing interest none acknowledgements this work was supported by the horizon 2020 sfs 01c 2015 project entitled innovation of sustainable sheep and goat production in europe isage grant number 679302 and the rural environment science analytical services division of the scottish government bc3 is supported by the basque government through the berc 2018 2021 program and by spanish ministry of economy and competitiveness mineco through bc3 maría de maeztu excellence accreditation mdm 2017 0714 agustin del prado is supported by the ramon y cajal programme we would like to thank all the people who provided the data which made this work possible in particular professor wolfgang schmidt for data from the experimental botanical garden of göttingen university also the lawes agricultural trust and rothamsted research for data from the e ra database the rothamsted long term experiments national capability lte ncg is supported by the uk biotechnology and biological sciences research council and the lawes agricultural trust appendix c supplementary data the following is the supplementary data to this article sensitivity analysis results for the century model when parameters were varied individually multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104562 appendix a sites used for regression modelling permanent grasslands dataset location climatic region data available source south tyrol italy alpine yield peratoner et al 2010 pojorata suceava county romania alpine yield samuil et al 2011 kärkevagge valley sweden alpine yield olofsson and shams 2007 negrentino and pree switzerland alpine yield stampfli 2001 eschikon switzerland alpine yield schneider et al 2004 rothamsted england atlantic yield private communication cockle park england atlantic yield kidd et al 2017 lelystad the netherlands atlantic yield schils and snijders 2004 aberystwyth wales atlantic yield williams et al 2003 vienna austria continental yield karrer 2011 auvergne france continental yield klumpp et al 2011 göttingen germany continental yield n private communication stuttgart germany continental yield thumm and tonn 2010 eifel mountains germany continental yield schellberg et al 1999 eifel mountains germany continental yield hejcman et al 2010 czarny potok poland continental yield n kopeć and gondek 2014 iasi county romania continental yield samuil et al 2009 north western switzerland continental yield niklaus et al 2001 hvanneyri iceland northern yield brynjólfsson 2008 vėžaičiai lithuania northern yield butkutė and daugėlienė 2008 nåntuna sweden northern yield marissink et al 2002 temporary grasslands the agrodiversity experiment 24 sites used atlantic continental northern southern yield n kirwan et al 2014 biodepth 5 sites used continental northern southern yield hector et al 1999 fao sub network for lowland grasslands 10 sites used atlantic yield private communication gm20 21 sites across england and wales atlantic yield n morrison et al 1980 novi sad serbiabanja luka bosnia hercegovinapristina kosovo continental yield n ćupina et al 2017 pleven bulgaria continental yield vasilev 2012 tomaszkowo poland continental n bałuch małecka and olszewska 2007 central latvia northern yield rancane et al 2016 vėžaičiai lithuania northern yield skuodienė and repšienė 2008 appendix b coefficients of regression equations i αi βi γi δi 0 15 1128199 19 9492871 171 2297218 379 6930803 region alpine 0 atlantic 0 na atlantic 0 atlantic 3 2947027 continental 1 0002833 continental 5 2174092 continental 2 0093908 northern 2 3116753 northern 70 2426315 northern 2 8885051 southern 1 2554504 1 0 0067281 0 0160201 0 2110533 0 5719420 2 0 0069159 0 0131461 0 1571394 1 2061140 3 0 0169409 0 0245117 0 5471275 0 7157295 4 0 3917243 0 2989545 2 7136310 4 2274162 5 0 1889399 0 3006537 6 2716467 22 1656249 6 1 3063298 1 0667277 0 0039319 0 0021845 7 0 0000187 2 2108232 0 0008956 0 0017167 8 0 0000175 0 0000149 0 0983881 0 6348516 9 0 0000347 0 0000487 16 5380800 1 2036786 10 0 0262419 0 0000639 1 2203143 0 8367894 11 0 0042733 0 0340660 1 4488548 0 0309453 12 1 3375788 0 0556828 0 0010329 79 2531653 13 0 0014676 0 0133913 0 0217244 5 0620701 14 0 1259848 3 7554609 0 0436554 0 0260712 15 0 0000182 0 1696452 0 0481049 0 0001132 16 0 0000355 0 0075429 11 4084793 17 0 0017150 0 0000353 0 0657122 18 0 4353109 0 0004892 19 0 0026230 0 0538573 20 0 0000339 0 1806854 21 0 0000369 22 0 0033288 
26125,we applied two approaches to model grassland yield and nitrogen n content the first was a series of regression equations the second was the century dynamic model the regression model was generated from data from eighty nine experimental sites across europe distinguishing between five climatic regions the century model was applied to six sites across these regions both approaches estimated mean grassland yields and n content reasonably well though the root mean squared error tended to be lower for the dynamic model the regression model achieved better correlations between observed and predicted values both models were more sensitive to uncertainties in weather than in soil properties with precipitation often accounting for the majority of model uncertainty the regression approach is applicable over large spatial scales but lacks precision making it suitable for considering general trends century is better applied at a local level where more detailed and specific analysis is required keywords grasslands yield nitrogen modelling 1 introduction effective grassland models allow researchers to evaluate different management strategies predict how the productivity and quality of grassland will change over time anticipate the consequences of climate change and generally gain a better understanding of grassland ecosystems different types of models have different ranges of applicability and effectiveness some are applicable over wide spatial scales while others are site specific some work well in certain regions but are less useful in other areas our research considers two very different approaches to modelling the first is an empirical model generated through stepwise regression on climatic locational and managerial variables and the second is a process based dynamic model namely century described by parton et al 1987 empirical pasture models may be site specific or they can be applied at a larger e g regional or national scale armstrong et al 1997 hurtado uria et al 2014 trnka et al 2006 these are simpler and therefore faster and less computationally demanding than process based models and require less input data qi et al 2017 compared the outputs of a process based model for the productivity of several grassland sites in the uk with those of an empirical meta model derived from the outputs of the same process based model while the empirical model accounted for less variation as would be expected it still produced sufficiently precise estimations of pasture yield there are disadvantages of empirical models unlike dynamic models they are restricted to a single output qi et al 2017 they are subject to issues with co linearity between predictor variables and they assume that past relationships will hold in the future lobell and burke 2010 they are also only applicable within the confines of the experiments which contributed to their development i e they cannot be used to predict grassland yield or quality under climate or management conditions different from those original experiments despite the drawbacks of this method it is still useful in determining trends in grassland responses to weather and management variation dynamic models simulate the different processes in a system looking at how the system changes over time they can be seen as being more biologically realistic than empirical models they are usually applied to a single site or several homogeneous sites and require a large number of inputs korhonen et al 2018 applied several different dynamic models to timothy grass swards in northern europe and canada and found that the more detailed the model the more accurate the results however highly detailed models require large amounts of input data making it difficult to apply more complex models to sites where only limited data is available the wide variety of grassland ecosystems also makes it difficult to develop a one size fits all model while models can be parametrised to individual sites there will always be areas where they function less well trnka et al 2006 a broad range of dynamic models exists for modelling grasslands as summarised by bellocchi et al 2013 and chang et al 2013 we chose to use the century model this is a tool for ecosystem analysis and can be applied to croplands forests and grasslands it has a focus on carbon nitrogen and water fluxes in the plant soil system and runs on a monthly time step it also allows for complex agricultural management practices metherell et al 1993 it was selected because the grassland part of the model is relatively simple and requires fewer inputs than many other dynamic grassland models it can be applied to a diverse range of grasslands and also because it has a relatively fast run time a daily version of century exists dailydaycent but this takes considerably longer to run and requires more input information having a relatively small number of inputs makes it easier to implement the model on a range of sites particularly as some sites have only very limited information available the main relevant inputs are grassland type temperature precipitation grassland management and soil properties century has predominantly been used to model soil carbon c and nitrogen n dynamics though parton et al 1993 used it to model plant production at several grassland sites around the world they found that the predictions were within 25 of the observations 60 of the time and that century produced slightly higher r2 values than empirical models century is designed to work on a wide range of ecosystems meaning that it can be applied throughout europe other modelling approaches such as ensemble modelling sándor et al 2017 and integrated assessment modelling rose 2014 were also considered however we wished to prioritise fast run times in order to be able to perform a detailed sensitivity analysis we also wanted to minimise the input information required so that we could apply the models to as many sites as possible the other approaches considered were not compatible with these goals in the present study we aim to evaluate the two modelling approaches one statistical and one dynamic in different climatic zones across europe for both permanent and temporary grasslands considering both yield dry matter production and n content these outputs were chosen due to their importance to grassland based livestock systems and also because while yield has been widely modelled with these methodologies n content has not no attempt has been made to develop regression equations to model grassland n content over large spatial scales similarly century has not generally been used to consider plant n content and so little is known about its effectiveness this research will address these gaps and determine if regression and or century are effective ways of modelling grassland n content we will also investigate the sensitivity of each model to input uncertainties and the circumstances under which each of the models performs best this will inform future grassland modelling work by enabling researchers to better evaluate their results when using similar models for predictive purposes such as looking at the effects of climate change or considering alternate management practices 2 methods 2 1 data both approaches required data from grassland experiments across europe to be included these experiments had to have recorded harvested plant dry matter and or n content over a period of at least three years the experimental data was assembled from published literature and through contacting experts and relevant institutions the locations of these experiments are shown in fig 1 the sites were divided into five geographic regions alpine atlantic continental northern and southern this regional classification is consistent with the climatic zones used by the intergovernmental panel on climate change ipcc sites were also divided into permanent and temporary grasslands permanent grasslands are dominated by one or more species of grass though may include many different plant types they have been used continuously as grassland for at least five years temporary grasslands are usually 100 grass or else a grass legume mixture and produce high yields they have been used as grassland for less than five years in making these divisions by region and grassland type we aimed to account for as much of the existing variation in grasslands as possible while still being able to group them in a manageable way furthermore more data was usually available for the temporary sites than the permanent ones in particular data on species composition so by separating the two types we were able to do a more detailed analysis of temporary grasslands than would otherwise have been possible the full list of sites can be found in appendix a monthly temperature and precipitation data for all sites was taken from the climatic research unit gridded dataset uea cru et al 2017 2 2 regression model to ensure that no single site dominated the analysis data from each experimental site was edited so that all those for a given region and grassland type contributed approximately the same number of data points each dataset was then divided into four quarters three quarters of the data from all datasets were used as input to a stepwise regression process in r r core team 2017 this was done separately for each grassland type and for both yield and n content resulting in the following equations yield permanent grassland yield t dm ha α0 αregion α1rainjfm α2rainamj α3rainja α4tempfm α5tempamj α6tempja α7rainjfm 2 α8rainamj 2 α9rainja 2 α10tempja 2 α11altitude α12cuts α13nf α14cuts2 α15nf2 α16nf rainjfm α17nf tempja applicable to the alpine atlantic continental and northern regions yield temporary grassland yield tdm ha β0 βregion β1rainjfm β2rainamj β3rainja β4tempjf β5tempma β6tempmj β7tempja β8rainjfm 2 β9rainamj 2 β10rainja 2 β11tempmj 2 β12tempja 2 β13altitude β14cuts β15legume β16nf β17altitude2 β18cuts2 β19legume2 β20nf2 β21nf rainja β22nf cuts applicable to the atlantic continental northern and southern regions n content permanent grassland n content kg ha γ0 γ1rainmarch γ2rainam γ3rainjja γ4tempjanuary γ5tempaugust γ6rainmarch 2 γ7rainjja 2 γ8altitude γ9cuts γ10cuts2 γ11nf γ12nf rainmarch γ13nf tempjanuary γ14nf tempaugust γ15nf cuts applicable to the continental region n content temporary grassland n content kg ha δ0 δregion δ1rainam δ2rainjja δ3tempjf δ4tempma δ5tempjja δ6rainam 2 δ7rainjja 2 δ8tempjf 2 δ9tempma 2 δ10tempjja 2 δ11altitude δ12cuts δ13legume δ14nf δ15altitude2 δ16cuts2 δ17legume2 δ18nf2 δ19nf tempma δ20nf cuts applicable to the atlantic continental and northern regions coefficients for these equations are listed in appendix b subscripts indicate months of the year for example rainam is total rainfall in april and may tempjja is average temperature in june july and august altitude is measured in metres cuts indicates the number of harvests per year legume is the percentage of nitrogen fixing plants at seeding for example 5 would be taken as 5 0 in the equation nf is the amount of nitrogen fertiliser used per year kg n ha these equations are only applicable to certain regions due to the availability of data for developing the equations the remaining quarter of the data was used for validation the process was then repeated a further three times with a different quarter being used for validation each time this permutational approach helps to prevent over fitting and allows standard errors of the resulting root mean squared errors rmses and correlations to be calculated 2 3 century model while the century model requires relatively little input information compared with many other dynamic ecosystem models it still requires certain site specific information and sufficient data for model parameterisation very few sites met all the necessary requirements six sites were eventually selected based on the availability of necessary information and also to ensure a range of sites from different regions and of different grassland types the selected sites are listed in table 1 the model was only applied to one temporary grassland site this was because temporary grassland experiments tended to be of much shorter duration and there was insufficient data to parameterise the model at the selected site hurley uk data from each of seven annual harvests was available rather than just an annual total harvested yield was measured at all sites but n content was only measured in four of the six experiments in order to optimally parameterise the century model the input parameters having the greatest effect on plant yield and n content were first identified this was done through a review of relevant literature necpálová et al 2015 rafique et al 2015 wang et al 2013 wu et al 2014 expert consultation and preliminary data analysis the sensitivity of the model to each suggested parameter was tested by checking how much the predicted yield and n content changed when the parameter was varied within a reasonable range the identified relevant parameters are shown in table 2 parameters representing the effects of temperature on growth ppdf 1 4 were often cited in the literature as being particularly relevant however it was found that including them in the optimisation process often led to over fitting and produced unrealistic predictions when the model was applied to anything other than the original experimental conditions instead reasonable values for these parameters were chosen based on preliminary model runs on the available data and century documentation for each site optimal values for the parameters were attained through markov chain monte carlo mcmc optimisation using the l bfgs b algorithm within the python scipy module jones et al 2001 the optimisation routine minimised the total error x where x s o i l c i y i n i y i r m s e p y o y o y for fertiliser treatment i n i r m s e p n o n o n for fertiliser treatment i rmse a b is the root mean squared error between a and b p y and p n are the model predictions for yield and plant n content o y and o n are the experimental observations for yield and plant n content o y and o n are the mean experimental observations for yield and plant n content soilc 100 gradient of total soil carbon at end of spin up period 3 a century simulation begins with a long spin up period which allows the system to stabilise before the experimental period begins by including the gradient of total soil carbon at the end of the spin up period as part of the error term we ensured that the parameter values chosen enable this stabilisation to be achieved this precise choice of gradient term was achieved through trial and error and is designed not to dominate the error term x while still achieving a sufficiently stable state the optimisation procedure was run for multiple management regimes e g varying fertiliser treatments mowing frequency grazing intensity etc depending on the availability of measured data simultaneously in order to obtain a single set of optimal parameters for each site applicable to all situations 2 4 model fit to assess the goodness of fit of the century model predicted and observed values for average yield and n content were compared and corresponding standard errors were evaluated in addition the rmse and correlation between predicted and observed yields and n content were calculated for both models and the rmse were divided into bias and variance terms 2 5 sensitivity analysis we looked at the sensitivity of the model predictions to uncertainty in different input parameters these are shown in table 3 along with ranges for their potential uncertainty based on fitton et al 2014 and gottschalk et al 2007 these parameters are prone to measurement errors or else were estimated from other sources rather than being measured on site and could lead to inaccuracies such errors have the potential to propagate through the models and influence the results by conducting a sensitivity analysis we determine how uncertainties in each input affect uncertainty in our modelled estimates for both models we calculated the contribution of each parameter as a percentage of the total uncertainty to do this we first calculated the standard deviation in the total uncertainty σ g when varying all parameters simultaneously within their uncertainty ranges this was done by running the model until σ g converged approximately 5000 runs with different combinations of parameters in each run the choice of parameter values was determined using latin hypercube sampling for reasons of computational efficiency which was implemented in python we repeated this process multiple times now keeping one parameter at its original value while allowing the others to vary this allowed us to calculate the standard deviation in the simulations with parameter i set to its original value σ i these values were used to calculate the contribution index c i for each parameter i as follows c i σ g σ i i 1 i m a x σ g σ i 100 where i max is the number of input parameters varied the higher the c i the greater the contribution of that parameter to the total uncertainty this methodology is based on that of gottschalk et al 2007 for the regression model we performed this process twice for each regression equation and each region once with the average fertiliser level from the experiments conducted in that region and once with no fertiliser the weather inputs were the monthly averages from the original experiments for the given region for century we performed this process for each fertiliser level used in the original experiments table 1 for the century model we also investigated the linearity of the uncertainty propagation for each parameter this was not necessary for the regression models since the linearity is obvious from the equations for each parameter we ran the model ten times setting the parameter to ten equally spaced steps within the uncertainty range while leaving the other parameters at their original values we then found the best fit regression using r between the change in yield or n content from the original prediction and the parameter value with terms of different orders for example for soil ph c h a n g e i n m o d e l p r e d i c t i o n α 0 α 1 p h α 2 p h 2 α 3 p h 3 α 4 p h 4 by comparing the r 2 values of this regression equation with an equivalent linear equation and by seeing which of the α i were statistically significant p 0 05 we could determine the linearity or non linearity of the model s response to uncertainty in a given parameter this was done for each of the five parameters and the analysis was performed separately for each site and fertiliser treatment this methodology is based on that of fitton et al 2014 and hastings et al 2010 3 results 3 1 regression model looking at the coefficients of the regression equations appendix b some trends become apparent for both yield and n content rainfall usually has a positive effect but when these terms are squared they are usually negative suggesting that exceptionally high rainfall decreases yield and n content higher spring temperatures lead to higher yields while higher winter temperatures lead to reduced n content and higher summer temperatures increase it more cuts per year implies high yields and n content but only up to a certain point with the cuts 2 term always being negative indicating that excessive harvests reduce yield and n content a similar effect was seen for legume percentage in temporary grasslands with both yield and n content increasing up to a certain threshold beyond which they begin to decrease the goodness of fit of the equations is evaluated in table 4 in all cases the fit was reasonably good with high correlations but also relatively high rmses though the latter were due entirely to variation rather than bias the equations for n content had better fit than those for yield having higher r2 values and correlations the models were usually similarly good for permanent and temporary grasslands though the rmses for permanent grasslands were slightly higher than those for temporary 3 2 century model the goodness of fit of the parameterised models is shown in table 5 the observed and predicted means were usually very close to one another as such the rmse tended to be dominated by variance rather than bias the correlations between predictions and observations showed more variation ranging from no correlation iceland to quite high correlation hurley it should also be noted that the standard errors of the predicted means were always less than those of the observed means for both yield and n content the predictions showed considerably less inter annual variation than there was in reality for yield the greatest discrepancies between observed and predicted means were in the atlantic region when fertiliser was used this region also had some of the highest rmses for permanent grasslands though many of the rmses were quite high two sites exhibited no correlation between observed and predicted yields these being the alpine site with fertiliser and the northern site without fertiliser for n content the model performed very well for the atlantic site though it is not clear if this is due to the region or due to it being the only temporary grassland in the analysis the model also performed well for the alpine site under the low fertiliser treatment the model was less successful at predicting n content in the continental and northern regions and was particularly poor in the northern region when no fertiliser was used where there was a large discrepancy between the predicted and observed means a high rmse and no correlation overall the dynamic model performed best in the atlantic region especially for the temporary grassland site and particularly poorly in the alpine region with high fertiliser use and the northern region with no fertiliser use 3 3 sensitivity analysis 3 3 1 regression model the sensitivity analysis results for the regression model are shown in table 6 there was no apparent difference in the variation of yield and n content between the fertiliser treatments when the input parameters were varied there was a much higher level of variation for southern temporary grasslands than in other regions while it appears that temporary grasslands exhibit more variation than permanent ones these are not comparable as the regression equations for permanent grasslands do not account for legume percentage and so this could not be varied uncertainty associated with precipitation measurements was by far the largest contributor to total uncertainty often accounting for more than 80 the exception was for yields of permanent grasslands in the continental region where temperature uncertainties had much more of an influence the contribution indices show that there was generally very little difference between the distribution of uncertainty in the fertilised and unfertilised cases though there were large differences in these distributions for yields of permanent grasslands in the atlantic and continental regions 3 3 2 century model the standard deviations of the total uncertainty σ g for each site are shown in fig 2 there was considerably more variation at the atlantic permanent site than at any of the others while for the atlantic temporary site the variation was very small the contribution indices for each site are shown in fig 3 overall the weather parameters made the greatest contribution to the total uncertainty with the soil parameters often contributing a negligible amount uncertainty in the yield results was usually due to the same input parameters as uncertainty in the n content results though the alpine site was a notable exception to this here the yield uncertainty was almost exclusively due to temperature variations 93 98 while for n content it was almost exclusively due to uncertainties in the precipitation amount 94 96 for the atlantic permanent and continental sites most of the uncertainty was due to potential precipitation errors 66 99 while for the northern region it was primarily due to potential temperature errors 51 88 results for the atlantic temporary and southern sites were more mixed with no one parameter dominating the uncertainty and with very different combinations of parameters making up the uncertainty for yield and n content and for the different fertiliser treatments though neither site was sensitive to variations in soil ph when each parameter was varied individually the results for yield and n content were very similar changing soil ph generally had very little effect on either yield or n content except at the atlantic permanent site where reducing soil ph led to large increases in both yield and n content 26 and 44 respectively varying the soil clay content also had little influence except at the southern european site where it did have an effect particularly for yield when no fertiliser was used ranging from a 7 increase with decreasing clay content to an 8 decrease with increasing clay content here the uncertainty propagation was linear when fertiliser was used but non linear without fertiliser varying soil bulk density led to some small changes in plant yield and n content again this was most noticeable at the southern site with no fertiliser 9 yield increase and 12 n content increase when bulk density is increased plant responses to uncertainty in bulk density were usually linear changing precipitation amounts had an effect at all sites and the uncertainty propagation was always non linear except for n content at the alpine site reductions in precipitation nearly always led to decreases in both yield and n content while increasing precipitation generally led to either increasing yields and n content or else very little change the strongest responses were at the atlantic permanent continental and southern sites the largest being a 42 decrease in n content at the atlantic permanent site with decreasing precipitation for temperature the results were very mixed there tended to be a greater response to changes in temperature under the no low fertiliser treatments though the direction of the response varied between the sites the uncertainty propagation was always linear at the northern site and always non linear at the continental and atlantic temporary sites but varied for the other locations full results can be found in the supplementary materials 4 discussion the present study set out to model the yield and n content of european grasslands using both a statistical regression and a dynamic model approach the models goodness of fit and sensitivity to input uncertainties were considered the results presented above address these objectives 4 1 regression model looking at the r2 values and the correlations for the regression equations there was a very good fit with the observed data also the standard errors of these measures were very low suggesting that the models were not over fitted however the rmses were relatively high likely due to the considerable amount of variation amongst the experimental sites and the large geographical regions involved it is not surprising that the equations for permanent grasslands produce higher rmses than those for temporary grasslands since permanent grasslands tend to be more variable and have a higher degree of plant species diversity and are therefore less predictable several previous studies have found difficulties with using a linear regression methodology to relate plant yields with weather conditions such as low signal to noise ratios lobell and burke 2010 large numbers of relevant variables and interactions of variables many of which were correlated with one another or were non linear and extreme climatic events having an influence lasting multiple years jenkinson et al 1994 these factors may also partly explain the high rmses though it is encouraging that there was no evidence of bias in the results suggesting that these regression equations can be a useful predictive tool albeit one which produces relatively large confidence intervals 4 2 century model for the century model there was more variance in the correlation coefficients than the error terms as the optimisation process minimised the rmse but did not look at correlation the hurley site had the largest discrepancies between predicted and observed annual totals this is likely because this experiment took place over a much shorter duration than the others so there were only four years of data to use for model parameterisation it is also the only temporary grassland site though without more temporary sites for comparison it is not clear if this has an influence on the fit of the model it is encouraging that the observed and predicted means were usually quite similar suggesting that while the model may struggle to capture inter annual variation it is producing the right value on average the instances where there was little to no correlation between predictions and observations sites in iceland switzerland with high fertiliser and germany with no fertiliser are more concerning while it is expected that the modelled results will not display the full range of inter annual variation because the model used monthly weather data rather than daily values it is hoped that they should pick up the general trends an absence of any correlation suggests that the model is not sufficiently capturing the effects of temperature and precipitation and these results should be treated with caution for the swiss site the high fertiliser treatment is very high 560 kg n ha 1a 1 and it may be that this is causing the model to allow grass growth to reach its maximum potential every year meaning it becomes relatively insensitive to weather parton et al 1993 found a similar result i e a lack of inter annual variation for some sites in ukraine and russia when using century to model grassland live biomass though for other sites the model was more effective the use of a model with a monthly time step rather than daily also means that the effect of rainfall distribution is not captured a plant will respond differently to exceptionally heavy rain on one day than it will to the same amount of rain over a longer period the use of a daily model would account for this and it would likely have a better fit than century though it would have a considerably longer run time while we considered using dailydaycent the daily version of century for this study the time it takes to run would have meant that such in depth parameterisation and sensitivity analysis would not have been possible the effectiveness of the century model varied considerably between the sites grassland types and fertiliser levels there are indications that it performed less well in the alpine and northern sites two of the more climatically extreme locations and better in the atlantic region where it is more temperate but it is difficult to draw a firm conclusion from such a small number of sites there is some evidence that dynamic crop models perform less well in mountainous areas or under stress conditions timsina and humphreys 2003 xiong et al 2008 so it may be that such models are generally more reliable in temperate regions 4 3 sensitivity analysis some general trends were apparent across the different sensitivity tests the level and distribution of the uncertainty was usually about the same for different fertiliser treatments this is consistent with the findings of fitton et al 2014 and suggests that there is no significant interaction between fertiliser use and the sensitivity of yield and n content to measurement uncertainties in terms of the linearity of the models responses the main causes of variation were uncertainties in precipitation and temperature measurements for both models the responses to these uncertainties were usually non linear for the regression model this is apparent from the equations this is logical since plants response to precipitation and temperature is non linear in general there being optimal values for growth beyond which plant performance will decrease the large effect of uncertainty in precipitation measurements is likely because errors in precipitation are cumulative if the measurements are wrong by 1 mm a day then they can be wrong by up to 30 mm a month for the regression equations multiple months are grouped together further multiplying the error this is not the case for errors in temperature measurements where an error of 1 c in daily measurements will lead to the same error in average monthly measurements for the regression model yield predictions for the southern region displayed a particularly high amount of variability when the inputs were varied and this was due almost exclusively to variations in the amount of precipitation this region had by far the lowest amount of rainfall suggesting that drier regions are more sensitive to uncertainties in rainfall measurements than wetter regions this is likely because soil water reserves are lower in such areas and thus a reduction in rainfall has more effect on plant growth than it would in wetter regions southern europe is predicted to become drier as a result of climate change ipcc 2013 suggesting that irrigation may become increasing necessary as these results suggest that water limitation is already an issue for the century model when looking at the parameters individually the largest changes occurred when precipitation was varied and precipitation also often dominated the total uncertainty when the parameters were allowed to fluctuate simultaneously the other major contributor to the uncertainty being temperature when we identified the parameters having the greatest influence on plant yield and n content for the purposes of model parameterisation table 2 many of these related to temperature and precipitation effects it is therefore consistent that the sensitivity analysis has shown that the model is more sensitive to weather parameters than soil properties plant production in the century model is constrained by temperature and moisture metherell et al 1993 which is likely why grass yields were so sensitive to variations in these parameters necpálová et al 2015 found a similar sensitivity of crop productivity to temperature and soil moisture when applying dailydaycent to a corn soybean cropping system this fits with areas where growth is typically limited by short growing seasons due to low temperatures i e alpine and northern regions having most of their sensitivity being due to uncertainties in temperature measurements while areas where growth is not temperature limited e g atlantic and continental regions were more affected by uncertainties in precipitation measures it is not clear why the atlantic permanent site exhibited such a large degree of uncertainty compared with the other sites though it is consistent with this site also having the largest rmses in its yield predictions table 5 this site does not experience such extreme climatic conditions as some of the others suggesting that this uncertainty may be due to some local property possibly relating to soil characteristics management practices or species composition it is possible that legumes in the plot are generating cyclical dynamics for which the model is not accounting a possible reason for the century model s lack of sensitivity to soil properties is that the soil pools are stabilised during the spin up period a shorter spin up time may lead to more uncertainty in contrast fitton et al 2014 found that crop yields are mostly sensitive to soil ph and not at all to uncertainties in precipitation or temperature however they use a variation on the contribution index formula which will tend to give opposite results suggesting that our findings are in agreement the results emphasise the need to ensure that weather measurements are as precise as possible especially for precipitation if at all possible data from on site weather stations should be used rather than larger scale estimates on the other hand estimations of soil parameters rather than direct measurements are acceptable as small errors have little effect on the results 4 4 model comparison overall there was a greater amount of uncertainty in the regression model predictions than those from the century model i e the standard deviation when the inputs were varied was higher for the regression model this is likely because the century model applies to a single site whereas the regression models are valid over a large geographic region meaning that they are considerably less precise similarly the rmses from the regression model were at the high end of the range of those produced by century on the other hand the correlations between observed and predicted values from the regression results were higher than those from century this suggests that the regression approach is better at modelling trends in the annual response of grassland yields and n content to temperature and precipitation since the correlations are so high but it is less precise at predicting absolute values due to the high sensitivity and large rmses in terms of the models utility the regression model is applicable over very large spatial scales making it particularly useful for considering general trends for example the impacts of climate change however because this model is purely statistical it cannot be used to extrapolate beyond the bounds of the experiments which were used in its development century is usually applied to a single site or multiple homogeneous sites which makes it more useful for local considerations such as alterations to management practices because it is process based extrapolation to consider alternative scenarios is possible to some extent applying the regression model to a single site would be problematic due to its imprecision while applying century to large spatial scales would require a huge amount of input data century and dailydaycent have been applied over large scales using a gridded approach e g del grosso et al 2009 but this leads to very approximate results and requires considerable effort to determine suitable input parameters the relative performance of the two models suggests that they each have their benefits and limitations and that users should carefully consider which approach is more appropriate for their needs author contributions m d c t a d p g p g b and e w designed the research m d performed the research and analysed the results g p and n f advised on century parameterisation m d and d h wrote code for the models m d wrote the paper all other authors provided feedback on the paper declaration of competing interest none acknowledgements this work was supported by the horizon 2020 sfs 01c 2015 project entitled innovation of sustainable sheep and goat production in europe isage grant number 679302 and the rural environment science analytical services division of the scottish government bc3 is supported by the basque government through the berc 2018 2021 program and by spanish ministry of economy and competitiveness mineco through bc3 maría de maeztu excellence accreditation mdm 2017 0714 agustin del prado is supported by the ramon y cajal programme we would like to thank all the people who provided the data which made this work possible in particular professor wolfgang schmidt for data from the experimental botanical garden of göttingen university also the lawes agricultural trust and rothamsted research for data from the e ra database the rothamsted long term experiments national capability lte ncg is supported by the uk biotechnology and biological sciences research council and the lawes agricultural trust appendix c supplementary data the following is the supplementary data to this article sensitivity analysis results for the century model when parameters were varied individually multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104562 appendix a sites used for regression modelling permanent grasslands dataset location climatic region data available source south tyrol italy alpine yield peratoner et al 2010 pojorata suceava county romania alpine yield samuil et al 2011 kärkevagge valley sweden alpine yield olofsson and shams 2007 negrentino and pree switzerland alpine yield stampfli 2001 eschikon switzerland alpine yield schneider et al 2004 rothamsted england atlantic yield private communication cockle park england atlantic yield kidd et al 2017 lelystad the netherlands atlantic yield schils and snijders 2004 aberystwyth wales atlantic yield williams et al 2003 vienna austria continental yield karrer 2011 auvergne france continental yield klumpp et al 2011 göttingen germany continental yield n private communication stuttgart germany continental yield thumm and tonn 2010 eifel mountains germany continental yield schellberg et al 1999 eifel mountains germany continental yield hejcman et al 2010 czarny potok poland continental yield n kopeć and gondek 2014 iasi county romania continental yield samuil et al 2009 north western switzerland continental yield niklaus et al 2001 hvanneyri iceland northern yield brynjólfsson 2008 vėžaičiai lithuania northern yield butkutė and daugėlienė 2008 nåntuna sweden northern yield marissink et al 2002 temporary grasslands the agrodiversity experiment 24 sites used atlantic continental northern southern yield n kirwan et al 2014 biodepth 5 sites used continental northern southern yield hector et al 1999 fao sub network for lowland grasslands 10 sites used atlantic yield private communication gm20 21 sites across england and wales atlantic yield n morrison et al 1980 novi sad serbiabanja luka bosnia hercegovinapristina kosovo continental yield n ćupina et al 2017 pleven bulgaria continental yield vasilev 2012 tomaszkowo poland continental n bałuch małecka and olszewska 2007 central latvia northern yield rancane et al 2016 vėžaičiai lithuania northern yield skuodienė and repšienė 2008 appendix b coefficients of regression equations i αi βi γi δi 0 15 1128199 19 9492871 171 2297218 379 6930803 region alpine 0 atlantic 0 na atlantic 0 atlantic 3 2947027 continental 1 0002833 continental 5 2174092 continental 2 0093908 northern 2 3116753 northern 70 2426315 northern 2 8885051 southern 1 2554504 1 0 0067281 0 0160201 0 2110533 0 5719420 2 0 0069159 0 0131461 0 1571394 1 2061140 3 0 0169409 0 0245117 0 5471275 0 7157295 4 0 3917243 0 2989545 2 7136310 4 2274162 5 0 1889399 0 3006537 6 2716467 22 1656249 6 1 3063298 1 0667277 0 0039319 0 0021845 7 0 0000187 2 2108232 0 0008956 0 0017167 8 0 0000175 0 0000149 0 0983881 0 6348516 9 0 0000347 0 0000487 16 5380800 1 2036786 10 0 0262419 0 0000639 1 2203143 0 8367894 11 0 0042733 0 0340660 1 4488548 0 0309453 12 1 3375788 0 0556828 0 0010329 79 2531653 13 0 0014676 0 0133913 0 0217244 5 0620701 14 0 1259848 3 7554609 0 0436554 0 0260712 15 0 0000182 0 1696452 0 0481049 0 0001132 16 0 0000355 0 0075429 11 4084793 17 0 0017150 0 0000353 0 0657122 18 0 4353109 0 0004892 19 0 0026230 0 0538573 20 0 0000339 0 1806854 21 0 0000369 22 0 0033288 
26126,existing water resources are under stress due to increasing demands associated with population and economic growth as well as the effects of climate change which can reduce available supply and increase demand alternative sources such as stormwater harvesting and treated wastewater are being considered in many cities to supplement existing water supplies stormwater harvesting schemes that supplement existing water supply systems are complex as they consist of an integrated set of components that perform the functions of collection storage treatment distribution and discharge many options and possible sizes exist for each component furthermore the selection and evaluation of stormwater harvesting schemes need to consider economic environmental and social criteria as well as technical feasibility in order to deal with this complexity an integrated framework is presented that assists authorities with determining if stormwater harvesting is preferable to other water sources and if so what type of stormwater scheme is best keywords framework stormwater harvesting multi criteria analysis benefit cost analysis environmental impact social impact alternative water sources cost energy and greenhouse gas emission database 1 introduction a number of major cities in the world are suffering stress on their water supply systems due to increases in population rainfall variability and climate change e g andreu et al 2009 hughes et al 2009 rosenberg and lund 2009 paton et al 2013 in response to these factors a number of cities are augmenting the more traditional surface and ground water sources with non traditional sources e g chung et al 2009 radcliffe 2010 leung et al 2012 maier et al 2013 beh et al 2014 paton et al 2014a b these sources include stormwater harvesting e g mitchell et al 2007 todeschini et al 2012 maier et al 2013 beh et al 2014 seawater desalination e g wittholz et al 2008 paton et al 2009 becker et al 2010 marchi et al 2016 di matteo et al 2017a and wastewater reuse e g harnett et al 2009 barton and argue 2009 leung et al 2012 stormwater harvesting is particularly attractive as a source of water supply as it provides other benefits such as reducing the load on stormwater infrastructure or combined sewer systems as well as reducing pollutant discharges to receiving waters pitt et al 2012 di matteo et al 2017a furthermore the volume of urban stormwater runoff is likely to be less affected by climate change than the runoff from rural catchments this is clearly demonstrated by the results of studies by sankarasubramanian and vogel 2003 and fu et al 2011 based on a study of 1337 river basins in the usa sankarasubramanian and vogel 2003 showed that a 10 reduction in precipitation is likely to produce a reduction in basin runoff of between 15 and 25 similarly fu et al 2011 found that a 10 reduction in precipitation is likely to produce reductions in runoff of 21 4 13 8 16 5 and 17 6 for the murray darling basin australia the spokane river basin usa and the yellow river basin china respectively as urban stormwater runs off catchments with a high impervious percentage a 10 reduction in precipitation will produce something approaching 10 reduction in runoff and hence it will be less affected by reductions in precipitation resulting from climate change several authors have reviewed stormwater harvesting practices in different parts of the world pitt et al 2012 hamdan 2009 in particular pitt et al 2012 found that developing countries that are highly urbanized but are facing water shortages including china and india focus on harvesting the maximum volume of runoff from urban areas without too much concern about water quality in developing countries with water shortages and large rural populations e g africa rainwater harvesting is focused on collecting roof runoff with storage in local tanks in developed countries with water shortages and large cities e g australia stormwater harvesting is widely practised at both the local and regional level somewhat surprisingly pitt et al 2012 found that stormwater harvesting is carried out in a number of developed countries that are not water stressed these countries include germany and the usa in germany a number of large cities including berlin and frankfurt undertake stormwater harvesting with the aim of achieving a sustainable approach to integrated urban water management in the usa stormwater harvesting is carried out in a number of cities from a wide range of climatic zones including west palm beach florida seattle washington portland oregon santa monica california new york city new york annapolis maryland and washington dc in the case of the usa the stormwater harvesting is usually aimed at reducing discharge to a combined sewer system or for environmental purposes the planning and design of stormwater harvesting facilities is a complicated process that needs to take into account multiple purposes and objectives many options and their combinations and a large number of constraints firstly the aims of the scheme need to be considered for example is the primary objective of the scheme to provide an additional source of water or to reduce the usage of existing sources is the purpose of the scheme only to harvest stormwater or is it also aimed at improving the quality of the harvested water or to reduce flooding in the downstream areas of the catchment similarly the primary sources of the stormwater need to be identified along with its primary intended uses for example is it intended that the harvested stormwater will be used for watering of public open space for commercial or industrial use or distributed through a third pipe system as a non potable source for residential consumers is it to be used as a potable source of water either indirectly after aquifer storage or directly by treating it and transferring it to a reservoir or tanks in the potable network is the stormwater harvesting and use to be part of a greenfield development or retrofitted to an existing urban area as a brownfield development having addressed these questions the performance criteria for the system need to be considered for example is the evaluation to be based purely on economic or financial grounds or will environmental and social factors also be important which factors need to be considered in an economic or a triple bottom line evaluation how are the benefits to be estimated what environmental and social factors need to be considered it should also be recognized that stormwater harvesting schemes consist of a number of primary components goonrey et al 2009 including 1 collection 2 storage 3 treatment 4 distribution and 5 discharge depending on the source s of the stormwater the objectives and purposes of the scheme and the intended end uses of the stormwater a large number of alternatives may exist for each of these components e g location size and layout consequently determination of the configuration of each of these components and the stormwater harvesting system as a whole that maximizes the desired system performance criteria is likely to require the use of simulation and or optimization models see maier et al 2019 based on the above discussion it is clear that the planning and design of a stormwater harvesting scheme is a complicated process with multiple objectives and many options therefore a framework is needed to assist in the planning and evaluation of such schemes to achieve the desired outcomes although a number of frameworks have been presented for selection and evaluation of low impact developments or green infrastructure alves et al 2016 liu et al 2016 gogate et al 2017 song and chung 2017 only two frameworks address the selection and evaluation of stormwater harvesting schemes goonrey et al 2009 inamdar et al 2013 goonrey et al 2009 present a decision making framework for choosing stormwater harvesting schemes based on technical considerations and financial cost they consider options for harvesting storage treatment and distribution of the stormwater as well as providing the treated stormwater for potable or non potable use although potable use is not considered in detail in their case study they highlight the need to include other factors including environmental economic social factors community participation and risk assessment but these are not part of their framework as presented this does not enable the framework to be used to assess the interactions and trade offs between competing objectives nor for the relationship between stormwater harvesting systems and other potential sources of water either new or existing in addition their framework only includes simplified methods for the modelling of some of the components and does not cater to the simulation of large complex stormwater harvesting systems accordingly the case study presented by goonrey et al 2009 consists of a small urban area of 4 52 ha where only two non potable uses of the water are considered namely outdoor residential use and indoor residential non potable use aquifer storage and recovery is also not considered thereby limiting the number of feasible scheme options to 19 i e a very simple system which are compared in terms of a single metric of cost per kl year inamdar et al 2013 present a gis based tool for selecting and ranking sites for stormwater harvesting in urban areas the ranking is based on the following three criteria a demand for stormwater b ratio of runoff to demand and c weighted demand distance a measure of the average distance from the harvesting site to the locations of the demand no economic environmental or social criteria were applied in the ranking and there was no consideration of the other components of a stormwater harvesting scheme e g storage or treatment which limits the scope and complexity of the types of systems the framework can be applied to consequently although there are existing frameworks for the selection and evaluation of stormwater harvesting schemes they do not cater to the detailed simulation and design of all components i e collection storage treatment distribution and discharge of complex stormwater harvesting systems nor do they consider the interactions and trade offs between competing technical economic environmental and social objectives in addition they have not been demonstrated for large complex case studies that consider a wide range of sources uses and system configurations this paper addresses these limitations by 1 presenting an integrated framework for the evaluation and design of large complex stormwater harvesting schemes that are used to supplement existing water supply systems the framework considers different sources of water and their combinations as well as different potential end uses caters to the detailed simulation and design of all components of the stormwater system and includes the interactions and trade offs between competing economic environmental and social objectives including benefits and costs 2 applying the framework to a realistic sized case study with a large number of potable and non potable use options for the harvested stormwater it should be noted that the framework is designed to cater for cases where harvested stormwater will be used to supplement existing water supply systems ideally an approach that considers all potential water sources in a system wide approach should be used to determine the optimal mix of sources see kasprzyk et al 2012 beh et al 2014 paton et al 2014a b huskova et al 2016 wu et al 2017 however a large system wide model usually involves a number of simplifying assumptions such that important detail is lost at the individual project level having carried out the system wide analysis individual projects need to be assessed using a framework similar to one presented in this paper furthermore system wide assessments might not be feasible for a number of reasons including a stormwater and water supply for the region may be managed by different authorities b political imperatives or federal government subsidies may require alternative water sources such as stormwater harvesting should be implemented rather than basing such decisions on the outcomes of system wide assessments and c the data and models needed to perform such system wide assessments are not readily available the framework the general framework to assess the performance of stormwater harvesting schemes is shown in fig 1 while this framework has some similarities with the framework presented by goonrey et al 2009 it includes the following novel aspects 1 it includes all potable and non potable uses of the harvested stormwater including blending with treated wastewater 2 it explicitly involves stakeholder inputs including the identification of system performance criteria and the weighting of these 3 it caters to the detailed simulation and design of all system components and their interactions including those for large complex systems 4 it includes a comprehensive economic environmental and social evaluation of the various stormwater options including externalities and 5 it uses a multi criteria framework the first step of the framework requires the definition of the problem and the gathering of data step a this step is where the intended end uses of the harvested stormwater system boundaries performance criteria and objectives are defined which requires significant input from stakeholders after the problem is defined it is necessary to generate the design options step b this step will take into account physical economic and social constraints if a particular design option is considered technically feasible it should be evaluated in detail step c the final step step d compares the different design options using multi criteria analysis and is used to select the final design as shown in fig 1 these steps are not necessarily undertaken in a linear fashion but generally involve an iterative process the steps are described in more detail below 1 1 step a problem definition and data gathering the first part of step a is to identify the intended end uses of the harvested stormwater this is a fundamental decision that affects all of the subsequent actions in the framework the possible options include potable and or non potable uses watering of public open space and industrial agricultural and residential use or combinations thereof the next part of step a is to define the system boundaries these include the physical boundary and the socio economic boundary for the assessment of benefits and costs various physical boundaries can be defined that include one or more of the following a the stormwater catchment s b components of the stormwater harvesting and distribution scheme c the areas and or customers expected to receive the harvested stormwater d downstream receiving waters and e catchments and infrastructure associated with alternative sources of supply the socio economic boundary needs to be broadly defined to include all parties affected by the scheme in one way or another these include a the customers of the scheme b the relevant water authority c government authorities and or d the community as a whole it is important to define both the physical and the socio economic boundaries in a way that is sufficiently broad so as to include all impacts benefits and costs that are relevant to the decision the definition of the system boundaries will depend on whether the stormwater harvesting system is part of a greenfield development or is being retrofitted to an existing stormwater collection system it will also depend on the intended end uses of the harvested water as outlined above the third part of step a is to identify the system performance criteria in general these will include economic environmental and social criteria the input of stakeholder groups is particularly useful here step a also involves the development of a suitable database for the system under study as shown in fig 1 this would normally include geographic water quality meteorological hydrological financial and water demand data this step also involves the identification of a number of options for each of the components of the scheme source capture treatment storage distribution and discharge a number of these options are given in fig 1 the interactions between these components also needs to be considered so that the final scheme forms an integrated system that satisfies the previously identified criteria 1 2 step b infrastructure system design design of the system infrastructure will most likely involve the use of a simulation model to estimate the performance of the integrated system including the effective yield and reliability of the capture and storage systems and the likely quality of the captured and treated stormwater components need to be sized based on experience and rules of thumb and evaluated in step c if necessary sizes and component selection can be adjusted and re evaluated in an iterative fashion between steps b and c this goes beyond the framework proposed by goonrey et al 2009 which used simple equations to estimate runoff and demand there are a number of suitable models for this step including swmm rossman 2004 music music development team 2003 and watercress clark and associates 2001 in some cases steps b and c can be combined through the use of an optimization model marchi et al 2016 1 3 step c evaluation of system performance this is an important component of the framework that is essential in order to obtain stakeholder buy in the stakeholders identify the performance criteria for the system in step a in this step all options should be evaluated in terms of these criteria this is a significant step that is not included in the framework presented by goonrey et al 2009 a number of the likely components of the economic environmental and social criteria that could be evaluated in stormwater harvesting schemes have been identified and are listed in table 1 it is conceivable that a number of evaluation components feed into more than one criterion it is important that each component should be counted in one and only one criterion in order to avoid double counting 1 3 1 economic criteria as listed in table 1 stormwater harvesting has several economic impacts which need to be assessed it is important to note that only the marginal costs should be considered i e costs or benefits that would not have been incurred in the absence of the project as the quality of harvested stormwater may be different from the quality of mains water there could be costs or benefits to the consumer associated with this aspect these costs or benefits obviously also depend on the end use while costs are relatively straight forward to estimate the direct benefits are far more complicated to delineate and this is a key challenge for assessing the economic viability of stormwater projects some benefits accrue to the users such as the potential to avoid water restrictions during drought others to the broader water supply network savings to the potable water system and others more broadly to society flood mitigation and local amenity additionally some benefits are essentially avoided costs that would have been incurred in the absence of the stormwater project an important benefit of stormwater reuse as an alternative supply source is that it often replaces water from the potable system thus avoiding the variable operating costs of potable supply and potentially deferring the next stage of augmentation of the potable system beh at al 2015a b in cases where stormwater replaces potable water the benefit of the scheme should preferably reflect the long run marginal cost lrmc of the potable water supply using only the short run variable cost of potable water delivery treatment and pumping as the avoided potable system costs will ignore the impact of the stormwater project on deferral of the next augmentation of the potable system ideally the variable water charge levied by the relevant water authority will reflect lrmc but this may not be true in many cases and as such would require an estimate of lrmc to be developed separately in general an assessment of the reliability of a water supply system that includes harvested stormwater requires simulation of the total supply system kasprzyk et al 2012 beh et al 2014 paton et al 2014a b huskova et al 2016 wu et al 2017 however in cases where the total supply from stormwater is supplementary and a small component of the supply the impact on system reliability is likely to be small 1 3 2 environmental criteria many of the environmental impacts associated with stormwater interception can be described in terms of energy reduced draw on surface water sources and avoided impact on receiving waters i e coastal areas of urban stormwater disposal each of these criteria expressed in terms of marginal impacts can be also be quantified in monetary values using market and non market valuation techniques national academies of sciences engineering and medicine 2016 alam et al 2006 market valuation techniques use information from existing markets to derive the value of ecosystem services to society while non market valuation techniques use survey methods to directly elicit society s willingness to pay or to accept compensation for different quantities and or qualities of an environmental good or service in some limited cases damage replacement or avoidance cost can be used bockstael et al 2000 liu et al 2010 werf 2010 typical environmental costs and benefits are listed in table 1 1 3 3 social criteria a considerable body of social research explores community acceptance of alternative water schemes for example mankad and tapsuwan 2011 provide an overview of acceptance and adoption of decentralised water supply schemes other references discuss factors affecting consumer acceptance of managed stormwater aquifer recharge schemes mankad et al 2015 leonard et al 2015 and recycled wastewater nancarrow et al 2008 hurlimann et al 2008 major themes in this research suggest the importance of social trust in the relevant water authorities to deliver water of the required quality and or reliability also important are factors such as the perception of risk fairness perceived need for long term alternative supply and cost for various end uses relative to the price of alternative sources of water 1 4 step d selection of best option once the performance of the various stormwater harvesting options has been assessed in relation to the selected economic environmental and social criteria the options can be ranked using multi criteria decision analysis mcda so as to identify the best option s as all assessment criteria are generally not considered equally important by stakeholders weights need to be assigned to each of the criteria there are many methods for assigning these weights al kloub et al 1997 hajkowicz et al 2000 kheireldin and fahmy 2001 pöyhönen and hämäläinen 2001 bottomley and doyle 2001 however there is no general agreement as to which method generates the best results barron and barrett 1996 a practical method to tackle this problem can be the exploration of the effect of different weights on the ranking of the alternatives using sensitivity or uncertainty analysis methods jessop 2004 rios insua 1990 hyde et al 2006 ganji et al 2016 while this does not improve the weight values it demonstrates the extent to which the different weights have a significant effect on the ranking the alternatives it should be noted that in cases where the number of stormwater harvesting options is large multi objective portfolio optimization approaches can be used to identify the subset of solutions that provides optimal trade offs between competing objectives see di matteo et al 2017b 2019 1 5 case study the framework outlined above is applied to a case study to demonstrate its general validity the case study analyzed in this work is based on the existing parafield stormwater harvesting scheme located in the salisbury area in the north of adelaide south australia fig 2 the use of an existing scheme allows access to more realistic data than would be available for a purely hypothetical scheme the range of options considered for this scheme is greater than those currently in use at this site and much greater than in the case studies considered by goonrey et al 2009 and inamdar et al 2013 the parafield scheme has its own collection treatment storage and distribution systems however in this case study the possibility of potable use of the harvested stormwater will be considered hence it needs to be seen as supplementary to the adelaide water supply system the city of adelaide has a population of 1 2 million people its potable water supply comes from a number of sources including local catchments in the mount lofty ranges inter basin transfers via three pipelines from the river murray 70 km to the east and a desalination plant in the south of the city fig 3 it also has a number of systems for non potable supply of a harvested stormwater that is primarily used for the watering of public open space and b treated wastewater that is primarily used for irrigation of crops a previous study into the optimal mix of water sources for adelaide wu et al 2017 showed that increased use of harvested stormwater and treated wastewater would lead to significant environmental benefits to the receiving waters of gulf st vincent west of the city in this context the parafield scheme is one of a number of decentralized schemes that warrant further detailed investigation the reliability of the adelaide water supply system is greater than 99 5 wu et al 2017 due to the presence of the desalination plant which has the capacity to supply around 50 of adelaide s annual demand the current parafield scheme harvests around 1100 ml year of stormwater from 1580 ha of residential and industrial area stormwater is first stored in an in stream basin 47 ml capacity and actually off stream where coarse sediments are settled out it is then pumped to a holding storage 48 ml capacity where clay particles can settle out the next step is reached by a gravity pipeline and consists of a cleansing reedbed 25 ml capacity that removes clay colloids and nutrients marks et al 2005 water is then pumped into an aquifer where it can be stored until it is needed currently water is extracted from the aquifer and used by industrial and residential consumers for non potable purposes the layout of the parafield system is presented in fig 4 although the parafield scheme is an existing system the evaluation was carried out assuming that it is a new scheme that is yet to be constructed this provides a better demonstration of the framework applied to a new stormwater harvesting scheme 1 6 step a problem definition and data gathering the end uses considered in this study include watering of public open space distribution to residential and industrial consumers via a third pipe network and potable use through the transfer to a water supply reservoir or direct injection into the mains fig 5 in each case an appropriate level of treatment is required the physical boundaries include the physical facilities themselves the catchment for stormwater runoff the locations where the stormwater will be used alternative sources of water properties adjacent to any wetlands little para reservoir and receiving waters for unused stormwater gulf st vincent in this case the alternative sources of water supply are catchments in the mount lofty ranges the river murray via interbasin transfer and the desalination plant the socio economic boundaries are drawn so as to include all those who benefit from or contribute to the cost of the scheme including all individuals industries local councils and the state government of south australia the system performance criteria were developed in conjunction with the stakeholders and are given on the right hand side of table 1 1 the economic criterion is the net present value of economic benefits minus economic costs of the scheme a discount rate of 6 and a project life of 25 years are used throughout the analysis these being the accepted values for assessing public sector projects in south australia 2 environmental criteria include equivalent annual energy consumption mwh per year benefits due to less water being taken from the river murray and benefits to marine ecosystems due to reduced stormwater flow to gulf st vincent 3 social criteria include the social acceptability of each of the options and the level of trust by the customers of the ability of the water authority to deliver water of the required quality an extensive database was assembled for the project including land use hydrological meteorological water consumption cost and energy information dandy et al 2013 1 7 infrastructure system design step b this step defines the stormwater options that need to be assessed and subsequently involves their preliminary design as shown in fig 5 a wide range of options is considered for the case study water can be used for irrigating public open spaces be distributed to residential consumers for non potable uses via a third pipe network or treated to potable standards and directly injected into the mains or transferred to a nearby reservoir where it is blended with surface water these options are summarized in table 2 as these options supply different demands they require different treatments and have different costs a simulation model of the catchment was developed using the watercress software clark and associates 2001 to estimate the stormwater yield of the catchment using daily rainfall data for the period 1980 to 2000 the model output indicated a mean annual yield from the scheme of 1200 ml year with a standard deviation of 220 ml year a conservative figure of 1100 ml year 3 ml day of harvested water is used in this study it should be noted that for schemes that involve asr 100 recovery of the water injected into the aquifer is not possible due to mixing with in situ brackish groundwater to account for this a recovery rate of 80 is assumed so the average annual supply from a scheme that includes asr is 880 ml year i e 80 of the harvestable catchment yield subsequent analysis by clark et al 2015 showed that a demand of 901 ml year can be met by a scheme including asr with a volumetric reliability of 99 5 for schemes without asr an annual yield of 370 ml year was estimated using the watercress model that is the use of asr more than doubled the harvestable volume at parafield and allowed storage until the time of the highest value use the present value of costs of the options are highly influenced by the pumping into and out of the aquifer and the level of water treatment required for example irrigation and non potable use will be associated with lower treatment costs than potable use the costs of all options include the cost of transfer pipelines in addition those options that involve distributing non potable water to residents options 5 6 7 and 8 to provide water for toilet flushing and washing machines require the construction of a third pipe residential distribution network options requiring a third pipe system have been divided into greenfield sites i e new developments labeled with the letter g in the following and brownfield sites i e retrofitting of existing areas labeled with the letter b the assumed cost of third pipe networks for options 5 to 8 is 1800 per household for greenfield systems and 4000 per household for brownfield systems note that options 9 12 do not include distribution costs as they utilize the existing potable distribution system finally it should be noted that the volume of water supplied using blending with treated wastewater options 4 7g 7b 8g and 8b is based on a blending ratio of 58 42 wastewater stormwater when asr is involved and 63 37 without asr due to the higher salinity of the stormwater after asr due to partial mixing with groundwater additional details regarding the 12 options considered can be found in appendix a of the supplemental material note that the options considered in this step are based on the parafield case study but similar information could be used to design the stormwater options in other regions 1 8 evaluation of system performance step c table 1 summarises how the various criteria were assessed more detail is given below 1 8 1 economic costs the cost analysis of each stormwater option includes capital operational and maintenance costs details of the estimation of the economic costs can be found in dandy et al 2013 and a summary of the equations used is given in appendix b of the supplemental material these costs include the costs of the existing infrastructure in parafield 4m for the harvesting scheme without asr 6m with asr included and 7m for distribution they also include the additional costs outside of parafield 3 7m for options that involve blending with recycled wastewater options 4 7 and 8 and 1 37m for options that involve transferring water from greenfield to the residential third pipe system at mawson lakes options 4 5 6 7 8 so as to assess the cost effectiveness of constructing a new scheme at a site with similar characteristics the costs also include the costs of new pumps pipes tanks and treatment facilities for each option note that costs need to reflect the design of the options so different options may have different pumping requirements in terms of flow and head delivered common aspects considered are 50 standby pump capacity peaking factor equal to 2 maximum operating time of 20 h day pump head of 30 m or 60 m depending on whether the pump is used for injecting into the aquifer or for extraction and distribution to users and a pump efficiency equal to 80 operational costs have been estimated using an electricity cost equal to 0 25 kwh while maintenance costs for pumps pipes and tanks are assumed to be 5 2 and 5 of the respective capital costs costs also include the water quality risk management costs to take into account labor travel and operating costs laboratory analytical costs for samples time for inspection monitoring and reporting the costs are estimated based on the level of exposure and the anticipated monitoring requirements a summary of these costs is given in table 3 page et al 2013b note that the risk management costs per kilolitre for options without asr options 1 5 and 7 are higher than those applying to the corresponding options with asr options 2 6 and 8 respectively as the fixed component of the risk management cost is averaged over a smaller volume of water 1 8 2 economic benefits to estimate the economic benefits due to reduced water supply from conventional sources it is necessary to take into account the end use of the water if stormwater is used for irrigation of open public spaces that would have been otherwise watered with potable water the stormwater is basically a substitute for potable water and the benefits should be estimated using the long run marginal cost of providing this this is also the case for stormwater being treated and provided as potable water only with permanent water restrictions on watering public spaces using potable water could the amenity value be included as a benefit according to the government of south australia 2011 the lrmc of supply for metropolitan adelaide in 2011 was in the range 2 00 to 2 75 per kl in this case study the savings in supply costs from conventional sources is estimated using a lrmc of 2 75 per kl when the treated stormwater is provided as non potable water through a third pipe network water is usually sold at a lower price than that of mains water in parafield the second tier price of mains water was 3 45 kl in 2011 12 whereas non potable water was sold at 2 59 kl the lower price will result in increased water consumption assuming a linear demand price curve for non potable consumption in this price range it is estimated that the consumption of harvested stormwater for outdoor use and toilet flushing will be 50 greater than if this water is supplied at a higher price from conventional sources dandy et al 2013 the net annual benefits for the third pipe options can be computed by summing the gross benefits of increased supply to the costs that would be incurred in providing potable water note that these costs take into account that a smaller volume would need to be provided and by subtracting the costs of providing non potable water see appendix c of supplemental material for the parafield case study an additional economic benefit is derived due to the reduced salinity of the harvested stormwater compared to that of the conventional water sources high salinity levels of around 300 mg l in mains water in salisbury cause damage to household plumbing fixtures and fittings hot water systems water filters and can affect cooling towers and boilers the economic benefit due to reduced salinity is based on the reduced corrosion of water using appliances equations to estimate the costs of salinity damage to households and industrial users are given in appendix b of the supplemental material no economic costs were associated with reduced reliability of supply for the stormwater harvesting scheme as its reliability is greater than 99 5 clark et al 2015 and hence is a similar order to the reliability of the conventional water supply system options 5 to 8 involve the distribution of harvested stormwater via a third pipe network for garden watering or toilet flushing the present value of benefits of reduced salinity for each option was estimated based on the following assumptions i the harvested stormwater or blended stormwater and wastewater are substitutes for potable mains water ii the third pipe system supplies residential consumers only iii the total water consumption of the average household is 180 kl per year of which 72 kl per year is outdoor use iv 25 or 27 kl per year of the indoor use of the average household 108 kl per year is used for toilet flushing v the benefits of lower salinity for outdoor use are negligible options 10 through 12 involve transferring harvested stormwater to little para reservoir where it is mixed with water in the reservoir prior to being treated through the little para water treatment plant and being distributed to consumers in addition to assumptions i and iii above the economic benefits due to the reduced salinity for these options have been estimated considering that the average annual supply of potable water from the reservoir is 15 000 ml and that 70 of the water from the reservoir is distributed to residential consumers and 30 to industrial and commercial consumers benefits for option 9 are considered equal to the benefits of options 11 and 12 as these options have similar salinities salinity benefits for options 1 4 irrigation of open spaces are considered negligible as given by assumption v above salinity benefits and costs are computed using average salinities of 326 mg l 125 mg l 240 mg l and 1160 mg l for mains water stormwater from schemes without asr stormwater from schemes with asr and wastewater respectively 1 8 3 environmental criteria the environmental impacts considered for this case study are energy consumption and other improvements to environmental aspects classified under the amenity service category for the parafield case study the improvements due to erosion control and flood mitigation were excluded as due to the small scale of scheme runoff volumes during peak flood events are unlikely to be affected significantly the embodied energy and operational energy of each option have been evaluated and details are given in appendix d of the supplemental material note that the embodied energy includes the following i embodied energy for the construction of the instream basin holding basin and reedbed for all options ii well excavation and well casing for the options that involve asr iii embodied energy of tanks note that different options have different storage requirements and iv embodied energy of the pipe infrastructure note that the embodied energy associated with the production of chemicals and pumps has been neglected because of limited data in addition the following two improvements to the environmental aspects have also been analyzed i the impact of reducing the volume of water withdrawn from the river murray and ii the impact of stormwater harvesting schemes on the marine life of gulf of st vincent note that the additional benefit derived from the ability to water additional public spaces was not considered as the open spaces would have been watered using mains water in the absence of the stormwater scheme also the increased value of residential properties near the wetland has been neglected as the parafield wetland is located on the grounds of the parafield airport it only has properties on one side and these are separated from the wetland by a fence and a road the benefit of reduced extraction of water from the murray river to supply adelaide is based on the average price paid for water that is being purchased by the australian government and returned to the environment under the restoring the balance in the murray darling basin program department of the environment 2013 this approach assumes that the use of stormwater will reduce the abstraction of water from the murray this reduced abstraction will result in increased flows in the river that will be used for environmental purposes and hence will have a similar benefit to the water purchased under the restoring the balance in the murray darling basin program if less water can be taken from the river murray by south australian water the state water corporation on an ongoing basis the present value of the increased volume of flow in the murray for environmental purposes is taken to be 2099 per ml www environment gov au water policy programs entitlement purchasing progress html accessed on sept 26 2013 this corresponds to the average price paid for high security water in the south australian murray based on the average annual yield note that in computing the benefits of a reduced withdrawal from the river murray it is assumed that 80 of the harvested stormwater will replace water pumped from the river murray and 20 will replace desalinated water hence a stormwater scheme that provides 1000 ml year of harvested stormwater will reduce the intake from the river murray by an average of 800 ml year this will result in a benefit in present value terms of 1 679m per gl of water supplied to users there are a number of issues related to the impact of stormwater on the waters of gulf st vincent these include the impact of nitrogen and suspended solids in the stormwater on seagrasses in the gulf and the aesthetic value of the coastal waters while it is possible to estimate the value of improved coastal water quality hatton macdonald et al 2015 and the cost of beach restoration measures it is extremely difficult to link this to a benefit associated with a small reduction in input of stormwater into the gulf due to the lack of biophysical modeling the loss of seagrass can affect the productivity of fisheries as well as beach erosion and restoration dandy et al 2013 estimate that the stormwater discharges to the gulf cost 63 000 per year in terms of reduced fishery production this is due to an average annual stormwater discharge of 114 gl per year assuming a linear relationship between reductions in stormwater inputs and productivity of fisheries this gives a benefit of 7000 gl in present value terms which is significantly smaller than other benefits in summary the present value of the environmental benefits related to the amenity services are estimated to be equal to 1 679m per gl of stormwater supplied to consumers per year to account for the increased flow in the river murray and 0 007m per gl of stormwater supplied to the user per year to account for the improved quality of coastal waters and the increase in fishery production 1 8 4 social criteria the social acceptance of stormwater use was assessed using two web based surveys the details of which can be found in alexander et al 2012 and mankad et al 2015 the results are shown in fig 5 the support and trust for the options is indicated by the percentage answering probably or definitely to the questions would you support case a b or c for stormwater use and would you trust authorities to ensure the quality of water of these cases in order to reduce possible confusion on the part of the respondents they were asked to consider only the following three options a treatment through a wetland and aquifer storage and recovery and then delivery to their house via a separate third pipe network where it could be used for garden watering toilet flushing and in the washing machine b treatment through a wetland aquifer storage and recovery and delivery to a water supply reservoir for blending with other source water before being further treated through a water treatment plant the water would then be distributed through the water supply mains for drinking and other purposes c treatment through a wetland and aquifer storage and recovery and then direct injection into the water supply mains for drinking and other purposes the results in fig 6 show that in general the support for stormwater schemes is high for all three options the survey did not contain questions specifically related to the use of harvested stormwater for watering public open space as this is an existing use of stormwater that has been carried out for a number of years it is assumed that there is 100 public support for it furthermore it is assumed that the trust of water authorities to ensure water of suitable quality for this option will be at least as high as for in house use via a third pipe network 1 9 multi criteria decision analysis step d in this study the options were evaluated according to the following criteria c1 c2 c3 where c1 is net present value of the scheme m c2 is the present value of benefits for reduced supply from the river murray m and c3 is the present value of benefits for reduced flow of stormwater to the gulf m c4 which is the reduction in energy consumption mwh year c5 which is the public support for stormwater harvesting options and c6 which is the public trust of authorities to ensure water quality as it is impossible to identify a single best framework for the multi criteria decision analysis zanakis et al 1998 the weighted sum method is used because of its ease of application furthermore the sensitivity of the chosen option s to the assumed weights can easily be assessed the weights reflect the relative importance placed on each criterion by the key stakeholders for example the general public or the city council note that some of the weights may be zero the relative weights could be determined by questioning and interacting with the key stakeholders in this case three sets of relative weights have been selected in order to provide a demonstration of how the multi criteria analysis approach is implemented the first set w1 gives a relative weight of one to the economic and environmental benefits c1 c2 c3 and to the reduction in energy consumption with relative weights of 0 5 to support and trust thus the two social criteria have a combined weight of one the second set of relative weights w2 gives a value of 0 5 to the economic and environmental benefits with the other values unchanged this effectively gives more weight to the reduction in energy consumption and the social criteria the third set of relative weights w3 gives a value of 2 to the economic and environmental benefits with the other relative weights unchanged resulting in the economic criterion being given greater importance before performing the multi criteria analysis each criterion was scaled from zero worst to one best to avoid criteria with small units e g c1 c2 c3 being dominated by those with large units e g c4 this has been accomplished by using a linear scaling as the social values are expressed in percentage terms they are simply converted into a fraction so that a value of one represents 100 support and zero represents no support 2 results and discussion this section shows and discusses the performance of the different stormwater scheme options table 4 shows the results of the economic analysis where capital operational and maintenance costs of the scheme and the distribution system are reported in the third column columns 4 5 and 6 show the present value of the benefits associated with the reduction in potable supply the additional benefits caused by the increased consumption of stormwater due to its lower price for options 5 8 where stormwater is used for residential non potable uses and the benefits of reduced salinity the last column of table 4 shows the net present value taking into account only economic costs and benefits as can be seen the options that have positive net benefits are options 2 to 4 which involve supply of stormwater for watering public open space and options 9 to 12 which involve supply of water for potable purposes either through direct injection into the mains or via transfer to little para reservoir all of the third pipe options have negative net benefits due to the large cost of the distribution system for options 7 and 8 salinity values that are larger than those of mains water due to mixing with recycled wastewater also contribute to the negative npv it is noted that the savings in potable supply costs constitute the largest economic benefit for this case study a sensitivity analysis about the assumed value of lrmc for conventional water supplies was carried out using a lrmc equal to 2 00 kl instead of 2 75 kl in this case options 2 to 4 and the potable options 10 to 12 still have a positive net present value but option 9 water from the asr scheme is treated and directly injected into potable water mains is not economically viable the result of two of the environmental criteria i e the benefits of withdrawing less water from the river murray and the benefit of discharging less stormwater into gulf st vincent gulf are given in columns 4 and 5 of table 5 it can be seen that in this setting these benefits are quite small compared to the npv of the economic benefits however if the benefits of additional water supply exceeded 32m option 8g would be worth investigating further if these benefits are considered option 1 would now have a positive npv the total energy consumption for each option was computed considering a discount factor of zero for future operating energy the embodied energy of each option was converted into an annual value by dividing by the life of the project 25 years in this case the total energy was compared with the equivalent energy necessary to provide the same volume of water with the current sources so as to quantify the energy savings of each option the details are given in appendix d of the supplemental material the net energy savings are given in column 6 of table 5 as can be seen options 1 to 4 would reduce the overall energy consumption options 5 to 8 have a larger energy consumption compared to the conventional sources due to the additional treatment required by the stormwater additional pumping for distribution and the embodied energy associated with the third pipe distribution system table 5 also reports the average volume of water supplied by the different options and the results of the social analysis which can be seen in the last two columns of the table it can be seen that options that directly use stormwater without asr options 1 5g and 5b have a lower yield given the limited storage availability option 10 does not have this limitation as the harvested stormwater can be treated and stored directly in little para reservoir options using asr have a larger yield although compared to the harvested volume the yield is reduced by the recovery efficiency from the aquifer it is also important to remember that options 4 7 and 8 have a larger yield as the stormwater is blended with recycled wastewater it is clear from table 5 that option 4 dominates all of the other options being higher or equal for all criteria however the multi criteria analysis will be completed so that all options can be ranked for example it could be the case that recycled wastewater is unavailable or of unsuitable quality and hence other options might need to be considered the ranking of projects obtained using the multi criteria analysis is shown in table 6 it can be seen that the ranking does not vary much for various values of the relative weights or between these and simply using the economic criterion of npv as option 4 dominates the others it always has the highest ranking and in general irrigation of open spaces and potable use options are always ranked highly it should be noted that the final scores using the relative weights are arbitrary so that a value of 1 85 for one option using relative weights w1 cannot be compared with a value of 1 70 for another option using relative weights w2 however within each specific set of weights it is possible to quantify how close the options are in terms of their ranking for example with the weights w1 there is not much difference between options 2 and 3 weighted scores equal to 2 05 and 2 04 respectively while option 4 is clearly more preferred with a weighted score of 2 79 the results in table 6 show that even when the reduction in energy consumption and the social criteria have twice the importance of the economic criterion the options are ranked in a similar order the fact that the order does not change much with changing values of the relative weights suggests that the ranking of the projects for this case study is relatively insensitive to the preferences of the stakeholders it is notable that option 10 involving indirect potable reuse ranks below options 2 and 3 for all three sets of weights even though the npv computed considering only economic costs and savings in potable supply of option 10 is larger than that of the other two options this is due to the much higher public acceptability of using harvested stormwater for watering of public open space than for indirect potable use also option 1 which has a negative npv when only economic costs and benefits are considered see table 4 ranks higher than options 9 and 12 for all sets of weights again reflecting the higher public acceptability of using harvested stormwater for watering public open space option 9 involving direct potable reuse after treatment ranks higher than option 12 indirect potable reuse after some treatment for all three sets of weights although the differences are quite small note that option 9 would rank lower than option 12 if only economic benefits were considered and it would not be cost effective without considering the salinity benefits of the third pipe options option 5 for greenfield sites supply after wetland treatment but without asr ranks the highest when social and environmental factors are considered and using weights w1 and w2 however as noted earlier none of the third pipe options are justified on purely economic grounds 3 conclusions this paper presents an integrated framework for the evaluation of stormwater harvesting schemes that are used to supplement existing water supply systems the framework combines traditional engineering economic methods such as benefit cost analysis with system modelling and multi criteria analysis the framework includes economic externalities as well as environmental and social impacts and can assist authorities with evaluating the performance of their projects by developing boundaries for analysis and understanding the implications of these boundaries this paper also demonstrates the differences between a traditional cost benefit analysis and a full multi criteria analysis where externalities such as energy consumption improved water quality increased urban amenity and social aspects are considered techniques for analyzing a wide array of economic benefits and costs water supply and water quality issues and environmental and social impacts are outlined and demonstrated in a realistic sized case study based on the parafield stormwater harvesting scheme both potable and non potable uses are considered in the case study the results obtained depend on the assumptions and data used and as shown in the case study on the perspective adopted from a whole of society perspective stormwater harvesting options that involve watering of public open spaces and using stormwater to satisfy potable demands have positive economic benefits when a full multi criteria analysis is carried out including environmental and social criteria the same eight options are ranked highly in a slightly changed order showing that in this case the ranking of options is relatively insensitive to the values used for the relative weights the case study was undertaken in order to demonstrate the application of the framework to a real world situation however additional research is required to address a number of the shortcomings in the case study in particular 1 the need to consider a broader range of social objectives including an equitable distribution of project benefits and costs between stakeholders see table 1 and 2 determination of the weights for the various objectives in the multi criteria analysis when there are multiple stakeholders nevertheless the case study illustrates the complexities involved in the holistic integrated assessment of the performance of stormwater harvesting schemes in a realistic setting considering a range of economic environmental and social criteria and the value of the proposed framework in providing guidance throughout this process declaration of competing interest the authors believe that they have no conflicts of interest in relation to this manuscript as submitted acknowledgments this research was undertaken as a part of the managed aquifer recharge and urban stormwater use options marsuo research project which is supported under the raising national water standards program through the national water commission and by csiro water for a healthy country flagship research program the goyder institute for water research city of salisbury adelaide and mount lofty ranges natural resources management board and the former united water international the authors would like to acknowledge the assistance of the project steering committee the technical reference committee and city of salisbury staff and would like to thank the following people for their inputs to the project jeff connor dave summers peter dillon aditi mankad andrea walton rosemary leonard mike young shiroma maheepala barbara brougham and josh cantone appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104554 
26126,existing water resources are under stress due to increasing demands associated with population and economic growth as well as the effects of climate change which can reduce available supply and increase demand alternative sources such as stormwater harvesting and treated wastewater are being considered in many cities to supplement existing water supplies stormwater harvesting schemes that supplement existing water supply systems are complex as they consist of an integrated set of components that perform the functions of collection storage treatment distribution and discharge many options and possible sizes exist for each component furthermore the selection and evaluation of stormwater harvesting schemes need to consider economic environmental and social criteria as well as technical feasibility in order to deal with this complexity an integrated framework is presented that assists authorities with determining if stormwater harvesting is preferable to other water sources and if so what type of stormwater scheme is best keywords framework stormwater harvesting multi criteria analysis benefit cost analysis environmental impact social impact alternative water sources cost energy and greenhouse gas emission database 1 introduction a number of major cities in the world are suffering stress on their water supply systems due to increases in population rainfall variability and climate change e g andreu et al 2009 hughes et al 2009 rosenberg and lund 2009 paton et al 2013 in response to these factors a number of cities are augmenting the more traditional surface and ground water sources with non traditional sources e g chung et al 2009 radcliffe 2010 leung et al 2012 maier et al 2013 beh et al 2014 paton et al 2014a b these sources include stormwater harvesting e g mitchell et al 2007 todeschini et al 2012 maier et al 2013 beh et al 2014 seawater desalination e g wittholz et al 2008 paton et al 2009 becker et al 2010 marchi et al 2016 di matteo et al 2017a and wastewater reuse e g harnett et al 2009 barton and argue 2009 leung et al 2012 stormwater harvesting is particularly attractive as a source of water supply as it provides other benefits such as reducing the load on stormwater infrastructure or combined sewer systems as well as reducing pollutant discharges to receiving waters pitt et al 2012 di matteo et al 2017a furthermore the volume of urban stormwater runoff is likely to be less affected by climate change than the runoff from rural catchments this is clearly demonstrated by the results of studies by sankarasubramanian and vogel 2003 and fu et al 2011 based on a study of 1337 river basins in the usa sankarasubramanian and vogel 2003 showed that a 10 reduction in precipitation is likely to produce a reduction in basin runoff of between 15 and 25 similarly fu et al 2011 found that a 10 reduction in precipitation is likely to produce reductions in runoff of 21 4 13 8 16 5 and 17 6 for the murray darling basin australia the spokane river basin usa and the yellow river basin china respectively as urban stormwater runs off catchments with a high impervious percentage a 10 reduction in precipitation will produce something approaching 10 reduction in runoff and hence it will be less affected by reductions in precipitation resulting from climate change several authors have reviewed stormwater harvesting practices in different parts of the world pitt et al 2012 hamdan 2009 in particular pitt et al 2012 found that developing countries that are highly urbanized but are facing water shortages including china and india focus on harvesting the maximum volume of runoff from urban areas without too much concern about water quality in developing countries with water shortages and large rural populations e g africa rainwater harvesting is focused on collecting roof runoff with storage in local tanks in developed countries with water shortages and large cities e g australia stormwater harvesting is widely practised at both the local and regional level somewhat surprisingly pitt et al 2012 found that stormwater harvesting is carried out in a number of developed countries that are not water stressed these countries include germany and the usa in germany a number of large cities including berlin and frankfurt undertake stormwater harvesting with the aim of achieving a sustainable approach to integrated urban water management in the usa stormwater harvesting is carried out in a number of cities from a wide range of climatic zones including west palm beach florida seattle washington portland oregon santa monica california new york city new york annapolis maryland and washington dc in the case of the usa the stormwater harvesting is usually aimed at reducing discharge to a combined sewer system or for environmental purposes the planning and design of stormwater harvesting facilities is a complicated process that needs to take into account multiple purposes and objectives many options and their combinations and a large number of constraints firstly the aims of the scheme need to be considered for example is the primary objective of the scheme to provide an additional source of water or to reduce the usage of existing sources is the purpose of the scheme only to harvest stormwater or is it also aimed at improving the quality of the harvested water or to reduce flooding in the downstream areas of the catchment similarly the primary sources of the stormwater need to be identified along with its primary intended uses for example is it intended that the harvested stormwater will be used for watering of public open space for commercial or industrial use or distributed through a third pipe system as a non potable source for residential consumers is it to be used as a potable source of water either indirectly after aquifer storage or directly by treating it and transferring it to a reservoir or tanks in the potable network is the stormwater harvesting and use to be part of a greenfield development or retrofitted to an existing urban area as a brownfield development having addressed these questions the performance criteria for the system need to be considered for example is the evaluation to be based purely on economic or financial grounds or will environmental and social factors also be important which factors need to be considered in an economic or a triple bottom line evaluation how are the benefits to be estimated what environmental and social factors need to be considered it should also be recognized that stormwater harvesting schemes consist of a number of primary components goonrey et al 2009 including 1 collection 2 storage 3 treatment 4 distribution and 5 discharge depending on the source s of the stormwater the objectives and purposes of the scheme and the intended end uses of the stormwater a large number of alternatives may exist for each of these components e g location size and layout consequently determination of the configuration of each of these components and the stormwater harvesting system as a whole that maximizes the desired system performance criteria is likely to require the use of simulation and or optimization models see maier et al 2019 based on the above discussion it is clear that the planning and design of a stormwater harvesting scheme is a complicated process with multiple objectives and many options therefore a framework is needed to assist in the planning and evaluation of such schemes to achieve the desired outcomes although a number of frameworks have been presented for selection and evaluation of low impact developments or green infrastructure alves et al 2016 liu et al 2016 gogate et al 2017 song and chung 2017 only two frameworks address the selection and evaluation of stormwater harvesting schemes goonrey et al 2009 inamdar et al 2013 goonrey et al 2009 present a decision making framework for choosing stormwater harvesting schemes based on technical considerations and financial cost they consider options for harvesting storage treatment and distribution of the stormwater as well as providing the treated stormwater for potable or non potable use although potable use is not considered in detail in their case study they highlight the need to include other factors including environmental economic social factors community participation and risk assessment but these are not part of their framework as presented this does not enable the framework to be used to assess the interactions and trade offs between competing objectives nor for the relationship between stormwater harvesting systems and other potential sources of water either new or existing in addition their framework only includes simplified methods for the modelling of some of the components and does not cater to the simulation of large complex stormwater harvesting systems accordingly the case study presented by goonrey et al 2009 consists of a small urban area of 4 52 ha where only two non potable uses of the water are considered namely outdoor residential use and indoor residential non potable use aquifer storage and recovery is also not considered thereby limiting the number of feasible scheme options to 19 i e a very simple system which are compared in terms of a single metric of cost per kl year inamdar et al 2013 present a gis based tool for selecting and ranking sites for stormwater harvesting in urban areas the ranking is based on the following three criteria a demand for stormwater b ratio of runoff to demand and c weighted demand distance a measure of the average distance from the harvesting site to the locations of the demand no economic environmental or social criteria were applied in the ranking and there was no consideration of the other components of a stormwater harvesting scheme e g storage or treatment which limits the scope and complexity of the types of systems the framework can be applied to consequently although there are existing frameworks for the selection and evaluation of stormwater harvesting schemes they do not cater to the detailed simulation and design of all components i e collection storage treatment distribution and discharge of complex stormwater harvesting systems nor do they consider the interactions and trade offs between competing technical economic environmental and social objectives in addition they have not been demonstrated for large complex case studies that consider a wide range of sources uses and system configurations this paper addresses these limitations by 1 presenting an integrated framework for the evaluation and design of large complex stormwater harvesting schemes that are used to supplement existing water supply systems the framework considers different sources of water and their combinations as well as different potential end uses caters to the detailed simulation and design of all components of the stormwater system and includes the interactions and trade offs between competing economic environmental and social objectives including benefits and costs 2 applying the framework to a realistic sized case study with a large number of potable and non potable use options for the harvested stormwater it should be noted that the framework is designed to cater for cases where harvested stormwater will be used to supplement existing water supply systems ideally an approach that considers all potential water sources in a system wide approach should be used to determine the optimal mix of sources see kasprzyk et al 2012 beh et al 2014 paton et al 2014a b huskova et al 2016 wu et al 2017 however a large system wide model usually involves a number of simplifying assumptions such that important detail is lost at the individual project level having carried out the system wide analysis individual projects need to be assessed using a framework similar to one presented in this paper furthermore system wide assessments might not be feasible for a number of reasons including a stormwater and water supply for the region may be managed by different authorities b political imperatives or federal government subsidies may require alternative water sources such as stormwater harvesting should be implemented rather than basing such decisions on the outcomes of system wide assessments and c the data and models needed to perform such system wide assessments are not readily available the framework the general framework to assess the performance of stormwater harvesting schemes is shown in fig 1 while this framework has some similarities with the framework presented by goonrey et al 2009 it includes the following novel aspects 1 it includes all potable and non potable uses of the harvested stormwater including blending with treated wastewater 2 it explicitly involves stakeholder inputs including the identification of system performance criteria and the weighting of these 3 it caters to the detailed simulation and design of all system components and their interactions including those for large complex systems 4 it includes a comprehensive economic environmental and social evaluation of the various stormwater options including externalities and 5 it uses a multi criteria framework the first step of the framework requires the definition of the problem and the gathering of data step a this step is where the intended end uses of the harvested stormwater system boundaries performance criteria and objectives are defined which requires significant input from stakeholders after the problem is defined it is necessary to generate the design options step b this step will take into account physical economic and social constraints if a particular design option is considered technically feasible it should be evaluated in detail step c the final step step d compares the different design options using multi criteria analysis and is used to select the final design as shown in fig 1 these steps are not necessarily undertaken in a linear fashion but generally involve an iterative process the steps are described in more detail below 1 1 step a problem definition and data gathering the first part of step a is to identify the intended end uses of the harvested stormwater this is a fundamental decision that affects all of the subsequent actions in the framework the possible options include potable and or non potable uses watering of public open space and industrial agricultural and residential use or combinations thereof the next part of step a is to define the system boundaries these include the physical boundary and the socio economic boundary for the assessment of benefits and costs various physical boundaries can be defined that include one or more of the following a the stormwater catchment s b components of the stormwater harvesting and distribution scheme c the areas and or customers expected to receive the harvested stormwater d downstream receiving waters and e catchments and infrastructure associated with alternative sources of supply the socio economic boundary needs to be broadly defined to include all parties affected by the scheme in one way or another these include a the customers of the scheme b the relevant water authority c government authorities and or d the community as a whole it is important to define both the physical and the socio economic boundaries in a way that is sufficiently broad so as to include all impacts benefits and costs that are relevant to the decision the definition of the system boundaries will depend on whether the stormwater harvesting system is part of a greenfield development or is being retrofitted to an existing stormwater collection system it will also depend on the intended end uses of the harvested water as outlined above the third part of step a is to identify the system performance criteria in general these will include economic environmental and social criteria the input of stakeholder groups is particularly useful here step a also involves the development of a suitable database for the system under study as shown in fig 1 this would normally include geographic water quality meteorological hydrological financial and water demand data this step also involves the identification of a number of options for each of the components of the scheme source capture treatment storage distribution and discharge a number of these options are given in fig 1 the interactions between these components also needs to be considered so that the final scheme forms an integrated system that satisfies the previously identified criteria 1 2 step b infrastructure system design design of the system infrastructure will most likely involve the use of a simulation model to estimate the performance of the integrated system including the effective yield and reliability of the capture and storage systems and the likely quality of the captured and treated stormwater components need to be sized based on experience and rules of thumb and evaluated in step c if necessary sizes and component selection can be adjusted and re evaluated in an iterative fashion between steps b and c this goes beyond the framework proposed by goonrey et al 2009 which used simple equations to estimate runoff and demand there are a number of suitable models for this step including swmm rossman 2004 music music development team 2003 and watercress clark and associates 2001 in some cases steps b and c can be combined through the use of an optimization model marchi et al 2016 1 3 step c evaluation of system performance this is an important component of the framework that is essential in order to obtain stakeholder buy in the stakeholders identify the performance criteria for the system in step a in this step all options should be evaluated in terms of these criteria this is a significant step that is not included in the framework presented by goonrey et al 2009 a number of the likely components of the economic environmental and social criteria that could be evaluated in stormwater harvesting schemes have been identified and are listed in table 1 it is conceivable that a number of evaluation components feed into more than one criterion it is important that each component should be counted in one and only one criterion in order to avoid double counting 1 3 1 economic criteria as listed in table 1 stormwater harvesting has several economic impacts which need to be assessed it is important to note that only the marginal costs should be considered i e costs or benefits that would not have been incurred in the absence of the project as the quality of harvested stormwater may be different from the quality of mains water there could be costs or benefits to the consumer associated with this aspect these costs or benefits obviously also depend on the end use while costs are relatively straight forward to estimate the direct benefits are far more complicated to delineate and this is a key challenge for assessing the economic viability of stormwater projects some benefits accrue to the users such as the potential to avoid water restrictions during drought others to the broader water supply network savings to the potable water system and others more broadly to society flood mitigation and local amenity additionally some benefits are essentially avoided costs that would have been incurred in the absence of the stormwater project an important benefit of stormwater reuse as an alternative supply source is that it often replaces water from the potable system thus avoiding the variable operating costs of potable supply and potentially deferring the next stage of augmentation of the potable system beh at al 2015a b in cases where stormwater replaces potable water the benefit of the scheme should preferably reflect the long run marginal cost lrmc of the potable water supply using only the short run variable cost of potable water delivery treatment and pumping as the avoided potable system costs will ignore the impact of the stormwater project on deferral of the next augmentation of the potable system ideally the variable water charge levied by the relevant water authority will reflect lrmc but this may not be true in many cases and as such would require an estimate of lrmc to be developed separately in general an assessment of the reliability of a water supply system that includes harvested stormwater requires simulation of the total supply system kasprzyk et al 2012 beh et al 2014 paton et al 2014a b huskova et al 2016 wu et al 2017 however in cases where the total supply from stormwater is supplementary and a small component of the supply the impact on system reliability is likely to be small 1 3 2 environmental criteria many of the environmental impacts associated with stormwater interception can be described in terms of energy reduced draw on surface water sources and avoided impact on receiving waters i e coastal areas of urban stormwater disposal each of these criteria expressed in terms of marginal impacts can be also be quantified in monetary values using market and non market valuation techniques national academies of sciences engineering and medicine 2016 alam et al 2006 market valuation techniques use information from existing markets to derive the value of ecosystem services to society while non market valuation techniques use survey methods to directly elicit society s willingness to pay or to accept compensation for different quantities and or qualities of an environmental good or service in some limited cases damage replacement or avoidance cost can be used bockstael et al 2000 liu et al 2010 werf 2010 typical environmental costs and benefits are listed in table 1 1 3 3 social criteria a considerable body of social research explores community acceptance of alternative water schemes for example mankad and tapsuwan 2011 provide an overview of acceptance and adoption of decentralised water supply schemes other references discuss factors affecting consumer acceptance of managed stormwater aquifer recharge schemes mankad et al 2015 leonard et al 2015 and recycled wastewater nancarrow et al 2008 hurlimann et al 2008 major themes in this research suggest the importance of social trust in the relevant water authorities to deliver water of the required quality and or reliability also important are factors such as the perception of risk fairness perceived need for long term alternative supply and cost for various end uses relative to the price of alternative sources of water 1 4 step d selection of best option once the performance of the various stormwater harvesting options has been assessed in relation to the selected economic environmental and social criteria the options can be ranked using multi criteria decision analysis mcda so as to identify the best option s as all assessment criteria are generally not considered equally important by stakeholders weights need to be assigned to each of the criteria there are many methods for assigning these weights al kloub et al 1997 hajkowicz et al 2000 kheireldin and fahmy 2001 pöyhönen and hämäläinen 2001 bottomley and doyle 2001 however there is no general agreement as to which method generates the best results barron and barrett 1996 a practical method to tackle this problem can be the exploration of the effect of different weights on the ranking of the alternatives using sensitivity or uncertainty analysis methods jessop 2004 rios insua 1990 hyde et al 2006 ganji et al 2016 while this does not improve the weight values it demonstrates the extent to which the different weights have a significant effect on the ranking the alternatives it should be noted that in cases where the number of stormwater harvesting options is large multi objective portfolio optimization approaches can be used to identify the subset of solutions that provides optimal trade offs between competing objectives see di matteo et al 2017b 2019 1 5 case study the framework outlined above is applied to a case study to demonstrate its general validity the case study analyzed in this work is based on the existing parafield stormwater harvesting scheme located in the salisbury area in the north of adelaide south australia fig 2 the use of an existing scheme allows access to more realistic data than would be available for a purely hypothetical scheme the range of options considered for this scheme is greater than those currently in use at this site and much greater than in the case studies considered by goonrey et al 2009 and inamdar et al 2013 the parafield scheme has its own collection treatment storage and distribution systems however in this case study the possibility of potable use of the harvested stormwater will be considered hence it needs to be seen as supplementary to the adelaide water supply system the city of adelaide has a population of 1 2 million people its potable water supply comes from a number of sources including local catchments in the mount lofty ranges inter basin transfers via three pipelines from the river murray 70 km to the east and a desalination plant in the south of the city fig 3 it also has a number of systems for non potable supply of a harvested stormwater that is primarily used for the watering of public open space and b treated wastewater that is primarily used for irrigation of crops a previous study into the optimal mix of water sources for adelaide wu et al 2017 showed that increased use of harvested stormwater and treated wastewater would lead to significant environmental benefits to the receiving waters of gulf st vincent west of the city in this context the parafield scheme is one of a number of decentralized schemes that warrant further detailed investigation the reliability of the adelaide water supply system is greater than 99 5 wu et al 2017 due to the presence of the desalination plant which has the capacity to supply around 50 of adelaide s annual demand the current parafield scheme harvests around 1100 ml year of stormwater from 1580 ha of residential and industrial area stormwater is first stored in an in stream basin 47 ml capacity and actually off stream where coarse sediments are settled out it is then pumped to a holding storage 48 ml capacity where clay particles can settle out the next step is reached by a gravity pipeline and consists of a cleansing reedbed 25 ml capacity that removes clay colloids and nutrients marks et al 2005 water is then pumped into an aquifer where it can be stored until it is needed currently water is extracted from the aquifer and used by industrial and residential consumers for non potable purposes the layout of the parafield system is presented in fig 4 although the parafield scheme is an existing system the evaluation was carried out assuming that it is a new scheme that is yet to be constructed this provides a better demonstration of the framework applied to a new stormwater harvesting scheme 1 6 step a problem definition and data gathering the end uses considered in this study include watering of public open space distribution to residential and industrial consumers via a third pipe network and potable use through the transfer to a water supply reservoir or direct injection into the mains fig 5 in each case an appropriate level of treatment is required the physical boundaries include the physical facilities themselves the catchment for stormwater runoff the locations where the stormwater will be used alternative sources of water properties adjacent to any wetlands little para reservoir and receiving waters for unused stormwater gulf st vincent in this case the alternative sources of water supply are catchments in the mount lofty ranges the river murray via interbasin transfer and the desalination plant the socio economic boundaries are drawn so as to include all those who benefit from or contribute to the cost of the scheme including all individuals industries local councils and the state government of south australia the system performance criteria were developed in conjunction with the stakeholders and are given on the right hand side of table 1 1 the economic criterion is the net present value of economic benefits minus economic costs of the scheme a discount rate of 6 and a project life of 25 years are used throughout the analysis these being the accepted values for assessing public sector projects in south australia 2 environmental criteria include equivalent annual energy consumption mwh per year benefits due to less water being taken from the river murray and benefits to marine ecosystems due to reduced stormwater flow to gulf st vincent 3 social criteria include the social acceptability of each of the options and the level of trust by the customers of the ability of the water authority to deliver water of the required quality an extensive database was assembled for the project including land use hydrological meteorological water consumption cost and energy information dandy et al 2013 1 7 infrastructure system design step b this step defines the stormwater options that need to be assessed and subsequently involves their preliminary design as shown in fig 5 a wide range of options is considered for the case study water can be used for irrigating public open spaces be distributed to residential consumers for non potable uses via a third pipe network or treated to potable standards and directly injected into the mains or transferred to a nearby reservoir where it is blended with surface water these options are summarized in table 2 as these options supply different demands they require different treatments and have different costs a simulation model of the catchment was developed using the watercress software clark and associates 2001 to estimate the stormwater yield of the catchment using daily rainfall data for the period 1980 to 2000 the model output indicated a mean annual yield from the scheme of 1200 ml year with a standard deviation of 220 ml year a conservative figure of 1100 ml year 3 ml day of harvested water is used in this study it should be noted that for schemes that involve asr 100 recovery of the water injected into the aquifer is not possible due to mixing with in situ brackish groundwater to account for this a recovery rate of 80 is assumed so the average annual supply from a scheme that includes asr is 880 ml year i e 80 of the harvestable catchment yield subsequent analysis by clark et al 2015 showed that a demand of 901 ml year can be met by a scheme including asr with a volumetric reliability of 99 5 for schemes without asr an annual yield of 370 ml year was estimated using the watercress model that is the use of asr more than doubled the harvestable volume at parafield and allowed storage until the time of the highest value use the present value of costs of the options are highly influenced by the pumping into and out of the aquifer and the level of water treatment required for example irrigation and non potable use will be associated with lower treatment costs than potable use the costs of all options include the cost of transfer pipelines in addition those options that involve distributing non potable water to residents options 5 6 7 and 8 to provide water for toilet flushing and washing machines require the construction of a third pipe residential distribution network options requiring a third pipe system have been divided into greenfield sites i e new developments labeled with the letter g in the following and brownfield sites i e retrofitting of existing areas labeled with the letter b the assumed cost of third pipe networks for options 5 to 8 is 1800 per household for greenfield systems and 4000 per household for brownfield systems note that options 9 12 do not include distribution costs as they utilize the existing potable distribution system finally it should be noted that the volume of water supplied using blending with treated wastewater options 4 7g 7b 8g and 8b is based on a blending ratio of 58 42 wastewater stormwater when asr is involved and 63 37 without asr due to the higher salinity of the stormwater after asr due to partial mixing with groundwater additional details regarding the 12 options considered can be found in appendix a of the supplemental material note that the options considered in this step are based on the parafield case study but similar information could be used to design the stormwater options in other regions 1 8 evaluation of system performance step c table 1 summarises how the various criteria were assessed more detail is given below 1 8 1 economic costs the cost analysis of each stormwater option includes capital operational and maintenance costs details of the estimation of the economic costs can be found in dandy et al 2013 and a summary of the equations used is given in appendix b of the supplemental material these costs include the costs of the existing infrastructure in parafield 4m for the harvesting scheme without asr 6m with asr included and 7m for distribution they also include the additional costs outside of parafield 3 7m for options that involve blending with recycled wastewater options 4 7 and 8 and 1 37m for options that involve transferring water from greenfield to the residential third pipe system at mawson lakes options 4 5 6 7 8 so as to assess the cost effectiveness of constructing a new scheme at a site with similar characteristics the costs also include the costs of new pumps pipes tanks and treatment facilities for each option note that costs need to reflect the design of the options so different options may have different pumping requirements in terms of flow and head delivered common aspects considered are 50 standby pump capacity peaking factor equal to 2 maximum operating time of 20 h day pump head of 30 m or 60 m depending on whether the pump is used for injecting into the aquifer or for extraction and distribution to users and a pump efficiency equal to 80 operational costs have been estimated using an electricity cost equal to 0 25 kwh while maintenance costs for pumps pipes and tanks are assumed to be 5 2 and 5 of the respective capital costs costs also include the water quality risk management costs to take into account labor travel and operating costs laboratory analytical costs for samples time for inspection monitoring and reporting the costs are estimated based on the level of exposure and the anticipated monitoring requirements a summary of these costs is given in table 3 page et al 2013b note that the risk management costs per kilolitre for options without asr options 1 5 and 7 are higher than those applying to the corresponding options with asr options 2 6 and 8 respectively as the fixed component of the risk management cost is averaged over a smaller volume of water 1 8 2 economic benefits to estimate the economic benefits due to reduced water supply from conventional sources it is necessary to take into account the end use of the water if stormwater is used for irrigation of open public spaces that would have been otherwise watered with potable water the stormwater is basically a substitute for potable water and the benefits should be estimated using the long run marginal cost of providing this this is also the case for stormwater being treated and provided as potable water only with permanent water restrictions on watering public spaces using potable water could the amenity value be included as a benefit according to the government of south australia 2011 the lrmc of supply for metropolitan adelaide in 2011 was in the range 2 00 to 2 75 per kl in this case study the savings in supply costs from conventional sources is estimated using a lrmc of 2 75 per kl when the treated stormwater is provided as non potable water through a third pipe network water is usually sold at a lower price than that of mains water in parafield the second tier price of mains water was 3 45 kl in 2011 12 whereas non potable water was sold at 2 59 kl the lower price will result in increased water consumption assuming a linear demand price curve for non potable consumption in this price range it is estimated that the consumption of harvested stormwater for outdoor use and toilet flushing will be 50 greater than if this water is supplied at a higher price from conventional sources dandy et al 2013 the net annual benefits for the third pipe options can be computed by summing the gross benefits of increased supply to the costs that would be incurred in providing potable water note that these costs take into account that a smaller volume would need to be provided and by subtracting the costs of providing non potable water see appendix c of supplemental material for the parafield case study an additional economic benefit is derived due to the reduced salinity of the harvested stormwater compared to that of the conventional water sources high salinity levels of around 300 mg l in mains water in salisbury cause damage to household plumbing fixtures and fittings hot water systems water filters and can affect cooling towers and boilers the economic benefit due to reduced salinity is based on the reduced corrosion of water using appliances equations to estimate the costs of salinity damage to households and industrial users are given in appendix b of the supplemental material no economic costs were associated with reduced reliability of supply for the stormwater harvesting scheme as its reliability is greater than 99 5 clark et al 2015 and hence is a similar order to the reliability of the conventional water supply system options 5 to 8 involve the distribution of harvested stormwater via a third pipe network for garden watering or toilet flushing the present value of benefits of reduced salinity for each option was estimated based on the following assumptions i the harvested stormwater or blended stormwater and wastewater are substitutes for potable mains water ii the third pipe system supplies residential consumers only iii the total water consumption of the average household is 180 kl per year of which 72 kl per year is outdoor use iv 25 or 27 kl per year of the indoor use of the average household 108 kl per year is used for toilet flushing v the benefits of lower salinity for outdoor use are negligible options 10 through 12 involve transferring harvested stormwater to little para reservoir where it is mixed with water in the reservoir prior to being treated through the little para water treatment plant and being distributed to consumers in addition to assumptions i and iii above the economic benefits due to the reduced salinity for these options have been estimated considering that the average annual supply of potable water from the reservoir is 15 000 ml and that 70 of the water from the reservoir is distributed to residential consumers and 30 to industrial and commercial consumers benefits for option 9 are considered equal to the benefits of options 11 and 12 as these options have similar salinities salinity benefits for options 1 4 irrigation of open spaces are considered negligible as given by assumption v above salinity benefits and costs are computed using average salinities of 326 mg l 125 mg l 240 mg l and 1160 mg l for mains water stormwater from schemes without asr stormwater from schemes with asr and wastewater respectively 1 8 3 environmental criteria the environmental impacts considered for this case study are energy consumption and other improvements to environmental aspects classified under the amenity service category for the parafield case study the improvements due to erosion control and flood mitigation were excluded as due to the small scale of scheme runoff volumes during peak flood events are unlikely to be affected significantly the embodied energy and operational energy of each option have been evaluated and details are given in appendix d of the supplemental material note that the embodied energy includes the following i embodied energy for the construction of the instream basin holding basin and reedbed for all options ii well excavation and well casing for the options that involve asr iii embodied energy of tanks note that different options have different storage requirements and iv embodied energy of the pipe infrastructure note that the embodied energy associated with the production of chemicals and pumps has been neglected because of limited data in addition the following two improvements to the environmental aspects have also been analyzed i the impact of reducing the volume of water withdrawn from the river murray and ii the impact of stormwater harvesting schemes on the marine life of gulf of st vincent note that the additional benefit derived from the ability to water additional public spaces was not considered as the open spaces would have been watered using mains water in the absence of the stormwater scheme also the increased value of residential properties near the wetland has been neglected as the parafield wetland is located on the grounds of the parafield airport it only has properties on one side and these are separated from the wetland by a fence and a road the benefit of reduced extraction of water from the murray river to supply adelaide is based on the average price paid for water that is being purchased by the australian government and returned to the environment under the restoring the balance in the murray darling basin program department of the environment 2013 this approach assumes that the use of stormwater will reduce the abstraction of water from the murray this reduced abstraction will result in increased flows in the river that will be used for environmental purposes and hence will have a similar benefit to the water purchased under the restoring the balance in the murray darling basin program if less water can be taken from the river murray by south australian water the state water corporation on an ongoing basis the present value of the increased volume of flow in the murray for environmental purposes is taken to be 2099 per ml www environment gov au water policy programs entitlement purchasing progress html accessed on sept 26 2013 this corresponds to the average price paid for high security water in the south australian murray based on the average annual yield note that in computing the benefits of a reduced withdrawal from the river murray it is assumed that 80 of the harvested stormwater will replace water pumped from the river murray and 20 will replace desalinated water hence a stormwater scheme that provides 1000 ml year of harvested stormwater will reduce the intake from the river murray by an average of 800 ml year this will result in a benefit in present value terms of 1 679m per gl of water supplied to users there are a number of issues related to the impact of stormwater on the waters of gulf st vincent these include the impact of nitrogen and suspended solids in the stormwater on seagrasses in the gulf and the aesthetic value of the coastal waters while it is possible to estimate the value of improved coastal water quality hatton macdonald et al 2015 and the cost of beach restoration measures it is extremely difficult to link this to a benefit associated with a small reduction in input of stormwater into the gulf due to the lack of biophysical modeling the loss of seagrass can affect the productivity of fisheries as well as beach erosion and restoration dandy et al 2013 estimate that the stormwater discharges to the gulf cost 63 000 per year in terms of reduced fishery production this is due to an average annual stormwater discharge of 114 gl per year assuming a linear relationship between reductions in stormwater inputs and productivity of fisheries this gives a benefit of 7000 gl in present value terms which is significantly smaller than other benefits in summary the present value of the environmental benefits related to the amenity services are estimated to be equal to 1 679m per gl of stormwater supplied to consumers per year to account for the increased flow in the river murray and 0 007m per gl of stormwater supplied to the user per year to account for the improved quality of coastal waters and the increase in fishery production 1 8 4 social criteria the social acceptance of stormwater use was assessed using two web based surveys the details of which can be found in alexander et al 2012 and mankad et al 2015 the results are shown in fig 5 the support and trust for the options is indicated by the percentage answering probably or definitely to the questions would you support case a b or c for stormwater use and would you trust authorities to ensure the quality of water of these cases in order to reduce possible confusion on the part of the respondents they were asked to consider only the following three options a treatment through a wetland and aquifer storage and recovery and then delivery to their house via a separate third pipe network where it could be used for garden watering toilet flushing and in the washing machine b treatment through a wetland aquifer storage and recovery and delivery to a water supply reservoir for blending with other source water before being further treated through a water treatment plant the water would then be distributed through the water supply mains for drinking and other purposes c treatment through a wetland and aquifer storage and recovery and then direct injection into the water supply mains for drinking and other purposes the results in fig 6 show that in general the support for stormwater schemes is high for all three options the survey did not contain questions specifically related to the use of harvested stormwater for watering public open space as this is an existing use of stormwater that has been carried out for a number of years it is assumed that there is 100 public support for it furthermore it is assumed that the trust of water authorities to ensure water of suitable quality for this option will be at least as high as for in house use via a third pipe network 1 9 multi criteria decision analysis step d in this study the options were evaluated according to the following criteria c1 c2 c3 where c1 is net present value of the scheme m c2 is the present value of benefits for reduced supply from the river murray m and c3 is the present value of benefits for reduced flow of stormwater to the gulf m c4 which is the reduction in energy consumption mwh year c5 which is the public support for stormwater harvesting options and c6 which is the public trust of authorities to ensure water quality as it is impossible to identify a single best framework for the multi criteria decision analysis zanakis et al 1998 the weighted sum method is used because of its ease of application furthermore the sensitivity of the chosen option s to the assumed weights can easily be assessed the weights reflect the relative importance placed on each criterion by the key stakeholders for example the general public or the city council note that some of the weights may be zero the relative weights could be determined by questioning and interacting with the key stakeholders in this case three sets of relative weights have been selected in order to provide a demonstration of how the multi criteria analysis approach is implemented the first set w1 gives a relative weight of one to the economic and environmental benefits c1 c2 c3 and to the reduction in energy consumption with relative weights of 0 5 to support and trust thus the two social criteria have a combined weight of one the second set of relative weights w2 gives a value of 0 5 to the economic and environmental benefits with the other values unchanged this effectively gives more weight to the reduction in energy consumption and the social criteria the third set of relative weights w3 gives a value of 2 to the economic and environmental benefits with the other relative weights unchanged resulting in the economic criterion being given greater importance before performing the multi criteria analysis each criterion was scaled from zero worst to one best to avoid criteria with small units e g c1 c2 c3 being dominated by those with large units e g c4 this has been accomplished by using a linear scaling as the social values are expressed in percentage terms they are simply converted into a fraction so that a value of one represents 100 support and zero represents no support 2 results and discussion this section shows and discusses the performance of the different stormwater scheme options table 4 shows the results of the economic analysis where capital operational and maintenance costs of the scheme and the distribution system are reported in the third column columns 4 5 and 6 show the present value of the benefits associated with the reduction in potable supply the additional benefits caused by the increased consumption of stormwater due to its lower price for options 5 8 where stormwater is used for residential non potable uses and the benefits of reduced salinity the last column of table 4 shows the net present value taking into account only economic costs and benefits as can be seen the options that have positive net benefits are options 2 to 4 which involve supply of stormwater for watering public open space and options 9 to 12 which involve supply of water for potable purposes either through direct injection into the mains or via transfer to little para reservoir all of the third pipe options have negative net benefits due to the large cost of the distribution system for options 7 and 8 salinity values that are larger than those of mains water due to mixing with recycled wastewater also contribute to the negative npv it is noted that the savings in potable supply costs constitute the largest economic benefit for this case study a sensitivity analysis about the assumed value of lrmc for conventional water supplies was carried out using a lrmc equal to 2 00 kl instead of 2 75 kl in this case options 2 to 4 and the potable options 10 to 12 still have a positive net present value but option 9 water from the asr scheme is treated and directly injected into potable water mains is not economically viable the result of two of the environmental criteria i e the benefits of withdrawing less water from the river murray and the benefit of discharging less stormwater into gulf st vincent gulf are given in columns 4 and 5 of table 5 it can be seen that in this setting these benefits are quite small compared to the npv of the economic benefits however if the benefits of additional water supply exceeded 32m option 8g would be worth investigating further if these benefits are considered option 1 would now have a positive npv the total energy consumption for each option was computed considering a discount factor of zero for future operating energy the embodied energy of each option was converted into an annual value by dividing by the life of the project 25 years in this case the total energy was compared with the equivalent energy necessary to provide the same volume of water with the current sources so as to quantify the energy savings of each option the details are given in appendix d of the supplemental material the net energy savings are given in column 6 of table 5 as can be seen options 1 to 4 would reduce the overall energy consumption options 5 to 8 have a larger energy consumption compared to the conventional sources due to the additional treatment required by the stormwater additional pumping for distribution and the embodied energy associated with the third pipe distribution system table 5 also reports the average volume of water supplied by the different options and the results of the social analysis which can be seen in the last two columns of the table it can be seen that options that directly use stormwater without asr options 1 5g and 5b have a lower yield given the limited storage availability option 10 does not have this limitation as the harvested stormwater can be treated and stored directly in little para reservoir options using asr have a larger yield although compared to the harvested volume the yield is reduced by the recovery efficiency from the aquifer it is also important to remember that options 4 7 and 8 have a larger yield as the stormwater is blended with recycled wastewater it is clear from table 5 that option 4 dominates all of the other options being higher or equal for all criteria however the multi criteria analysis will be completed so that all options can be ranked for example it could be the case that recycled wastewater is unavailable or of unsuitable quality and hence other options might need to be considered the ranking of projects obtained using the multi criteria analysis is shown in table 6 it can be seen that the ranking does not vary much for various values of the relative weights or between these and simply using the economic criterion of npv as option 4 dominates the others it always has the highest ranking and in general irrigation of open spaces and potable use options are always ranked highly it should be noted that the final scores using the relative weights are arbitrary so that a value of 1 85 for one option using relative weights w1 cannot be compared with a value of 1 70 for another option using relative weights w2 however within each specific set of weights it is possible to quantify how close the options are in terms of their ranking for example with the weights w1 there is not much difference between options 2 and 3 weighted scores equal to 2 05 and 2 04 respectively while option 4 is clearly more preferred with a weighted score of 2 79 the results in table 6 show that even when the reduction in energy consumption and the social criteria have twice the importance of the economic criterion the options are ranked in a similar order the fact that the order does not change much with changing values of the relative weights suggests that the ranking of the projects for this case study is relatively insensitive to the preferences of the stakeholders it is notable that option 10 involving indirect potable reuse ranks below options 2 and 3 for all three sets of weights even though the npv computed considering only economic costs and savings in potable supply of option 10 is larger than that of the other two options this is due to the much higher public acceptability of using harvested stormwater for watering of public open space than for indirect potable use also option 1 which has a negative npv when only economic costs and benefits are considered see table 4 ranks higher than options 9 and 12 for all sets of weights again reflecting the higher public acceptability of using harvested stormwater for watering public open space option 9 involving direct potable reuse after treatment ranks higher than option 12 indirect potable reuse after some treatment for all three sets of weights although the differences are quite small note that option 9 would rank lower than option 12 if only economic benefits were considered and it would not be cost effective without considering the salinity benefits of the third pipe options option 5 for greenfield sites supply after wetland treatment but without asr ranks the highest when social and environmental factors are considered and using weights w1 and w2 however as noted earlier none of the third pipe options are justified on purely economic grounds 3 conclusions this paper presents an integrated framework for the evaluation of stormwater harvesting schemes that are used to supplement existing water supply systems the framework combines traditional engineering economic methods such as benefit cost analysis with system modelling and multi criteria analysis the framework includes economic externalities as well as environmental and social impacts and can assist authorities with evaluating the performance of their projects by developing boundaries for analysis and understanding the implications of these boundaries this paper also demonstrates the differences between a traditional cost benefit analysis and a full multi criteria analysis where externalities such as energy consumption improved water quality increased urban amenity and social aspects are considered techniques for analyzing a wide array of economic benefits and costs water supply and water quality issues and environmental and social impacts are outlined and demonstrated in a realistic sized case study based on the parafield stormwater harvesting scheme both potable and non potable uses are considered in the case study the results obtained depend on the assumptions and data used and as shown in the case study on the perspective adopted from a whole of society perspective stormwater harvesting options that involve watering of public open spaces and using stormwater to satisfy potable demands have positive economic benefits when a full multi criteria analysis is carried out including environmental and social criteria the same eight options are ranked highly in a slightly changed order showing that in this case the ranking of options is relatively insensitive to the values used for the relative weights the case study was undertaken in order to demonstrate the application of the framework to a real world situation however additional research is required to address a number of the shortcomings in the case study in particular 1 the need to consider a broader range of social objectives including an equitable distribution of project benefits and costs between stakeholders see table 1 and 2 determination of the weights for the various objectives in the multi criteria analysis when there are multiple stakeholders nevertheless the case study illustrates the complexities involved in the holistic integrated assessment of the performance of stormwater harvesting schemes in a realistic setting considering a range of economic environmental and social criteria and the value of the proposed framework in providing guidance throughout this process declaration of competing interest the authors believe that they have no conflicts of interest in relation to this manuscript as submitted acknowledgments this research was undertaken as a part of the managed aquifer recharge and urban stormwater use options marsuo research project which is supported under the raising national water standards program through the national water commission and by csiro water for a healthy country flagship research program the goyder institute for water research city of salisbury adelaide and mount lofty ranges natural resources management board and the former united water international the authors would like to acknowledge the assistance of the project steering committee the technical reference committee and city of salisbury staff and would like to thank the following people for their inputs to the project jeff connor dave summers peter dillon aditi mankad andrea walton rosemary leonard mike young shiroma maheepala barbara brougham and josh cantone appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104554 
26127,modellers simulating the state of the physical and biological environment using agents and fields face the challenge of storing the state variables like the distribution of biomass and the location and properties of animals these variables differ from each other depending on how exactly they represent their state in this paper we describe an approach for storing multiple kinds of representations of model state variables using a unified physical data model we identified a set of aspects in which these representations differ from each other and which have an implication for the data model based on these aspects we defined a physical data model for storing spatio temporal objects which we implemented in an open source software library we illustrate how the resulting data model can be used to store multiple kinds of model state variables and explain how this data model can be useful in environmental modelling software keywords simulation modelling field object hdf5 lue 1 introduction when simulating the state of the physical and biological environment using a computer model model developers face the challenge of storing this state as represented by the model variables in one or more datasets for post processing depending on how the model variables represent state storing each of them may require different physical data models dataset formats or database designs for example when modelling the influence of grazing deer on the spatial distribution of biomass the state to be stored might be the continuous distribution of the biomass through time and space the changing location in space of each deer the changing non spatial weight of each deer and the sex of each deer which is not dependent on time and space the modeller might in this case decide to store biomass using a collection of rasters one raster per simulated location in time stored in one of the raster formats supported by the geospatial data abstraction library gdal gdal ogr contributors 2019 animal location using a collection of spatial points per animal stored in one of the vector formats supported by gdal animal weight by a collection of floating point numbers per animal stored in an sqlite database sqlite developers 2000 2019 and sex by a single code per animal also stored in the sqlite database since gdal does not support time some ad hoc convention is needed in this case to relate information in the datasets to simulated time an additional convention is needed to relate the information about the simulated deer in the various datasets to each other having to use multiple physical data models and conventions is inconvenient for modellers error prone and results in models that are less easy to maintain which kinds of representations of state variables are used in environmental models depends on various factors of which the modelling paradigm used is an important one two main paradigms for modelling environmental processes are field based modelling and agent based modelling filatova et al 2013 goodchild 2013 parker 2005 in field based modelling modelled systems are represented by continuous spatial fields of information these fields are often represented by rasters like a raster containing amounts of biomass in an area in agent based modelling systems are represented by interacting discrete objects of information that are mostly bounded in time and space these entities can be representations of physical real world phenomena like deer but they can also represent organizational entities like stakeholders or companies many field based models exist in which agents are manipulated and vice versa such as in the example of the deer biomass system instead of choosing between a field based or agent based modelling approach often an integrated approach is taken in which both continuous fields and discrete agents are manipulated examples of which can be found in bennett and tang 2006 castilla rho et al 2015 sbihi et al 2015 schelhaas et al 2007 and schippers et al 2014 in both field and agent based models and especially in integrated field and agent based models multiple kinds of representations of state variables are manipulated various physical data models exist for storing these kinds of state variables apis for storing rasters and objects are for instance provided by gdal gdal ogr contributors 2019 hdf5 the hdf group 1997 2019 netcdf4 unidata 2008 2018 rasdaman baumann et al 1998 scidb becla et al 2013 and postgis postgis development community 2019 there are important shortcomings of current options for storing state variables most physical data models support either the storage of rasters or of objects but not both when creating integrated models the modeller needs to be familiar with multiple data models store information in multiple datasets and may have to store information like the locations in time multiple times this results in a complex error prone and inefficient workflow some physical data models do not support random access this implies that when a certain piece of information needs to be found possibly the whole dataset needs to be read first this increases the runtime of models and their memory requirements existing physical data models often have limited support for storing locations in time or only support locations in time relative to the gregorian calendar forward models working with very small or long time scales ranging from nano seconds to millions of years then need to resort to ad hoc conventions for storing locations in time some physical data models do not benefit from the increased amount of resources offered by high performance computing hpc facilities for example they do not have support for parallel i o also for performance reasons some hpc facilities do not support the installation of server processes this precludes the use of data models that require a database management system e g postgis rasdaman scidb some physical data models have limitations with respect to the number of objects that can be stored or the size of the objects that can be stored ideally the number of objects and the size of the objects should only be limited by limitations of the hardware not by limitations of the data model in this work we tried to answer the question whether it is possible to design a unified physical data model that can be used to store different simulation model state variable representations in an integrated way such a data model should ideally simplify the way environmental modellers handle the storage of model state and not suffer from the above mentioned shortcomings of current data models also the data model should be receptive of features currently not found in some of the popular data models like the handling of simulated time mobility and collections of objects how model state is organized in a dataset might influence the way state is represented by model variables at runtime whenever possible time consuming conversions must be prevented for example although in this work we focus on the physical data model and not on the representation of model state variables our results align with the conceptual data model described in de bakker et al 2017 which could be used as a basis for representing model state variables in order to answer our research question we first looked at the different kinds of information that are relevant to store in a unified data model this is described in section 2 the objective was to extract the commonalities from different kinds of representations of state variables and use these as a basis for our data model this work resulted in a definition of spatio temporal objects which can be found in the same section this definition of spatio temporal objects allowed us to frame different kinds of model entities as variations of a single kind of spatio temporal object two important factors guided our data model design performance and the management of complexity of the data model in the case of simulation models that read and write much data the total runtime can become a performance bottleneck for a data model to be relevant in environmental modelling it must offer good performance for this we applied the principle of locality to our data model this is described in section 3 1 when permuting the different approaches for representing locations in time locations in space and properties we initially ended up with a large number of different kinds of representations we concluded that it was unfeasible to design a physical data model for each of these instead we focused on identifying a lowest abstraction level for representing abstract temporal object information and built our data model on top of that using abstraction levels of increasing functionality fig 1 arrays of temporal information form the first and lowest layer of a stack of three layers of abstractions this layer is described in section 3 1 in the second layer described in section 3 2 various kinds of spatio temporal object information are defined in terms of the arrays of temporal information from the first layer in the third and highest layer of abstraction the spatio temporal objects are defined in terms of the abstractions in the second layer this layer is described in section 3 3 in section 4 we describe the implementation of our data model using this implementation we were able in a case study to see how well different kinds of state variables from environmental models could be represented section 5 shows how this was done this work resulted in a physical data model for storing spatio temporal objects on top of the hdf5 data model fig 1 the data model is implemented in c and is currently exposed through a c api and a python api section 8 contains information about how to obtain a copy of the source code 2 spatio temporal objects a unified physical data model for storing state variables from environmental models must be able to represent the different kinds of representations of state variables in a uniform manner with this we mean that it must be possible to view each kind of state variable as an example of a more general kind of state variable for example it must be possible to view a global variable representing growth rate for grass a collection of multiple wandering deer fig 2 a and a single temporal biomass field fig 2b as similar in terms of the physical data model in order to come up with an approach for such a unified physical data model we first looked at the differences and commonalities between the different kinds of information represented by these state variables in the simplest case a state variable represents a single value that does not change during the simulation and is not located at a specific location in space a floating point value representing the earth s standard gravity in a plot scale environmental model is an example of this at the other extreme a state variable might represent a collection of objects each of which is located at a specific location in space and has a property value both of which change during the simulation a variable containing information about the locations in space and weights of a collection of wandering deer is an example of this besides these two extreme cases there are many more kinds of state variables differing with respect to the size and variability of the collection of objects they represent whether or not information changes through time or space and the kind of property values they contain we will now describe each of these aspects in turn in order to end up with an approach to viewing each different kind of state variable as an example of a more general one we define an object as a uniquely identifiable entity whose state is simulated by a model objects of the same kind for example deer are grouped in collections objects can be added and removed from such a collection for example deer are born and die so during a simulation the set of objects that participate in the calculations may change we call this changing set of participating objects the set of active objects or the active set the active set represents who are active although we use concrete examples for objects in this discussion we want to stress that we assume as little as possible about the nature of the objects what a collection of objects represents is up to the application that is the simulation model current kinds of state variables do not always represent collections of objects as in the case with the floating point variable representing the earth s standard gravity mentioned above as a first generalization step towards unifying all kinds of state variables we decided that all information represented by them must be associated with one or more objects in the case of the earth s standard gravity this object represents the planet earth all our objects have a presence in time and in space with presence in time we mean that for each object information must be stored about when the object was active objects might be active at very specific locations in time or during longer periods for example it is common for objects representing living things like deer to be active from simulated birth to death with presence in space we mean that for each location in time that an object was active information must be stored about where the object was active objects might be active at specific locations in space or in regions with a spatial extent a bird s location can be represented by a point and a tree s crown by a polygon for example additionally objects can be stationary in which case the location in space is fixed through time or mobile we use the concept of properties to represent what is present at locations in time and space that an object is active for example a relevant property manipulated by the model might be the weight of each deer all objects in a collection have the same set of properties so either weight is stored for each deer or weight is not stored at all for a single object there are multiple ways to represent a property value depending on the kind of property a deer s weight can be represented by a single number a natural park s biomass field by a 2 d array and a bird s direction and speed by a 1 d euclidean vector the final step we took towards unifying state variables is to represent all individual property values by n d arrays where the dimensionality of these arrays is the same per property given these generalization steps and in the context of our physical data model we can now define spatio temporal objects as uniquely identifiable objects for which locations in time locations in space and properties are stored for those locations in time that the objects where active our goal is that all model state variable representations ranging from scalar constants to mobile agents with discretized properties can be framed as collections of such spatio temporal objects a physical data model capable of storing such collections of objects is then capable of storing such a diverse set of state variable representations in the next section we describe our approach to representing these objects in a physical data model 3 storing spatio temporal objects 3 1 arrays of temporal information abstraction level 1 arrays of temporal information are at the lowest level of abstraction of our data model layer 1 in figure 1 they represent any kind of information for which variation through time has to be stored in the data model at this abstraction level it is not relevant what kind of information is stored exactly this could be the id of an object a coordinate in time or space or an object s property value the principle that guided our approach was the principle of locality patterson and hennessy 2008 in the context of modelling this principle states that at any moment in time a model accesses a relatively small portion of the data there are two types of locality temporal locality means that if a model uses data it will probably use the same data again soon spatial locality means that if a model uses data nearby data in terms of memory addresses will probably be used soon the principle of locality and the fact that different kinds of memory differ a lot in speed and price with the fastest memory being the most expensive led to the memory hierarchy found in computers the fastest memory is located near the cpu cores and the slowest memory is located further away on ssd drives and spinning disks for example in between multiple levels of caches are available like the caches in the storage devices themselves the main memory and the cpu caches memory is copied from lower distant to the cpu cores levels to upper levels in blocks of multiple values besides the values requested nearby values are also copied under the assumption that the cpu will probably need those values soon too spatial locality for software to benefit from the memory caches in the memory hierarchy it must store the values compactly in the order in which they are accessed that way the caches are filled with relevant values more often this approach to organize the values based on what is beneficial in the context of the principle of locality and the memory hierarchy is called data oriented design sharp 1980 fabian 2018 the goal of data oriented design is to decrease the idle time of cpu cores by increasing the likelihood that required values are in the nearby memory caches to understand what this means in the context of forward modelling we looked at the data access patterns in forward models defined using three modelling environments pcraster field based modelling environment karssenberg et al 2010 netlogo agent based modelling environment wilensky 1999 and mason agent based modelling environment luke et al 2005 in these models we can typically but not always identify a number of nested iterations from outer to inner 1 time a model iterates through time until the final state of the modelled system has been calculated each system s state is calculated based on the system s previous state s 2 operations each individual state is calculated by performing a number of operations in sequence this set of operations is the core of the model and implements the modelled processes 3 objects each individual operation iterates over a set of objects to calculate some result in this calculation the operation typically uses a small number of object properties an example of this is an operation that calculates the health of each deer in a collection based on each deer s age and weight this iteration scheme assumes that a simulation model consists of a sequence of operations performed iteratively through time on selections of objects this is a relatively simple approach but it has some benefits we think are good to have the most important one being that the approach naturally aligns with data oriented design the iteration scheme suggests a preferred ordering for the storage of information values for only the same kind of object information like the values of a single property for all objects should be stored close to each other in terms of memory addresses these collections of values should then be ordered by time this will increase the likelihood that modelling software benefits from the caching in the memory hierarchy when an operation iterates over a collection of objects reading specific property values the memory caches will likely be filled by relevant property values not including information that is not currently needed based on the principle of locality data orientation and the iteration scheme often used in forward modelling we decided to store information for each specific kind of object information as close together as possible and sorted by time this means that for example for all objects all locations in space are stored close together and property values of a single property are stored close together as well this contrasts with an approach where for each object individually all information is stored together the most compact way to store multiple values in computer memory is as an array whose element values are stored in contiguous memory locations therefore we concentrated on finding ways to organize spatio temporal object information as arrays each array has a shape which is the collection of the size of each of the array s dimensions the number of these dimensions is the array s rank for example a spatial raster can be represented by an array with rank two whose shape is equal to the number of rows and columns a single array can contain multiple arrays with a smaller rank for example multiple spatial rasters associated with multiple locations in time can be represented by an array with rank three whose shape is equal to the number of locations in time rows and columns this packing of arrays in larger arrays results in the most compact way to store object related information which is beneficial in the light of the above mentioned memory caching whether or not individual arrays containing information per object and per location in time can be packed in larger arrays depends on the shape of each individual array here we define object arrays as arrays containing a piece of information for a single object and for a single location in time this could be an object s id or a property value for example we call the array resulting from packing one or more object arrays in larger arrays the value array multiple object arrays can be tightly packed in value arrays when the shapes of the object arrays are equal object arrays containing object ids are all 0 d arrays that can be packed in a single 1 d value array object arrays containing 2 d arrays representing for instance the biomass property of multiple natural park areas cannot be packed in a single 3 d value array the criteria for deciding whether or not object arrays can be packed in value arrays are shown in table 1 permuting these criteria resulted in six approaches for packing object arrays into value arrays here it is not yet relevant what exactly is represented by the information in the object arrays for example object ids locations in time or property values what is relevant is that there are different kinds of object arrays and that the differences between them determine how to represent them in a physical data model our hypothesis is that with a limited set of six kinds of relatively low level data models for storing arrays of temporal information we can represent the much larger set of different kinds of higher level spatio temporal objects six figures corresponding with the six object array kinds from table 1 illustrate the object array kinds and their packing into value arrays object arrays of different objects can have the same shape figs 3 5 and 7 or a different shape figs 4 6 and 8 note that the object arrays shown in the figures are small 1 d arrays but in reality object arrays can be very large and have a very different shape object arrays for temporal information can have a constant shape figs 5 and 6 or a variable shape figs 7 and 8 packing object arrays into value arrays is not possible for every kind of object array fig 8 shows that for each object array and each location in time the object information needs to be stored in a separate value array this potentially results in a large number of value arrays the approach taken in this extreme case will not be beneficial for the memory caching on the other hand figs 3 and 5 show that all object arrays of all locations in time in the case of fig 5 can be stored in a single value array ordered by location in time this approach will potentially be beneficial for the memory caching the collection of active objects often changes during a simulation section 2 in case of the packing of the temporal object array kinds shown in figs 5 8 this information is not part of the value arrays it has to be explicitly stored somewhere else in the data model for details about our approach for doing this for each of the temporal object array kinds we refer to section appendixa 3 2 spatio temporal object information abstraction level 2 in section 2 we defined spatio temporal objects as uniquely identifiable objects for which locations in time locations in space and properties are stored for those locations in time that the objects where active we will now describe for each of these pieces of object information how we decided to represent it in terms of the six kinds of object arrays described in the previous section these abstractions correspond with the second abstraction layer implemented in our data model fig 1 3 2 1 identity object identity can be represented by a unique unsigned integer value which represented as an array corresponds with a 0 d array with an empty shape this shape is the same for all objects same shape array in case of object information that does not change through time the associated object identity can be represented by the constant value same shape array object array kind fig 9 a in case of object information that does change through time for each location in time the ids of the active objects can be represented by the variable value same constant shape array object array kind fig 5 sections appendixa 1 appendixa 2 and appendixa 3 or by encoding the ids of the active objects in the meta information of the value array used to store each object array section appendixa 4 3 2 2 locations in time we implemented three different approaches for storing locations in time that objects can be active time points time boxes and time cells fig 10 which objects are actually active at these locations is handled by object tracking section appendixa a time point is used to represent a specific location in time objects can be active a time box is used to represent a period of time objects can be active it is defined by a start time point and an end time point time cells are used to discretize time boxes for more fine grained tracking of object activity we represent time points by positive integral numbers representing an amount of time duration since an epoch details about this approach for handling time can be found in appendixb time durations can be represented by 1 d object arrays with shape 1 and this shape does not change through time this matches the variable value same constant shape array object array kind fig 5 time boxes can be handled similarly as time points but instead of storing a single time point per location in time two increasing time points need to be stored so a single time box can be represented by a 1 d array with two durations in it representing the start and end time points the shape of this array is 2 and this shape stays the same through time fig 9b this matches the variable value same constant shape array object array kind fig 5 for time cells we store an additional count for each time box counts are unsigned integers represented by variable value same constant shape array object arrays 3 2 3 locations in space representing locations in space where an object is active can be done in multiple ways for example the simple feature access standard open geospatial consortium 2011 defines points lines triangles and multi polygons amongst others we implemented a sub set of these approaches space points and space boxes which we extended to support defining locations in 1 d 2 d and 3 d space both stationary and mobile locations in space are supported a space point is used to represent a specific location in space where an object is active a space box is used to represent a linear rectangular or cuboidal region of space where an object is active it is defined by two diagonally opposite space points a space point can be represented by a coordinate in each spatial dimension a coordinate can be represented by a number integer or floating point the corresponding object array of a space point then is a 1 d array with a shape equal to the rank of the space 1 2 or 3 dimensions for each point this array contains the coordinates stationary space points are represented by the constant value same shape array object array kind fig 3 since object arrays containing space points for multiple locations in time have the same shape the same 1 d array as in the stationary case mobile space points are represented by the variable value same constant shape array object array kind fig 9c as with space points a space box can be stored in a 1 d object array but this time the shape is equal to twice the shape of a single space point the object array kinds for representing this information are the same as those for representing space points constant value same shape array object array kind fig 3 in case of stationary space boxes and variable value same constant shape array object array kind fig 5 in case of mobile space boxes 3 2 4 properties in environmental modelling various kinds of property values are used to represent an object s trait these kinds of values differ from each other based on whether or not these values vary through time whether or not the shape of the arrays representing the values differ per object and whether or not the shape of these arrays varies through time for example the weight property of each deer can be represented by a single number which varies through time while the surface elevation property of a natural park area can be represented by a constant or variable 2 d numeric array depending on whether the elevation changes through time the criteria with which property values can be classified match the ones we identified for our six kinds of object arrays described in section 3 1 in the previous sections specific kinds of object arrays were used to represent specific kinds of object information in the case of properties all six object array kinds from our taxonomy can be used for storing property values this way our data model can support a wide variety of kinds of property values used in environmental modelling as an example the weight of deer through time can be stored in a variable value same constant shape array object array kind fig 5 for each location in time that each deer was active the value array will contain a floating point number representing the weight as another example a simulation model modelling the evolution of the elevation of the land surface of multiple research area objects can store 2 d digital elevation models as variable value different constant shape array object arrays fig 6 assuming the research areas are stationary and have a different but constant shape property values can be discretized through either or both time and space we decided to store information about how property values are discretized as a property itself in case of a spatial raster for example information about the number of rows and columns a 2 d property value is discretized in is stored in a separate property and linked to the property being discretized the advantage of handling information about a discretization as a property is that this information itself can vary through time and potentially even through space this is useful in simulation models where the spatial resolution of rasters changes through time for example 3 3 spatio temporal objects abstraction level 3 since we are now able to represent the individual kinds of object information identity locations in time locations in space and properties we can focus on the representation of actual spatio temporal objects in which all this information is combined and can be accessed in an integrated way the abstractions mentioned in this section are part of the third and highest abstraction level implemented in our physical data model fig 1 one of the goals we had when designing the abstractions at this level was that it must be possible to coherently store object related information at different kinds of locations in time and space for example for a collection of birds it has to be possible to store information about the winter grounds the migration route and the summer grounds another goal we had was that no information should be stored twice in the data model we identified two levels of grouping of object related information at the first level of grouping we combine information about which objects exist identity when they are active locations in time where each object is locations in space and what is present at those locations properties we call this level of grouping the property set the collection of locations in time the time domain and the collection of locations in space the space domain within a property set the information about the active sets locations in space and properties is ordered by the locations in time also within a property set there can only be one time domain and one space domain these domains are shared between all properties in the same set object related information that is located at different locations in time or space is stored in different property sets for example the constant sex of deer and their variable weight are stored in separate property sets having different time and space domains at the second and higher level of grouping we group objects of the same kind with their property sets we call such a collection a phenomenon within a phenomenon all objects are of the same kind like deer or natural parks and each object can be identified by a unique id identity also all information related to a specific kind of objects is stored within a single phenomenon in one or more property sets this grouping of spatio temporal object information is similar to the conceptual data model presented in de bakker et al 2017 whose design had a similar goal as our physical data model to represent different kinds of state variables in a uniform manner in order to make it more convenient for environmental modellers to create models in which these different kinds of variables are manipulated the conceptual data model is shown in fig 11 it defines what information is represented and the physical data model defines how this information can be stored in a dataset in an environmental modelling environment the conceptual data model can be used as a basis for the representations or data types of the model state variables whereas the physical data model can be used to allow these state variables to be persisted for later retrieval to illustrate the grouping of spatio temporal object information in our data model we will describe how to represent wandering deer the implementation of this example is described in more detail in section 5 fig 12 shows the conceptual data model for a deer phenomenon these deer are simulated using a model in which the spatial distribution of biomass in an area and the location and weight of the deer are influenced by each other the representation of the deer in our physical data model fig 13 is very similar to the conceptual data model for each deer the following information needs to be stored a unique id the sex the location in space and the weight the id and sex do not change through time while the location in space and the weight do since within a property set there can only be a single combination of a time and space domain this means that two property sets are defined in the first property set named constant no locations in time and space are stored for each deer we store whether the deer is male or female in a property in the second property set named variable we use the time cell time domain kind the mobile space point space domain kind and a property for storing 0 d numeric values representing the weights fig 13 shows that all object information is stored using one of the six array kinds described in section 3 1 in this case most information is stored using the variable value same constant shape array kinds but this depends on the specific kind of information stored when storing rasters for differently shaped areas for example a property containing variable value different constant shape array values must be used and when these rasters change shape through time a property with variable value different variable shape array values must be used instead 4 implementation we implemented our physical data model using hdf5 the hdf group 1997 2019 besides being a software library and a file format hdf5 is a data model itself the hdf5 logical data model can be used to organize information to be stored in a file the most important parts of the hdf5 data model are groups datasets and attributes groups are used to aggregate other groups and datasets while datasets are used for storing multidimensional arrays with attributes additional information can be associated with groups and datasets the hierarchical nature of the data model groups can contain other groups allows for complex nested structures to be represented multiple hdf5 datasets and groups and attributes can be stored in a single hdf5 file hdf5 datasets can be optionally configured to be extendable along one or more dimensions we have defined our data model in terms of the hdf5 logical data model using mainly groups datasets and attributes the implementation named lue 1 1 lue stands for life the universe and everything mentioned in douglas adams hitchhiker s guide to the galaxy novels here it refers to the fact that our data model is able to represent data about different kinds of information used in environmental modelling and possibly other domains we pronounce lue as the french pronounce louis lu ee consists of a c library that implements the data model and a python package with which the data model is made accessible from python scripts in the c library we implemented the abstraction layers as discussed in section 3 separately fig 1 we will now describe the implementation of each of these layers 4 1 arrays of temporal information for each of the object array kinds described in section 3 1 and shown in figs 3 8 we have implemented code to store the value arrays this low level of abstraction is implemented on top of a c library that wraps the hdf5 c library for each object array kind this layer contains code for reading and writing individual object arrays or when applicable collections of object arrays hdf5 datasets are used to store value arrays it is possible to read or write a single 1 d object array containing three floating point values or multiple 1 d object arrays in one go each containing three floating point values or such a collection of object arrays for multiple locations in time 4 2 spatio temporal object information for all kinds of object information as described in section 3 2 we developed code to read and write this information from and to a file this code reuses the code mentioned in the previous section instead of reading and writing arrays of abstract temporal information here object ids locations in time locations in space and properties are explicitly handled the implementation contains higher level classes like mobilespacepoint and property which in their implementation use the lower level code for reading and writing arrays of temporal information this makes it more convenient in higher level code to handle information related to spatio temporal objects for example listing 1 shows the declaration of a function for creating a mobilespacepoint instance such an instance manages the i o of space points that move through space the function is implemented in terms of the lower level api for handling arrays of temporal information how this is done exactly is a detail that the caller does not have to be constantly aware of listing 1 subset of c api for creating mobile space points image 1 4 3 spatio temporal objects as described in section 3 3 the highest layer of abstraction of our physical data model contains mainly grouping constructs for aggregating information about the ids locations in time locations in space and properties of the objects we have mapped these groups onto hdf5 groups for example listing 2 shows the declaration of a function for creating a propertyset instance such an instance provides access to its object tracker its time and space domains and its properties listing 2 subset of c api for creating a property set image 2 it is possible to combine multiple phenomena into a single hdf5 file this makes it possible to store all simulation state in a single lue dataset 4 4 python package in order to make it possible to manipulate lue data sets from python we implemented a python package on top of the higher abstraction levels of the c library numpy arrays oliphant 2006 are used to interface object arrays between the python and c code this makes it possible to integrate the lue physical data model with python applications manipulating numpy arrays like scipy jones et al 2001 and pcraster karssenberg et al 2010 the pybind11 c library jakob et al 2017 is used to expose the relevant parts of the lue c api to python and vice versa examples of using the lue python package can be found in the next section 5 example deer and biomass model in this section we will describe how the state variables from a simple integrated agent based and field based model can be represented by our data model using the lue python package we will highlight parts of the code the complete model can be found in the source code repository associated with this manuscript see section 8 the purpose of the deer and biomass model used in this example is to illustrate that our data model is capable of storing both traditional agent based and field based model output in this case the deer are the agents and the natural park area with a biomass raster is the field example output from the model is shown in fig 2 the model we used is simple and not realistic the data is what we want to focus on here not the validity of the model it is possible to store more complex kinds of data than used here in our data model too like properties with temporal discretizations and phenomena with multiple time and space domains we realize that what we call a high level of abstraction can be considered a low level of abstraction by modellers we envision that in real world applications an additional layer of abstraction will be built on top of our highest level of abstraction making it easy for modellers to read and write model state information this is shown as the upper layer of abstraction in fig 1 concepts for how this can be done have been described in de bakker et al 2017 the following examples assume that the lue and numpy python packages have been imported a dataset is created and two phenomena are added to it listing 3 the returned phenomenon instances deer and park in this example can be used to add property sets to the file after the model has finished executing all model state variables have been stored in a single file formatted according to the conventions described in this manuscript given that each lue dataset is also an hdf5 file the dataset can be copied to any other platform and the information stored can be retrieved again using the lue apis and hdf5 apis listing 3 import required packages image 3 5 1 initialize dataset before we can write the state of simulated spatio temporal objects to a dataset we must initialize the dataset this will prepare the dataset for receiving state information in the deer and biomass model the location and weight of each deer and the distribution of biomass in the park within which the deer are located change through time as described in the previous text time domains are contained in property sets instead of storing multiple time domains with the same locations in time in multiple property sets they can be shared this makes the resulting file smaller and prevents inconsistent time domains in our example the time domains of the property sets containing temporal deer and park information can be shared for this we create a separate phenomenon called simulation whose sole purpose is to contain a property set with a time domain to be shared with other property sets listing 4 we could also have shared the time domain of a property set in the deer phenomenon with a property set in the park phenomenon or vice versa listing 4 create property set with time cell domain and three hourly time steps image 4 the park phenomenon in our example model contains only a single object the park within which all the deer are located it contains a property set named space extent with a property named biomass for storing the 2 d biomass arrays that change through time listing 5 it also contains a property set named constant with a property named space discretization for storing the 1 d space discretization information number of rows and number of columns the latter property is linked to the first to make explicit that biomass is discretized through space the simulated biomass field is shown in fig 2b listing 5 create park phenomenon property sets image 5 the deer phenomenon contains for each deer the sex the location in space and the weight preparing the lue dataset for receiving state information during the simulation works similar to the previous case of the park phenomenon in the case of deer the space domain is different mobile space points are used instead of a stationary space box and the weight property value has a different shape 0 d instead of 2 d the simulated deer are shown in fig 2a fig 14 shows the resulting state of the data model 5 2 write model state once the dataset is initialized we can start the simulation and write the state of simulated spatio temporal objects to the dataset in our case this implies writing the initial state of the biomass raster of the park and the sex location and weight of each deer after that the interaction between the biomass field and the deer agents are simulated through time and the biomass location and weight of each deer are iteratively updated and written to the dataset in listing 6 the sex of each deer is written to the dataset given a phenomenon property sets can be obtained by name and given those properties can be obtained querying and assigning to property values works as if they are numpy arrays in this example the value array is expanded to make space for an object array per deer the expand method returns the value array on which it is called which is then indexed for all object arrays assigning to these object arrays writes the new values to the dataset listing 6 write the sex of each deer to the dataset image 6 writing state that changes through time requires also writing the object ids of the objects for which this state is written the active set appendixa tracking the object ids of the active deer is shown in listing 7 mobile space points are represented by variable value same constant shape array object arrays which implies that for tracking the ids of the objects the index of the active set and the collection of active object ids must be stored listing 7 write the object ids of the active deer to the dataset image 7 writing the updated location in space of active deer is shown in listing 8 after the new deer locations have been simulated additional space is created in the dataset and their coordinates are written listing 8 write the locations of the active deer to the dataset image 8 6 discussion we have designed and implemented a physical data model for simulated spatio temporal objects with this data model information about the state of the simulated environment can be stored for post processing although there are multiple approaches to represent state variables depending on how they represent a value and the temporal and spatial extent and variability we identified common aspects of these approaches using a limited set of six kinds of object arrays figs 3 8 these object arrays and the packing of them into value arrays form the basis of our data model layers with increasingly more domain specific code are defined on top of this base layer with the layer containing the support for spatio temporal objects at the top fig 1 using this top level layer of abstraction we were able to represent some example kinds of state variables suggesting that our data model is able to represent a diverse set of state variable kinds that otherwise would require multiple data models section 5 our data model provides a unified approach to storing spatio temporal objects using this data model the environmental modeller can represent multiple kinds of state variables the modeller is not forced to use a single data model for all state variables or use multiple very different data models in our view this is the main contribution of this work in our data model information about objects is stored in multiple arrays where each array contains a single kind of object related information this contrasts with many existing data models where this information is stored in database table records as mentioned in section 1 how information is organized in a physical data model might have an effect on how information is best organized in state variables in simulation models our data model suggests these state variables to be organized using arrays per kind of object related information with for the current modelled location in time the state of the current active sets of objects this is different from the common approach in agent based modelling of representing state variables by class instances aggregating all object related information during this work we have identified some limitations of our data model which have to do with usability and functionality our data model is capable of representing complex kinds of state variables with changing collections of mobile objects and temporal discretizations of property values for example although the data model is currently useable we think it could be made more user friendly for simple cases by adding an additional layer of abstraction on top of the layer implementing the storage of spatio temporal objects this higher level data model would represent popular existing data models like scalars rasters and raster stacks and time series and serve as an easy to use replacement for existing data models this would allow modellers to get familiar with the lue data model without having to learn about the lower level details first which are necessary for the more complex cases with respect to functionality some aspects are still missing from the current version of our data model that will be added at a later stage examples of these are the representation of relations between objects uncertainty in temporal and spatial locations and property values spatial projections discretization of presence in space other kinds of discretizations through space more space domain item type like lines and polygons spatial indices and information about topology whether or not the data model has to be adjusted for these additional aspects to be added remains to be seen although it is of crucial importance in a high performance computing context we have not evaluated the performance of our data model since our data model is implemented in terms of the hdf5 library the performance characteristics of this library determine the performance of our data model to reach maximum performance we must tune our use of hdf5 for our specific use case of storing spatio temporal objects and probably use parallel i o on parallel file systems this will be the focus of future research 7 conclusion in this paper we showed the feasibility of a physical data model capable of storing different kinds of simulation model state variables in an integrated way first we defined spatio temporal objects as uniquely identifiable objects for which locations in time locations in space and properties are stored for those locations in time that the objects were active by assuming that absence of information about presence in time or space just means that information is valid for all simulated time or space we were able to frame all considered kinds of state variables as spatio temporal objects including simple state variable kinds like scalars or non temporal rasters next we presented our physical data model as consisting of multiple levels of abstraction at the lowest level arrays of general temporal arrays are represented on top of that spatio temporal object information object identity locations in time and space and properties is represented in terms of the general temporal arrays at the highest level of abstraction spatio temporal objects are represented as aggregates of spatio temporal object information the resulting data model has been implemented in the lue software package which contains apis in c and python we have described an example of how lue can be used from an integrated field based and agent based simulation model to store various kinds of state variables in a single dataset 8 software availability the unified physical data model is implemented as part of a software package called lue which is hosted on github at https github com pcraster lue the data model is implemented by kor de jong in c and provides an api for c and python clients the code used for this work corresponds to git tag 535d52b77092686e8d0641ae2e7983e5e9eddd3c and is freely available under the mit open source license a document called readme md is included in the root of the source code repository detailing the instructions for building the software lue is portable software and has been successfully built on various platforms operating systems linux macos compilers clang gcc architecture x86 64 the readme document also contains a link to the user documentation of the lue python package the example model used in section 5 can also be found on github at https github com pcraster paper 2019 physical data model funding this work was supported by the research it innovation programme utrecht university the netherlands and the global geo health data center utrecht university and university medical center utrecht the netherlands author contributions kdj and dk both worked on the concepts kdj designed and implemented the data model kdj wrote the manuscript with inputs from dk dk coordinated the project declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a object tracking here we describe in some more detail how we keep track of for which objects information is stored in the physical data model this is relevant for the four object array kinds used to store temporal information for each location in time and for each active object we store where each object array can be found how this is done differs per object array kind appendix a 1 variable value same constant shape array per location in time the value array a contains the values for all active objects fig 5 an additional array a c t i v e o b j e c t i d is used to store the ids of the active objects an id is a unique number see section 3 2 1 the order of information in a and a c t i v e o b j e c t i d is the same an additional array a c t i v e s e t i n d e x is used to store the index into a and a c t i v e o b j e c t i d where for each location in time the id and object array of the first active object can be found see also table a 2 table a 2 procedures for storing and finding variable value same constant shape array object arrays table a 2 store append id to a c t i v e o b j e c t i d and object array to a find given index of location in time read index s of section of active objects from a c t i v e s e t i n d e x and determine index o of object id in a c t i v e o b j e c t i d the object array is located at a s o appendix a 2 variable value same variable shape array per location in time a separate value array a t contains the values for all active objects fig 7 similar to the previous case additional arrays a c t i v e o b j e c t i d and a c t i v e s e t i n d e x are used to track the ids of active objects and the offset into a c t i v e o b j e c t i d of the id of the first active object respectively each value array a t is named after its corresponding index of location in time see also table a 3 table a 3 procedures for storing and finding variable value same variable shape array object arrays table a 3 store append id to a c t i v e o b j e c t i d and object array to a t find given index of location in time read index of section of active objects from a c t i v e s e t i n d e x and determine index o of object id in a c t i v e o b j e c t i d the object array is located at a t o appendix a 3 variable value different constant shape array per object a separate value array a o contains the values for all locations in time that the object was active fig 6 again an additional array a c t i v e o b j e c t i d is used to track the ids of active objects array a c t i v e o b j e c t i n d e x is used to track the indices into the a o arrays of the active objects array a c t i v e s e t i n d e x is used to store the offset into a c t i v e o b j e c t i d and a c t i v e o b j e c t i n d e x of the first active object per location in time see also table a 4 table a 4 procedures for storing and finding variable value different constant shape array object arrays table a 4 store append id to a c t i v e o b j e c t i d append index of object array in a o to a c t i v e o b j e c t i n d e x and append object array to a o find given index of location in time read index of section of active objects from a c t i v e s e t i n d e x and determine index of object id in a c t i v e o b j e c t i d given this index lookup index t of object array in a c t i v e o b j e c t i n d e x the object array is located at a o t appendix a 4 variable value different variable shape array per location in time and per object a separate value array a t o contains the object array fig 8 see also table a 5 table a 5 procedures for storing and finding variable value different variable shape array object arrays table a 5 store create value array named after current location in time and object and write object array to it find open value array named after current location in time and object appendix b representing time in order to be able to represent time points of different resolutions like nanoseconds and centuries we used an approach inspired by the c chrono library 2 2 http en cppreference com w cpp header chrono time points are dependent on a clock a clock represents a period of time since an epoch and has a certain resolution this resolution is defined by the clock s tick period which has a unit seconds or years for example and a count two for ticks of 2 s or years for example given this a time point can be represented by a duration which is represented by a number of ticks we decided to represent a collection of time points by a single clock represented by an epoch a unit a string and a count a positive integral number and a duration for each time point represented by counts positive integral numbers we represent epochs by an epoch kind and an optional origin and optional calendar all strings currently supported epoch kinds are common era ce and formation of earth but more can be added the currently supported calendar is gregorian appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104553 
26127,modellers simulating the state of the physical and biological environment using agents and fields face the challenge of storing the state variables like the distribution of biomass and the location and properties of animals these variables differ from each other depending on how exactly they represent their state in this paper we describe an approach for storing multiple kinds of representations of model state variables using a unified physical data model we identified a set of aspects in which these representations differ from each other and which have an implication for the data model based on these aspects we defined a physical data model for storing spatio temporal objects which we implemented in an open source software library we illustrate how the resulting data model can be used to store multiple kinds of model state variables and explain how this data model can be useful in environmental modelling software keywords simulation modelling field object hdf5 lue 1 introduction when simulating the state of the physical and biological environment using a computer model model developers face the challenge of storing this state as represented by the model variables in one or more datasets for post processing depending on how the model variables represent state storing each of them may require different physical data models dataset formats or database designs for example when modelling the influence of grazing deer on the spatial distribution of biomass the state to be stored might be the continuous distribution of the biomass through time and space the changing location in space of each deer the changing non spatial weight of each deer and the sex of each deer which is not dependent on time and space the modeller might in this case decide to store biomass using a collection of rasters one raster per simulated location in time stored in one of the raster formats supported by the geospatial data abstraction library gdal gdal ogr contributors 2019 animal location using a collection of spatial points per animal stored in one of the vector formats supported by gdal animal weight by a collection of floating point numbers per animal stored in an sqlite database sqlite developers 2000 2019 and sex by a single code per animal also stored in the sqlite database since gdal does not support time some ad hoc convention is needed in this case to relate information in the datasets to simulated time an additional convention is needed to relate the information about the simulated deer in the various datasets to each other having to use multiple physical data models and conventions is inconvenient for modellers error prone and results in models that are less easy to maintain which kinds of representations of state variables are used in environmental models depends on various factors of which the modelling paradigm used is an important one two main paradigms for modelling environmental processes are field based modelling and agent based modelling filatova et al 2013 goodchild 2013 parker 2005 in field based modelling modelled systems are represented by continuous spatial fields of information these fields are often represented by rasters like a raster containing amounts of biomass in an area in agent based modelling systems are represented by interacting discrete objects of information that are mostly bounded in time and space these entities can be representations of physical real world phenomena like deer but they can also represent organizational entities like stakeholders or companies many field based models exist in which agents are manipulated and vice versa such as in the example of the deer biomass system instead of choosing between a field based or agent based modelling approach often an integrated approach is taken in which both continuous fields and discrete agents are manipulated examples of which can be found in bennett and tang 2006 castilla rho et al 2015 sbihi et al 2015 schelhaas et al 2007 and schippers et al 2014 in both field and agent based models and especially in integrated field and agent based models multiple kinds of representations of state variables are manipulated various physical data models exist for storing these kinds of state variables apis for storing rasters and objects are for instance provided by gdal gdal ogr contributors 2019 hdf5 the hdf group 1997 2019 netcdf4 unidata 2008 2018 rasdaman baumann et al 1998 scidb becla et al 2013 and postgis postgis development community 2019 there are important shortcomings of current options for storing state variables most physical data models support either the storage of rasters or of objects but not both when creating integrated models the modeller needs to be familiar with multiple data models store information in multiple datasets and may have to store information like the locations in time multiple times this results in a complex error prone and inefficient workflow some physical data models do not support random access this implies that when a certain piece of information needs to be found possibly the whole dataset needs to be read first this increases the runtime of models and their memory requirements existing physical data models often have limited support for storing locations in time or only support locations in time relative to the gregorian calendar forward models working with very small or long time scales ranging from nano seconds to millions of years then need to resort to ad hoc conventions for storing locations in time some physical data models do not benefit from the increased amount of resources offered by high performance computing hpc facilities for example they do not have support for parallel i o also for performance reasons some hpc facilities do not support the installation of server processes this precludes the use of data models that require a database management system e g postgis rasdaman scidb some physical data models have limitations with respect to the number of objects that can be stored or the size of the objects that can be stored ideally the number of objects and the size of the objects should only be limited by limitations of the hardware not by limitations of the data model in this work we tried to answer the question whether it is possible to design a unified physical data model that can be used to store different simulation model state variable representations in an integrated way such a data model should ideally simplify the way environmental modellers handle the storage of model state and not suffer from the above mentioned shortcomings of current data models also the data model should be receptive of features currently not found in some of the popular data models like the handling of simulated time mobility and collections of objects how model state is organized in a dataset might influence the way state is represented by model variables at runtime whenever possible time consuming conversions must be prevented for example although in this work we focus on the physical data model and not on the representation of model state variables our results align with the conceptual data model described in de bakker et al 2017 which could be used as a basis for representing model state variables in order to answer our research question we first looked at the different kinds of information that are relevant to store in a unified data model this is described in section 2 the objective was to extract the commonalities from different kinds of representations of state variables and use these as a basis for our data model this work resulted in a definition of spatio temporal objects which can be found in the same section this definition of spatio temporal objects allowed us to frame different kinds of model entities as variations of a single kind of spatio temporal object two important factors guided our data model design performance and the management of complexity of the data model in the case of simulation models that read and write much data the total runtime can become a performance bottleneck for a data model to be relevant in environmental modelling it must offer good performance for this we applied the principle of locality to our data model this is described in section 3 1 when permuting the different approaches for representing locations in time locations in space and properties we initially ended up with a large number of different kinds of representations we concluded that it was unfeasible to design a physical data model for each of these instead we focused on identifying a lowest abstraction level for representing abstract temporal object information and built our data model on top of that using abstraction levels of increasing functionality fig 1 arrays of temporal information form the first and lowest layer of a stack of three layers of abstractions this layer is described in section 3 1 in the second layer described in section 3 2 various kinds of spatio temporal object information are defined in terms of the arrays of temporal information from the first layer in the third and highest layer of abstraction the spatio temporal objects are defined in terms of the abstractions in the second layer this layer is described in section 3 3 in section 4 we describe the implementation of our data model using this implementation we were able in a case study to see how well different kinds of state variables from environmental models could be represented section 5 shows how this was done this work resulted in a physical data model for storing spatio temporal objects on top of the hdf5 data model fig 1 the data model is implemented in c and is currently exposed through a c api and a python api section 8 contains information about how to obtain a copy of the source code 2 spatio temporal objects a unified physical data model for storing state variables from environmental models must be able to represent the different kinds of representations of state variables in a uniform manner with this we mean that it must be possible to view each kind of state variable as an example of a more general kind of state variable for example it must be possible to view a global variable representing growth rate for grass a collection of multiple wandering deer fig 2 a and a single temporal biomass field fig 2b as similar in terms of the physical data model in order to come up with an approach for such a unified physical data model we first looked at the differences and commonalities between the different kinds of information represented by these state variables in the simplest case a state variable represents a single value that does not change during the simulation and is not located at a specific location in space a floating point value representing the earth s standard gravity in a plot scale environmental model is an example of this at the other extreme a state variable might represent a collection of objects each of which is located at a specific location in space and has a property value both of which change during the simulation a variable containing information about the locations in space and weights of a collection of wandering deer is an example of this besides these two extreme cases there are many more kinds of state variables differing with respect to the size and variability of the collection of objects they represent whether or not information changes through time or space and the kind of property values they contain we will now describe each of these aspects in turn in order to end up with an approach to viewing each different kind of state variable as an example of a more general one we define an object as a uniquely identifiable entity whose state is simulated by a model objects of the same kind for example deer are grouped in collections objects can be added and removed from such a collection for example deer are born and die so during a simulation the set of objects that participate in the calculations may change we call this changing set of participating objects the set of active objects or the active set the active set represents who are active although we use concrete examples for objects in this discussion we want to stress that we assume as little as possible about the nature of the objects what a collection of objects represents is up to the application that is the simulation model current kinds of state variables do not always represent collections of objects as in the case with the floating point variable representing the earth s standard gravity mentioned above as a first generalization step towards unifying all kinds of state variables we decided that all information represented by them must be associated with one or more objects in the case of the earth s standard gravity this object represents the planet earth all our objects have a presence in time and in space with presence in time we mean that for each object information must be stored about when the object was active objects might be active at very specific locations in time or during longer periods for example it is common for objects representing living things like deer to be active from simulated birth to death with presence in space we mean that for each location in time that an object was active information must be stored about where the object was active objects might be active at specific locations in space or in regions with a spatial extent a bird s location can be represented by a point and a tree s crown by a polygon for example additionally objects can be stationary in which case the location in space is fixed through time or mobile we use the concept of properties to represent what is present at locations in time and space that an object is active for example a relevant property manipulated by the model might be the weight of each deer all objects in a collection have the same set of properties so either weight is stored for each deer or weight is not stored at all for a single object there are multiple ways to represent a property value depending on the kind of property a deer s weight can be represented by a single number a natural park s biomass field by a 2 d array and a bird s direction and speed by a 1 d euclidean vector the final step we took towards unifying state variables is to represent all individual property values by n d arrays where the dimensionality of these arrays is the same per property given these generalization steps and in the context of our physical data model we can now define spatio temporal objects as uniquely identifiable objects for which locations in time locations in space and properties are stored for those locations in time that the objects where active our goal is that all model state variable representations ranging from scalar constants to mobile agents with discretized properties can be framed as collections of such spatio temporal objects a physical data model capable of storing such collections of objects is then capable of storing such a diverse set of state variable representations in the next section we describe our approach to representing these objects in a physical data model 3 storing spatio temporal objects 3 1 arrays of temporal information abstraction level 1 arrays of temporal information are at the lowest level of abstraction of our data model layer 1 in figure 1 they represent any kind of information for which variation through time has to be stored in the data model at this abstraction level it is not relevant what kind of information is stored exactly this could be the id of an object a coordinate in time or space or an object s property value the principle that guided our approach was the principle of locality patterson and hennessy 2008 in the context of modelling this principle states that at any moment in time a model accesses a relatively small portion of the data there are two types of locality temporal locality means that if a model uses data it will probably use the same data again soon spatial locality means that if a model uses data nearby data in terms of memory addresses will probably be used soon the principle of locality and the fact that different kinds of memory differ a lot in speed and price with the fastest memory being the most expensive led to the memory hierarchy found in computers the fastest memory is located near the cpu cores and the slowest memory is located further away on ssd drives and spinning disks for example in between multiple levels of caches are available like the caches in the storage devices themselves the main memory and the cpu caches memory is copied from lower distant to the cpu cores levels to upper levels in blocks of multiple values besides the values requested nearby values are also copied under the assumption that the cpu will probably need those values soon too spatial locality for software to benefit from the memory caches in the memory hierarchy it must store the values compactly in the order in which they are accessed that way the caches are filled with relevant values more often this approach to organize the values based on what is beneficial in the context of the principle of locality and the memory hierarchy is called data oriented design sharp 1980 fabian 2018 the goal of data oriented design is to decrease the idle time of cpu cores by increasing the likelihood that required values are in the nearby memory caches to understand what this means in the context of forward modelling we looked at the data access patterns in forward models defined using three modelling environments pcraster field based modelling environment karssenberg et al 2010 netlogo agent based modelling environment wilensky 1999 and mason agent based modelling environment luke et al 2005 in these models we can typically but not always identify a number of nested iterations from outer to inner 1 time a model iterates through time until the final state of the modelled system has been calculated each system s state is calculated based on the system s previous state s 2 operations each individual state is calculated by performing a number of operations in sequence this set of operations is the core of the model and implements the modelled processes 3 objects each individual operation iterates over a set of objects to calculate some result in this calculation the operation typically uses a small number of object properties an example of this is an operation that calculates the health of each deer in a collection based on each deer s age and weight this iteration scheme assumes that a simulation model consists of a sequence of operations performed iteratively through time on selections of objects this is a relatively simple approach but it has some benefits we think are good to have the most important one being that the approach naturally aligns with data oriented design the iteration scheme suggests a preferred ordering for the storage of information values for only the same kind of object information like the values of a single property for all objects should be stored close to each other in terms of memory addresses these collections of values should then be ordered by time this will increase the likelihood that modelling software benefits from the caching in the memory hierarchy when an operation iterates over a collection of objects reading specific property values the memory caches will likely be filled by relevant property values not including information that is not currently needed based on the principle of locality data orientation and the iteration scheme often used in forward modelling we decided to store information for each specific kind of object information as close together as possible and sorted by time this means that for example for all objects all locations in space are stored close together and property values of a single property are stored close together as well this contrasts with an approach where for each object individually all information is stored together the most compact way to store multiple values in computer memory is as an array whose element values are stored in contiguous memory locations therefore we concentrated on finding ways to organize spatio temporal object information as arrays each array has a shape which is the collection of the size of each of the array s dimensions the number of these dimensions is the array s rank for example a spatial raster can be represented by an array with rank two whose shape is equal to the number of rows and columns a single array can contain multiple arrays with a smaller rank for example multiple spatial rasters associated with multiple locations in time can be represented by an array with rank three whose shape is equal to the number of locations in time rows and columns this packing of arrays in larger arrays results in the most compact way to store object related information which is beneficial in the light of the above mentioned memory caching whether or not individual arrays containing information per object and per location in time can be packed in larger arrays depends on the shape of each individual array here we define object arrays as arrays containing a piece of information for a single object and for a single location in time this could be an object s id or a property value for example we call the array resulting from packing one or more object arrays in larger arrays the value array multiple object arrays can be tightly packed in value arrays when the shapes of the object arrays are equal object arrays containing object ids are all 0 d arrays that can be packed in a single 1 d value array object arrays containing 2 d arrays representing for instance the biomass property of multiple natural park areas cannot be packed in a single 3 d value array the criteria for deciding whether or not object arrays can be packed in value arrays are shown in table 1 permuting these criteria resulted in six approaches for packing object arrays into value arrays here it is not yet relevant what exactly is represented by the information in the object arrays for example object ids locations in time or property values what is relevant is that there are different kinds of object arrays and that the differences between them determine how to represent them in a physical data model our hypothesis is that with a limited set of six kinds of relatively low level data models for storing arrays of temporal information we can represent the much larger set of different kinds of higher level spatio temporal objects six figures corresponding with the six object array kinds from table 1 illustrate the object array kinds and their packing into value arrays object arrays of different objects can have the same shape figs 3 5 and 7 or a different shape figs 4 6 and 8 note that the object arrays shown in the figures are small 1 d arrays but in reality object arrays can be very large and have a very different shape object arrays for temporal information can have a constant shape figs 5 and 6 or a variable shape figs 7 and 8 packing object arrays into value arrays is not possible for every kind of object array fig 8 shows that for each object array and each location in time the object information needs to be stored in a separate value array this potentially results in a large number of value arrays the approach taken in this extreme case will not be beneficial for the memory caching on the other hand figs 3 and 5 show that all object arrays of all locations in time in the case of fig 5 can be stored in a single value array ordered by location in time this approach will potentially be beneficial for the memory caching the collection of active objects often changes during a simulation section 2 in case of the packing of the temporal object array kinds shown in figs 5 8 this information is not part of the value arrays it has to be explicitly stored somewhere else in the data model for details about our approach for doing this for each of the temporal object array kinds we refer to section appendixa 3 2 spatio temporal object information abstraction level 2 in section 2 we defined spatio temporal objects as uniquely identifiable objects for which locations in time locations in space and properties are stored for those locations in time that the objects where active we will now describe for each of these pieces of object information how we decided to represent it in terms of the six kinds of object arrays described in the previous section these abstractions correspond with the second abstraction layer implemented in our data model fig 1 3 2 1 identity object identity can be represented by a unique unsigned integer value which represented as an array corresponds with a 0 d array with an empty shape this shape is the same for all objects same shape array in case of object information that does not change through time the associated object identity can be represented by the constant value same shape array object array kind fig 9 a in case of object information that does change through time for each location in time the ids of the active objects can be represented by the variable value same constant shape array object array kind fig 5 sections appendixa 1 appendixa 2 and appendixa 3 or by encoding the ids of the active objects in the meta information of the value array used to store each object array section appendixa 4 3 2 2 locations in time we implemented three different approaches for storing locations in time that objects can be active time points time boxes and time cells fig 10 which objects are actually active at these locations is handled by object tracking section appendixa a time point is used to represent a specific location in time objects can be active a time box is used to represent a period of time objects can be active it is defined by a start time point and an end time point time cells are used to discretize time boxes for more fine grained tracking of object activity we represent time points by positive integral numbers representing an amount of time duration since an epoch details about this approach for handling time can be found in appendixb time durations can be represented by 1 d object arrays with shape 1 and this shape does not change through time this matches the variable value same constant shape array object array kind fig 5 time boxes can be handled similarly as time points but instead of storing a single time point per location in time two increasing time points need to be stored so a single time box can be represented by a 1 d array with two durations in it representing the start and end time points the shape of this array is 2 and this shape stays the same through time fig 9b this matches the variable value same constant shape array object array kind fig 5 for time cells we store an additional count for each time box counts are unsigned integers represented by variable value same constant shape array object arrays 3 2 3 locations in space representing locations in space where an object is active can be done in multiple ways for example the simple feature access standard open geospatial consortium 2011 defines points lines triangles and multi polygons amongst others we implemented a sub set of these approaches space points and space boxes which we extended to support defining locations in 1 d 2 d and 3 d space both stationary and mobile locations in space are supported a space point is used to represent a specific location in space where an object is active a space box is used to represent a linear rectangular or cuboidal region of space where an object is active it is defined by two diagonally opposite space points a space point can be represented by a coordinate in each spatial dimension a coordinate can be represented by a number integer or floating point the corresponding object array of a space point then is a 1 d array with a shape equal to the rank of the space 1 2 or 3 dimensions for each point this array contains the coordinates stationary space points are represented by the constant value same shape array object array kind fig 3 since object arrays containing space points for multiple locations in time have the same shape the same 1 d array as in the stationary case mobile space points are represented by the variable value same constant shape array object array kind fig 9c as with space points a space box can be stored in a 1 d object array but this time the shape is equal to twice the shape of a single space point the object array kinds for representing this information are the same as those for representing space points constant value same shape array object array kind fig 3 in case of stationary space boxes and variable value same constant shape array object array kind fig 5 in case of mobile space boxes 3 2 4 properties in environmental modelling various kinds of property values are used to represent an object s trait these kinds of values differ from each other based on whether or not these values vary through time whether or not the shape of the arrays representing the values differ per object and whether or not the shape of these arrays varies through time for example the weight property of each deer can be represented by a single number which varies through time while the surface elevation property of a natural park area can be represented by a constant or variable 2 d numeric array depending on whether the elevation changes through time the criteria with which property values can be classified match the ones we identified for our six kinds of object arrays described in section 3 1 in the previous sections specific kinds of object arrays were used to represent specific kinds of object information in the case of properties all six object array kinds from our taxonomy can be used for storing property values this way our data model can support a wide variety of kinds of property values used in environmental modelling as an example the weight of deer through time can be stored in a variable value same constant shape array object array kind fig 5 for each location in time that each deer was active the value array will contain a floating point number representing the weight as another example a simulation model modelling the evolution of the elevation of the land surface of multiple research area objects can store 2 d digital elevation models as variable value different constant shape array object arrays fig 6 assuming the research areas are stationary and have a different but constant shape property values can be discretized through either or both time and space we decided to store information about how property values are discretized as a property itself in case of a spatial raster for example information about the number of rows and columns a 2 d property value is discretized in is stored in a separate property and linked to the property being discretized the advantage of handling information about a discretization as a property is that this information itself can vary through time and potentially even through space this is useful in simulation models where the spatial resolution of rasters changes through time for example 3 3 spatio temporal objects abstraction level 3 since we are now able to represent the individual kinds of object information identity locations in time locations in space and properties we can focus on the representation of actual spatio temporal objects in which all this information is combined and can be accessed in an integrated way the abstractions mentioned in this section are part of the third and highest abstraction level implemented in our physical data model fig 1 one of the goals we had when designing the abstractions at this level was that it must be possible to coherently store object related information at different kinds of locations in time and space for example for a collection of birds it has to be possible to store information about the winter grounds the migration route and the summer grounds another goal we had was that no information should be stored twice in the data model we identified two levels of grouping of object related information at the first level of grouping we combine information about which objects exist identity when they are active locations in time where each object is locations in space and what is present at those locations properties we call this level of grouping the property set the collection of locations in time the time domain and the collection of locations in space the space domain within a property set the information about the active sets locations in space and properties is ordered by the locations in time also within a property set there can only be one time domain and one space domain these domains are shared between all properties in the same set object related information that is located at different locations in time or space is stored in different property sets for example the constant sex of deer and their variable weight are stored in separate property sets having different time and space domains at the second and higher level of grouping we group objects of the same kind with their property sets we call such a collection a phenomenon within a phenomenon all objects are of the same kind like deer or natural parks and each object can be identified by a unique id identity also all information related to a specific kind of objects is stored within a single phenomenon in one or more property sets this grouping of spatio temporal object information is similar to the conceptual data model presented in de bakker et al 2017 whose design had a similar goal as our physical data model to represent different kinds of state variables in a uniform manner in order to make it more convenient for environmental modellers to create models in which these different kinds of variables are manipulated the conceptual data model is shown in fig 11 it defines what information is represented and the physical data model defines how this information can be stored in a dataset in an environmental modelling environment the conceptual data model can be used as a basis for the representations or data types of the model state variables whereas the physical data model can be used to allow these state variables to be persisted for later retrieval to illustrate the grouping of spatio temporal object information in our data model we will describe how to represent wandering deer the implementation of this example is described in more detail in section 5 fig 12 shows the conceptual data model for a deer phenomenon these deer are simulated using a model in which the spatial distribution of biomass in an area and the location and weight of the deer are influenced by each other the representation of the deer in our physical data model fig 13 is very similar to the conceptual data model for each deer the following information needs to be stored a unique id the sex the location in space and the weight the id and sex do not change through time while the location in space and the weight do since within a property set there can only be a single combination of a time and space domain this means that two property sets are defined in the first property set named constant no locations in time and space are stored for each deer we store whether the deer is male or female in a property in the second property set named variable we use the time cell time domain kind the mobile space point space domain kind and a property for storing 0 d numeric values representing the weights fig 13 shows that all object information is stored using one of the six array kinds described in section 3 1 in this case most information is stored using the variable value same constant shape array kinds but this depends on the specific kind of information stored when storing rasters for differently shaped areas for example a property containing variable value different constant shape array values must be used and when these rasters change shape through time a property with variable value different variable shape array values must be used instead 4 implementation we implemented our physical data model using hdf5 the hdf group 1997 2019 besides being a software library and a file format hdf5 is a data model itself the hdf5 logical data model can be used to organize information to be stored in a file the most important parts of the hdf5 data model are groups datasets and attributes groups are used to aggregate other groups and datasets while datasets are used for storing multidimensional arrays with attributes additional information can be associated with groups and datasets the hierarchical nature of the data model groups can contain other groups allows for complex nested structures to be represented multiple hdf5 datasets and groups and attributes can be stored in a single hdf5 file hdf5 datasets can be optionally configured to be extendable along one or more dimensions we have defined our data model in terms of the hdf5 logical data model using mainly groups datasets and attributes the implementation named lue 1 1 lue stands for life the universe and everything mentioned in douglas adams hitchhiker s guide to the galaxy novels here it refers to the fact that our data model is able to represent data about different kinds of information used in environmental modelling and possibly other domains we pronounce lue as the french pronounce louis lu ee consists of a c library that implements the data model and a python package with which the data model is made accessible from python scripts in the c library we implemented the abstraction layers as discussed in section 3 separately fig 1 we will now describe the implementation of each of these layers 4 1 arrays of temporal information for each of the object array kinds described in section 3 1 and shown in figs 3 8 we have implemented code to store the value arrays this low level of abstraction is implemented on top of a c library that wraps the hdf5 c library for each object array kind this layer contains code for reading and writing individual object arrays or when applicable collections of object arrays hdf5 datasets are used to store value arrays it is possible to read or write a single 1 d object array containing three floating point values or multiple 1 d object arrays in one go each containing three floating point values or such a collection of object arrays for multiple locations in time 4 2 spatio temporal object information for all kinds of object information as described in section 3 2 we developed code to read and write this information from and to a file this code reuses the code mentioned in the previous section instead of reading and writing arrays of abstract temporal information here object ids locations in time locations in space and properties are explicitly handled the implementation contains higher level classes like mobilespacepoint and property which in their implementation use the lower level code for reading and writing arrays of temporal information this makes it more convenient in higher level code to handle information related to spatio temporal objects for example listing 1 shows the declaration of a function for creating a mobilespacepoint instance such an instance manages the i o of space points that move through space the function is implemented in terms of the lower level api for handling arrays of temporal information how this is done exactly is a detail that the caller does not have to be constantly aware of listing 1 subset of c api for creating mobile space points image 1 4 3 spatio temporal objects as described in section 3 3 the highest layer of abstraction of our physical data model contains mainly grouping constructs for aggregating information about the ids locations in time locations in space and properties of the objects we have mapped these groups onto hdf5 groups for example listing 2 shows the declaration of a function for creating a propertyset instance such an instance provides access to its object tracker its time and space domains and its properties listing 2 subset of c api for creating a property set image 2 it is possible to combine multiple phenomena into a single hdf5 file this makes it possible to store all simulation state in a single lue dataset 4 4 python package in order to make it possible to manipulate lue data sets from python we implemented a python package on top of the higher abstraction levels of the c library numpy arrays oliphant 2006 are used to interface object arrays between the python and c code this makes it possible to integrate the lue physical data model with python applications manipulating numpy arrays like scipy jones et al 2001 and pcraster karssenberg et al 2010 the pybind11 c library jakob et al 2017 is used to expose the relevant parts of the lue c api to python and vice versa examples of using the lue python package can be found in the next section 5 example deer and biomass model in this section we will describe how the state variables from a simple integrated agent based and field based model can be represented by our data model using the lue python package we will highlight parts of the code the complete model can be found in the source code repository associated with this manuscript see section 8 the purpose of the deer and biomass model used in this example is to illustrate that our data model is capable of storing both traditional agent based and field based model output in this case the deer are the agents and the natural park area with a biomass raster is the field example output from the model is shown in fig 2 the model we used is simple and not realistic the data is what we want to focus on here not the validity of the model it is possible to store more complex kinds of data than used here in our data model too like properties with temporal discretizations and phenomena with multiple time and space domains we realize that what we call a high level of abstraction can be considered a low level of abstraction by modellers we envision that in real world applications an additional layer of abstraction will be built on top of our highest level of abstraction making it easy for modellers to read and write model state information this is shown as the upper layer of abstraction in fig 1 concepts for how this can be done have been described in de bakker et al 2017 the following examples assume that the lue and numpy python packages have been imported a dataset is created and two phenomena are added to it listing 3 the returned phenomenon instances deer and park in this example can be used to add property sets to the file after the model has finished executing all model state variables have been stored in a single file formatted according to the conventions described in this manuscript given that each lue dataset is also an hdf5 file the dataset can be copied to any other platform and the information stored can be retrieved again using the lue apis and hdf5 apis listing 3 import required packages image 3 5 1 initialize dataset before we can write the state of simulated spatio temporal objects to a dataset we must initialize the dataset this will prepare the dataset for receiving state information in the deer and biomass model the location and weight of each deer and the distribution of biomass in the park within which the deer are located change through time as described in the previous text time domains are contained in property sets instead of storing multiple time domains with the same locations in time in multiple property sets they can be shared this makes the resulting file smaller and prevents inconsistent time domains in our example the time domains of the property sets containing temporal deer and park information can be shared for this we create a separate phenomenon called simulation whose sole purpose is to contain a property set with a time domain to be shared with other property sets listing 4 we could also have shared the time domain of a property set in the deer phenomenon with a property set in the park phenomenon or vice versa listing 4 create property set with time cell domain and three hourly time steps image 4 the park phenomenon in our example model contains only a single object the park within which all the deer are located it contains a property set named space extent with a property named biomass for storing the 2 d biomass arrays that change through time listing 5 it also contains a property set named constant with a property named space discretization for storing the 1 d space discretization information number of rows and number of columns the latter property is linked to the first to make explicit that biomass is discretized through space the simulated biomass field is shown in fig 2b listing 5 create park phenomenon property sets image 5 the deer phenomenon contains for each deer the sex the location in space and the weight preparing the lue dataset for receiving state information during the simulation works similar to the previous case of the park phenomenon in the case of deer the space domain is different mobile space points are used instead of a stationary space box and the weight property value has a different shape 0 d instead of 2 d the simulated deer are shown in fig 2a fig 14 shows the resulting state of the data model 5 2 write model state once the dataset is initialized we can start the simulation and write the state of simulated spatio temporal objects to the dataset in our case this implies writing the initial state of the biomass raster of the park and the sex location and weight of each deer after that the interaction between the biomass field and the deer agents are simulated through time and the biomass location and weight of each deer are iteratively updated and written to the dataset in listing 6 the sex of each deer is written to the dataset given a phenomenon property sets can be obtained by name and given those properties can be obtained querying and assigning to property values works as if they are numpy arrays in this example the value array is expanded to make space for an object array per deer the expand method returns the value array on which it is called which is then indexed for all object arrays assigning to these object arrays writes the new values to the dataset listing 6 write the sex of each deer to the dataset image 6 writing state that changes through time requires also writing the object ids of the objects for which this state is written the active set appendixa tracking the object ids of the active deer is shown in listing 7 mobile space points are represented by variable value same constant shape array object arrays which implies that for tracking the ids of the objects the index of the active set and the collection of active object ids must be stored listing 7 write the object ids of the active deer to the dataset image 7 writing the updated location in space of active deer is shown in listing 8 after the new deer locations have been simulated additional space is created in the dataset and their coordinates are written listing 8 write the locations of the active deer to the dataset image 8 6 discussion we have designed and implemented a physical data model for simulated spatio temporal objects with this data model information about the state of the simulated environment can be stored for post processing although there are multiple approaches to represent state variables depending on how they represent a value and the temporal and spatial extent and variability we identified common aspects of these approaches using a limited set of six kinds of object arrays figs 3 8 these object arrays and the packing of them into value arrays form the basis of our data model layers with increasingly more domain specific code are defined on top of this base layer with the layer containing the support for spatio temporal objects at the top fig 1 using this top level layer of abstraction we were able to represent some example kinds of state variables suggesting that our data model is able to represent a diverse set of state variable kinds that otherwise would require multiple data models section 5 our data model provides a unified approach to storing spatio temporal objects using this data model the environmental modeller can represent multiple kinds of state variables the modeller is not forced to use a single data model for all state variables or use multiple very different data models in our view this is the main contribution of this work in our data model information about objects is stored in multiple arrays where each array contains a single kind of object related information this contrasts with many existing data models where this information is stored in database table records as mentioned in section 1 how information is organized in a physical data model might have an effect on how information is best organized in state variables in simulation models our data model suggests these state variables to be organized using arrays per kind of object related information with for the current modelled location in time the state of the current active sets of objects this is different from the common approach in agent based modelling of representing state variables by class instances aggregating all object related information during this work we have identified some limitations of our data model which have to do with usability and functionality our data model is capable of representing complex kinds of state variables with changing collections of mobile objects and temporal discretizations of property values for example although the data model is currently useable we think it could be made more user friendly for simple cases by adding an additional layer of abstraction on top of the layer implementing the storage of spatio temporal objects this higher level data model would represent popular existing data models like scalars rasters and raster stacks and time series and serve as an easy to use replacement for existing data models this would allow modellers to get familiar with the lue data model without having to learn about the lower level details first which are necessary for the more complex cases with respect to functionality some aspects are still missing from the current version of our data model that will be added at a later stage examples of these are the representation of relations between objects uncertainty in temporal and spatial locations and property values spatial projections discretization of presence in space other kinds of discretizations through space more space domain item type like lines and polygons spatial indices and information about topology whether or not the data model has to be adjusted for these additional aspects to be added remains to be seen although it is of crucial importance in a high performance computing context we have not evaluated the performance of our data model since our data model is implemented in terms of the hdf5 library the performance characteristics of this library determine the performance of our data model to reach maximum performance we must tune our use of hdf5 for our specific use case of storing spatio temporal objects and probably use parallel i o on parallel file systems this will be the focus of future research 7 conclusion in this paper we showed the feasibility of a physical data model capable of storing different kinds of simulation model state variables in an integrated way first we defined spatio temporal objects as uniquely identifiable objects for which locations in time locations in space and properties are stored for those locations in time that the objects were active by assuming that absence of information about presence in time or space just means that information is valid for all simulated time or space we were able to frame all considered kinds of state variables as spatio temporal objects including simple state variable kinds like scalars or non temporal rasters next we presented our physical data model as consisting of multiple levels of abstraction at the lowest level arrays of general temporal arrays are represented on top of that spatio temporal object information object identity locations in time and space and properties is represented in terms of the general temporal arrays at the highest level of abstraction spatio temporal objects are represented as aggregates of spatio temporal object information the resulting data model has been implemented in the lue software package which contains apis in c and python we have described an example of how lue can be used from an integrated field based and agent based simulation model to store various kinds of state variables in a single dataset 8 software availability the unified physical data model is implemented as part of a software package called lue which is hosted on github at https github com pcraster lue the data model is implemented by kor de jong in c and provides an api for c and python clients the code used for this work corresponds to git tag 535d52b77092686e8d0641ae2e7983e5e9eddd3c and is freely available under the mit open source license a document called readme md is included in the root of the source code repository detailing the instructions for building the software lue is portable software and has been successfully built on various platforms operating systems linux macos compilers clang gcc architecture x86 64 the readme document also contains a link to the user documentation of the lue python package the example model used in section 5 can also be found on github at https github com pcraster paper 2019 physical data model funding this work was supported by the research it innovation programme utrecht university the netherlands and the global geo health data center utrecht university and university medical center utrecht the netherlands author contributions kdj and dk both worked on the concepts kdj designed and implemented the data model kdj wrote the manuscript with inputs from dk dk coordinated the project declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a object tracking here we describe in some more detail how we keep track of for which objects information is stored in the physical data model this is relevant for the four object array kinds used to store temporal information for each location in time and for each active object we store where each object array can be found how this is done differs per object array kind appendix a 1 variable value same constant shape array per location in time the value array a contains the values for all active objects fig 5 an additional array a c t i v e o b j e c t i d is used to store the ids of the active objects an id is a unique number see section 3 2 1 the order of information in a and a c t i v e o b j e c t i d is the same an additional array a c t i v e s e t i n d e x is used to store the index into a and a c t i v e o b j e c t i d where for each location in time the id and object array of the first active object can be found see also table a 2 table a 2 procedures for storing and finding variable value same constant shape array object arrays table a 2 store append id to a c t i v e o b j e c t i d and object array to a find given index of location in time read index s of section of active objects from a c t i v e s e t i n d e x and determine index o of object id in a c t i v e o b j e c t i d the object array is located at a s o appendix a 2 variable value same variable shape array per location in time a separate value array a t contains the values for all active objects fig 7 similar to the previous case additional arrays a c t i v e o b j e c t i d and a c t i v e s e t i n d e x are used to track the ids of active objects and the offset into a c t i v e o b j e c t i d of the id of the first active object respectively each value array a t is named after its corresponding index of location in time see also table a 3 table a 3 procedures for storing and finding variable value same variable shape array object arrays table a 3 store append id to a c t i v e o b j e c t i d and object array to a t find given index of location in time read index of section of active objects from a c t i v e s e t i n d e x and determine index o of object id in a c t i v e o b j e c t i d the object array is located at a t o appendix a 3 variable value different constant shape array per object a separate value array a o contains the values for all locations in time that the object was active fig 6 again an additional array a c t i v e o b j e c t i d is used to track the ids of active objects array a c t i v e o b j e c t i n d e x is used to track the indices into the a o arrays of the active objects array a c t i v e s e t i n d e x is used to store the offset into a c t i v e o b j e c t i d and a c t i v e o b j e c t i n d e x of the first active object per location in time see also table a 4 table a 4 procedures for storing and finding variable value different constant shape array object arrays table a 4 store append id to a c t i v e o b j e c t i d append index of object array in a o to a c t i v e o b j e c t i n d e x and append object array to a o find given index of location in time read index of section of active objects from a c t i v e s e t i n d e x and determine index of object id in a c t i v e o b j e c t i d given this index lookup index t of object array in a c t i v e o b j e c t i n d e x the object array is located at a o t appendix a 4 variable value different variable shape array per location in time and per object a separate value array a t o contains the object array fig 8 see also table a 5 table a 5 procedures for storing and finding variable value different variable shape array object arrays table a 5 store create value array named after current location in time and object and write object array to it find open value array named after current location in time and object appendix b representing time in order to be able to represent time points of different resolutions like nanoseconds and centuries we used an approach inspired by the c chrono library 2 2 http en cppreference com w cpp header chrono time points are dependent on a clock a clock represents a period of time since an epoch and has a certain resolution this resolution is defined by the clock s tick period which has a unit seconds or years for example and a count two for ticks of 2 s or years for example given this a time point can be represented by a duration which is represented by a number of ticks we decided to represent a collection of time points by a single clock represented by an epoch a unit a string and a count a positive integral number and a duration for each time point represented by counts positive integral numbers we represent epochs by an epoch kind and an optional origin and optional calendar all strings currently supported epoch kinds are common era ce and formation of earth but more can be added the currently supported calendar is gregorian appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104553 
26128,a matlab based digital elevation model dem data processing toolbox mdem feifei pan a xiaohuan xi b cheng wang b a department of geography and the environment university of north texas denton tx 76203 usa department of geography and the environment university of north texas denton tx 76203 usa department of geography and the environment university of north texas denton tx 76203 usa b key laboratory of digital earth institute of remote sensing and digital earth chinese academy of sciences beijing 100094 china key laboratory of digital earth institute of remote sensing and digital earth chinese academy of sciences beijing 100094 china key laboratory of digital earth institute of remote sensing and digital earth chinese academy of sciences beijing 100094 china corresponding author a matlab based digital elevation model dem data processing toolbox called mdem is built upon an improved dem data processing algorithm known as the pdem for rectifying flat areas and depressions in dem data and producing realistic patterns of flow accumulation and extracted channel networks compared to the original pdem the computation efficiency of the mdem has been improved significantly through adding one fill sink step i e replace all sink pixels elevations in a depression by the lowest elevation along the boundary of the depression prior to applying the linear interpolation to treat each depression the mdem has a graphical user interface gui and six functions including rectifying flat areas and depressions in dem data using the improved pdem algorithm calculation of flow direction calculation of flow accumulation area watershed delineation extraction of channel networks and calculation of the topographic index all these functions are essential for computing useful topographic variables for hydrological modeling and applications keywords matlab digital elevation model dem flat and sink pixels flow direction flow accumulation area channel networks topographic index 1 introduction since the 1980s digital elevation model dem data have been widely utilized in hydrological modeling and applications because they are essential to compute a number of key topographic characteristics e g slope aspect flow direction flow accumulation area topographic index and to delineate watersheds and extract channel networks e g band 1986 o callaghan and mark 1984 quinn et al 1991 quinn et al 1995 wolock and mccabe 1995 tarboton 1997 zhang et al 1999 jones 1998 pan et al 2004 pan et al 2012 gallant and hutchinson 2011 qin et al 2011 radula et al 2018 among these topographic variables only slope and aspect can be calculated without determining flow direction at each pixel of a dem dataset and all other topographic variables can be computed only after flow directions at flat and sink pixels are accurately resolved therefore treating flat and sink pixels for determining flow directions at these pixels is the first indispensable step in processing dem data for computing topographic variables because virtually all dem data contain real or artificial flat and sink pixels although many algorithms have been developed and published in the literature for treating flat and sink pixels there are several limitations associated with these methods e g artificial parallel patterns in the extracted channel networks by jenson and domingue 1988 method channel network geospatial data required for forcing flat and sink pixels flow to the nearest channel pixels in the river burning method e g hutchinson 1989 addition or subtraction of arbitrary small numbers to or from elevations of flat and sink pixels in the topographic parameterization topaz method garbrecht and martz 1997 martz and garbrecht 1998 to eliminate these limitations pan et al 2012 developed an effective algorithm called pdem to rectify flat and sink pixels using linear interpolation between lower elevation grid cells on the edge of each flat area or depression fad and the higher elevation grid cells on the opposite edge of the fad linear interpolation across flat areas or depressions provides a natural way to scale elevation adjustments based on the vertical scale of the surrounding topography thereby avoiding the addition subtraction of arbitrary small numbers to from elevations in the flat areas and depressions tests for two virtual terrains and one real terrain showed that the pdem can effectively rectify flat areas and depressions and produce realistic patterns of channel networks pan et al 2012 although the pdem is effective the computational efficiency of the pdem is low as shown in pan et al 2012 the pdem took about 22 583 iteration steps to completely treat 13 573 flat or sink pixels in a dem dataset i e ncdem asc in the pdem folder that can be downloaded from http www geography unt edu fpan pdem another reason for the pdem s inefficient running time is that as the pdem is treating flat and sink pixels it is also utilizing cpu time to simultaneously display locations of flat and sink pixels on the screen the pdem was coded in processing https www processing org an open source and open platform programming language which is still under development the frequent upgrade of processing requires the pdem to be frequently updated to the latest version of processing which has caused problems in maintaining the consistency of the pdem the problems associated with the current version of the pdem promoted us to improve the computational efficiency in the pdem algorithm and develop a matlab based dem data processing toolbox using the improved pdem algorithm which is called mdem considering the pdem algorithm is a better method for treating flat and sink pixels in dem data pan et al 2012 the second goal for developing the mdem is to include the following functions in the mdem after flat and sink pixels in dem data are treated calculation of flow direction and flow accumulation area delineation of watershed extraction of channel network and calculation of the topographic index the aforementioned topographic variables are all important for hydrological modeling and applications and among them the topographic index is the key variable for the topmodel based hydrological modeling while the topmodel beven and kirkby 1979 has been widely used to study the topographic effects on hydrologic and hydraulic processes including flood frequency stream flow generation flow paths geomorphic characteristics and water quality therefore the mdem will be useful for processing dem data and computing topographic variables for hydrological modeling and applications the remainder of this paper is organized into following three sections section 2 describes methods for designing implementing and testing the mdem section 3 presents results and discussion conclusions and outlook of the mdem are given in section 4 2 methodology 2 1 mdem design architecture the mdem is coded in matlab and the app designer function imbedded in matlab is used for building the graphical user interface gui of the mdem the flowchart of the mdem is illustrated in fig 1 and the gui of the mdem is shown in fig 2 the mdem toolbox contains three main components data i o i e input dem data and parameters and ouput results data processing i e process dem data and compute topographic variables and data visualization i e display results in matlab s figure windows the current version of the mdem i e mdem v1 0 takes dem data in the arcgis grid ascii format which can be read by matlab s arcgridread function prior to running the mdem it is necessary to convert a dem raster file into an ascii file using the arcgis data conversion function each ascii file has a six line header including 1 ncols number of columns 2 nrows number of rows 3 xllcorner x coordinate of the lower left corner 4 yllcorner y coordinate of the lower left corner 5 cellsize grid cell size and 6 nodata value no data value all outputs from the mdem are saved in the same ascii format with a six line header the mdem has the following six data processing functions function 1 is run pdem which is to treat flat and sink pixels in dem data using the improved pdem algorithm to be described in section 2 2 as this function is running numbers of iteration step flat areas and depressions and flat and sink pixels are displayed in the command window during each iteration step after all flat and sink pixels are treated with resolved flow directions the processed dem dataset is saved in p dem txt if the input dem file name is dem txt through typing the input file name in the input dem textbox see the mdem s gui shown in fig 2 the processed dem dataset is also displayed in matlab s figure window function 2 is flow direction which is to compute flow direction based on the treated dem dataset i e all flat and sink pixels are treated by the pdem algorithm using the single flow direction method after all flat and sink pixels are treated flow direction at any pixel can be determined using any of following three flow direction methods 1 single flow direction sfd method o callaghan and mark 1984 2 bi flow direction bfd method tarboton 1997 and 3 multiple flow direction mfd method quinn et al 1991 considering it is simple to display only one flow direction at each pixel the flow direction function of the mdem is to compute flow directions using the sfd method and display the resolved flow direction image in matlab s figure window the computed flow direction is saved in a grid ascii file called fdir dem txt if the input dem file is dem txt function 3 is flow accumulation area a which is to compute flow accumulation area based on the computed flow direction at each pixel the flow accumulation area of a pixel represents the total upslope area flowing through the pixel to compute flow accumulation area flow directions are tracked upslope starting from the pixel of interest to the upstream divide of the watershed and then tracking downslope pixels contributing to the drainage area of the pixel of interest after the flow accumulation area of each pixel is computed an image of ln a is displayed in matlab s figure window since the flow accumulation areas in channels are several magnitude larger than those of hillslope pixels ln a is displayed the computed flow accumulation area is saved in a grid ascii file called facc dem txt if the input dem file is dem txt function 4 is watershed delineation which is to delineate a watershed based on the input outlet coordinates the mdem provides a drop down list for three options of the input outlet coordinates see fig 2 i lon lat longitude and latitude in decimal degrees if the input dem dataset is in the geographic projection ii east north east and north in meters if the input dem dataset is in the universal transverse mercator utm projection and iii col row column and row indexes of the dem dataset users can use matlab s data cursor tool to identify an outlet s column and row indexes from the displayed flow accumulation area i e ln a as a watershed outlet is often selected from a cross section on the channel network which is easy to identify from the displayed ln a because channel pixels have much larger ln a values than the hillslope pixels after input the outlet coordinates click on the watershed delineation button a watershed will be delineated based on the input outlet the boundary of the delineated watershed is displayed overlaying the image of ln a and the mask of the delineated watershed is saved in a grid ascii file called wd dem txt if the input dem file is dem txt function 5 is channel networks which is to extract the channel network based on input ln a threshold see fig 2 after a watershed is delineated channel networks inside the delineated watershed can be extracted based on an input threshold of ln a through the ln a threshold textbox i e if a pixel s ln a threshold it is a channel pixel after channel networks are extracted an image showing the processed dem overlaid by the extracted channel networks is displayed in matlab s figure window and the extracted channel networks are saved in ch dem txt if the input file is dem txt function 6 is topographic index ln a tanb which is to compute the natural logarithm of the ratio of the specific flow accumulation area a i e the flow accumulation area a per unit contour length which is usually set to be the dem grid cell size to the slope tanb i e ln a tanb the topographic index is only computed inside the delineated watershed if the watershed is not fully inside the dem domain the computed topographic indexes at some pixels are subject to errors because some of their upslope areas may not be captured by the dem dataset if that is the case a new dem dataset needs to be re clipped to cover the entire watershed since the multiple flow direction mfd method proposed by quinn et al 1991 is better for computing ln a tanb in terms of the accuracy pan et al 2004 this mfd method is used in the mdem s topographic index function after all topographic indexes inside the delineated watershed are computed an image of ln a tanb is shown in matlab s figure window and the results are saved in tpi dem txt if the input file is dem txt 2 2 the improved pdem algorithm the key component in the mdem is to treat flat and sink pixels in dem data using the pdem algorithm pan et al 2012 the fundamental basis of the pdem algorithm is to recompute elevation at flat or sink pixels inside a flat area or depression using the linear interpolation method a flat area or depression fad is defined as an area containing only flat that has the same elevation as all its eight neighboring pixels or only sink that is lower than all its eight neighboring pixels pixels or both in the pdem algorithm flat and sink pixels are not differentiated and they are referred to as fs flat or sink pixels all connected fs pixels are grouped into one fad to treat each fad the first step is to determine the pixel with the lowest elevation along the boundary of the fad and designate this pixel as the outlet of this fad to use the linear interpolation method to recompute elevation at a fs pixel inside this fad a line is drawn from the fs pixel to the outlet if this line is completely inside the fad it is extended backward to the other side of the boundary and intersection between this line and the boundary is denoted as the inflow pixel fig 3 a the linear interpolation method is used to compute elevation at this fs pixel 1 z z o l i z i l o l o l i where z o and z i are elevations at the outlet and inflow pixels respectively l o and l i are distances from the fs pixel to the outlet and inflow pixels respectively if the line from a fs pixel to the outlet passes out of the fad fig 3b the linear interpolation for this fs pixel s elevation cannot be carried out at the current iteration step using this outlet pixel because the objective of the linear interpolation is to force all fs pixels in the fad to eventually flow to the outlet pixel without passing out of the fad therefore a number of iterations are required for treating all fads till all fs pixels are eliminated in the dem since the pdem algorithm considered flat and sink pixels as fs pixels and used the same linear interpolation method to treat them there was no fill sink step i e replace all sink pixels elevations in a depression by the lowest elevation along the boundary of the depression in the old version of the pdem algorithm on the other hand even if sinks are filled they will become flat pixels and their flow directions are still not defined that is why in the old version of the pdem there was no fill sink step in the improved pdem algorithm during each iteration step we will add the fill sink step prior to using the linear interpolation method to recompute elevations at fs pixels in depressions if we replace elevations of all sink pixels by the lowest elevation among the boundary pixels some boundary pixels with the same elevation as the lowest elevation will become flat pixels if prior to filling sinks they can only flow to the depression after the sinks are filled these boundary pixels will become flat pixels and their flow directions are not defined the previously defined depression area will expand and its boundary will also change overall filling sinks can cause some previously unconnected fads to merge which will reduce numbers of fads and iterations and ultimately improve the computational efficiency on the other hand filling sinks can also reduce the probability of dead loops or oscillations in the iteration of treating fads because without filling sinks the linear interpolation can change some outlet pixels of depressions that can only flow to the depressions to sink pixels and these sink pixels need to be treated in the next iteration step this will cause a back and forth oscillation in the iteration of treating fads and significantly prolong the running time for completely resolving flat and sink pixels 2 3 experimental testing strategy to test the performance of the mdem ten dem datasets across southeast of south carolina with relatively flat terrain were clipped from three the u s geological survey usgs national elevation database ned 1 arc second about 30 m resolution 1 1 dem tiles n34w082 n34w081 n35w080 table 1 lists dem file name number of columns ncols number of rows nrows x coordinate xllcorner and y coordinate yllcorner of lower left corner and number of fs pixels nfsp of each test dem dataset as shown in table 1 all the test dem data have more than 1800 fs pixels and their nsfps range from 1808 to 12896 both the pdem and mdem were applied to process these ten dem datasets using a macbook pro with a 2 9 ghz intel core i7 processor number of iterations ni for completely treating the fs pixels and associated running time rt of each test dem dataset are also listed in table 1 through comparing nis and rts between the pdem and mdem the improvement of the computational efficiency in the mdem can be quantitively determined 3 results and discussion 3 1 improvement in the computational efficiency the ten dem datasets listed in table 1 were processed by both the pdem and mdem prior to comparing the computational efficiency between the pdem and mdem all the mdem s processed dem datasets were compared with the corresponding pdem s processed dem datasets and it was found that they were identical which indicates that the mdem can achieve the same results as the pdem however the results listed in table 1 clearly show that the mdem is more efficient than the pdem because the number of iterations ni and the running time rt for the mdem are much less than those of the pdem and the percentage decreases in ni and rt range from 87 to 99 and from 89 to 99 respectively based on the number of iterations ni and the running time rt given in table 1 four scatter plots i e nfsp vs nip and nfsp vs rtp for the pdem and nfsp vs nim and nfsp vs rtm for the mdem were plotted in fig 4 all these scatter plots indicate that as nfsp increases both ni and rt increase for both methods fig 4 also demonstrates that including a fill sink step prior to the linear interpolation in the mdem can significantly reduce the number of iterations and running time and thus improve the computational efficiency does this imply that multiple steps of fill sink are necessary prior to the linear interpolation since as the number of fill sink steps increases the number of fads in a dem dataset will decrease and the number of iterations for treating the fads would decrease but the running time does not necessarily decrease because as the number of fads decreases the total number of fs pixels inside fads could increase and the running time would increase therefore there might exist a trade off between the number of fill sink steps and the running time to identify the optimal number of fill sink steps a series of tests with different numbers of fill sink steps were carried out scatter plots of number of fill sink steps versus ni and rt for the mdem as illustrated in fig 5 indicate that as the number of fill sink steps increased the number of iterations decreased but the running time was not reduced by the increase in the number of fill sink steps therefore including only one step of fill sink prior to the linear interpolation is optimal and sufficient 3 2 mdem to demonstrate that all designed functions described in section 2 1 were implemented in the mdem a 1 arc second dem dataset lwwg txt covering the little washita watershed in the southwestern oklahoma was clipped from one usgs ned 1 arc second n35w099 1 1 dem tile downloaded from the u s geological survey usgs national elevation database ned all the matlab scripts and example files i e ten test dem datasets listed in table 1 and lwwg txt for the mdem were zipped in mdem tar that can be downloaded from http www geography unt edu fpan mdem unzip mdem tar and yield a folder called mdem to run the mdem first launch matlab and then in the matlab command window find and open the mdem folder in the mdem folder click app mlapp the interface of the mdem as shown in fig 2 will pop up on the screen in order to calculate slope and flow accumulation area properly using dem data in the geographic projection the mdem first computes the grid cell size in meters at the center of the input dem dataset using the haversine formula van brummelen 2013 and then the computed central grid cell size is used to represent the grid cell size for the entire dem dataset however it should be noted here that the computed slope flow accumulation area and topographic index are subject to errors if the input dem dataset is in the geographic projection because for the dem data in the geographic projection the horizontal distance between two pixels is not constant across the entire domain of the dem and the variation in the pixel distance should be considered if the spatial extent of the dem dataset is not small nevertheless the mdem can handle dem data in both the geographic and utm projections in the input dem textbox type in lwwg txt then click run pdem first the mdem determines the grid cell size of the input dem for the geographic projection the grid cell size is in decimal degree thus the following line cell is 2 777778e 04 in degrees which is about 2 824999e 01 meters is showing in the command window to show the progress of processing dem the iteration information of each step is also displayed in the matlab command window for example the first iteration step shows iteration step 1 number of flat areas 16460 number of flat and sink pixels 16863 after 74 iteration steps all flat and sink pixels are treated and the processed dem is displayed in the matlab figure window fig 6 and saved as p lwwg txt next click flow direction to compute flow direction at each pixel and display the computed flow directions in the matlab figure window fig 7 based on the computed flow directions the flow accumulation area a can be computed by clicking flow accumulation area a button the mdem displays ln a in the matlab figure window fig 8 and the computed a is saved in facc lwwg txt an outlet is required for delineating a watershed the mdem provides three options for inputing the outlet coordinates as described in section 2 1 from the drop down list lon lat is selected for the geographic projection if the little washita river gaging station near ninnekah oklahoma 34 56 41 n 97 57 08 w is chosen as the outlet input 97 952222 in lon east col textbox and 34 944722 in lat north row textbox and click watershed delineation to delineate the little washita river watershed in the matlab command window run watershed delineation is displayed and after it is done the following line is displayed done watershed delineation watershed area 1596 12 m2 watershed mask is saved in wd lwwg txt in the matlab figure window ln a is displayed a green circle centered at the selected outlet is also shown on the figure and the delineated watershed boundary is shown as a red curve generally an outlet should cross a channel while by looking at the figure of ln a we can find that the input outlet is not across the channel which is associated with very high ln a and there is a slight northward shift using matlab data cursor to identify the column and row indexes of the outlet we can find that x 1337 y 197 fig 9 a if we move y downward by 4 pixels i e y 201 and move x left by 1 pixel i e x 1336 we can reach the channel fig 9b now we can use two options to input the correct outlet coordinates i e 1 select col row and input 1336 in lon east col textbox input 201 in lat north row textbox click watershed delineation and yield a watershed with a drainage area of 536873870 m2 207 3 mile2 2 select lon lat input 97 952500 97 952222 0 000278 in lon east col textbox input 34 943610 34 944722 4 0 000278 in lat north row textbox click watershed delineation and yield a watershed with a drainage area of 536872273 m2 207 3 mile2 both options produced an almost identical watershed drainage area which is close to the drainage area of 208 mile2 listed in the little washita river gaging station near ninnekah website https waterdata usgs gov nwis inventory site no 07327490 after the watershed is delineated a red curve representing the watershed boundary is plotted on the ln a image fig 9c and the delineated watershed mask is saved in wd lwwg txt after the watershed is delineated channel networks can be extracted inside the watershed based on the input ln a threshold for example if input 10 in the ln a threshold textbox and click channel networks the extracted channel networks will be shown in blue overlaying on the processed dem image fig 10 and channel network mask is saved in ch lwwg txt after the watershed is delineated the topographic index i e ln a tanb at each pixel inside the delineated watershed can also be computed fig 11 is an image of the computed topographic index which is saved in tpi lwwg txt to properly calculate the topography index the dem dataset should be re projected from the geographic projection to the utm projection using 30 m grid cell size for 1 arc second dem which can be implemented by employing the arcgis raster projection function using the bilinear resampling method using the arcgis data conversion tool to convert the raster file into the ascii format lwwub txt input lwwwub txt in the mdem run the same first three steps i e run pdem flow direction and flow accumulation area to delineate watershed using the dem in the utm projection the online geographic utm tool provided by noaa https www ngs noaa gov cgi bin utm getut prla was used to convert the outlet s latitude and longitude to the utm zone 14 s east and north in meters i e east 595678m north 3867414m it was found that the selected outlet was not across the channel and it needs to be shifted downward by two pixels 60 m in y direction y 3867354 m and eastward by one pixel 30 m in x direction x 595708 m click watershed delineation yield a watershed with a drainage area of 538369200 m2 208 mile2 which is exactly the same as the watershed drainage area listed on the usgs ninnekah gaging station website then channel networks and the topographic index can be computed inside the delineated watershed and displayed in the matlab figure windows figs 12 and 13 and the corresponding files are saved in ch lwwub txt and tpi lwwub txt respectively 4 conclusions and outlook there are several improvements in the mdem compared with the pdem 1 the processing is replaced by the matlab app designer function for building the graphical user interface gui of the mdem 2 visualization of locations of flat and sink pixels at each iteration step is eliminated from the mdem instead the progress of treating flat and sink pixels i e numbers of iteration step flat areas and depressions and flat and sink pixels is displayed in the matlab command window which can improve the mdem computational efficiency 3 in the mdem a fill sink step is applied to each depression prior to applying the linear interpolation to treat flat and sink pixels during each iteration step which can greatly shorten the running time by more than 89 4 in addition to treating flat and sink pixels the mdem also has other five functions i e calculation of flow direction calculation of flow accumulation area delineation of watershed extraction of channel network and calculation of the topographic index although the mdem is more efficient than the pdem processing of flat and sink pixels in some fine resolution e g 10 m or 5 m dem data with relatively flat terrains could still be time consuming to further improve the computational efficiency of the mdem the matlab parallel computing toolbox will be incorporated with the mdem for developing a new version 2 0 of the mdem after the mdem v2 0 is completed it will be posted at the same website as the mdem v1 0 acknowledgement this research was partially funded by the joint research fund for overseas chinese scholars and scholars in hong kong and macao of national natural science foundation of china no 416628101 and the university of north texas scholarly and creative activity sca award f pan 
26128,a matlab based digital elevation model dem data processing toolbox mdem feifei pan a xiaohuan xi b cheng wang b a department of geography and the environment university of north texas denton tx 76203 usa department of geography and the environment university of north texas denton tx 76203 usa department of geography and the environment university of north texas denton tx 76203 usa b key laboratory of digital earth institute of remote sensing and digital earth chinese academy of sciences beijing 100094 china key laboratory of digital earth institute of remote sensing and digital earth chinese academy of sciences beijing 100094 china key laboratory of digital earth institute of remote sensing and digital earth chinese academy of sciences beijing 100094 china corresponding author a matlab based digital elevation model dem data processing toolbox called mdem is built upon an improved dem data processing algorithm known as the pdem for rectifying flat areas and depressions in dem data and producing realistic patterns of flow accumulation and extracted channel networks compared to the original pdem the computation efficiency of the mdem has been improved significantly through adding one fill sink step i e replace all sink pixels elevations in a depression by the lowest elevation along the boundary of the depression prior to applying the linear interpolation to treat each depression the mdem has a graphical user interface gui and six functions including rectifying flat areas and depressions in dem data using the improved pdem algorithm calculation of flow direction calculation of flow accumulation area watershed delineation extraction of channel networks and calculation of the topographic index all these functions are essential for computing useful topographic variables for hydrological modeling and applications keywords matlab digital elevation model dem flat and sink pixels flow direction flow accumulation area channel networks topographic index 1 introduction since the 1980s digital elevation model dem data have been widely utilized in hydrological modeling and applications because they are essential to compute a number of key topographic characteristics e g slope aspect flow direction flow accumulation area topographic index and to delineate watersheds and extract channel networks e g band 1986 o callaghan and mark 1984 quinn et al 1991 quinn et al 1995 wolock and mccabe 1995 tarboton 1997 zhang et al 1999 jones 1998 pan et al 2004 pan et al 2012 gallant and hutchinson 2011 qin et al 2011 radula et al 2018 among these topographic variables only slope and aspect can be calculated without determining flow direction at each pixel of a dem dataset and all other topographic variables can be computed only after flow directions at flat and sink pixels are accurately resolved therefore treating flat and sink pixels for determining flow directions at these pixels is the first indispensable step in processing dem data for computing topographic variables because virtually all dem data contain real or artificial flat and sink pixels although many algorithms have been developed and published in the literature for treating flat and sink pixels there are several limitations associated with these methods e g artificial parallel patterns in the extracted channel networks by jenson and domingue 1988 method channel network geospatial data required for forcing flat and sink pixels flow to the nearest channel pixels in the river burning method e g hutchinson 1989 addition or subtraction of arbitrary small numbers to or from elevations of flat and sink pixels in the topographic parameterization topaz method garbrecht and martz 1997 martz and garbrecht 1998 to eliminate these limitations pan et al 2012 developed an effective algorithm called pdem to rectify flat and sink pixels using linear interpolation between lower elevation grid cells on the edge of each flat area or depression fad and the higher elevation grid cells on the opposite edge of the fad linear interpolation across flat areas or depressions provides a natural way to scale elevation adjustments based on the vertical scale of the surrounding topography thereby avoiding the addition subtraction of arbitrary small numbers to from elevations in the flat areas and depressions tests for two virtual terrains and one real terrain showed that the pdem can effectively rectify flat areas and depressions and produce realistic patterns of channel networks pan et al 2012 although the pdem is effective the computational efficiency of the pdem is low as shown in pan et al 2012 the pdem took about 22 583 iteration steps to completely treat 13 573 flat or sink pixels in a dem dataset i e ncdem asc in the pdem folder that can be downloaded from http www geography unt edu fpan pdem another reason for the pdem s inefficient running time is that as the pdem is treating flat and sink pixels it is also utilizing cpu time to simultaneously display locations of flat and sink pixels on the screen the pdem was coded in processing https www processing org an open source and open platform programming language which is still under development the frequent upgrade of processing requires the pdem to be frequently updated to the latest version of processing which has caused problems in maintaining the consistency of the pdem the problems associated with the current version of the pdem promoted us to improve the computational efficiency in the pdem algorithm and develop a matlab based dem data processing toolbox using the improved pdem algorithm which is called mdem considering the pdem algorithm is a better method for treating flat and sink pixels in dem data pan et al 2012 the second goal for developing the mdem is to include the following functions in the mdem after flat and sink pixels in dem data are treated calculation of flow direction and flow accumulation area delineation of watershed extraction of channel network and calculation of the topographic index the aforementioned topographic variables are all important for hydrological modeling and applications and among them the topographic index is the key variable for the topmodel based hydrological modeling while the topmodel beven and kirkby 1979 has been widely used to study the topographic effects on hydrologic and hydraulic processes including flood frequency stream flow generation flow paths geomorphic characteristics and water quality therefore the mdem will be useful for processing dem data and computing topographic variables for hydrological modeling and applications the remainder of this paper is organized into following three sections section 2 describes methods for designing implementing and testing the mdem section 3 presents results and discussion conclusions and outlook of the mdem are given in section 4 2 methodology 2 1 mdem design architecture the mdem is coded in matlab and the app designer function imbedded in matlab is used for building the graphical user interface gui of the mdem the flowchart of the mdem is illustrated in fig 1 and the gui of the mdem is shown in fig 2 the mdem toolbox contains three main components data i o i e input dem data and parameters and ouput results data processing i e process dem data and compute topographic variables and data visualization i e display results in matlab s figure windows the current version of the mdem i e mdem v1 0 takes dem data in the arcgis grid ascii format which can be read by matlab s arcgridread function prior to running the mdem it is necessary to convert a dem raster file into an ascii file using the arcgis data conversion function each ascii file has a six line header including 1 ncols number of columns 2 nrows number of rows 3 xllcorner x coordinate of the lower left corner 4 yllcorner y coordinate of the lower left corner 5 cellsize grid cell size and 6 nodata value no data value all outputs from the mdem are saved in the same ascii format with a six line header the mdem has the following six data processing functions function 1 is run pdem which is to treat flat and sink pixels in dem data using the improved pdem algorithm to be described in section 2 2 as this function is running numbers of iteration step flat areas and depressions and flat and sink pixels are displayed in the command window during each iteration step after all flat and sink pixels are treated with resolved flow directions the processed dem dataset is saved in p dem txt if the input dem file name is dem txt through typing the input file name in the input dem textbox see the mdem s gui shown in fig 2 the processed dem dataset is also displayed in matlab s figure window function 2 is flow direction which is to compute flow direction based on the treated dem dataset i e all flat and sink pixels are treated by the pdem algorithm using the single flow direction method after all flat and sink pixels are treated flow direction at any pixel can be determined using any of following three flow direction methods 1 single flow direction sfd method o callaghan and mark 1984 2 bi flow direction bfd method tarboton 1997 and 3 multiple flow direction mfd method quinn et al 1991 considering it is simple to display only one flow direction at each pixel the flow direction function of the mdem is to compute flow directions using the sfd method and display the resolved flow direction image in matlab s figure window the computed flow direction is saved in a grid ascii file called fdir dem txt if the input dem file is dem txt function 3 is flow accumulation area a which is to compute flow accumulation area based on the computed flow direction at each pixel the flow accumulation area of a pixel represents the total upslope area flowing through the pixel to compute flow accumulation area flow directions are tracked upslope starting from the pixel of interest to the upstream divide of the watershed and then tracking downslope pixels contributing to the drainage area of the pixel of interest after the flow accumulation area of each pixel is computed an image of ln a is displayed in matlab s figure window since the flow accumulation areas in channels are several magnitude larger than those of hillslope pixels ln a is displayed the computed flow accumulation area is saved in a grid ascii file called facc dem txt if the input dem file is dem txt function 4 is watershed delineation which is to delineate a watershed based on the input outlet coordinates the mdem provides a drop down list for three options of the input outlet coordinates see fig 2 i lon lat longitude and latitude in decimal degrees if the input dem dataset is in the geographic projection ii east north east and north in meters if the input dem dataset is in the universal transverse mercator utm projection and iii col row column and row indexes of the dem dataset users can use matlab s data cursor tool to identify an outlet s column and row indexes from the displayed flow accumulation area i e ln a as a watershed outlet is often selected from a cross section on the channel network which is easy to identify from the displayed ln a because channel pixels have much larger ln a values than the hillslope pixels after input the outlet coordinates click on the watershed delineation button a watershed will be delineated based on the input outlet the boundary of the delineated watershed is displayed overlaying the image of ln a and the mask of the delineated watershed is saved in a grid ascii file called wd dem txt if the input dem file is dem txt function 5 is channel networks which is to extract the channel network based on input ln a threshold see fig 2 after a watershed is delineated channel networks inside the delineated watershed can be extracted based on an input threshold of ln a through the ln a threshold textbox i e if a pixel s ln a threshold it is a channel pixel after channel networks are extracted an image showing the processed dem overlaid by the extracted channel networks is displayed in matlab s figure window and the extracted channel networks are saved in ch dem txt if the input file is dem txt function 6 is topographic index ln a tanb which is to compute the natural logarithm of the ratio of the specific flow accumulation area a i e the flow accumulation area a per unit contour length which is usually set to be the dem grid cell size to the slope tanb i e ln a tanb the topographic index is only computed inside the delineated watershed if the watershed is not fully inside the dem domain the computed topographic indexes at some pixels are subject to errors because some of their upslope areas may not be captured by the dem dataset if that is the case a new dem dataset needs to be re clipped to cover the entire watershed since the multiple flow direction mfd method proposed by quinn et al 1991 is better for computing ln a tanb in terms of the accuracy pan et al 2004 this mfd method is used in the mdem s topographic index function after all topographic indexes inside the delineated watershed are computed an image of ln a tanb is shown in matlab s figure window and the results are saved in tpi dem txt if the input file is dem txt 2 2 the improved pdem algorithm the key component in the mdem is to treat flat and sink pixels in dem data using the pdem algorithm pan et al 2012 the fundamental basis of the pdem algorithm is to recompute elevation at flat or sink pixels inside a flat area or depression using the linear interpolation method a flat area or depression fad is defined as an area containing only flat that has the same elevation as all its eight neighboring pixels or only sink that is lower than all its eight neighboring pixels pixels or both in the pdem algorithm flat and sink pixels are not differentiated and they are referred to as fs flat or sink pixels all connected fs pixels are grouped into one fad to treat each fad the first step is to determine the pixel with the lowest elevation along the boundary of the fad and designate this pixel as the outlet of this fad to use the linear interpolation method to recompute elevation at a fs pixel inside this fad a line is drawn from the fs pixel to the outlet if this line is completely inside the fad it is extended backward to the other side of the boundary and intersection between this line and the boundary is denoted as the inflow pixel fig 3 a the linear interpolation method is used to compute elevation at this fs pixel 1 z z o l i z i l o l o l i where z o and z i are elevations at the outlet and inflow pixels respectively l o and l i are distances from the fs pixel to the outlet and inflow pixels respectively if the line from a fs pixel to the outlet passes out of the fad fig 3b the linear interpolation for this fs pixel s elevation cannot be carried out at the current iteration step using this outlet pixel because the objective of the linear interpolation is to force all fs pixels in the fad to eventually flow to the outlet pixel without passing out of the fad therefore a number of iterations are required for treating all fads till all fs pixels are eliminated in the dem since the pdem algorithm considered flat and sink pixels as fs pixels and used the same linear interpolation method to treat them there was no fill sink step i e replace all sink pixels elevations in a depression by the lowest elevation along the boundary of the depression in the old version of the pdem algorithm on the other hand even if sinks are filled they will become flat pixels and their flow directions are still not defined that is why in the old version of the pdem there was no fill sink step in the improved pdem algorithm during each iteration step we will add the fill sink step prior to using the linear interpolation method to recompute elevations at fs pixels in depressions if we replace elevations of all sink pixels by the lowest elevation among the boundary pixels some boundary pixels with the same elevation as the lowest elevation will become flat pixels if prior to filling sinks they can only flow to the depression after the sinks are filled these boundary pixels will become flat pixels and their flow directions are not defined the previously defined depression area will expand and its boundary will also change overall filling sinks can cause some previously unconnected fads to merge which will reduce numbers of fads and iterations and ultimately improve the computational efficiency on the other hand filling sinks can also reduce the probability of dead loops or oscillations in the iteration of treating fads because without filling sinks the linear interpolation can change some outlet pixels of depressions that can only flow to the depressions to sink pixels and these sink pixels need to be treated in the next iteration step this will cause a back and forth oscillation in the iteration of treating fads and significantly prolong the running time for completely resolving flat and sink pixels 2 3 experimental testing strategy to test the performance of the mdem ten dem datasets across southeast of south carolina with relatively flat terrain were clipped from three the u s geological survey usgs national elevation database ned 1 arc second about 30 m resolution 1 1 dem tiles n34w082 n34w081 n35w080 table 1 lists dem file name number of columns ncols number of rows nrows x coordinate xllcorner and y coordinate yllcorner of lower left corner and number of fs pixels nfsp of each test dem dataset as shown in table 1 all the test dem data have more than 1800 fs pixels and their nsfps range from 1808 to 12896 both the pdem and mdem were applied to process these ten dem datasets using a macbook pro with a 2 9 ghz intel core i7 processor number of iterations ni for completely treating the fs pixels and associated running time rt of each test dem dataset are also listed in table 1 through comparing nis and rts between the pdem and mdem the improvement of the computational efficiency in the mdem can be quantitively determined 3 results and discussion 3 1 improvement in the computational efficiency the ten dem datasets listed in table 1 were processed by both the pdem and mdem prior to comparing the computational efficiency between the pdem and mdem all the mdem s processed dem datasets were compared with the corresponding pdem s processed dem datasets and it was found that they were identical which indicates that the mdem can achieve the same results as the pdem however the results listed in table 1 clearly show that the mdem is more efficient than the pdem because the number of iterations ni and the running time rt for the mdem are much less than those of the pdem and the percentage decreases in ni and rt range from 87 to 99 and from 89 to 99 respectively based on the number of iterations ni and the running time rt given in table 1 four scatter plots i e nfsp vs nip and nfsp vs rtp for the pdem and nfsp vs nim and nfsp vs rtm for the mdem were plotted in fig 4 all these scatter plots indicate that as nfsp increases both ni and rt increase for both methods fig 4 also demonstrates that including a fill sink step prior to the linear interpolation in the mdem can significantly reduce the number of iterations and running time and thus improve the computational efficiency does this imply that multiple steps of fill sink are necessary prior to the linear interpolation since as the number of fill sink steps increases the number of fads in a dem dataset will decrease and the number of iterations for treating the fads would decrease but the running time does not necessarily decrease because as the number of fads decreases the total number of fs pixels inside fads could increase and the running time would increase therefore there might exist a trade off between the number of fill sink steps and the running time to identify the optimal number of fill sink steps a series of tests with different numbers of fill sink steps were carried out scatter plots of number of fill sink steps versus ni and rt for the mdem as illustrated in fig 5 indicate that as the number of fill sink steps increased the number of iterations decreased but the running time was not reduced by the increase in the number of fill sink steps therefore including only one step of fill sink prior to the linear interpolation is optimal and sufficient 3 2 mdem to demonstrate that all designed functions described in section 2 1 were implemented in the mdem a 1 arc second dem dataset lwwg txt covering the little washita watershed in the southwestern oklahoma was clipped from one usgs ned 1 arc second n35w099 1 1 dem tile downloaded from the u s geological survey usgs national elevation database ned all the matlab scripts and example files i e ten test dem datasets listed in table 1 and lwwg txt for the mdem were zipped in mdem tar that can be downloaded from http www geography unt edu fpan mdem unzip mdem tar and yield a folder called mdem to run the mdem first launch matlab and then in the matlab command window find and open the mdem folder in the mdem folder click app mlapp the interface of the mdem as shown in fig 2 will pop up on the screen in order to calculate slope and flow accumulation area properly using dem data in the geographic projection the mdem first computes the grid cell size in meters at the center of the input dem dataset using the haversine formula van brummelen 2013 and then the computed central grid cell size is used to represent the grid cell size for the entire dem dataset however it should be noted here that the computed slope flow accumulation area and topographic index are subject to errors if the input dem dataset is in the geographic projection because for the dem data in the geographic projection the horizontal distance between two pixels is not constant across the entire domain of the dem and the variation in the pixel distance should be considered if the spatial extent of the dem dataset is not small nevertheless the mdem can handle dem data in both the geographic and utm projections in the input dem textbox type in lwwg txt then click run pdem first the mdem determines the grid cell size of the input dem for the geographic projection the grid cell size is in decimal degree thus the following line cell is 2 777778e 04 in degrees which is about 2 824999e 01 meters is showing in the command window to show the progress of processing dem the iteration information of each step is also displayed in the matlab command window for example the first iteration step shows iteration step 1 number of flat areas 16460 number of flat and sink pixels 16863 after 74 iteration steps all flat and sink pixels are treated and the processed dem is displayed in the matlab figure window fig 6 and saved as p lwwg txt next click flow direction to compute flow direction at each pixel and display the computed flow directions in the matlab figure window fig 7 based on the computed flow directions the flow accumulation area a can be computed by clicking flow accumulation area a button the mdem displays ln a in the matlab figure window fig 8 and the computed a is saved in facc lwwg txt an outlet is required for delineating a watershed the mdem provides three options for inputing the outlet coordinates as described in section 2 1 from the drop down list lon lat is selected for the geographic projection if the little washita river gaging station near ninnekah oklahoma 34 56 41 n 97 57 08 w is chosen as the outlet input 97 952222 in lon east col textbox and 34 944722 in lat north row textbox and click watershed delineation to delineate the little washita river watershed in the matlab command window run watershed delineation is displayed and after it is done the following line is displayed done watershed delineation watershed area 1596 12 m2 watershed mask is saved in wd lwwg txt in the matlab figure window ln a is displayed a green circle centered at the selected outlet is also shown on the figure and the delineated watershed boundary is shown as a red curve generally an outlet should cross a channel while by looking at the figure of ln a we can find that the input outlet is not across the channel which is associated with very high ln a and there is a slight northward shift using matlab data cursor to identify the column and row indexes of the outlet we can find that x 1337 y 197 fig 9 a if we move y downward by 4 pixels i e y 201 and move x left by 1 pixel i e x 1336 we can reach the channel fig 9b now we can use two options to input the correct outlet coordinates i e 1 select col row and input 1336 in lon east col textbox input 201 in lat north row textbox click watershed delineation and yield a watershed with a drainage area of 536873870 m2 207 3 mile2 2 select lon lat input 97 952500 97 952222 0 000278 in lon east col textbox input 34 943610 34 944722 4 0 000278 in lat north row textbox click watershed delineation and yield a watershed with a drainage area of 536872273 m2 207 3 mile2 both options produced an almost identical watershed drainage area which is close to the drainage area of 208 mile2 listed in the little washita river gaging station near ninnekah website https waterdata usgs gov nwis inventory site no 07327490 after the watershed is delineated a red curve representing the watershed boundary is plotted on the ln a image fig 9c and the delineated watershed mask is saved in wd lwwg txt after the watershed is delineated channel networks can be extracted inside the watershed based on the input ln a threshold for example if input 10 in the ln a threshold textbox and click channel networks the extracted channel networks will be shown in blue overlaying on the processed dem image fig 10 and channel network mask is saved in ch lwwg txt after the watershed is delineated the topographic index i e ln a tanb at each pixel inside the delineated watershed can also be computed fig 11 is an image of the computed topographic index which is saved in tpi lwwg txt to properly calculate the topography index the dem dataset should be re projected from the geographic projection to the utm projection using 30 m grid cell size for 1 arc second dem which can be implemented by employing the arcgis raster projection function using the bilinear resampling method using the arcgis data conversion tool to convert the raster file into the ascii format lwwub txt input lwwwub txt in the mdem run the same first three steps i e run pdem flow direction and flow accumulation area to delineate watershed using the dem in the utm projection the online geographic utm tool provided by noaa https www ngs noaa gov cgi bin utm getut prla was used to convert the outlet s latitude and longitude to the utm zone 14 s east and north in meters i e east 595678m north 3867414m it was found that the selected outlet was not across the channel and it needs to be shifted downward by two pixels 60 m in y direction y 3867354 m and eastward by one pixel 30 m in x direction x 595708 m click watershed delineation yield a watershed with a drainage area of 538369200 m2 208 mile2 which is exactly the same as the watershed drainage area listed on the usgs ninnekah gaging station website then channel networks and the topographic index can be computed inside the delineated watershed and displayed in the matlab figure windows figs 12 and 13 and the corresponding files are saved in ch lwwub txt and tpi lwwub txt respectively 4 conclusions and outlook there are several improvements in the mdem compared with the pdem 1 the processing is replaced by the matlab app designer function for building the graphical user interface gui of the mdem 2 visualization of locations of flat and sink pixels at each iteration step is eliminated from the mdem instead the progress of treating flat and sink pixels i e numbers of iteration step flat areas and depressions and flat and sink pixels is displayed in the matlab command window which can improve the mdem computational efficiency 3 in the mdem a fill sink step is applied to each depression prior to applying the linear interpolation to treat flat and sink pixels during each iteration step which can greatly shorten the running time by more than 89 4 in addition to treating flat and sink pixels the mdem also has other five functions i e calculation of flow direction calculation of flow accumulation area delineation of watershed extraction of channel network and calculation of the topographic index although the mdem is more efficient than the pdem processing of flat and sink pixels in some fine resolution e g 10 m or 5 m dem data with relatively flat terrains could still be time consuming to further improve the computational efficiency of the mdem the matlab parallel computing toolbox will be incorporated with the mdem for developing a new version 2 0 of the mdem after the mdem v2 0 is completed it will be posted at the same website as the mdem v1 0 acknowledgement this research was partially funded by the joint research fund for overseas chinese scholars and scholars in hong kong and macao of national natural science foundation of china no 416628101 and the university of north texas scholarly and creative activity sca award f pan 
26129,the commencement of seas5 model for operational seasonal climate forecasting by the european centre for medium range weather forecasts ecmwf is a new development it replaces the ecmwf system 4 which had a large international community of seasonal climate forecast users to assist potential users of seas5 forecasts a systematic and detailed evaluation of forecast skill and reliability of climate variables over land areas is valuable in this regional study we evaluate seas5 performance in forecasting precipitation and daily minimum temperature tmin and daily maximum temperature tmax for the australian continent based on 36 years of re forecast data we evaluate forecasts after simple mean corrections and statistically calibrated forecasts using the bayesian joint probability bjp modelling approach we also provide a comparison with system 4 a new simpler and more efficient bjp algorithm is introduced to facilitate this study and support wider use of the algorithm in other applications keywords forecast calibration and evaluation forecast skill forecast reliability precipitation temperature 1 introduction australia has a highly variable climate skillful reliable and high resolution seasonal climate forecasts from global circulation models gcms are in demand for supporting proactive decision making in many climate sensitive sectors troccoli 2010 the australian bureau of meteorology bom recently upgraded its seasonal climate forecasting model to access s1 and is beginning to develop its next version access s2 hudson et al 2017 a limitation of access s1 is its relatively short re forecast period from 1990 it is planned that access s2 will have a longer re forecast period from 1981 internationally a significant recent development in seasonal climate forecasting is the commencement of seas5 from the european centre for medium range weather forecasts ecmwf seas5 re forecasts are available for 1981 onwards compared to its predecessor system 4 seas5 is a substantially changed forecast system it includes upgraded versions of the atmosphere and ocean models at higher resolutions and adds a prognostic sea ice model johnson et al 2018 reported major improvements in performance from system 4 to seas5 based on some large scale climate and ocean diagnostics emerton et al 2018 demonstrated the value of seas5 in global hydrological seasonal forecasting ecmwf system 4 became operational in 2011 molteni et al 2011 its forecasting performance has been extensively evaluated kim et al 2012 assessed temperature and precipitation forecasts for northern hemisphere winter at the global scale forecasts were found to be the most skillful in the tropics especially in the el nino region weisheimer and palmer 2014 evaluated the reliability of temperature and precipitation forecasts worldwide they assigned ratings from 5 perfect to 1 dangerous to the forecasts for different parts of the world they found that for surface temperature and even more for precipitation forecast probabilities are not reliable when different from climatology and away from the el nino region the commencement of ecmwf seas5 is a new development to assist potential users of seas5 forecasts a systematic and detailed evaluation of forecast skill and reliability of climate variables over land areas is valuable in this regional study we evaluate seas5 performance in forecasting precipitation daily minimum temperature tmin and daily maximum temperature tmax for the australian continent based on 36 years of re forecast data we compare its performance with system 4 there are significant benefits from post processing climate forecasts raw gcm forecasts are biased unreliable in ensemble uncertainty spread too wide or too narrow and often less skillful than the naïve climatology forecasts shukla and lettenmaier 2013 schepen and wang 2014 post processing aims to resolve these problems while extracting the most out of the underlying forecast skill of the gcms zhao et al 2017 examples of statistical post processing methods are gneiting et al 2005 wilks and hamill 2007 unger et al 2009 schepen et al 2014 zhang et al 2017 and narapusetty et al 2018 in this study we apply the bayesian joint probability bjp model wang et al 2009 wang and robertson 2011 to calibrate the seas5 forecasts and evaluate seas5 performance both before and after the calibration the bjp model has been extensively applied to calibrate seasonal forecasts hawthorne et al 2013 peng et al 2014 schepen and wang 2014 schepen et al 2014 2016 2018 bennett et al 2016 2017 zhao et al 2017 2019 strazzo et al 2018 in the bjp model a joint probability distribution is used to characterize the relationship between the raw gcm ensemble means and observations the distribution is subsequently used to produce calibrated forecasts by conditioning the distribution on new raw forecasts while the original bjp model was developed to deal with a number of data issues it is used here primarily for its ability to deal with zero values that can occur with both raw forecasts and observations of precipitation either separately or concurrently see section 3 1 later while the bjp method has worked well in the past the original mathematical formulation is onerous and computer coding for its numerical implementation is demanding in this paper we introduce a new bjp algorithm that is much simplified in its mathematical formulation and more straightforward to code importantly it is computationally much more efficient for large scale applications such as over the australian continent calibration involves a huge number of grid cells and many lead times and repeated applications are required for rigorous cross validation the computation effort is large especially for high resolution models such as the seas5 the development of the new bjp algorithm makes post processing easier and faster 2 data 2 1 gcm forecasts ecmwf system 4 and seas5 ensemble forecast datasets for the australian continent are used in this study system 4 was introduced in november 2011 benefiting from a nemo ocean model and an ocean data assimilation system nemovar it was a fully coupled ocean atmosphere dynamical forecasting system the horizontal resolution of the integrated forecast system ifs atmosphere module was approximately 80 km the era interim reanalysis data provided the ocean module with forcing fluxes and the atmosphere module with unperturbed initial conditions ocean initial conditions were derived from ora s4 for the re forecasts and from the real time nemovar for the operational period perturbed initial conditions and stochastic physics were used to generate ensembles for the re forecasts and operational forecasts seas5 possesses improved atmosphere and ocean model configurations with higher resolutions approximately 36 km in ifs horizontal resolution and includes an interactive sea ice model lim2 the atmosphere initialization of seas5 is the same as system 4 while its ocean and sea ice initial conditions are supported by a new operational ocean analysis system ocean5 zuo et al 2018 this ocean system is composed of oras5 for historical reanalysis and ocean5 rt for daily real time analysis unperturbed and perturbed atmospheric initial conditions and stochastic model perturbations are applied to generate ensemble members the re forecasts and operational forecasts for system 4 were initialized on the 1st of every month and run for 7 months the re forecast period was 1981 2010 with an ensemble size of 15 members the real time forecast period was 2011 2017 with an ensemble size of 51 members seas5 has the same initialization date and forecast horizon as system 4 its re forecast period is 1981 2016 with an ensemble size of 25 members the real time forecasts started in 2017 with an ensemble size of 51 members in this study we evaluate seas5 based on the re forecasts for 1 1 1981 1 12 2016 using 25 ensemble members for comparison we use a combination of re forecasts and operational forecasts of system 4 for the same period only the first 15 ensemble members of the operational forecasts of system 4 are used for consistency with the re forecasts of system 4 target variables of forecasts in this study are monthly and seasonal totals of precipitation and monthly and seasonal averages of tmin and averages of tmax to compare the performance of seas5 with system 4 the seas5 data resolution are re gridded to the coarser grid of system 4 resolution 2 2 observed data monthly and seasonal observation data are obtained from the awap australian water availability project climate datasets of the bureau of meteorology jones et al 2009 the awap analysis datasets provide daily values of climate variables at high spatial resolution 0 05 0 05 for australia based on interpolation of gauge data the data period used in this study is 1 1 1981 30 06 2017 the awap data are re gridded to the grid of system 4 3 methods 3 1 the bayesian joint probability bjp model the ensemble mean of a raw gcm forecast for a climate variable is denoted by y 1 and the corresponding observation by y 2 in mathematical terms we call y 1 a predictor and y 2 a predictand we first apply transformations to normalize the variables to 1 z 1 ψ y 1 δ 2 z 2 ψ y 2 δ where ψ denotes a generic normalizing transformation function and δ is a vector of transformation parameters in this study we apply the log sinh transformation to precipitation variables and the yeo johnson transformation to temperature variables yeo and johnson 2000 wang et al 2012 we estimate a single best set of transformation parameter values for each variable by using a bayesian maximum a posteriori map solution schepen et al 2016 next we assume the transformed predictor and predictand to follow a bivariate normal joint distribution 3 z z 1 z 2 n μ σ where μ and σ are the mean vector and covariance matrix respectively we denote the parameter set as θ μ σ the bivariate normal distribution applies to continuous variables but in case of precipitation variables z 1 and z 2 can be subject to a lower threshold of z 1 c and z 2 c corresponding to zero forecast precipitation and zero observed precipitation to overcome this problem we treat a threshold data value as a censored data record with an unknown exact value that is equal to or below the threshold wang and robertson 2011 given a data series d z t t 1 2 n we use bayesian inference to find the posterior distribution of the model parameters 4 p θ d p θ p d θ where p θ is the prior distribution of the parameters and p d θ is the likelihood by setting an appropriate prior distribution it is possible to formulate gibbs sampling to numerically sample p θ d once representative parameter samples are available the bjp model can be used in predictive mode to calibrate a new forecast y 1 t or z 1 t after transformation the posterior predictive distribution of z 2 t is given by 5 f z 2 t p z 2 t z 1 t θ p θ d d θ again gibbs sampling can be formulated to numerically sample f z 2 t by back transforming each of the sample members we produce a calibrated ensemble forecast of y 2 t the derivation and implementation of the gibbs sampling algorithm including pseudo code are given in appendix a compared with the original algorithm of the bjp model wang et al 2009 wang and robertson 2011 the new algorithm is greatly simplified mathematically and more straightforward to code importantly it is computationally much more efficient on rare occasions we find the calibrated forecasts for precipitation can be unrealistically large the problem is caused by extremely large raw forecast values from the gcm models so large that the transformed forecasts are considered improbable according to the marginal distribution of the raw forecasts we use a pragmatic approach here to circumvent this problem given the marginal distribution n z 1 μ 1 σ 1 2 for z 1 we check if the following is true 6 φ z 1 t μ 1 σ 1 2 p extreme where φ is the cumulative distribution function of the marginal distribution and p extreme is a high non exceedance probability to indicate an extreme threshold if true we set z 1 t back to the extreme threshold 7 z 1 t φ 1 p extreme μ 1 σ 1 2 where φ 1 is the inverse cumulative distribution function of the marginal distribution we then derive the calibrated forecast for y 2 t by conditioning z 2 t on this adjusted z 1 t in this study we set p extreme 0 999 3 2 forecast evaluation we evaluate forecasts for the following target variables 1 monthly total of precipitation monthly average of tmin and monthly average of tmax at zero lead time i e forecast at the start of a month for that month 2 seasonal 3 month total of precipitation seasonal average of tmin and seasonal average of tmax at 1 month lead time i e forecast for a rolling season beginning in 1 month s time 3 seasonal total of precipitation seasonal average of tmin and seasonal average of tmax at 4 month lead time i e forecast for a rolling season beginning in 4 months time the shorter target period of 1 is chosen because the forecasts for the first month ahead are the most skillful the longer target periods of 2 and 3 are for covering the rest of the gcm forecast horizon with just two sets of evaluation the evaluation is made at each of the 1348 grid cells as in system 4 covering australia we evaluate both mean corrected and bjp calibrated forecasts of seas5 and system 4 as raw forecasts are sometimes applied without any bias correction we additionally evaluate raw forecasts to confirm that it is not a good practice to directly use raw forecasts to produce the mean corrected forecasts we apply a simple multiplicative mean correction to precipitation and an additive mean correction to temperatures formulas for these simple mean corrections are given in appendix b the mean corrections are made separately for individual grid cells and for individual months or seasons of the year using a leave one year out cross validation setup to produce calibrated forecasts for each of the target variables we also establish and apply bjp models separately for individual grid cells and for individual months or seasons of the year using a leave one year out cross validation setup we use the continuous ranked probability score crps as a measure of forecast error corresponding to the difference between an ensemble forecast and its corresponding observation hersbach 2000 a brief description of crps is given in appendix c for crps to be low ensemble forecasts need to be both accurate and have the right spread not too narrow or too wide we also evaluate crps for climatology ensemble forecasts see appendix a for more details about climatology ensemble forecasts this second crps is used as a reference so that percentage of reduction in average crps from the reference can be calculated to give a crps skill score see appendix c a skill score of zero indicates that the forecasts are only as good as the reference forecasts a negative positive skill score indicates that the forecasts are poorer better than the reference forecasts forecasts can reach a skill score of 100 when perfectly matching observations we use the pit probability integral transform histograms to check ensemble forecast reliability for example whether forecasts are biased too wide or too narrow in ensemble spread as a measure of forecast uncertainty a brief description of pit histogram is given in appendix c further details on pit calculation including how to deal with forecasts with censor threshold values can be found in wang et al 2009 and wang and robertson 2011 further details on using pit histograms as a diagnostic tool can be found in gneiting et al 2007 the pit values for all grid cells and months or seasons are pooled together to construct the pit histograms 4 results 4 1 skill of monthly total or average forecasts with zero lead time precipitation results of crps skill score for mean corrected and bjp calibrated forecasts based on seas5 are shown in fig 1 the mean corrected forecasts generally have positive crps skill score over most of australia the high skill area changes with season there are small pockets of negative skill these pockets are mostly removed after the bjp calibration with climatology like ensemble forecasts taking over resulting in near zero skill score the widespread positive skill of the mean corrected forecasts is largely retained after the calibration although there is a slight reduction in skill in some areas the reduction in skill is likely caused by the reliability improvement due to the bjp calibration see later section 4 3 wilks 2018 pointed out that the crps may reward forecasts that are narrower at the expense of reliability in ensemble spread it is also possible that the training of a sophisticated calibration model may introduce error lowering the skill score under cross validation focusing on the bjp calibrated forecasts the average skill score is the highest for march and october the high skill area is in the north and north central australia for march and in south central australia in october initial conditions of atmosphere land and ocean are all expected to influence the climate in the immediate month ahead results of crps skill score difference between seas5 and system 4 are shown in fig 2 seas5 is overall more skillful than system 4 the difference is more pronounced with the mean corrected forecasts the bjp calibration reduces the skill gap between the two forecasting systems tmin and tmax results of crps skill score for monthly average of tmin forecasts based on seas5 are shown in fig 3 both the mean corrected forecasts and calibrated forecasts are more skillful overall than forecasts for precipitation the mean corrected forecasts have larger but fewer pockets of negative skill than forecasts for precipitation the negative skill score can be below 30 again the bjp calibration successfully removes the pockets of negative skill the average skill score of the bjp calibrated forecasts is the highest for march and october as with precipitation from the plots shown in fig 4 of the skill difference between the two forecasting systems seas5 shows only a small overall improvement with visible gain for some months but loss for some other months after the bjp calibration the difference between the two systems is overall unnoticeable but variation with forecast month remains obvious for example gain for december and loss for june results of evaluation of forecasts for tmax are shown in supplementary material fig s1 and fig s2 the crps skill score tends to be much higher than for tmin forecasts seas5 shows remarkable improvement over system 4 the average skill score of the bjp calibrated forecasts is the highest for october september march and april 4 2 skill of seasonal total or average forecasts with 1 month lead time precipitation results of crps skill score for forecasts based on seas5 are shown in fig 5 the skill score is low when compared with the monthly forecasts with zero lead time fig 1 again the bjp calibration removes the negative skill seen in the mean corrected forecasts the spatial pattern of areas of positive skill score of the bjp calibrated forecasts is remarkably consistent with the statistical evidence presented by schepen et al 2012 figure 14 for using lagged climate indices to forecast seasonal rainfall over australia this consistency suggests that the influence of large global circulations on seasonal rainfall over australia one month ahead is 1 well modelled by seas5 and 2 responsible for nearly all the forecast skill from the skill score difference between the two forecasting systems shown in fig 6 seas5 shows improvement in mean corrected forecasts over system 4 in most months but the improvement is unnoticeable after the bjp calibration tmin and tmax results of crps skill score are shown in fig 7 for tmin forecasts and in fig s3 for tmax forecasts again the bjp calibration removes the negative skill seen in the mean corrected forecasts the skill score is overall much lower than the monthly forecasts with zero lead time fig 3 and fig s1 forecasts for tmax are more skillful than for tmin and both are much better than for precipitation fig 5 from the skill score difference for tmin between the two forecasting systems as shown in fig 8 seas5 shows overall improvement in mean corrected forecasts over system 4 with visible gain for some seasons but loss for some other seasons the improvement is less noticeable after the bjp calibration but variation with forecast season remains obvious for example gain for nov dec jan and loss for may jun jul for tmax the improvement is more pronounced and can be clearly seen even after the bjp calibration fig s4 4 3 reliability of ensemble forecasts pit histograms are shown in fig 9 for precipitation and in fig 10 for tmin the mean corrected monthly forecasts of both seas5 and system 4 show a u shape indicating that the forecast ensemble spread is too narrow the same can be said about the mean corrected seasonal forecasts but to a lesser extent especially for precipitation from results for tmax shown in supplementary material fig s5 the mean corrected forecasts of both seas5 and system 4 are more reliable than tmin in ensemble spread especially the seasonal forecasts in all cases the bjp calibrated forecasts show a reasonably uniform shape indicating that the calibrated forecast ensemble spread is reliable 4 4 other results results of crps skill score for mean corrected and bjp calibrated seasonal forecasts based on seas5 at 4 month lead time are provided in supplementary material figs s6 s8 compared with the seasonal forecasts at 1 month lead time the spatial extent and magnitude of positive skill score are reduced considerably as before among the three forecast variables forecasts for tmax are the most skillful and forecasts for precipitation are the least skillful raw seas5 forecasts for all variables and lead times are found to be poor often with large bias and highly negative skill score especially for tmin and tmax results not shown the forecasts tend to be too high for tmin and too low for tmax we strongly discourage the direct use of raw forecasts 5 conclusions for precipitation tmin and tmax over the australian continent seas5 offers considerable skill for monthly forecasts with zero lead time the skill is the highest for tmax and the lowest for precipitation compared with system 4 seas5 shows very large improvement to forecasts for tmax large improvement to precipitation but little improvement to tmin overall we suggest that the seas5 monthly forecasts at zero lead time that is forecasts at the start of a month for that month can be valuable for applications in australia however the skill varies across australia with high skill areas changing with time of the year for precipitation and tmin the average skill score is the highest for march and october for tmax the average skill score is the highest for october september march and april seas5 forecast skill drops significantly beyond one month forecast horizon the seasonal forecasts with 1 month lead time are low in skill for precipitation and very patchy for temperatures this is despite some improvement over system 4 especially for tmax overall we suggest that seas5 temperature forecasts for beyond one month forecast horizon can be useful for only small parts of australia but precipitation forecasts offer little value interestingly the spatial pattern of areas of positive skill score of the bjp calibrated forecasts of seasonal precipitation total with 1 month lead time is remarkably consistent with the statistical evidence presented by schepen et al 2012 figure 14 for using lagged climate indices to forecast seasonal rainfall over australia this consistency suggests that the influence of large global circulations on seasonal rainfall over australia one month ahead is 1 well modelled by seas5 and 2 responsible for nearly all the forecast skill the mean corrected seas5 forecasts are found to be unreliable in ensemble spread generally too narrow in other words the forecasts are more confident than warranted this is especially so for the monthly forecasts with zero lead time seas5 does not appear to show any improvement over system 4 in terms of reliability for australian precipitation and temperatures another issue is that the mean corrected seas5 forecasts are found to be less skillful than climatology ensemble forecasts for some locations and times of the year the bjp calibration is found to be effective in making forecasts reliable and ensuring that forecasts are not worse than climatology ensemble forecasts while retaining the positive skill of the gcm forecasts we suggest that the calibrated ensemble forecasts are better forecasts for practical applications by contrast raw seas5 forecasts for all variables and lead times are found to be poor often with large bias and highly negative skill score we strongly discourage the direct use of raw forecasts the new bjp algorithm introduced in this paper is much simplified mathematically and more straightforward to code up than the original bjp model we find it highly efficient computationally in our post processing of seas5 and system 4 forecasts for the very large number grid cells across the australia continent and the leave one year out cross validation of 36 years of forecast data software availability pseudo computer code is given in appendix a r and python packages can be made available on request acknowledgements we thank the european centre for medium range weather forecasts ecmwf for providing the seas5 and system 4 data the data for seas5 can be obtained through the copernicus climate data store cds climate copernicus eu florian pappenberger acknowledges funding by the copernicus program which supported the production of seas5 hindcasts we thank the bureau of meteorology for awap data used in this study appendix d supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix d supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104550 appendix a the bayesian joint probability model parameter inference and predictive use via gibbs sampling in section 3 1 we have presented the bjp model for a bivariate problem the original bjp model wang et al 2009 wang and robertson 2011 was formulated as a multivariate problem allowing for multiple predictors and multiple predictands in this appendix we also present the bjp model in a general multivariate formulation of which the bivariate case is a special one a 1 the model consider d variables composed of both predictands and predictors with n number of data records a 1 z t z 1 t z 2 t z d t t where t 1 2 n z t is a column vector and t is for transpose we assume that the individual variables are normally distributed or have been normalized through transformation and z t is a sample drawn from a multivariate normal distribution a 2 z t n μ σ where μ is a d 1 mean vector and σ a d d covariance matrix we assume that the variables may sometimes have missing values or left censored values for example z i t z i c a 2 parameter inference the posterior distribution of the model parameters is according to bayes theorem a 3 p μ σ d p μ σ p d μ σ p μ σ t 1 n p z t μ σ where d z t t 1 2 n p μ σ is a prior distribution of the model parameters and p d μ σ is the likelihood we apply the non informative multivariate jeffreys prior gelman et al 2014 a 4 p μ σ σ d 1 2 by combining eq a 2 eq a 4 we can derive following conditional distributions for the parameters gelman et al 2014 a 5 σ inv wishart n 1 s a 6 μ n z σ n where a 7 s t 1 n z t z z t z t a 8 z 1 n t 1 n z t the symbol means conditional on all other variables and inv wishart n 1 is the inverse wishart distribution with n 1 degrees of freedom when one of variables z i t is missing a value we can also derive its conditional distribution a 9 z i t n μ i t σ i i where a 10 σ i i σ i i σ i i σ i i 1 σ i i a 11 μ i t μ i σ i i σ i i 1 z i t μ i with i denoting all index entries in 1 2 d except i the conditional distributions of eq a 5 eq a 11 are used to set up gibbs sampling to sequentially draw one sample from each of the conditional distributions and repeat many cycles of such sampling this yields a parameter chain which when converged forms multiple parameter sets to numerically represent the joint posterior distribution of the parameters and the missing values note that when z i t has a censored value instead of a missing value we can also use the conditional distribution of eq a 9 to sample a value we just need to restrict the sampling to the range of z i t z i c here we include pseudo code for implementing the gibbs sampling for parameter inference image 1 a 3 prediction once samples of the posterior distribution of the parameters are available we can apply the model for prediction the vector z t z 1 t z 2 t z d t t consists of both predictor variables and predictand variables in a prediction we want to find the posterior distribution of the predictands given information about the predictors sampling of this distribution can be conveniently done by treating the predictand variables as having missing values and applying the gibbs sampling step as outlined in eq a 9 eq a 11 in fact some of the predictors can also have missing or censored values and be included in the gibbs sampling for variables with censored values we need to restrict the sampling to the range of z i t z i c as discussed in section a 2 here we include pseudo code for implementing the gibbs sampling for prediction image 2 a 4 generating reference predictions sometimes it is useful to generate predictions using a naïve model to provide a reference for evaluating predictions from using a more advanced model one such naïve model is simply based on the distribution of historical observations of the predictands recall the full joint probability model of eq a 2 we can reduce the model to a subset a 12 z t n μ σ where z t consists of only the predictand variables given the parameter samples already obtained through gibbs sampling in section a 2 we can generate samples of z t by sampling the distribution in eq a 12 these samples can be used as ensemble reference predictions here we include pseudo code for implementing the gibbs sampling for generating reference predictions image 3 appendix b simple bias corrections for precipitation we use a multiplicative bias correction b 1 y 2 t t 1 n y 2 t t 1 n y 1 t y 1 t for temperature we use an additive bias correction b 2 y 2 t y 1 t 1 n t 1 n y 2 t 1 n t 1 n y 1 t where y 2 t denotes observed data y 1 t denotes raw ensemble forecast mean n is the number of events used in model fitting y 1 t is a new forecast ensemble member and y 2 t is a bias corrected forecast ensemble member the bias correction is applied to all members appendix c crps skill score and pit histogram the continuous ranked probability score crps hersbach 2000 is adopted to evaluate the forecast skill the average crps for time steps t 1 2 n is formulated as c 1 c r p s 1 n t 1 n f t x h x y t 2 d x where f t x is the cumulative distribution function cdf of an ensemble forecast at time t for variable x y t is the observed value and h is the heaviside step function which equals 0 if x y t and equals 1 otherwise crps is also calculated for a set of reference forecasts see section 3 2 a skill score is then computed as c 2 c r p s s k i l l s c o r e c r p s r e f c r p s c r p s r e f 100 reliability of the ensemble forecasts is evaluated by calculating the forecast probability integral transform pit values and plotting the probability integral transform pit histogram gneiting et al 2007 for each pair of forecast and observation the pit value is calculated by p i t t f t y t y t 0 a f t 0 y t 0 where a is randomly sampled from the uniform distribution 0 1 wang and robertson 2011 theoretically if the probabilistic forecasts are perfectly reliable the collection of pit values should follow a standard uniform distribution in this study the pit histogram is generated by pooling the pairwise forecasts and observations for all grids and all months or seasons the reliability can be examined by inspecting the uniformity of the pit histogram the more uniform the histogram is the more reliable the forecasts are 
26129,the commencement of seas5 model for operational seasonal climate forecasting by the european centre for medium range weather forecasts ecmwf is a new development it replaces the ecmwf system 4 which had a large international community of seasonal climate forecast users to assist potential users of seas5 forecasts a systematic and detailed evaluation of forecast skill and reliability of climate variables over land areas is valuable in this regional study we evaluate seas5 performance in forecasting precipitation and daily minimum temperature tmin and daily maximum temperature tmax for the australian continent based on 36 years of re forecast data we evaluate forecasts after simple mean corrections and statistically calibrated forecasts using the bayesian joint probability bjp modelling approach we also provide a comparison with system 4 a new simpler and more efficient bjp algorithm is introduced to facilitate this study and support wider use of the algorithm in other applications keywords forecast calibration and evaluation forecast skill forecast reliability precipitation temperature 1 introduction australia has a highly variable climate skillful reliable and high resolution seasonal climate forecasts from global circulation models gcms are in demand for supporting proactive decision making in many climate sensitive sectors troccoli 2010 the australian bureau of meteorology bom recently upgraded its seasonal climate forecasting model to access s1 and is beginning to develop its next version access s2 hudson et al 2017 a limitation of access s1 is its relatively short re forecast period from 1990 it is planned that access s2 will have a longer re forecast period from 1981 internationally a significant recent development in seasonal climate forecasting is the commencement of seas5 from the european centre for medium range weather forecasts ecmwf seas5 re forecasts are available for 1981 onwards compared to its predecessor system 4 seas5 is a substantially changed forecast system it includes upgraded versions of the atmosphere and ocean models at higher resolutions and adds a prognostic sea ice model johnson et al 2018 reported major improvements in performance from system 4 to seas5 based on some large scale climate and ocean diagnostics emerton et al 2018 demonstrated the value of seas5 in global hydrological seasonal forecasting ecmwf system 4 became operational in 2011 molteni et al 2011 its forecasting performance has been extensively evaluated kim et al 2012 assessed temperature and precipitation forecasts for northern hemisphere winter at the global scale forecasts were found to be the most skillful in the tropics especially in the el nino region weisheimer and palmer 2014 evaluated the reliability of temperature and precipitation forecasts worldwide they assigned ratings from 5 perfect to 1 dangerous to the forecasts for different parts of the world they found that for surface temperature and even more for precipitation forecast probabilities are not reliable when different from climatology and away from the el nino region the commencement of ecmwf seas5 is a new development to assist potential users of seas5 forecasts a systematic and detailed evaluation of forecast skill and reliability of climate variables over land areas is valuable in this regional study we evaluate seas5 performance in forecasting precipitation daily minimum temperature tmin and daily maximum temperature tmax for the australian continent based on 36 years of re forecast data we compare its performance with system 4 there are significant benefits from post processing climate forecasts raw gcm forecasts are biased unreliable in ensemble uncertainty spread too wide or too narrow and often less skillful than the naïve climatology forecasts shukla and lettenmaier 2013 schepen and wang 2014 post processing aims to resolve these problems while extracting the most out of the underlying forecast skill of the gcms zhao et al 2017 examples of statistical post processing methods are gneiting et al 2005 wilks and hamill 2007 unger et al 2009 schepen et al 2014 zhang et al 2017 and narapusetty et al 2018 in this study we apply the bayesian joint probability bjp model wang et al 2009 wang and robertson 2011 to calibrate the seas5 forecasts and evaluate seas5 performance both before and after the calibration the bjp model has been extensively applied to calibrate seasonal forecasts hawthorne et al 2013 peng et al 2014 schepen and wang 2014 schepen et al 2014 2016 2018 bennett et al 2016 2017 zhao et al 2017 2019 strazzo et al 2018 in the bjp model a joint probability distribution is used to characterize the relationship between the raw gcm ensemble means and observations the distribution is subsequently used to produce calibrated forecasts by conditioning the distribution on new raw forecasts while the original bjp model was developed to deal with a number of data issues it is used here primarily for its ability to deal with zero values that can occur with both raw forecasts and observations of precipitation either separately or concurrently see section 3 1 later while the bjp method has worked well in the past the original mathematical formulation is onerous and computer coding for its numerical implementation is demanding in this paper we introduce a new bjp algorithm that is much simplified in its mathematical formulation and more straightforward to code importantly it is computationally much more efficient for large scale applications such as over the australian continent calibration involves a huge number of grid cells and many lead times and repeated applications are required for rigorous cross validation the computation effort is large especially for high resolution models such as the seas5 the development of the new bjp algorithm makes post processing easier and faster 2 data 2 1 gcm forecasts ecmwf system 4 and seas5 ensemble forecast datasets for the australian continent are used in this study system 4 was introduced in november 2011 benefiting from a nemo ocean model and an ocean data assimilation system nemovar it was a fully coupled ocean atmosphere dynamical forecasting system the horizontal resolution of the integrated forecast system ifs atmosphere module was approximately 80 km the era interim reanalysis data provided the ocean module with forcing fluxes and the atmosphere module with unperturbed initial conditions ocean initial conditions were derived from ora s4 for the re forecasts and from the real time nemovar for the operational period perturbed initial conditions and stochastic physics were used to generate ensembles for the re forecasts and operational forecasts seas5 possesses improved atmosphere and ocean model configurations with higher resolutions approximately 36 km in ifs horizontal resolution and includes an interactive sea ice model lim2 the atmosphere initialization of seas5 is the same as system 4 while its ocean and sea ice initial conditions are supported by a new operational ocean analysis system ocean5 zuo et al 2018 this ocean system is composed of oras5 for historical reanalysis and ocean5 rt for daily real time analysis unperturbed and perturbed atmospheric initial conditions and stochastic model perturbations are applied to generate ensemble members the re forecasts and operational forecasts for system 4 were initialized on the 1st of every month and run for 7 months the re forecast period was 1981 2010 with an ensemble size of 15 members the real time forecast period was 2011 2017 with an ensemble size of 51 members seas5 has the same initialization date and forecast horizon as system 4 its re forecast period is 1981 2016 with an ensemble size of 25 members the real time forecasts started in 2017 with an ensemble size of 51 members in this study we evaluate seas5 based on the re forecasts for 1 1 1981 1 12 2016 using 25 ensemble members for comparison we use a combination of re forecasts and operational forecasts of system 4 for the same period only the first 15 ensemble members of the operational forecasts of system 4 are used for consistency with the re forecasts of system 4 target variables of forecasts in this study are monthly and seasonal totals of precipitation and monthly and seasonal averages of tmin and averages of tmax to compare the performance of seas5 with system 4 the seas5 data resolution are re gridded to the coarser grid of system 4 resolution 2 2 observed data monthly and seasonal observation data are obtained from the awap australian water availability project climate datasets of the bureau of meteorology jones et al 2009 the awap analysis datasets provide daily values of climate variables at high spatial resolution 0 05 0 05 for australia based on interpolation of gauge data the data period used in this study is 1 1 1981 30 06 2017 the awap data are re gridded to the grid of system 4 3 methods 3 1 the bayesian joint probability bjp model the ensemble mean of a raw gcm forecast for a climate variable is denoted by y 1 and the corresponding observation by y 2 in mathematical terms we call y 1 a predictor and y 2 a predictand we first apply transformations to normalize the variables to 1 z 1 ψ y 1 δ 2 z 2 ψ y 2 δ where ψ denotes a generic normalizing transformation function and δ is a vector of transformation parameters in this study we apply the log sinh transformation to precipitation variables and the yeo johnson transformation to temperature variables yeo and johnson 2000 wang et al 2012 we estimate a single best set of transformation parameter values for each variable by using a bayesian maximum a posteriori map solution schepen et al 2016 next we assume the transformed predictor and predictand to follow a bivariate normal joint distribution 3 z z 1 z 2 n μ σ where μ and σ are the mean vector and covariance matrix respectively we denote the parameter set as θ μ σ the bivariate normal distribution applies to continuous variables but in case of precipitation variables z 1 and z 2 can be subject to a lower threshold of z 1 c and z 2 c corresponding to zero forecast precipitation and zero observed precipitation to overcome this problem we treat a threshold data value as a censored data record with an unknown exact value that is equal to or below the threshold wang and robertson 2011 given a data series d z t t 1 2 n we use bayesian inference to find the posterior distribution of the model parameters 4 p θ d p θ p d θ where p θ is the prior distribution of the parameters and p d θ is the likelihood by setting an appropriate prior distribution it is possible to formulate gibbs sampling to numerically sample p θ d once representative parameter samples are available the bjp model can be used in predictive mode to calibrate a new forecast y 1 t or z 1 t after transformation the posterior predictive distribution of z 2 t is given by 5 f z 2 t p z 2 t z 1 t θ p θ d d θ again gibbs sampling can be formulated to numerically sample f z 2 t by back transforming each of the sample members we produce a calibrated ensemble forecast of y 2 t the derivation and implementation of the gibbs sampling algorithm including pseudo code are given in appendix a compared with the original algorithm of the bjp model wang et al 2009 wang and robertson 2011 the new algorithm is greatly simplified mathematically and more straightforward to code importantly it is computationally much more efficient on rare occasions we find the calibrated forecasts for precipitation can be unrealistically large the problem is caused by extremely large raw forecast values from the gcm models so large that the transformed forecasts are considered improbable according to the marginal distribution of the raw forecasts we use a pragmatic approach here to circumvent this problem given the marginal distribution n z 1 μ 1 σ 1 2 for z 1 we check if the following is true 6 φ z 1 t μ 1 σ 1 2 p extreme where φ is the cumulative distribution function of the marginal distribution and p extreme is a high non exceedance probability to indicate an extreme threshold if true we set z 1 t back to the extreme threshold 7 z 1 t φ 1 p extreme μ 1 σ 1 2 where φ 1 is the inverse cumulative distribution function of the marginal distribution we then derive the calibrated forecast for y 2 t by conditioning z 2 t on this adjusted z 1 t in this study we set p extreme 0 999 3 2 forecast evaluation we evaluate forecasts for the following target variables 1 monthly total of precipitation monthly average of tmin and monthly average of tmax at zero lead time i e forecast at the start of a month for that month 2 seasonal 3 month total of precipitation seasonal average of tmin and seasonal average of tmax at 1 month lead time i e forecast for a rolling season beginning in 1 month s time 3 seasonal total of precipitation seasonal average of tmin and seasonal average of tmax at 4 month lead time i e forecast for a rolling season beginning in 4 months time the shorter target period of 1 is chosen because the forecasts for the first month ahead are the most skillful the longer target periods of 2 and 3 are for covering the rest of the gcm forecast horizon with just two sets of evaluation the evaluation is made at each of the 1348 grid cells as in system 4 covering australia we evaluate both mean corrected and bjp calibrated forecasts of seas5 and system 4 as raw forecasts are sometimes applied without any bias correction we additionally evaluate raw forecasts to confirm that it is not a good practice to directly use raw forecasts to produce the mean corrected forecasts we apply a simple multiplicative mean correction to precipitation and an additive mean correction to temperatures formulas for these simple mean corrections are given in appendix b the mean corrections are made separately for individual grid cells and for individual months or seasons of the year using a leave one year out cross validation setup to produce calibrated forecasts for each of the target variables we also establish and apply bjp models separately for individual grid cells and for individual months or seasons of the year using a leave one year out cross validation setup we use the continuous ranked probability score crps as a measure of forecast error corresponding to the difference between an ensemble forecast and its corresponding observation hersbach 2000 a brief description of crps is given in appendix c for crps to be low ensemble forecasts need to be both accurate and have the right spread not too narrow or too wide we also evaluate crps for climatology ensemble forecasts see appendix a for more details about climatology ensemble forecasts this second crps is used as a reference so that percentage of reduction in average crps from the reference can be calculated to give a crps skill score see appendix c a skill score of zero indicates that the forecasts are only as good as the reference forecasts a negative positive skill score indicates that the forecasts are poorer better than the reference forecasts forecasts can reach a skill score of 100 when perfectly matching observations we use the pit probability integral transform histograms to check ensemble forecast reliability for example whether forecasts are biased too wide or too narrow in ensemble spread as a measure of forecast uncertainty a brief description of pit histogram is given in appendix c further details on pit calculation including how to deal with forecasts with censor threshold values can be found in wang et al 2009 and wang and robertson 2011 further details on using pit histograms as a diagnostic tool can be found in gneiting et al 2007 the pit values for all grid cells and months or seasons are pooled together to construct the pit histograms 4 results 4 1 skill of monthly total or average forecasts with zero lead time precipitation results of crps skill score for mean corrected and bjp calibrated forecasts based on seas5 are shown in fig 1 the mean corrected forecasts generally have positive crps skill score over most of australia the high skill area changes with season there are small pockets of negative skill these pockets are mostly removed after the bjp calibration with climatology like ensemble forecasts taking over resulting in near zero skill score the widespread positive skill of the mean corrected forecasts is largely retained after the calibration although there is a slight reduction in skill in some areas the reduction in skill is likely caused by the reliability improvement due to the bjp calibration see later section 4 3 wilks 2018 pointed out that the crps may reward forecasts that are narrower at the expense of reliability in ensemble spread it is also possible that the training of a sophisticated calibration model may introduce error lowering the skill score under cross validation focusing on the bjp calibrated forecasts the average skill score is the highest for march and october the high skill area is in the north and north central australia for march and in south central australia in october initial conditions of atmosphere land and ocean are all expected to influence the climate in the immediate month ahead results of crps skill score difference between seas5 and system 4 are shown in fig 2 seas5 is overall more skillful than system 4 the difference is more pronounced with the mean corrected forecasts the bjp calibration reduces the skill gap between the two forecasting systems tmin and tmax results of crps skill score for monthly average of tmin forecasts based on seas5 are shown in fig 3 both the mean corrected forecasts and calibrated forecasts are more skillful overall than forecasts for precipitation the mean corrected forecasts have larger but fewer pockets of negative skill than forecasts for precipitation the negative skill score can be below 30 again the bjp calibration successfully removes the pockets of negative skill the average skill score of the bjp calibrated forecasts is the highest for march and october as with precipitation from the plots shown in fig 4 of the skill difference between the two forecasting systems seas5 shows only a small overall improvement with visible gain for some months but loss for some other months after the bjp calibration the difference between the two systems is overall unnoticeable but variation with forecast month remains obvious for example gain for december and loss for june results of evaluation of forecasts for tmax are shown in supplementary material fig s1 and fig s2 the crps skill score tends to be much higher than for tmin forecasts seas5 shows remarkable improvement over system 4 the average skill score of the bjp calibrated forecasts is the highest for october september march and april 4 2 skill of seasonal total or average forecasts with 1 month lead time precipitation results of crps skill score for forecasts based on seas5 are shown in fig 5 the skill score is low when compared with the monthly forecasts with zero lead time fig 1 again the bjp calibration removes the negative skill seen in the mean corrected forecasts the spatial pattern of areas of positive skill score of the bjp calibrated forecasts is remarkably consistent with the statistical evidence presented by schepen et al 2012 figure 14 for using lagged climate indices to forecast seasonal rainfall over australia this consistency suggests that the influence of large global circulations on seasonal rainfall over australia one month ahead is 1 well modelled by seas5 and 2 responsible for nearly all the forecast skill from the skill score difference between the two forecasting systems shown in fig 6 seas5 shows improvement in mean corrected forecasts over system 4 in most months but the improvement is unnoticeable after the bjp calibration tmin and tmax results of crps skill score are shown in fig 7 for tmin forecasts and in fig s3 for tmax forecasts again the bjp calibration removes the negative skill seen in the mean corrected forecasts the skill score is overall much lower than the monthly forecasts with zero lead time fig 3 and fig s1 forecasts for tmax are more skillful than for tmin and both are much better than for precipitation fig 5 from the skill score difference for tmin between the two forecasting systems as shown in fig 8 seas5 shows overall improvement in mean corrected forecasts over system 4 with visible gain for some seasons but loss for some other seasons the improvement is less noticeable after the bjp calibration but variation with forecast season remains obvious for example gain for nov dec jan and loss for may jun jul for tmax the improvement is more pronounced and can be clearly seen even after the bjp calibration fig s4 4 3 reliability of ensemble forecasts pit histograms are shown in fig 9 for precipitation and in fig 10 for tmin the mean corrected monthly forecasts of both seas5 and system 4 show a u shape indicating that the forecast ensemble spread is too narrow the same can be said about the mean corrected seasonal forecasts but to a lesser extent especially for precipitation from results for tmax shown in supplementary material fig s5 the mean corrected forecasts of both seas5 and system 4 are more reliable than tmin in ensemble spread especially the seasonal forecasts in all cases the bjp calibrated forecasts show a reasonably uniform shape indicating that the calibrated forecast ensemble spread is reliable 4 4 other results results of crps skill score for mean corrected and bjp calibrated seasonal forecasts based on seas5 at 4 month lead time are provided in supplementary material figs s6 s8 compared with the seasonal forecasts at 1 month lead time the spatial extent and magnitude of positive skill score are reduced considerably as before among the three forecast variables forecasts for tmax are the most skillful and forecasts for precipitation are the least skillful raw seas5 forecasts for all variables and lead times are found to be poor often with large bias and highly negative skill score especially for tmin and tmax results not shown the forecasts tend to be too high for tmin and too low for tmax we strongly discourage the direct use of raw forecasts 5 conclusions for precipitation tmin and tmax over the australian continent seas5 offers considerable skill for monthly forecasts with zero lead time the skill is the highest for tmax and the lowest for precipitation compared with system 4 seas5 shows very large improvement to forecasts for tmax large improvement to precipitation but little improvement to tmin overall we suggest that the seas5 monthly forecasts at zero lead time that is forecasts at the start of a month for that month can be valuable for applications in australia however the skill varies across australia with high skill areas changing with time of the year for precipitation and tmin the average skill score is the highest for march and october for tmax the average skill score is the highest for october september march and april seas5 forecast skill drops significantly beyond one month forecast horizon the seasonal forecasts with 1 month lead time are low in skill for precipitation and very patchy for temperatures this is despite some improvement over system 4 especially for tmax overall we suggest that seas5 temperature forecasts for beyond one month forecast horizon can be useful for only small parts of australia but precipitation forecasts offer little value interestingly the spatial pattern of areas of positive skill score of the bjp calibrated forecasts of seasonal precipitation total with 1 month lead time is remarkably consistent with the statistical evidence presented by schepen et al 2012 figure 14 for using lagged climate indices to forecast seasonal rainfall over australia this consistency suggests that the influence of large global circulations on seasonal rainfall over australia one month ahead is 1 well modelled by seas5 and 2 responsible for nearly all the forecast skill the mean corrected seas5 forecasts are found to be unreliable in ensemble spread generally too narrow in other words the forecasts are more confident than warranted this is especially so for the monthly forecasts with zero lead time seas5 does not appear to show any improvement over system 4 in terms of reliability for australian precipitation and temperatures another issue is that the mean corrected seas5 forecasts are found to be less skillful than climatology ensemble forecasts for some locations and times of the year the bjp calibration is found to be effective in making forecasts reliable and ensuring that forecasts are not worse than climatology ensemble forecasts while retaining the positive skill of the gcm forecasts we suggest that the calibrated ensemble forecasts are better forecasts for practical applications by contrast raw seas5 forecasts for all variables and lead times are found to be poor often with large bias and highly negative skill score we strongly discourage the direct use of raw forecasts the new bjp algorithm introduced in this paper is much simplified mathematically and more straightforward to code up than the original bjp model we find it highly efficient computationally in our post processing of seas5 and system 4 forecasts for the very large number grid cells across the australia continent and the leave one year out cross validation of 36 years of forecast data software availability pseudo computer code is given in appendix a r and python packages can be made available on request acknowledgements we thank the european centre for medium range weather forecasts ecmwf for providing the seas5 and system 4 data the data for seas5 can be obtained through the copernicus climate data store cds climate copernicus eu florian pappenberger acknowledges funding by the copernicus program which supported the production of seas5 hindcasts we thank the bureau of meteorology for awap data used in this study appendix d supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix d supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104550 appendix a the bayesian joint probability model parameter inference and predictive use via gibbs sampling in section 3 1 we have presented the bjp model for a bivariate problem the original bjp model wang et al 2009 wang and robertson 2011 was formulated as a multivariate problem allowing for multiple predictors and multiple predictands in this appendix we also present the bjp model in a general multivariate formulation of which the bivariate case is a special one a 1 the model consider d variables composed of both predictands and predictors with n number of data records a 1 z t z 1 t z 2 t z d t t where t 1 2 n z t is a column vector and t is for transpose we assume that the individual variables are normally distributed or have been normalized through transformation and z t is a sample drawn from a multivariate normal distribution a 2 z t n μ σ where μ is a d 1 mean vector and σ a d d covariance matrix we assume that the variables may sometimes have missing values or left censored values for example z i t z i c a 2 parameter inference the posterior distribution of the model parameters is according to bayes theorem a 3 p μ σ d p μ σ p d μ σ p μ σ t 1 n p z t μ σ where d z t t 1 2 n p μ σ is a prior distribution of the model parameters and p d μ σ is the likelihood we apply the non informative multivariate jeffreys prior gelman et al 2014 a 4 p μ σ σ d 1 2 by combining eq a 2 eq a 4 we can derive following conditional distributions for the parameters gelman et al 2014 a 5 σ inv wishart n 1 s a 6 μ n z σ n where a 7 s t 1 n z t z z t z t a 8 z 1 n t 1 n z t the symbol means conditional on all other variables and inv wishart n 1 is the inverse wishart distribution with n 1 degrees of freedom when one of variables z i t is missing a value we can also derive its conditional distribution a 9 z i t n μ i t σ i i where a 10 σ i i σ i i σ i i σ i i 1 σ i i a 11 μ i t μ i σ i i σ i i 1 z i t μ i with i denoting all index entries in 1 2 d except i the conditional distributions of eq a 5 eq a 11 are used to set up gibbs sampling to sequentially draw one sample from each of the conditional distributions and repeat many cycles of such sampling this yields a parameter chain which when converged forms multiple parameter sets to numerically represent the joint posterior distribution of the parameters and the missing values note that when z i t has a censored value instead of a missing value we can also use the conditional distribution of eq a 9 to sample a value we just need to restrict the sampling to the range of z i t z i c here we include pseudo code for implementing the gibbs sampling for parameter inference image 1 a 3 prediction once samples of the posterior distribution of the parameters are available we can apply the model for prediction the vector z t z 1 t z 2 t z d t t consists of both predictor variables and predictand variables in a prediction we want to find the posterior distribution of the predictands given information about the predictors sampling of this distribution can be conveniently done by treating the predictand variables as having missing values and applying the gibbs sampling step as outlined in eq a 9 eq a 11 in fact some of the predictors can also have missing or censored values and be included in the gibbs sampling for variables with censored values we need to restrict the sampling to the range of z i t z i c as discussed in section a 2 here we include pseudo code for implementing the gibbs sampling for prediction image 2 a 4 generating reference predictions sometimes it is useful to generate predictions using a naïve model to provide a reference for evaluating predictions from using a more advanced model one such naïve model is simply based on the distribution of historical observations of the predictands recall the full joint probability model of eq a 2 we can reduce the model to a subset a 12 z t n μ σ where z t consists of only the predictand variables given the parameter samples already obtained through gibbs sampling in section a 2 we can generate samples of z t by sampling the distribution in eq a 12 these samples can be used as ensemble reference predictions here we include pseudo code for implementing the gibbs sampling for generating reference predictions image 3 appendix b simple bias corrections for precipitation we use a multiplicative bias correction b 1 y 2 t t 1 n y 2 t t 1 n y 1 t y 1 t for temperature we use an additive bias correction b 2 y 2 t y 1 t 1 n t 1 n y 2 t 1 n t 1 n y 1 t where y 2 t denotes observed data y 1 t denotes raw ensemble forecast mean n is the number of events used in model fitting y 1 t is a new forecast ensemble member and y 2 t is a bias corrected forecast ensemble member the bias correction is applied to all members appendix c crps skill score and pit histogram the continuous ranked probability score crps hersbach 2000 is adopted to evaluate the forecast skill the average crps for time steps t 1 2 n is formulated as c 1 c r p s 1 n t 1 n f t x h x y t 2 d x where f t x is the cumulative distribution function cdf of an ensemble forecast at time t for variable x y t is the observed value and h is the heaviside step function which equals 0 if x y t and equals 1 otherwise crps is also calculated for a set of reference forecasts see section 3 2 a skill score is then computed as c 2 c r p s s k i l l s c o r e c r p s r e f c r p s c r p s r e f 100 reliability of the ensemble forecasts is evaluated by calculating the forecast probability integral transform pit values and plotting the probability integral transform pit histogram gneiting et al 2007 for each pair of forecast and observation the pit value is calculated by p i t t f t y t y t 0 a f t 0 y t 0 where a is randomly sampled from the uniform distribution 0 1 wang and robertson 2011 theoretically if the probabilistic forecasts are perfectly reliable the collection of pit values should follow a standard uniform distribution in this study the pit histogram is generated by pooling the pairwise forecasts and observations for all grids and all months or seasons the reliability can be examined by inspecting the uniformity of the pit histogram the more uniform the histogram is the more reliable the forecasts are 
