index,text
500,forecasting river ice breakup is critical for supporting emergency responses to river ice related flooding along rivers in the northern hemisphere however due to complex river ice processes forecasting river ice breakup is more challenging than predicting open water flood conditions although considerable progress has been made in understanding the mechanisms and characteristics of breakup processes and in forecasting breakup timing using empirical methods at the local scale fewer advances have been made in understanding and forecasting breakup using physically based models particularly at the catchment scale in this study we present a physically based coupled hydrological and water temperature modelling framework for breakup prediction in cold region catchments in real time the modelling framework was applied for operational forecasting of the 2019 breakup event along the athabasca river at fort mcmurray in alberta further model validation was performed by hindcasting the 2016 2017 and 2018 breakup events the model shows promising results for predicting the ice cover breakup with an average error of about 5 days demonstrating its usefulness in real time operational forecasting importantly the model generates breakup progression at the catchment scale providing an advantage over existing site specific breakup prediction methods keywords forecasting operational water management river ice flood risk hydroclimatic extremes cold region 1 introduction ice cover formation and breakup are important annual events in cold region environments that experience the seasonal effects of river ice in the northern hemisphere ice affects about 60 of river systems prowse et al 2007 ice covers that usually develop in the preceding fall season disintegrate with rising temperatures in spring the breakup process can be either mechanical or thermal depending upon hydro meteorological conditions river freeze up stage discharge and the properties of the ice cover mild weather with low flow conditions allows the ice cover to deteriorate gradually leading to a thermal breakup event in contrast rapid and high runoff conditions can lead to a mechanical breakup if runoff is sufficient to lift and dislodge the ice cover before significant thermal ice cover deterioration occurs beltaos 2003 mechanical breakups may lead to severe ice jam formation and has the potential to produce large scale flooding that can disrupt river navigation interrupt hydropower production and deliver detrimental effects for freshwater species and their habitats beltaos 2008 economic consequences from ice jam induced flooding often surpass damages caused by open water flood events this is primarily due to the fact that ice jam floods under the same or even lower discharge conditions can result in two to three times higher water depths in rivers compared to open water flood events primarily due to the aggregated thickness and increased roughness of ice jams prowse and beltaos 2002 an example of this can be found in the canadian province of new brunswick where detailed flood damage records are available beltaos and prowse 2001 reported that ice jam related flooding constitutes only one third of the total flood events but results in more than two thirds of the total flood damages adjusting for inflation on previous calculations french 2018 estimated that the annual financial costs of river ice jams in north america is about usd 300 million in 2017 value thus in cold region environments appropriate forecasting of ice cover breakup has significant value since breakup predictions can provide adequate lead time to mitigate and minimize flood risk sun and trevor 2018a however due to the complexity of river ice processes forecasting river ice breakup is more challenging than predicting open water flood conditions empirical models such as fuzzy logic e g mahabir et al 2008 zhao et al 2015 artificial neural networks e g wang et al 2010 guo et al 2018 adaptive neuro fuzzy inference systems e g mahabir et al 2006a 2007 and a combination of multi models e g sun 2018 sun and trevor 2018a have been widely used in river ice breakup predictions the cumulative degree days of thawing melting cddm is another empirical approach based on air temperature that is also commonly used to estimate breakup dates e g prowse and beltaos 2002 beltaos et al 2006 das et al 2017 rokaya et al 2019a some hydraulic models e g beltaos et al 2012 brayall and hicks 2012 have also been applied in the forecasting context to assess the probable flood water levels however despite considerable progress in understanding the mechanisms and characteristics of ice cover breakup and in forecasting breakup timing or flow magnitude using existing methods there are still some outstanding challenges to be tackled the physical processes underlying the cause effect relationship are not considered in empirical models lindenschmidt et al 2019 they also require fairly long historical observed data to train the model which might not always be available ice jams due to their scouring capacity are particularly known to damage hydrometric gauges so gaps in ice related events are more common making the ice affected records shorter than those for open water events burrell et al 2015 while hydraulic models are physically based they are data extensive and require flow and other boundary condition inputs from hydrological models or other data sources moreover their application in river ice has largely been in scenario analyses rather than operational breakup predictions most importantly existing forecasting methods that are in operational use are highly site specific in nature therefore they do not provide assessments at larger spatial scales on the contrary basin scale spatial progression of ice cover breakup is crucial in river ice forecasting since breakup and subsequent ice runs along upstream reaches can affect downstream breakup conditions despite advances in river ice hydrology and river ice modelling little progress has been made in advancing process based physical models in river ice forecasting a recent review of the ice jam flood literature by rokaya et al 2018a found little evidence of physically based model applications in operational forecasting of river ice floods thus as a step towards advancing the physically based modelling approach for breakup predictions we are introducing a novel operational modelling framework that couples hydrological and water temperature models for forecasting of river ice breakup in real time this approach involves coupling a physically based land surface hydrological model mesh pietroniro et al 2007 with a water temperature model rbm yearsley 2009 to simulate breakup timing in an operational setting using meteorological forecasts from the global deterministic prediction system draxler et al 2015 with a lead time of 240 h as a first step towards developing an operational river ice forecasting system we first tested the ability of the coupled model mesh rbm to simulate historical freeze up and breakup timing the results showed that the coupled model can successfully simulate river freeze up and breakup events see morales marín et al 2019 for further details the model is particularly suited for cold region catchments due to its ability to simulate snow processes and their effects on stream water temperature in this paper we present the operational application of the coupled model for breakup predictions to complement ice jam flood forecasting the model was applied successfully in forecasting of the 2019 breakup event along the athabasca river at fort mcmurray a known ice jam prone town in western canada for further validation of model results hindcasting was performed for the 2016 2017 and 2018 breakup events using archived meteorological forecasts 2 existing ice cover breakup prediction methods adapting the categories of ice cover breakup prediction methods proposed by white 2003 the existing methods can be tentatively categorized into four groups as follows 2 1 statistical methods the majority of earlier river ice breakup prediction methods were statistically based threshold models e g galbraith 1981 tuthill et al 1996 regression models e g mahabir et al 2006b sun and trevor 2018a and discriminant function analysis e g massie et al 2002 white and daly 2002 are some of the commonly used statistical models in threshold models historical data for each hydrological indicator is examined to select threshold levels such that no ice jam occurred below a lower threshold or ice jams always occurred above an upper threshold regression models are based on the concept of functional relationships they employ a linear equation to build a relationship between multiple inputs and a single output sun and trevor 2018a discriminant function analysis performs multivariate tests to determine differences between groups on the basis of observations of several predictor variables white 2003 however extensive historical data are often not available for ice jam conditions making it more difficult to develop statistical relationships burrell et al 2015 2 2 empirical methods fuzzy expert systems artificial neural networks anns and machine learning techniques are some of the common empirical methods that have been applied for breakup prediction in the fuzzy logic method linguistic terms are used to represent the values for inputs and outputs and their relationships are defined using if then rules the results of each set of rules are either combined or defuzzified converting the solution set into a single crisp value to obtain the final result mahabir et al 2002 compared to statistical methods fuzzy systems rely less on historical data since they are based on logical linguistic rules mahabir et al 2008 however there are challenges in describing the base rules and fine tuning of membership functions alternatively fuzzy logic models are also combined with anns to form adaptive neuro fuzzy inference systems anfis that allow representation of non linear relationships zhao et al 2015 sun and trevor 2015 provide a detailed comparison of the performance of some of the fuzzy logic models anns consist of computer algorithms that can be trained to reproduce the existing relationship between input and output variables of complex non linear systems through a learning process that involves algorithm training massie et al 2002 the forecasting approach is based on two steps first the learning process using historical ice conditions and then the predicting process anns have been increasingly employed in river ice forecasting due to their ability to predict ice conditions even with limited data however the iterative procedures of neural networks are sensitive to the selected initial weight values and the networks are sometimes trapped by local minima during the training stage which could prevent a solution from reaching the global minimum guo et al 2018 some recent studies have also combined multiple models and included other machine learning techniques sun and trevor 2018a combined anfis and ann to predict annual maximum water levels during river ice breakup whereas the stacking ensemble learning method has also been used to predict the breakup timing sun 2018 sun and trevor 2018b similarly several other machine learning methods have also been employed in river ice forecasting e g wang et al 2010 sun and trevor 2017 aleshin and malygin 2018 however existing empirical models do not consider the physical processes underlying the cause effect relationship lindenschmidt et al 2019 and tend to be site specific 2 3 hydraulic models relatively few studies have been undertaken that apply hydraulic models in river ice forecasting beltaos et al 2012 applied hec ras for operational forecasting of ice jam floods in the saint john river in eastern canada this work was supported by intensive field instrumentation and measurements blackburn and hicks 2003 tested the suitability of a 1 d dynamic model for flood forecasting during ice jam release surge events and brayall and hicks 2012 investigated the applicability of 2 d modelling for predicting ice jam flood levels although several hydraulic river ice models such as mike ice thériault et al 2010 crissp1d chen et al 2006 and river1d blackburn and she 2019 are capable of simulating water temperature and identifying breakup timing they have been rarely used in breakup forecasting possibly owing to intensive data requirements for model parameterization and setting the boundary conditions in a forecasting context yu et al 2019 states that the uncertainties in flows and ice conditions and the brevity of events hamper the predictive capability of hydraulic models 2 4 remote sensing remote sensing products have also been used in monitoring near real time river ice breakup beaton et al 2019 they are usually employed to track upstream river ice conditions and breakup progression so that predictions can be made based on current observations beaton et al 2017 used radarsat 2 for near real time monitoring of ice breakup to support flood forecasting and warning modis and radarsat 2 were used by lindenschmidt and li 2019 to distinguish intact ice covers and running ice accumulations and by lindenschmidt et al 2019 to monitor ice cover conditions and estimate total ice volume that could potentially accumulate in an ice jam other studies e g unterschultz et al 2009 chaouch et al 2014 muhammad et al 2016 have also used various remote sensing products to monitor ice cover breakup although remote sensing products have great utility for monitoring river ice conditions and aid other forecasting methods there are limitations in making predictions based on these products alone from an operational forecasting perspective some products such as radarsat 2 have coarser time resolution every two days whereas others such as modis have coarser spatial resolution 250 m there can also be conflict between different data users with commercial interest prevailing over research needs lindenschmidt et al 2019 3 data and methods 3 1 study site the athabasca river basin arb is one of the largest unregulated river basins in canada the athabasca river originates in jasper national park in central alberta and flows north east towards lake athabasca draining an area of over 150 000 km2 across the provinces of alberta and saskatchewan fig 1 the river is 1538 km long and the elevation ranges from 3747 m at its origin at mount columbia to 187 m at its terminal point at lake athabasca peters and prowse 2006 the arb has a continental climate with large seasonal variations with an average temperature of 17 4 c in january and 17 1 c in july the annual precipitation is around 400 mm of which roughly 75 falls as rain low winter flows snowmelt runoff in spring and peak flows in june characterize the hydrological regime of the basin burn et al 2004 ice jams have been observed during breakup periods along several reaches of the athabasca river but they exhibit a high flood risk at the town of fort mcmurray tfm see fig 1 for location of tfm in the study basin the tfm lies at the confluence of the athabasca and clearwater rivers the change in river bed slope from steep to mild the presence of islands and bars that provide obstacles to flow and ice and contributions from several small tributaries that deliver additional ice and flows make the river reach along the tfm more susceptible to ice jam flooding compared to other reaches she 2008 andrishak and hicks 2011 the 1977 ice jam flooding of the tfm caused cad 2 6 million dollars of damage claims whereas the 1997 event is estimated to have resulted in several million dollars of damages mahabir et al 2006b 3 2 mesh rbm mesh developed by environment and climate change canada eccc is a semi distributed physically based land surface scheme coupled with a hydrological model pietroniro et al 2007 mesh uses the canadian land surface scheme verseghy et al 1993 to compute vertical energy and water balance for soil vegetation and snow watrof soulis et al 2000 or pdmrof mekonnen et al 2014 for lateral soil and surface water movement and watroute for flow routing through the river channels kouwen 1988 the model is particularly suited for cold region catchments due to its ability to simulate complex snow processes including snow redistributions and snow melt mesh has been applied in both small and large scale basins e g davison et al 2016 yassin et al 2017 rokaya et al 2019b the river basin model rbm is a process based stream temperature model yearsley 2009 that solves the 1d heat advection equation as 1 d t w a x d t h a i r w a t e r w x ρ w c p q t r b δ t t r b δ x q e f f δ t e f f δ x where tw is water temperature c ax is the cross section area at distance x m 2 wx is stream width at distance x m ρ w is water density kg m 3 cp is the specific heat of water j kg 1 c 1 qtrb is the advected flow from tributaries or the subsurface m 3 s 1 δttrb is the difference between advected temperature from tributaries ttrb or the subsurface tw c qeff is the advected flow from heat dumps m 3 s 1 δteff is the difference between advected temperature from heat dumps teff and tw c δ x is the longitudinal distance along the river axis m and t is time s the net heat flux at the air water interface h air water is calculated in j m 2 s 1 as 2 h a i r w a t e r h s w h r s h a h a r h e v a p h c o n d h b a c k where hsw is short wave solar radiation j m 2 s 1 hrs is reflected short wave solar radiation j m 2 s 1 ha is long wave atmospheric radiation j m 2 s 1 har is reflected atmospheric radiation j m 2 s 1 hevap is the evaporative heat flux j m 2 s 1 hcond is conductive heat flux j m 2 s 1 and hback is black body radiation from the water surface j m 2 s 1 rbm calculates water temperature for a specific segment of the river based on the water temperature of the upstream reach incoming flows to the river segment and exchange of heat at the air water surface advected heat of the inflows from tributaries are also accounted for yearsley 2012 our modelling approach includes coupling mesh with rbm to simulate stream water temperature a detailed description of this coupling method is available from morales marín et al 2019 note that when negative water temperatures are calculated by solving eq 1 the model replaces such temperatures with a low temperature threshold equal to 0 c for numerical stability therefore the simulated water temperatures do not drop to negative values during the winter in spring water starts heating up from close to 0 c currently the model does not consider ice effects so the heat consumed by the melting of ice is not directly accounted for which introduces some uncertainty in the simulated results 3 3 global deterministic prediction system gdps gdps is an operational forecasting system based on the global environmental multiscale model côté et al 1998 from eccc that provides deterministic predictions of atmospheric variables 10 days into the future from the current day the forecasts are produced two times a day 00 and 12 utc on a 1500 751 latitude longitude grid at a resolution of 0 24 draxler et al 2015 recently a previous 4d variational data assimilation system was replaced by a new 4d ensemble variational data assimilation system which has shown to improve accuracy in forecasting up to 120 h lead time buehner et al 2015 since the data provides adequate lead time it has been found useful for ice jam flood forecasting lindenschmidt et al 2019 3 4 flow and water level data standardized flow data were retrieved from hydat a relational database of eccc that contains actual computed data of daily and monthly mean flows water levels sediment concentrations and in some locations peaks and extremes while standardized data also called historical hydrometric data are not available for current years eccc provides another set of data called real time hydrometric data real time data are provisional data that still need to undergo a quality check before they are officially released the under ice flow measurement data were provided by the regional office of water survey of canada wsc in alberta 3 5 ice cover breakup date the initiation of breakup is defined as occurring when transverse cracks form in the ice cover shulyakovskii 1972 or when the ice cover begins sustained movement at a specific location beltaos 1997 the end of breakup is assumed when a section of river becomes predominantly ice free pavelsky and smith 2004 the town of fort mcmurray falls under the jurisdiction of the regional municipality of wood buffalo rmwb which also regularly monitors river ice breakup along with alberta environment and parks the rmwb declares end of breakup when ice covers recede downstream of the tfm and ice jams no longer pose any significant flood risk in this study we adopt the definition provided by beltaos 1997 and define the breakup date as a point in time when sustained ice cover movement occurs from the flood risk perspective the prediction of the onset of breakup is important because moving ice blocks can lead to ice jamming and subsequent flooding for the end of breakup the dates provided by the rmwb were used 3 6 verification of forecasts and hindcasts the ice progression maps ice observation reports and photos from trail cameras were used for verification of forecasts and hindcasts ice progression maps and ice observation reports are released immediately after each observation flight by alberta environment and parks available from https rivers alberta ca additionally modis images were also used for verification of the 2018 hindcasts 4 forecasting system setup 4 1 mesh rbm setup to begin a mesh model was set up for the athabasca river basin with a drainage area of 132 057 km2 with the outlet at the gauging station 07da001 which is located a few km downstream of the tfm the drainage basin was prepared using greenkenue canadian hydraulics centre 2010 an advanced data preparation analysis and visualization tool the topographic data were obtained from the canadian digital elevation data the land use data were retrieved from natural resources canada and the soil data was made available by soil landscapes of canada the mesh model was set up with a spatial resolution of 0 125 resulting in 1326 grid cells and channels mesh requires seven meteorological input files precipitation air temperature wind speed barometric pressure specific humidity incoming longwave radiation and incoming shortwave radiation which are all available from the gdps rbm is also set up on the same orthogonal grid with a spatial resolution of 0 125 mesh and rbm were run independently in sequence fig 2 at first mesh was run at a 3 hour time step to simulate streamflow water depth width and averaged water velocity in every channel then these hydrological and hydraulic variables were aggregated to a daily time step and fed into rbm rbm also requires meteorological variables and river network topology however since meteorological observations are rarely discretized at the resolution of each segment node meteorological data were interpolated onto the river network then rbm was executed to compute water depth averaged temperatures at every channel of the stream network 4 2 mesh calibration and validation for historical model calibration and validation the required meteorological forcing files except for precipitation were obtained from the regional deterministic prediction system rdps caron et al 2015 which has spatial and temporal resolutions of 10 km and 1 h respectively similar to gdps used in forecasting rdps is a part of global environmental multiscale numerical weather prediction model côté et al 1998 developed and operationally run by eccc the precipitation data was retrieved from canadian precipitation analysis capa mahfouf et al 2007 which covers all of north america on a 10 km grid and provides reliable estimates of precipitation fortin et al 2015 it is a suitable forcing product for hydrologic modelling particularly for regions in the mid to high latitudes with sparse gauge networks lespinas et al 2015 boluwade et al 2018 the model was previously calibrated and validated for the 2002 2014 period see morales marín et al 2019 for further details in this calibration we focused on improved simulation of the ice cover period since most of the ice cover duration occurs during the low flow season for the athabasca river the previous baseflow algorithm was replaced with a new algorithm which was based on the revised version april 2016 of the watflood manual land cover and soil parameters representing the exchange of energy and water balance between the land surface and atmosphere were adopted from previous calibrations whereas hydrology parameters such as routing and baseflow were re calibrated the calibration was performed using a parallel version of the dynamically dimensioned search tolson and shoemaker 2008 algorithm in ostrich an open source auto calibration and multi algorithm parameter optimization software matott 2005 using the log of nash and sutcliffe ns efficiency as an objective function mesh was run at an hourly time step but streamflows were generated at a daily scale for comparisons and analyses the model was first re calibrated for the 2002 2009 period and then re validated for the 2010 2014 period the year 2002 was considered a model spin up period to diminish the effects of initial conditions on outputs of the model the results are presented in fig 3 a and b the use of a new routing and baseflow module resulted in improved calibration and validation a nslog value of 0 85 ns of 0 85 and pbias of 8 93 were obtained in the calibration period whereas the validation years resulted in nslog of 0 89 ns of 0 88 and pbias of 7 85 these results represent an improvement of approximately 25 compared to previous calibration and validation results see morales marín et al 2019 we further fine tuned the model to ensure that the model was able to simulate the breakup period which is a focus of this study in this additional model fine tuning the period between river freeze up and breakup from 1st october to 31st may for three years i e 2015 2016 2016 2017 and 2017 2018 that were not incorporated into a previous model calibration and validation were calibrated to ensure that the model was able to simulate under ice and breakup flow accurately the results were plotted against the under ice flow measurements conducted in the field by wsc fig 3c these observations are usually performed three times per winter by wsc to determine the amount the open water rating curve is required to be shifted to correct for under ice flows a comparison between simulated and measured flows for these under ice flow measurements showed a ns of 0 67 root mean square error of 26 74 m3 s and pbias of 11 07 4 3 rbm calibration and validation after mesh was calibrated and validated the hydrological and river network data from mesh was imported to rbm rbm was then simulated for the 2002 2012 period the first year i e 2002 was considered as a spin up period to avoid the effects of initial conditions the rbm results were compared with water temperature data available from alberta environment and parks http aep alberta ca water reports data surface water quality data for the station athabasca river upstream of fort mcmurray ab07cc0030 the station is located 100 m above the confluence with the horse river while model simulated temperature data are average daily temperatures the measured water temperature data are instantaneous measurements that are performed around mid day between 0 and 1 m below the water surface fig 4 demonstrates the model s ability to simulate the timing and magnitude of measured water temperatures a ns value of 0 89 r 2 of 0 91 rmse of 2 37 c and pbias of 1 80 were obtained there are some discrepancies between simulated and measured water temperatures near and below 5 c which could be due to the fact that the model does not incorporate ice effects however it is also important to note that while simulated water temperatures are average daily temperatures the observed data are instantaneous measurements recorded at point in time which will also lead to some differences 4 4 operational forecasting the generation of flow forecasts involves a two step process first hindcasting yesterday and then forecasting next 10 days it is necessary to save and update the basin state variables and hydrologic conditions so that initial conditions for running the model are accurate for each day for instance for day 1 mesh is first simulated for yesterday s hydrological conditions hindcasting and all basin state variables are then saved the saved state variables of day 1 are used as initial conditions to generate flow forecasts for the next 10 days the process is repeated for day 2 however in the hindcasting mode state variables from day 1 provide initial basin conditions for day 2 see fig 5 for a schematic view of this forecasting setup in hindcasting mode the model is forced with precipitation from capa and other required meteorological variables from the rdps while mesh is run at an hourly time step in hindcasting mode forecasting is carried out at 3 h time steps in correspondence with the temporal resolution of the forcing datasets once the mesh runs are completed the generated streamflows and other hydrological variables e g water depth width and average velocity in stream channel for the next 240 h are transferred to rbm which simulates stream water temperature for the next 9 days one day less is obtained in rbm due to time zone differences and aggregation at a daily time step gdps forecasts are issued at utc 00 00 which translates to 17 00 local time of the previous day for the study basin utc 7 for example forecasts issued at 00 00 on april 21 2018 for the next 240 h extend from 17 00 april 20 2018 to 16 00 april 30 2018 resulting in only nine full days of predicted daily aggregated water temperatures following this breakup dates are identified from simulated water temperatures in the earlier water temperature modelling study of the athabasca river morales marín et al 2019 reported that for the historical period the breakup at the tfm always occurred when the simulated water temperature tw was between 5 c and 6 c when heat required for melting of ice was not directly accounted for thus for simplicity and consistency in the operational forecasting we consider breakup to occur when simulated tw reaches 5 c the 5 c threshold should not be considered as a standard water temperature value at which breakup occurs but rather as an informative indicator to aid the prediction of breakup in a forecasting context however the literature shows that higher water temperatures at breakup are not unrealistic in the st john river up to 2 c in the lower peace river about 6 c beltaos 2017 and in the lower mackenzie river fort good hope up to 9 5 c parkinson 1982 water temperatures at breakup have been reported beltaos 2017 states that breakup water temperatures of several degrees c may be realistic in northern canada due to the large solar radiation flux associated with spring conditions at high latitudes and the extensive lengths of major rivers of northern canada 5 results and discussion 5 1 the chronology of the 2019 breakup the monitoring of the 2019 breakup was carried out using observation flights and trail cameras from alberta environment and parks the first observation flight took place on april 5 2019 the flight covered a 155 km reach of the athabasca river from south of grand rapids at the house river confluence to grant island which is downstream of the tfm a 30 km reach of the clearwater river was also covered aerial observation and photos revealed that the ice cover was largely intact the second observation flight on april 8 showed similar results the ice cover along the river was largely intact but there were some open water leads along some rapids on april 9 a third observation flight was conducted covering a 160 km reach of the athabasca river as well as a 30 km reach of the clearwater river the ice cover in the athabasca river upstream of crooked rapids located 40 km upstream of the tfm had broken up resulting in ice runs towards the tfm but an intact ice cover persisted between crooked rapids and the tfm which obstructed flowing ice and consequently a 16 km ice jam formed the athabasca river was largely open water upstream of the jam the observation flight on the next day april 10 revealed similar patterns the athabasca river was largely open water upstream of the ice jam whereas intact ice cover remained downstream of the toe of the ice jam i e crooked rapids however the intact ice cover on the athabasca river from crooked rapids to the tfm was found to be degraded compared to observations from april 9 the images from the observation flights on april 11 showed similar results for the upstream reach of the athabasca river however the ice cover on the lower reach of the athabasca river downstream of the tfm and along the clearwater river had started to break up thermally the observation flight on april 12 showed that the ice cover on the clearwater river was actively breaking up along the athabasca river the ice jam that had formed at crooked rapids had started to release and push downstream there was about 5 km of intact ice between the tfm golf course and the mouth of the clearwater river downstream of the confluence of the clearwater river and athabasca river ice was progressing downstream and clearing the subsequent observation flights from april 14 15 16 17 and 19 showed similar results the ice jam that had formed upstream of the intact ice still remained albeit the length of the jam melted from 15 km on april 14 to 13 km on april 15 and to 12 km on april 16 similarly the jam s length reduced to 10 km on april 17 and 4 km on april 19 the 3 km of intact ice cover that had resulted in the jamming continued to persist from april 14 to 19 the ice jam finally melted completely on april 20 and on april 22 the rmwb declared the end of the breakup at the tfm for 2019 5 2 forecasting of the 2019 breakup lindenschmidt et al 2019 using the air temperature based cddm method from the meteorological station fort mcmurray a wmo id 71689 latitude 56 3912 and longitude 111 1324 computed that breakup at the tfm during the 1958 2014 period occurred when cddm values were between 137 and 451 c days we commenced the forecasting of the 2019 breakup on march 25 2019 when the cddm value i e 123 7 c days was approaching the minimum historical value i e 137 c days and when the ice cover was still intact and the water level in the river was almost constant suggesting no increased spring freshet had occurred the water temperature forecasts were then generated for each day for the next nine days fig 6 shows the forecasts that were produced on april 1 2 and 3 the april 1 forecast showed that the water temperature would be well below the breakup threshold i e t w 5 c until april 9 but some sections near boiler rapids would experience higher water temperatures on april 10 the forecast from april 2 displays similar patterns the water temperature was projected to be below the breakup thresholds until april 9 but for april 10 and 11 it predicted that water temperatures would exceed the breakup threshold around broiler rapids and crooked rapids which are both upstream of the tfm see fig 1 for their locations finally the forecast on april 3 predicted that the water temperature at the tfm would exceed the breakup threshold on april 12 suggesting the occurrence of breakup 5 3 verification of 2019 forecasts fig 7 shows the observed ice progression maps from the 2019 breakup which were compiled from observation flights the map of april 12 fig 7d shows that the clearwater river was largely open water on april 12 however some segments of the athabasca river still remained intact our forecast from april 3 fig 6 which extends from april 4 to april 12 projected a breakup at the tfm on april 12 since the water temperature forecasts were above the breakup threshold for the grid where the tfm is located the water temperatures in grid cells were simulated at a spatial resolution of 0 125 the grid where the tfm is located encompasses a large area that includes both the athabasca river and the clearwater river therefore it is not possible to compute intra grid water temperature variability however from the forecasting point of view our predictions were successful since a large portion of the area was predominantly ice free however a small stretch of intact ice cover remained since the flows in the river were not large enough to result in vertical movement of the ice cover that would have resulted in cracks and subsequent breakups there was also about 15 km of ice jam in the upper reach near cascade rapids between crooked rapids and the tfm the model however is not equipped to simulate river ice processes such as the structural integrity and strength of the ice cover thus there was an error of 10 days in the forecasting of the 2019 breakup event if end of the breakup is considered 5 4 hindcasting of 2018 breakup the onset of ice cover breakup at the tfm in 2018 started around 11 00 mountain standard time mst on april 26 it was initiated by a large water wave and ice run that exerted significant pressure on the river ice cover at the tfm consequently the ice cover broke up resulting in high water levels another water wave and ice run occurred later that same day around 18 00 mst which pushed ice and water up the clearwater river and resulted in a peak stage of about 245 m a s l around 23 35 mst lindenschmidt et al 2019 on april 28 the rmwb declared the end of river breakup for the tfm meaning ice jams no longer posed any significant flood risk at the time of the declaration on april 28 the ice cover had already receded northward downstream of the confluence of the athabasca and clearwater rivers regional municipality of wood buffalo 2018 fig 8 shows the hindcast from april 20 2018 it shows that for the next four days the predicted water temperatures were still low at the tfm and upstream grids suggesting ice cover breakup had not initiated then for april 25 it displays that water temperature in the grid upstream of the tfm were slightly increased for april 26 the hindcast reveals that the water temperature was closer to the breakup threshold i e t w 5 c at the upstream grid of the tfm but still below the breakup threshold at the tfm for april 27 it shows that the temperature exceeded the breakup threshold suggesting the occurrence of ice cover breakup the hindcast for april 28 is consistent with the prediction for april 27 and shows that water temperature would remain above the breakup threshold 5 5 verification of 2018 hindcast the verification of hindcasts was performed by comparing water temperature simulations with modis images for the tfm in the absence of real time measured water temperature data and ice progression maps modis images were obtained for april 24 25 and 26 2018 the april 24 image shows that there was an intact ice cover along the athabasca river upstream of fort mcmurray fig 9 a the prediction for april 24 issued on april 20 from fig 8 also displays the water temperature as being below the breakup threshold at the tfm fig 9a the aerial photography taken the next day reveals that the ice cover conditions remained the same for most of the river on april 25 2018 lindenschmidt et al 2019 on april 26 2018 the ice cover broke up upstream of grand rapids along the athabasca river and ran past the tfm at 12 30 local time a modis image was acquired for the study site which is presented in fig 9b although it is challenging to differentiate between an intact and a stationary consolidated ice cover from running ice due to the coarse resolution of the modis imagery analyses showed that the ice present on the river that day was not an intact ice cover but rather an ice run which was confirmed by an aerial survey in the late afternoon of the same day lindenschmidt and li 2019 most of the ice cover between grand rapids and the tfm was broken up and running towards the tfm but some intact ice covers were still observed upstream of grand rapids lindenschmidt et al 2019 see also fig 9b our hindcast in fig 8 for april 26 also reveals similar patterns to what was observed on the field fig 9b shows that the water temperature for a number of grids upstream of grand rapids is still below the breakup threshold whereas the grids where grand rapids and the tfm are located show slightly higher water temperatures however the model fails to capture breakups along some rapids that are observed in the modis image this is mainly because breakups in these rapids occurs early due to geomorphology rather than warming water temperature finally on april 28 2018 a completion of the river breakup at the tfm was declared by the rmwb from the water temperature perspective our hindcasts show april 27 to be the breakup date for the tfm whereas on the site the breakup started on april 26 and completed on april 28 thus our hindcast resulted in an error of one day for the 2018 breakup at the tfm 5 6 2016 and 2017 breakup events using the archived forecast data from 2016 to 2017 we further evaluated the performance of the model in this experiment hindcast setups were run similar to the setup for the 2018 event the breakup in 2017 occurred on april 25 the ice jam lodged at the tfm water treatment plant released early in the morning breaking the intact ice cover that had remained near the bridges and clearwater river confluence the end of breakup was announced by the rmwb on april 29 fig 10 shows the hindcast generated for april 17 2017 for the 2017 breakup our hindcast predicts april 23 as being the potential breakup date based on the water temperature breakup threshold this translates to an error of 2 days when compared with the beginning of the breakup but an error of 6 days when compared with the completion of the breakup event a similar analysis was conducted for the 2016 breakup event the model results for the 2016 breakup event reveal that the breakup threshold for simulated water temperature was surpassed on april 18 2016 see fig 11 in 2016 the breakup occurred on april 17 and the end of breakup was announced on april 19 2016 thus our prediction from hindcasting has one day bias when compared to both the beginning and end of the breakup date unfortunately adequate data were not available for more comprehensive assessments of the simulated water temperature alberta environment and parks measures surface water temperature but measurements are instantaneous at a point in time and irregular for instance for station ab07cc0030 which is located in the study site measurements were carried out on march 14 2016 at 17 15 and then on may 11 2016 at 18 15 the last available data was for march 13 2017 at 13 20 when accessed on march 10 2019 these gaps in measured data make any systematic comparisons challenging 5 7 spatial variability in breakup one of the advantages of using catchment scale models over other site specific breakup prediction methods is that it allows generation of spatial patterns of breakup theoretically from the perspective of air temperature breakup should occur first in the headwater sub basins in the south and gradually progress northward towards the basin outlet however breakup is a complex process and differs spatially across and within river catchments rokaya et al 2018b studied the trends in timing of breakup flow in canada and found earlier trends in south eastern and western canada and delayed breakup trends in atlantic canada within the mackenzie river basin which encompasses the athabasca river basin de rham et al 2008 found breakup timing to vary spatially by about 8 weeks with some years up to 12 weeks morales marín et al 2019 studied the breakup timing in the athabasca river basin from 2002 to 2012 and reported spatially diverse breakup patterns across the basin their findings show that breakup in tributaries occurs first from late march to early april in the mainstream river breakup occurred first in the upper southwestern and middle sections of the basin and gradually progressed northward toward the tfm at the headwaters breakup occurs late due to low air and water temperatures in the rocky mountains fig 12 shows spatial variability in breakup from the hindcast of april 20 2018 for the 2018 breakup event that occurred on april 28 the spatial patterns are similar to the findings reported by morales marín et al 2019 it shows that by the time breakup occurred at the tfm most of the tributary reaches had already experienced breakup in the mainstem river most of the river segments were already above the breakup threshold temperature while headwater reaches were still experiencing low temperatures the understanding and knowledge of spatial variability of breakup is very crucial from a forecasting perspective for instance if the forecast shows that water temperatures are still below breakup thresholds in the tfm but above such thresholds in upstream reaches then it can be expected that breakup will likely occur sooner in the tfm breakup subsequent ice runs and warmer water from upstream reaches affect downstream breakup timing therefore forecasting basin scale breakup dates will provide more valuable information compared to site specific methods 6 discussion river ice breakup jamming and subsequent flooding are complex phenomena primarily governed by channel morphology freeze up conditions ice characteristics climatic factors and snowmelt runoff several types of models and modelling approaches have been utilized in the past ranging from simple degree days of melting methods to sophisticated hydraulic models to predict breakup timing flow and backwater staging with advances in computing recent years have seen a surge in applications of artificial neural networks and other machine learning despite recent advances many of the existing forecasting methods that are in operational use are site specific and only applicable at local scales our integrated hydrological and water temperature modelling framework provides an opportunity to predict breakup progression for large scale catchments our operational forecasting of river ice breakup for the 2019 event and hindcasting for three breakup events in 2016 2017 and 2018 showed an average error of about 5 days for the tfm the error margins are comparatively smaller than other existing applications for instance to complement their real time ice jam flood forecasting work for the athabasca river at fort mcmurray for the 2018 breakup lindenschmidt et al 2019 used an air temperature based cddm method the cddm was computed from daily air temperatures recorded at the tfm weather station from their analyses of historical breakup events from 1958 to 2014 they identified the cddm to range between 137 and 451 c days and forecasting was commenced when cddm values for 2018 were approaching a minimum value i e 131 c days this however poses a large uncertainty since historical analyses show a large variation in cddm values 314 c days the longer range of probable breakup dates means more computing and monitoring resources are required our alternative method provides a relatively narrow window of breakup dates so extensive ice jam flood forecasting work can be commenced when positive water temperatures or breakups are forecasted we analyzed our model s performance for the tfm but we could not compare our results for other locations due to the lack of observed data the validation of models has always been a challenging issue in breakup prediction observation data are taken infrequently forcing some interpretation to rely on subjective opinions in 2019 we were able to track ice cover breakup progression from the observation flight data provided by alberta environment and parks in 2018 as part of separate ice jam flood forecasting work see lindenschmidt et al 2019 we tried to access several remote sensing products for real time monitoring and validation even though we were able to access some modis data for verification for the 2018 breakup event acquiring sentinel 1 and radarsat 2 imagery for the 2018 breakup was difficult the sentinel 1 satellite did not fly the study reach on april 26 2018 and our radarsat 2 image requests were in conflict with commercial users there is also room for model improvement morales marín et al 2019 note that inadequate data hinders correct setup of model boundary conditions there are also several other uncertainties related to meteorological forcing data propagation of errors from the hydrological to the water temperature model and inadequate incorporation of anthropogenic impacts although we have taken some measures to improve accuracy of upstream water temperatures in mountainous headwaters using remote sensing data to help improve hydrological simulations see morales marín et al 2019 for details more improvements are required to reduce uncertainties stemming from meteorological forecasts and errors propagating from mesh to rbm similarly at the catchment scale our results show that there is not a smooth continuity in water temperature due to spatial variability in air temperature however further analyses are required to determine atmospheric and hydrological linkages additionally improvements are also required to better characterize local geomorphology and incorporate ice effects in 2018 and 2019 we observed some breakup events along the rapids even when air water temperatures were below breakup thresholds such localized phenomena along the rapids are difficult to estimate with the current model configuration similarly simulating water temperatures closer to 0 c was challenging in absence of consideration of ice effects currently several water temperature models exist but to the best of our knowledge this is the first application of a coupled hydrological and water temperature model for operational breakup prediction and we believe it will spur on further research towards the application of physically based models in real time operational forecasting of river ice breakup our hydrological model mesh performs both water and energy balances and is best suited to cold region large scale catchments although our analyses were carried out at a daily time step this modelling setup also offers the possibility of running forecasts at sub daily resolution since gdps forecast data are available at 3 hourly temporal resolutions furthermore both mesh and rbm are community based open source software although we ran mesh and rbm independently efforts are ongoing to compile a rbm module in mesh which would make the coupling more efficient and user friendly we expect a new mesh version with the rbm module will be released in the near future the python toolbox used to pre process mesh inputs and rbm input information and c software to extract water temperature time series at individual cells time series of grid domains time series of individual reaches and to estimate multi annual averages and monthly averages from rbm are available from github https github com lamhydro rbmpospro we believe the open nature of these models and modelling approaches will encourage further research towards advancement of ice related flood research 7 conclusion forecasting ice cover breakup in spring is critical in cold region catchments to support the emergency responses to river ice related flooding in this study we presented a physically based coupled hydrological and water temperature modelling framework to predict the timing of river ice breakup the framework was successfully applied to forecast and hindcast breakup dates for four spring ice cover breakup events 2016 2019 along the athabasca river at fort mcmurray the coupled model shows promising results with an error of up to 10 days in its forecast and up to 6 days in its hindcasts more importantly this framework can be applied at the catchment scale enabling identification of the spatial progression of breakup across the river basin it is hoped that these research findings will advance the operational forecasting of ice related flooding which will aid preparedness and mitigation of ice jam flooding reduce flood damages and protect human life along rivers prone to major ice jam events author contributions pr and kel conceived the idea for this study and brainstormed with lm on research methodology pr set up the operational forecasting system and lm assisted in setting up the rbm model pr performed model simulations analysed the results and wrote the first draft of the paper kel and lm edited and commented on the manuscript and contributed to the text and figure presentation in later iterations declaration of competing interest the authors declare no competing interests acknowledgments the authors are thankful to staff from the river forecasting centre of alberta environment and park for providing field updates as well as ice observation reports maps and photos after each observation flight we are also grateful to daniel prinz from environment and climate change canada for providing archived meteorological forcing data for 2016 and 2017 the under ice flow measurement data were graciously provided by the wsc regional office in alberta the authors are also thankful to zhaoqin li from the university of saskatchewan for extracting modis images for the 2018 breakup the funding for this research was provided by the global water futures program at the university of saskatchewan supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103554 appendix supplementary materials image application 1 
500,forecasting river ice breakup is critical for supporting emergency responses to river ice related flooding along rivers in the northern hemisphere however due to complex river ice processes forecasting river ice breakup is more challenging than predicting open water flood conditions although considerable progress has been made in understanding the mechanisms and characteristics of breakup processes and in forecasting breakup timing using empirical methods at the local scale fewer advances have been made in understanding and forecasting breakup using physically based models particularly at the catchment scale in this study we present a physically based coupled hydrological and water temperature modelling framework for breakup prediction in cold region catchments in real time the modelling framework was applied for operational forecasting of the 2019 breakup event along the athabasca river at fort mcmurray in alberta further model validation was performed by hindcasting the 2016 2017 and 2018 breakup events the model shows promising results for predicting the ice cover breakup with an average error of about 5 days demonstrating its usefulness in real time operational forecasting importantly the model generates breakup progression at the catchment scale providing an advantage over existing site specific breakup prediction methods keywords forecasting operational water management river ice flood risk hydroclimatic extremes cold region 1 introduction ice cover formation and breakup are important annual events in cold region environments that experience the seasonal effects of river ice in the northern hemisphere ice affects about 60 of river systems prowse et al 2007 ice covers that usually develop in the preceding fall season disintegrate with rising temperatures in spring the breakup process can be either mechanical or thermal depending upon hydro meteorological conditions river freeze up stage discharge and the properties of the ice cover mild weather with low flow conditions allows the ice cover to deteriorate gradually leading to a thermal breakup event in contrast rapid and high runoff conditions can lead to a mechanical breakup if runoff is sufficient to lift and dislodge the ice cover before significant thermal ice cover deterioration occurs beltaos 2003 mechanical breakups may lead to severe ice jam formation and has the potential to produce large scale flooding that can disrupt river navigation interrupt hydropower production and deliver detrimental effects for freshwater species and their habitats beltaos 2008 economic consequences from ice jam induced flooding often surpass damages caused by open water flood events this is primarily due to the fact that ice jam floods under the same or even lower discharge conditions can result in two to three times higher water depths in rivers compared to open water flood events primarily due to the aggregated thickness and increased roughness of ice jams prowse and beltaos 2002 an example of this can be found in the canadian province of new brunswick where detailed flood damage records are available beltaos and prowse 2001 reported that ice jam related flooding constitutes only one third of the total flood events but results in more than two thirds of the total flood damages adjusting for inflation on previous calculations french 2018 estimated that the annual financial costs of river ice jams in north america is about usd 300 million in 2017 value thus in cold region environments appropriate forecasting of ice cover breakup has significant value since breakup predictions can provide adequate lead time to mitigate and minimize flood risk sun and trevor 2018a however due to the complexity of river ice processes forecasting river ice breakup is more challenging than predicting open water flood conditions empirical models such as fuzzy logic e g mahabir et al 2008 zhao et al 2015 artificial neural networks e g wang et al 2010 guo et al 2018 adaptive neuro fuzzy inference systems e g mahabir et al 2006a 2007 and a combination of multi models e g sun 2018 sun and trevor 2018a have been widely used in river ice breakup predictions the cumulative degree days of thawing melting cddm is another empirical approach based on air temperature that is also commonly used to estimate breakup dates e g prowse and beltaos 2002 beltaos et al 2006 das et al 2017 rokaya et al 2019a some hydraulic models e g beltaos et al 2012 brayall and hicks 2012 have also been applied in the forecasting context to assess the probable flood water levels however despite considerable progress in understanding the mechanisms and characteristics of ice cover breakup and in forecasting breakup timing or flow magnitude using existing methods there are still some outstanding challenges to be tackled the physical processes underlying the cause effect relationship are not considered in empirical models lindenschmidt et al 2019 they also require fairly long historical observed data to train the model which might not always be available ice jams due to their scouring capacity are particularly known to damage hydrometric gauges so gaps in ice related events are more common making the ice affected records shorter than those for open water events burrell et al 2015 while hydraulic models are physically based they are data extensive and require flow and other boundary condition inputs from hydrological models or other data sources moreover their application in river ice has largely been in scenario analyses rather than operational breakup predictions most importantly existing forecasting methods that are in operational use are highly site specific in nature therefore they do not provide assessments at larger spatial scales on the contrary basin scale spatial progression of ice cover breakup is crucial in river ice forecasting since breakup and subsequent ice runs along upstream reaches can affect downstream breakup conditions despite advances in river ice hydrology and river ice modelling little progress has been made in advancing process based physical models in river ice forecasting a recent review of the ice jam flood literature by rokaya et al 2018a found little evidence of physically based model applications in operational forecasting of river ice floods thus as a step towards advancing the physically based modelling approach for breakup predictions we are introducing a novel operational modelling framework that couples hydrological and water temperature models for forecasting of river ice breakup in real time this approach involves coupling a physically based land surface hydrological model mesh pietroniro et al 2007 with a water temperature model rbm yearsley 2009 to simulate breakup timing in an operational setting using meteorological forecasts from the global deterministic prediction system draxler et al 2015 with a lead time of 240 h as a first step towards developing an operational river ice forecasting system we first tested the ability of the coupled model mesh rbm to simulate historical freeze up and breakup timing the results showed that the coupled model can successfully simulate river freeze up and breakup events see morales marín et al 2019 for further details the model is particularly suited for cold region catchments due to its ability to simulate snow processes and their effects on stream water temperature in this paper we present the operational application of the coupled model for breakup predictions to complement ice jam flood forecasting the model was applied successfully in forecasting of the 2019 breakup event along the athabasca river at fort mcmurray a known ice jam prone town in western canada for further validation of model results hindcasting was performed for the 2016 2017 and 2018 breakup events using archived meteorological forecasts 2 existing ice cover breakup prediction methods adapting the categories of ice cover breakup prediction methods proposed by white 2003 the existing methods can be tentatively categorized into four groups as follows 2 1 statistical methods the majority of earlier river ice breakup prediction methods were statistically based threshold models e g galbraith 1981 tuthill et al 1996 regression models e g mahabir et al 2006b sun and trevor 2018a and discriminant function analysis e g massie et al 2002 white and daly 2002 are some of the commonly used statistical models in threshold models historical data for each hydrological indicator is examined to select threshold levels such that no ice jam occurred below a lower threshold or ice jams always occurred above an upper threshold regression models are based on the concept of functional relationships they employ a linear equation to build a relationship between multiple inputs and a single output sun and trevor 2018a discriminant function analysis performs multivariate tests to determine differences between groups on the basis of observations of several predictor variables white 2003 however extensive historical data are often not available for ice jam conditions making it more difficult to develop statistical relationships burrell et al 2015 2 2 empirical methods fuzzy expert systems artificial neural networks anns and machine learning techniques are some of the common empirical methods that have been applied for breakup prediction in the fuzzy logic method linguistic terms are used to represent the values for inputs and outputs and their relationships are defined using if then rules the results of each set of rules are either combined or defuzzified converting the solution set into a single crisp value to obtain the final result mahabir et al 2002 compared to statistical methods fuzzy systems rely less on historical data since they are based on logical linguistic rules mahabir et al 2008 however there are challenges in describing the base rules and fine tuning of membership functions alternatively fuzzy logic models are also combined with anns to form adaptive neuro fuzzy inference systems anfis that allow representation of non linear relationships zhao et al 2015 sun and trevor 2015 provide a detailed comparison of the performance of some of the fuzzy logic models anns consist of computer algorithms that can be trained to reproduce the existing relationship between input and output variables of complex non linear systems through a learning process that involves algorithm training massie et al 2002 the forecasting approach is based on two steps first the learning process using historical ice conditions and then the predicting process anns have been increasingly employed in river ice forecasting due to their ability to predict ice conditions even with limited data however the iterative procedures of neural networks are sensitive to the selected initial weight values and the networks are sometimes trapped by local minima during the training stage which could prevent a solution from reaching the global minimum guo et al 2018 some recent studies have also combined multiple models and included other machine learning techniques sun and trevor 2018a combined anfis and ann to predict annual maximum water levels during river ice breakup whereas the stacking ensemble learning method has also been used to predict the breakup timing sun 2018 sun and trevor 2018b similarly several other machine learning methods have also been employed in river ice forecasting e g wang et al 2010 sun and trevor 2017 aleshin and malygin 2018 however existing empirical models do not consider the physical processes underlying the cause effect relationship lindenschmidt et al 2019 and tend to be site specific 2 3 hydraulic models relatively few studies have been undertaken that apply hydraulic models in river ice forecasting beltaos et al 2012 applied hec ras for operational forecasting of ice jam floods in the saint john river in eastern canada this work was supported by intensive field instrumentation and measurements blackburn and hicks 2003 tested the suitability of a 1 d dynamic model for flood forecasting during ice jam release surge events and brayall and hicks 2012 investigated the applicability of 2 d modelling for predicting ice jam flood levels although several hydraulic river ice models such as mike ice thériault et al 2010 crissp1d chen et al 2006 and river1d blackburn and she 2019 are capable of simulating water temperature and identifying breakup timing they have been rarely used in breakup forecasting possibly owing to intensive data requirements for model parameterization and setting the boundary conditions in a forecasting context yu et al 2019 states that the uncertainties in flows and ice conditions and the brevity of events hamper the predictive capability of hydraulic models 2 4 remote sensing remote sensing products have also been used in monitoring near real time river ice breakup beaton et al 2019 they are usually employed to track upstream river ice conditions and breakup progression so that predictions can be made based on current observations beaton et al 2017 used radarsat 2 for near real time monitoring of ice breakup to support flood forecasting and warning modis and radarsat 2 were used by lindenschmidt and li 2019 to distinguish intact ice covers and running ice accumulations and by lindenschmidt et al 2019 to monitor ice cover conditions and estimate total ice volume that could potentially accumulate in an ice jam other studies e g unterschultz et al 2009 chaouch et al 2014 muhammad et al 2016 have also used various remote sensing products to monitor ice cover breakup although remote sensing products have great utility for monitoring river ice conditions and aid other forecasting methods there are limitations in making predictions based on these products alone from an operational forecasting perspective some products such as radarsat 2 have coarser time resolution every two days whereas others such as modis have coarser spatial resolution 250 m there can also be conflict between different data users with commercial interest prevailing over research needs lindenschmidt et al 2019 3 data and methods 3 1 study site the athabasca river basin arb is one of the largest unregulated river basins in canada the athabasca river originates in jasper national park in central alberta and flows north east towards lake athabasca draining an area of over 150 000 km2 across the provinces of alberta and saskatchewan fig 1 the river is 1538 km long and the elevation ranges from 3747 m at its origin at mount columbia to 187 m at its terminal point at lake athabasca peters and prowse 2006 the arb has a continental climate with large seasonal variations with an average temperature of 17 4 c in january and 17 1 c in july the annual precipitation is around 400 mm of which roughly 75 falls as rain low winter flows snowmelt runoff in spring and peak flows in june characterize the hydrological regime of the basin burn et al 2004 ice jams have been observed during breakup periods along several reaches of the athabasca river but they exhibit a high flood risk at the town of fort mcmurray tfm see fig 1 for location of tfm in the study basin the tfm lies at the confluence of the athabasca and clearwater rivers the change in river bed slope from steep to mild the presence of islands and bars that provide obstacles to flow and ice and contributions from several small tributaries that deliver additional ice and flows make the river reach along the tfm more susceptible to ice jam flooding compared to other reaches she 2008 andrishak and hicks 2011 the 1977 ice jam flooding of the tfm caused cad 2 6 million dollars of damage claims whereas the 1997 event is estimated to have resulted in several million dollars of damages mahabir et al 2006b 3 2 mesh rbm mesh developed by environment and climate change canada eccc is a semi distributed physically based land surface scheme coupled with a hydrological model pietroniro et al 2007 mesh uses the canadian land surface scheme verseghy et al 1993 to compute vertical energy and water balance for soil vegetation and snow watrof soulis et al 2000 or pdmrof mekonnen et al 2014 for lateral soil and surface water movement and watroute for flow routing through the river channels kouwen 1988 the model is particularly suited for cold region catchments due to its ability to simulate complex snow processes including snow redistributions and snow melt mesh has been applied in both small and large scale basins e g davison et al 2016 yassin et al 2017 rokaya et al 2019b the river basin model rbm is a process based stream temperature model yearsley 2009 that solves the 1d heat advection equation as 1 d t w a x d t h a i r w a t e r w x ρ w c p q t r b δ t t r b δ x q e f f δ t e f f δ x where tw is water temperature c ax is the cross section area at distance x m 2 wx is stream width at distance x m ρ w is water density kg m 3 cp is the specific heat of water j kg 1 c 1 qtrb is the advected flow from tributaries or the subsurface m 3 s 1 δttrb is the difference between advected temperature from tributaries ttrb or the subsurface tw c qeff is the advected flow from heat dumps m 3 s 1 δteff is the difference between advected temperature from heat dumps teff and tw c δ x is the longitudinal distance along the river axis m and t is time s the net heat flux at the air water interface h air water is calculated in j m 2 s 1 as 2 h a i r w a t e r h s w h r s h a h a r h e v a p h c o n d h b a c k where hsw is short wave solar radiation j m 2 s 1 hrs is reflected short wave solar radiation j m 2 s 1 ha is long wave atmospheric radiation j m 2 s 1 har is reflected atmospheric radiation j m 2 s 1 hevap is the evaporative heat flux j m 2 s 1 hcond is conductive heat flux j m 2 s 1 and hback is black body radiation from the water surface j m 2 s 1 rbm calculates water temperature for a specific segment of the river based on the water temperature of the upstream reach incoming flows to the river segment and exchange of heat at the air water surface advected heat of the inflows from tributaries are also accounted for yearsley 2012 our modelling approach includes coupling mesh with rbm to simulate stream water temperature a detailed description of this coupling method is available from morales marín et al 2019 note that when negative water temperatures are calculated by solving eq 1 the model replaces such temperatures with a low temperature threshold equal to 0 c for numerical stability therefore the simulated water temperatures do not drop to negative values during the winter in spring water starts heating up from close to 0 c currently the model does not consider ice effects so the heat consumed by the melting of ice is not directly accounted for which introduces some uncertainty in the simulated results 3 3 global deterministic prediction system gdps gdps is an operational forecasting system based on the global environmental multiscale model côté et al 1998 from eccc that provides deterministic predictions of atmospheric variables 10 days into the future from the current day the forecasts are produced two times a day 00 and 12 utc on a 1500 751 latitude longitude grid at a resolution of 0 24 draxler et al 2015 recently a previous 4d variational data assimilation system was replaced by a new 4d ensemble variational data assimilation system which has shown to improve accuracy in forecasting up to 120 h lead time buehner et al 2015 since the data provides adequate lead time it has been found useful for ice jam flood forecasting lindenschmidt et al 2019 3 4 flow and water level data standardized flow data were retrieved from hydat a relational database of eccc that contains actual computed data of daily and monthly mean flows water levels sediment concentrations and in some locations peaks and extremes while standardized data also called historical hydrometric data are not available for current years eccc provides another set of data called real time hydrometric data real time data are provisional data that still need to undergo a quality check before they are officially released the under ice flow measurement data were provided by the regional office of water survey of canada wsc in alberta 3 5 ice cover breakup date the initiation of breakup is defined as occurring when transverse cracks form in the ice cover shulyakovskii 1972 or when the ice cover begins sustained movement at a specific location beltaos 1997 the end of breakup is assumed when a section of river becomes predominantly ice free pavelsky and smith 2004 the town of fort mcmurray falls under the jurisdiction of the regional municipality of wood buffalo rmwb which also regularly monitors river ice breakup along with alberta environment and parks the rmwb declares end of breakup when ice covers recede downstream of the tfm and ice jams no longer pose any significant flood risk in this study we adopt the definition provided by beltaos 1997 and define the breakup date as a point in time when sustained ice cover movement occurs from the flood risk perspective the prediction of the onset of breakup is important because moving ice blocks can lead to ice jamming and subsequent flooding for the end of breakup the dates provided by the rmwb were used 3 6 verification of forecasts and hindcasts the ice progression maps ice observation reports and photos from trail cameras were used for verification of forecasts and hindcasts ice progression maps and ice observation reports are released immediately after each observation flight by alberta environment and parks available from https rivers alberta ca additionally modis images were also used for verification of the 2018 hindcasts 4 forecasting system setup 4 1 mesh rbm setup to begin a mesh model was set up for the athabasca river basin with a drainage area of 132 057 km2 with the outlet at the gauging station 07da001 which is located a few km downstream of the tfm the drainage basin was prepared using greenkenue canadian hydraulics centre 2010 an advanced data preparation analysis and visualization tool the topographic data were obtained from the canadian digital elevation data the land use data were retrieved from natural resources canada and the soil data was made available by soil landscapes of canada the mesh model was set up with a spatial resolution of 0 125 resulting in 1326 grid cells and channels mesh requires seven meteorological input files precipitation air temperature wind speed barometric pressure specific humidity incoming longwave radiation and incoming shortwave radiation which are all available from the gdps rbm is also set up on the same orthogonal grid with a spatial resolution of 0 125 mesh and rbm were run independently in sequence fig 2 at first mesh was run at a 3 hour time step to simulate streamflow water depth width and averaged water velocity in every channel then these hydrological and hydraulic variables were aggregated to a daily time step and fed into rbm rbm also requires meteorological variables and river network topology however since meteorological observations are rarely discretized at the resolution of each segment node meteorological data were interpolated onto the river network then rbm was executed to compute water depth averaged temperatures at every channel of the stream network 4 2 mesh calibration and validation for historical model calibration and validation the required meteorological forcing files except for precipitation were obtained from the regional deterministic prediction system rdps caron et al 2015 which has spatial and temporal resolutions of 10 km and 1 h respectively similar to gdps used in forecasting rdps is a part of global environmental multiscale numerical weather prediction model côté et al 1998 developed and operationally run by eccc the precipitation data was retrieved from canadian precipitation analysis capa mahfouf et al 2007 which covers all of north america on a 10 km grid and provides reliable estimates of precipitation fortin et al 2015 it is a suitable forcing product for hydrologic modelling particularly for regions in the mid to high latitudes with sparse gauge networks lespinas et al 2015 boluwade et al 2018 the model was previously calibrated and validated for the 2002 2014 period see morales marín et al 2019 for further details in this calibration we focused on improved simulation of the ice cover period since most of the ice cover duration occurs during the low flow season for the athabasca river the previous baseflow algorithm was replaced with a new algorithm which was based on the revised version april 2016 of the watflood manual land cover and soil parameters representing the exchange of energy and water balance between the land surface and atmosphere were adopted from previous calibrations whereas hydrology parameters such as routing and baseflow were re calibrated the calibration was performed using a parallel version of the dynamically dimensioned search tolson and shoemaker 2008 algorithm in ostrich an open source auto calibration and multi algorithm parameter optimization software matott 2005 using the log of nash and sutcliffe ns efficiency as an objective function mesh was run at an hourly time step but streamflows were generated at a daily scale for comparisons and analyses the model was first re calibrated for the 2002 2009 period and then re validated for the 2010 2014 period the year 2002 was considered a model spin up period to diminish the effects of initial conditions on outputs of the model the results are presented in fig 3 a and b the use of a new routing and baseflow module resulted in improved calibration and validation a nslog value of 0 85 ns of 0 85 and pbias of 8 93 were obtained in the calibration period whereas the validation years resulted in nslog of 0 89 ns of 0 88 and pbias of 7 85 these results represent an improvement of approximately 25 compared to previous calibration and validation results see morales marín et al 2019 we further fine tuned the model to ensure that the model was able to simulate the breakup period which is a focus of this study in this additional model fine tuning the period between river freeze up and breakup from 1st october to 31st may for three years i e 2015 2016 2016 2017 and 2017 2018 that were not incorporated into a previous model calibration and validation were calibrated to ensure that the model was able to simulate under ice and breakup flow accurately the results were plotted against the under ice flow measurements conducted in the field by wsc fig 3c these observations are usually performed three times per winter by wsc to determine the amount the open water rating curve is required to be shifted to correct for under ice flows a comparison between simulated and measured flows for these under ice flow measurements showed a ns of 0 67 root mean square error of 26 74 m3 s and pbias of 11 07 4 3 rbm calibration and validation after mesh was calibrated and validated the hydrological and river network data from mesh was imported to rbm rbm was then simulated for the 2002 2012 period the first year i e 2002 was considered as a spin up period to avoid the effects of initial conditions the rbm results were compared with water temperature data available from alberta environment and parks http aep alberta ca water reports data surface water quality data for the station athabasca river upstream of fort mcmurray ab07cc0030 the station is located 100 m above the confluence with the horse river while model simulated temperature data are average daily temperatures the measured water temperature data are instantaneous measurements that are performed around mid day between 0 and 1 m below the water surface fig 4 demonstrates the model s ability to simulate the timing and magnitude of measured water temperatures a ns value of 0 89 r 2 of 0 91 rmse of 2 37 c and pbias of 1 80 were obtained there are some discrepancies between simulated and measured water temperatures near and below 5 c which could be due to the fact that the model does not incorporate ice effects however it is also important to note that while simulated water temperatures are average daily temperatures the observed data are instantaneous measurements recorded at point in time which will also lead to some differences 4 4 operational forecasting the generation of flow forecasts involves a two step process first hindcasting yesterday and then forecasting next 10 days it is necessary to save and update the basin state variables and hydrologic conditions so that initial conditions for running the model are accurate for each day for instance for day 1 mesh is first simulated for yesterday s hydrological conditions hindcasting and all basin state variables are then saved the saved state variables of day 1 are used as initial conditions to generate flow forecasts for the next 10 days the process is repeated for day 2 however in the hindcasting mode state variables from day 1 provide initial basin conditions for day 2 see fig 5 for a schematic view of this forecasting setup in hindcasting mode the model is forced with precipitation from capa and other required meteorological variables from the rdps while mesh is run at an hourly time step in hindcasting mode forecasting is carried out at 3 h time steps in correspondence with the temporal resolution of the forcing datasets once the mesh runs are completed the generated streamflows and other hydrological variables e g water depth width and average velocity in stream channel for the next 240 h are transferred to rbm which simulates stream water temperature for the next 9 days one day less is obtained in rbm due to time zone differences and aggregation at a daily time step gdps forecasts are issued at utc 00 00 which translates to 17 00 local time of the previous day for the study basin utc 7 for example forecasts issued at 00 00 on april 21 2018 for the next 240 h extend from 17 00 april 20 2018 to 16 00 april 30 2018 resulting in only nine full days of predicted daily aggregated water temperatures following this breakup dates are identified from simulated water temperatures in the earlier water temperature modelling study of the athabasca river morales marín et al 2019 reported that for the historical period the breakup at the tfm always occurred when the simulated water temperature tw was between 5 c and 6 c when heat required for melting of ice was not directly accounted for thus for simplicity and consistency in the operational forecasting we consider breakup to occur when simulated tw reaches 5 c the 5 c threshold should not be considered as a standard water temperature value at which breakup occurs but rather as an informative indicator to aid the prediction of breakup in a forecasting context however the literature shows that higher water temperatures at breakup are not unrealistic in the st john river up to 2 c in the lower peace river about 6 c beltaos 2017 and in the lower mackenzie river fort good hope up to 9 5 c parkinson 1982 water temperatures at breakup have been reported beltaos 2017 states that breakup water temperatures of several degrees c may be realistic in northern canada due to the large solar radiation flux associated with spring conditions at high latitudes and the extensive lengths of major rivers of northern canada 5 results and discussion 5 1 the chronology of the 2019 breakup the monitoring of the 2019 breakup was carried out using observation flights and trail cameras from alberta environment and parks the first observation flight took place on april 5 2019 the flight covered a 155 km reach of the athabasca river from south of grand rapids at the house river confluence to grant island which is downstream of the tfm a 30 km reach of the clearwater river was also covered aerial observation and photos revealed that the ice cover was largely intact the second observation flight on april 8 showed similar results the ice cover along the river was largely intact but there were some open water leads along some rapids on april 9 a third observation flight was conducted covering a 160 km reach of the athabasca river as well as a 30 km reach of the clearwater river the ice cover in the athabasca river upstream of crooked rapids located 40 km upstream of the tfm had broken up resulting in ice runs towards the tfm but an intact ice cover persisted between crooked rapids and the tfm which obstructed flowing ice and consequently a 16 km ice jam formed the athabasca river was largely open water upstream of the jam the observation flight on the next day april 10 revealed similar patterns the athabasca river was largely open water upstream of the ice jam whereas intact ice cover remained downstream of the toe of the ice jam i e crooked rapids however the intact ice cover on the athabasca river from crooked rapids to the tfm was found to be degraded compared to observations from april 9 the images from the observation flights on april 11 showed similar results for the upstream reach of the athabasca river however the ice cover on the lower reach of the athabasca river downstream of the tfm and along the clearwater river had started to break up thermally the observation flight on april 12 showed that the ice cover on the clearwater river was actively breaking up along the athabasca river the ice jam that had formed at crooked rapids had started to release and push downstream there was about 5 km of intact ice between the tfm golf course and the mouth of the clearwater river downstream of the confluence of the clearwater river and athabasca river ice was progressing downstream and clearing the subsequent observation flights from april 14 15 16 17 and 19 showed similar results the ice jam that had formed upstream of the intact ice still remained albeit the length of the jam melted from 15 km on april 14 to 13 km on april 15 and to 12 km on april 16 similarly the jam s length reduced to 10 km on april 17 and 4 km on april 19 the 3 km of intact ice cover that had resulted in the jamming continued to persist from april 14 to 19 the ice jam finally melted completely on april 20 and on april 22 the rmwb declared the end of the breakup at the tfm for 2019 5 2 forecasting of the 2019 breakup lindenschmidt et al 2019 using the air temperature based cddm method from the meteorological station fort mcmurray a wmo id 71689 latitude 56 3912 and longitude 111 1324 computed that breakup at the tfm during the 1958 2014 period occurred when cddm values were between 137 and 451 c days we commenced the forecasting of the 2019 breakup on march 25 2019 when the cddm value i e 123 7 c days was approaching the minimum historical value i e 137 c days and when the ice cover was still intact and the water level in the river was almost constant suggesting no increased spring freshet had occurred the water temperature forecasts were then generated for each day for the next nine days fig 6 shows the forecasts that were produced on april 1 2 and 3 the april 1 forecast showed that the water temperature would be well below the breakup threshold i e t w 5 c until april 9 but some sections near boiler rapids would experience higher water temperatures on april 10 the forecast from april 2 displays similar patterns the water temperature was projected to be below the breakup thresholds until april 9 but for april 10 and 11 it predicted that water temperatures would exceed the breakup threshold around broiler rapids and crooked rapids which are both upstream of the tfm see fig 1 for their locations finally the forecast on april 3 predicted that the water temperature at the tfm would exceed the breakup threshold on april 12 suggesting the occurrence of breakup 5 3 verification of 2019 forecasts fig 7 shows the observed ice progression maps from the 2019 breakup which were compiled from observation flights the map of april 12 fig 7d shows that the clearwater river was largely open water on april 12 however some segments of the athabasca river still remained intact our forecast from april 3 fig 6 which extends from april 4 to april 12 projected a breakup at the tfm on april 12 since the water temperature forecasts were above the breakup threshold for the grid where the tfm is located the water temperatures in grid cells were simulated at a spatial resolution of 0 125 the grid where the tfm is located encompasses a large area that includes both the athabasca river and the clearwater river therefore it is not possible to compute intra grid water temperature variability however from the forecasting point of view our predictions were successful since a large portion of the area was predominantly ice free however a small stretch of intact ice cover remained since the flows in the river were not large enough to result in vertical movement of the ice cover that would have resulted in cracks and subsequent breakups there was also about 15 km of ice jam in the upper reach near cascade rapids between crooked rapids and the tfm the model however is not equipped to simulate river ice processes such as the structural integrity and strength of the ice cover thus there was an error of 10 days in the forecasting of the 2019 breakup event if end of the breakup is considered 5 4 hindcasting of 2018 breakup the onset of ice cover breakup at the tfm in 2018 started around 11 00 mountain standard time mst on april 26 it was initiated by a large water wave and ice run that exerted significant pressure on the river ice cover at the tfm consequently the ice cover broke up resulting in high water levels another water wave and ice run occurred later that same day around 18 00 mst which pushed ice and water up the clearwater river and resulted in a peak stage of about 245 m a s l around 23 35 mst lindenschmidt et al 2019 on april 28 the rmwb declared the end of river breakup for the tfm meaning ice jams no longer posed any significant flood risk at the time of the declaration on april 28 the ice cover had already receded northward downstream of the confluence of the athabasca and clearwater rivers regional municipality of wood buffalo 2018 fig 8 shows the hindcast from april 20 2018 it shows that for the next four days the predicted water temperatures were still low at the tfm and upstream grids suggesting ice cover breakup had not initiated then for april 25 it displays that water temperature in the grid upstream of the tfm were slightly increased for april 26 the hindcast reveals that the water temperature was closer to the breakup threshold i e t w 5 c at the upstream grid of the tfm but still below the breakup threshold at the tfm for april 27 it shows that the temperature exceeded the breakup threshold suggesting the occurrence of ice cover breakup the hindcast for april 28 is consistent with the prediction for april 27 and shows that water temperature would remain above the breakup threshold 5 5 verification of 2018 hindcast the verification of hindcasts was performed by comparing water temperature simulations with modis images for the tfm in the absence of real time measured water temperature data and ice progression maps modis images were obtained for april 24 25 and 26 2018 the april 24 image shows that there was an intact ice cover along the athabasca river upstream of fort mcmurray fig 9 a the prediction for april 24 issued on april 20 from fig 8 also displays the water temperature as being below the breakup threshold at the tfm fig 9a the aerial photography taken the next day reveals that the ice cover conditions remained the same for most of the river on april 25 2018 lindenschmidt et al 2019 on april 26 2018 the ice cover broke up upstream of grand rapids along the athabasca river and ran past the tfm at 12 30 local time a modis image was acquired for the study site which is presented in fig 9b although it is challenging to differentiate between an intact and a stationary consolidated ice cover from running ice due to the coarse resolution of the modis imagery analyses showed that the ice present on the river that day was not an intact ice cover but rather an ice run which was confirmed by an aerial survey in the late afternoon of the same day lindenschmidt and li 2019 most of the ice cover between grand rapids and the tfm was broken up and running towards the tfm but some intact ice covers were still observed upstream of grand rapids lindenschmidt et al 2019 see also fig 9b our hindcast in fig 8 for april 26 also reveals similar patterns to what was observed on the field fig 9b shows that the water temperature for a number of grids upstream of grand rapids is still below the breakup threshold whereas the grids where grand rapids and the tfm are located show slightly higher water temperatures however the model fails to capture breakups along some rapids that are observed in the modis image this is mainly because breakups in these rapids occurs early due to geomorphology rather than warming water temperature finally on april 28 2018 a completion of the river breakup at the tfm was declared by the rmwb from the water temperature perspective our hindcasts show april 27 to be the breakup date for the tfm whereas on the site the breakup started on april 26 and completed on april 28 thus our hindcast resulted in an error of one day for the 2018 breakup at the tfm 5 6 2016 and 2017 breakup events using the archived forecast data from 2016 to 2017 we further evaluated the performance of the model in this experiment hindcast setups were run similar to the setup for the 2018 event the breakup in 2017 occurred on april 25 the ice jam lodged at the tfm water treatment plant released early in the morning breaking the intact ice cover that had remained near the bridges and clearwater river confluence the end of breakup was announced by the rmwb on april 29 fig 10 shows the hindcast generated for april 17 2017 for the 2017 breakup our hindcast predicts april 23 as being the potential breakup date based on the water temperature breakup threshold this translates to an error of 2 days when compared with the beginning of the breakup but an error of 6 days when compared with the completion of the breakup event a similar analysis was conducted for the 2016 breakup event the model results for the 2016 breakup event reveal that the breakup threshold for simulated water temperature was surpassed on april 18 2016 see fig 11 in 2016 the breakup occurred on april 17 and the end of breakup was announced on april 19 2016 thus our prediction from hindcasting has one day bias when compared to both the beginning and end of the breakup date unfortunately adequate data were not available for more comprehensive assessments of the simulated water temperature alberta environment and parks measures surface water temperature but measurements are instantaneous at a point in time and irregular for instance for station ab07cc0030 which is located in the study site measurements were carried out on march 14 2016 at 17 15 and then on may 11 2016 at 18 15 the last available data was for march 13 2017 at 13 20 when accessed on march 10 2019 these gaps in measured data make any systematic comparisons challenging 5 7 spatial variability in breakup one of the advantages of using catchment scale models over other site specific breakup prediction methods is that it allows generation of spatial patterns of breakup theoretically from the perspective of air temperature breakup should occur first in the headwater sub basins in the south and gradually progress northward towards the basin outlet however breakup is a complex process and differs spatially across and within river catchments rokaya et al 2018b studied the trends in timing of breakup flow in canada and found earlier trends in south eastern and western canada and delayed breakup trends in atlantic canada within the mackenzie river basin which encompasses the athabasca river basin de rham et al 2008 found breakup timing to vary spatially by about 8 weeks with some years up to 12 weeks morales marín et al 2019 studied the breakup timing in the athabasca river basin from 2002 to 2012 and reported spatially diverse breakup patterns across the basin their findings show that breakup in tributaries occurs first from late march to early april in the mainstream river breakup occurred first in the upper southwestern and middle sections of the basin and gradually progressed northward toward the tfm at the headwaters breakup occurs late due to low air and water temperatures in the rocky mountains fig 12 shows spatial variability in breakup from the hindcast of april 20 2018 for the 2018 breakup event that occurred on april 28 the spatial patterns are similar to the findings reported by morales marín et al 2019 it shows that by the time breakup occurred at the tfm most of the tributary reaches had already experienced breakup in the mainstem river most of the river segments were already above the breakup threshold temperature while headwater reaches were still experiencing low temperatures the understanding and knowledge of spatial variability of breakup is very crucial from a forecasting perspective for instance if the forecast shows that water temperatures are still below breakup thresholds in the tfm but above such thresholds in upstream reaches then it can be expected that breakup will likely occur sooner in the tfm breakup subsequent ice runs and warmer water from upstream reaches affect downstream breakup timing therefore forecasting basin scale breakup dates will provide more valuable information compared to site specific methods 6 discussion river ice breakup jamming and subsequent flooding are complex phenomena primarily governed by channel morphology freeze up conditions ice characteristics climatic factors and snowmelt runoff several types of models and modelling approaches have been utilized in the past ranging from simple degree days of melting methods to sophisticated hydraulic models to predict breakup timing flow and backwater staging with advances in computing recent years have seen a surge in applications of artificial neural networks and other machine learning despite recent advances many of the existing forecasting methods that are in operational use are site specific and only applicable at local scales our integrated hydrological and water temperature modelling framework provides an opportunity to predict breakup progression for large scale catchments our operational forecasting of river ice breakup for the 2019 event and hindcasting for three breakup events in 2016 2017 and 2018 showed an average error of about 5 days for the tfm the error margins are comparatively smaller than other existing applications for instance to complement their real time ice jam flood forecasting work for the athabasca river at fort mcmurray for the 2018 breakup lindenschmidt et al 2019 used an air temperature based cddm method the cddm was computed from daily air temperatures recorded at the tfm weather station from their analyses of historical breakup events from 1958 to 2014 they identified the cddm to range between 137 and 451 c days and forecasting was commenced when cddm values for 2018 were approaching a minimum value i e 131 c days this however poses a large uncertainty since historical analyses show a large variation in cddm values 314 c days the longer range of probable breakup dates means more computing and monitoring resources are required our alternative method provides a relatively narrow window of breakup dates so extensive ice jam flood forecasting work can be commenced when positive water temperatures or breakups are forecasted we analyzed our model s performance for the tfm but we could not compare our results for other locations due to the lack of observed data the validation of models has always been a challenging issue in breakup prediction observation data are taken infrequently forcing some interpretation to rely on subjective opinions in 2019 we were able to track ice cover breakup progression from the observation flight data provided by alberta environment and parks in 2018 as part of separate ice jam flood forecasting work see lindenschmidt et al 2019 we tried to access several remote sensing products for real time monitoring and validation even though we were able to access some modis data for verification for the 2018 breakup event acquiring sentinel 1 and radarsat 2 imagery for the 2018 breakup was difficult the sentinel 1 satellite did not fly the study reach on april 26 2018 and our radarsat 2 image requests were in conflict with commercial users there is also room for model improvement morales marín et al 2019 note that inadequate data hinders correct setup of model boundary conditions there are also several other uncertainties related to meteorological forcing data propagation of errors from the hydrological to the water temperature model and inadequate incorporation of anthropogenic impacts although we have taken some measures to improve accuracy of upstream water temperatures in mountainous headwaters using remote sensing data to help improve hydrological simulations see morales marín et al 2019 for details more improvements are required to reduce uncertainties stemming from meteorological forecasts and errors propagating from mesh to rbm similarly at the catchment scale our results show that there is not a smooth continuity in water temperature due to spatial variability in air temperature however further analyses are required to determine atmospheric and hydrological linkages additionally improvements are also required to better characterize local geomorphology and incorporate ice effects in 2018 and 2019 we observed some breakup events along the rapids even when air water temperatures were below breakup thresholds such localized phenomena along the rapids are difficult to estimate with the current model configuration similarly simulating water temperatures closer to 0 c was challenging in absence of consideration of ice effects currently several water temperature models exist but to the best of our knowledge this is the first application of a coupled hydrological and water temperature model for operational breakup prediction and we believe it will spur on further research towards the application of physically based models in real time operational forecasting of river ice breakup our hydrological model mesh performs both water and energy balances and is best suited to cold region large scale catchments although our analyses were carried out at a daily time step this modelling setup also offers the possibility of running forecasts at sub daily resolution since gdps forecast data are available at 3 hourly temporal resolutions furthermore both mesh and rbm are community based open source software although we ran mesh and rbm independently efforts are ongoing to compile a rbm module in mesh which would make the coupling more efficient and user friendly we expect a new mesh version with the rbm module will be released in the near future the python toolbox used to pre process mesh inputs and rbm input information and c software to extract water temperature time series at individual cells time series of grid domains time series of individual reaches and to estimate multi annual averages and monthly averages from rbm are available from github https github com lamhydro rbmpospro we believe the open nature of these models and modelling approaches will encourage further research towards advancement of ice related flood research 7 conclusion forecasting ice cover breakup in spring is critical in cold region catchments to support the emergency responses to river ice related flooding in this study we presented a physically based coupled hydrological and water temperature modelling framework to predict the timing of river ice breakup the framework was successfully applied to forecast and hindcast breakup dates for four spring ice cover breakup events 2016 2019 along the athabasca river at fort mcmurray the coupled model shows promising results with an error of up to 10 days in its forecast and up to 6 days in its hindcasts more importantly this framework can be applied at the catchment scale enabling identification of the spatial progression of breakup across the river basin it is hoped that these research findings will advance the operational forecasting of ice related flooding which will aid preparedness and mitigation of ice jam flooding reduce flood damages and protect human life along rivers prone to major ice jam events author contributions pr and kel conceived the idea for this study and brainstormed with lm on research methodology pr set up the operational forecasting system and lm assisted in setting up the rbm model pr performed model simulations analysed the results and wrote the first draft of the paper kel and lm edited and commented on the manuscript and contributed to the text and figure presentation in later iterations declaration of competing interest the authors declare no competing interests acknowledgments the authors are thankful to staff from the river forecasting centre of alberta environment and park for providing field updates as well as ice observation reports maps and photos after each observation flight we are also grateful to daniel prinz from environment and climate change canada for providing archived meteorological forcing data for 2016 and 2017 the under ice flow measurement data were graciously provided by the wsc regional office in alberta the authors are also thankful to zhaoqin li from the university of saskatchewan for extracting modis images for the 2018 breakup the funding for this research was provided by the global water futures program at the university of saskatchewan supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103554 appendix supplementary materials image application 1 
501,spatial markov random walk models smm have been demonstrated to accurately predict conservative solute transport across a wide range of hydro geological systems with recent advances enabling the smm to model systems with linear kinetic reactive processes however the proposed reactive smm s applicability is limited to systems that can be partitioned into a series of identical periodic cells where travel times across cells are highly correlated to the solute s entrance position at the cell inlet in real geologic settings the spatial layout and size of grains varies through space decorrelating the relationship between travel time and transverse position here we generalize previous smm implementations and implement a bernoulli ctrw where transport behavior can be captured in disordered and non periodic porous media we validate our upscaled model predictions with results from direct numerical simulation of transport in a 2d porous column that cannot be partitioned into identical periodic elements we parameterize our model based on a subset of simulation statistics and explore how model accuracy changes due to our sampling method this finding yields important insights for optimizing efficiency of the upscaled transport model parameterization and can guide field sampling of geological structures as well as multiscale investigation of laboratory observations keywords reactive transport upscaled modeling statistical sampling methods 1 introduction in natural subsurface porous media settings the spatial layout and broad distribution of solid phase grain geometries and irregularity of grain shapes gives rise to a complex flow setting specifically the fluid velocity field is highly heterogeneous and characterized by high velocity preferential channels that rapidly transport dissolved solute over larger distances as well as grain cavities and dead end pores that result in stagnant waters that delays downstream transport the velocity distribution can span several orders of magnitude which ultimately manifests as anomalous i e non fickian behavior at typical experimental observation scales goltz and roberts 1986 harvey and gorelick 2000 zinn and harvey 2003 major et al 2011 understanding solute transport becomes even more complex when mass can exchange between the fluid and solid phase via chemical reactions i e when solute adsorbs and desorbs to the solid grain surface as demonstrated by roberts et al 1986 dentz and castro 2009 maghrebi et al 2014 maghrebi et al 2015 such chemical interactions further delay transport relative to the case of a non reactive solute and impact larger scale transport behavior cvetkovic and dagan 1994 capturing such complexity and anomalous behavior in mathematical modeling frameworks is challenging but remains critical for many scientific and engineering applications including contaminant transport in aquifers rathi et al 2017 biological membranes wood et al 2007 and packed bed reactors luz et al 2018 high fidelity direct numerical simulations dns are a common method for simulating flow and transport through porous media blunt et al 2013 de anna et al 2013 kang et al 2014 ceriotti et al 2019 in dns the flow field porous medium geometry and evolution of a solute plume as it traverses space time are all fully resolved an advantage of dns is that physical mechanisms that drive transport can be studied separately and are well described providing an in depth understanding of the hydro geologic system and associated transport processes however such simulations demand large computational resources and so applying dns at the field scale is practically impossible hence for studies at the field scale upscaled models which effectively represent physical processes without explicitly resolving them provide an attractive alternative a variety of upscaled models have emerged to capture anomalous transport in subsurface hydrological systems including among others multi rate mass transfer mrmt haggerty and gorelick 1995 fractional advection dispersion benson et al 2000 continuous time random walk ctrw berkowitz et al 2006 and time domain random walk tdrw models noetinger et al 2016 here we consider the ctrw modelling framework in a ctrw a solute plume is typically conceptualized as a large ensemble of particles who transition through time and space by sampling a waiting time velocity distribution the particle s velocity series can be modeled as a spatial markov process where at each step a particle traverses a fixed streamwise distance and its associated travel time is modeled with an effective travel time distribution parameter successive model jumps are assumed independent when the jump distance exceeds a velocity correlation length scale berkowitz and scher 1997 berkowitz et al 2006 and correlated at smaller scales le borgne et al 2008a 2008b kang et al 2015a when jumps are sufficiently small such that correlation effects are important the velocity correlation structure is often imposed by assuming a velocity transition matrix that is stationary in space le borgne et al 2008a bolster et al 2014 sund et al 2015 or assuming velocity transitions via a bernoulli process dentz et al 2016 morales et al 2017 hyman et al 2019 kang et al 2019 puyguiraud gouze dentz 2019 puyguiraud et al 2019 ctrw models have demonstrated success in a variety of porous and fractured media studies le borgne et al 2011 kang et al 2015a sherman et al 2018 but often are applied in synthetic settings where the input correlation parameters can be directly measured to date the majority of ctrw studies have focused on conservative transport recently ctrw models that consider reactive transport have also emerged edery et al 2010 hansen et al 2014 aquino and dentz 2017 sund et al 2017 sherman et al 2019 wright et al 2019 here we focus on upscaling reactive transport where solute mass can exchange between the fluid and solid grain boundaries via a chemical reaction we build upon a previous study where the smm with adsorption and desorption was considered in an idealized periodic wavy channel sherman et al 2019 in this modelling approach a particle s inlet position at the periodic channel s throat informs the particle s random travel time across the cell the particle s outlet position which informs the next step in the markov chain and the number of times a particle strikes the boundary which subsequently controls adsorption desorption processes the approach was successful in the idealized wavy channel considered because the travel time statistics in each periodic cell are identical hence the proposed model by sherman et al can be fully parameterized through direct numerical simulation of transport through one periodic cell in more complex porous media geometries the spatially varying distribution of grains means the relationship between a particle s trajectory statistics and a particle s transverse position are no longer spatially stationary and thus transport behavior is further complicated in more realistic heterogeneous geologic settings the adsorption desorption smm therefore must be generalized if to be applied in realistic non periodic domains this study has two primary objectives 1 we extend the adsorption desorption smm so that upscaled particle trajectories are not dependent on transverse position to do so we introduce a bernoulli smm and relate a tortuosity parameter which describes particle motion in the transverse direction to the number of boundary hits when tortuosity is high particles have increased pathline travel distances and are therefore more likely to contact the grain boundaries and react 2 we investigate the impact of sampled statistics on model parameterization and uncertainty smm parameterization has to date been achieved empirically through direct numerical simulations le borgne et al 2008a kang et al 2014 sund et al 2016 most et al 2019 inverse modeling algorithms sherman et al 2017 and parametric analytical markov models kang et al 2015b dentz et al 2016 hakoun et al 2019 here we focus on parameterization via numerical simulations as these allow transport to be quantified in great detail and enable a full exploration of the parameter space specifically we sample the studied domain randomly to parameterize the ctrw by assigning equal probability to each location in the domain i e drawing slice locations from a uniform distribution then we observe how for a given sampling area the spatial distribution and size of sample slices influence ctrw predictions we find that ctrw models parameterized via many small sampling slices distributed through space improve model prediction capability and reduce uncertainty in predictions here the studied domain is a stochastically generated 2d porous media column where particle trajectories vary spatially although still highly idealized the spatial heterogeneity of the porous medium structure makes this system considerably more complicated than previously studied periodic domains and the proposed ctrw framework provides a step forward towards modeling adsorption desorption transport in such complex systems 2 pore scale setting and observables this section describes the methodology employed to model pore scale solute transport as well as providing the definitions for the observables that are later used in sections 3 and 4 within upscaling our pore scale simulations rely on an artificially generated porous geometry and the related flow field for more details see a 1 a 2 2 1 pore scale transport simulation we model the evolution of the solute plume through the porous media column with sorbing boundaries as khan 1962 zhang et al 2017 1 c x t t u x c x t d c x t x γ f l u i d 2 s x t t λ s x t α c x t d c n x σ s u r f a c e the solute concentration field c x t evolves through pore space γ fluid and time due to the fluid velocity u x and molecular diffusion d i e solute transport through the fluid is governed by the advection diffusion equation the fluid velocity is here computed numerically as detailed in a 2 solute concentration on the solid surface σ surface labeled s x t changes as solute mass exchanges between the fluid and solid phases with desorption rate λ and adsorption rate α hence the rate of mass exchange between the solid and fluid phases is simply the difference between the adsorption and desorption rates which mass conservation requires to be equivalent to the diffusive flux of solute at the boundary d c n where n is the unit normal at the boundary in the numerical experiments considered here solute enters the column inlet via a flux weighted pulse injection i e the solute concentration injected at the inlet is proportional to the velocity we solve eqs 1 2 with a lagrangian approach the solute plume is discretized into many particles who transition through space and time according to a langevin equation 3 x i n 1 x i n u i δ t η i 2 d δ t 4 y i n 1 y i n v i δ t ξ i 2 d δ t 5 t i n 1 t i n δ t t i where particle i has position x y and velocity u v at model step n diffusion enables particles to deviate from velocity streamlines which here is modeled as a brownian process η ξ are independent identically distributed random variables sampled from a standard normal distribution zero mean and unit variance additionally diffusion enables particles to strike the solid phase boundary when such interaction occurs a particle attaches to the solid phase with sorbing probability psorb if sorption occurs the particle remains attached for time t where t is a waiting time sampled from an exponential distribution with mean λ 1 if no sorption occurs which is always the case for conservative transport elastic boundary conditions and a zero waiting time t 0 are imposed we elaborate on details related to numerical implementation of adsorption desorption processes in the following section 2 1 1 we refer to simulations solved with eqs 3 5 as direct numerical simulations dns in subsequent sections 2 1 1 adsorption desorption processes we model adsorption and desorption processes following boccardo et al 2018 every instance a particle interacts with the solid phase boundary via diffusion it sorbs to the boundary with a probability psorb to leading order for a sufficiently small time step psorb is given by 6 p s o r b α π δ t d where α is a sorption rate d is a diffusion coefficient and δt is the simulation time step size where δt d α 2 when a particle contacts the solid phase we draw a random number ui from a uniform distribution u 0 1 and the particle sorbs if ui psorb and elastically reflects if ui psorb if a particle adsorbs to the boundary its waiting time ti is then sampled from an exponential function with mean λ 1 i e ψ t λ exp λ t 7 t i ψ t u i p s o r b 0 u i p s o r b note that ψ t is consistent with the assumption of linear desorption kinetics we characterize the importance of adsorption desorption processes through the porous media column with dimensionless damköhler numbers daa dad which compare adsorptive and desorptive timescales with diffusive time scales here high fidelity simulations are run for a range of daa and dad spanning several orders of magnitude o 10 10 3 allowing us to study the evolution of the solute plume as adsorption desorption processes become increasingly important respectively we define the adsorptive and desorptive damköhler numbers as 8 d a a ℓ c α π d d a d ℓ c 2 λ d here ℓ c is a characteristic length quantifying the average distance between the fluid phase with a solid boundary which is determined by the domain geometry here ℓ c 3 8 10 5 m similarly we investigate how reactive transport behavior changes as diffusion strength increases we quantify the relative strength of diffusion with a péclet number which gives the ratio of diffusive to advective transport time scales the péclet number pe depends on the mean fluid velocity accounting for x and y directions u d and ℓ c and is given as 9 p e u ℓ c d we tune d α λ in order to change dimensionless quantities 2 2 transport statistics one of the objectives of this study is to parameterize an upscaled model that accurately predicts transport through the porous column while minimizing the sampling area from which upscaled parameters are derived to do so we sample lagrangian statistics along particle trajectories from the high resolution simulations and quantify how particle ensemble statistics evolve through space and time in detail then we parameterize the upscaled model with only a subset of the observed data specifically we install control planes perpendicular to the mean imposed pressure gradient and measure transport statistics at the first crossing of each control plane control planes are spaced over a distance 2 04 10 5 m which is sufficiently o 10 times smaller than a velocity correlation length scale such that a particle s velocity evolution can be faithfully portrayed here the solute plume is comprised of many particles b with a collective mass m the ensemble of particles is denoted ω b the total number of particles o 104 is sufficiently large such that transport statistics converge below we detail the employed parameters i e the probability density functions pdfs of particle velocities travel times paths tortuosity and the velocity correlation function 2 2 1 velocity pdf at time t a particle s velocity depends on its position x we define a particle s instantaneous lagrangian velocity v x as the fluid velocity at x v x x b u x b where b denotes a vector of particles lagrangian velocities are sampled for particle first crossings of control planes meaning all points on a given control plane have the same x coordinate for a control plane located at x x i we construct the lagrangian velocity distribution ψ v xi 10 ψ v x i 1 m ω b d b δ v v x x i b note here that the ψ v xi distribution is spatially dependent as it depends on xi 2 2 2 travel time pdf closely related to velocity is the travel time distribution here travel time over two control planes refers to the elapsed time between first particle breakthrough at each control plane we define the first arrival time τcp of a particle at a control plane at x x i as 11 τ c p x i b t λ x i b λ x i inf x x i b x i the first arrival time of a particle is simply the first instance in which its x position exceeds xi the distribution of first arrival times at a control plane denoted in this study as a breakthrough curve is defined 12 ψ t x i 1 m ω b d b δ t τ c p x i b tracking particle breakthrough times enables us to calculate the elapsed travel time δτcp x i j between any two control planes at xi and xj xj xi as δ τ c p τ c p x i b τ c p x j b then for any spatial slice x i j spanning between xj and xi we define the distribution of travel times for the ensemble of particles 13 ψ δ τ c p x i j 1 m ω b d b δ δ τ c p δ τ c p x i j b note when x j 0 the travel time and breakthrough time distributions for xi are equivalent ψ δ τ c p x i 0 ψ t x i 2 2 3 tortuosity pdf the irregularity and distribution of the solid phase creates complex flow features resulting in streamlines not aligned with the primary pressure gradient i e streamlines are not straight but meander also in the transverse direction furthermore diffusion allows solute to sample many streamlines as a result a particle s total travel distance is greater than the linear distance in the streamwise direction tortuosity quantifies the ratio of the total pathline distance to the linear streamwise travel distance and this is closely correlated with travel time the total travel distance δs between successive model steps n and n 1 in the high resolution simulation is estimated with a euclidean distance δ s n n 1 x n 1 x n it follows that the total pathline distance s n n k from model step n to n k is the summation of distances for each model step s n n k i 0 k 1 δ s n i n i 1 then a particle that first crosses a control plane at x x j at model step n and first crosses a control plane x x i at model step n k has a corresponding travel distance between control planes of s x i j s n n k note in the literature there are various definitions of tortuosity that have been used to study different subsurface properties see ghanbarian et al 2013 here we adopt a pathline dependent effective tortuosity that aligns with langrangian observations and can naturally be included as an upscaled model parameter sherman et al 2020 a particle s effective tortuosity χ x i j between control planes xi and xj is the total pathline travel distance s x i j divided by the linear distance between control planes δ x i j x i x j 14 χ x i j s x i j δ x i j the effective tortuosity distribution between any two control planes at xi and xj is defined as 15 ψ χ i j 1 m ω b d b δ χ i j χ i j b for any slice spanning control planes xi and xj we can define a local mean effective tortuosity χ i j l where angle brackets denote an arithmetic average across the particle ensemble 2 2 4 velocity correlation recall the travel time and tortuosity distribution are defined over a slice spanning two control planes we set control planes relative to a velocity correlation length scale which is calculated from velocity correlation function 16 c v v s 1 n n 1 n 0 d s v s v s s σ v v 2 where v refers to a velocity fluctuation about a mean σ v v 2 is the velocity variance s is the spatial lag distance and n is the number of particles allowing for the average across the particle ensemble the velocity correlation length scale lc is then the integral of the correlation function 17 l c 0 c v v s d s the velocity correlation length scales are 1 51 10 4 1 89 10 4 1 87 10 4 m for p e 100 1000 and respectively demonstrating that velocity correlation features more rapidly diminish with increasing diffusive strength the entire porous media column is approximately o 100 o 10 times the velocity correlation scale along the longitudinal and transverse directions respectively 3 flow and transport observations in this section we investigate the spatial variation of eulerian and lagrangian flow and transport properties we partition our domain into non overlapping equally sized slices and measure the spatial evolution of transport statistics control planes are spaced at distances of 0 1 1 and 10 times the correlation length scale lc here we highlight distributions for control planes spaced 0 1lc and p e 1000 3 1 eulerian observations the top three rows in fig 1 show eulerian characteristics of the column i e the distribution of grains porosity and the eulerian velocity field the phase field displays a disordered structure the grain geometries shown in blue are highly irregular and each grain has a unique shape and size the porosity ϕ in each sample slice defined as the fluid area divided by the entire sample area ϕ a f l u i d a s a m p l e varies through space with values ranging within 0 23 0 97 the entire column has a mean porosity of 0 64 the fluid velocity field is characterized by a broad distribution spanning over 10 orders of magnitude where high velocity preferential pathways connect and cut through the surrounding slow moving waters relatively high velocity channels form when the pore space gap between grains constricts due to the incompressibilty constraint for this reason high velocity channels form in areas of low porosity the relatively small longitudinal sample size of the sample facilitates the occurrence of channelling ceriotti et al 2019 whose occurrence is documented in three dimensional media siena et al 2019 3 2 lagrangian observations here we show how lagrangian particle statistics are influenced by the underlying flow field structure 3 2 1 spatial evolution of puyguiraud gouze dentz 2019 lagrangian pdfs fig 1 displays the spatial evolution of the travel time tortuosity and velocity pdfs for control planes spaced by distance 0 1lc here velocity pdfs correspond to values at a particle s first crossing of a control plane while travel time and tortuosity are determined from particle trajectories between successive control planes we immediately observe that the eulerian flow field structure directly influences lagrangian statistics the transverse breakthrough position distribution tbpd through space highlights where particles cross each control plane in the transverse direction colors correspond to log probabilities the largest tbpd values correspond to regions of high fluid velocity demonstrating that mass is channelized in high velocity pathways as a result of such channelization the lagrangian velocity pdf transitions through space as observed in puyguiraud et al 2019 and hakoun et al 2019 when the velocity field displays strong dominant channeling features such as near x 0 001 0 009 the statistics of the travel time tortuosity and velocity homogenize with the width of the pdfs decreasing such behavior is expected because mass preferentially enters fast channels meaning all channelized mass shares similar trajectories these regions are made up of relatively fast velocity zones which results in faster mean travel times consequently the lagrangian velocity and travel time distribution are negatively correlated i e as mean velocity increases mean travel time decreases because particles are traveling faster similarly the mean tortuosity decreases with increasing channelization as channelization is associated with high velocity pathways the fastest channels tend to be aligned with x meaning particles that enter such channels will have local tortuosity values closer to 1 in regions of slow flow diffusion becomes increasingly important for particle trajectories which increases tortuosity values on average the largest tortuosity values are o 100 meaning the total particle travel distance is two orders of magnitude greater than the x linear distance these large values can occur near pore cavities where the velocity field can advect particles counter to the primary pressure gradient and cause particles to recirculate through slow waters hence particles travel relatively long distances at slow velocities which results in large travel times that manifest in late breakthrough tailing behavior 3 2 2 solid fluid phase interaction in addition to the aforementioned lagrangian statistics we also track the number of times a particle hits a boundary from inlet to outlet recall that adsorption to the solid phase boundary only occurs when diffusion brings a particle into contact with the solid phase if a particle adsorbs then it waits according to an exponential distribution which delays transport see section 2 1 1 the number of times a particle hits the solid phase is therefore important for the reactive case fig 2 shows the joint pdf between number of hits and tortuosity for pe 100 1000 colors correspond to log probabilities here hits and tortuosity are measured over control planes with spacing 0 1lc for both pe the mean number of hits tends to increase with tortuosity demonstrating that particles are more likely to interact with the solid phase as particle travel distance increases however the majority of particles display local tortuosity values of o 1 this has important implications for adsorption desorption of particles as it implies that slow particles with high tortuosity preferentially adsorb and are delayed in contrast fast particles preferentially transport via high velocity channels and therefore have less interaction and are less likely to adsorb to the solid boundary on average the joint pdfs for both pe display similar structures however mean tortuosity and hits increases for the pe 100 case 4 upscaled stochastic modeling bernoulli ctrw this study s objective is to predict transport at the column scale through an upscaled model parameterized using a limited portion of the 2d column we also investigate how the spatial distribution and area of sample slices influences upscaled model parameterization and prediction these objectives are studied in the context of a continuous time random walk ctrw which is a lagrangian stochastic framework that naturally aligns with particle tracking methods applied in the high resolution numerical simulations described in section 2 1 in a ctrw framework the solute plume is conceptualized as many discrete tracer particles that transition through time and space according to probabilistic rules as formulated in this study particles traverse a fixed distance ℓ at every model step with an associated travel time τ which is sampled from a travel time distribution ψ τ to upscale adsorption desorption processes at every ctrw step we model the number of times a particle hits the solid boundary and each hit has an associated waiting time sampled from a distribution ψ τsorb here set as an exponential distribution upscaled particle trajectories follow a langevin equation 18 x i n 1 x i n ℓ 19 t i n 1 t i n τ i n 1 j 1 n h i t s k τ s o r b j at every model step particles jump a fixed distance ℓ in time τ which is randomly sampled from a travel time distribution ψ τ the cumulative waiting time j 1 n h i t s k τ s o r b j delays particle transport due to adsorption desorption and equals zero for the conservative case in this framework the lagrangian velocity pdf is not explicitly used as the velocity information is captured in the travel time distribution further details on implementation are discussed in the following sections 4 1 implementation of ctrw correlation structures 4 1 1 velocity correlation it has been well documented that strong correlation properties exists in porous and fractured media which may significantly impact solute transport behavior at typical scales of interest le borgne et al 2008a de anna et al 2013 bolster et al 2014 kang et al 2014 morales et al 2017 as a result for scales less than the velocity correlation length fast slow particles preferentially remain fast slow thus when a ctrw jump distance is significantly less than the velocity correlation length scale ℓ lc velocity correlation needs to be captured to faithfully portray transport behavior the bernoulli ctrw model which we will apply in this study is a spatial markov model smm that accounts for velocity correlation in a parsimonious simplistic framework and was first introduced by dentz et al 2016 in the bernoulli ctrw framework a particle s velocity transitions via a bernoulli process that is the particle persists with the same travel time as the previous model step with probability p and samples from a global distribution otherwise 20 τ i n 1 τ i n p ψ τ 1 p we assume that particle velocity transitions at a constant spatial rate 1 lc with p exp ℓ l c which is consistent with past studies hyman et al 2019 kang et al 2019 note that including correlation between successive model jumps adds negligible computational costs when compared with an uncorrelated model 4 1 2 tortuosity hit correlation and adsorption desorption fig 2 shows a clear correlation structure between particle tortuosity and the number of times a particle strikes the solid phase boundary upscaling the number of boundary hits is important for estimating retardation effects due to adsorption desorption to impose such correlation effects we sample from a conditional tortuosity hit distribution to upscale boundary hits we sample dns lagrangian statistics of the porous column and in each sampling slice store a travel time tortuosity number of hits tri pair for each particle trajectory at every model step each particle samples a travel time and its associated tortuosity value in the case of reactive transport the sampled tortuosity conditionally informs the number of hits specifically we implement hits by sampling a conditional distribution ψ χ hits which is created from the tri pair sampled data the waiting time due to adsorption desorption is calculated from the number of hits we assume knowledge of daa dad and make them identical to the values prescribed in the pore scale numerical simulations the cumulative waiting time for a single particle at one ctrw step is j 1 n h i t s τ s o r b j for every hit a particle adsorbs with probability psorb psorb is based on daa and is calculated with eq 6 if the particle adsorbs then we sample a waiting time from an exponential distribution ψ τ s o r b λ exp λ τ s o r b with λ corresponding to dad hence sampling travel time tortuosity and boundary hits for each particle trajectory is sufficient to parameterize an upscaled reactive ctrw model 4 2 bernoulli ctrw parameterization in this study the column is partitioned into equally sized slices with x length of size ℓ and with y height equivalent to column height we select n slices which are used for ctrw parameterization the cumulative sampling area a is smaller than the entire column area the slice locations are randomly determined by sampling a uniform distribution i e every spatial position in the domain has equal probability of being selected in each slice the ensemble of travel times τ tortuosity χ and number of boundary hits statistics are stored this provides sufficient data to construct ψ τ and ψ χ hits and apply the adsorption desorption bernoulli ctrw framework 4 2 1 sampling sensitivity a primary objective of this study is to complete a sensitivity analysis relating sampling statistics to ctrw predictions to do so we parameterize ctrw models with different sampling slice sizes ranging from o 1 to o 10 times the velocity correlation length scale furthermore we investigate how increasing the sampling area used for parameterization statistics affects ctrw predictions fig 3 provides a conceptual visualization of the method for sampling the domain in the top and bottom subfigures a fixed sampling area a is selected highlighted in red the sampling area in the top subfigure is comprised of one large slice while the sampling area in the bottom subfigure consists of many smaller slices the spatial distribution of slices is randomly selected from a uniform distribution simulation statistics corresponding to the randomly selected samples parameterize our bernoulli ctrw model we investigate whether the size and spatial distribution of the slices influences ctrw predictions since the slice locations are randomly sampled we run 100 realizations for each sampling area slice size combination each realization has a different set of sampling slice locations this enables us to quantify both the accuracy and sensitivity of ctrw predictions over an ensemble for a given sampling area slice size combination parameter we sample the existing dns statistics corresponding to the full simulation of the porous media column and assume knowledge of the velocity correlation length scale a priori i e lc is not realization dependent 5 results in this section transport predictions of the proposed ctrw are compared with pore scale simulations for different pe daa dad combinations first we present the conservative transport case and test model sensitivity to input parameters then we test ctrw model performance under varying adsorption desorption rates the section concludes by examining how diffusion influences both reactive and conservative transport behavior 5 1 ctrw predictions and uncertainty for conservative transport we parameterize a ctrw model and predict first passage time distributions at the porous media column outlet the ctrw performance is tested for different sample slice size cumulative sampling area combinations here we choose sampling slice sizes of ℓ 0 1 l c 0 2 l c 0 4 l c 1 l c 10 l c the cumulative sampling area a has size 1 8 2 8 4 8 the area of the total domain recall each sampling area is divided into discrete slices with width in x ℓ and height in y equal to the total column height the top row of fig 4 shows the entire ensemble distribution gray and mean red ctrw outlet breakthrough curve predictions for a 1 8 and slice sizes ℓ 0 1 l c 1 l c 10 l c for conservative transport for each slice size we run 100 ctrw realizations with each realization sampling the domain randomly but uniformly the mean breakthrough curves accurately predict tailing behavior for all slice sizes however early and peak arrival are only captured when the sample slices are smaller than the velocity correlation length scale additionally the spread across ctrw realizations decreases as the sample slice size decreases as demonstrated by the reduced gray shaded area in the right subfigure one large continuous slice with size 1 8 the length of the column is sampled to parameterize the ctrw predictions are least accurate and uncertainty is greatest for this case recall that the mean travel time distribution evolves through space and so sampling one region of the domain may not be representative of the global column statistics when the sampled statistics are not representative ctrw predictions will fail these results suggest that for a given sampling area we can reduce uncertainty across realizations by taking many small slices that are spatially distributed such a sampling method yields statistics more representative of the global column system at large the solid lines in the left panel of fig 5 display the mean squared error of conservative breakthrough curve predictions for ctrw models parameterized with different sample slice lengths m s e 1 n t 1 n c d n s c c t r w 2 where brackets denote the average over realizations and n is number of time points where particle concentration values are recorded colors denote the total sampling area a 1 8 2 8 4 8 for black red blue lines respectively as expected model prediction accuracy improves as the sampling area increases because the sampled statistics become more representative of the global statistics and thus spatial heterogenities in lagrangian statistics are better captured for all sampling areas the largest ctrw error occurs when the parameterization slice size is o 10 times larger than lc note that when the slice size is much greater than the velocity correlation length scale ℓ lc the probability of a particle velocity persisting over successive model steps goes to zero and so the bernoulli ctrw effectively reduces to an uncorrelated random walk the most accurate predictions when the tortuosity correction is not applied always occur when sample slices are smaller than lc however the optimal slice sizes varies for different sampling areas this result demonstrates the importance of parameterizing models with representative statistics an uncorrelated ctrw model most closely resembles using statistics from one continuous slice however even in this idealized porous media system parameterizing the ctrw with sample sizes larger than the correlation scale provide less accurate predictions on average suggesting that correlated ctrw models may be practically more suitable for prediction purposes especially in this context where computational costs between sampling methods for a fixed sampling area are approximately equal in addition to reduced accuracy parameterizing the ctrw with a slice size greater than the velocity correlation length scale ℓ lc generates increased uncertainty across realizations we quantify uncertainty with the time averaged variance of log arrival time pdf values the solid lines in the right panel in fig 5 display the variance vs slice size length for different sampling areas as the sampling area increases the uncertainty in prediction across ctrw realizations decreases because sampled statistics become more representative of global statistics and better capture spatial heterogeneities that impact transport behavior on the other hand the ensemble variance decreases as the sampling slice size decreases this suggests that when the sampling volume needed for representative statistics is unknown obtaining statistics from many small slices randomly distributed throughout the domain will reduce prediction uncertainty using data from fewer larger sample slices does not fully capture the entire spectrum of spatial heterogeneity and thus generates increased variability across the ensemble of predictions 5 2 accounting for spatial heterogeneity via tortuosity the spatial variability of lagrangian and eulerian statistics makes it challenging to characterize the upscaled model by sampling only a portion of the porous media column in this section we explore methods that correct the previously sampled distributions to account for spatial variations in the flow field structure thereby improving mean model predictions and reducing the spread across the ensemble we do so by relating tortuosity and travel time next in section 5 2 1 we assume full knowledge of the mean tortuosity field to correct the sampled data we acknowledge that such information is currently practically unobtainable without relying on full simulation of the domain thus this section serves merely as a proof of concept then in section 5 2 2 we evenly space sampling areas throughout the entire column and find that this method on average better captures spatial heterogeneity of transport characteristics than sampling randomly 5 2 1 correcting sampled data via tortuosity a proof of concept the spatial evolution of the travel time and tortuosity distributions and their associated mean values are positively correlated i e large travel distances are more likely to have large travel times here we investigate how correcting for spatial variations in mean travel distance influences upscaled model prediction to do so we assume full knowledge of the mean tortuosity field we divide the porous media column into slices with length ℓ and each slice has a mean tortuosity value χ x g then as done previously we sample a portion of the domain and construct a travel time distribution ψ τ from high resolution simulations we store an additional piece of information the mean tortuosity for each local sample slice χ l hence we now sample a travel time mean tortuosity pair ψ τ χ l our sampled travel time in the ctrw is then scaled according to the current location of the particle reflecting that areas of the domain where particles travel a farther shorter distance should have larger smaller travel times the scaled ctrw time equation becomes 21 t i n 1 t i n τ i n 1 χ x n 1 g χ x l here our scaling factor χ x n 1 g χ x l is simply the tortuosity corresponding to the particle s current x position over the mean tortuosity of the sampled travel time slice if the sampled mean tortuosity is smaller than the mean tortuosity of the particle s current position then the time is increased by the correction factor to account for the fact that the particle will likely experience a higher tortuosity due to its current position in the column in essence this scaling ensures that the ensemble of upscaled particle trajectories has a mean tortuosity that matches what was observed in the pore scale simulation the bottom row of fig 4 compares the mean ctrw predictions with simulated breakthrough curves and displays the spread across realizations when a tortuosity correction is applied using eq 21 for all sample slice sizes the tortuosity correction improves mean model performance most notably for early arrival times more importantly the spread across realizations also decreases with the tortuosity correction hence the correction both improves model accuracy and decreases uncertainty by accounting for the spatial heterogeneity of tortuosity through the column the dashed lines in fig 5 shows the mean model error vs slice size as well as the realization variance vs slice size for all sampling area slice size combinations the tortuosity correction reduces model error and the prediction uncertainty knowledge of the spatial heterogeneity of tortuosity has a marked impact on the model results similar to the trends observed when no correction is applied the model uncertainty increases with increasing slice size although the rate of such increase is largely reduced in this corrected model however a different trend emerges for mean squared error with the tortuosity correction the mean ctrw error decreases with increasing slice size when sample slice sizes are larger than the velocity correlation length scale ℓ lc such correlation effects are built into the measured statistics and do not need to be explicitly represented in the upscaled model hence for large slice sizes only the larger scale spatial transitions in lagrangian statistics caused by heterogeneities of the fluid flow structure need to be captured to upscale transport which is explicitly done in this method by scaling travel times based on the tortuosity field 5 2 2 evenly distributed sample slices knowledge of the full lagrangian statistics is practically unobtainable in most non insilico or field settings we investigate here strategies to obtain a representative sample of lagrangian statistics by constraining the model on partial information of the tortuosity field our first attempt was to reconstruct the full log tortuosity field with conditional kriging i e using sample slices as conditioning points the results of these numerical experiments are here omitted since the model performances were similar to those obtained with random sampling and no tortuosity correction i e shown in fig 4 top row here we then simply distribute sample slices such that they span the entire domain and are spaced by equal distance again a fraction a of the porous media domain is sampled and divided into n slices of equal size a n the center of each sample slice is set such that it is equi distant from neighbor slices with distance between centers equal to l n where l is the column length we compare ctrw prediction accuracy when parameterized with the evenly spaced samples with predictions via uniformly distributed samples for various sample area slice size combinations fig 6 solid lines show the mse with the uniformly random method for sample area fractions a 1 8 2 8 4 8 and dashed lines correspond to the evenly spaced sampling method for all slice lengths ℓ sampling the porous media with equi distant spaced sample slices improves model predictions especially as sample slice size increases this is because the equi spaced sampled tortuosity distribution is more representative with respect to its global generally unknown distribution than when random sampling is performed note however that results obtained in this case rely on a single realization and may be subject to the particular case analyzed here i e they might not be generally exportable to any other setting 5 3 adsorption desorption results the bernoulli ctrw model is extended to include adsorption desorption processes for particle boundary interactions we test reactive ctrw performance for a wide range of daa dad values and combinations recall that adsorption desorption is only possible if a particle strikes the solid boundary thus predicting the number of particle hits is important for accurately capturing transport of the reactive solute plume 5 3 1 upscaling hits particle boundary interactions for every sample slice we track tortuosity travel time and particle boundary hits and use these values to estimate the global distribution from which the ctrw is parameterized we test two methods to upscale the number of particle hits 1 at every model step particles sample randomly from a hit distribution ψ hits which is the aggregate of hit distributions for each sample slice and independent from all other variables 2 at every model step we sample from a hit distribution dependent on tortuosity ψ hits tortuosity i e we sample a tortuosity value which then informs the number of hits using a realization dependent conditional hits tortuosity distribution with similar features to the one shown in fig 2 we estimate the total number of times a particle hits a grain boundary from inlet to outlet with the two upscaled methods and compare with high fidelity pore scale simulations fig 7 compares the distribution of total hits for the two upscaled methods vs simulation data for two sample slice sizes the total hit distribution spans several orders of magnitude for sample slices of size ℓ 1 l c the peak of the distribution is overestimated and low hit values are underestimated when the bernoulli model samples ψ hits however conditioning the hits on particle tortuosity enables the upscaled model to faithfully represent the particle boundary interactions this indicates that over small scales the relationship between hits and tortuosity should be considered in upscaled models because particles with very tortuous paths are more likely to interact with boundaries for the sample slice of size ℓ l c both upscaling methods accurately capture the total hit distribution suggesting the number of hits and tortuosity correlation structure homogenizes after particles have traveled sufficient distance in the longitudinal direction 5 3 2 upscaling transport with sorption desorption we predict breakthrough curves at the column outlet with an adsorption desorption bernoulli ctrw and compare with high fidelity simulations simulations are tested for all combinations of d a a 10 10 2 10 3 and d a d 10 10 2 10 3 additionally we consider extreme conditions where particles sorb with probability 1 if they contact the solid boundary p s o r b 1 and have large waiting times d a d 10 5 this extreme case allows us to test the veracity of the proposed upscaled ctrw model both adsorption and desorption rates influence column scale particle transport a particle that hits the solid boundary nhits time has an expected delay in transport e t d e l a y n h i t s p s o r b λ 1 hence transport is delayed as psorb controlled by adsorption rate and λ 1 controlled by desorption rate increase fig 8 displays predicted and simulated breakthrough curves for selected daa dad combinations and p e 1000 the left and right columns in fig 8 correspond to sample slice sizes of length ℓ 1 l c 1 l c respectively the solid line corresponds to ctrw predictions without correcting for spatial variations in the tortuosity field and the dashed lines indicates a tortuosity correction when the daa dad are relatively low o 100 breakthrough curves remain relatively similar to the conservative case as daa dad increase the mean arrival breakthrough time increases and tailing is enhanced early time breakthrough is not significantly delayed by adsorption desorption processes as these early particles are transported via fast channels where interaction with the solid boundary is limited or even zero additionally we consider a case with p s o r b 1 and d a d 10 5 meaning every time a particle hits a boundary it is guaranteed to adsorb and will likely remain attached to the boundary for a relatively large time in this case there is a distinct peak at early times followed by enhanced tailing the distinct peak corresponds to particles that have little or zero interaction with the boundary while particles that hit the boundary have longer waiting times the largest breakthough times are delayed from o 104 in the conservative case to o 106 the reactive bernoulli ctrw model captures breakthrough curves across the d a a d a d parameter space trends are similar to those observed in the conservative case the spread across ctrw realizations increases when sample slice size ℓ increases additionally the tortuosity correction dashed lines improves breakthrough prediction especially at early times recall that early travel times are those particles in fast channels who are less likely to interact with the solid boundary and thus the tortuosity correction has the same effect as in the conservative case however as the adsorption desorption processes play an increasing role i e as daa dad increase the tortuosity correction becomes less significant because the reactive boundary processes dominate retardation 5 4 impact of péclet number on reactive transport we test the influence of diffusion on reactive transport by comparing breakthrough curves for p e 100 1000 recall that diffusion is the mechanism which enables particles to interact with solid boundary hence when diffusion strength increases the mean tortuosity and number of times a particle hits the boundary increases on average fig 2 the mean number of hits from inlet to outlet is 871 281 for p e 100 1000 respectively fig 9 shows outlet breakthrough curves for the p e 100 blue and p e 1000 red cases under different adsorption and desorption rates in the conservative case diffusion does not have a significant impact on particle breakthrough times and so breakthrough curves appear similar across pe as psorb and waiting time dad increase the peak breakthrough concentration is delayed and tailing is enhanced because particles now have increased likelihood of sorbing to grain boundaries in the extreme case where p s o r b 1 and λ 1 166 the peak concentration is significantly delayed for the p e 100 case increasing diffusion strength increases the likelihood of hitting the boundary which significantly delays transport when the adsorption rate is high and desorption rate is low the reactive bernoulli ctrw model s robustness is tested by varying pe fig 9 for all the cases considered the ctrw model solid lines captures reactive transport through the porous media column here the ctrw model is parameterized with sample slice size ℓ 0 1 l c and sample area fraction a 1 8 again we run 100 ctrw realizations with each realization sampling the domain randomly from a uniform distribution of the available space the spread across realizations remains relatively small for this sample slice hence parameterizing the reactive bernoulli ctrw with slices smaller than the correlation length scales can accurately predict transport under a wide variety of conditions 6 discussion and conclusions we run direct numerical simulation of transport through a 2d porous column with adsorption desorption the spatial distribution of grains and irregularity of grain geometry results in a fluid velocity field that is complex and spatially variant consequently lagrangian transport statistics are spatially dependent non stationary and capturing such features in an upscaled modelling framework is challenging we use direct numerical simulations to explore reactive transport through the column which guides development of a more general spatial markov model with adsorption desorption the simulations show 1 the velocity travel time and tortuosity distributions are highly intertwined high velocity channels are mainly aligned with the primary pressure gradient meaning particles transported in such channels travel a shorter distance resulting in relatively fast travel times 2 high velocity regions channelize mass which homogenizes corresponding lagrangian statistics as a significant portion of mass is forced through the small high velocity region 3 tortuosity and the number of times particles hit the solid grain boundary exhibit a strong correlation structure such that more hits are likely with high tortuosity values in the context of adsorption desorption this means that slow velocity particles preferentially sorb desorb further delaying downstream transport these correlation structures are parameterized in a reactive bernoulli ctrw model in the case of conservative transport the bernoulli ctrw accurately predicts column outlet breakthrough curves which is consistent with past studies morales et al 2017 hyman et al 2019 kang et al 2019 we extend the bernoulli framework to include adsorption desorption processes for scales where velocity correlation is important tortuosity and the number of times a particle strikes the boundary exhibit a correlation structure that must be captured in the upscaled model for accurate prediction of adsorption desorption to the grain surfaces we capture such correlation by sampling a tortuosity travel time pair at each model step and then sampling a particle hit number conditioned on the tortuosity this method enables the hit distribution and adsorption desorption effects on transport to be incorporated in the upscaled framework failing to include such correlation inaccurately represents the hit distribution of particles which subsequently propagates as error for adsorption desorption transport predictions the proposed reactive bernoulli ctrw model faithfully portrays transport influenced by adsorption desorption processes under a range of damköhler and péclet numbers previous reactive smm models sherman et al 2019 assume that the porous media can be partitioned into identical cells which is an assumption violated in the considered geometry we demonstrate that under certain adsorption desorption conditions conservative breakthrough curves are delayed and their shapes significantly altered in a way that cannot be accounted for via a simple retardation coefficient hence there is great utility in upscaling the underlying correlation structures of adsorption desorption processes as we have done here although our proposed model was only tested in an idealized 2d setting it provides a framework which can be expanded to aid in our understanding of more complicated 3d porous media geometries where for example pore scale sampling could be obtained from high resolution imaging of geological media or from three dimensional artificially generated pore structures application of our approach to these systems is envisaged in future research efforts we parameterize our upscaled ctrw models by sampling statistics from a portion of the column and explore model sensitivity to sample slice size and total sampling area when particle sample slice sizes exceed the velocity correlation length scale both velocity correlation via the bernoulli parameter and the tortuosity hit correlation structure decorrelate and no longer need to be accounted for in an upscaling framework if this is the case one might naturally question the use of a more complicated correlated model as a simpler framework could be substituted by considering statistics over a larger sampling area we address this issue by parameterizing the ctrw model with different sampling slice size sampling area combinations for a given sampling area mean ctrw breakthrough curve predictions are most accurate when sample sizes are less than the velocity correlation length scale meaning velocity and tortuosity hit correlational effects are important for these sample sizes more importantly sampling many small slices distributed through the domain significantly reduces the uncertainty of ctrw predictions this suggests that spatially distributing samples throughout the domain better captures global system statistics than sampling a few large continuous slices which agrees with the observations that show lagrangian statistics change significantly through space the spatial evolution of tortuosity and travel time are highly correlated and non stationary since only a portion of the porous column is sampled to parameterize the ctrw models the sampled tortuosity and travel times may not be representative of the global sample in a numerical experiment we assume full knowledge of the mean tortuosity field and leverage this knowledge to correct for spatial heterogeneity of statistics specifically we scale the travel time by a linear coefficient dependent on the sampled tortuosity and particle s current spatial position which significantly improves model performance however full knowledge of the tortuosity field is currently not obtainable without simulation of the entire domain and so this spatial correction serves as a proof of concept that upscaled models can be significantly improved if such spatial knowledge is included in input parameters this leaves a common unresolved issue in upscaled transport modeling studies in order to upscale transport we often first need to simulate transport input upscaled modelling parameters must reflect the spatial heterogeneity of lagrangian statistics for accurate model predictions however accounting for that spatial heterogeneity without sampling the entire domain is challenging note that in this study geo statistical interpolation methods including conditional kriging techniques were applied to infer global lagrangian statistics from sampled areas results not shown however these methods failed to capture the spatial evolution of statistics in sufficient detail to improve model prediction and model predictions were only improved once the global distribution was sufficiently sampled via a large sampling fraction a hence if these models are to be applied in real geologic systems methods must be developed to uncover global distributions from a small number of field measurements declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper credit authorship contribution statement thomas sherman conceptualization methodology formal analysis writing original draft writing review editing emanuela bianchi janetti conceptualization methodology writing original draft writing review editing gaël raymond guédon conceptualization methodology writing original draft writing review editing giovanni porta conceptualization formal analysis methodology writing original draft writing review editing supervision funding acquisition diogo bolster conceptualization formal analysis methodology writing original draft writing review editing supervision funding acquisition acknowledgments this material is based upon work supported by or in part by the us army research office under contract grant number w911nf 18 1 0338 ts is supported by the national science foundation graduate research fellowship under grant no dge 1841556 appendix a geometry and flow field we detail here below the methodology followed to generate the porous media column and numerically compute the single phase flow field a1 porous media column generation the synthetic porous media column is generated using a variant of the method introduced by smolarkiewicz and winter 2010 and hyman and winter 2014 it follows four successive steps as described here while three dimensional structures are possible we focus here on the two dimensional version of the method 1 a two dimensional regular grid with uniform spacing h is generated and is populated with random values u x x x y being the vector of grid node coordinates sampled from a continuous uniform distribution on the closed interval 0 1 fig a1 2 this random field is convolved with a symmetric flattened gaussian kernel a 1 k x σ θ min 1 2 π σ 2 exp x 2 y 2 2 σ 2 θ 2 π σ 2 with variance σ 2 and flattening factor θ to generate the isotropic correlated random topography fig a1 a 2 t x r 2 k x y β θ u y d y the correlation length in the topography is controlled by the value of β while the roughness of the topography is determined by the value of θ after the topography is generated it is normalized to the interval 0 1 note that the convolution is computed in the frequency domain as a consequence t x is double periodic in space 3 a level threshold γ 0 1 is applied to given t x to map values onto a phase indicator function i where i x 1 in the fluid nodes and i x 0 otherwise i e a 3 i x 0 if t x γ 1 if t x γ the resulting two dimensional field i x is representative of a porous media structure with fluid and solid nodes identified by the value of i x fig a1 intuitively as γ increases so does the volume of the fluid space in the porous medium 4 the given i x is finally processed to remove isolated disconnected regions that would prevent convergence of the fluid flow solver the main difference here with respect to the original method is the use of a flattened gaussian kernel instead of a gaussian kernel this modification permits us to control the roughness of the fluid solid boundaries of the porous medium here we generate a porous media column with the generation parameters and geometric observables listed in table a1 we measure the degree of spatial correlation of the fluid space i e the integral scale of the phase indicator function β by calculating the empirical variogram of i and by characterizing it through fitting of a spherical model see e g guédon et al 2017 for additional details we note that the column analyzed here has about 539 integral scales in the x direction and about 27 in the y direction a2 flow single phase fluid flow in the porous media column is directly solved on the fluid domain and delimited by the phase indicator function by using a finite volume method we solve the steady state navier stokes equations for incompressible isothermal and newtonian fluid a 4 u x 0 a 5 u x u x 1 ρ p x ν u x here u x denotes the velocity vector p is pressure ρ is fluid density and ν is fluid kinematic viscosity the openfoam open source cfd library release v1712 limited 2017 is used to solve the target system of equations through the built in solver simplefoam the computational grid is created using an in house mesh generator that converts the pixels of the porous media column i e the phase indicator function field into square grid cells periodic boundary conditions are applied in the x and y directions to force flow in the x direction we add a pressure drop between the inlet and outlet faces in the x direction no slip conditions are implemented at the fluid solid boundaries walls after steady state is reached the flow rate is such that stokes flow conditions are satisfied the measured permeability of the porous media column is 4 55 10 11 m2 appendix b mean tortusoity mean travel time correlation we demonstrate the strong correlation between mean tortuosity and the mean travel time the mean tortuosity field changes through space and is leveraged to account for spatial heterogeneity of the flow field and improve bernoulli ctrw predictions in section 5 2 1 the column is partitioned into non overlapping slices of equal size lagrangian statistics in each slice are arithmetically averaged and the spatial evolution of mean values are observed fig b1 shows the mean tortuosity and mean travel time through space values have been normalized by the maximum mean value for each respective distribution the mean tortuosity and mean travel times are nearly perfectly correlated with r 2 99 as the mean travel distance of particle increases the mean travel time increases the distance particles travel is related to the flow field structure which is controlled by the distribution and spatial position of grains this observation is exciting from an upscaled modeling perspective as it implies that if the spatial evolution of the tortuosity field is uncovered then the evolution of the travel time distribution may also be estimated by linear scaling the mean tortuosity values provide the global distibution χ x n 1 g which is incorporated into the bernoulli ctrw framework to scale travel times based on the local column s grain distributions see section section 5 2 1 
501,spatial markov random walk models smm have been demonstrated to accurately predict conservative solute transport across a wide range of hydro geological systems with recent advances enabling the smm to model systems with linear kinetic reactive processes however the proposed reactive smm s applicability is limited to systems that can be partitioned into a series of identical periodic cells where travel times across cells are highly correlated to the solute s entrance position at the cell inlet in real geologic settings the spatial layout and size of grains varies through space decorrelating the relationship between travel time and transverse position here we generalize previous smm implementations and implement a bernoulli ctrw where transport behavior can be captured in disordered and non periodic porous media we validate our upscaled model predictions with results from direct numerical simulation of transport in a 2d porous column that cannot be partitioned into identical periodic elements we parameterize our model based on a subset of simulation statistics and explore how model accuracy changes due to our sampling method this finding yields important insights for optimizing efficiency of the upscaled transport model parameterization and can guide field sampling of geological structures as well as multiscale investigation of laboratory observations keywords reactive transport upscaled modeling statistical sampling methods 1 introduction in natural subsurface porous media settings the spatial layout and broad distribution of solid phase grain geometries and irregularity of grain shapes gives rise to a complex flow setting specifically the fluid velocity field is highly heterogeneous and characterized by high velocity preferential channels that rapidly transport dissolved solute over larger distances as well as grain cavities and dead end pores that result in stagnant waters that delays downstream transport the velocity distribution can span several orders of magnitude which ultimately manifests as anomalous i e non fickian behavior at typical experimental observation scales goltz and roberts 1986 harvey and gorelick 2000 zinn and harvey 2003 major et al 2011 understanding solute transport becomes even more complex when mass can exchange between the fluid and solid phase via chemical reactions i e when solute adsorbs and desorbs to the solid grain surface as demonstrated by roberts et al 1986 dentz and castro 2009 maghrebi et al 2014 maghrebi et al 2015 such chemical interactions further delay transport relative to the case of a non reactive solute and impact larger scale transport behavior cvetkovic and dagan 1994 capturing such complexity and anomalous behavior in mathematical modeling frameworks is challenging but remains critical for many scientific and engineering applications including contaminant transport in aquifers rathi et al 2017 biological membranes wood et al 2007 and packed bed reactors luz et al 2018 high fidelity direct numerical simulations dns are a common method for simulating flow and transport through porous media blunt et al 2013 de anna et al 2013 kang et al 2014 ceriotti et al 2019 in dns the flow field porous medium geometry and evolution of a solute plume as it traverses space time are all fully resolved an advantage of dns is that physical mechanisms that drive transport can be studied separately and are well described providing an in depth understanding of the hydro geologic system and associated transport processes however such simulations demand large computational resources and so applying dns at the field scale is practically impossible hence for studies at the field scale upscaled models which effectively represent physical processes without explicitly resolving them provide an attractive alternative a variety of upscaled models have emerged to capture anomalous transport in subsurface hydrological systems including among others multi rate mass transfer mrmt haggerty and gorelick 1995 fractional advection dispersion benson et al 2000 continuous time random walk ctrw berkowitz et al 2006 and time domain random walk tdrw models noetinger et al 2016 here we consider the ctrw modelling framework in a ctrw a solute plume is typically conceptualized as a large ensemble of particles who transition through time and space by sampling a waiting time velocity distribution the particle s velocity series can be modeled as a spatial markov process where at each step a particle traverses a fixed streamwise distance and its associated travel time is modeled with an effective travel time distribution parameter successive model jumps are assumed independent when the jump distance exceeds a velocity correlation length scale berkowitz and scher 1997 berkowitz et al 2006 and correlated at smaller scales le borgne et al 2008a 2008b kang et al 2015a when jumps are sufficiently small such that correlation effects are important the velocity correlation structure is often imposed by assuming a velocity transition matrix that is stationary in space le borgne et al 2008a bolster et al 2014 sund et al 2015 or assuming velocity transitions via a bernoulli process dentz et al 2016 morales et al 2017 hyman et al 2019 kang et al 2019 puyguiraud gouze dentz 2019 puyguiraud et al 2019 ctrw models have demonstrated success in a variety of porous and fractured media studies le borgne et al 2011 kang et al 2015a sherman et al 2018 but often are applied in synthetic settings where the input correlation parameters can be directly measured to date the majority of ctrw studies have focused on conservative transport recently ctrw models that consider reactive transport have also emerged edery et al 2010 hansen et al 2014 aquino and dentz 2017 sund et al 2017 sherman et al 2019 wright et al 2019 here we focus on upscaling reactive transport where solute mass can exchange between the fluid and solid grain boundaries via a chemical reaction we build upon a previous study where the smm with adsorption and desorption was considered in an idealized periodic wavy channel sherman et al 2019 in this modelling approach a particle s inlet position at the periodic channel s throat informs the particle s random travel time across the cell the particle s outlet position which informs the next step in the markov chain and the number of times a particle strikes the boundary which subsequently controls adsorption desorption processes the approach was successful in the idealized wavy channel considered because the travel time statistics in each periodic cell are identical hence the proposed model by sherman et al can be fully parameterized through direct numerical simulation of transport through one periodic cell in more complex porous media geometries the spatially varying distribution of grains means the relationship between a particle s trajectory statistics and a particle s transverse position are no longer spatially stationary and thus transport behavior is further complicated in more realistic heterogeneous geologic settings the adsorption desorption smm therefore must be generalized if to be applied in realistic non periodic domains this study has two primary objectives 1 we extend the adsorption desorption smm so that upscaled particle trajectories are not dependent on transverse position to do so we introduce a bernoulli smm and relate a tortuosity parameter which describes particle motion in the transverse direction to the number of boundary hits when tortuosity is high particles have increased pathline travel distances and are therefore more likely to contact the grain boundaries and react 2 we investigate the impact of sampled statistics on model parameterization and uncertainty smm parameterization has to date been achieved empirically through direct numerical simulations le borgne et al 2008a kang et al 2014 sund et al 2016 most et al 2019 inverse modeling algorithms sherman et al 2017 and parametric analytical markov models kang et al 2015b dentz et al 2016 hakoun et al 2019 here we focus on parameterization via numerical simulations as these allow transport to be quantified in great detail and enable a full exploration of the parameter space specifically we sample the studied domain randomly to parameterize the ctrw by assigning equal probability to each location in the domain i e drawing slice locations from a uniform distribution then we observe how for a given sampling area the spatial distribution and size of sample slices influence ctrw predictions we find that ctrw models parameterized via many small sampling slices distributed through space improve model prediction capability and reduce uncertainty in predictions here the studied domain is a stochastically generated 2d porous media column where particle trajectories vary spatially although still highly idealized the spatial heterogeneity of the porous medium structure makes this system considerably more complicated than previously studied periodic domains and the proposed ctrw framework provides a step forward towards modeling adsorption desorption transport in such complex systems 2 pore scale setting and observables this section describes the methodology employed to model pore scale solute transport as well as providing the definitions for the observables that are later used in sections 3 and 4 within upscaling our pore scale simulations rely on an artificially generated porous geometry and the related flow field for more details see a 1 a 2 2 1 pore scale transport simulation we model the evolution of the solute plume through the porous media column with sorbing boundaries as khan 1962 zhang et al 2017 1 c x t t u x c x t d c x t x γ f l u i d 2 s x t t λ s x t α c x t d c n x σ s u r f a c e the solute concentration field c x t evolves through pore space γ fluid and time due to the fluid velocity u x and molecular diffusion d i e solute transport through the fluid is governed by the advection diffusion equation the fluid velocity is here computed numerically as detailed in a 2 solute concentration on the solid surface σ surface labeled s x t changes as solute mass exchanges between the fluid and solid phases with desorption rate λ and adsorption rate α hence the rate of mass exchange between the solid and fluid phases is simply the difference between the adsorption and desorption rates which mass conservation requires to be equivalent to the diffusive flux of solute at the boundary d c n where n is the unit normal at the boundary in the numerical experiments considered here solute enters the column inlet via a flux weighted pulse injection i e the solute concentration injected at the inlet is proportional to the velocity we solve eqs 1 2 with a lagrangian approach the solute plume is discretized into many particles who transition through space and time according to a langevin equation 3 x i n 1 x i n u i δ t η i 2 d δ t 4 y i n 1 y i n v i δ t ξ i 2 d δ t 5 t i n 1 t i n δ t t i where particle i has position x y and velocity u v at model step n diffusion enables particles to deviate from velocity streamlines which here is modeled as a brownian process η ξ are independent identically distributed random variables sampled from a standard normal distribution zero mean and unit variance additionally diffusion enables particles to strike the solid phase boundary when such interaction occurs a particle attaches to the solid phase with sorbing probability psorb if sorption occurs the particle remains attached for time t where t is a waiting time sampled from an exponential distribution with mean λ 1 if no sorption occurs which is always the case for conservative transport elastic boundary conditions and a zero waiting time t 0 are imposed we elaborate on details related to numerical implementation of adsorption desorption processes in the following section 2 1 1 we refer to simulations solved with eqs 3 5 as direct numerical simulations dns in subsequent sections 2 1 1 adsorption desorption processes we model adsorption and desorption processes following boccardo et al 2018 every instance a particle interacts with the solid phase boundary via diffusion it sorbs to the boundary with a probability psorb to leading order for a sufficiently small time step psorb is given by 6 p s o r b α π δ t d where α is a sorption rate d is a diffusion coefficient and δt is the simulation time step size where δt d α 2 when a particle contacts the solid phase we draw a random number ui from a uniform distribution u 0 1 and the particle sorbs if ui psorb and elastically reflects if ui psorb if a particle adsorbs to the boundary its waiting time ti is then sampled from an exponential function with mean λ 1 i e ψ t λ exp λ t 7 t i ψ t u i p s o r b 0 u i p s o r b note that ψ t is consistent with the assumption of linear desorption kinetics we characterize the importance of adsorption desorption processes through the porous media column with dimensionless damköhler numbers daa dad which compare adsorptive and desorptive timescales with diffusive time scales here high fidelity simulations are run for a range of daa and dad spanning several orders of magnitude o 10 10 3 allowing us to study the evolution of the solute plume as adsorption desorption processes become increasingly important respectively we define the adsorptive and desorptive damköhler numbers as 8 d a a ℓ c α π d d a d ℓ c 2 λ d here ℓ c is a characteristic length quantifying the average distance between the fluid phase with a solid boundary which is determined by the domain geometry here ℓ c 3 8 10 5 m similarly we investigate how reactive transport behavior changes as diffusion strength increases we quantify the relative strength of diffusion with a péclet number which gives the ratio of diffusive to advective transport time scales the péclet number pe depends on the mean fluid velocity accounting for x and y directions u d and ℓ c and is given as 9 p e u ℓ c d we tune d α λ in order to change dimensionless quantities 2 2 transport statistics one of the objectives of this study is to parameterize an upscaled model that accurately predicts transport through the porous column while minimizing the sampling area from which upscaled parameters are derived to do so we sample lagrangian statistics along particle trajectories from the high resolution simulations and quantify how particle ensemble statistics evolve through space and time in detail then we parameterize the upscaled model with only a subset of the observed data specifically we install control planes perpendicular to the mean imposed pressure gradient and measure transport statistics at the first crossing of each control plane control planes are spaced over a distance 2 04 10 5 m which is sufficiently o 10 times smaller than a velocity correlation length scale such that a particle s velocity evolution can be faithfully portrayed here the solute plume is comprised of many particles b with a collective mass m the ensemble of particles is denoted ω b the total number of particles o 104 is sufficiently large such that transport statistics converge below we detail the employed parameters i e the probability density functions pdfs of particle velocities travel times paths tortuosity and the velocity correlation function 2 2 1 velocity pdf at time t a particle s velocity depends on its position x we define a particle s instantaneous lagrangian velocity v x as the fluid velocity at x v x x b u x b where b denotes a vector of particles lagrangian velocities are sampled for particle first crossings of control planes meaning all points on a given control plane have the same x coordinate for a control plane located at x x i we construct the lagrangian velocity distribution ψ v xi 10 ψ v x i 1 m ω b d b δ v v x x i b note here that the ψ v xi distribution is spatially dependent as it depends on xi 2 2 2 travel time pdf closely related to velocity is the travel time distribution here travel time over two control planes refers to the elapsed time between first particle breakthrough at each control plane we define the first arrival time τcp of a particle at a control plane at x x i as 11 τ c p x i b t λ x i b λ x i inf x x i b x i the first arrival time of a particle is simply the first instance in which its x position exceeds xi the distribution of first arrival times at a control plane denoted in this study as a breakthrough curve is defined 12 ψ t x i 1 m ω b d b δ t τ c p x i b tracking particle breakthrough times enables us to calculate the elapsed travel time δτcp x i j between any two control planes at xi and xj xj xi as δ τ c p τ c p x i b τ c p x j b then for any spatial slice x i j spanning between xj and xi we define the distribution of travel times for the ensemble of particles 13 ψ δ τ c p x i j 1 m ω b d b δ δ τ c p δ τ c p x i j b note when x j 0 the travel time and breakthrough time distributions for xi are equivalent ψ δ τ c p x i 0 ψ t x i 2 2 3 tortuosity pdf the irregularity and distribution of the solid phase creates complex flow features resulting in streamlines not aligned with the primary pressure gradient i e streamlines are not straight but meander also in the transverse direction furthermore diffusion allows solute to sample many streamlines as a result a particle s total travel distance is greater than the linear distance in the streamwise direction tortuosity quantifies the ratio of the total pathline distance to the linear streamwise travel distance and this is closely correlated with travel time the total travel distance δs between successive model steps n and n 1 in the high resolution simulation is estimated with a euclidean distance δ s n n 1 x n 1 x n it follows that the total pathline distance s n n k from model step n to n k is the summation of distances for each model step s n n k i 0 k 1 δ s n i n i 1 then a particle that first crosses a control plane at x x j at model step n and first crosses a control plane x x i at model step n k has a corresponding travel distance between control planes of s x i j s n n k note in the literature there are various definitions of tortuosity that have been used to study different subsurface properties see ghanbarian et al 2013 here we adopt a pathline dependent effective tortuosity that aligns with langrangian observations and can naturally be included as an upscaled model parameter sherman et al 2020 a particle s effective tortuosity χ x i j between control planes xi and xj is the total pathline travel distance s x i j divided by the linear distance between control planes δ x i j x i x j 14 χ x i j s x i j δ x i j the effective tortuosity distribution between any two control planes at xi and xj is defined as 15 ψ χ i j 1 m ω b d b δ χ i j χ i j b for any slice spanning control planes xi and xj we can define a local mean effective tortuosity χ i j l where angle brackets denote an arithmetic average across the particle ensemble 2 2 4 velocity correlation recall the travel time and tortuosity distribution are defined over a slice spanning two control planes we set control planes relative to a velocity correlation length scale which is calculated from velocity correlation function 16 c v v s 1 n n 1 n 0 d s v s v s s σ v v 2 where v refers to a velocity fluctuation about a mean σ v v 2 is the velocity variance s is the spatial lag distance and n is the number of particles allowing for the average across the particle ensemble the velocity correlation length scale lc is then the integral of the correlation function 17 l c 0 c v v s d s the velocity correlation length scales are 1 51 10 4 1 89 10 4 1 87 10 4 m for p e 100 1000 and respectively demonstrating that velocity correlation features more rapidly diminish with increasing diffusive strength the entire porous media column is approximately o 100 o 10 times the velocity correlation scale along the longitudinal and transverse directions respectively 3 flow and transport observations in this section we investigate the spatial variation of eulerian and lagrangian flow and transport properties we partition our domain into non overlapping equally sized slices and measure the spatial evolution of transport statistics control planes are spaced at distances of 0 1 1 and 10 times the correlation length scale lc here we highlight distributions for control planes spaced 0 1lc and p e 1000 3 1 eulerian observations the top three rows in fig 1 show eulerian characteristics of the column i e the distribution of grains porosity and the eulerian velocity field the phase field displays a disordered structure the grain geometries shown in blue are highly irregular and each grain has a unique shape and size the porosity ϕ in each sample slice defined as the fluid area divided by the entire sample area ϕ a f l u i d a s a m p l e varies through space with values ranging within 0 23 0 97 the entire column has a mean porosity of 0 64 the fluid velocity field is characterized by a broad distribution spanning over 10 orders of magnitude where high velocity preferential pathways connect and cut through the surrounding slow moving waters relatively high velocity channels form when the pore space gap between grains constricts due to the incompressibilty constraint for this reason high velocity channels form in areas of low porosity the relatively small longitudinal sample size of the sample facilitates the occurrence of channelling ceriotti et al 2019 whose occurrence is documented in three dimensional media siena et al 2019 3 2 lagrangian observations here we show how lagrangian particle statistics are influenced by the underlying flow field structure 3 2 1 spatial evolution of puyguiraud gouze dentz 2019 lagrangian pdfs fig 1 displays the spatial evolution of the travel time tortuosity and velocity pdfs for control planes spaced by distance 0 1lc here velocity pdfs correspond to values at a particle s first crossing of a control plane while travel time and tortuosity are determined from particle trajectories between successive control planes we immediately observe that the eulerian flow field structure directly influences lagrangian statistics the transverse breakthrough position distribution tbpd through space highlights where particles cross each control plane in the transverse direction colors correspond to log probabilities the largest tbpd values correspond to regions of high fluid velocity demonstrating that mass is channelized in high velocity pathways as a result of such channelization the lagrangian velocity pdf transitions through space as observed in puyguiraud et al 2019 and hakoun et al 2019 when the velocity field displays strong dominant channeling features such as near x 0 001 0 009 the statistics of the travel time tortuosity and velocity homogenize with the width of the pdfs decreasing such behavior is expected because mass preferentially enters fast channels meaning all channelized mass shares similar trajectories these regions are made up of relatively fast velocity zones which results in faster mean travel times consequently the lagrangian velocity and travel time distribution are negatively correlated i e as mean velocity increases mean travel time decreases because particles are traveling faster similarly the mean tortuosity decreases with increasing channelization as channelization is associated with high velocity pathways the fastest channels tend to be aligned with x meaning particles that enter such channels will have local tortuosity values closer to 1 in regions of slow flow diffusion becomes increasingly important for particle trajectories which increases tortuosity values on average the largest tortuosity values are o 100 meaning the total particle travel distance is two orders of magnitude greater than the x linear distance these large values can occur near pore cavities where the velocity field can advect particles counter to the primary pressure gradient and cause particles to recirculate through slow waters hence particles travel relatively long distances at slow velocities which results in large travel times that manifest in late breakthrough tailing behavior 3 2 2 solid fluid phase interaction in addition to the aforementioned lagrangian statistics we also track the number of times a particle hits a boundary from inlet to outlet recall that adsorption to the solid phase boundary only occurs when diffusion brings a particle into contact with the solid phase if a particle adsorbs then it waits according to an exponential distribution which delays transport see section 2 1 1 the number of times a particle hits the solid phase is therefore important for the reactive case fig 2 shows the joint pdf between number of hits and tortuosity for pe 100 1000 colors correspond to log probabilities here hits and tortuosity are measured over control planes with spacing 0 1lc for both pe the mean number of hits tends to increase with tortuosity demonstrating that particles are more likely to interact with the solid phase as particle travel distance increases however the majority of particles display local tortuosity values of o 1 this has important implications for adsorption desorption of particles as it implies that slow particles with high tortuosity preferentially adsorb and are delayed in contrast fast particles preferentially transport via high velocity channels and therefore have less interaction and are less likely to adsorb to the solid boundary on average the joint pdfs for both pe display similar structures however mean tortuosity and hits increases for the pe 100 case 4 upscaled stochastic modeling bernoulli ctrw this study s objective is to predict transport at the column scale through an upscaled model parameterized using a limited portion of the 2d column we also investigate how the spatial distribution and area of sample slices influences upscaled model parameterization and prediction these objectives are studied in the context of a continuous time random walk ctrw which is a lagrangian stochastic framework that naturally aligns with particle tracking methods applied in the high resolution numerical simulations described in section 2 1 in a ctrw framework the solute plume is conceptualized as many discrete tracer particles that transition through time and space according to probabilistic rules as formulated in this study particles traverse a fixed distance ℓ at every model step with an associated travel time τ which is sampled from a travel time distribution ψ τ to upscale adsorption desorption processes at every ctrw step we model the number of times a particle hits the solid boundary and each hit has an associated waiting time sampled from a distribution ψ τsorb here set as an exponential distribution upscaled particle trajectories follow a langevin equation 18 x i n 1 x i n ℓ 19 t i n 1 t i n τ i n 1 j 1 n h i t s k τ s o r b j at every model step particles jump a fixed distance ℓ in time τ which is randomly sampled from a travel time distribution ψ τ the cumulative waiting time j 1 n h i t s k τ s o r b j delays particle transport due to adsorption desorption and equals zero for the conservative case in this framework the lagrangian velocity pdf is not explicitly used as the velocity information is captured in the travel time distribution further details on implementation are discussed in the following sections 4 1 implementation of ctrw correlation structures 4 1 1 velocity correlation it has been well documented that strong correlation properties exists in porous and fractured media which may significantly impact solute transport behavior at typical scales of interest le borgne et al 2008a de anna et al 2013 bolster et al 2014 kang et al 2014 morales et al 2017 as a result for scales less than the velocity correlation length fast slow particles preferentially remain fast slow thus when a ctrw jump distance is significantly less than the velocity correlation length scale ℓ lc velocity correlation needs to be captured to faithfully portray transport behavior the bernoulli ctrw model which we will apply in this study is a spatial markov model smm that accounts for velocity correlation in a parsimonious simplistic framework and was first introduced by dentz et al 2016 in the bernoulli ctrw framework a particle s velocity transitions via a bernoulli process that is the particle persists with the same travel time as the previous model step with probability p and samples from a global distribution otherwise 20 τ i n 1 τ i n p ψ τ 1 p we assume that particle velocity transitions at a constant spatial rate 1 lc with p exp ℓ l c which is consistent with past studies hyman et al 2019 kang et al 2019 note that including correlation between successive model jumps adds negligible computational costs when compared with an uncorrelated model 4 1 2 tortuosity hit correlation and adsorption desorption fig 2 shows a clear correlation structure between particle tortuosity and the number of times a particle strikes the solid phase boundary upscaling the number of boundary hits is important for estimating retardation effects due to adsorption desorption to impose such correlation effects we sample from a conditional tortuosity hit distribution to upscale boundary hits we sample dns lagrangian statistics of the porous column and in each sampling slice store a travel time tortuosity number of hits tri pair for each particle trajectory at every model step each particle samples a travel time and its associated tortuosity value in the case of reactive transport the sampled tortuosity conditionally informs the number of hits specifically we implement hits by sampling a conditional distribution ψ χ hits which is created from the tri pair sampled data the waiting time due to adsorption desorption is calculated from the number of hits we assume knowledge of daa dad and make them identical to the values prescribed in the pore scale numerical simulations the cumulative waiting time for a single particle at one ctrw step is j 1 n h i t s τ s o r b j for every hit a particle adsorbs with probability psorb psorb is based on daa and is calculated with eq 6 if the particle adsorbs then we sample a waiting time from an exponential distribution ψ τ s o r b λ exp λ τ s o r b with λ corresponding to dad hence sampling travel time tortuosity and boundary hits for each particle trajectory is sufficient to parameterize an upscaled reactive ctrw model 4 2 bernoulli ctrw parameterization in this study the column is partitioned into equally sized slices with x length of size ℓ and with y height equivalent to column height we select n slices which are used for ctrw parameterization the cumulative sampling area a is smaller than the entire column area the slice locations are randomly determined by sampling a uniform distribution i e every spatial position in the domain has equal probability of being selected in each slice the ensemble of travel times τ tortuosity χ and number of boundary hits statistics are stored this provides sufficient data to construct ψ τ and ψ χ hits and apply the adsorption desorption bernoulli ctrw framework 4 2 1 sampling sensitivity a primary objective of this study is to complete a sensitivity analysis relating sampling statistics to ctrw predictions to do so we parameterize ctrw models with different sampling slice sizes ranging from o 1 to o 10 times the velocity correlation length scale furthermore we investigate how increasing the sampling area used for parameterization statistics affects ctrw predictions fig 3 provides a conceptual visualization of the method for sampling the domain in the top and bottom subfigures a fixed sampling area a is selected highlighted in red the sampling area in the top subfigure is comprised of one large slice while the sampling area in the bottom subfigure consists of many smaller slices the spatial distribution of slices is randomly selected from a uniform distribution simulation statistics corresponding to the randomly selected samples parameterize our bernoulli ctrw model we investigate whether the size and spatial distribution of the slices influences ctrw predictions since the slice locations are randomly sampled we run 100 realizations for each sampling area slice size combination each realization has a different set of sampling slice locations this enables us to quantify both the accuracy and sensitivity of ctrw predictions over an ensemble for a given sampling area slice size combination parameter we sample the existing dns statistics corresponding to the full simulation of the porous media column and assume knowledge of the velocity correlation length scale a priori i e lc is not realization dependent 5 results in this section transport predictions of the proposed ctrw are compared with pore scale simulations for different pe daa dad combinations first we present the conservative transport case and test model sensitivity to input parameters then we test ctrw model performance under varying adsorption desorption rates the section concludes by examining how diffusion influences both reactive and conservative transport behavior 5 1 ctrw predictions and uncertainty for conservative transport we parameterize a ctrw model and predict first passage time distributions at the porous media column outlet the ctrw performance is tested for different sample slice size cumulative sampling area combinations here we choose sampling slice sizes of ℓ 0 1 l c 0 2 l c 0 4 l c 1 l c 10 l c the cumulative sampling area a has size 1 8 2 8 4 8 the area of the total domain recall each sampling area is divided into discrete slices with width in x ℓ and height in y equal to the total column height the top row of fig 4 shows the entire ensemble distribution gray and mean red ctrw outlet breakthrough curve predictions for a 1 8 and slice sizes ℓ 0 1 l c 1 l c 10 l c for conservative transport for each slice size we run 100 ctrw realizations with each realization sampling the domain randomly but uniformly the mean breakthrough curves accurately predict tailing behavior for all slice sizes however early and peak arrival are only captured when the sample slices are smaller than the velocity correlation length scale additionally the spread across ctrw realizations decreases as the sample slice size decreases as demonstrated by the reduced gray shaded area in the right subfigure one large continuous slice with size 1 8 the length of the column is sampled to parameterize the ctrw predictions are least accurate and uncertainty is greatest for this case recall that the mean travel time distribution evolves through space and so sampling one region of the domain may not be representative of the global column statistics when the sampled statistics are not representative ctrw predictions will fail these results suggest that for a given sampling area we can reduce uncertainty across realizations by taking many small slices that are spatially distributed such a sampling method yields statistics more representative of the global column system at large the solid lines in the left panel of fig 5 display the mean squared error of conservative breakthrough curve predictions for ctrw models parameterized with different sample slice lengths m s e 1 n t 1 n c d n s c c t r w 2 where brackets denote the average over realizations and n is number of time points where particle concentration values are recorded colors denote the total sampling area a 1 8 2 8 4 8 for black red blue lines respectively as expected model prediction accuracy improves as the sampling area increases because the sampled statistics become more representative of the global statistics and thus spatial heterogenities in lagrangian statistics are better captured for all sampling areas the largest ctrw error occurs when the parameterization slice size is o 10 times larger than lc note that when the slice size is much greater than the velocity correlation length scale ℓ lc the probability of a particle velocity persisting over successive model steps goes to zero and so the bernoulli ctrw effectively reduces to an uncorrelated random walk the most accurate predictions when the tortuosity correction is not applied always occur when sample slices are smaller than lc however the optimal slice sizes varies for different sampling areas this result demonstrates the importance of parameterizing models with representative statistics an uncorrelated ctrw model most closely resembles using statistics from one continuous slice however even in this idealized porous media system parameterizing the ctrw with sample sizes larger than the correlation scale provide less accurate predictions on average suggesting that correlated ctrw models may be practically more suitable for prediction purposes especially in this context where computational costs between sampling methods for a fixed sampling area are approximately equal in addition to reduced accuracy parameterizing the ctrw with a slice size greater than the velocity correlation length scale ℓ lc generates increased uncertainty across realizations we quantify uncertainty with the time averaged variance of log arrival time pdf values the solid lines in the right panel in fig 5 display the variance vs slice size length for different sampling areas as the sampling area increases the uncertainty in prediction across ctrw realizations decreases because sampled statistics become more representative of global statistics and better capture spatial heterogeneities that impact transport behavior on the other hand the ensemble variance decreases as the sampling slice size decreases this suggests that when the sampling volume needed for representative statistics is unknown obtaining statistics from many small slices randomly distributed throughout the domain will reduce prediction uncertainty using data from fewer larger sample slices does not fully capture the entire spectrum of spatial heterogeneity and thus generates increased variability across the ensemble of predictions 5 2 accounting for spatial heterogeneity via tortuosity the spatial variability of lagrangian and eulerian statistics makes it challenging to characterize the upscaled model by sampling only a portion of the porous media column in this section we explore methods that correct the previously sampled distributions to account for spatial variations in the flow field structure thereby improving mean model predictions and reducing the spread across the ensemble we do so by relating tortuosity and travel time next in section 5 2 1 we assume full knowledge of the mean tortuosity field to correct the sampled data we acknowledge that such information is currently practically unobtainable without relying on full simulation of the domain thus this section serves merely as a proof of concept then in section 5 2 2 we evenly space sampling areas throughout the entire column and find that this method on average better captures spatial heterogeneity of transport characteristics than sampling randomly 5 2 1 correcting sampled data via tortuosity a proof of concept the spatial evolution of the travel time and tortuosity distributions and their associated mean values are positively correlated i e large travel distances are more likely to have large travel times here we investigate how correcting for spatial variations in mean travel distance influences upscaled model prediction to do so we assume full knowledge of the mean tortuosity field we divide the porous media column into slices with length ℓ and each slice has a mean tortuosity value χ x g then as done previously we sample a portion of the domain and construct a travel time distribution ψ τ from high resolution simulations we store an additional piece of information the mean tortuosity for each local sample slice χ l hence we now sample a travel time mean tortuosity pair ψ τ χ l our sampled travel time in the ctrw is then scaled according to the current location of the particle reflecting that areas of the domain where particles travel a farther shorter distance should have larger smaller travel times the scaled ctrw time equation becomes 21 t i n 1 t i n τ i n 1 χ x n 1 g χ x l here our scaling factor χ x n 1 g χ x l is simply the tortuosity corresponding to the particle s current x position over the mean tortuosity of the sampled travel time slice if the sampled mean tortuosity is smaller than the mean tortuosity of the particle s current position then the time is increased by the correction factor to account for the fact that the particle will likely experience a higher tortuosity due to its current position in the column in essence this scaling ensures that the ensemble of upscaled particle trajectories has a mean tortuosity that matches what was observed in the pore scale simulation the bottom row of fig 4 compares the mean ctrw predictions with simulated breakthrough curves and displays the spread across realizations when a tortuosity correction is applied using eq 21 for all sample slice sizes the tortuosity correction improves mean model performance most notably for early arrival times more importantly the spread across realizations also decreases with the tortuosity correction hence the correction both improves model accuracy and decreases uncertainty by accounting for the spatial heterogeneity of tortuosity through the column the dashed lines in fig 5 shows the mean model error vs slice size as well as the realization variance vs slice size for all sampling area slice size combinations the tortuosity correction reduces model error and the prediction uncertainty knowledge of the spatial heterogeneity of tortuosity has a marked impact on the model results similar to the trends observed when no correction is applied the model uncertainty increases with increasing slice size although the rate of such increase is largely reduced in this corrected model however a different trend emerges for mean squared error with the tortuosity correction the mean ctrw error decreases with increasing slice size when sample slice sizes are larger than the velocity correlation length scale ℓ lc such correlation effects are built into the measured statistics and do not need to be explicitly represented in the upscaled model hence for large slice sizes only the larger scale spatial transitions in lagrangian statistics caused by heterogeneities of the fluid flow structure need to be captured to upscale transport which is explicitly done in this method by scaling travel times based on the tortuosity field 5 2 2 evenly distributed sample slices knowledge of the full lagrangian statistics is practically unobtainable in most non insilico or field settings we investigate here strategies to obtain a representative sample of lagrangian statistics by constraining the model on partial information of the tortuosity field our first attempt was to reconstruct the full log tortuosity field with conditional kriging i e using sample slices as conditioning points the results of these numerical experiments are here omitted since the model performances were similar to those obtained with random sampling and no tortuosity correction i e shown in fig 4 top row here we then simply distribute sample slices such that they span the entire domain and are spaced by equal distance again a fraction a of the porous media domain is sampled and divided into n slices of equal size a n the center of each sample slice is set such that it is equi distant from neighbor slices with distance between centers equal to l n where l is the column length we compare ctrw prediction accuracy when parameterized with the evenly spaced samples with predictions via uniformly distributed samples for various sample area slice size combinations fig 6 solid lines show the mse with the uniformly random method for sample area fractions a 1 8 2 8 4 8 and dashed lines correspond to the evenly spaced sampling method for all slice lengths ℓ sampling the porous media with equi distant spaced sample slices improves model predictions especially as sample slice size increases this is because the equi spaced sampled tortuosity distribution is more representative with respect to its global generally unknown distribution than when random sampling is performed note however that results obtained in this case rely on a single realization and may be subject to the particular case analyzed here i e they might not be generally exportable to any other setting 5 3 adsorption desorption results the bernoulli ctrw model is extended to include adsorption desorption processes for particle boundary interactions we test reactive ctrw performance for a wide range of daa dad values and combinations recall that adsorption desorption is only possible if a particle strikes the solid boundary thus predicting the number of particle hits is important for accurately capturing transport of the reactive solute plume 5 3 1 upscaling hits particle boundary interactions for every sample slice we track tortuosity travel time and particle boundary hits and use these values to estimate the global distribution from which the ctrw is parameterized we test two methods to upscale the number of particle hits 1 at every model step particles sample randomly from a hit distribution ψ hits which is the aggregate of hit distributions for each sample slice and independent from all other variables 2 at every model step we sample from a hit distribution dependent on tortuosity ψ hits tortuosity i e we sample a tortuosity value which then informs the number of hits using a realization dependent conditional hits tortuosity distribution with similar features to the one shown in fig 2 we estimate the total number of times a particle hits a grain boundary from inlet to outlet with the two upscaled methods and compare with high fidelity pore scale simulations fig 7 compares the distribution of total hits for the two upscaled methods vs simulation data for two sample slice sizes the total hit distribution spans several orders of magnitude for sample slices of size ℓ 1 l c the peak of the distribution is overestimated and low hit values are underestimated when the bernoulli model samples ψ hits however conditioning the hits on particle tortuosity enables the upscaled model to faithfully represent the particle boundary interactions this indicates that over small scales the relationship between hits and tortuosity should be considered in upscaled models because particles with very tortuous paths are more likely to interact with boundaries for the sample slice of size ℓ l c both upscaling methods accurately capture the total hit distribution suggesting the number of hits and tortuosity correlation structure homogenizes after particles have traveled sufficient distance in the longitudinal direction 5 3 2 upscaling transport with sorption desorption we predict breakthrough curves at the column outlet with an adsorption desorption bernoulli ctrw and compare with high fidelity simulations simulations are tested for all combinations of d a a 10 10 2 10 3 and d a d 10 10 2 10 3 additionally we consider extreme conditions where particles sorb with probability 1 if they contact the solid boundary p s o r b 1 and have large waiting times d a d 10 5 this extreme case allows us to test the veracity of the proposed upscaled ctrw model both adsorption and desorption rates influence column scale particle transport a particle that hits the solid boundary nhits time has an expected delay in transport e t d e l a y n h i t s p s o r b λ 1 hence transport is delayed as psorb controlled by adsorption rate and λ 1 controlled by desorption rate increase fig 8 displays predicted and simulated breakthrough curves for selected daa dad combinations and p e 1000 the left and right columns in fig 8 correspond to sample slice sizes of length ℓ 1 l c 1 l c respectively the solid line corresponds to ctrw predictions without correcting for spatial variations in the tortuosity field and the dashed lines indicates a tortuosity correction when the daa dad are relatively low o 100 breakthrough curves remain relatively similar to the conservative case as daa dad increase the mean arrival breakthrough time increases and tailing is enhanced early time breakthrough is not significantly delayed by adsorption desorption processes as these early particles are transported via fast channels where interaction with the solid boundary is limited or even zero additionally we consider a case with p s o r b 1 and d a d 10 5 meaning every time a particle hits a boundary it is guaranteed to adsorb and will likely remain attached to the boundary for a relatively large time in this case there is a distinct peak at early times followed by enhanced tailing the distinct peak corresponds to particles that have little or zero interaction with the boundary while particles that hit the boundary have longer waiting times the largest breakthough times are delayed from o 104 in the conservative case to o 106 the reactive bernoulli ctrw model captures breakthrough curves across the d a a d a d parameter space trends are similar to those observed in the conservative case the spread across ctrw realizations increases when sample slice size ℓ increases additionally the tortuosity correction dashed lines improves breakthrough prediction especially at early times recall that early travel times are those particles in fast channels who are less likely to interact with the solid boundary and thus the tortuosity correction has the same effect as in the conservative case however as the adsorption desorption processes play an increasing role i e as daa dad increase the tortuosity correction becomes less significant because the reactive boundary processes dominate retardation 5 4 impact of péclet number on reactive transport we test the influence of diffusion on reactive transport by comparing breakthrough curves for p e 100 1000 recall that diffusion is the mechanism which enables particles to interact with solid boundary hence when diffusion strength increases the mean tortuosity and number of times a particle hits the boundary increases on average fig 2 the mean number of hits from inlet to outlet is 871 281 for p e 100 1000 respectively fig 9 shows outlet breakthrough curves for the p e 100 blue and p e 1000 red cases under different adsorption and desorption rates in the conservative case diffusion does not have a significant impact on particle breakthrough times and so breakthrough curves appear similar across pe as psorb and waiting time dad increase the peak breakthrough concentration is delayed and tailing is enhanced because particles now have increased likelihood of sorbing to grain boundaries in the extreme case where p s o r b 1 and λ 1 166 the peak concentration is significantly delayed for the p e 100 case increasing diffusion strength increases the likelihood of hitting the boundary which significantly delays transport when the adsorption rate is high and desorption rate is low the reactive bernoulli ctrw model s robustness is tested by varying pe fig 9 for all the cases considered the ctrw model solid lines captures reactive transport through the porous media column here the ctrw model is parameterized with sample slice size ℓ 0 1 l c and sample area fraction a 1 8 again we run 100 ctrw realizations with each realization sampling the domain randomly from a uniform distribution of the available space the spread across realizations remains relatively small for this sample slice hence parameterizing the reactive bernoulli ctrw with slices smaller than the correlation length scales can accurately predict transport under a wide variety of conditions 6 discussion and conclusions we run direct numerical simulation of transport through a 2d porous column with adsorption desorption the spatial distribution of grains and irregularity of grain geometry results in a fluid velocity field that is complex and spatially variant consequently lagrangian transport statistics are spatially dependent non stationary and capturing such features in an upscaled modelling framework is challenging we use direct numerical simulations to explore reactive transport through the column which guides development of a more general spatial markov model with adsorption desorption the simulations show 1 the velocity travel time and tortuosity distributions are highly intertwined high velocity channels are mainly aligned with the primary pressure gradient meaning particles transported in such channels travel a shorter distance resulting in relatively fast travel times 2 high velocity regions channelize mass which homogenizes corresponding lagrangian statistics as a significant portion of mass is forced through the small high velocity region 3 tortuosity and the number of times particles hit the solid grain boundary exhibit a strong correlation structure such that more hits are likely with high tortuosity values in the context of adsorption desorption this means that slow velocity particles preferentially sorb desorb further delaying downstream transport these correlation structures are parameterized in a reactive bernoulli ctrw model in the case of conservative transport the bernoulli ctrw accurately predicts column outlet breakthrough curves which is consistent with past studies morales et al 2017 hyman et al 2019 kang et al 2019 we extend the bernoulli framework to include adsorption desorption processes for scales where velocity correlation is important tortuosity and the number of times a particle strikes the boundary exhibit a correlation structure that must be captured in the upscaled model for accurate prediction of adsorption desorption to the grain surfaces we capture such correlation by sampling a tortuosity travel time pair at each model step and then sampling a particle hit number conditioned on the tortuosity this method enables the hit distribution and adsorption desorption effects on transport to be incorporated in the upscaled framework failing to include such correlation inaccurately represents the hit distribution of particles which subsequently propagates as error for adsorption desorption transport predictions the proposed reactive bernoulli ctrw model faithfully portrays transport influenced by adsorption desorption processes under a range of damköhler and péclet numbers previous reactive smm models sherman et al 2019 assume that the porous media can be partitioned into identical cells which is an assumption violated in the considered geometry we demonstrate that under certain adsorption desorption conditions conservative breakthrough curves are delayed and their shapes significantly altered in a way that cannot be accounted for via a simple retardation coefficient hence there is great utility in upscaling the underlying correlation structures of adsorption desorption processes as we have done here although our proposed model was only tested in an idealized 2d setting it provides a framework which can be expanded to aid in our understanding of more complicated 3d porous media geometries where for example pore scale sampling could be obtained from high resolution imaging of geological media or from three dimensional artificially generated pore structures application of our approach to these systems is envisaged in future research efforts we parameterize our upscaled ctrw models by sampling statistics from a portion of the column and explore model sensitivity to sample slice size and total sampling area when particle sample slice sizes exceed the velocity correlation length scale both velocity correlation via the bernoulli parameter and the tortuosity hit correlation structure decorrelate and no longer need to be accounted for in an upscaling framework if this is the case one might naturally question the use of a more complicated correlated model as a simpler framework could be substituted by considering statistics over a larger sampling area we address this issue by parameterizing the ctrw model with different sampling slice size sampling area combinations for a given sampling area mean ctrw breakthrough curve predictions are most accurate when sample sizes are less than the velocity correlation length scale meaning velocity and tortuosity hit correlational effects are important for these sample sizes more importantly sampling many small slices distributed through the domain significantly reduces the uncertainty of ctrw predictions this suggests that spatially distributing samples throughout the domain better captures global system statistics than sampling a few large continuous slices which agrees with the observations that show lagrangian statistics change significantly through space the spatial evolution of tortuosity and travel time are highly correlated and non stationary since only a portion of the porous column is sampled to parameterize the ctrw models the sampled tortuosity and travel times may not be representative of the global sample in a numerical experiment we assume full knowledge of the mean tortuosity field and leverage this knowledge to correct for spatial heterogeneity of statistics specifically we scale the travel time by a linear coefficient dependent on the sampled tortuosity and particle s current spatial position which significantly improves model performance however full knowledge of the tortuosity field is currently not obtainable without simulation of the entire domain and so this spatial correction serves as a proof of concept that upscaled models can be significantly improved if such spatial knowledge is included in input parameters this leaves a common unresolved issue in upscaled transport modeling studies in order to upscale transport we often first need to simulate transport input upscaled modelling parameters must reflect the spatial heterogeneity of lagrangian statistics for accurate model predictions however accounting for that spatial heterogeneity without sampling the entire domain is challenging note that in this study geo statistical interpolation methods including conditional kriging techniques were applied to infer global lagrangian statistics from sampled areas results not shown however these methods failed to capture the spatial evolution of statistics in sufficient detail to improve model prediction and model predictions were only improved once the global distribution was sufficiently sampled via a large sampling fraction a hence if these models are to be applied in real geologic systems methods must be developed to uncover global distributions from a small number of field measurements declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper credit authorship contribution statement thomas sherman conceptualization methodology formal analysis writing original draft writing review editing emanuela bianchi janetti conceptualization methodology writing original draft writing review editing gaël raymond guédon conceptualization methodology writing original draft writing review editing giovanni porta conceptualization formal analysis methodology writing original draft writing review editing supervision funding acquisition diogo bolster conceptualization formal analysis methodology writing original draft writing review editing supervision funding acquisition acknowledgments this material is based upon work supported by or in part by the us army research office under contract grant number w911nf 18 1 0338 ts is supported by the national science foundation graduate research fellowship under grant no dge 1841556 appendix a geometry and flow field we detail here below the methodology followed to generate the porous media column and numerically compute the single phase flow field a1 porous media column generation the synthetic porous media column is generated using a variant of the method introduced by smolarkiewicz and winter 2010 and hyman and winter 2014 it follows four successive steps as described here while three dimensional structures are possible we focus here on the two dimensional version of the method 1 a two dimensional regular grid with uniform spacing h is generated and is populated with random values u x x x y being the vector of grid node coordinates sampled from a continuous uniform distribution on the closed interval 0 1 fig a1 2 this random field is convolved with a symmetric flattened gaussian kernel a 1 k x σ θ min 1 2 π σ 2 exp x 2 y 2 2 σ 2 θ 2 π σ 2 with variance σ 2 and flattening factor θ to generate the isotropic correlated random topography fig a1 a 2 t x r 2 k x y β θ u y d y the correlation length in the topography is controlled by the value of β while the roughness of the topography is determined by the value of θ after the topography is generated it is normalized to the interval 0 1 note that the convolution is computed in the frequency domain as a consequence t x is double periodic in space 3 a level threshold γ 0 1 is applied to given t x to map values onto a phase indicator function i where i x 1 in the fluid nodes and i x 0 otherwise i e a 3 i x 0 if t x γ 1 if t x γ the resulting two dimensional field i x is representative of a porous media structure with fluid and solid nodes identified by the value of i x fig a1 intuitively as γ increases so does the volume of the fluid space in the porous medium 4 the given i x is finally processed to remove isolated disconnected regions that would prevent convergence of the fluid flow solver the main difference here with respect to the original method is the use of a flattened gaussian kernel instead of a gaussian kernel this modification permits us to control the roughness of the fluid solid boundaries of the porous medium here we generate a porous media column with the generation parameters and geometric observables listed in table a1 we measure the degree of spatial correlation of the fluid space i e the integral scale of the phase indicator function β by calculating the empirical variogram of i and by characterizing it through fitting of a spherical model see e g guédon et al 2017 for additional details we note that the column analyzed here has about 539 integral scales in the x direction and about 27 in the y direction a2 flow single phase fluid flow in the porous media column is directly solved on the fluid domain and delimited by the phase indicator function by using a finite volume method we solve the steady state navier stokes equations for incompressible isothermal and newtonian fluid a 4 u x 0 a 5 u x u x 1 ρ p x ν u x here u x denotes the velocity vector p is pressure ρ is fluid density and ν is fluid kinematic viscosity the openfoam open source cfd library release v1712 limited 2017 is used to solve the target system of equations through the built in solver simplefoam the computational grid is created using an in house mesh generator that converts the pixels of the porous media column i e the phase indicator function field into square grid cells periodic boundary conditions are applied in the x and y directions to force flow in the x direction we add a pressure drop between the inlet and outlet faces in the x direction no slip conditions are implemented at the fluid solid boundaries walls after steady state is reached the flow rate is such that stokes flow conditions are satisfied the measured permeability of the porous media column is 4 55 10 11 m2 appendix b mean tortusoity mean travel time correlation we demonstrate the strong correlation between mean tortuosity and the mean travel time the mean tortuosity field changes through space and is leveraged to account for spatial heterogeneity of the flow field and improve bernoulli ctrw predictions in section 5 2 1 the column is partitioned into non overlapping slices of equal size lagrangian statistics in each slice are arithmetically averaged and the spatial evolution of mean values are observed fig b1 shows the mean tortuosity and mean travel time through space values have been normalized by the maximum mean value for each respective distribution the mean tortuosity and mean travel times are nearly perfectly correlated with r 2 99 as the mean travel distance of particle increases the mean travel time increases the distance particles travel is related to the flow field structure which is controlled by the distribution and spatial position of grains this observation is exciting from an upscaled modeling perspective as it implies that if the spatial evolution of the tortuosity field is uncovered then the evolution of the travel time distribution may also be estimated by linear scaling the mean tortuosity values provide the global distibution χ x n 1 g which is incorporated into the bernoulli ctrw framework to scale travel times based on the local column s grain distributions see section section 5 2 1 
502,identifying groundwater contaminant sources based on a kelm surrogate model together with four heuristic optimization algorithms ying zhao conceptualization writing review editing a b ruizhuo qu writing original draft methodology software c zhenxiang xing funding acquisition visualization a b wenxi lu methodology software d a school of water conservancy and civil engineering northeast agricultural university harbin 150030 china school of water conservancy and civil engineering northeast agricultural university harbin 150030 china aschool of water conservancy and civil engineering northeast agricultural university harbin 150030 china b key laboratory of high efficiency utilization of agricultural water soil resources of ministry of agriculture of china harbin 150030 china key laboratory of high efficiency utilization of agricultural water soil resources of ministry of agriculture of china harbin 150030 china bkey laboratory of high efficiency utilization of agricultural water soil resources of ministry of agriculture of china harbin 150030 china c school of environment harbin institute of technology harbin 150090 china school of environment harbin institute of technology harbin 150090 china cschool of environment harbin institute of technology harbin 150090 china d college of new energy and environment jilin university changchun 130021 china college of new energy and environment jilin university changchun 130021 china dcollege of new energy and environment jilin university changchun 130021 china corresponding author identifying groundwater contaminant sources involves a reverse determination of the source characteristics by monitoring contaminant concentrations in a few observation wells however due to the ill posed nature and high time consumption of identification an efficient identification process with accurate estimated results is particularly important to improve the efficiency of identifying groundwater contaminant sources a kernel based extreme learning machine was used as a surrogate for the time consuming simulation model four heuristic search algorithms were used to improve the accuracy of the identification results the proposed approach was tested in both hypothetical and actual cases the conclusions are 1 by forward and backward calculation of the surrogate model the time cost of identifying groundwater sources can be reduced significantly 2 when a traditional genetic algorithm and a particle swarm optimization algorithm are combined with quantum computing computational efficiency and accuracy are both improved and 3 by using various search algorithms to identify unknown contaminant sources in the actual case the range of release histories of each contaminant source can be obtained decreasing the ill posed nature of the identification result obtained by a single algorithm and improving the reliability of the identification results keywords groundwater contaminant heuristic search algorithms identifying contaminant sources kernel based extreme learning machine 1 introduction unlike surface water contaminant groundwater contamination is concealed underground and is often not quickly discovered meaning that information on contaminant sources is difficult to obtain to provide a reliable basis for treatment and restoration of groundwater contaminant sources identification of the groundwater contaminant source release history has become a key point however current research on identifying groundwater contaminant sources suffers from three main problems first most existing research studied only hypothetical cases ayvaz 2010 liu et al 2010 wang and jin 2013 gurarslan and karahan 2015 although hypothetical cases have many advantages they also have limitations for example in hypothetical cases it can be assumed that the aquifer parameters are known and that the groundwater model is completely correct hence only information about the groundwater source is unknown for real world cases not only is information about groundwater contaminant sources unknown but also the aquifer parameters aquifer parameters can be obtained through experiments and calculations but there are also uncertainties in the constructed aquifer in addition a large amount of uncertainty exists in groundwater models including the uncertainty of the observed data and the model structure singh et al 2010 the uncertainty of real world cases seriously affects the identification of groundwater contaminant sources however unless the validity of the identification method is verified by a hypothetical case there are also instability problems when the model is applied to actual cases therefore established identification models should be applied to both hypothetical and actual cases so that the validity and practicability of the identification model can both be verified second there are efficiency and accuracy issues in identifying the release histories of contaminant sources rapid and stable identification of these release histories is the key to this research the main method used has become the identification of groundwater contaminant sources by simulation and optimization however if the simulation model is repeatedly used for iterative solution of the optimization model it will result in a huge computational load which seriously restricts the feasibility of the simulation optimization method in practical inversion identification therefore constructing surrogates for simulation models has become a leading issue in recent research forrester and keane 2009 srivastava and singh 2014 zhao et al 2016 qin et al 2007 used polynomial regression to create surrogate models the most widely used approach is the kriging surrogate model simpson et al 2001 luo and lu 2014 wan et al 2005 kriging is used because it can spatially model and predict fields and processes at the same time it can approximate and map complex nonlinear functions however this method easily falls into the local optimal solution especially when the distance constraint is too small which will affect the robustness of the algorithm zhou et al 2015 there is also a least squares support vector machine that captures the complex relationship between inputs and outputs and effectively extracts structural information from noisy data zhang et al 2009 the disadvantage is that it is difficult to determine a reasonable parameter range which affects the speed and accuracy of fault diagnoses to a certain extent zhang et al 2008 in addition surrogate models include support vector machines jin et al 2001 multivariate adaptive regression splines hou and lu 2018 ensemble surrogate model xing et al 2019 and others in this study a kernel based extreme learning machine kelm was used as a surrogate for the simulation model when running the optimization model the kelm model learns quickly huang et al 2012 but during its operation multiple types of problems must be converted into sub problems of the two classifications due to the introduction of the kernel function the kernel limit learning machine can be simplified to the execution of a least squares support vector machine for a traditional feed forward neural network artificial adjustment parameters are needed resulting in slow operation and a tendency to fall into local optima whereas the kelm model has strong generalization performance jain et al 1996 li et al 2016 use of surrogate models to save time in identifying the release histories of contaminant sources should also consider the accuracy of the identification results razavi et al 2012 the accuracy of the surrogate model in approximating the simulation model depends on the coverage of the sample space by the training sample but the number of training samples used to establish the surrogate model is limited and there is no guarantee that the training sample set can cover the sample space well therefore it is difficult to establish a sufficiently reliable surrogate model using only the training samples obtained by one time sampling relevant research currently available is based on one off sampling rao 2006 mirghani et al 2012 srivastava and singh 2015 and has not given sufficient attention to this issue latin hypercube sampling lhs is a multidimensional stratified sampling method proposed by mckay et al 2000 for computer simulation experiments the lhs method is a mathematical method based on stochastic mathematical theory compared with the traditional random sampling method lhs introduces the concept of layering which randomly samples the sample values throughout the sample space at the same time it can also ensure that data points do not agglomerate or in other words it ensures good coverage of the entire sampling space in addition the surrogate model is used to determine the optimization model identification space from both forward and backward directions and the training sample of the surrogate model is improved based on the groundwater contaminant source inversion identification results as feedback information this method not only enables the training samples to cover the neighborhood of the optimal solution effectively but also improves identification efficiency the optimization model coupled with the surrogate model is generally a heuristic optimization algorithm singh and datta 2006 used a genetic algorithm to simultaneously identify the locations and release histories of groundwater contaminant sources and compared the identification results with observations with errors ayvaz 2010 used the harmonic search algorithm to solve an optimization model for groundwater contaminant source identification and compared the observations with different degrees of error on the identification results zhao et al 2016 used the simulation optimization method based on the kriging surrogate model to invert the identification of contaminant sources in a two dimensional heterogeneous aquifer this study combined traditional optimization algorithms with quantum computation to improve computing power the concept of quantum computation was first proposed by benioff 1980 and has the characteristics of exponential storage capacity parallelism and exponential acceleration its theory of entanglement superposition and interference of quantum states may also contribute to our understanding of intelligence this may be a solution to calculation problems narayanan and moore 1996 first proposed a good combination of evolutionary algorithms with quantum computation and successfully solved the traveling salesman problem perfectly mirghani et al 2009 organically integrated quantum computation and genetic algorithms creating a solution that had both the traditional advantages of genetic algorithms and the excellent characteristics of quantum computation including the advantage of seeking the best solution faster calculation rate and smaller overall calculation scale mikki and kishk 2006 proposed a quantum particle swarm algorithm and used it to analyze and calculate the antenna layout problem comparing the two algorithms reveals that the optimization results and convergence speed were better than those of the classical particle swarm optimization algorithm han et al 2001 sun et al 2004 therefore using a genetic algorithm ga and a particle swarm optimization algorithm pso two traditional heuristic algorithms were combined with quantum computing and the stability of the identification of groundwater contaminant source release history in the two cases was compared with the four optimization methods with regard to the time cost and accuracy of the identification results third identifying groundwater contaminant source release histories is a typical inverse problem and in some cases the identification result is ill posed neuman 1973 first introduced inverse problem solving theory into the field of hydrology and water resources and divided inversion methods into direct and indirect methods for the inverse problem if the solution satisfies the three conditions of existence uniqueness and stability the problem is said to be well posed for an hypothetical model the solution to the inverse problem is still very complex in practical applications it is difficult to obtain the distribution of contaminant at any point in time at each node and the observed data generally contain errors therefore ill posed problems arise and have always been difficult to solve therefore to avoid ill posed problems the four optimization models are coupled with the kelm surrogate model to identify groundwater contaminant sources and release histories and obtain four sets of identification results the results of the four groups are not completely similar and there are great differences in identifying the individual contaminant sources therefore for the unknown contaminant source release histories it is not reliable to use only one model to obtain results based on the four sets of results a historical value range of source release histories is provided instead of a single identification result to address the challenges of ill posed nature and high time consumption both the surrogate model and the optimization algorithm have been improved and combined in this study to improve the accuracy and efficiency of solving the inverse problem to improve the efficiency of identifying groundwater contaminant sources the kelm is used as a surrogate for the time consuming simulation model forward and backward surrogate models are used for real world cases to decrease the number of surrogate samples furthermore four heuristic search algorithms are used to improve the accuracy of the identification results the above approach has been tested in both hypothetical and actual cases fig 1 2 methodology 2 1 simulation model the simulation optimization method contains a simulation model that makes up the principal section as an equality constraint in an optimization formulation datta et al 2011 the governing partial differential equation for steady state flow in a two dimensional aquifer system can be stated as follows 1 x i t i j h x j q i j 1 2 where x is the gradient operator vector tij is the transmissivity tensor and q is the volumetric flux per unit area the partial differential equation that describes the transport of a contaminant in a two dimensional aquifer system can be written as follows pinder and bredehoeft 1968 2 θ c t x i θ d i j c x i x i θ c v i c s q b i j 1 2 where b is the thickness of the aquifer c the dissolved solids concentration dij the hydrodynamic dispersion tensor vi the average linear seepage velocity cs the dissolved solids concentration in a source or sink flux and θ the effective porosity note that eq 2 is related to eq 1 through darcy s law as follows 3 v i k i j θ h x j i j 1 2 where kij is the hydraulic conductivity and h is the hydraulic head 2 2 surrogate model the surrogate model replaces the simulation model coupling with the optimization model to identify groundwater contaminant the input data to the simulation model and the surrogate model were the same and were obtained by the latin hypercube sampling method that is the inputs were the source concentration released history the outputs were also same that is the observation value of the observation wells in the actual case to improve identification efficiency for appropriate samples the surrogate model was used from both forward and backward directions the inputs of the forward surrogate model were the samples of contaminant source release histories and the outputs were the observed well values obtained through the simulation model in contrast in the input and output data of the backward surrogate model the contaminant source release histories were derived from the known actual well observations in addition the selected surrogate model was kelm which has fast and stable convergence the extreme learning machine elm is a new machine learning algorithm proposed by huang et al 2006 the output weights of the learning network can be solved for analytically through one step calculation the generalization ability and learning speed are greatly improved over the traditional neural network shi et al 2014 wong et al 2013 however the elm regression model has some problems such as unsatisfactory generalization ability and instability to solve these problems huang et al 2012 proposed the kelm and introduced a kernel function to replace the hidden layer feature mapping process that contained the activation function in the original elm this version can produce stable output and its classification and fitting abilities are better than the non core elm method chen et al 2014 the main calculation steps are as follows for n training samples xk pk k 1 n the output of elm can be expressed as 4 z 1 l β z g ω z x k m z f x k y k where g ω z xk mz is the output function of the z th hidden neuron g is the excitation function ω z is the weight vector connecting the input node and the z th hidden neuron mz is the threshold of the z th hidden neuron β z is the weight vector connecting the i th hidden neuron to the output neuron yk is the output of the limit learning machine and l is the number of hidden layer neurons wang and han 2014 eq 4 can be expressed as follows 5 h β y where β β1 β l t y y 1 yn t and h is the output matrix of the elm hidden layer 6 h g ω 1 x 1 m 1 g ω l x 1 m l g ω 1 x n m 1 g ω l x n m l n l if the ultimate learning machine model with l hidden nodes can learn n training samples unbiased then the following equation exists wang and han 2014 7 z 1 l β z g ω z x k m z f x k t k k 1 n where tp is the target value eq 7 can be expressed as follows 8 h β p using the least squares methods to solve eq 8 9 β h p where h is the moore penrose generalized inverse matrix of h then the kelm original optimization problem can be expressed as 10 m i n 1 2 β 2 w 2 k 1 n δ k 2 s t h x k t β p k δ k where β is a vector in the eigenspace f w is the regularized parameter h xk is the mapping of the input variable x in the f space and δ k is the error according to the karush kuhn tucker kkt theory elm training is equivalent to solving the following dual optimization problem 11 l d 1 2 β 2 w 2 k 1 n δ k 2 k 1 n ε k h x k t β p k δ k where ε z represents a lagrangian operator according to the kkt optimization condition eq 11 can be obtained as follows 12 l d β 0 β k 1 n ε k h x k l d δ k 0 θ z w δ k l d δ k 0 h x k t β t k δ k 0 the kernel matrix of elm is 13 k e l m h h t 14 k e l m z p h x k h x z k x k x z kelm s output function can be expressed as 15 f x h x h t h h t 1 v 1 p k x x 1 k x x k k e l m 1 w 1 p where p p 1 pn t 2 3 optimization model genetic algorithm particle swarm optimization were chosen and combines with quantum computation the quantum genetic algorithm uses the quantum chromosome coding method and uses the effect of quantum gates to update the parent individuals thereby generating new offspring individuals compared with traditional genetic algorithms qga can find the optimal solution to a problem in a shorter time with a smaller population size quantum particle swarm optimization uses information sharing strategy which has better aggregation degree and higher search accuracy so qpso has a simple structure good versatility and fast convergence the design variables and objective functions of the four optimization models are the same namely observation values of observation wells and contaminant sources release history the constraints of different models are slightly different but the minimum value must be greater than zero 2 3 1 quantum computation the essence of quantum computation is to reduce the complexity of large scale problems by using the properties of quantum theory the superposition coherence entanglement and quantum computational parallelism of quantum computation 2 3 1 1 superposition of quantum states in computer languages information is described as binary either 0 or 1 however the existence of qubits is unique and they can exist in an intermediate state other than 0 and 1 this state is derived from the superposition of states of 0 and 1 this property is a quantum state in quantum information and a classic difference in physical state used in classical information this property can set the specific state n to a common register according to the quantum mechanics correlation principle it is derived from the specific state of the 2n ground states superposed onto each other the coherent superposition state ϕ among them the relationship between the ground state and the superposition state can be described as 16 φ e γ e φ e 2 3 1 2 coherence of quantum states the specific concept of quantum state coherence is the following if the ground state of a quantum system is in a superposition state then the quantum system is considered to be coherent in quantum computing it is important to make the different ground states constituting the superposition state interfere with the interaction of quantum gates so that the phase can be changed suppose that a superposition of a quantum can be expressed as φ 3 10 0 1 10 1 1 10 3 1 using the quantum gate u 1 2 1 1 1 1 to interact with it the effect between the two can be expressed as φ 2 5 0 1 5 1 from eq 16 when the probability of the ground state 0 decreases 1 increases instead 2 3 1 3 entanglement of quantum states entanglement of quantum states occurs in quantum systems that contain multiple subsystems if certain states exist in the two subsystems where they occur if they cannot be represented as the tensor product of the two subsystems then this state can be called an entangled state nielsen and chuang 2007 the entanglement of quanta is an important feature distinguishing quantum mechanics from classical physics almost all quantum information processing is related to quantum entanglement and therefore quantum entanglement properties occupy an important position in quantum information science 3 parallelism in quantum computation the parallel capability of quantum computation is actually an extension of the superposition of quantum states under certain ground conditions the quantum states are used to evolve quantum states in quantum registers and the vector matrix a in hilbert space is used to represent quantum gates quantum gates are constrained and therefore the effect on the quantum state in hilbert space is manifested as wholeness that is the representation of all the base points and the above characteristics are what we call quantum parallelism this property makes it possible to effectively improve calculation speed making it possible to complete work that is difficult for classical computers 3 1 criteria for surrogate evaluation some common statistical indicators specifically relative error and coefficient of determination were used to test model performance 1 the relative error re for a particular parameter is defined as follows and is expressed as a percentage 17 r e z t z t z t 100 where t is the number of data points z t is an observed value and z t is a predicted value 2 the coefficient of determination r 2 reflects the agreement between the observed and predicted values and ranges from 0 to 1 r 2 can be expressed as follows 18 r 2 1 t 1 n z t z t 2 t 1 n z t z t 2 where z t is the mean of the observations 4 basic introduction to the aquifer fig 2 shows the plan view for a hypothetical case the aquifer has specified head boundary conditions on the upper left ab and lower right cd sides and there is no flow on the other sides the head values on both sides ab and cd are 100 0 m and 80 0 m respectively the aquifer is divided into five hydraulic conductivity zones and the hydraulic conductivity values in each zone are constant the hydraulic conductivities are k1 34 56 m d k2 17 28 m d k3 8 64 m d k4 25 92 m d and k5 60 48 m d table 1 shows the hydrogeological parameters of the aquifer there are two contaminant sources and seven sampling locations and one pumping well is present at the center of this aquifer table 2 lists the actual values for the source fluxes the actual case is the case studied by gzyl et al 2014 fig 3 shows its plan its location is in the city of jaworzno in the wąwolnica valley in southern poland there are five main areal sources of contaminant 1 cso main 4 internal 5 cso north and ls divided into lsn 6a and lss 6b the exact value of the source contaminant concentration is unknown however it is known that the contaminant released by the contaminant source are constant during the simulation fifteen observation wells were arranged along the direction perpendicular to the water flow the simulation period was 120 months detailed information on the hydrological and geological parameters of the study area can be found in that paper the gms model was used to simulate contaminant migration fig 4 shows the distribution of the contaminant plumes for the actual case the actual observed value at the observation well and the values simulated by the four identification models are shown in fig 5 5 results and discussion 5 1 the surrogate model 5 1 1 hypothetical case the surrogate model was trained differently in the hypothetical and actual cases in the hypothetical case 500 sets of input values were sampled the input data were entered into the simulation model to obtain the corresponding output data which were the observed concentrations at the observation well 450 groups were used as training data and 50 groups were used as test data r 2 was used to characterize the training results the results showed that r 2 was as high as 0 9990 in the hypothetical case fig 6 shows the training results for seven observation wells the simulation accuracy for each observation well was greater than 0 99 indicating that the kelm surrogate model had higher simulation accuracy for extreme values moreover the kelm surrogate model was stable and had high simulation accuracy for observation wells at different positions 5 1 2 actual case in the actual case 50 sets of input data were sampled first forty data sets were used to train the simulation model and the other ten data sets were used to test the accuracy of the surrogate model the results show that in the actual case r 2 was as high as 0 9948 fig 7 shows the training results for the 15 observation wells the accuracies at wells 1 2 3 and 10 were relatively low with r 2 greater than 0 82 but the prediction accuracy was still acceptable the reason for this is that the values required for the surrogate model simulation were too small compared with the observed values at the other observation wells the simulated values of r 2 were all greater than 0 92 at the other observation wells nine of the fifteen observation wells had r 2 greater than 0 99 indicating that the kelm surrogate model also had high accuracy in actual cases after training with 50 data sets the kelm surrogate model could be coupled with the optimization model the calculation search range was provided before optimization which improved calculation efficiency the reverse kelm surrogate model could quickly determine the search range the input of the forward surrogate model is the contaminant source release history and the output is the observation values however the input of the backward surrogate model is the observation values and the output value is the contaminant source release history to be sought fig 8 shows the predicted results an r 2 of 0 9605 was obtained indicating that the range of values obtained using the reverse kelm surrogate model could be applied to the optimization model the trained surrogate model was coupled with the four optimization algorithms and the identification results were obtained if the error between the simulated observed contaminant concentrations at the observation wells corresponding to the identification results and the actual observed contaminant concentrations was within a reasonable range the result was considered as the final contaminant concentration of the source otherwise training data were added within a certain numerical range according to the identification result and the above steps were repeated until the error was reduced to an acceptable range in this study the accuracy of contaminant identification results based on 50 sets of training data was low therefore to improve identification accuracy the number of initial input and output data sets was increased by 50 groups the value range of the additional 50 sets of training data is appropriately adjusted according to the previous identification results the resulting 100 sets of input and output data for each contaminant source conformed to the normal distribution fig 9 which shows that 100 data sets were sufficient according to the identification results of the four models the next sampling value range was determined as shown in table 3 the upper limit of each interval is the maximum of the four optimization model identification results the lower limit of the interval is to choose the minimum value 5 2 optimization results 5 2 1 hypothetical case in the hypothetical case the surrogate model was trained and coupled with the optimization model to identify the contaminant concentrations of the groundwater contaminant sources for the four optimization models the initial population was 50 and the number of iterations was 500 after 500 iterations the iterative process of each optimization model and the final identification result were recorded as shown in fig 10 it is apparent that the error of all the optimization models was stable after 500 iterations and that the optimization result was the final identification result the accuracy of the identification was reflected by the r 2 and re values which are shown in table 4 comparing the results of the four optimization algorithms shows that the quantum genetic algorithm qga and quantum particle swarm optimization gpso algorithms achieved higher accuracy than the other two optimization algorithms and were very effective in identifying groundwater contaminant sources in comparison the accuracy of the ga algorithm was lower but still within a reasonable range taking an overall view the r 2 of the four optimization algorithms were all greater than 0 94 and re was within 10 indicating that the four identification models had high accuracy for identification and could be applied to the actual case 5 2 2 actual case in the actual case after the optimization interval was determined 50 data sets were used to identify the groundwater contaminant sources fig 11 shows the results for the four identification models different identification models had different identification results for the contaminant sources especially for contaminant sources lsn and lss however the results obtained by the four identification models were very close to each other at five of the contaminant sources among the observed contaminant concentrations at the 15 observation wells contaminant source cso main had the greatest impact and was therefore the easiest to identify contaminant sources internal and cso north had the second greatest effect in contrast sources lsn and lss had the least impact on observed well values the four groups of identification results were input into the simulation model to obtain the observed well values and compared with the known observed values the results are shown in table 5 clearly the r 2 of the observed values corresponding to the identification results are all between 0 93 and 0 94 indicating that the overall trend of the observed values is consistent with the trend of the actual observed values however the re of the simulated observed values is larger for example the re of the ga algorithm is close to 30 the re values of the other simulated observations all exceed 20 and there are large numerical errors for individual observation wells fig 12 shows the concentration of contaminant sources identified by the 100 groups of training values it is evident that different identification models still have different results for the same contaminant source identification for example at contaminant source lss the maximum value calculated by the ga was 2 4580 mg l but the qpso algorithm calculated that no contaminant was discharged there compared with table 5 the discharge contaminant concentration of each contaminant source also changed for example at contaminant source internal the concentration decreased from 0 5 0 3 mg l to 0 3 0 2 mg l for the other three contaminant sources the discharge concentration change was relatively small by analyzing the accuracy of the simulated observation values corresponding to the identification result it is possible to judge whether the identification result is acceptable table 6 shows the accuracy of the second set of identification results corresponding to the simulated observation values it is apparent that r 2 changed less than 1 the reason for this is that even when the number of training groups was increased there was no significant change in the overall prediction trend of the surrogate model however re showed a relatively obvious change in the case with fewer training data points the re of the ga algorithm was close to 30 after the number of training data points was increased re decreased to less than 20 the re of the other three optimization algorithms all remained within 20 indicating that the simulated observation values conformed to the overall trend of the actual observation values and that the accuracy of a single value was also within a reasonable range to judge whether identification results are reasonable and reliable it is necessary to test not only the accuracy of the simulated observations but also the convergence of the four identification models fig 13 shows convergence plots for the four models after 200 iterations the convergence curves of all the optimization algorithms are stable among these after 200 iterations of the pso algorithm although the iteration curve was stable it did not last for a long time however the fitted value was less than 0 4 indicating that the identification result can be accepted in identifying the release histories of groundwater contaminant sources not only must the accuracy and reliability of the identification results be considered but also the time cost of the identification process fig 14 presents a direct comparison of the time taken by the four optimization models to perform 200 iterations after the same initial population numbers were set and based on the same 100 groups of training data the time taken by the four identification models was very short but the qpso model had the shortest running time the calculations of all models were done a pc with an intel 3 4 ghz processor and 16 gb ram compared with the performance of the four identification models in the two cases the identification accuracy in the hypothetical case was better than that in the actual case the main reason was that it was more difficult to reconstruct the actual hydrologic and geological information in the actual case however a comprehensive analysis of the performance of the four identification models in both cases showed that each model performed very well in the case with less training data kelm surrogate simulation accuracy could still be maintained at a high level ga and pso which are two traditional optimization models have good simulation foundation and high identification accuracy however the combination of these two models with quantum computing not only improved their computational efficiency but also their computational accuracy the pso algorithm was still unstable after 200 iterations but the qpso algorithm became stable after about 50 iterations and the calculation time was shortened therefore the combination of a traditional optimization model with quantum computing can improve the model nevertheless this study still needs to integrate the identification results of the four identification models and to achieve identification of the release histories of unknown contaminant sources otherwise the results obtained by the algorithm will be invalid the solution will not be unique or the solution will be discontinuous under the boundary to avoid this problem the identification results of the four models should be considered comprehensively and the final identification results should not be a set of data but an interval table 7 shows the final results in the research of gzyl et al 2014 the accurate release histories of the actual contaminant sources were the parameters to be solved for and the intervals for these release histories were finally obtained as shown in table 7 by comparing the intervals of the contaminant source release histories the value range obtained through synthesis of the four optimization models in this study could be made narrower among the five contaminant sources the identification result interval of contaminant source cso north was significantly different from that of gzyl et al 2014 the main reason was that in both studies the release histories of the contaminant sources were the parameter to be solved for moreover there were differences among the various models for simulating groundwater contaminant transport which may have had a great influence on the results in addition the numerical solutions for groundwater flow and transport were also different which could have had a slight impact on the results 6 conclusions the contaminant identification model used in this study was applied to two cases one hypothetical and another actual the kernel based extreme learning machine coupled optimization model was used to obtain the identification model the contaminant release history identification model was used in the hypothetical case and the actual case respectively the results show that the four identification models not only have high accuracy but also fast identification speed the shape area size and hydrological and geological parameters are set in the hypothetical case in contrast it was more difficult to generalize the actual case than to set up the hypothetical case moreover the release histories of the contaminant sources were unknown by identifying the contaminant concentrations of the sources in the actual case and comparing them with the hypothetical case it could be concluded that 1 kelm has faster running speed and higher precision than the simulation model coupled with the four optimization models it performed well in all cases in the actual case after using the forward surrogate model the surrogate model is used to reversely determine the source s contaminant concentration range and the surrogate model is retrained by using this method to identify the release histories of unknown contaminant sources not only can sufficiently training data be ensured but also the concentration range of contaminant sources can be determined more quickly and the calculation efficiency can be improved 2 through comparative analysis differences were found in the performance of the four optimization models with fewer input and output data points the identification results of the combination of quantum computing with ga or pso were better than those of the uncombined models moreover by comparing the time required for contaminant source identification the optimization algorithm combined with quantum computing was faster the combination of quantum computing characteristics with traditional heuristic optimization algorithms not only can achieve a stable identification process but also has high identification accuracy and fast identification speed 3 the release histories of groundwater contaminant sources were identified by means of four optimization methods more importantly four different identification results were obtained by the four identification models which together provided a range of concentrations for the contaminant sources this avoids the problem of ill posed identifications from a single optimization model improves the stability of the identification result and provides a reliable basis for contaminant control credit authorship contribution statement ying zhao conceptualization writing review editing ruizhuo qu writing original draft methodology software zhenxiang xing funding acquisition visualization wenxi lu methodology software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national natural science foundation of china no 41807196 no 51979038 no 41672232 no 41972252 the national key r d program of china no 2017yfc0406004 the postdoctoral science foundation of china no 2018m641793 the postdoctoral science foundation of heilongjiang province of china no lbh z19002 the natural science foundation of heilongjiang province of china no e2015024 and the academic backbones foundation of northeast agricultural university no 16xg11 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103540 appendix supplementary materials image application 1 
502,identifying groundwater contaminant sources based on a kelm surrogate model together with four heuristic optimization algorithms ying zhao conceptualization writing review editing a b ruizhuo qu writing original draft methodology software c zhenxiang xing funding acquisition visualization a b wenxi lu methodology software d a school of water conservancy and civil engineering northeast agricultural university harbin 150030 china school of water conservancy and civil engineering northeast agricultural university harbin 150030 china aschool of water conservancy and civil engineering northeast agricultural university harbin 150030 china b key laboratory of high efficiency utilization of agricultural water soil resources of ministry of agriculture of china harbin 150030 china key laboratory of high efficiency utilization of agricultural water soil resources of ministry of agriculture of china harbin 150030 china bkey laboratory of high efficiency utilization of agricultural water soil resources of ministry of agriculture of china harbin 150030 china c school of environment harbin institute of technology harbin 150090 china school of environment harbin institute of technology harbin 150090 china cschool of environment harbin institute of technology harbin 150090 china d college of new energy and environment jilin university changchun 130021 china college of new energy and environment jilin university changchun 130021 china dcollege of new energy and environment jilin university changchun 130021 china corresponding author identifying groundwater contaminant sources involves a reverse determination of the source characteristics by monitoring contaminant concentrations in a few observation wells however due to the ill posed nature and high time consumption of identification an efficient identification process with accurate estimated results is particularly important to improve the efficiency of identifying groundwater contaminant sources a kernel based extreme learning machine was used as a surrogate for the time consuming simulation model four heuristic search algorithms were used to improve the accuracy of the identification results the proposed approach was tested in both hypothetical and actual cases the conclusions are 1 by forward and backward calculation of the surrogate model the time cost of identifying groundwater sources can be reduced significantly 2 when a traditional genetic algorithm and a particle swarm optimization algorithm are combined with quantum computing computational efficiency and accuracy are both improved and 3 by using various search algorithms to identify unknown contaminant sources in the actual case the range of release histories of each contaminant source can be obtained decreasing the ill posed nature of the identification result obtained by a single algorithm and improving the reliability of the identification results keywords groundwater contaminant heuristic search algorithms identifying contaminant sources kernel based extreme learning machine 1 introduction unlike surface water contaminant groundwater contamination is concealed underground and is often not quickly discovered meaning that information on contaminant sources is difficult to obtain to provide a reliable basis for treatment and restoration of groundwater contaminant sources identification of the groundwater contaminant source release history has become a key point however current research on identifying groundwater contaminant sources suffers from three main problems first most existing research studied only hypothetical cases ayvaz 2010 liu et al 2010 wang and jin 2013 gurarslan and karahan 2015 although hypothetical cases have many advantages they also have limitations for example in hypothetical cases it can be assumed that the aquifer parameters are known and that the groundwater model is completely correct hence only information about the groundwater source is unknown for real world cases not only is information about groundwater contaminant sources unknown but also the aquifer parameters aquifer parameters can be obtained through experiments and calculations but there are also uncertainties in the constructed aquifer in addition a large amount of uncertainty exists in groundwater models including the uncertainty of the observed data and the model structure singh et al 2010 the uncertainty of real world cases seriously affects the identification of groundwater contaminant sources however unless the validity of the identification method is verified by a hypothetical case there are also instability problems when the model is applied to actual cases therefore established identification models should be applied to both hypothetical and actual cases so that the validity and practicability of the identification model can both be verified second there are efficiency and accuracy issues in identifying the release histories of contaminant sources rapid and stable identification of these release histories is the key to this research the main method used has become the identification of groundwater contaminant sources by simulation and optimization however if the simulation model is repeatedly used for iterative solution of the optimization model it will result in a huge computational load which seriously restricts the feasibility of the simulation optimization method in practical inversion identification therefore constructing surrogates for simulation models has become a leading issue in recent research forrester and keane 2009 srivastava and singh 2014 zhao et al 2016 qin et al 2007 used polynomial regression to create surrogate models the most widely used approach is the kriging surrogate model simpson et al 2001 luo and lu 2014 wan et al 2005 kriging is used because it can spatially model and predict fields and processes at the same time it can approximate and map complex nonlinear functions however this method easily falls into the local optimal solution especially when the distance constraint is too small which will affect the robustness of the algorithm zhou et al 2015 there is also a least squares support vector machine that captures the complex relationship between inputs and outputs and effectively extracts structural information from noisy data zhang et al 2009 the disadvantage is that it is difficult to determine a reasonable parameter range which affects the speed and accuracy of fault diagnoses to a certain extent zhang et al 2008 in addition surrogate models include support vector machines jin et al 2001 multivariate adaptive regression splines hou and lu 2018 ensemble surrogate model xing et al 2019 and others in this study a kernel based extreme learning machine kelm was used as a surrogate for the simulation model when running the optimization model the kelm model learns quickly huang et al 2012 but during its operation multiple types of problems must be converted into sub problems of the two classifications due to the introduction of the kernel function the kernel limit learning machine can be simplified to the execution of a least squares support vector machine for a traditional feed forward neural network artificial adjustment parameters are needed resulting in slow operation and a tendency to fall into local optima whereas the kelm model has strong generalization performance jain et al 1996 li et al 2016 use of surrogate models to save time in identifying the release histories of contaminant sources should also consider the accuracy of the identification results razavi et al 2012 the accuracy of the surrogate model in approximating the simulation model depends on the coverage of the sample space by the training sample but the number of training samples used to establish the surrogate model is limited and there is no guarantee that the training sample set can cover the sample space well therefore it is difficult to establish a sufficiently reliable surrogate model using only the training samples obtained by one time sampling relevant research currently available is based on one off sampling rao 2006 mirghani et al 2012 srivastava and singh 2015 and has not given sufficient attention to this issue latin hypercube sampling lhs is a multidimensional stratified sampling method proposed by mckay et al 2000 for computer simulation experiments the lhs method is a mathematical method based on stochastic mathematical theory compared with the traditional random sampling method lhs introduces the concept of layering which randomly samples the sample values throughout the sample space at the same time it can also ensure that data points do not agglomerate or in other words it ensures good coverage of the entire sampling space in addition the surrogate model is used to determine the optimization model identification space from both forward and backward directions and the training sample of the surrogate model is improved based on the groundwater contaminant source inversion identification results as feedback information this method not only enables the training samples to cover the neighborhood of the optimal solution effectively but also improves identification efficiency the optimization model coupled with the surrogate model is generally a heuristic optimization algorithm singh and datta 2006 used a genetic algorithm to simultaneously identify the locations and release histories of groundwater contaminant sources and compared the identification results with observations with errors ayvaz 2010 used the harmonic search algorithm to solve an optimization model for groundwater contaminant source identification and compared the observations with different degrees of error on the identification results zhao et al 2016 used the simulation optimization method based on the kriging surrogate model to invert the identification of contaminant sources in a two dimensional heterogeneous aquifer this study combined traditional optimization algorithms with quantum computation to improve computing power the concept of quantum computation was first proposed by benioff 1980 and has the characteristics of exponential storage capacity parallelism and exponential acceleration its theory of entanglement superposition and interference of quantum states may also contribute to our understanding of intelligence this may be a solution to calculation problems narayanan and moore 1996 first proposed a good combination of evolutionary algorithms with quantum computation and successfully solved the traveling salesman problem perfectly mirghani et al 2009 organically integrated quantum computation and genetic algorithms creating a solution that had both the traditional advantages of genetic algorithms and the excellent characteristics of quantum computation including the advantage of seeking the best solution faster calculation rate and smaller overall calculation scale mikki and kishk 2006 proposed a quantum particle swarm algorithm and used it to analyze and calculate the antenna layout problem comparing the two algorithms reveals that the optimization results and convergence speed were better than those of the classical particle swarm optimization algorithm han et al 2001 sun et al 2004 therefore using a genetic algorithm ga and a particle swarm optimization algorithm pso two traditional heuristic algorithms were combined with quantum computing and the stability of the identification of groundwater contaminant source release history in the two cases was compared with the four optimization methods with regard to the time cost and accuracy of the identification results third identifying groundwater contaminant source release histories is a typical inverse problem and in some cases the identification result is ill posed neuman 1973 first introduced inverse problem solving theory into the field of hydrology and water resources and divided inversion methods into direct and indirect methods for the inverse problem if the solution satisfies the three conditions of existence uniqueness and stability the problem is said to be well posed for an hypothetical model the solution to the inverse problem is still very complex in practical applications it is difficult to obtain the distribution of contaminant at any point in time at each node and the observed data generally contain errors therefore ill posed problems arise and have always been difficult to solve therefore to avoid ill posed problems the four optimization models are coupled with the kelm surrogate model to identify groundwater contaminant sources and release histories and obtain four sets of identification results the results of the four groups are not completely similar and there are great differences in identifying the individual contaminant sources therefore for the unknown contaminant source release histories it is not reliable to use only one model to obtain results based on the four sets of results a historical value range of source release histories is provided instead of a single identification result to address the challenges of ill posed nature and high time consumption both the surrogate model and the optimization algorithm have been improved and combined in this study to improve the accuracy and efficiency of solving the inverse problem to improve the efficiency of identifying groundwater contaminant sources the kelm is used as a surrogate for the time consuming simulation model forward and backward surrogate models are used for real world cases to decrease the number of surrogate samples furthermore four heuristic search algorithms are used to improve the accuracy of the identification results the above approach has been tested in both hypothetical and actual cases fig 1 2 methodology 2 1 simulation model the simulation optimization method contains a simulation model that makes up the principal section as an equality constraint in an optimization formulation datta et al 2011 the governing partial differential equation for steady state flow in a two dimensional aquifer system can be stated as follows 1 x i t i j h x j q i j 1 2 where x is the gradient operator vector tij is the transmissivity tensor and q is the volumetric flux per unit area the partial differential equation that describes the transport of a contaminant in a two dimensional aquifer system can be written as follows pinder and bredehoeft 1968 2 θ c t x i θ d i j c x i x i θ c v i c s q b i j 1 2 where b is the thickness of the aquifer c the dissolved solids concentration dij the hydrodynamic dispersion tensor vi the average linear seepage velocity cs the dissolved solids concentration in a source or sink flux and θ the effective porosity note that eq 2 is related to eq 1 through darcy s law as follows 3 v i k i j θ h x j i j 1 2 where kij is the hydraulic conductivity and h is the hydraulic head 2 2 surrogate model the surrogate model replaces the simulation model coupling with the optimization model to identify groundwater contaminant the input data to the simulation model and the surrogate model were the same and were obtained by the latin hypercube sampling method that is the inputs were the source concentration released history the outputs were also same that is the observation value of the observation wells in the actual case to improve identification efficiency for appropriate samples the surrogate model was used from both forward and backward directions the inputs of the forward surrogate model were the samples of contaminant source release histories and the outputs were the observed well values obtained through the simulation model in contrast in the input and output data of the backward surrogate model the contaminant source release histories were derived from the known actual well observations in addition the selected surrogate model was kelm which has fast and stable convergence the extreme learning machine elm is a new machine learning algorithm proposed by huang et al 2006 the output weights of the learning network can be solved for analytically through one step calculation the generalization ability and learning speed are greatly improved over the traditional neural network shi et al 2014 wong et al 2013 however the elm regression model has some problems such as unsatisfactory generalization ability and instability to solve these problems huang et al 2012 proposed the kelm and introduced a kernel function to replace the hidden layer feature mapping process that contained the activation function in the original elm this version can produce stable output and its classification and fitting abilities are better than the non core elm method chen et al 2014 the main calculation steps are as follows for n training samples xk pk k 1 n the output of elm can be expressed as 4 z 1 l β z g ω z x k m z f x k y k where g ω z xk mz is the output function of the z th hidden neuron g is the excitation function ω z is the weight vector connecting the input node and the z th hidden neuron mz is the threshold of the z th hidden neuron β z is the weight vector connecting the i th hidden neuron to the output neuron yk is the output of the limit learning machine and l is the number of hidden layer neurons wang and han 2014 eq 4 can be expressed as follows 5 h β y where β β1 β l t y y 1 yn t and h is the output matrix of the elm hidden layer 6 h g ω 1 x 1 m 1 g ω l x 1 m l g ω 1 x n m 1 g ω l x n m l n l if the ultimate learning machine model with l hidden nodes can learn n training samples unbiased then the following equation exists wang and han 2014 7 z 1 l β z g ω z x k m z f x k t k k 1 n where tp is the target value eq 7 can be expressed as follows 8 h β p using the least squares methods to solve eq 8 9 β h p where h is the moore penrose generalized inverse matrix of h then the kelm original optimization problem can be expressed as 10 m i n 1 2 β 2 w 2 k 1 n δ k 2 s t h x k t β p k δ k where β is a vector in the eigenspace f w is the regularized parameter h xk is the mapping of the input variable x in the f space and δ k is the error according to the karush kuhn tucker kkt theory elm training is equivalent to solving the following dual optimization problem 11 l d 1 2 β 2 w 2 k 1 n δ k 2 k 1 n ε k h x k t β p k δ k where ε z represents a lagrangian operator according to the kkt optimization condition eq 11 can be obtained as follows 12 l d β 0 β k 1 n ε k h x k l d δ k 0 θ z w δ k l d δ k 0 h x k t β t k δ k 0 the kernel matrix of elm is 13 k e l m h h t 14 k e l m z p h x k h x z k x k x z kelm s output function can be expressed as 15 f x h x h t h h t 1 v 1 p k x x 1 k x x k k e l m 1 w 1 p where p p 1 pn t 2 3 optimization model genetic algorithm particle swarm optimization were chosen and combines with quantum computation the quantum genetic algorithm uses the quantum chromosome coding method and uses the effect of quantum gates to update the parent individuals thereby generating new offspring individuals compared with traditional genetic algorithms qga can find the optimal solution to a problem in a shorter time with a smaller population size quantum particle swarm optimization uses information sharing strategy which has better aggregation degree and higher search accuracy so qpso has a simple structure good versatility and fast convergence the design variables and objective functions of the four optimization models are the same namely observation values of observation wells and contaminant sources release history the constraints of different models are slightly different but the minimum value must be greater than zero 2 3 1 quantum computation the essence of quantum computation is to reduce the complexity of large scale problems by using the properties of quantum theory the superposition coherence entanglement and quantum computational parallelism of quantum computation 2 3 1 1 superposition of quantum states in computer languages information is described as binary either 0 or 1 however the existence of qubits is unique and they can exist in an intermediate state other than 0 and 1 this state is derived from the superposition of states of 0 and 1 this property is a quantum state in quantum information and a classic difference in physical state used in classical information this property can set the specific state n to a common register according to the quantum mechanics correlation principle it is derived from the specific state of the 2n ground states superposed onto each other the coherent superposition state ϕ among them the relationship between the ground state and the superposition state can be described as 16 φ e γ e φ e 2 3 1 2 coherence of quantum states the specific concept of quantum state coherence is the following if the ground state of a quantum system is in a superposition state then the quantum system is considered to be coherent in quantum computing it is important to make the different ground states constituting the superposition state interfere with the interaction of quantum gates so that the phase can be changed suppose that a superposition of a quantum can be expressed as φ 3 10 0 1 10 1 1 10 3 1 using the quantum gate u 1 2 1 1 1 1 to interact with it the effect between the two can be expressed as φ 2 5 0 1 5 1 from eq 16 when the probability of the ground state 0 decreases 1 increases instead 2 3 1 3 entanglement of quantum states entanglement of quantum states occurs in quantum systems that contain multiple subsystems if certain states exist in the two subsystems where they occur if they cannot be represented as the tensor product of the two subsystems then this state can be called an entangled state nielsen and chuang 2007 the entanglement of quanta is an important feature distinguishing quantum mechanics from classical physics almost all quantum information processing is related to quantum entanglement and therefore quantum entanglement properties occupy an important position in quantum information science 3 parallelism in quantum computation the parallel capability of quantum computation is actually an extension of the superposition of quantum states under certain ground conditions the quantum states are used to evolve quantum states in quantum registers and the vector matrix a in hilbert space is used to represent quantum gates quantum gates are constrained and therefore the effect on the quantum state in hilbert space is manifested as wholeness that is the representation of all the base points and the above characteristics are what we call quantum parallelism this property makes it possible to effectively improve calculation speed making it possible to complete work that is difficult for classical computers 3 1 criteria for surrogate evaluation some common statistical indicators specifically relative error and coefficient of determination were used to test model performance 1 the relative error re for a particular parameter is defined as follows and is expressed as a percentage 17 r e z t z t z t 100 where t is the number of data points z t is an observed value and z t is a predicted value 2 the coefficient of determination r 2 reflects the agreement between the observed and predicted values and ranges from 0 to 1 r 2 can be expressed as follows 18 r 2 1 t 1 n z t z t 2 t 1 n z t z t 2 where z t is the mean of the observations 4 basic introduction to the aquifer fig 2 shows the plan view for a hypothetical case the aquifer has specified head boundary conditions on the upper left ab and lower right cd sides and there is no flow on the other sides the head values on both sides ab and cd are 100 0 m and 80 0 m respectively the aquifer is divided into five hydraulic conductivity zones and the hydraulic conductivity values in each zone are constant the hydraulic conductivities are k1 34 56 m d k2 17 28 m d k3 8 64 m d k4 25 92 m d and k5 60 48 m d table 1 shows the hydrogeological parameters of the aquifer there are two contaminant sources and seven sampling locations and one pumping well is present at the center of this aquifer table 2 lists the actual values for the source fluxes the actual case is the case studied by gzyl et al 2014 fig 3 shows its plan its location is in the city of jaworzno in the wąwolnica valley in southern poland there are five main areal sources of contaminant 1 cso main 4 internal 5 cso north and ls divided into lsn 6a and lss 6b the exact value of the source contaminant concentration is unknown however it is known that the contaminant released by the contaminant source are constant during the simulation fifteen observation wells were arranged along the direction perpendicular to the water flow the simulation period was 120 months detailed information on the hydrological and geological parameters of the study area can be found in that paper the gms model was used to simulate contaminant migration fig 4 shows the distribution of the contaminant plumes for the actual case the actual observed value at the observation well and the values simulated by the four identification models are shown in fig 5 5 results and discussion 5 1 the surrogate model 5 1 1 hypothetical case the surrogate model was trained differently in the hypothetical and actual cases in the hypothetical case 500 sets of input values were sampled the input data were entered into the simulation model to obtain the corresponding output data which were the observed concentrations at the observation well 450 groups were used as training data and 50 groups were used as test data r 2 was used to characterize the training results the results showed that r 2 was as high as 0 9990 in the hypothetical case fig 6 shows the training results for seven observation wells the simulation accuracy for each observation well was greater than 0 99 indicating that the kelm surrogate model had higher simulation accuracy for extreme values moreover the kelm surrogate model was stable and had high simulation accuracy for observation wells at different positions 5 1 2 actual case in the actual case 50 sets of input data were sampled first forty data sets were used to train the simulation model and the other ten data sets were used to test the accuracy of the surrogate model the results show that in the actual case r 2 was as high as 0 9948 fig 7 shows the training results for the 15 observation wells the accuracies at wells 1 2 3 and 10 were relatively low with r 2 greater than 0 82 but the prediction accuracy was still acceptable the reason for this is that the values required for the surrogate model simulation were too small compared with the observed values at the other observation wells the simulated values of r 2 were all greater than 0 92 at the other observation wells nine of the fifteen observation wells had r 2 greater than 0 99 indicating that the kelm surrogate model also had high accuracy in actual cases after training with 50 data sets the kelm surrogate model could be coupled with the optimization model the calculation search range was provided before optimization which improved calculation efficiency the reverse kelm surrogate model could quickly determine the search range the input of the forward surrogate model is the contaminant source release history and the output is the observation values however the input of the backward surrogate model is the observation values and the output value is the contaminant source release history to be sought fig 8 shows the predicted results an r 2 of 0 9605 was obtained indicating that the range of values obtained using the reverse kelm surrogate model could be applied to the optimization model the trained surrogate model was coupled with the four optimization algorithms and the identification results were obtained if the error between the simulated observed contaminant concentrations at the observation wells corresponding to the identification results and the actual observed contaminant concentrations was within a reasonable range the result was considered as the final contaminant concentration of the source otherwise training data were added within a certain numerical range according to the identification result and the above steps were repeated until the error was reduced to an acceptable range in this study the accuracy of contaminant identification results based on 50 sets of training data was low therefore to improve identification accuracy the number of initial input and output data sets was increased by 50 groups the value range of the additional 50 sets of training data is appropriately adjusted according to the previous identification results the resulting 100 sets of input and output data for each contaminant source conformed to the normal distribution fig 9 which shows that 100 data sets were sufficient according to the identification results of the four models the next sampling value range was determined as shown in table 3 the upper limit of each interval is the maximum of the four optimization model identification results the lower limit of the interval is to choose the minimum value 5 2 optimization results 5 2 1 hypothetical case in the hypothetical case the surrogate model was trained and coupled with the optimization model to identify the contaminant concentrations of the groundwater contaminant sources for the four optimization models the initial population was 50 and the number of iterations was 500 after 500 iterations the iterative process of each optimization model and the final identification result were recorded as shown in fig 10 it is apparent that the error of all the optimization models was stable after 500 iterations and that the optimization result was the final identification result the accuracy of the identification was reflected by the r 2 and re values which are shown in table 4 comparing the results of the four optimization algorithms shows that the quantum genetic algorithm qga and quantum particle swarm optimization gpso algorithms achieved higher accuracy than the other two optimization algorithms and were very effective in identifying groundwater contaminant sources in comparison the accuracy of the ga algorithm was lower but still within a reasonable range taking an overall view the r 2 of the four optimization algorithms were all greater than 0 94 and re was within 10 indicating that the four identification models had high accuracy for identification and could be applied to the actual case 5 2 2 actual case in the actual case after the optimization interval was determined 50 data sets were used to identify the groundwater contaminant sources fig 11 shows the results for the four identification models different identification models had different identification results for the contaminant sources especially for contaminant sources lsn and lss however the results obtained by the four identification models were very close to each other at five of the contaminant sources among the observed contaminant concentrations at the 15 observation wells contaminant source cso main had the greatest impact and was therefore the easiest to identify contaminant sources internal and cso north had the second greatest effect in contrast sources lsn and lss had the least impact on observed well values the four groups of identification results were input into the simulation model to obtain the observed well values and compared with the known observed values the results are shown in table 5 clearly the r 2 of the observed values corresponding to the identification results are all between 0 93 and 0 94 indicating that the overall trend of the observed values is consistent with the trend of the actual observed values however the re of the simulated observed values is larger for example the re of the ga algorithm is close to 30 the re values of the other simulated observations all exceed 20 and there are large numerical errors for individual observation wells fig 12 shows the concentration of contaminant sources identified by the 100 groups of training values it is evident that different identification models still have different results for the same contaminant source identification for example at contaminant source lss the maximum value calculated by the ga was 2 4580 mg l but the qpso algorithm calculated that no contaminant was discharged there compared with table 5 the discharge contaminant concentration of each contaminant source also changed for example at contaminant source internal the concentration decreased from 0 5 0 3 mg l to 0 3 0 2 mg l for the other three contaminant sources the discharge concentration change was relatively small by analyzing the accuracy of the simulated observation values corresponding to the identification result it is possible to judge whether the identification result is acceptable table 6 shows the accuracy of the second set of identification results corresponding to the simulated observation values it is apparent that r 2 changed less than 1 the reason for this is that even when the number of training groups was increased there was no significant change in the overall prediction trend of the surrogate model however re showed a relatively obvious change in the case with fewer training data points the re of the ga algorithm was close to 30 after the number of training data points was increased re decreased to less than 20 the re of the other three optimization algorithms all remained within 20 indicating that the simulated observation values conformed to the overall trend of the actual observation values and that the accuracy of a single value was also within a reasonable range to judge whether identification results are reasonable and reliable it is necessary to test not only the accuracy of the simulated observations but also the convergence of the four identification models fig 13 shows convergence plots for the four models after 200 iterations the convergence curves of all the optimization algorithms are stable among these after 200 iterations of the pso algorithm although the iteration curve was stable it did not last for a long time however the fitted value was less than 0 4 indicating that the identification result can be accepted in identifying the release histories of groundwater contaminant sources not only must the accuracy and reliability of the identification results be considered but also the time cost of the identification process fig 14 presents a direct comparison of the time taken by the four optimization models to perform 200 iterations after the same initial population numbers were set and based on the same 100 groups of training data the time taken by the four identification models was very short but the qpso model had the shortest running time the calculations of all models were done a pc with an intel 3 4 ghz processor and 16 gb ram compared with the performance of the four identification models in the two cases the identification accuracy in the hypothetical case was better than that in the actual case the main reason was that it was more difficult to reconstruct the actual hydrologic and geological information in the actual case however a comprehensive analysis of the performance of the four identification models in both cases showed that each model performed very well in the case with less training data kelm surrogate simulation accuracy could still be maintained at a high level ga and pso which are two traditional optimization models have good simulation foundation and high identification accuracy however the combination of these two models with quantum computing not only improved their computational efficiency but also their computational accuracy the pso algorithm was still unstable after 200 iterations but the qpso algorithm became stable after about 50 iterations and the calculation time was shortened therefore the combination of a traditional optimization model with quantum computing can improve the model nevertheless this study still needs to integrate the identification results of the four identification models and to achieve identification of the release histories of unknown contaminant sources otherwise the results obtained by the algorithm will be invalid the solution will not be unique or the solution will be discontinuous under the boundary to avoid this problem the identification results of the four models should be considered comprehensively and the final identification results should not be a set of data but an interval table 7 shows the final results in the research of gzyl et al 2014 the accurate release histories of the actual contaminant sources were the parameters to be solved for and the intervals for these release histories were finally obtained as shown in table 7 by comparing the intervals of the contaminant source release histories the value range obtained through synthesis of the four optimization models in this study could be made narrower among the five contaminant sources the identification result interval of contaminant source cso north was significantly different from that of gzyl et al 2014 the main reason was that in both studies the release histories of the contaminant sources were the parameter to be solved for moreover there were differences among the various models for simulating groundwater contaminant transport which may have had a great influence on the results in addition the numerical solutions for groundwater flow and transport were also different which could have had a slight impact on the results 6 conclusions the contaminant identification model used in this study was applied to two cases one hypothetical and another actual the kernel based extreme learning machine coupled optimization model was used to obtain the identification model the contaminant release history identification model was used in the hypothetical case and the actual case respectively the results show that the four identification models not only have high accuracy but also fast identification speed the shape area size and hydrological and geological parameters are set in the hypothetical case in contrast it was more difficult to generalize the actual case than to set up the hypothetical case moreover the release histories of the contaminant sources were unknown by identifying the contaminant concentrations of the sources in the actual case and comparing them with the hypothetical case it could be concluded that 1 kelm has faster running speed and higher precision than the simulation model coupled with the four optimization models it performed well in all cases in the actual case after using the forward surrogate model the surrogate model is used to reversely determine the source s contaminant concentration range and the surrogate model is retrained by using this method to identify the release histories of unknown contaminant sources not only can sufficiently training data be ensured but also the concentration range of contaminant sources can be determined more quickly and the calculation efficiency can be improved 2 through comparative analysis differences were found in the performance of the four optimization models with fewer input and output data points the identification results of the combination of quantum computing with ga or pso were better than those of the uncombined models moreover by comparing the time required for contaminant source identification the optimization algorithm combined with quantum computing was faster the combination of quantum computing characteristics with traditional heuristic optimization algorithms not only can achieve a stable identification process but also has high identification accuracy and fast identification speed 3 the release histories of groundwater contaminant sources were identified by means of four optimization methods more importantly four different identification results were obtained by the four identification models which together provided a range of concentrations for the contaminant sources this avoids the problem of ill posed identifications from a single optimization model improves the stability of the identification result and provides a reliable basis for contaminant control credit authorship contribution statement ying zhao conceptualization writing review editing ruizhuo qu writing original draft methodology software zhenxiang xing funding acquisition visualization wenxi lu methodology software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national natural science foundation of china no 41807196 no 51979038 no 41672232 no 41972252 the national key r d program of china no 2017yfc0406004 the postdoctoral science foundation of china no 2018m641793 the postdoctoral science foundation of heilongjiang province of china no lbh z19002 the natural science foundation of heilongjiang province of china no e2015024 and the academic backbones foundation of northeast agricultural university no 16xg11 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103540 appendix supplementary materials image application 1 
503,particle deposition in porous media alters hydraulic properties including porosity and permeability the extent of these alterations depends on both porous media structure and its geometrical and topological properties in the present study a lattice boltzmann modeling is developed and used to systematically simulate particle clogging and to explore the evolution of hydraulic properties using realistic pore structures obtained from x ray tomography a total of six different porous media are studied where three domains have different porosities and grain sizes but the same pore connectivities to explore the geometrical effects and three domains have the same porosity but different grain arrangements and pore connectivities to study the effect of porous media topology the results have shown the impact of the underlying pore scale mechanisms resulting in porous media clogging and how they are affected by the initial porosity and topology of the media moreover simulation has been utilized to develop porosity permeability relations covering the initial sample permeability all the way to complete clogging of the media where permeability vanishes to provide more generic relations the obtained coefficients of the porosity permeability formulations are correlated to each porous media geometrical and topological properties keywords porous media clogging process lattice boltzmann method porosity permeability reduction 1 introduction a wide range of environmental and engineering problems such as subsurface applications depend on transport of migrating solid particles and their deposition and clogging of the media vu et al 2019 kacimov and obnosov 2019 this includes treatment of septic tank effluent and wastewater jones and taylor 1965 bouma 1975 beach and mccray 2003 abdoli et al 2018a 2018b artificial recharges okubo and matsumoto 1983 okubo and matsumoto 1979 zhang et al 2015 biological clogging in soil soleimani et al 2009 benioug et al 2017 adsorption at fluid interfaces zhang et al 2012 2013 subsurface heat storage pfeiffer et al 2000 spread of contaminant plumes and bioremediation molz et al 1986 baveye and valocchi 1989 vandevivere et al 1995 geologic carbon sequestration raoof et al 2012 2013 extraction of fossil energy and in civil and environmental engineering applications the blockage of porous media and permeability reduction of marine sediments caused by trapped fine particles is a common concern in application such as oil or natural gas extractions in oilfields to reduce the environmental damage and to increase the oil production the produced water is re injected into the reservoir the presence of solid particles in the injected water frequently causes an increase in injection pressure and therefore a decrease in medium permeability when pumping the groundwater the retained particles accumulate near the well bore leading to the reduction or even total loss of the well capacity de zwart 2007 estimating permeability value as a function of porosity grain size and the structure of fines in pore space can be challenging a raoof et al 2013 porous media with a same porosity values often have different pore structures this causes a non unique relationship between porosity and permeability of porous media which is a reason for failure of models working based on first principles to predict clogging in porous media several different methods have been applied to study permeability reduction which includes experimental stephan and chase 2000 mays and hunt 2005 hubbe et al 2009 kim and whittle 2006 and numerical kampel et al 2008 raoof et al 2013 mahmoodlu et al 2016 ju et al 2017 approaches most of the studies however are limited to domains with simple grain geometries such as cylindrical shapes hirabayashi et al 2012 used lattice boltzmann method lbm to study the motion and sedimentation of microscopic immersed particles in a viscous flow in a periodic porous media containing cylindrical grains they found that particle trapping and permeability reduction is mainly controlled by friction between particles and grain surfaces therefore the surface roughness of particles and grains were the major clogging parameters as the permeability alterations can be due to the attached particles as well as the moving ones with different velocities they suggested that the ratio of the number of settled particles to the moving particles is a key parameter to characterize permeability reduction sato et al 2013 used real sand grains and lbm to study the effect of trapped particles on reducing the permeability of porous media they found that the reductions in permeability could not only be attributed to changes in volume fraction of the fine particles to the pore volume but also to the size distribution of the fine particles their study showed that when the porosity within the medium is roughly constant along the sample whether the fine particles are fixed at random positions or are trapped at the narrow pore throats the resulting permeabilities are of the same order of magnitude they used volume fraction and the specific surface areas of both grains and clogging particles to define a relation for permeability alteration to explore the accuracy of the applied numerical methods qiu 2015 studied the effect of different parameters including flow velocity and particle sizes in trapping of particles between grains using lbm and a hybrid navier stokes method they found that lbm is more accurate at close distances to the grain surfaces as it provides a better representation of the curved grain no slip boundary conditions and the local velocity profile in addition there are a range of studies pan et al 2001 jin et al 2004 boek and venturoli 2010 that have utilized the lbm to simulate fluid flow and estimate permeability ahfir et al 2016 investigated the influence of grain size distribution of the porous medium on clogging and found that the larger number of particles are deposited in the narrower pores however they did not state the fundamental mechanism of clogging in porous media caused by particle transport boccardo et al 2014 applied clean bed filtration theory cft for the simulation of experimental column tests at the pore scale they investigated particle transport via eulerian steady state simulations where particle concentration is solving by the corresponding advection diffusion equation they achieved an accurate and efficient integration of large nanoparticle transport into continuum models for lateral flow assays khan et al 2017 used experimental observations to obtain permeability and porosity changes in porous media they have combined discrete element method dem and pore scale network modeling and found that the dual pore scale model can predict the permeability of the invaded core in the regions away from the injection face they could predict the porosity by adding the effect of external filter cake based on their observation while retention of large particles enhances the surface deposition of small particles retention of small particles does not change the rate of entrapment for other small particle anbar et al 2018 studied changes in permeability and darcy coefficient due to compaction and sedimentation of sand migration for sphere packing by using of lbm in their work compaction effects were simulated by growth grain diameter in their media furthermore they studied the effect of sedimentation of particles in porous media on permeability it was found that permeability variation was directional where permeability change along the flow direction was almost twice of the other directions clogging is known to depend on physical factors which control hydrodynamic effects porosity is a geometrical property which determines the total pore space available for fluid flow and pore connectivity determines the topology of the media which controls the effectiveness of porosity to guide flow through the medium raoof et al 2013 several classical mathematical models consider particle transport without including the effect of clogging caused by sedimentation of particles this consideration prevents local flow disturbances due to particle attachment and movements which may control the development of transport and clogging in the media in this study a two way coupling considers the effect clogging on local fluid flow as well as the effect of fluid flow on the particle motion we should note that the coupling does not involve the effect of particle motion on flow field and it considers the effect of particles on fluid flow once sedimentation is taken place ideally clogging can be investigated by considering the retention of colloids with real shapes and volume immersed in the fluid phases on the surface of grains in this study a total of six domains are used for pore scale simulation of clogging three real geometries which have identical topologies but different porosities are used to study porous media geometrical effects and the other three domains where position of solid grains were changed are used to study porous media topological effects two common relations linking permeability and porosity i e carman kozeny and power law relations were fitted to the results of lattice boltzmann modeling of all six domains to find the values of their coefficients and to further explore how these coefficients are related to porous media properties 2 methodology the applied method in this study includes two steps for simulating the motion of microscopic solid particles in porous media i simulating flow in porous media and ii simulating particle transport lattice boltzmann method lbm provides convenient way to implement the boundary conditions compared to the other numerical methods therefore lbm is suitable for simulating viscose flow in complex porous media geometries liu et al 2016 bakhshian et al 2016 chen and doolen 1998 inamuro et al 1999 bakhshian et al 2019 we consider the following simplifying assumptions to simulate particle transport and sedimentation in porous media in response to fluid drag the concentration of the point particles in the fluid is sufficiently low so that the moving particles in the flow do not affect the physical properties of fluid such as its dynamic viscosity and density therefore these properties are considered constant through the simulations transport of particles is due to the hydrodynamic drag forces i e the interaction between particles gravity van der waals force and electrostatic double layer forces are not considered this is often applied in studies where flow is horizontal feng et al 2015 jafari et al 2010 the criterion for sedimentation is the distance between the particle surface and the boundary of solid grains i e it is assumed that the interaction forces are favorable and do not hinder attachment of particles after the come very close to the grain surfaces 3 mathematical modeling 3 1 fluid flow simulation mass balance and momentum conservation equations for the flow of an incompressible newtonian fluid are described as 1 u 0 2 ρ u t u u p μ 2 u where ρ u p and μ are fluid density vector velocity pressure and fluid kinematic viscosity respectively eqs 1 and 2 are solved by using a two dimensional lattice with 9 velocity vectors d2q9 model via lbm with a single relaxation time srt and a bgk bhatnagar gross krook collision operator non slip boundary condition is applied by using of bounce back boundary condition at the interface of fluid and solid grains at each time step lbm equations are solved in a transient form until steady state flow is reached 3 1 1 overview of lattice boltzmann method the evolution of the fluid velocity field is modeled by the lattice boltzmann equation with source term and bgk collision model of a density distribution function given as 3 f a x c a δ t t δ t f a x t f a x t f a e q x t τ δ t f a here f α is distribution function for fluid flow and f α e q is the equilibrium distribution function used for direction α in addition c α τ and fα mention velocity vector hydrodynamic relaxation time and the component of external force in α direction respectively the value of external force in our simulations in all directions is zero and other parameters are defined as 4 f α e q ω α ρ 1 3 c α u c 2 9 2 c α u 2 c 4 3 2 u u c 2 where c is related to the lattice sound speed and ωα is the weight coefficient for the α direction for d2q9 discrete velocity model expressed as 5 c δ x δ t 6 c s c 3 where δx δt and cs are lattice length and time step that both are selected as 1 and sound speed respectively kinematic viscosity υ and weighting factor ωα in α direction are expressed as 7 υ τ 0 5 c s 2 8 ω α 4 9 α 0 1 9 α 1 2 3 4 1 36 α 5 6 7 8 the macro variables such as density of the fluid ρ x t and the macroscopic fluid velocity u x t can be calculated using 9 ρ x t α f α x t 10 ρ x t u x t α f α x t c α for the d2q9 model is employed nine directions are considered for velocity as 11 c α 0 α 0 cos π α 1 2 sin π α 1 2 c α 1 2 3 4 cos π α 4 1 2 2 sin π α 4 1 2 2 2 α 5 6 7 8 3 2 particle transport 3 2 1 introduction of effective forces the motion of particles in the porous medium is affected by drag force and brownian force in our simulations using lagrangian particle tracking solid particles are followed using the equations of motion as 12 m d v d t f d r a g f b where v fdrag and fb are particle velocity drag force and brownian force respectively when the re number of the domain is low and particle is not close to channel wall a particle experiences stokes drag force computed by 13 f d r a g 6 π μ a c u v where μ ac u and v are viscosity particle radius fluid velocity and particle velocity which are next replaced in eq 3 additionally brownian motion for particles suspended in a liquid or gas caused by collisions with molecules of the surrounding medium affects particle transport particularly for small particles the effective force due to the brownian motion can be expressed as 14 f b 6 π μ a c 2 d 0 d t f g x where the diffusivity in bulk solution is d 0 k b t 6 π μ a c where kb t and dt are boltzmann constant temperature and time step size respectively gaussian distribution function fg x is a continuous function which approximates the exact binomial distribution of physical events expressed as f g x 1 2 π σ 2 e x a 2 2 σ 2 15 where a and σ the mean and the standard deviation of gaussian distribution function are set to 0 and 1 respectively 3 2 2 deposition mechanism in each time step the flow is computed and afterwards particle transport is calculated to obtain the location of the particle i e eq 12 colloids may attach at the solid surfaces of the grains during their movement and can ultimately result in clogging of the individual pores as well as the whole porous media in this study the distance between the surface of colloid and grain surface is utilized to determine particle deposition when the minimum distance between the particle and grain surface is close to the particle radius rp ε where ε is a small distance representing interaction due to the surfaces electric potentials particle will attach to the grain surface and therefore removed from the fluid phase as progressively more particles attach to the grain the pore shape changes in our simulations when the accumulated volume of attached particles in a cell occupies more than 90 of the cell volume the fluid cell is converted to a solid cell having pore sizes becoming smaller due to attachment there will be more fluid shear force inserted on the attached particles causing them to move towards the nearby depression zones with less fluid flow in this study we consider this effect based on the study by benioug et al 2017 when a particle is subject to attachment we explore the host lbm cell as well as the neighboring cells to explore the locations with the minimum drag force to be considered as the attachment location we have performed several simulations and found that neighboring cells are most realistically defined as cells with a maximum distance of half a grain radius fro0m the initial attachment location the attachment term represents deposition of particles due to several underlying mechanisms including interception deposition ripening and favorable dlvo net forces the dlvo theory includes two major components which add up to provide the total interaction force seetha et al 2014 these components are van der waals forces based on the interparticle distances and the electrostatic double layer forces based on the charges of colloids and grain surfaces seetha et al 2017a 2017b under favorable condition which is assumed in this study as both double layer forces between colloids and grain surfaces and the van der waals forces are attractive no energy barrier is developed to inhibit colloid deposition at grain surfaces these processes results in attachment of colloids that come in close contact with the solid phase the ripening phenomenon is also included as the attached particles become solid cells i e collectors to which additional colloids attachment can occur considering only the attachment mechanism provides the advantage that the observed impact of porous media complexities i e its geometry and topology on clogging is caused by only one factor i e particle attachment rather than multiple parameters such as coupled attachment and detachment and their non linear interactions the type of porous media the flowing fluid properties and colloid properties determine whether colloid attachment is the operating mechanisms won et al 2019 boccardo et al 2014 cai and zhang 2016 su et al 2019 or colloid detachment should be also considered klimenko et al 2020 benioug et al 2017 yang and balhoff 2017 4 results and discussion we have considered a total of 6 different pore scale domains to perform lbm simulations and colloid transport and clogging the geometry of domains was obtained using x ray tomography to provide a set of three domains with similar topological properties i e similar arrangement of pores and grains but different porosity values ϕ we have numerically performed erosion on solid grains of the original domain without changing grain locations to obtain three domains with increasing porosity while keeping the pore connectivity unaffected fig 1 this is done using imagej package rueden et al 2017 were image pixels corresponding to the surface of the solid grains were removed in a layer by layer manner for all grains until a domain with a desired porosity value was obtained to explore the effect of different topologies under identical initial porosities three extra pore structures where created this is done by randomly swapping the location of individual solid grains as shown in fig 2 for the simulations the top and bottom flow boundaries were considered as no flow and simulated by bounce back boundary condition the inlet located at the left side and outlet located at the right side boundaries were considered as constant velocity and pressure respectively the half way bounce back boundary condition was applied to the boundary of solid grains the domain size and the particle diameter are 800 800 and 0 608 micrometers respectively after doing some grid independent tests it was obvious that the best domain size is 500 500 in lattice unit the ratio of solid density to fluid density and relaxation time used in eq 7 are taken as 1 05 and 0 65 in lattice unit respectively we have tested our model by simulating fluid flow in well defined domain composed of cylindrical grains to obtain permeability and its corresponding carman kozeny parameters for which verified data are available in the literature yazdchi et al 2011 colloids were injected through fluid flow with the constant flow velocity in inlet simulations show initial distribution of particles in inlet has negligible effect on results see appendix a colloids were injected at equal distances on a vertical cross section located behind the porous media inlet face 4 1 pore space evolution the simulation results are used to compute the porosity and permeability values and to investigate the relations between these two parameters in each case the simulation was continued until the complete clogging of the media is taken place as shown in fig 3 the obtained two dimensional distributions of the absorbed mass shown by yellow color in fig 3 were used to calculate the profile of 1d averaged absorbed mass along the domain fig 3b to obtain the average values the domain was divided into several sub domains normal to the flow direction and adsorbed mass was averages in each sub domain fig 3b shows that a major part of clogging takes place at the inlet section of each domain particularly for the domain with the initial porosity value of 0 44 4 2 porosity permeability relations for evolving porous media we have chosen to fit the carman kozeny and power law relations hommel et al 2018 to our pore scale lbm simulations to obtain a relation between porosity and permeability moreover these relations include coefficients which we have tried to relate them to the properties of each porous medium the measured porosity and permeability values for each sample are based on the whole domain size rather than a subsection of the domain where for example most of clogging is taking place this choice is based on the definition of representative elementary volume rev requiring a domain of several pores grains along each direction to provide representative macroscopic values given our initial domain sizes with several pores along each principal direction we have considered each whole domain as a rev and measured macroscopic properties and their evolutions by integrating within the whole domain doing so our reported values between different samples are consistent with their initial porosity and permeability values and also consistent with each other as they are all based on the whole domain size 4 2 1 carman kozeny and power law relations carman kozeny kozeny 1927 carman 1937 hommel et al 2018 is a well known relation widely used to link porosity and permeability values in both experimental and modeling studies this equation is based on simplifying porous media as parallel capillary tubes with equal geometry and size 16 δ p l 180 μ φ s 2 d p 2 1 φ 2 φ 3 υ where ϕ s is sphericity dp and ϕ are particle diameter and porosity respectively this equation can be reformulated as 17 k φ s 2 d p 2 180 φ 3 1 φ 2 where k is permeability eq 17 can be written as 18 k φ 3 σ 1 φ 2 s 2 eq 18 is may be written using tortuosity σ and specific surface area s which compared to ϕ s and dp are more relevant parameters to natural porous media gallo et al 1998 macquarrie and mayer 2005 mostaghimi et al 2013 showed that using the kozeny carman equation especially for complex tortuous heterogeneous or poorly connected porous media significantly overestimates the permeability the original kozeny carman equation may be also written as 19 k k 0 φ 3 s 0 φ 0 3 s where to provide a simpler formulation the specific surface area s may be a replaced by 1 φ during clogging both porosity φ and specific surface area s change over time s 0 is the initial specific surface area due to clogging permeability approaches zero without necessarily porosity approaching zero throughout the sample to consider this effect we applied a modified form of carman kozeny relation as 20 k k 0 a φ φ c r φ 0 φ c r b 1 φ φ c r 1 φ 0 φ c r c where ϕ0 ϕcr and k 0 are the initial and critical porosity values and initial permeability respectively this equation is derived from eqs 17 and 18 and has been made dimensionless by using of k 0 ϕ0 ϕ cr and 1 ϕ0 ϕ cr terms the addition to this relation in comparison to the eqs 17 and 18 is the critical porosity ϕ cr which is used to describe the nonzero porosity threshold at which the permeability approaches zero the idea is that only for ϕ ϕ cr the pore space is connected and thus only the pore space exceeding this limit ϕ ϕ cr contributes to flow comparison the eq 20 with eqs 17 and 18 shows what values have been substituted with coefficients a b and c in the original form of the kozeny carman relation provided in section 4 2 1 eq 20 also studied by wijngaarden et al 2013 steefel et al 2015b xie et al 2015 pandey et al 2015 changes in parameters related to geometry as sphericity ϕ s or characteristic particle diameter d p are assumed negligible however as geometry changes should be considered in our simulations eq 20 was applied which has a similar form as the original kozeny carman relation some studies marshall 1958 taylor et al 1990 require calculations for geometrical corrections during damaging the pore structure in our presented model the only terms required to calculate are permeability and the starting and ending critical porosities power law relation hommel et al 2018 verma and pruess 1988 ives and pienvichitr 1965 is another well known relation widely used to describe permeability changes in both experimental and modeling studies this provides a rather simple relation where the only major fitting parameter is the exponent b and there is no strong term for specific changes of media geometry this relation involves the medium specific exponent a as a parameter 21 k k 0 a φ φ 0 b in this study the power law relation is expresses as 22 k k 0 a φ φ c r φ 0 φ c r b 4 3 fit on carman kozeny and power law relations we have fitted the carman kozeny and power law relations to the computed porosity and permeability values obtained from lbm simulations fig 4 shows that porous media geometry and topology both influence the relation between permeability and porosity change of porous media topology affects the relation between permeability and porosity and moreover changes the critical porosity of the media at which clogging takes place this is specially the case for the media with the larger porosity of 55 as the presence of larger pores gives more freedom for particle movements by change of topology fig 4 also shows a wide range of permeability for a given porosity value which can vary even more than two folds the solid lines in fig 4 are the plots of carman kozeny and power law relations i e eqs 20 and 22 describing the porosity permeability relation based on the shown data points table 1 provides properties of the porous media initially as well as after the clogging all three cases have shown that the final porosities under which permeability values tend to zero are different for each topology table 2 provides the coefficients of carman kozeny relation coefficient a is nearly constant for both topologies and different initial porosities coefficient b shows slightly more sensitivity to the initial porosity compared to the topology coefficient c shows dependency on both topology and initial porosity at low initial porosities and becomes less sensitive for domains with higher initial porosity comparing eqs 19 and 20 shows that coefficient c depends on specific surface area as clogging affects the specific surface area and porosity of the media coefficient c is sensitive to these two parameters schneider et al 1996 used a form of kozeny carman model that the values of exponents a and b were set respectively to 3 0 and 2 0 and in some cases these values were changed depending on the rock type coefficient c in their model was assumed to be equal to 1 0 some studies like taylor et al 1990 vandevivere et al 1995 vandevivere et al 1995 martys et al 1994 have replaced 1 ϕ term with the specific surface area and considered the value of a b and c to be equal to 3 0 1 0 and 1 0 respectively using a rearranged form of carman kozeny relation amaefule et al 1993 set coefficients a and b to 3 0 and 2 0 and coefficient c was expressed as γ 1 s 2 σ based on the relation of amaefule et al 1993 together with the experimental data civan 2000 used another form of carman kozeny relation k φ γ φ 1 φ β to find values of γ 4 1 5 82 and β 2 96 3 89 table 3 provides coefficient values corresponding to the power law relation coefficient a remains nearly unchanged while coefficient b indicates a strong dependency on both initial porosity and topology the exponent b is an empirical parameter in eq 22 hommel et al 2018 as discussed in section 4 2 1 the initial porous medium properties and the conditions that cause the pore geometry to change determine the value of b for instance bernabe et al 2003 suggested that the value of b is between 2 5 and 3 0 for plastic compaction 8 0 for mineral precipitation b 10 0 for chemical alteration and b 20 0 for mineral dissolution doyen 1988 measured an evolving pore space and calibrated a value of b 3 8 for water containing co2 garing et al 2015 observed an increase in permeability which was fitted using a power law relation with an exponent b ranging between 4 87 and 23 72 verma and pruess 1988 stated that the value of b may range from 6 0 to less than 1 0 depending on the porous material type which for their study ranged from 1 98 to 0 73 based on experimental data collected from sandstone units in our study the simulations resulted in values of exponent b between 0 63 and 1 22 4 4 effect of clogging on flow field streamlines pore clogging alters pore structures which in turn changes the flow field and the fluid streamlines fig 5 shows the change in the path of a set of streamlines during clogging particle retention at pore throats changes the preferential flow and streamlines become longer and more tortuous at the same time the major streamlines become less in number since the remaining free paths that fluid can freely flow through them become less frequent as shown in fig 5 the corresponding velocity distributions of plots in fig 5 are shown in fig 6 flow velocities with larger magnitudes and variations become more dominant close to the inlet of the sample where clogging is effectively taking place because of the mass conservation law as during clogging process in all three cases the number of main paths are decreased the fluid velocity magnetude becomes larger over time because of lower initial porosity in case φ0 0 44 active pore throats are clogged sooner and average fluid velocity over time is higher compared to the other two cases 4 5 effect of clogging on pore size evolution to explore the effect of clogging on pore size evolution we have chosen the sample with the initial porosity of 0 55 to analyze change of pore sizes from the initial state i e ϕ0 0 55 until complete clogging fig 7 the location of pore throats i e the narrowest constrictions between pores is used to divide the total pore space into an ensemble of individual pores which are marked by different colors in fig 7 the solid rectangles show a section of the domain close to the inlet where most of the clogging takes place and finally becomes disconnected fig 7 c to diminish the permeability the resulting pore size distributions at three different times i e times corresponding to the initial intermediate and the final clogging state of the sample are provided in fig 7 d the distribution of pore sizes during clogging shows that clogging did not significantly affect the pore size distributions and only pores smaller than 150 µm show a slight change in their population density this is because while pore clogging significantly affects permeability by locally disconnecting flow pathways a major fraction of pores remains unclogged and therefore pore size changes are less noticeable compared to the permeability changes 5 summary and conclusion porosity and permeability are the two major transport properties of porous media and relating permeability to porosity changes is the subject of several studies this study explored porosity changes due to particle transport and clogging and how permeability is affected by this change the realistic pore space was obtained using x ray computed tomography and further used to create several extra pore structures by modifying grain sizes as well as grain locations and their arrangements this provided a total of six domains to systematically study the effect of geometrical parameters i e porosity change due to alteration of grain sizes and topological parameters i e pore connectivity changes due to alteration in grain arrangements on porous media clogging for each domain fluid flow was simulated using a lattice boltzmann method and lagrangian approach was applied to simulate transport of particles a new method based on velocity variations at the grain surface was implemented to obtain the ultimate location of particle deposition after particles come close to the grain surface particle transport and clogging was continued up to full clogging of the media where permeability approached zero at a critical porosity which was different for each domain the macroscopic i e sample scale behavior was described using carman kozeny and power law relations to relate permeability to porosity variations the use of systematic domains with different geometrical and topological properties enabled us to link the coefficients in the carman kozeny and power law relations to porous media properties in carman kozeny relation coefficient a was nearly constant with a value of 1 05 for different initial porosities and different topologies however coefficient b was affected by the initial porosity while change in topology had not significant effect on it coefficient c showed strong dependency on both topology and initial porosity at low initial porosities and the dependency on topology reduced for domains with higher initial porosity in power law relations coefficient a remains nearly constant for different domains while coefficient b indicated a strong dependency on both initial porosity and topology the preferred particle deposition close to the inlet of the sample and along the flow paths of particles caused a non uniform change of porosity along the sample depending on geometrical and topological properties of the media therefore internal variations of different parameters were studied including profile of absorbed mass in each domain during clogging process change of pore sizes and the main streamlines and their evolution during clogging the evolution of pore sizes during the clogging showed that the collection of pores with an intermediate size relative to the range of pore sizes present in porous media were affected the most by clogging the main flow streamlines were initially short and uniformly distributed along the domain however during clogging streamlines decreased in number and became much longer in length which shows an increase of tortuosity in the presence of pore clogging in this study we employed 2d domains to enable us to perform several simulations needed for a systematic study and to obtain the optimum location for particle deposition based on the velocity variations at the grain surfaces while these simulations provided valuable insight into the role of geometrical and topological parameters in porosity permeability relations 3d simulations are required to provide more realistic values for correlation parameters based on pore connectivities in the three dimensional space furthermore in the developed method particles are point objects when they are being transported in fluid however we have developed and tested a method to determine the realistic location of the attached particles based on the velocity distribution at the grain surface the assumption of point particles in our method up to the point that they are attached makes it suitable for small particles for which the particles do not significantly disturb the fluid flow during their transport and their effect on fluid flow is considerable only after they are attached and changed the pore space and therefore affecting fluid flow author statements the corresponding author of this manuscript certify that the contributors and conflicts of interest statements included in this paper are correct and have been approved by all co authors declaration of competing interest the authors have no conflict of interest to declare acknowledgments we note that there are no data sharing issues since all of the numerical information is provided in the figures produced by solving the equations in the paper we thank the anonymous reviewers for their insightful comments and suggestions we thank enno de vries for his help on generating the pore scale domains supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103530 appendix b supplementary materials image application 1 appendix a to evaluate the sensitivity of simulations to changes in the injection location of particles at the inlet we performed simulations applying different initial distribution of particles at the inlet face of porous media fig a1 shows that initial distribution of the particles has negligible effect on the relation between porosity and permeability this can be explained as i the domain is large enough and the inlet effects are not propagated through the whole media and ii the x ray imaged pore structure used in this study is rather homogenous making it less sensitive to the conditions at the inlet b fig a2 provides log scale plots for fitting of carman kozeny and power law relations same data are shown using a linear scale in fig 4 of the manuscript this figure shows that both relations were able to fit data and provided similar fit qualities 
503,particle deposition in porous media alters hydraulic properties including porosity and permeability the extent of these alterations depends on both porous media structure and its geometrical and topological properties in the present study a lattice boltzmann modeling is developed and used to systematically simulate particle clogging and to explore the evolution of hydraulic properties using realistic pore structures obtained from x ray tomography a total of six different porous media are studied where three domains have different porosities and grain sizes but the same pore connectivities to explore the geometrical effects and three domains have the same porosity but different grain arrangements and pore connectivities to study the effect of porous media topology the results have shown the impact of the underlying pore scale mechanisms resulting in porous media clogging and how they are affected by the initial porosity and topology of the media moreover simulation has been utilized to develop porosity permeability relations covering the initial sample permeability all the way to complete clogging of the media where permeability vanishes to provide more generic relations the obtained coefficients of the porosity permeability formulations are correlated to each porous media geometrical and topological properties keywords porous media clogging process lattice boltzmann method porosity permeability reduction 1 introduction a wide range of environmental and engineering problems such as subsurface applications depend on transport of migrating solid particles and their deposition and clogging of the media vu et al 2019 kacimov and obnosov 2019 this includes treatment of septic tank effluent and wastewater jones and taylor 1965 bouma 1975 beach and mccray 2003 abdoli et al 2018a 2018b artificial recharges okubo and matsumoto 1983 okubo and matsumoto 1979 zhang et al 2015 biological clogging in soil soleimani et al 2009 benioug et al 2017 adsorption at fluid interfaces zhang et al 2012 2013 subsurface heat storage pfeiffer et al 2000 spread of contaminant plumes and bioremediation molz et al 1986 baveye and valocchi 1989 vandevivere et al 1995 geologic carbon sequestration raoof et al 2012 2013 extraction of fossil energy and in civil and environmental engineering applications the blockage of porous media and permeability reduction of marine sediments caused by trapped fine particles is a common concern in application such as oil or natural gas extractions in oilfields to reduce the environmental damage and to increase the oil production the produced water is re injected into the reservoir the presence of solid particles in the injected water frequently causes an increase in injection pressure and therefore a decrease in medium permeability when pumping the groundwater the retained particles accumulate near the well bore leading to the reduction or even total loss of the well capacity de zwart 2007 estimating permeability value as a function of porosity grain size and the structure of fines in pore space can be challenging a raoof et al 2013 porous media with a same porosity values often have different pore structures this causes a non unique relationship between porosity and permeability of porous media which is a reason for failure of models working based on first principles to predict clogging in porous media several different methods have been applied to study permeability reduction which includes experimental stephan and chase 2000 mays and hunt 2005 hubbe et al 2009 kim and whittle 2006 and numerical kampel et al 2008 raoof et al 2013 mahmoodlu et al 2016 ju et al 2017 approaches most of the studies however are limited to domains with simple grain geometries such as cylindrical shapes hirabayashi et al 2012 used lattice boltzmann method lbm to study the motion and sedimentation of microscopic immersed particles in a viscous flow in a periodic porous media containing cylindrical grains they found that particle trapping and permeability reduction is mainly controlled by friction between particles and grain surfaces therefore the surface roughness of particles and grains were the major clogging parameters as the permeability alterations can be due to the attached particles as well as the moving ones with different velocities they suggested that the ratio of the number of settled particles to the moving particles is a key parameter to characterize permeability reduction sato et al 2013 used real sand grains and lbm to study the effect of trapped particles on reducing the permeability of porous media they found that the reductions in permeability could not only be attributed to changes in volume fraction of the fine particles to the pore volume but also to the size distribution of the fine particles their study showed that when the porosity within the medium is roughly constant along the sample whether the fine particles are fixed at random positions or are trapped at the narrow pore throats the resulting permeabilities are of the same order of magnitude they used volume fraction and the specific surface areas of both grains and clogging particles to define a relation for permeability alteration to explore the accuracy of the applied numerical methods qiu 2015 studied the effect of different parameters including flow velocity and particle sizes in trapping of particles between grains using lbm and a hybrid navier stokes method they found that lbm is more accurate at close distances to the grain surfaces as it provides a better representation of the curved grain no slip boundary conditions and the local velocity profile in addition there are a range of studies pan et al 2001 jin et al 2004 boek and venturoli 2010 that have utilized the lbm to simulate fluid flow and estimate permeability ahfir et al 2016 investigated the influence of grain size distribution of the porous medium on clogging and found that the larger number of particles are deposited in the narrower pores however they did not state the fundamental mechanism of clogging in porous media caused by particle transport boccardo et al 2014 applied clean bed filtration theory cft for the simulation of experimental column tests at the pore scale they investigated particle transport via eulerian steady state simulations where particle concentration is solving by the corresponding advection diffusion equation they achieved an accurate and efficient integration of large nanoparticle transport into continuum models for lateral flow assays khan et al 2017 used experimental observations to obtain permeability and porosity changes in porous media they have combined discrete element method dem and pore scale network modeling and found that the dual pore scale model can predict the permeability of the invaded core in the regions away from the injection face they could predict the porosity by adding the effect of external filter cake based on their observation while retention of large particles enhances the surface deposition of small particles retention of small particles does not change the rate of entrapment for other small particle anbar et al 2018 studied changes in permeability and darcy coefficient due to compaction and sedimentation of sand migration for sphere packing by using of lbm in their work compaction effects were simulated by growth grain diameter in their media furthermore they studied the effect of sedimentation of particles in porous media on permeability it was found that permeability variation was directional where permeability change along the flow direction was almost twice of the other directions clogging is known to depend on physical factors which control hydrodynamic effects porosity is a geometrical property which determines the total pore space available for fluid flow and pore connectivity determines the topology of the media which controls the effectiveness of porosity to guide flow through the medium raoof et al 2013 several classical mathematical models consider particle transport without including the effect of clogging caused by sedimentation of particles this consideration prevents local flow disturbances due to particle attachment and movements which may control the development of transport and clogging in the media in this study a two way coupling considers the effect clogging on local fluid flow as well as the effect of fluid flow on the particle motion we should note that the coupling does not involve the effect of particle motion on flow field and it considers the effect of particles on fluid flow once sedimentation is taken place ideally clogging can be investigated by considering the retention of colloids with real shapes and volume immersed in the fluid phases on the surface of grains in this study a total of six domains are used for pore scale simulation of clogging three real geometries which have identical topologies but different porosities are used to study porous media geometrical effects and the other three domains where position of solid grains were changed are used to study porous media topological effects two common relations linking permeability and porosity i e carman kozeny and power law relations were fitted to the results of lattice boltzmann modeling of all six domains to find the values of their coefficients and to further explore how these coefficients are related to porous media properties 2 methodology the applied method in this study includes two steps for simulating the motion of microscopic solid particles in porous media i simulating flow in porous media and ii simulating particle transport lattice boltzmann method lbm provides convenient way to implement the boundary conditions compared to the other numerical methods therefore lbm is suitable for simulating viscose flow in complex porous media geometries liu et al 2016 bakhshian et al 2016 chen and doolen 1998 inamuro et al 1999 bakhshian et al 2019 we consider the following simplifying assumptions to simulate particle transport and sedimentation in porous media in response to fluid drag the concentration of the point particles in the fluid is sufficiently low so that the moving particles in the flow do not affect the physical properties of fluid such as its dynamic viscosity and density therefore these properties are considered constant through the simulations transport of particles is due to the hydrodynamic drag forces i e the interaction between particles gravity van der waals force and electrostatic double layer forces are not considered this is often applied in studies where flow is horizontal feng et al 2015 jafari et al 2010 the criterion for sedimentation is the distance between the particle surface and the boundary of solid grains i e it is assumed that the interaction forces are favorable and do not hinder attachment of particles after the come very close to the grain surfaces 3 mathematical modeling 3 1 fluid flow simulation mass balance and momentum conservation equations for the flow of an incompressible newtonian fluid are described as 1 u 0 2 ρ u t u u p μ 2 u where ρ u p and μ are fluid density vector velocity pressure and fluid kinematic viscosity respectively eqs 1 and 2 are solved by using a two dimensional lattice with 9 velocity vectors d2q9 model via lbm with a single relaxation time srt and a bgk bhatnagar gross krook collision operator non slip boundary condition is applied by using of bounce back boundary condition at the interface of fluid and solid grains at each time step lbm equations are solved in a transient form until steady state flow is reached 3 1 1 overview of lattice boltzmann method the evolution of the fluid velocity field is modeled by the lattice boltzmann equation with source term and bgk collision model of a density distribution function given as 3 f a x c a δ t t δ t f a x t f a x t f a e q x t τ δ t f a here f α is distribution function for fluid flow and f α e q is the equilibrium distribution function used for direction α in addition c α τ and fα mention velocity vector hydrodynamic relaxation time and the component of external force in α direction respectively the value of external force in our simulations in all directions is zero and other parameters are defined as 4 f α e q ω α ρ 1 3 c α u c 2 9 2 c α u 2 c 4 3 2 u u c 2 where c is related to the lattice sound speed and ωα is the weight coefficient for the α direction for d2q9 discrete velocity model expressed as 5 c δ x δ t 6 c s c 3 where δx δt and cs are lattice length and time step that both are selected as 1 and sound speed respectively kinematic viscosity υ and weighting factor ωα in α direction are expressed as 7 υ τ 0 5 c s 2 8 ω α 4 9 α 0 1 9 α 1 2 3 4 1 36 α 5 6 7 8 the macro variables such as density of the fluid ρ x t and the macroscopic fluid velocity u x t can be calculated using 9 ρ x t α f α x t 10 ρ x t u x t α f α x t c α for the d2q9 model is employed nine directions are considered for velocity as 11 c α 0 α 0 cos π α 1 2 sin π α 1 2 c α 1 2 3 4 cos π α 4 1 2 2 sin π α 4 1 2 2 2 α 5 6 7 8 3 2 particle transport 3 2 1 introduction of effective forces the motion of particles in the porous medium is affected by drag force and brownian force in our simulations using lagrangian particle tracking solid particles are followed using the equations of motion as 12 m d v d t f d r a g f b where v fdrag and fb are particle velocity drag force and brownian force respectively when the re number of the domain is low and particle is not close to channel wall a particle experiences stokes drag force computed by 13 f d r a g 6 π μ a c u v where μ ac u and v are viscosity particle radius fluid velocity and particle velocity which are next replaced in eq 3 additionally brownian motion for particles suspended in a liquid or gas caused by collisions with molecules of the surrounding medium affects particle transport particularly for small particles the effective force due to the brownian motion can be expressed as 14 f b 6 π μ a c 2 d 0 d t f g x where the diffusivity in bulk solution is d 0 k b t 6 π μ a c where kb t and dt are boltzmann constant temperature and time step size respectively gaussian distribution function fg x is a continuous function which approximates the exact binomial distribution of physical events expressed as f g x 1 2 π σ 2 e x a 2 2 σ 2 15 where a and σ the mean and the standard deviation of gaussian distribution function are set to 0 and 1 respectively 3 2 2 deposition mechanism in each time step the flow is computed and afterwards particle transport is calculated to obtain the location of the particle i e eq 12 colloids may attach at the solid surfaces of the grains during their movement and can ultimately result in clogging of the individual pores as well as the whole porous media in this study the distance between the surface of colloid and grain surface is utilized to determine particle deposition when the minimum distance between the particle and grain surface is close to the particle radius rp ε where ε is a small distance representing interaction due to the surfaces electric potentials particle will attach to the grain surface and therefore removed from the fluid phase as progressively more particles attach to the grain the pore shape changes in our simulations when the accumulated volume of attached particles in a cell occupies more than 90 of the cell volume the fluid cell is converted to a solid cell having pore sizes becoming smaller due to attachment there will be more fluid shear force inserted on the attached particles causing them to move towards the nearby depression zones with less fluid flow in this study we consider this effect based on the study by benioug et al 2017 when a particle is subject to attachment we explore the host lbm cell as well as the neighboring cells to explore the locations with the minimum drag force to be considered as the attachment location we have performed several simulations and found that neighboring cells are most realistically defined as cells with a maximum distance of half a grain radius fro0m the initial attachment location the attachment term represents deposition of particles due to several underlying mechanisms including interception deposition ripening and favorable dlvo net forces the dlvo theory includes two major components which add up to provide the total interaction force seetha et al 2014 these components are van der waals forces based on the interparticle distances and the electrostatic double layer forces based on the charges of colloids and grain surfaces seetha et al 2017a 2017b under favorable condition which is assumed in this study as both double layer forces between colloids and grain surfaces and the van der waals forces are attractive no energy barrier is developed to inhibit colloid deposition at grain surfaces these processes results in attachment of colloids that come in close contact with the solid phase the ripening phenomenon is also included as the attached particles become solid cells i e collectors to which additional colloids attachment can occur considering only the attachment mechanism provides the advantage that the observed impact of porous media complexities i e its geometry and topology on clogging is caused by only one factor i e particle attachment rather than multiple parameters such as coupled attachment and detachment and their non linear interactions the type of porous media the flowing fluid properties and colloid properties determine whether colloid attachment is the operating mechanisms won et al 2019 boccardo et al 2014 cai and zhang 2016 su et al 2019 or colloid detachment should be also considered klimenko et al 2020 benioug et al 2017 yang and balhoff 2017 4 results and discussion we have considered a total of 6 different pore scale domains to perform lbm simulations and colloid transport and clogging the geometry of domains was obtained using x ray tomography to provide a set of three domains with similar topological properties i e similar arrangement of pores and grains but different porosity values ϕ we have numerically performed erosion on solid grains of the original domain without changing grain locations to obtain three domains with increasing porosity while keeping the pore connectivity unaffected fig 1 this is done using imagej package rueden et al 2017 were image pixels corresponding to the surface of the solid grains were removed in a layer by layer manner for all grains until a domain with a desired porosity value was obtained to explore the effect of different topologies under identical initial porosities three extra pore structures where created this is done by randomly swapping the location of individual solid grains as shown in fig 2 for the simulations the top and bottom flow boundaries were considered as no flow and simulated by bounce back boundary condition the inlet located at the left side and outlet located at the right side boundaries were considered as constant velocity and pressure respectively the half way bounce back boundary condition was applied to the boundary of solid grains the domain size and the particle diameter are 800 800 and 0 608 micrometers respectively after doing some grid independent tests it was obvious that the best domain size is 500 500 in lattice unit the ratio of solid density to fluid density and relaxation time used in eq 7 are taken as 1 05 and 0 65 in lattice unit respectively we have tested our model by simulating fluid flow in well defined domain composed of cylindrical grains to obtain permeability and its corresponding carman kozeny parameters for which verified data are available in the literature yazdchi et al 2011 colloids were injected through fluid flow with the constant flow velocity in inlet simulations show initial distribution of particles in inlet has negligible effect on results see appendix a colloids were injected at equal distances on a vertical cross section located behind the porous media inlet face 4 1 pore space evolution the simulation results are used to compute the porosity and permeability values and to investigate the relations between these two parameters in each case the simulation was continued until the complete clogging of the media is taken place as shown in fig 3 the obtained two dimensional distributions of the absorbed mass shown by yellow color in fig 3 were used to calculate the profile of 1d averaged absorbed mass along the domain fig 3b to obtain the average values the domain was divided into several sub domains normal to the flow direction and adsorbed mass was averages in each sub domain fig 3b shows that a major part of clogging takes place at the inlet section of each domain particularly for the domain with the initial porosity value of 0 44 4 2 porosity permeability relations for evolving porous media we have chosen to fit the carman kozeny and power law relations hommel et al 2018 to our pore scale lbm simulations to obtain a relation between porosity and permeability moreover these relations include coefficients which we have tried to relate them to the properties of each porous medium the measured porosity and permeability values for each sample are based on the whole domain size rather than a subsection of the domain where for example most of clogging is taking place this choice is based on the definition of representative elementary volume rev requiring a domain of several pores grains along each direction to provide representative macroscopic values given our initial domain sizes with several pores along each principal direction we have considered each whole domain as a rev and measured macroscopic properties and their evolutions by integrating within the whole domain doing so our reported values between different samples are consistent with their initial porosity and permeability values and also consistent with each other as they are all based on the whole domain size 4 2 1 carman kozeny and power law relations carman kozeny kozeny 1927 carman 1937 hommel et al 2018 is a well known relation widely used to link porosity and permeability values in both experimental and modeling studies this equation is based on simplifying porous media as parallel capillary tubes with equal geometry and size 16 δ p l 180 μ φ s 2 d p 2 1 φ 2 φ 3 υ where ϕ s is sphericity dp and ϕ are particle diameter and porosity respectively this equation can be reformulated as 17 k φ s 2 d p 2 180 φ 3 1 φ 2 where k is permeability eq 17 can be written as 18 k φ 3 σ 1 φ 2 s 2 eq 18 is may be written using tortuosity σ and specific surface area s which compared to ϕ s and dp are more relevant parameters to natural porous media gallo et al 1998 macquarrie and mayer 2005 mostaghimi et al 2013 showed that using the kozeny carman equation especially for complex tortuous heterogeneous or poorly connected porous media significantly overestimates the permeability the original kozeny carman equation may be also written as 19 k k 0 φ 3 s 0 φ 0 3 s where to provide a simpler formulation the specific surface area s may be a replaced by 1 φ during clogging both porosity φ and specific surface area s change over time s 0 is the initial specific surface area due to clogging permeability approaches zero without necessarily porosity approaching zero throughout the sample to consider this effect we applied a modified form of carman kozeny relation as 20 k k 0 a φ φ c r φ 0 φ c r b 1 φ φ c r 1 φ 0 φ c r c where ϕ0 ϕcr and k 0 are the initial and critical porosity values and initial permeability respectively this equation is derived from eqs 17 and 18 and has been made dimensionless by using of k 0 ϕ0 ϕ cr and 1 ϕ0 ϕ cr terms the addition to this relation in comparison to the eqs 17 and 18 is the critical porosity ϕ cr which is used to describe the nonzero porosity threshold at which the permeability approaches zero the idea is that only for ϕ ϕ cr the pore space is connected and thus only the pore space exceeding this limit ϕ ϕ cr contributes to flow comparison the eq 20 with eqs 17 and 18 shows what values have been substituted with coefficients a b and c in the original form of the kozeny carman relation provided in section 4 2 1 eq 20 also studied by wijngaarden et al 2013 steefel et al 2015b xie et al 2015 pandey et al 2015 changes in parameters related to geometry as sphericity ϕ s or characteristic particle diameter d p are assumed negligible however as geometry changes should be considered in our simulations eq 20 was applied which has a similar form as the original kozeny carman relation some studies marshall 1958 taylor et al 1990 require calculations for geometrical corrections during damaging the pore structure in our presented model the only terms required to calculate are permeability and the starting and ending critical porosities power law relation hommel et al 2018 verma and pruess 1988 ives and pienvichitr 1965 is another well known relation widely used to describe permeability changes in both experimental and modeling studies this provides a rather simple relation where the only major fitting parameter is the exponent b and there is no strong term for specific changes of media geometry this relation involves the medium specific exponent a as a parameter 21 k k 0 a φ φ 0 b in this study the power law relation is expresses as 22 k k 0 a φ φ c r φ 0 φ c r b 4 3 fit on carman kozeny and power law relations we have fitted the carman kozeny and power law relations to the computed porosity and permeability values obtained from lbm simulations fig 4 shows that porous media geometry and topology both influence the relation between permeability and porosity change of porous media topology affects the relation between permeability and porosity and moreover changes the critical porosity of the media at which clogging takes place this is specially the case for the media with the larger porosity of 55 as the presence of larger pores gives more freedom for particle movements by change of topology fig 4 also shows a wide range of permeability for a given porosity value which can vary even more than two folds the solid lines in fig 4 are the plots of carman kozeny and power law relations i e eqs 20 and 22 describing the porosity permeability relation based on the shown data points table 1 provides properties of the porous media initially as well as after the clogging all three cases have shown that the final porosities under which permeability values tend to zero are different for each topology table 2 provides the coefficients of carman kozeny relation coefficient a is nearly constant for both topologies and different initial porosities coefficient b shows slightly more sensitivity to the initial porosity compared to the topology coefficient c shows dependency on both topology and initial porosity at low initial porosities and becomes less sensitive for domains with higher initial porosity comparing eqs 19 and 20 shows that coefficient c depends on specific surface area as clogging affects the specific surface area and porosity of the media coefficient c is sensitive to these two parameters schneider et al 1996 used a form of kozeny carman model that the values of exponents a and b were set respectively to 3 0 and 2 0 and in some cases these values were changed depending on the rock type coefficient c in their model was assumed to be equal to 1 0 some studies like taylor et al 1990 vandevivere et al 1995 vandevivere et al 1995 martys et al 1994 have replaced 1 ϕ term with the specific surface area and considered the value of a b and c to be equal to 3 0 1 0 and 1 0 respectively using a rearranged form of carman kozeny relation amaefule et al 1993 set coefficients a and b to 3 0 and 2 0 and coefficient c was expressed as γ 1 s 2 σ based on the relation of amaefule et al 1993 together with the experimental data civan 2000 used another form of carman kozeny relation k φ γ φ 1 φ β to find values of γ 4 1 5 82 and β 2 96 3 89 table 3 provides coefficient values corresponding to the power law relation coefficient a remains nearly unchanged while coefficient b indicates a strong dependency on both initial porosity and topology the exponent b is an empirical parameter in eq 22 hommel et al 2018 as discussed in section 4 2 1 the initial porous medium properties and the conditions that cause the pore geometry to change determine the value of b for instance bernabe et al 2003 suggested that the value of b is between 2 5 and 3 0 for plastic compaction 8 0 for mineral precipitation b 10 0 for chemical alteration and b 20 0 for mineral dissolution doyen 1988 measured an evolving pore space and calibrated a value of b 3 8 for water containing co2 garing et al 2015 observed an increase in permeability which was fitted using a power law relation with an exponent b ranging between 4 87 and 23 72 verma and pruess 1988 stated that the value of b may range from 6 0 to less than 1 0 depending on the porous material type which for their study ranged from 1 98 to 0 73 based on experimental data collected from sandstone units in our study the simulations resulted in values of exponent b between 0 63 and 1 22 4 4 effect of clogging on flow field streamlines pore clogging alters pore structures which in turn changes the flow field and the fluid streamlines fig 5 shows the change in the path of a set of streamlines during clogging particle retention at pore throats changes the preferential flow and streamlines become longer and more tortuous at the same time the major streamlines become less in number since the remaining free paths that fluid can freely flow through them become less frequent as shown in fig 5 the corresponding velocity distributions of plots in fig 5 are shown in fig 6 flow velocities with larger magnitudes and variations become more dominant close to the inlet of the sample where clogging is effectively taking place because of the mass conservation law as during clogging process in all three cases the number of main paths are decreased the fluid velocity magnetude becomes larger over time because of lower initial porosity in case φ0 0 44 active pore throats are clogged sooner and average fluid velocity over time is higher compared to the other two cases 4 5 effect of clogging on pore size evolution to explore the effect of clogging on pore size evolution we have chosen the sample with the initial porosity of 0 55 to analyze change of pore sizes from the initial state i e ϕ0 0 55 until complete clogging fig 7 the location of pore throats i e the narrowest constrictions between pores is used to divide the total pore space into an ensemble of individual pores which are marked by different colors in fig 7 the solid rectangles show a section of the domain close to the inlet where most of the clogging takes place and finally becomes disconnected fig 7 c to diminish the permeability the resulting pore size distributions at three different times i e times corresponding to the initial intermediate and the final clogging state of the sample are provided in fig 7 d the distribution of pore sizes during clogging shows that clogging did not significantly affect the pore size distributions and only pores smaller than 150 µm show a slight change in their population density this is because while pore clogging significantly affects permeability by locally disconnecting flow pathways a major fraction of pores remains unclogged and therefore pore size changes are less noticeable compared to the permeability changes 5 summary and conclusion porosity and permeability are the two major transport properties of porous media and relating permeability to porosity changes is the subject of several studies this study explored porosity changes due to particle transport and clogging and how permeability is affected by this change the realistic pore space was obtained using x ray computed tomography and further used to create several extra pore structures by modifying grain sizes as well as grain locations and their arrangements this provided a total of six domains to systematically study the effect of geometrical parameters i e porosity change due to alteration of grain sizes and topological parameters i e pore connectivity changes due to alteration in grain arrangements on porous media clogging for each domain fluid flow was simulated using a lattice boltzmann method and lagrangian approach was applied to simulate transport of particles a new method based on velocity variations at the grain surface was implemented to obtain the ultimate location of particle deposition after particles come close to the grain surface particle transport and clogging was continued up to full clogging of the media where permeability approached zero at a critical porosity which was different for each domain the macroscopic i e sample scale behavior was described using carman kozeny and power law relations to relate permeability to porosity variations the use of systematic domains with different geometrical and topological properties enabled us to link the coefficients in the carman kozeny and power law relations to porous media properties in carman kozeny relation coefficient a was nearly constant with a value of 1 05 for different initial porosities and different topologies however coefficient b was affected by the initial porosity while change in topology had not significant effect on it coefficient c showed strong dependency on both topology and initial porosity at low initial porosities and the dependency on topology reduced for domains with higher initial porosity in power law relations coefficient a remains nearly constant for different domains while coefficient b indicated a strong dependency on both initial porosity and topology the preferred particle deposition close to the inlet of the sample and along the flow paths of particles caused a non uniform change of porosity along the sample depending on geometrical and topological properties of the media therefore internal variations of different parameters were studied including profile of absorbed mass in each domain during clogging process change of pore sizes and the main streamlines and their evolution during clogging the evolution of pore sizes during the clogging showed that the collection of pores with an intermediate size relative to the range of pore sizes present in porous media were affected the most by clogging the main flow streamlines were initially short and uniformly distributed along the domain however during clogging streamlines decreased in number and became much longer in length which shows an increase of tortuosity in the presence of pore clogging in this study we employed 2d domains to enable us to perform several simulations needed for a systematic study and to obtain the optimum location for particle deposition based on the velocity variations at the grain surfaces while these simulations provided valuable insight into the role of geometrical and topological parameters in porosity permeability relations 3d simulations are required to provide more realistic values for correlation parameters based on pore connectivities in the three dimensional space furthermore in the developed method particles are point objects when they are being transported in fluid however we have developed and tested a method to determine the realistic location of the attached particles based on the velocity distribution at the grain surface the assumption of point particles in our method up to the point that they are attached makes it suitable for small particles for which the particles do not significantly disturb the fluid flow during their transport and their effect on fluid flow is considerable only after they are attached and changed the pore space and therefore affecting fluid flow author statements the corresponding author of this manuscript certify that the contributors and conflicts of interest statements included in this paper are correct and have been approved by all co authors declaration of competing interest the authors have no conflict of interest to declare acknowledgments we note that there are no data sharing issues since all of the numerical information is provided in the figures produced by solving the equations in the paper we thank the anonymous reviewers for their insightful comments and suggestions we thank enno de vries for his help on generating the pore scale domains supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103530 appendix b supplementary materials image application 1 appendix a to evaluate the sensitivity of simulations to changes in the injection location of particles at the inlet we performed simulations applying different initial distribution of particles at the inlet face of porous media fig a1 shows that initial distribution of the particles has negligible effect on the relation between porosity and permeability this can be explained as i the domain is large enough and the inlet effects are not propagated through the whole media and ii the x ray imaged pore structure used in this study is rather homogenous making it less sensitive to the conditions at the inlet b fig a2 provides log scale plots for fitting of carman kozeny and power law relations same data are shown using a linear scale in fig 4 of the manuscript this figure shows that both relations were able to fit data and provided similar fit qualities 
504,building a quantitative relation between the spatial heterogeneity of the hydraulic conductivity fields and the macroscale behavior of solute transport is fundamental for groundwater environment problem in this work the deep learning technique is explored to build the functional mapping between the hydraulic conductivity field and the longitudinal macro dispersivity we examine the capability of the deep neural network in estimating macro dispersivities of conductivity fields with different variances the universality of the trained deep neural network is investigated comparisons of the neural network results and the reference values macro dispersivities from transport simulation suggest the promising potential of deep learning technique in porous media with moderate heterogeneity for a given size of training datasets the deep neural network produces better macro dispersivity estimation for the conductivity field with smaller variance the trained neural network by conductivity fields with larger variance has stronger universality for macro dispersivity estimation this study demonstrates that deep neural network can be an effective alternative for estimating macroscale behavior of solute transport by directly interpreting hydraulic conductivity fields keywords macrodispersivity convolutional neural work hydraulic conductivity field contaminant transport heterogeneity groundwater 1 introduction describing solute transport in porous media is fundamental for many groundwater contamination problems de barros et al 2016 han et al 2016 guo et al 2019 the dispersive transport models are the widely used models at various scales therefore estimating the dispersivity at corresponding scale is essential for any efforts in predicting the spreading of a contaminant plume at field scale the structure of natural porous media usually exhibits spatial variability which has a profound impact on solute transport fiori et al 2016 cvetkovic et al 2016 for applying the advection dispersion models under field conditions hydro geologists have proven that the magnitude of field scale dispersivity macro dispersivity can be several orders of magnitude higher than lab scale value for the same material fiori et al 2017 this increase mainly attributes to the spatial variability of aquifer structure which can be generally described by the spatial distribution of the hydraulic conductivity considering the heterogeneous distribution of hydraulic conductivity as a random field and relating flow and transport to its statistical moments has been one of the primary goals of the field of stochastic modelling e g dagan 1989 gelhar 1993 rubin 2003 a fundamental issue addressed by these works is how macro dispersivity can be related to the statistical properties of the hydraulic conductivity field e g rehfeldt et al 1992 fiori et al 2015a zech et al 2015 however the general applicability of the stochastic approach is sometimes questionable due to several foundational assumptions and the first and second order spatial statistics cannot provide sufficient information on estimating of macro dispersivity e g zinn and harvey 2003 zheng and gorelick 2003 zheng et al 2011 molinari et al 2015 sanchez vila and fernàndez garcia 2016 the conductivity fields with the same first two moments may produce very different solute spreading because of the spatial patterns that are not characterized by these statistics e g zinn and harvey 2003 bianchi and pedretti 2018 the concepts of connectivity and geological entropy then emerge as other attempts to characterize the transport behavior from the heterogeneous conductivity fields e g renard and allard 2013 rizzo and de barros 2017 fiori 2014 fiori and jankovic 2012 freixas et al 2017 fernàndez garcia et al 2010 bianchi and pedretti 2017 2018 in short researchers have made great efforts to predict solute transport behavior only from a characteristic description of the conductivity field despite the helpfulness of these works in understanding the correlation between the heterogeneity of conductivity field and the transport behavior a direct and efficient functional mapping between the conductivity field and the transport behavior for predictive purposes remains to be solved deep learning methods are representation learning methods which can be fed with raw data and automatically discover the representations needed for classification the convolutional neural network cnn which is a particular network type of deep learning is designed to process data with multiple arrays and has dominated the field of image classification and achieved many practical success lecun et al 2015 researchers have adopted cnn to solve various problems in water science shen 2018 e g extracting flow features from flow fields ströfer et al 2019 serving as a surrogate model for uncertainty quantification of partial differential equations zhu and zabaras 2018 mo et al 2018 reconstructing porous media mosser et al 2017 cang et al 2017 wang et al 2018 and reducing dimensionality of complex geological models laloy et al 2017 2018 chan and elsheikh 2017 particularly cnn has also been used to predict the effective properties of heterogeneous materials directly srisutthiyakorn 2016 cang et al 2018 wu et al 2018 vasilyeva and tyrylgin 2018 srisutthiyakorn 2016 demonstrated the possibility of predicting permeability from rock images by cnn wu et al 2018 improved the performance of cnn in predicting permeability by involving physical information vasilyeva and tyrylgin 2018 constructed a cnn to accelerate the calculation of elasticity tensors of random poroelastic media since the macro dispersivity is supposed to be formation specific zech et al 2015 it should be possible to develop a surrogate model that directly map a conductivity field to corresponding macro dispersivity by cnn regarding conductivity fields as abstract images of aquifers and resembling the task of image classification lecun et al 1995 krizhevsky et al 2012 cnn takes a conductivity field as input and gives the classification label macro dispersivity as output by recognizing the features in the field in this work a cnn of eight layers is set up to learn a map between stochastic conductivity field and longitudinal macro dispersivity based on synthetic two dimensional conductivity fields a direct and efficient functional map between macro dispersivity and conductivity field is built such surrogate model has the potential to enable fast estimation of macro dispersivities of aquifers without performing direct simulations of physical models which are usually time consuming and need many input parameters to our knowledge this is the first study to interpret macro dispersivity from conductivity field by deep neural network directly two dimensional isotropic multi gaussian fields are generated with a sequential gaussian simulation bellin and rubin 1996 reference macro dispersivities are calculated by conducting numerical trace tests the performance of the proposed neural network architecture is validated through various training and test datasets for a specific size of training datasets the estimating capability of the neural network declines with increasing variances namely increasing heterogeneity of conductivity fields notably the universality of the deep neural network in estimating macro dispersivities of conductivity fields with different degree of heterogeneity is also explored assessments of the proposed neural network architecture show that directly estimating macro dispersivity from distribution of hydraulic conductivity is promising the rest of the paper is organized as follows section 2 recapitulates the entire computation framework the cnn architecture and generation of training dataset results of numerical tests and discussions are presented in section 3 finally section 4 concludes the paper 2 methodology since the conductivity fields can be regarded as abstract images of aquifers and the macro dispersivity which is an effective coefficient to characterize field scale dispersion process in dispersive subsurface mass transport models can be considered as an inherent property of an aquifer using the deep learning methods to estimate macro dispersivity from conductivity field is worth a try in this section a framework to train a convolutional neural network is set up and the detailed steps are presented 2 1 sketch of training framework the objective of the training framework presented here is to train a deep learning model for fast estimation of macro dispersivity from the spatial distribution of the hydraulic conductivity this training framework consists of the following steps as illustrated in fig 1 1 generating training datasets two dimensional random fields of the hydraulic conductivity are generated direct simulations with the random walk particle tracking method salamon et al 2006b are then used to compute the macro dispersivities of the generated conductivity fields the field dispersivity pairs consist of the training datasets for the deep neural network model the details of generating the training datasets are presented in section 2 2 2 training the cnn the training datasets from the previous step are then used to train our cnn that takes a heterogeneous conductivity field as input and gives macro dispersivity as output section 2 3 specifies the cnn architecture and training process 3 estimating macro dispersivities the trained cnn is then used to estimate macro dispersivities of new conductivity fields that are not in the training datasets it should be noted that this work is established on two dimensional synthetic conductivity fields and the macro dispersivity here is referred to as longitudinal macro dispersivity three dimensional synthetic or real world conductivity field will be the goal of future works 2 2 building of training datasets simulations of uniform flow trace tests are conducted on the conductivity fields to estimate the corresponding macro dispersivities square bi dimensional confined aquifers with the uniform mean flow in the x direction is considered and the flow is driven by a mean hydraulic gradient equal to j 0 0176 the computational domain used in the study is illustrated in fig 2 the domain is discretized into square cells each of dimension 0 25ilnk there are 140 grid cells in each coordinate direction corresponding to an overall grid dimension of 35ilnk for boundaries parallel to the mean flow boundaries conditions are no flux the rest boundaries are the constant head we have to emphasize that the grid block number per correlation scale is fixed to 4 in this work only training sets of different variances are considered the effects of training sets of different correlation lengths are not investigated here and there may be a distribution shift that links to the ilnk change liu and ziebart 2014 this issue will be investigated in the future study 2 2 1 generating hydraulic conductivity fields the synthetic aquifers are characterized by spatially varying hydraulic conductivity k x and the ln k fields follow a gaussian random function the isotropic exponential covariance model is considered 1 c l n k r σ l n k 2 exp r i l n k where r is the separation vector between two points of the aquifers ln k denotes the log hydraulic conductivity σ l n k 2 is the variance of ln k and ilnk is the correlation length the log conductivity fields are generated through hydro gen bellin and rubin 1996 the geometric mean of the ln k fields is set equal to 116 7 m day k g 116 7 m day which is a reasonable value for a sandy aquifer 2 2 2 generating velocity fields before the transport simulations velocity fields must be developed at first groundwater flow is described by the flow mass conservation equation and darcy s law under steady state flow conditions the flow equation can be formally expressed as 2 k x h x 0 where h x is the hydraulic head and k x is the spatially variable hydraulic conductivity a finite difference groundwater flow model modflow harbaugh et al 2000 is used to solve the flow problem and the velocity field corresponding to each conductivity field is then determined with the mean hydraulic gradient the darcy s velocity distribution is computed by 3 q x k x h x 2 2 3 transport simulation with particle tracking using the solute mass conservation equation and fick s law at the local scale solute transport through porous media is locally governed by the advection dispersion equation and can be written as 4 n c x t t q x c x t n d c x t where q is the darcy velocity n is the porosity c is the solute concentration and d is the local hydro dynamic dispersion tensor solute transport simulations are conducted based on velocity fields from eq 2 through a random walk particle tracking transport code rw3d fernàndez garcia et al 2005 salamon et al 2006b 2006a henri and fernàndez garcia 2014 2015 the porosity is assumed to be spatially homogeneous with a value of 0 35 the tracer plume migration is simulated by partitioning the tracer mass into a large number of representative mass particles advection is simulated by moving particles within the velocity field whereas dispersion is emulated by brownian motion this lagrangian method is free of numerical dispersion more details about the numerical algorithm can be found in the work of fernàndez garcia et al 2005 and salamon et al 2006b at the beginning of simulations a number of particles 1000 are uniformly distributed in a vertical line perpendicular to the mean flow direction this line is located 5ilnk away from the up gradient boundary to relieve boundary effects rubin and dagan 1988 1989 the source size is 25ilnk in the transverse direction to the mean flow and is centered with the transverse direction for estimating longitudinal macro dispersivities a control plane transverse to the mean flow direction is placed 3ilnk away from the down gradient boundary it should be noted that 1000 particles are enough to offer an accurate estimate of macro dispersivity in this work the validation of the chosen particle number is displayed in appendix a 2 2 4 estimating of macro dispersivities from temporal moments the first arrival time passing through the control plane is tracked until particles exited the lower constant head boundary then the estimation of associated temporal moments of the breakthrough curve btc can be conducted without having to evaluate the actual shape of the btc the nth absolute temporal moment mn x is expressed as the expected value of the arrival time of particles at the control plane to the nth power fernàndez garcia et al 2005 fadili et al 1999 shapiro and cvetkovic 1988 5 m n x 1 m total 0 t n q c f x t d t 1 k 1 n p m p k k 1 n p m p k t p k x n where m total is the total injected tracer mass q is the total water flux passing through the control plane x is the mean flow direction coordinate c f is the flux concentration of tracer passing through the control plane m p k is the mass of a particle t p k is the first arrival passage time of the kth particle and np is the total number of particles arrival at the x control plane the nth central temporal moment m t n x is then calculated by the relationship between central and absolute temporal moments fernàndez garcia et al 2005 kendall and stuart 1977 6 m t n x 1 m total 0 t m 1 x n q c f x t d t r 0 n n r m n r x m 1 x r macro dispersivities from temporal moments are calculated as fernàndez garcia and gómez hernández 2007 fernàndez garcia et al 2005 aris 1958 goltz and roberts 1987 7 a 11 x p x p 2 m t 2 x p m 1 x p 2 where x p is distance between the tracer source and the control plane a 11 is the longitudinal macro dispersivity macro dispersivity from eq 7 is the apparent longitudinal macro dispersivity that corresponds to a specific hydraulic conductivity field the apparent longitudinal macro dispersivity is viewed as equivalent value in homogeneous porous media applying it to the classical advection dispersion equation will lead to the same first two temporal moments of the btc as observed in the simulations for heterogeneous porous media fernàndez garcia et al 2005 finally the estimated macro dispersivities and the corresponding hydraulic conductivity fields constitute the training and validation datasets other details on the evaluation of macro dispersivities from temporal moments can be found in the work of fernàndez garcia et al 2005 and fernàndez garcia and gómez hernández 2007 furthermore it should be noted that the calculated macro dispersivities did not reach its asymptotic value due to the limitation of the computation domain 2 2 5 validation of physical model simulation fiori et al 2015b have shown that for purely advective transport the mean advective velocity u calculated from the results of the transport simulation is equal to the mean value of the component of the eulerian velocity field parallel to the mean flow direction u mean the same as the work of bianchi and pedretti 2018 and jankovic et al 2017 the accuracy of the numerical simulation is evaluated by checking the equality between the two mean velocities for each simulation the mean advective velocity u is calculated by 8 u l t where l is the distance between the injection source and the control plane t is the average arrival time of the particles the components of the eulerian velocity field at the center of each grid block are obtained by averaging the specific discharge across the numerical grid block interfaces under homogeneous effective porosity the fluxes are derived from the cell by cell balance of the groundwater flow model modflow for this work 50 simulations with different conductivity fields are conducted to assess the equality of u and u mean the rations between u and u mean are plotted in fig 3 and are all close to one besides the effective macro dispersivities from numerical simulations are compared with analytical models of dagan 1984 and gelhar and axness 1983 dagan 1984 and gelhar and axness 1983 derived analytical models of ensemble macrodispersivity for moderate heterogeneity σ l n k 2 1 the same setup of simulation as the work of fernàndez garcia and gómez hernández 2007 is used at first the effective macro dispersivities of 50 realizations are compared with dagan 1984 analytical model the result is displayed in fig 4 then another 50 simulations with the setup of section 2 2 are conducted the effective and apparent macro dispersivities of 50 realizations are compared with dagan 1984 and gelhar and axness 1983 analytical models in fig 5 the effective macro dispersivity is the averaged of apparent macro dispersivities from 50 realizations from both figures it can be seen that the effective macro dispersivity generally agrees well with the model of dagan but has not reached the asymptotic limit of gelhar overall these results show that the physical model simulations are accurate 2 3 architecture of the cnn neural networks are a set of algorithms that are composed of some highly interconnected processing elements neurons working through a hierarchy of layers schmidhuber 2015 a regular cnn consists of some convolutional layers pooling layers and fully connected layers fig 1 a convolutional layer is composed of some convolution kernels which are used to compute the feature maps when a 2d image is inputted a convolutional layer g is obtained by employing some filters τ q r k i k j where q is the number of filters and k is kernel size to evolve an input pixel or point value h m n to get the feature value g m n q h m n at location m n as 9 g m n q h m n f i 1 k i j 1 k j τ i j q h m i n j this results in a convolutional lay g consisting of q feature maps two other important parameters for the convolutional layer are the stride and padding stride determines the distance between two successive moves of the filter and padding specifies the padding of the borders of the input image with zeros for size preservation pooling layers combine the outputs of neuron clusters from the convolutional layer into a single neuron and so the learnable parameters are reduced in the problem of estimating macro dispersivity from the conductivity field as concerned in this study the convolutional layers extract features from an input conductivity field by using small squares of the input field the feature maps of the conductivity field encompass the correlation structure of conductivity field as well as the higher order moments the pooling layers reduce the dimensionality of each feature map but retain the vital information for estimating the macro dispersivity the fully connected layers flatten the feature map matrixes and combine these features to create a model with macro dispersivity as the final output the cnn architecture proposed here is implemented in the machine learning framework pytorch paszke et al 2019 and is based on the alexnet krizhevsky et al 2012 the cnn here includes five convolutional layers each followed by a batch normalization operation krizhevsky et al 2012 a relu activation function and a pooling layer and three consecutive fully connected layers fig 1 the numbers of layers channels and neurons are empirically selected to ensure enough complexity of the neural network and minimize the mean squared error of the estimated macro dispersivities for the training datasets the first convolutional layer has 16 feature maps the first four convolutional layers all have a convolutional kernel of size 7 7 to extract different features from the corresponding input the last convolutional layer has a convolutional kernel of size 5 5 in the pooling layers the max pooling function and a kernel size of 2 2 with strides 2 2 are adopted the three fully connected layers all have 256 neurons the details of the deep neural network architecture are displayed in table 1 the other details of network architecture and hyperparameter selection of the particular problem considered are presented in appendix b the computational cost is of the order of seconds for estimating the macro dispersivities of 1000 conductivity fields with 140 140 data points by using the trained cnn and is three orders of magnitude lower than the simulations of above mentioned physical models 2 4 performance criteria the root square error rmse and the coefficient of determination r 2 are used to evaluate the performance of our deep neural network the results of simulations with random walk particle tracking method are taken as reference values for rmse a value of 0 means a perfect fit between the reference values and the estimated values the smaller rmse value stands for better performance the definition of rmse is shown below 10 r m s e i 1 n a i sim a i cnn 2 n where a i sim is the ith simulated macro dispersivity in validation test datasets a i cnn is the ith estimated macro dispersivity by the cnn the definition of the coefficient of determination is presented as 11 r 2 1 i 1 n a i sim a cnn mean 2 i 1 n a i sim a sim mean 2 where a cnn mean represents the average of the estimating macro dispersivities by the cnn a sim mean is the average of the reference macro dispersivities in the test dataset the r 2 values range between 0 and 1 and the value of 1 indicates a perfect correlation 3 synthetic experiments setting synthetic experiments are conducted to demonstrate the capability of the neural network to estimate the macro dispersivity by considering different variances of conductivity fields 3 1 datasets we evaluated the cnn using datasets of conductivity fields produced with increasing variances of 0 1 0 2 0 3 and 0 5 called var0 1 var0 2 var0 3 and var0 5 respectively the discretized field realizations vary highly with increasing variance and the intrinsic dimensionality of the random field is 19600 for 140 140 grids which are the cases presented here these field characteristics create a significant challenge for data driven models to capture our current data includes three sets the training validation and test sets each training set contains 4000 training data and the validation and test set each consist of 1000 input conductivity fields all datasets are organized in the form of images the training set is limited to 4000 samples because each simulation run of the random walk has high computational cost a sample costs about 10 min cpu intel xeon e5 2630 v3 2 4ghz 16 cores ram 96gb ddr4 2133mhz ecc a training set of 4000 samples costs almost a month also the data on field scale dispersion in real aquifers is even more limited zech et al 2015 gelhar et al 1992 generating more input realizations of hydraulic conductivity using only the training dataset is another critical problem shen 2018 the interest of this work is to validate the potential possibility of building the field to dispersivity mapping with the limited training dataset 3 2 training the cnn is trained with adam kingma and ba 2014 a variant of stochastic gradient descent with the loss function being l 2 regularized mean square error mse which is implemented as weight decay in pytorch the initial learning rate is 0 001 the weight decay is 0 0005 the batch size is 64 a learning rate scheduler which drops 10 times on plateau of the rooted mse is also used the model is trained 200 epochs the estimating performance is evaluated with validation and test fields that are not included in the training datasets the macro dispersivities estimated from the simulations of the random walk particle tracking method are taken as reference values 3 3 experiments at first cnn is trained with the four datasets of different variances respectively the estimation performance of these trained neural networks is tested with the corresponding test set the estimating capability of the proposed deep neural network is demonstrated and the influence of variance of conductivity field on estimation accuracy of macro dispersivity is revealed next the performance of the neural networks trained with conductivity fields of relatively large variance training sets of var0 5 var0 3 and var0 2 are tested with the test sets of conductivity fields of relatively small variance test sets of var0 1 var0 2 and var0 3 it is found that the neural networks trained by large variances can estimate conductivity fields with small variances finally the estimating capability and universality of the trained neural networks training set of var0 1 var0 2 and var0 3 are verified by using test datasets with large variances test sets of var0 2 var0 3 and var0 5 the universality of the neural network to estimate conductivity fields with relatively large variances is validated detailed setup about the three experiments is presented in table 2 4 results and discussion 4 1 experiment 1 training cnn with datasets of different variances fig 6 displays the training process with epochs for the training sets of different variances it is seen that the validation rmse increases with increasing variance and the validation r 2 decreases with increasing variance however the cnn trained by var0 5 still has a decent r 2 value 0 73 on the validation set these trained neural networks are then tested with the test set of the same variance as the training set all test sets are consists of 1000 samples of conductivity fields the test results are illustrated in fig 7 it should be noted that the values of rmse and r 2 are calculated from the test set of 1000 samples but only 50 samples of conductivity field which are randomly sampled from test set are displayed from fig 7 it is found that the neural networks trained with different variances all have achieved accurate estimation of macro dispersivity in terms of rmse and r 2 for all the four cases most macro dispersivities estimated from the neural networks fall in the error range of 20 marked as baby blue shaded regions in figures the cnn trained by the dataset var0 1 has smallest rmse 0 05 and largest r 2 0 869 and the cnn trained by dataset var0 5 has largest rmse 0 375 and smallest r 2 0 72 from the two criteria it can be seen that the estimation performance generally gets worse with the increasing variance of dataset however for var0 1 and var0 2 the decreasing of estimation performance is not obvious on the whole all the four trained cnns have a satisfactory estimation of macro dispersivity with the limited size of the training set from this experiment it is found that cnn can achieve acceptable estimations of macro dispersivities for conductivity fields with weak heterogeneity σ lnk 2 0 5 and increasing variances of conductivity fields would make a functional mapping difficult for the cnn with the limited size of training datasets larger training datasets may introduce better estimation performance 4 2 experiment 2 test trained cnns with test sets of small variances in experiment 2 we will use the trained cnns in experiment 1 to estimate macro dispersivities of conductivity fields with smaller variances than the training set for the neural network trained by conductivity fields of σ l n k 2 0 2 an only test set of var0 1 are used fig 8 in fig 8 nearly all estimated macro dispersivities are within 20 error range of the reference values the large value of r 2 0 831 confirms the results and the rmse is close to the results of fig 7 a1 the cnn trained by var0 2 can give accurate estimates of the test set of var0 1 as same as the test set of var0 2 fig 7 b1 fig 9 shows the results of using cnn trained by var0 3 to estimate macro dispersivities for test sets of var0 1 and var0 2 for the test set of var0 1 fig 9 a1 b1 underestimation of macro dispersivities can be observed for most data points nevertheless there are still at least half of the data points are within the error range the r 2 0 496 validates the estimation performance for the test set of var0 2 fig 9 a2 b2 the cnn has achieved quite good estimation performance most estimated macro dispersivities are within the error range of the reference values and the r 2 is 0 829 the rmse 0 114 is almost equal to the result 0 110 from the neural network trained by var0 2 these results show that the cnn trained by var0 3 can estimate test set var0 2 and var0 1 however the estimation performance drops with the increasing disparity of variances the results of using cnn trained by var0 5 to estimate macro dispersivities for the test sets of var0 1 var0 2 and var0 3 are displayed in fig 10 for the test fields of var0 1 fig 10 a1 b1 more than 50 of estimations fall within the 20 error range while for the test set fields of var0 2 and var0 3 most estimations fall within the 20 error range the r 2 values for the test sets generally increase with increasing variances of conductivity fields specifically for the test set of var0 3 r 2 0 74 the cnn trained by var0 5 can provide almost the same estimation performance as the test set of var0 5 r 2 0 72 similar to the results of fig 9 the estimation performance drops with the increasing difference of variances in summary experiment 2 firstly demonstrates that the cnn trained by fields with a specific variance has some ability to estimate conductivity fields with the other smaller variances and the estimating performance decreases with increasing differences of variances secondly the neural network trained by large variance has a stronger capacity in estimating macro dispersivities of conductivity fields with small variances more specifically it seems that the cnn trained by var0 5 has better universality than the cnn trained by var0 3 because the cnn trained by var0 5 works much better in estimating test set of var0 3 than using the cnn trained by var0 3 to estimate test set of var0 1 although the disparity between variances of train set and test set are 0 2 for the two cases furthermore the cnn trained by var0 5 still has better performance in estimating test set of var0 2 than estimating test set of var0 1 with the cnn trained by var0 3 4 3 experiment 3 test trained cnns with test sets of large variances in experiment 3 we will use the trained cnns in experiment 1 to estimate macro dispersivities of conductivity fields with larger variances than the training set fig 11 illustrates the results of estimating macro dispersivities of conductivity fields with σ f 2 0 2 0 3 and 0 5 by using the cnn trained by train set of σ f 2 0 1 for the test set of var0 2 fig 11 a1 b1 almost all data points fall within the error range the neural network has achieved fairly good estimations r 2 0 777 with increasing variance of test set fig 11 a2 b2 underestimations of macro dispersivities happen for large reference values for the test set of var0 5 fig 11 a3 b3 the underestimations of macro dispersivities for large reference values become prominent correspondingly the values of r 2 decrease with increasing variances in fig 12 the neural network trained by var0 2 is used to estimate the test set of var0 3 and var0 5 similar to fig 11 the estimation performance of the neural network decreases with increasing variance comparing fig 12 with fig 11 a2 a3 b2 b3 it is found that the neural network trained by var0 2 has stronger estimating ability than the neural network trained by var0 1 for example the neural network trained by var0 1 generally underestimates the macro dispersivities for the test set of var0 5 r 2 0 117 while the neural network trained by var0 2 can get more satisfactory estimations for the test set of var0 5 r 2 0 584 in fig 13 the neural network trained by var0 3 is used to estimate macro dispersivities for the test set of var0 5 the rmse and r 2 are almost consistent with the results of fig 7 a4 b4 in which the macro dispersivities for the test set of var0 5 are estimated by the neural network trained with the train set of var0 5 rmse 0 375 r 2 0 72 the estimating performance of the neural network trained by var0 3 is better than the neural networks trained by var0 2 and var0 1 comparing fig 13 with fig 11 a3 b3 and fig 12 a2 b2 the r 2 of macro dispersivities for test set of var0 5 from the neural network trained by var0 3 is the largest r 2 0 703 generally speaking experiment 3 illustrates that the neural network trained by conductivity fields with a specific variance can be used to estimate conductivity fields with the larger variances and the estimating performance decreases with increasing variances of conductivity fields the neural network which trained by large variance may have a large capacity in estimating macro dispersivity 4 4 features from the cnn in this section results from the above three experiments are summarized and some discussion about the features extracted by the cnn are presented fig 14 displays the comparison of r 2 from the above three experiments generally the cnn trained by conductivity fields with relatively large variances can achieve better performance on estimations of macro dispersivities for conductivity fields with relatively small variances the neural network trained by highly heterogeneous fields seems to have a high ability to extract features of heterogeneity in fig 14 a4 the neural network trained by var0 5 has much better performance than the neural network trained by var0 1 although the neural network trained by var0 5 has worse performance in its own test set however there seems to be an exception from fig 14 a3 the neural network trained by var0 1 has better performance than the neural network trained by var0 3 we think part of the reason may be that the neural network of var0 1 has been better trained than the neural network of var0 3 besides the difference between conductivity fields of var0 1 and var0 3 is relatively small which makes the two trained neural networks have similar abilities to extract features of heterogeneity overall cnn can provide estimations of macro dispersivities with the limited size of training data it is mainly because the macro dispersivity is indeed a macroscopic description of the impact of the conductivity field aquifer structure on solute transport and a functional mapping from the conductivity field to macro dispersivity is expected to exist some insights on how the cnn estimates macro dispersivity form conductivity field and why the cnn has universality in estimating conductivity fields with different degree of heterogeneity are discussed below in the cnn feature maps are obtained by doing convolution on the input conductivity field with filters convolutional weights these filtered results are usually visualized to illustrate what cnn learns sixteen typical filtered results from the first convolutional layer are presented in fig 15 b for estimating macro dispersivity of the conductivity field shown in fig 15 a it should be noted that pixels of these features have been mapped to the range 0 1 with sigmoid function and are multiplied by the mean value of the lnk field the feature maps show how the trained cnn analyzes a conductivity field to make the estimation it can be seen that in fig 15 b the cnn attempts to identify different features of the heterogeneous conductivity field apparently we can see that feature 1 3 10 13 16 mainly capture the distributions of relatively high lnk values of the original lnk field while feature 2 4 7 11 12 14 mainly capture the distributions of relatively low lnk values of the original lnk field other features also reflect some other heterogeneous patterns that are not prominent by capturing the multiple hierarchic features of the heterogeneity in the conductivity field cnn is able to build a functional mapping between macro dispersivities and conductivity fields the ability to extract these features may be the reason why a cnn trained by a specific variance of conductivity fields can be used to estimate fields of the other variances because cnn has already learned how to extract different heterogeneous patterns it should be noted that the features presented in fig 15 are only for a qualitative visualization of how the neural network learns the heterogeneity from the conductivity fields in practice mathematically definition of the learning process the parameters e g convolutional weights biases of the cnn are learned by minimizing the estimation error of the training datasets with back propagation and preventing overfitting by imposing sparsity 5 conclusion building a relationship between macro dispersivity and conductivity field is of significant practical importance to characterize the transport behavior in this work a direct and efficient map between macro dispersivity and conductivity field is built by the convolutional neural network based on synthetic field datasets the effectiveness of cnn is validated through various training and test datasets the universality of cnn is also investigated that is to say whether or not the cnn trained by a specific group of conductivity fields can be used to estimate the other types of conductivity fields the following conclusions can be drawn from these experiments 1 the estimating performance of cnn generally drops with increasing variances of conductivity fields increasing heterogeneity for the given size of training datasets 4000 fields and data points 140 140 there are many factors that may influence the performance of cnn based model such as cnn architecture input data points and dataset size the effects of these factors need further research 2 the cnn trained by conductivity fields with a specific variance has universality in estimating macro dispersivity to a certain extent because a well trained cnn will have the capacity to extract different patterns of heterogeneity consequently the trained cnn can extract some standard heterogeneous features of conductivity fields for estimating macro dispersivities 3 furthermore the universality of the trained cnn decreases with the increasing disparity between variances of conductivity fields in training set and test set and the cnn trained by conductivity fields with relatively large variances can have stronger universality of estimating the reason may be that the neural networks trained by conductivity fields with large variances high heterogeneity can extract more heterogeneous patterns than the neural network trained by conductivity fields with small variances low heterogeneity since conductivity fields with large variances have more complicated spatial heterogeneity than conductivity fields with small variances this guess also needs to be further verified 4 in general the deep neural network is a very promising approach in building direct mapping between complicated subsurface structure and solute transport behavior as the first attempt at interpreting macro dispersivity from hydraulic conductivity field with deep neural network there still exist some shortcomings or questions that need to be solved in following work the conductivity fields used in this work are synthetic and do not correspond to any real world data hence at this time we cannot project the performance of cnn with real conductivity fields this work is limited to weak heterogeneity and the macro dispersivity studied in this work varied by only one order of magnitude because of the synthetic nature of the fields real aquifers have macro dispersivities covering several orders of magnitudes moreover the requisite size of the training datasets is reasonably large getting such a large size of the dataset in real world problems is very difficult as a data driven model the estimation of cnn is limited to the information provided by the training dataset these deficiencies may be improved by adding physical information into the neural network raissi et al 2017 2019 or using more complicated neural networks the main goal of this work is on the proof of concept for adopting image recognition and specifically cnn in estimating macro dispersivity of heterogeneous formation the deep neural network should have a high potential in fast estimating macro dispersivity from conductivity field for engineering applications however we must note that there have also been some advancements in the computational performance of physically driven models for example rizzo et al 2019 have developed the parallel random walk particle tracking code par 2 an efficient gpu parallelized particle tracking code the gpu accelerated solute transport simulator can drastically reduce the total simulation time of the random walk particle tracking method over all the deep learning method provides another way to interpret macro dispersivity directly from the information of subsurface structure and has the potential of leading to high impact developments declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements editors and reviewers are greatly acknowledged for providing extremely helpful comments this study was supported by the national natural science foundation of china grant 51861125202 and 51629901 the numerical calculations in this paper have been done on the supercomputing system in the supercomputing center of wuhan university the code and data used in this work will be made available at https github com chinazzk cnn for macrodispersivity upon publication of this manuscript appendix a choosing the number of particles in order to choose an appropriate number of particles several rw3d simulations are carried out the setup of transport simulation is the same as section 2 2 by changing the number of particles for trace source six trace tests are conducted the calculated apparent dispersivities from control planes are displayed in fig a 16 it can be seen that 1000 particles are enough to give accurate estimations of apparent macro dispersivities appendix b optimization of the network hyperparameters b1 architecture of cnn the basic structure of cnn is from a tutorial of pytorch in github https github com yunjey pytorch tutorial the original basic cnn which has convolutional kernel 5 5 is designed for mnist dataset and does not work for the presented problem the dataset var0 1 is used here to evaluate different neural network hyperparameters the training set has 4000 samples and the validation set has 1000 samples a lot of training experiments are conducted to modify the architecture of cnn for applying to the problem considered the network hyperparameters are then empirically optimized by following the general architecture of alexnet firstly we experiment with different sizes of the convolutional kernel 3 3 5 5 and 7 7 in the basic cnn the convolutional kernel 7 7 has the best performance and then three architectures of cnn which have a different number of convolutional layers are designed for the problem fig b 17 the kernel size of last convolutional layer in cnn 5lays is 5 5 due to the small size of the feature map from the fourth convolutional layer the three fully connected layers all have the same number of neurons the fully connected layers in cnn 3lays has 3136 neurons the fully connected layers in cnn 4lays and cnn 5lays has 512 and 256 neurons respectively the number of neurons in fully connected layer is just the number of neurons in the last convolutional layer for each cnn the loss function is taken as the regularized mse and the validation metric as the r 2 score in eq 11 adam optimizer is used for training 200 epochs with learning rate 0 001 and a plateau scheduler on the test rmse in eq 10 the batch size is 64 the weight decay is set at 0 0005 the three networks displayed in fig b 17 are validated on the validation set the variation of rmse and r 2 scores with epoch are displayed in fig b 18 in fig b 18 cnn 5lays has the best performance on the validation set besides we have also compared the cnn 5lays with an 18 layer resnet he et al 2016 the cnn 5lays has also illustrated a better performance on the validation set estimation and learning curve for the selected network cnn 5lays are presented in section 4 1 b2 hyperparameters for training process after the selection of cnn architecture we have tried many configurations of initial learning rate weight decay and batch size at last the learning rate is set as 0 001 weight decay is set as 0 0005 and batch size is set as 64 by trial and error this is the best configuration that we can find in fig b 19 the different performances of three learning rates are displayed it can be seen that the learning rate 0 001 has the best estimation performance on the validation dataset in fig b 20 the different performances of three batch sizes are displayed the impact of batch size on estimation performance is minimal supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103545 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
504,building a quantitative relation between the spatial heterogeneity of the hydraulic conductivity fields and the macroscale behavior of solute transport is fundamental for groundwater environment problem in this work the deep learning technique is explored to build the functional mapping between the hydraulic conductivity field and the longitudinal macro dispersivity we examine the capability of the deep neural network in estimating macro dispersivities of conductivity fields with different variances the universality of the trained deep neural network is investigated comparisons of the neural network results and the reference values macro dispersivities from transport simulation suggest the promising potential of deep learning technique in porous media with moderate heterogeneity for a given size of training datasets the deep neural network produces better macro dispersivity estimation for the conductivity field with smaller variance the trained neural network by conductivity fields with larger variance has stronger universality for macro dispersivity estimation this study demonstrates that deep neural network can be an effective alternative for estimating macroscale behavior of solute transport by directly interpreting hydraulic conductivity fields keywords macrodispersivity convolutional neural work hydraulic conductivity field contaminant transport heterogeneity groundwater 1 introduction describing solute transport in porous media is fundamental for many groundwater contamination problems de barros et al 2016 han et al 2016 guo et al 2019 the dispersive transport models are the widely used models at various scales therefore estimating the dispersivity at corresponding scale is essential for any efforts in predicting the spreading of a contaminant plume at field scale the structure of natural porous media usually exhibits spatial variability which has a profound impact on solute transport fiori et al 2016 cvetkovic et al 2016 for applying the advection dispersion models under field conditions hydro geologists have proven that the magnitude of field scale dispersivity macro dispersivity can be several orders of magnitude higher than lab scale value for the same material fiori et al 2017 this increase mainly attributes to the spatial variability of aquifer structure which can be generally described by the spatial distribution of the hydraulic conductivity considering the heterogeneous distribution of hydraulic conductivity as a random field and relating flow and transport to its statistical moments has been one of the primary goals of the field of stochastic modelling e g dagan 1989 gelhar 1993 rubin 2003 a fundamental issue addressed by these works is how macro dispersivity can be related to the statistical properties of the hydraulic conductivity field e g rehfeldt et al 1992 fiori et al 2015a zech et al 2015 however the general applicability of the stochastic approach is sometimes questionable due to several foundational assumptions and the first and second order spatial statistics cannot provide sufficient information on estimating of macro dispersivity e g zinn and harvey 2003 zheng and gorelick 2003 zheng et al 2011 molinari et al 2015 sanchez vila and fernàndez garcia 2016 the conductivity fields with the same first two moments may produce very different solute spreading because of the spatial patterns that are not characterized by these statistics e g zinn and harvey 2003 bianchi and pedretti 2018 the concepts of connectivity and geological entropy then emerge as other attempts to characterize the transport behavior from the heterogeneous conductivity fields e g renard and allard 2013 rizzo and de barros 2017 fiori 2014 fiori and jankovic 2012 freixas et al 2017 fernàndez garcia et al 2010 bianchi and pedretti 2017 2018 in short researchers have made great efforts to predict solute transport behavior only from a characteristic description of the conductivity field despite the helpfulness of these works in understanding the correlation between the heterogeneity of conductivity field and the transport behavior a direct and efficient functional mapping between the conductivity field and the transport behavior for predictive purposes remains to be solved deep learning methods are representation learning methods which can be fed with raw data and automatically discover the representations needed for classification the convolutional neural network cnn which is a particular network type of deep learning is designed to process data with multiple arrays and has dominated the field of image classification and achieved many practical success lecun et al 2015 researchers have adopted cnn to solve various problems in water science shen 2018 e g extracting flow features from flow fields ströfer et al 2019 serving as a surrogate model for uncertainty quantification of partial differential equations zhu and zabaras 2018 mo et al 2018 reconstructing porous media mosser et al 2017 cang et al 2017 wang et al 2018 and reducing dimensionality of complex geological models laloy et al 2017 2018 chan and elsheikh 2017 particularly cnn has also been used to predict the effective properties of heterogeneous materials directly srisutthiyakorn 2016 cang et al 2018 wu et al 2018 vasilyeva and tyrylgin 2018 srisutthiyakorn 2016 demonstrated the possibility of predicting permeability from rock images by cnn wu et al 2018 improved the performance of cnn in predicting permeability by involving physical information vasilyeva and tyrylgin 2018 constructed a cnn to accelerate the calculation of elasticity tensors of random poroelastic media since the macro dispersivity is supposed to be formation specific zech et al 2015 it should be possible to develop a surrogate model that directly map a conductivity field to corresponding macro dispersivity by cnn regarding conductivity fields as abstract images of aquifers and resembling the task of image classification lecun et al 1995 krizhevsky et al 2012 cnn takes a conductivity field as input and gives the classification label macro dispersivity as output by recognizing the features in the field in this work a cnn of eight layers is set up to learn a map between stochastic conductivity field and longitudinal macro dispersivity based on synthetic two dimensional conductivity fields a direct and efficient functional map between macro dispersivity and conductivity field is built such surrogate model has the potential to enable fast estimation of macro dispersivities of aquifers without performing direct simulations of physical models which are usually time consuming and need many input parameters to our knowledge this is the first study to interpret macro dispersivity from conductivity field by deep neural network directly two dimensional isotropic multi gaussian fields are generated with a sequential gaussian simulation bellin and rubin 1996 reference macro dispersivities are calculated by conducting numerical trace tests the performance of the proposed neural network architecture is validated through various training and test datasets for a specific size of training datasets the estimating capability of the neural network declines with increasing variances namely increasing heterogeneity of conductivity fields notably the universality of the deep neural network in estimating macro dispersivities of conductivity fields with different degree of heterogeneity is also explored assessments of the proposed neural network architecture show that directly estimating macro dispersivity from distribution of hydraulic conductivity is promising the rest of the paper is organized as follows section 2 recapitulates the entire computation framework the cnn architecture and generation of training dataset results of numerical tests and discussions are presented in section 3 finally section 4 concludes the paper 2 methodology since the conductivity fields can be regarded as abstract images of aquifers and the macro dispersivity which is an effective coefficient to characterize field scale dispersion process in dispersive subsurface mass transport models can be considered as an inherent property of an aquifer using the deep learning methods to estimate macro dispersivity from conductivity field is worth a try in this section a framework to train a convolutional neural network is set up and the detailed steps are presented 2 1 sketch of training framework the objective of the training framework presented here is to train a deep learning model for fast estimation of macro dispersivity from the spatial distribution of the hydraulic conductivity this training framework consists of the following steps as illustrated in fig 1 1 generating training datasets two dimensional random fields of the hydraulic conductivity are generated direct simulations with the random walk particle tracking method salamon et al 2006b are then used to compute the macro dispersivities of the generated conductivity fields the field dispersivity pairs consist of the training datasets for the deep neural network model the details of generating the training datasets are presented in section 2 2 2 training the cnn the training datasets from the previous step are then used to train our cnn that takes a heterogeneous conductivity field as input and gives macro dispersivity as output section 2 3 specifies the cnn architecture and training process 3 estimating macro dispersivities the trained cnn is then used to estimate macro dispersivities of new conductivity fields that are not in the training datasets it should be noted that this work is established on two dimensional synthetic conductivity fields and the macro dispersivity here is referred to as longitudinal macro dispersivity three dimensional synthetic or real world conductivity field will be the goal of future works 2 2 building of training datasets simulations of uniform flow trace tests are conducted on the conductivity fields to estimate the corresponding macro dispersivities square bi dimensional confined aquifers with the uniform mean flow in the x direction is considered and the flow is driven by a mean hydraulic gradient equal to j 0 0176 the computational domain used in the study is illustrated in fig 2 the domain is discretized into square cells each of dimension 0 25ilnk there are 140 grid cells in each coordinate direction corresponding to an overall grid dimension of 35ilnk for boundaries parallel to the mean flow boundaries conditions are no flux the rest boundaries are the constant head we have to emphasize that the grid block number per correlation scale is fixed to 4 in this work only training sets of different variances are considered the effects of training sets of different correlation lengths are not investigated here and there may be a distribution shift that links to the ilnk change liu and ziebart 2014 this issue will be investigated in the future study 2 2 1 generating hydraulic conductivity fields the synthetic aquifers are characterized by spatially varying hydraulic conductivity k x and the ln k fields follow a gaussian random function the isotropic exponential covariance model is considered 1 c l n k r σ l n k 2 exp r i l n k where r is the separation vector between two points of the aquifers ln k denotes the log hydraulic conductivity σ l n k 2 is the variance of ln k and ilnk is the correlation length the log conductivity fields are generated through hydro gen bellin and rubin 1996 the geometric mean of the ln k fields is set equal to 116 7 m day k g 116 7 m day which is a reasonable value for a sandy aquifer 2 2 2 generating velocity fields before the transport simulations velocity fields must be developed at first groundwater flow is described by the flow mass conservation equation and darcy s law under steady state flow conditions the flow equation can be formally expressed as 2 k x h x 0 where h x is the hydraulic head and k x is the spatially variable hydraulic conductivity a finite difference groundwater flow model modflow harbaugh et al 2000 is used to solve the flow problem and the velocity field corresponding to each conductivity field is then determined with the mean hydraulic gradient the darcy s velocity distribution is computed by 3 q x k x h x 2 2 3 transport simulation with particle tracking using the solute mass conservation equation and fick s law at the local scale solute transport through porous media is locally governed by the advection dispersion equation and can be written as 4 n c x t t q x c x t n d c x t where q is the darcy velocity n is the porosity c is the solute concentration and d is the local hydro dynamic dispersion tensor solute transport simulations are conducted based on velocity fields from eq 2 through a random walk particle tracking transport code rw3d fernàndez garcia et al 2005 salamon et al 2006b 2006a henri and fernàndez garcia 2014 2015 the porosity is assumed to be spatially homogeneous with a value of 0 35 the tracer plume migration is simulated by partitioning the tracer mass into a large number of representative mass particles advection is simulated by moving particles within the velocity field whereas dispersion is emulated by brownian motion this lagrangian method is free of numerical dispersion more details about the numerical algorithm can be found in the work of fernàndez garcia et al 2005 and salamon et al 2006b at the beginning of simulations a number of particles 1000 are uniformly distributed in a vertical line perpendicular to the mean flow direction this line is located 5ilnk away from the up gradient boundary to relieve boundary effects rubin and dagan 1988 1989 the source size is 25ilnk in the transverse direction to the mean flow and is centered with the transverse direction for estimating longitudinal macro dispersivities a control plane transverse to the mean flow direction is placed 3ilnk away from the down gradient boundary it should be noted that 1000 particles are enough to offer an accurate estimate of macro dispersivity in this work the validation of the chosen particle number is displayed in appendix a 2 2 4 estimating of macro dispersivities from temporal moments the first arrival time passing through the control plane is tracked until particles exited the lower constant head boundary then the estimation of associated temporal moments of the breakthrough curve btc can be conducted without having to evaluate the actual shape of the btc the nth absolute temporal moment mn x is expressed as the expected value of the arrival time of particles at the control plane to the nth power fernàndez garcia et al 2005 fadili et al 1999 shapiro and cvetkovic 1988 5 m n x 1 m total 0 t n q c f x t d t 1 k 1 n p m p k k 1 n p m p k t p k x n where m total is the total injected tracer mass q is the total water flux passing through the control plane x is the mean flow direction coordinate c f is the flux concentration of tracer passing through the control plane m p k is the mass of a particle t p k is the first arrival passage time of the kth particle and np is the total number of particles arrival at the x control plane the nth central temporal moment m t n x is then calculated by the relationship between central and absolute temporal moments fernàndez garcia et al 2005 kendall and stuart 1977 6 m t n x 1 m total 0 t m 1 x n q c f x t d t r 0 n n r m n r x m 1 x r macro dispersivities from temporal moments are calculated as fernàndez garcia and gómez hernández 2007 fernàndez garcia et al 2005 aris 1958 goltz and roberts 1987 7 a 11 x p x p 2 m t 2 x p m 1 x p 2 where x p is distance between the tracer source and the control plane a 11 is the longitudinal macro dispersivity macro dispersivity from eq 7 is the apparent longitudinal macro dispersivity that corresponds to a specific hydraulic conductivity field the apparent longitudinal macro dispersivity is viewed as equivalent value in homogeneous porous media applying it to the classical advection dispersion equation will lead to the same first two temporal moments of the btc as observed in the simulations for heterogeneous porous media fernàndez garcia et al 2005 finally the estimated macro dispersivities and the corresponding hydraulic conductivity fields constitute the training and validation datasets other details on the evaluation of macro dispersivities from temporal moments can be found in the work of fernàndez garcia et al 2005 and fernàndez garcia and gómez hernández 2007 furthermore it should be noted that the calculated macro dispersivities did not reach its asymptotic value due to the limitation of the computation domain 2 2 5 validation of physical model simulation fiori et al 2015b have shown that for purely advective transport the mean advective velocity u calculated from the results of the transport simulation is equal to the mean value of the component of the eulerian velocity field parallel to the mean flow direction u mean the same as the work of bianchi and pedretti 2018 and jankovic et al 2017 the accuracy of the numerical simulation is evaluated by checking the equality between the two mean velocities for each simulation the mean advective velocity u is calculated by 8 u l t where l is the distance between the injection source and the control plane t is the average arrival time of the particles the components of the eulerian velocity field at the center of each grid block are obtained by averaging the specific discharge across the numerical grid block interfaces under homogeneous effective porosity the fluxes are derived from the cell by cell balance of the groundwater flow model modflow for this work 50 simulations with different conductivity fields are conducted to assess the equality of u and u mean the rations between u and u mean are plotted in fig 3 and are all close to one besides the effective macro dispersivities from numerical simulations are compared with analytical models of dagan 1984 and gelhar and axness 1983 dagan 1984 and gelhar and axness 1983 derived analytical models of ensemble macrodispersivity for moderate heterogeneity σ l n k 2 1 the same setup of simulation as the work of fernàndez garcia and gómez hernández 2007 is used at first the effective macro dispersivities of 50 realizations are compared with dagan 1984 analytical model the result is displayed in fig 4 then another 50 simulations with the setup of section 2 2 are conducted the effective and apparent macro dispersivities of 50 realizations are compared with dagan 1984 and gelhar and axness 1983 analytical models in fig 5 the effective macro dispersivity is the averaged of apparent macro dispersivities from 50 realizations from both figures it can be seen that the effective macro dispersivity generally agrees well with the model of dagan but has not reached the asymptotic limit of gelhar overall these results show that the physical model simulations are accurate 2 3 architecture of the cnn neural networks are a set of algorithms that are composed of some highly interconnected processing elements neurons working through a hierarchy of layers schmidhuber 2015 a regular cnn consists of some convolutional layers pooling layers and fully connected layers fig 1 a convolutional layer is composed of some convolution kernels which are used to compute the feature maps when a 2d image is inputted a convolutional layer g is obtained by employing some filters τ q r k i k j where q is the number of filters and k is kernel size to evolve an input pixel or point value h m n to get the feature value g m n q h m n at location m n as 9 g m n q h m n f i 1 k i j 1 k j τ i j q h m i n j this results in a convolutional lay g consisting of q feature maps two other important parameters for the convolutional layer are the stride and padding stride determines the distance between two successive moves of the filter and padding specifies the padding of the borders of the input image with zeros for size preservation pooling layers combine the outputs of neuron clusters from the convolutional layer into a single neuron and so the learnable parameters are reduced in the problem of estimating macro dispersivity from the conductivity field as concerned in this study the convolutional layers extract features from an input conductivity field by using small squares of the input field the feature maps of the conductivity field encompass the correlation structure of conductivity field as well as the higher order moments the pooling layers reduce the dimensionality of each feature map but retain the vital information for estimating the macro dispersivity the fully connected layers flatten the feature map matrixes and combine these features to create a model with macro dispersivity as the final output the cnn architecture proposed here is implemented in the machine learning framework pytorch paszke et al 2019 and is based on the alexnet krizhevsky et al 2012 the cnn here includes five convolutional layers each followed by a batch normalization operation krizhevsky et al 2012 a relu activation function and a pooling layer and three consecutive fully connected layers fig 1 the numbers of layers channels and neurons are empirically selected to ensure enough complexity of the neural network and minimize the mean squared error of the estimated macro dispersivities for the training datasets the first convolutional layer has 16 feature maps the first four convolutional layers all have a convolutional kernel of size 7 7 to extract different features from the corresponding input the last convolutional layer has a convolutional kernel of size 5 5 in the pooling layers the max pooling function and a kernel size of 2 2 with strides 2 2 are adopted the three fully connected layers all have 256 neurons the details of the deep neural network architecture are displayed in table 1 the other details of network architecture and hyperparameter selection of the particular problem considered are presented in appendix b the computational cost is of the order of seconds for estimating the macro dispersivities of 1000 conductivity fields with 140 140 data points by using the trained cnn and is three orders of magnitude lower than the simulations of above mentioned physical models 2 4 performance criteria the root square error rmse and the coefficient of determination r 2 are used to evaluate the performance of our deep neural network the results of simulations with random walk particle tracking method are taken as reference values for rmse a value of 0 means a perfect fit between the reference values and the estimated values the smaller rmse value stands for better performance the definition of rmse is shown below 10 r m s e i 1 n a i sim a i cnn 2 n where a i sim is the ith simulated macro dispersivity in validation test datasets a i cnn is the ith estimated macro dispersivity by the cnn the definition of the coefficient of determination is presented as 11 r 2 1 i 1 n a i sim a cnn mean 2 i 1 n a i sim a sim mean 2 where a cnn mean represents the average of the estimating macro dispersivities by the cnn a sim mean is the average of the reference macro dispersivities in the test dataset the r 2 values range between 0 and 1 and the value of 1 indicates a perfect correlation 3 synthetic experiments setting synthetic experiments are conducted to demonstrate the capability of the neural network to estimate the macro dispersivity by considering different variances of conductivity fields 3 1 datasets we evaluated the cnn using datasets of conductivity fields produced with increasing variances of 0 1 0 2 0 3 and 0 5 called var0 1 var0 2 var0 3 and var0 5 respectively the discretized field realizations vary highly with increasing variance and the intrinsic dimensionality of the random field is 19600 for 140 140 grids which are the cases presented here these field characteristics create a significant challenge for data driven models to capture our current data includes three sets the training validation and test sets each training set contains 4000 training data and the validation and test set each consist of 1000 input conductivity fields all datasets are organized in the form of images the training set is limited to 4000 samples because each simulation run of the random walk has high computational cost a sample costs about 10 min cpu intel xeon e5 2630 v3 2 4ghz 16 cores ram 96gb ddr4 2133mhz ecc a training set of 4000 samples costs almost a month also the data on field scale dispersion in real aquifers is even more limited zech et al 2015 gelhar et al 1992 generating more input realizations of hydraulic conductivity using only the training dataset is another critical problem shen 2018 the interest of this work is to validate the potential possibility of building the field to dispersivity mapping with the limited training dataset 3 2 training the cnn is trained with adam kingma and ba 2014 a variant of stochastic gradient descent with the loss function being l 2 regularized mean square error mse which is implemented as weight decay in pytorch the initial learning rate is 0 001 the weight decay is 0 0005 the batch size is 64 a learning rate scheduler which drops 10 times on plateau of the rooted mse is also used the model is trained 200 epochs the estimating performance is evaluated with validation and test fields that are not included in the training datasets the macro dispersivities estimated from the simulations of the random walk particle tracking method are taken as reference values 3 3 experiments at first cnn is trained with the four datasets of different variances respectively the estimation performance of these trained neural networks is tested with the corresponding test set the estimating capability of the proposed deep neural network is demonstrated and the influence of variance of conductivity field on estimation accuracy of macro dispersivity is revealed next the performance of the neural networks trained with conductivity fields of relatively large variance training sets of var0 5 var0 3 and var0 2 are tested with the test sets of conductivity fields of relatively small variance test sets of var0 1 var0 2 and var0 3 it is found that the neural networks trained by large variances can estimate conductivity fields with small variances finally the estimating capability and universality of the trained neural networks training set of var0 1 var0 2 and var0 3 are verified by using test datasets with large variances test sets of var0 2 var0 3 and var0 5 the universality of the neural network to estimate conductivity fields with relatively large variances is validated detailed setup about the three experiments is presented in table 2 4 results and discussion 4 1 experiment 1 training cnn with datasets of different variances fig 6 displays the training process with epochs for the training sets of different variances it is seen that the validation rmse increases with increasing variance and the validation r 2 decreases with increasing variance however the cnn trained by var0 5 still has a decent r 2 value 0 73 on the validation set these trained neural networks are then tested with the test set of the same variance as the training set all test sets are consists of 1000 samples of conductivity fields the test results are illustrated in fig 7 it should be noted that the values of rmse and r 2 are calculated from the test set of 1000 samples but only 50 samples of conductivity field which are randomly sampled from test set are displayed from fig 7 it is found that the neural networks trained with different variances all have achieved accurate estimation of macro dispersivity in terms of rmse and r 2 for all the four cases most macro dispersivities estimated from the neural networks fall in the error range of 20 marked as baby blue shaded regions in figures the cnn trained by the dataset var0 1 has smallest rmse 0 05 and largest r 2 0 869 and the cnn trained by dataset var0 5 has largest rmse 0 375 and smallest r 2 0 72 from the two criteria it can be seen that the estimation performance generally gets worse with the increasing variance of dataset however for var0 1 and var0 2 the decreasing of estimation performance is not obvious on the whole all the four trained cnns have a satisfactory estimation of macro dispersivity with the limited size of the training set from this experiment it is found that cnn can achieve acceptable estimations of macro dispersivities for conductivity fields with weak heterogeneity σ lnk 2 0 5 and increasing variances of conductivity fields would make a functional mapping difficult for the cnn with the limited size of training datasets larger training datasets may introduce better estimation performance 4 2 experiment 2 test trained cnns with test sets of small variances in experiment 2 we will use the trained cnns in experiment 1 to estimate macro dispersivities of conductivity fields with smaller variances than the training set for the neural network trained by conductivity fields of σ l n k 2 0 2 an only test set of var0 1 are used fig 8 in fig 8 nearly all estimated macro dispersivities are within 20 error range of the reference values the large value of r 2 0 831 confirms the results and the rmse is close to the results of fig 7 a1 the cnn trained by var0 2 can give accurate estimates of the test set of var0 1 as same as the test set of var0 2 fig 7 b1 fig 9 shows the results of using cnn trained by var0 3 to estimate macro dispersivities for test sets of var0 1 and var0 2 for the test set of var0 1 fig 9 a1 b1 underestimation of macro dispersivities can be observed for most data points nevertheless there are still at least half of the data points are within the error range the r 2 0 496 validates the estimation performance for the test set of var0 2 fig 9 a2 b2 the cnn has achieved quite good estimation performance most estimated macro dispersivities are within the error range of the reference values and the r 2 is 0 829 the rmse 0 114 is almost equal to the result 0 110 from the neural network trained by var0 2 these results show that the cnn trained by var0 3 can estimate test set var0 2 and var0 1 however the estimation performance drops with the increasing disparity of variances the results of using cnn trained by var0 5 to estimate macro dispersivities for the test sets of var0 1 var0 2 and var0 3 are displayed in fig 10 for the test fields of var0 1 fig 10 a1 b1 more than 50 of estimations fall within the 20 error range while for the test set fields of var0 2 and var0 3 most estimations fall within the 20 error range the r 2 values for the test sets generally increase with increasing variances of conductivity fields specifically for the test set of var0 3 r 2 0 74 the cnn trained by var0 5 can provide almost the same estimation performance as the test set of var0 5 r 2 0 72 similar to the results of fig 9 the estimation performance drops with the increasing difference of variances in summary experiment 2 firstly demonstrates that the cnn trained by fields with a specific variance has some ability to estimate conductivity fields with the other smaller variances and the estimating performance decreases with increasing differences of variances secondly the neural network trained by large variance has a stronger capacity in estimating macro dispersivities of conductivity fields with small variances more specifically it seems that the cnn trained by var0 5 has better universality than the cnn trained by var0 3 because the cnn trained by var0 5 works much better in estimating test set of var0 3 than using the cnn trained by var0 3 to estimate test set of var0 1 although the disparity between variances of train set and test set are 0 2 for the two cases furthermore the cnn trained by var0 5 still has better performance in estimating test set of var0 2 than estimating test set of var0 1 with the cnn trained by var0 3 4 3 experiment 3 test trained cnns with test sets of large variances in experiment 3 we will use the trained cnns in experiment 1 to estimate macro dispersivities of conductivity fields with larger variances than the training set fig 11 illustrates the results of estimating macro dispersivities of conductivity fields with σ f 2 0 2 0 3 and 0 5 by using the cnn trained by train set of σ f 2 0 1 for the test set of var0 2 fig 11 a1 b1 almost all data points fall within the error range the neural network has achieved fairly good estimations r 2 0 777 with increasing variance of test set fig 11 a2 b2 underestimations of macro dispersivities happen for large reference values for the test set of var0 5 fig 11 a3 b3 the underestimations of macro dispersivities for large reference values become prominent correspondingly the values of r 2 decrease with increasing variances in fig 12 the neural network trained by var0 2 is used to estimate the test set of var0 3 and var0 5 similar to fig 11 the estimation performance of the neural network decreases with increasing variance comparing fig 12 with fig 11 a2 a3 b2 b3 it is found that the neural network trained by var0 2 has stronger estimating ability than the neural network trained by var0 1 for example the neural network trained by var0 1 generally underestimates the macro dispersivities for the test set of var0 5 r 2 0 117 while the neural network trained by var0 2 can get more satisfactory estimations for the test set of var0 5 r 2 0 584 in fig 13 the neural network trained by var0 3 is used to estimate macro dispersivities for the test set of var0 5 the rmse and r 2 are almost consistent with the results of fig 7 a4 b4 in which the macro dispersivities for the test set of var0 5 are estimated by the neural network trained with the train set of var0 5 rmse 0 375 r 2 0 72 the estimating performance of the neural network trained by var0 3 is better than the neural networks trained by var0 2 and var0 1 comparing fig 13 with fig 11 a3 b3 and fig 12 a2 b2 the r 2 of macro dispersivities for test set of var0 5 from the neural network trained by var0 3 is the largest r 2 0 703 generally speaking experiment 3 illustrates that the neural network trained by conductivity fields with a specific variance can be used to estimate conductivity fields with the larger variances and the estimating performance decreases with increasing variances of conductivity fields the neural network which trained by large variance may have a large capacity in estimating macro dispersivity 4 4 features from the cnn in this section results from the above three experiments are summarized and some discussion about the features extracted by the cnn are presented fig 14 displays the comparison of r 2 from the above three experiments generally the cnn trained by conductivity fields with relatively large variances can achieve better performance on estimations of macro dispersivities for conductivity fields with relatively small variances the neural network trained by highly heterogeneous fields seems to have a high ability to extract features of heterogeneity in fig 14 a4 the neural network trained by var0 5 has much better performance than the neural network trained by var0 1 although the neural network trained by var0 5 has worse performance in its own test set however there seems to be an exception from fig 14 a3 the neural network trained by var0 1 has better performance than the neural network trained by var0 3 we think part of the reason may be that the neural network of var0 1 has been better trained than the neural network of var0 3 besides the difference between conductivity fields of var0 1 and var0 3 is relatively small which makes the two trained neural networks have similar abilities to extract features of heterogeneity overall cnn can provide estimations of macro dispersivities with the limited size of training data it is mainly because the macro dispersivity is indeed a macroscopic description of the impact of the conductivity field aquifer structure on solute transport and a functional mapping from the conductivity field to macro dispersivity is expected to exist some insights on how the cnn estimates macro dispersivity form conductivity field and why the cnn has universality in estimating conductivity fields with different degree of heterogeneity are discussed below in the cnn feature maps are obtained by doing convolution on the input conductivity field with filters convolutional weights these filtered results are usually visualized to illustrate what cnn learns sixteen typical filtered results from the first convolutional layer are presented in fig 15 b for estimating macro dispersivity of the conductivity field shown in fig 15 a it should be noted that pixels of these features have been mapped to the range 0 1 with sigmoid function and are multiplied by the mean value of the lnk field the feature maps show how the trained cnn analyzes a conductivity field to make the estimation it can be seen that in fig 15 b the cnn attempts to identify different features of the heterogeneous conductivity field apparently we can see that feature 1 3 10 13 16 mainly capture the distributions of relatively high lnk values of the original lnk field while feature 2 4 7 11 12 14 mainly capture the distributions of relatively low lnk values of the original lnk field other features also reflect some other heterogeneous patterns that are not prominent by capturing the multiple hierarchic features of the heterogeneity in the conductivity field cnn is able to build a functional mapping between macro dispersivities and conductivity fields the ability to extract these features may be the reason why a cnn trained by a specific variance of conductivity fields can be used to estimate fields of the other variances because cnn has already learned how to extract different heterogeneous patterns it should be noted that the features presented in fig 15 are only for a qualitative visualization of how the neural network learns the heterogeneity from the conductivity fields in practice mathematically definition of the learning process the parameters e g convolutional weights biases of the cnn are learned by minimizing the estimation error of the training datasets with back propagation and preventing overfitting by imposing sparsity 5 conclusion building a relationship between macro dispersivity and conductivity field is of significant practical importance to characterize the transport behavior in this work a direct and efficient map between macro dispersivity and conductivity field is built by the convolutional neural network based on synthetic field datasets the effectiveness of cnn is validated through various training and test datasets the universality of cnn is also investigated that is to say whether or not the cnn trained by a specific group of conductivity fields can be used to estimate the other types of conductivity fields the following conclusions can be drawn from these experiments 1 the estimating performance of cnn generally drops with increasing variances of conductivity fields increasing heterogeneity for the given size of training datasets 4000 fields and data points 140 140 there are many factors that may influence the performance of cnn based model such as cnn architecture input data points and dataset size the effects of these factors need further research 2 the cnn trained by conductivity fields with a specific variance has universality in estimating macro dispersivity to a certain extent because a well trained cnn will have the capacity to extract different patterns of heterogeneity consequently the trained cnn can extract some standard heterogeneous features of conductivity fields for estimating macro dispersivities 3 furthermore the universality of the trained cnn decreases with the increasing disparity between variances of conductivity fields in training set and test set and the cnn trained by conductivity fields with relatively large variances can have stronger universality of estimating the reason may be that the neural networks trained by conductivity fields with large variances high heterogeneity can extract more heterogeneous patterns than the neural network trained by conductivity fields with small variances low heterogeneity since conductivity fields with large variances have more complicated spatial heterogeneity than conductivity fields with small variances this guess also needs to be further verified 4 in general the deep neural network is a very promising approach in building direct mapping between complicated subsurface structure and solute transport behavior as the first attempt at interpreting macro dispersivity from hydraulic conductivity field with deep neural network there still exist some shortcomings or questions that need to be solved in following work the conductivity fields used in this work are synthetic and do not correspond to any real world data hence at this time we cannot project the performance of cnn with real conductivity fields this work is limited to weak heterogeneity and the macro dispersivity studied in this work varied by only one order of magnitude because of the synthetic nature of the fields real aquifers have macro dispersivities covering several orders of magnitudes moreover the requisite size of the training datasets is reasonably large getting such a large size of the dataset in real world problems is very difficult as a data driven model the estimation of cnn is limited to the information provided by the training dataset these deficiencies may be improved by adding physical information into the neural network raissi et al 2017 2019 or using more complicated neural networks the main goal of this work is on the proof of concept for adopting image recognition and specifically cnn in estimating macro dispersivity of heterogeneous formation the deep neural network should have a high potential in fast estimating macro dispersivity from conductivity field for engineering applications however we must note that there have also been some advancements in the computational performance of physically driven models for example rizzo et al 2019 have developed the parallel random walk particle tracking code par 2 an efficient gpu parallelized particle tracking code the gpu accelerated solute transport simulator can drastically reduce the total simulation time of the random walk particle tracking method over all the deep learning method provides another way to interpret macro dispersivity directly from the information of subsurface structure and has the potential of leading to high impact developments declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements editors and reviewers are greatly acknowledged for providing extremely helpful comments this study was supported by the national natural science foundation of china grant 51861125202 and 51629901 the numerical calculations in this paper have been done on the supercomputing system in the supercomputing center of wuhan university the code and data used in this work will be made available at https github com chinazzk cnn for macrodispersivity upon publication of this manuscript appendix a choosing the number of particles in order to choose an appropriate number of particles several rw3d simulations are carried out the setup of transport simulation is the same as section 2 2 by changing the number of particles for trace source six trace tests are conducted the calculated apparent dispersivities from control planes are displayed in fig a 16 it can be seen that 1000 particles are enough to give accurate estimations of apparent macro dispersivities appendix b optimization of the network hyperparameters b1 architecture of cnn the basic structure of cnn is from a tutorial of pytorch in github https github com yunjey pytorch tutorial the original basic cnn which has convolutional kernel 5 5 is designed for mnist dataset and does not work for the presented problem the dataset var0 1 is used here to evaluate different neural network hyperparameters the training set has 4000 samples and the validation set has 1000 samples a lot of training experiments are conducted to modify the architecture of cnn for applying to the problem considered the network hyperparameters are then empirically optimized by following the general architecture of alexnet firstly we experiment with different sizes of the convolutional kernel 3 3 5 5 and 7 7 in the basic cnn the convolutional kernel 7 7 has the best performance and then three architectures of cnn which have a different number of convolutional layers are designed for the problem fig b 17 the kernel size of last convolutional layer in cnn 5lays is 5 5 due to the small size of the feature map from the fourth convolutional layer the three fully connected layers all have the same number of neurons the fully connected layers in cnn 3lays has 3136 neurons the fully connected layers in cnn 4lays and cnn 5lays has 512 and 256 neurons respectively the number of neurons in fully connected layer is just the number of neurons in the last convolutional layer for each cnn the loss function is taken as the regularized mse and the validation metric as the r 2 score in eq 11 adam optimizer is used for training 200 epochs with learning rate 0 001 and a plateau scheduler on the test rmse in eq 10 the batch size is 64 the weight decay is set at 0 0005 the three networks displayed in fig b 17 are validated on the validation set the variation of rmse and r 2 scores with epoch are displayed in fig b 18 in fig b 18 cnn 5lays has the best performance on the validation set besides we have also compared the cnn 5lays with an 18 layer resnet he et al 2016 the cnn 5lays has also illustrated a better performance on the validation set estimation and learning curve for the selected network cnn 5lays are presented in section 4 1 b2 hyperparameters for training process after the selection of cnn architecture we have tried many configurations of initial learning rate weight decay and batch size at last the learning rate is set as 0 001 weight decay is set as 0 0005 and batch size is set as 64 by trial and error this is the best configuration that we can find in fig b 19 the different performances of three learning rates are displayed it can be seen that the learning rate 0 001 has the best estimation performance on the validation dataset in fig b 20 the different performances of three batch sizes are displayed the impact of batch size on estimation performance is minimal supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103545 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
