index,text
25565,we present metgen a sub daily multi variable stochastic weather generator implemented as an r library that can be used to perform gap filling and to extend in time meteorological observation series metgen is tailored to provide surrogate series of air temperature relative air humidity global radiation and wind speed needed for surface water stress estimation that requires sub daily resolution multiple gauged stations can be used to increase the calibration data although spatial dependence is not modeled the approach relies on generalized linear models that use among their covariates large scale variables derived from era5 reanalyses metgen aims at preserving key features of the meteorological variables along with inter variable dependencies we illustrate the abilities of metgen using a case study with three stations in central tunisia we consider as alternatives a univariate and a multivariate bias correction techniques along with the un processed large scale variables keywords stochastic weather generator bias correction surface water stress estimation sub daily resolution era5 reanalyses 1 introduction in semi arid areas water is a major limitation factor for agricultural production indeed these areas are characterized by short rainy seasons and strong variability of precipitation events in time and space baccour et al 2012 natural variations in the water cycle affect the availability of water leading to irregularities in agricultural production saadi et al 2018 and constitutes the main driver of agricultural droughts the vegetation health status being generally representative of water availability sheffield and wood 2012 an important issue concerns the detection of surface water stress and the estimation of evapotranspiration et water stress may be deduced from et with energy balance models at satellite overpass time energy balance models compute instantaneous et as the residual term of the land surface energy balance equation once net radiation soil heat flux and sensible heat flux are derived from remotely sensed surface temperature hoedjes et al 2008 norman et al 1995 timmermans et al 2007 such water stress estimates are particularly informative for the detection of incipient plant stress during early stages of drought development compared to estimates derived from other wave lengths microwave or visible and allow to launch early drought alerts otkin et al 2013 energy balance models use as inputs satellite data normalized difference vegetation index albedo and surface temperature and in situ meteorological observations air temperature airt relative air humidity rh global radiation gr and wind speed ws as provided by gauged networks et and water stress estimates computed from the instantaneous surface energy budget constrained by the surface temperature require meteorological observations acquired at the satellite overpass time to ensure precise timing with satellite information in situ meteorological observations must be available at sub daily resolution in this work we use satellite data provided by the latest modis collection http earthexplorer usgs gov that has a 1 km spatial resolution et and water stress are estimated over a region covered by several modis grid cells this region is defined so that it can be considered to be homogeneous in terms of climate and weather as a consequence a single multi variable meteorological series representative of the region is needed nevertheless there may be several gauged stations in the region with different observation periods and different gaps in the observation series moreover it is often the case that the observed meteorological series are available over too short periods of time therefore an important task is to develop a rigorous way to obtain a representative sub daily multi variable meteorological surrogate series in which gaps are filled and that extends in time the original series by exploiting the information provided by all the gauged stations in the region stochastic weather generators swgs are stochastic models based on statistical approaches for simulating at high spatial resolution surrogate meteorological series that are similar to observation series in terms of distributional properties preserving both systematic and random variations ailliot et al 2015 swgs are thus very useful models to perform coherent gap filling and to generate realistic surrogate series over periods for which no observations are available in the aforementioned surface water stress application the sub daily series of the four meteorological variables airt rh gr and ws display both annual and diurnal cycles once these primary systematic variations are accounted for there will likely remain some random variations there are two main strategies in swgs to model systematic and random variability which are applicable in the case of multiple meteorological variables at several gauged stations the weather type approach that breaks down weather into classes or types or states of typical recurring meteorological situations e g clear blue sky cloudy rainy etc and the stochastic regression approach that captures the variability of weather smoothly by using suitable covariates in the weather type approach the underlying assumption is that each time step belongs to one weather type and all the time steps belonging to the same weather type can be modeled with a relatively simple statistical approach in other words the temporal sequence is grouped into blocks each block being associated to one weather type ailliot et al 2015 wilks and wilby 1999 one of the earliest weather type swg proposed by katz 1977 models precipitation as a two state markov chain that corresponds to two simple weather types wet and dry types richardson 1981 builds on the latter model and represents the intensity of precipitation for the wet weather type as an exponential probability distribution the other three variables maximum minimum temperature and solar radiation are modeled with a multivariate normal distribution whose means and standard deviations change according to the wet or dry types in more recent approaches more general weather types may be obtained automatically as an unsupervised classification problem they thus are defined as the classes resulting from the clustering of time steps with each time step characterized by climatic or meteorological features flecher et al 2010 weather types may also be defined indirectly as the states of a latent variable using for instance hidden markov models ailliot et al 2009 the analog approach in swgs can be seen as pushing the weather type strategy to the limit where each time step constitutes a weather type oriani et al 2014 yiou 2014 analog based swgs may be entirely non parametric and they may succeed in reproducing complex patterns between variables and between sites however non parametric analog based swgs are essentially resampling schemes that are unable to simulate values and patterns that differ from those present in the observations this may be a serious drawback when observation periods are not long enough to contain all potential patterns to account for annual cycles weather types may be modeled separately for each season with the difficulty that the definition of seasons might be somewhat arbitrary flecher et al 2010 garavaglia et al 2010 except in the case of rainfall benoit et al 2020 sub daily weather typing for variables such as temperature humidity and solar radiation requires a suitable model of the daily cycle the interpretation of weather types at the sub daily scale may be less intuitive than at the daily scale it is not yet clear how to adapt the weather type strategy in order to account for the presence of diurnal cycles ailliot et al 2015 stochastic regression or equivalently conditional distribution modeling is another widely used strategy that can account for both systematic and random variability in swgs for instance williams 1998 links the two parameters of the gamma distribution that models the intensity of rainfall and the probability of rainfall to information on the rainfall pattern on the preceding day the time of the year etc with a one hidden layer feed forward neural network more generally instead of focusing on estimating the conditional mean as is the case in conventional regression stochastic regression seeks to estimate the full conditional distribution from which simulations can be drawn thereby allowing to reproduce the observed variability covariates that carry temporal and spatial information can be introduced letting the parameters of the conditional distribution vary in time and space as an alternative to neural networks generalized linear models glms have been used within swgs for the past 20 years or so see for instance chandler 2005 verdin et al 2018 and the references therein in glms the conditional distribution belongs to the exponential family that encompasses the gaussian distribution the link between the parameters and the covariates is established with a potentially transformed e g with a logarithm linear regression mccullagh and nelder 1989 routines to implement glms are readily available in standard statistical software e g r r core team 2020 spatial dependence may be accounted for by modeling the dependence structure of the residuals e g with gaussian processes verdin et al 2018 if informative enough covariates are used glm based swgs can reproduce very accurately both systematic and random variability chandler 2020 in particular despite that glm based swgs generally operate at the daily resolution sub daily resolution modeling may be achieved by introducing covariates carrying sub daily information most glm based swgs simulate only one or two meteorological variables very often precipitation and temperature as in verdin et al 2018 one notable exception is chandler 2020 who proposes a simple scheme to model jointly several meteorological variables based on the decomposition of the multivariate density into a product of conditional univariate densities in this work we introduce metgen an swg based on glm hence relying on stochastic regression that extends the approach described in chandler 2020 to the sub daily resolution its implementation is publicly and freely available as an r library https cran r project org package metgen in metgen the scheme proposed in chandler 2020 to model jointly several meteorological variables is adapted to the four meteorological variables airt rh gr and ws required for surface water stress estimation for which inter variable dependencies are rather strong in contrast to chandler 2020 who has proposed a way to model spatial dependence inter site dependence is not explicitly modeled in metgen the proposed swg works in a manner similar to the so called regional approach developed in hydrology hosking and wallis 2005 indeed several stations within the region of interest may be used to calibrate the swg to augment the size of the data set instead of relying on the homogeneity assumption of the regional approach spatial variability when present is modeled with covariates in addition to the covariates proposed in chandler 2015 special covariates are considered to enable the reproduction of diurnal cycles based on pairs of sines and cosines similarly as for annual cycles an important category of covariates used to carry sub daily information albeit at a large scale horizontal resolution of 31 km are the meteorological reanalyses provided by era5 available at hourly resolution hersbach et al 2020 since metgen makes use of reanalyses in its covariates and because there are no other to our knowledge publicly and freely available multi variable sub daily swg we resorted to two statistical downscaling methods as comparative approaches statistical downscaling aims to bridge the gap between low resolution and potentially biased simulations from global climate models and the high resolution series required for impact studies such as observation series from gauged networks ayar et al 2016 maraun et al 2010 although often used to obtain climate change scenarios over future periods statistical downscaling methods may be applied to reanalysis products in order to generate surrogate series over past periods ayar et al 2016 an active area of research in statistical downscaling concerns the so called bias correction methods françois et al 2020 bias correction aims at transforming the low resolution series of meteorological variables such as provided by reanalyses so as to match in terms of distributional properties e g in terms of means the high resolution series such as measured at gauged stations in order to assess whether explicitly accounting for inter variable dependencies is essential we include as comparative approaches a univariate cdf t developed in michelangeli et al 2009 and a multivariate bias correction method mbcn proposed by cannon 2018 both methods provide fast non parametric i e without strong distributional assumptions alternatives to metgen and are implemented as publicly available r libraries the paper is organized as follows section 2 presents our study area the merguellil plain in central tunisia together with the meteorological data provided by gauged stations and derived from era5 reanalyses the multi variable sub daily glm based swg metgen is described in section 3 along with the two aforementioned bias correction methods and their adaptation to enable their application at the sub daily resolution section 4 is dedicated to the comparison of the statistical methods at generating surrogate meteorological series both in terms of the ability to fill gaps in the observation series and in terms of the ability to extend in time the observation series section 5 reports an evaluation of the surrogate meteorological series in terms of surface water stress estimation in section 6 a discussion is presented followed by conclusions and research perspectives in section 7 2 study area and meteorological data 2 1 study area the merguellil plain the study area is part of the downstream plain of the merguellil catchment called the merguellil plain see fig 1 lying in a semi arid region located in central tunisia the catchment is characterized by a relatively mountainous upstream area 1200 km2 and by a downstream alluvial plain 676 km2 the upstream area presents a hilly topography altitude between 200 and 1200 m with a median elevation of 500 m leduc et al 2007 in the plain the landscape is mainly flat and the vegetation is typical of semi arid regions rainfed agriculture olive tree and cereals and summer vegetables melons peppers and tomatoes downstream farms are composed mainly of small cultivated areas molle and wester 2009 the upstream and downstream areas are separated by the el haouareb dam fig 1 which was built in 1989 to protect villages from inundations and to store irrigation water for the plain ben ammar et al 2006 the study area is influenced both by the mediterranean climate dry subhumid and the pre saharan climate arid baccour et al 2012 it is characterized by the inter annual irregularity of precipitation with an average of annual rainfall of about 300 mm per year and by a high evaporative demand of about 1600 mm per year water supply is by far insufficient to meet water demand which is rising steadily the rise is due to the increase in population and industrial development and most importantly to the intensification of agriculture which is the main water consumer around 80 leduc et al 2004 2 2 meteorological observations hourly observation series of the four meteorological variables air temperature airt relative air humidity rh global radiation gr and wind speed ws needed for surface water stress estimation are collected from the three gauged stations ben salem chebika and barrouta located in the merguellil plain see fig 1 the observation period approximate number of observations and approximate percentage of missing values are given in table 1 in addition fig 2 illustrates the observation period and the positions of the gaps in the series the three gauged stations provide similar meteorological information owing to their geographical proximity 7 9 km between ben salem and chebika 11 7 km between ben salem and barrouta and 11 km between chebika and barrouta as shown by the annual and diurnal cycles in fig 3 these stations share similar climatic behaviors with the exception of the wind speed observed at chebika lower wind speed values are caused by the presence of a windbreak in the vicinity of the station in addition inter station pair plots not shown confirmed the strong relationship in the meteorological information provided by the three stations the observation series at these three stations enter in the calibration of the stochastic generator metgen proposed in this work when building the statistical model spatial covariates are selected to account for differences in the distribution of the meteorological variables at each of the station in particular a special covariate is used for the wind to account for the presence of the windbreak see details in 3 1 ben salem is selected as the reference station as it complies best with the meteorological standards according to the wmo guidelines omm 2010 therefore only the simulations of surrogate series corresponding to ben salem station are used in the evaluation and comparison of metgen 2 3 meteorological reanalyses era5 reanalyses combine forecast models and observations through data assimilation schemes thereby providing a multivariate spatially complete and coherent record without gaps of atmospheric land and oceanic climate variables dee et al 2011 hersbach et al 2020 in particular era5 reanalyses are available for a long period in the past from 1950 till now hersbach et al 2020 despite being available at hourly resolution the era5 spatial resolution is low horizontal resolution of 31 km hersbach et al 2020 and thus local scale variability might not sufficiently be accounted for hooker et al 2018 besides the mismatch in spatial resolution several limitations affected the quality of previous reanalyses such as era interim which were improved with respect to most aspects for era5 hersbach et al 2020 the three gauged stations from the merguellil plain lie in the same era5 grid cell whose center is shown in fig 1 era5 reanalyses were extracted at this grid cell and were combined to obtain the six large scale meteorological variables listed in table 2 in most cases the large scale variables correspond to raw reanalysis products there are two exceptions the first one concerns the wind speed that was derived by taking the euclidean norm of the 10 m vertical and horizontal wind components the second exception concerns the relative humidity that was derived based on 2 m temperature and 2 m dewpoint temperature era5 products according to the procedures defined in allen et al 2005 the six large scale variables from table 2 serve as covariates in the statistical methods described in section 3 to obtain surrogate meteorological series for the merguellil plain among these the first four are the large scale counterpart of the meteorological variables needed for the surface water stress application to evaluate the quality of the reanalysis products these four large scale variables are used without further processing as one of the candidate surrogate meteorological series it is expected that the statistical methods should be able to correct departures in terms of distributional properties of the large scale variables therefore other large scale data whether reanalyses or remote sensed could be used instead of era5 3 statistical methods 3 1 metgen a regional multi variable sub daily glm based swg we focus on metgen implementation for the surface water stress application in central tunisia the workflow sequence summarized in fig 4 is the main contribution of this work and can be adapted in principle to any study area and to any other meteorological variables metgen is regional in the sense that the observations from several gauged stations can be used in the calibration to increase the sample size as discussed in 2 2 any spatial variability in terms of distribution is captured through dedicated covariates once calibrated metgen simulates series at all the gauged stations however for our surface water stress application a single series representative of the region is needed to this end we make use of the series corresponding to ben salem as it is our reference station see 2 2 3 1 1 multi variable modeling conditioning variables the first step to implement metgen consists in modeling the inter variable dependencies by means of conditioning variables see fig 4 this follows the proposal of rglimclim chandler 2015 by which a multivariate distribution can be decomposed with the product rule into conditional univariate distributions to determine the order of the decomposition in the product rule and to reduce the number of conditioning variables we rely on the dependence graph shown in fig 5 it has been adapted from the one made in the hydef project https www imperial ac uk media imperial college research centres and groups environmental and water resource engineering ucl15feb2012 pdf to apply rglimclim in the uk more precisely the multivariate distribution of the four meteorological variables needed for the surface water stress application boils down to modeling four conditional univariate distributions one for each meteorological variable and including in the covariates the appropriate conditioning variables 1 p w s x 2 p a i r t w s x 3 p r h a i r t w s x 4 p g r a i r t x where x are additional covariates to be described in 3 1 3 the choice of the conditional distribution model the selection of covariates and the calibration can be performed separately for each meteorological variable the simulation of the multi variable surrogate series proceeds following the order dictated by the dependence graph in fig 5 wind speed is simulated first then air temperature is simulated including among the covariates the series simulated for wind speed relative humidity is simulated afterwards with the previously simulated series for air temperature and wind speed included in the covariates and finally global radiation is simulated conditionally on the series simulated for air temperature 3 1 2 conditional univariate distribution models generalized linear models glms in the second step of metgen potential conditional univariate distribution models which are from the generalized linear model glm family for each meteorological variable must be defined see fig 4 at present three possible choices of probability distributions for the glms are available in metgen the gaussian distribution with constant homoscedastic or non constant variance heteroscedastic and the gamma distribution in the glms the parameters of the probability distributions may vary according to covariates in metgen we made the following choices to link the covariates to the probability distribution parameters let x μ and x σ be two covariate vectors let β μ and β σ be regression coefficient vectors of the same length as x μ and x σ respectively and let μ 0 σ 0 0 and ν 0 0 be three constants then the parameters of the conditional distributions are provided as follows for each of the three possible choices 5 homoscedastic gaussian μ x μ x μ β μ μ 0 location param σ 0 scale param 6 heteroscedastic gaussian μ x μ x μ β μ μ 0 location param σ x σ exp x σ β σ σ 0 scale param 7 gamma μ x μ exp x μ β μ μ 0 location param ν 0 shape param each of these models may be fitted by maximizing the log likelihood with the glm function in the base package of r for 5 and 7 and with the package lmvar for 6 some preprocessing is performed on the raw observed series before model fitting first time steps for which global radiation is assumed to be zero i e during the night are determined based on the time of the sunrise and of the sunset at the coordinates of the station and for the given day of the year see r package insol model fitting and simulation for global radiation is performed only on identified diurnal time steps second preliminary transformations are defined for three meteorological variables ws rh and gr so as to remove range constraints and make them more likely to be suitably modeled by the gaussian distribution see table 3 for each meteorological variable either two the homo or heteroscedastic gaussian see 5 6 or three choices of probability distributions including also the gamma see 7 for ws and gr that take only positive values with a preliminary transformation when necessary are considered as potential models for each meteorological variable see the complete list in table 3 3 1 3 mandatory and optional covariates this corresponds to steps 3 and 4 in fig 4 mandatory covariates which are always included in the models defined in 5 7 are set as follows for the location parameter μ x μ either of the gaussian distributions in 5 6 or of the gamma distribution in 7 x μ includes the conditioning variables and the large scale variables listed in the column 2 and 3 respectively of table 4 to limit model complexity the mandatory covariates included in x σ for the scale parameter of the heteroscedastic gaussian distribution σ x σ in 6 only include the large scale variables listed in the 3rd column of table 4 in addition to these mandatory covariates other covariates may be optionally included in x μ to model systematic temporal variability annual and diurnal cycles and to account for temporal persistence memory effects let y t s be the meteorological variable of interest either wind speed air temperature relative humidity or global radiation at time step t and at site s 1 s the following optional covariates are considered pairs of cosines and sines with annual oscillations 8 annual cycle covariates cos 2 π d k d sin 2 π d k d with 1 d 366 the day of the year associated to time step t and k d 365 183 91 30 pairs of cosines and sines with diurnal oscillations 9 diurnal cycle covariates cos 2 π h k h sin 2 π h k h with 1 h 24 the hour of the day associated to time step t and k h 24 12 6 lagged values of the meteorological variable 10 var lagk y t k s k 1 lagged values of the spatial average of the meteorological variable 11 sa lagk 1 s s 1 s y t k s k 1 lagged values of b moving averages with b 1 12 mab lagk 1 b j 1 b y t j 1 k s k 1 lagged values of spatial b moving averages with b 1 13 smab lagk 1 b s j 1 b s 1 s y t j 1 k s k 1 optional covariates that convey information on the spatial variability can also be considered besides the conventional x and y coordinates along with elevation we include a special binary covariate to account for the presence of the windbreak at chebika station 0 indicates no windbreak while 1 indicates the presence of a windbreak this binary covariate allows the intercept term in x μ β μ μ 0 see 5 7 to take on a different value according to whether there is a windbreak or not 3 1 4 selection of the conditional distribution model and of optional covariates this is the last step to implement metgen step 5 in fig 4 for each meteorological variable a conditional distribution model see table 3 must be selected and additional optional covariates may be included in the covariate set statistical tests that are conventionally used to perform model and covariate selection e g based on p values or likelihood ratios rely on the assumption that observations are conditionally independent which is likely not the case in our application to circumvent this issue model and covariate selection are performed with a calibration validation scheme in which the data is split into a calibration period used for model fitting and a validation period used to assess model performance the performance criteria are detailed in table 5 the first stage consists of selecting the conditional distribution model for a given meteorological variable all the potential models see table 3 with the covariate set restricted to the mandatory covariates see table 4 are fitted on the calibration period the choice of conditional distribution model yielding the best performance computed on the validation period is retained for the subsequent stages in the following stages the conditional distribution model is thus fixed and optional covariates among those in 8 13 along with spatial covariates are added to the covariate set gradually the model with the larger covariate set is fitted on the calibration period the optional covariates are retained when the performance criteria computed on the validation period are improved the strategy adopted for performance assessment is based on the minimization of 14 the mean squared error of quantile quantile plots qq plots that assesses how well the distribution is reproduced as long as no lacks of fit are detected from the plots see the visual criteria in table 5 an example of addition of optional covariates that arose was when a lack of auto correlation in the surrogate series was noticed that led to the inclusion of memory effects see 10 13 another example is the presence of the windbreak at chebika station that generates differences in the diurnal and annual cycles see fig 3a and fig 3e and led to the design and the inclusion of a special binary covariate indicating the presence of the windbreak as explained in 3 1 3 3 2 sub daily bias correction techniques 3 2 1 a univariate and a multivariate bias correction techniques initial bias correction techniques such as the quantile matching method d é qu é 2007 are univariate i e they seek to transform a single series representing a single meteorological variable at a single location the quantile matching method relies on a transformation that combines the cumulative distribution functions cdfs of the high resolution and the low resolution series estimated over a calibration period this transformation ensures that the cdf of the corrected series matches the high resolution series cdf accurately over the calibration period michelangeli et al 2009 built on the quantile matching method to propose a transformation called cdf t that incorporates additionally the cdf of the low resolution series over the study or validation period more recent bias correction methods are multivariate i e they correct jointly multiple series either from several locations or for several meteorological variables or both seeking to reproduce in addition to univariate distributional properties the dependence structures present in the series cannon 2018 vrac and friederichs 2015 one such recent multivariate bias correction method is the n dimensional probability density function transform mbcn proposed by cannon 2018 the mbcn method looks iteratively for linear combinations of the variables and performs bias correction with a univariate bias correction method such as quantile matching or cdf t on the linear combinations rather than on each variable separately these two bias correction techniques cdf t and mbcn are described next as previously let y t s be the meteorological variable of interest either wind speed air temperature relative humidity or global radiation at time step t and at given site s let x t m be its large scale counterpart provided by the era5 reanalyses as listed in the first four rows of table 2 at the same time step t and at the grid cell m that contains the site s in other words y t s is the high resolution meteorological variable from the gauged station and x t m is its large scale version obtained from the reanalyses let us assume that there is a period used for calibration for which both y t s and x t m are available and a period used for validation for which only x t m is available cdf t estimates y t s a single meteorological variable at a single site over the validation period as 15 y t s f x m f x m f y s f x m x t m where x t m is the value of the large scale variable x t m that actually occurred on time t of the validation period f z and f z denote respectively the empirical cumulative distribution function of the random variable z and its inverse the quantile function and f z f z indicates the empirical distribution function estimated over the calibration validation period see michelangeli et al 2009 for more details in contrast to cdf t which needs to be applied separately to each variable meteorological variable mbcn works directly with all the meteorological variables for which bias correction needs to be performed let y t s be the vector of four meteorological variables at site s and time step t similarly let x t m x t m be the 4 dimensional vector of large scale meteorological variables at the grid cell containing the gauged station for the calibration validation period mbcn relies on random orthogonal matrices r a univariate bias correction technique in the implementation of mbcn the quantile delta mapping is used see cannon 2012 for detailed explanations is applied separately by working on each element of the rotated vectors x t m r x t m r and y t s r then the bias corrected large scale variable vectors are rotated back these steps are summarized as follows with t denoting the univariate bias correction operator applied elementwise 16 x t m t x t m r r 1 17 x t m t x t m r r 1 this procedure is iterated with new random matrices r until the multivariate distribution of x t m matches the one of y t s then x t m contains the bias corrected series in the validation period 3 2 2 working with anomalies at a single station as our goal for the surface water stress application is to obtain a single surrogate series for each of the four meteorological variables that is representative of a homogeneous area the two bias correction techniques are applied to the observation series at a single station namely ben salem which is the reference station see 2 2 in other words the large scale variables obtained from era5 reanalyses are corrected either with cdf t or with mbcn to fill the gaps and extend in time the observation series at ben salem cdf t being a univariate approach is applied separately for each meteorological variable while mbcn is applied to all four meteorological variables at once the following procedure is adopted to apply the two bias correction techniques at the sub daily resolution a conventional way to deal with the presence of annual cycles is to split the year into seasons and to apply bias correction separately on each season see for instance ayar et al 2016 however the four meteorological variables used in the energy balance model ws airt rh gr also display clear diurnal cycles see fig 3 as this strategy splitting the year into seasons is not straightforward to extend to deal with diurnal cycles we propose instead to work on anomalies of diurnal cycles with the diurnal cycle that is allowed to change with the season more precisely diurnal cycles are computed for three seasons summer june to august winter november to march and inter season the remaining months observed large scale anomalies are computed by subtracting the observed large scale diurnal cycles from the observation large scale series working with anomalies allows to remove systematic fluctuations from the meteorological variables and to focus on random fluctuations around the diurnal cycles bias corrected meteorological series are obtained by adding the observed diurnal cycles for each season to the bias corrected anomalies 4 evaluation and comparison 4 1 cross validation scheme a cross validation scheme is used to evaluate the statistical methods described in section 3 in terms of their ability to extend in time the original series i e to simulate on periods for which no observations are available indeed cross validation is convenient for small data sets and is frequently used to evaluate out of sample performance bruce et al 2020 filzmoser et al 2009 the cross validation scheme is made of three temporal partitions of the observations at all the stations cv1 cv2 and cv3 as presented in fig 6 a in each partition the observation period from 2011 to 2016 is split into a calibration period made of four years used for model fitting and a validation period consisting of two years for model evaluation and comparison the statistical models thus simulate out of sample surrogate series over the 2 year validation set of each partition to account for the stochastic aspect of metgen the series are replicated 50 times i e for each time step 50 values are drawn from the conditional models the replications are limited to 50 to keep the computation times reasonable while still permitting to explore the uncertainty captured by the models out of sample surrogate series are generated over the complete observation period by putting together the validation periods of the three partitions for each partition of the cross validation scheme the conditional distribution model and optional covariates must be selected for metgen as described in 3 1 4 to this end the calibration set of the partition which has four years is split into a smaller calibration period of three years and a validation period of one year see fig 6b the selected conditional distribution model with the selected optional covariate set and the mandatory covariates is then calibrated anew over the whole calibration set four years of the partition note that different selections of model and of optional covariates may occur for each of the three partitions of the cross validation scheme the selections of conditional distribution models and optional covariates for each of the three partitions cv1 cv2 and cv3 are as indicated in table 6 for all the partitions the conditional distribution model selected for all four meteorological variables is the heteroscedastic gaussian distribution plus a preliminary transformation when needed models m 2 w s m 2 a i r t m 2 r h and m 2 g r from table 3 in addition to the mandatory covariates from table 4 the optional covariates included in the final set of covariates are listed in table 6 for a given meteorological variable different covariate sets may be selected for each partition of the cross validation scheme as the selection was performed separately for each partition nevertheless the covariate sets are very similar in most cases for the wind speed for example the special binary covariate indicating the presence of a windbreak at one of the station was deemed necessary for all partitions no other covariate conveying spatial information was retained besides no optional covariates related to seasonal or diurnal cycle for the relative humidity and no optional covariates related to memory effects for the global radiation were included for any partition as the improvement in performance was not significant some interactions among the covariates were tested but none of them brought significant performance improvements in our application so that none were retained 4 2 cross validation evaluation in what follows we report the evaluation and comparison of the three statistical models the swg metgen and the two bias correction techniques cdf t and mbcn see 3 based on the surrogate meteorological series simulated with the cross validation scheme described in 4 1 recall that for metgen the surrogate series is replicated 50 times and that only the series related to the ben salem station is kept in this evaluation in addition to the surrogate series from the statistical models the un processed large scale variables obtained from the era5 reanalyses see table 2 are included in the comparison 4 2 1 annual and diurnal cycles fig 7 presents the annual top row and diurnal bottom row cycles for each of the four meteorological variables annual cycles are computed by averaging values in each month over the observation period similarly diurnal cycles are obtained as hourly averages the cycles of the un processed large scale variables yellow diamonds in fig 7 accurately reproduce the observations cycles blue dots in fig 7 for most meteorological variables this is the case for air temperature fig 7f and fig 7b relative humidity fig 7g and fig 7c and global radiation fig 7h and fig 7d however the wind speed cycles are under estimated in the un processed large scale variables series the afternoon peak present in the diurnal cycle fig 7e is too low about 3 m s instead of about 4 5 m s in the observation cycle and the annual cycle values fig 7a are consistently below the observed ones despite relying on this information through its covariates metgen is able to correct fairly well the under estimation shown in the un processed large scale series and to reproduce much more accurately annual and diurnal cycles of the wind speed variable the series produced by the bias correction techniques are bias corrected anomalies to which observed diurnal cycles are added see 3 2 since the root mean squared errors not reported of the bias corrected anomalies are low it follows that the cycles of the corresponding series are well reproduced 4 2 2 goodness of fit of the whole distribution and of extreme values quantile quantile plots qq plots are used instead of scatter plots to assess whether the distribution of the observation series is well reproduced by the surrogate series indeed all the statistical methods considered to generate surrogate series aim at providing a correction of the distributional properties of the large scale variables rather than reproducing the chronology of the observed series similarly as in 14 let y i and y i with i 1 n be respectively the sorted observations and the sorted surrogates of a given meteorological variable for one of the approach either one of the three statistical methods or the un processed large scale variables corresponding to the ben salem station root mean squared errors rmses of the qq plots relative to the standard deviation of the observations are defined as follows 18 1 n i 1 n y i y i 2 1 n i 1 n y i 1 n i 1 n y i 2 the relative rmse is near zero when the qq plot is well aligned on the first bisector which means that the distribution of the observations is well reproduced by the surrogates it is below above one when the rmse is smaller greater than the standard deviation i e when the surrogate series is better worse than the empirical average at reproducing the observations the relative rmses for each meteorological variable and for each type of surrogate series are reported in table 7 for metgen the median of the relative rmses of the 50 replications is reported the relative rmse is computed either on all the quantiles indicated as 0 100 or the 1 highest quantiles to focus on how extreme values are reproduced on the complete distribution i e 0 100 all three statistical methods performed quite well relative rmses are all rather close to zero the multivariate bias correction technique mbcn performed best in general although often not by much the un processed large scale variables always have the poorest performance relative rmses are higher especially for the wind speed this indicates that all three statistical methods improved upon the distributional properties of the un processed large scale variables nevertheless their performance is rather decent as the relative rmses are always lower than one on the extreme values i e the 1 highest quantiles the relative rmses give a very different picture metgen outperforms the two bias correction techniques for the wind speed and the air temperature the un processed large scale variables relative rmses may be quite high higher than one in three instances and sometime by a rather large factor the extreme values of the relative humidity variable were the most difficult to reproduce in all surrogate series and is the only case in which all statistical methods do worst than the un processed large scale variables this might be caused by the upper bound on the values taken by the relative humidity 4 2 3 inter variable dependencies accurately reproducing inter variable dependencies is particularly important since surface water stress is generally triggered by a combination of meteorological factors in this evaluation inter variable dependence is summarized by kendall s τ a non parametric correlation coefficient based on ranks that is suitable for non gaussian distributions as opposed to the pearson correlation coefficient joe 1997 positive values of kendall s τ indicate that both variables tend to increase or decrease simultaneously while negative values indicate that they tend to vary in an opposite manner a value near zero signals a lack of dependence the comparison of correlation coefficients is carried out for each of the three seasons considered for the sub daily bias correction techniques see 3 2 summer june to august winter november to march and inter season the remaining months in fig 8 the correlation coefficients computed from the observations are on the x axis while those derived from the surrogate series are on the y axis there is an overall relatively good alignment along the first bisector the red line showing that the inter variable dependence strength is rather well preserved using the different surrogate series and for all three seasons nevertheless kendall s τ coefficients computed from the large scale variables series tend to be less tightly aligned especially when the wind speed ws variable is involved for example the correlation coefficient in winter fig 8c between the wind speed ws and the air temperature airt in the observation series is about 0 2 whereas it is about 0 1 for the un processed large scale variable series the correlation is always improved with the bias corrected series and in most cases with the metgen series a relatively strong negative dependence between the air temperature airt and the relative humidity rh is preserved in the different surrogate series and in the three seasons especially in summer as it reaches 0 65 fig 8a 4 3 gap filling exercise metgen can run in gap filling mode as the simulation proceeds step by step when observations are found missing surrogate values are simulated by metgen and the covariates introducing memory effects see 10 13 are updated based on the simulated values simulations of metgen in gap filling mode can be repeated to account for the uncertainty the two bias correction techniques may also be used to perform gap filling however in contrast to metgen the surrogate series are produced in the same way as for a validation period a gap filling exercise is carried out visually by inspecting the chronological plots of the surrogate series over one day december 26th 2014 for which the observed values of all four meteorological variables from the ben salem station were removed artificially fig 9 presents the observed series in blue at the ben salem station over the day selected for the gap filling exercise the surrogate series of metgen produced in gap filling mode 50 replications of the two bias correction techniques and of the un processed large scale variable are superimposed see color legend in fig 9 we observe that the simulated values from the three statistical methods reproduce rather well the original values observed at the ben salem station the 50 simulations from metgen are generally centered around the large scale series and most importantly their spread covers the observed series we also note that metgen is able to rectify values that are too low e g the wind speed in fig 9a or too high e g the relative humidity in fig 9c that are present in the un processed large scale variable series 5 surface water stress application 5 1 sparse a dual source energy balance model surface water stress may be deduced from evapotranspiration et using energy balance models at satellite overpass time energy balance models compute instantaneous latent heat flux le expressed in w m2 as the residual term of the land surface energy balance equation hoedjes et al 2008 norman et al 1995 timmermans et al 2007 in this application of surface water stress estimation we use the dual source model soil plant atmosphere and remote evapotranspiration sparse boulet et al 2015 which is based on the same rationale as tseb two source energy balance model norman et al 1995 sparse derives from the remotely sensed surface temperature tsurf separate estimates of the instantaneous fluxes of the soil subscript s and vegetation subscript v components of the energy budget at the satellite overpass time sparse can be run under the two following modes a prescribed mode which simulates evaporation and transpiration rates for known stress levels for instance the two extremes of the water status spectrum fully stressed or maximum moisture i e potential conditions the prescribed mode provides an estimate of the potential latent flux for the soil and the vegetation lespot and levpot respectively a retrieval mode which simulates actual evaporation and transpiration the respective stress levels between non evaporating transpiring and potential rates correspond to two unknown which are solved from the single piece of information tsurf boulet et al 2015 the surface water stress index si is derived from the actual and potential evapotranspiration rates simulated from the retrieval and the prescribed mode respectively at the time of the satellite overpass si can be defined so as to describe the water status of a single component either of the soil or of the vegetation using les or lev in this application we used rather the definition of si to describe the water status of the soil vegetation composite 19 s i 1 l e v l e s l e v p o t l e s p o t the stress index values obtained directly with 19 may contain negative values due to enhanced turbulence in unstable conditions as negative values cannot theoretically occur stress index values below 0 5 are replaced by zeros besides si daily evapotranspiration etd is computed from an extrapolation algorithm in order to reconstruct its sub daily variations by assuming the self preservation of the evaporative fraction delogu et al 2012 sparse is only ran when remote sensing data are available i e on clear sky days the implementation of sparse in the matlab environment is freely available online http tully ups tlse fr gilles boulet sparse 5 2 remote sensing data modis in addition to meteorological information air temperature relative air humidity global radiation and wind speed sparse uses as inputs satellite data normalized difference vegetation index ndvi albedo and surface temperature that provide a description of the initial conditions and of the characteristics of the surface cover to this end we relied on remotely sensed data from the latest collection 6 of modis http earthexplorer usgs gov that are available from the year 2000 more precisely we used the temporal 16 day composite series of modis ndvi mod13a2 daily land surface temperature lst surface emissivity and viewing angle from mod11a1 and 8 day of albedo series mcd43a3 having a spatial resolution of 500 m these data are acquired for the observation period available at the ben salem station 2012 2016 at the resolution of the modis sensor 1 km we extracted a sub image covering the whole merguellil plain see fig 1 in addition we performed a temporal interpolation of albedo and ndvi data to have daily information at the time of the satellite overpass last ndvi information is used to compute remotely sensed leaf area index 5 3 evaluation of the surrogate series in terms of si and etd estimates we compare various estimates of instantaneous surface water stress index si and of daily evapotranspiration etd by constraining sparse on one hand with the aforementioned modis data and on the other hand with different choices of meteorological information our ground truth is the estimates of si and etd obtained when the observed meteorological series at ben salem station are used other estimates of si and etd are obtained when these observed meteorological series are replaced by the surrogate meteorological series produced with the cross validation scheme see 4 1 for the three statistical methods metgen cdf t and mbcn and by the un processed large scale variable series out of the 50 replications of metgen we designed two surrogate series corresponding to meteorological conditions leading to lower or higher surface water stress the low stress conditions consist of high humidity levels set to the 97 5 quantile of rh and low levels of ws airt and gr set to 2 5 quantiles the quantile levels are reversed to obtain the high stress conditions in fig 10 the comparison between the various estimates of si and etd is first carried out in terms of distribution with qq plots on the x axis etd fig 10a and si fig 10b are the ground truth estimates i e when the observed meteorological series at ben salem station are used to constrain sparse on the y axis of both panels of fig 10 the estimates are obtained by replacing the observed meteorological series with one of the surrogate meteorological series the distribution of etd see fig 10a is overall well reproduced by all the estimates computed with the surrogate meteorological series the low and high stress condition surrogate series from metgen form a sort of confidence band around the first bisector more pronounced differences are observed in the comparison of the qq plots of si in fig 10b as explained in 5 1 a truncation of si estimates is performed to reduce the importance of negative values which are theoretically not realistic this creates an atom i e a concentration of values at zero in the distribution of si estimates which explains the shape of the qq plot close to zero the atom is especially important when the un processed large scale variable series are used as meteorological information to obtain the si estimates the atom makes a horizontal line of zero values starting at 0 in the yellow curve of the qq plot in fig 10b the atom is also present for the high stress condition series from metgen and the two bias corrected series the plotting symbols are covered partially the si estimates from the un processed large scale variables are globally too low the yellow curve of the qq plot is well under the first bisector the si estimates deduced with the surrogate series from the two bias correction techniques cdf t and mbcn display a much milder under estimation that concerns mostly the lower values indicative of high stress levels the si estimates computed with the low and high stress condition series from metgen can be thought of as forming a sort of confidence band for the lower index values however their behavior for the higher si values is much harder to interpret and would require further investigation in order to translate differences in distribution as visualized by discrepancies from the first bisector in the qq plot from fig 10b into a more hydrologically interpretable analysis we propose a comparison based on the probability that the si estimate exceeds a given threshold so called exceedance probability in fig 11 threshold values are represented on the x axis while the exceedance probabilities are on the y axis both ranging from 0 to 1 black dots represent the exceedance probabilities computed from the ground truth si estimates along with an empirical 95 confidence band in gray based on binomial proportion confidence intervals 20 p 1 p n 1 96 where p is the probability to have a si estimate value that exceeds the threshold and n is the number of the available time steps the exceedance probabilities computed from the other si estimates when relying on the surrogate series for the meteorological information constraining the sparse model are as indicated in the color legend in fig 11 with the si estimates obtained when using the un processed large scale variable series the exceedance probability yellow diamonds in fig 11 falls below the gray confidence band for almost all threshold values this is coherent with the under estimation detected from the qq plot in fig 11 with the si estimates obtained when using the surrogate series from the two bias correction techniques the exceedance probability also tend to fall slightly below the confidence band but only for the lower thresholds the si estimates based on the low and high stress condition series from metgen form a band that overlaps the empirical confidence band for most threshold values except the larger ones a final analysis is carried out to illustrate the fluctuations of the si estimates chronologically over a short period fig 12 presents an extract of the si estimates during one month may 2016 the estimates are derived at the satellite overpass times during this month with the different choices of meteorological information either the observed series or one of the surrogate series used to constrain the sparse energy balance model for metgen the high stress condition series was used as it follows more closely the ground truth estimates in fig 12 we observe that the si estimates derived with the surrogate series generated by the bias correction techniques or the un processed large scale variables tend to over or under estimate the ground truth estimates 6 discussion we proposed an adaptation of the glm based swg developed in chandler 2015 called metgen to the sub daily resolution indeed sub daily resolution is necessary when meteorological observations are used as inputs in energy balance models to estimate surface water stress in order to ensure a precise timing with satellite overpass time by decomposing the joint distribution into a product of conditional univariate distributions metgen can model any number of meteorological variables simultaneously glms are a flexible family to model the conditional distributions of meteorological variables from which surrogate series can be simulated stochastically thereby allowing to take into account uncertainty although the inter station dependence is not modeled metgen can exploit the observations of several gauged stations as long as spatial variability can be modeled through covariates this allows to increase the sample size similarly as in the regional approach frequently used in hydrology in addition for the surface water stress application surrogate series at a single station are needed hence inter site dependence is not an issue the framework of metgen available freely as an r library https cran r project org package metgen can be useful in many cases since consistent gap filling and the extension in time of a multi variable sub daily observation series is a frequent issue er raki et al 2010 leauthaud et al 2017 since observations at successive time steps and at neighboring gauged stations during the same time steps are likely to be dependent standard model selection procedure that relies for instance on the bayesian information criterion schwarz 1978 and covariate selection that relies on p values of the coefficients cannot be used for metgen for this reason we put forward a model selection procedure that relies entirely on out of sample data through a calibration validation scheme model selection proceeds iteratively by first selecting the type of conditional distribution model using the mandatory covariates only at the successive stages optional covariates may be introduced to improve goodness of fit criteria computed on the validation set in addition to the mean squared error from the qq plot we included several visual criteria that help to assess additional important features that may not be reflected in the mean squared error one notable example of such primary features are the annual and diurnal cycles that lead to the design and the inclusion of the special covariate used to account for the presence of a windbreak at one of the stations this covariate proved essential in order to reproduce correctly the different shapes of the annual and diurnal cycles of the wind speed at each station this shows that the metgen framework can handle a certain level of spatial variability among the stations used for calibration our analyses showed that relying directly on the un processed large variables obtained from era5 reanalyses hersbach et al 2020 as surrogate series may lead to some considerable biases this is especially true for the wind speed variable as can be seen in the comparison of the annual and diurnal cycles and in the relative rmse of the qq plots bias correction techniques are devised to correct this kind of systematic departures in terms of distribution of the large scale variables although based on a completely different kind of statistical approaches than swgs bias correction techniques can easily be adapted to the task of gap filling and temporal extension for which metgen is designed these techniques are more straightforward to apply since no model selection is needed nevertheless in contrast to metgen they are not stochastic a single replication of the surrogate series is produced in the gap filling exercise from the replicated surrogate series simulated by metgen confidence bands could be obtained by computing a low and a high quantiles such as 2 5 and 97 5 for a 95 confidence level for each time step in addition despite the development of multivariate approaches the bias correction techniques are not devised explicitly to exploit meteorological data from neighboring gauged stations lying in the same era5 grid cell in metgen we made the choice to include among the mandatory covariates in the glms the large scale variables in order to allow the simulated series to be guided by the non stationary behavior present in these covariates e g to follow trends and cycles owing to the inclusion of these large scale covariates metgen may also be thought of as performing in some sense a form of bias correction fodor et al 2013 chen et al 2010 the mechanisms that allow the swg metgen and the bias correction techniques to reproduce seasonal and diurnal behavior are very different the surrogate series produced by the two bias correction techniques are constructed by adding the observations diurnal cycles computed separately for three seasons to the bias corrected anomalies the fact that the cycles annual and diurnal computed from these surrogate series are very similar to the observations cycles for all meteorological variables essentially means that the bias corrected anomalies are almost zero mean the construction of these surrogate series also explains why inter variable correlations can suitably vary from one season to another this explains also likely why there are no large differences between the univariate and the multivariate bias correction techniques as there is probably not much residual inter variable dependencies left in the anomalies in most cases the plot symbols of cdf t are hidden by those of mbcn in contrast the surrogate series simulated by metgen are able to mimic seasonal and diurnal behavior by exploiting the information in the covariates included in the glms even when no covariates dedicated to the cycles are included such as for the relative humidity variable information on seasonal and diurnal behavior can be drawn from other covariates such as the large scale variables in particular the reproduction of the annual and diurnal cycles and of the seasonal variation of the inter variable correlations of the surrogate series simulated by metgen is only due to the information present in the covariates this mechanism is in our opinion more flexible as for instance it allows the starting and ending dates of each season to vary seamlessly from year to year instead of resorting to a specialized model carey smith et al 2014 in our analyses one of the main differences among the surrogate meteorological series concerns the wind speed variable this can be explained by the fact that the wind is the most turbulent of the meteorological variables being modeled the turbulence yields more frequent and important random fluctuations that are challenging for the statistical models as metgen relies on stochastic regression it can better cope with the wind speed fluctuations than the bias correction techniques these differences in the wind speed surrogate series might be related to the differences in the resulting surface water stress index estimates indeed the sparse energy balance model is particularly sensitive to wind speed especially high wind speed values that can lead to low si estimates these lower si values are particularly important as they are indicative of incipient water stress when the low and high stress condition series derived from the replicated series of metgen are used as inputs to the sparse model a form of confidence band is obtained that seems particularly reliable for the lower si values more work on the influence of each meteorological variable on the si estimates is required to improve these low and high stress condition series in order to yield proper confidence bands besides the daily evapotranspiration estimates etd are not very sensitive to which meteorological variables are used as inputs in the sparse model this lack of sensitivity may be explained by the fact that an extrapolation scheme that might act as a form of filter of the differences is applied to obtain daily values from instantaneous ones however the sensitivity of the sparse model should be explored further 7 conclusion the framework of metgen provides a solution to the task of obtaining a representative sub daily multi variable meteorological series which mimic the observation series but in which gaps are filled and whose simulation period can extend the observation period it relies on glms to model the conditional distributions with large scale variables used as covariates these large scale variables derived from era5 reanalyses in our application are useful to provide a temporal dynamic of the weather they may present in some cases important biases that can be reduced thanks to the glms and the introduction of other covariates metgen was compared to two bias correction techniques applied on the anomalies of seasonal diurnal cycles in contrast to the bias correction techniques metgen can exploit the observations series from nearby gauged stations having a relatively similar climatic behavior and can generate several replications of the same series to allow uncertainty assessment nevertheless metgen requires a careful selection of the conditional distribution models and of the covariate sets and the simulation can be slow while the bias correction techniques are more straightforward to implement and fast to run the analyses performed in this work provide a two pronged evaluation of the surrogate series generated by metgen and the two bias correction techniques firstly the evaluation is carried out in terms of the ability to reproduce several statistical properties of the meteorological variables and secondly in terms of surface water stress estimation when the series serve as inputs in the sparse energy balance model although the performance of the two bias correction techniques was similar for some criteria to metgen the evaluation in terms of the surface water stress application tends to indicate that the surrogate series generated by metgen may be used to deduce reliable confidence intervals especially for the lower si values that are important for early drought detection perspectives for this work include the study of the sensitivity of metgen to the choice of the dependence graph that serves to simplify the decomposition of the multivariate distribution into a product of conditional univariate distributions in addition the introduction of a mechanism to explicitly model spatial dependence would certainly be a useful development see verdin et al 2018 also it would be interesting to evaluate metgen in diverse climatic conditions e g a coastal area and for other meteorological variables e g precipitation the two main challenges for metgen implementation are to find representative enough large scale variables and to perform a thorough selection of the glms and of the optional covariates last more thorough analyses pertaining to agricultural drought monitoring should be performed declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors gratefully acknowledge financial support by the lmi naila the phc utique amande a french tunisian program eranetmed chaams and the european prima project altos we would also wish to thank all the technical teams of the ird and inat for their strong collaboration and support 
25565,we present metgen a sub daily multi variable stochastic weather generator implemented as an r library that can be used to perform gap filling and to extend in time meteorological observation series metgen is tailored to provide surrogate series of air temperature relative air humidity global radiation and wind speed needed for surface water stress estimation that requires sub daily resolution multiple gauged stations can be used to increase the calibration data although spatial dependence is not modeled the approach relies on generalized linear models that use among their covariates large scale variables derived from era5 reanalyses metgen aims at preserving key features of the meteorological variables along with inter variable dependencies we illustrate the abilities of metgen using a case study with three stations in central tunisia we consider as alternatives a univariate and a multivariate bias correction techniques along with the un processed large scale variables keywords stochastic weather generator bias correction surface water stress estimation sub daily resolution era5 reanalyses 1 introduction in semi arid areas water is a major limitation factor for agricultural production indeed these areas are characterized by short rainy seasons and strong variability of precipitation events in time and space baccour et al 2012 natural variations in the water cycle affect the availability of water leading to irregularities in agricultural production saadi et al 2018 and constitutes the main driver of agricultural droughts the vegetation health status being generally representative of water availability sheffield and wood 2012 an important issue concerns the detection of surface water stress and the estimation of evapotranspiration et water stress may be deduced from et with energy balance models at satellite overpass time energy balance models compute instantaneous et as the residual term of the land surface energy balance equation once net radiation soil heat flux and sensible heat flux are derived from remotely sensed surface temperature hoedjes et al 2008 norman et al 1995 timmermans et al 2007 such water stress estimates are particularly informative for the detection of incipient plant stress during early stages of drought development compared to estimates derived from other wave lengths microwave or visible and allow to launch early drought alerts otkin et al 2013 energy balance models use as inputs satellite data normalized difference vegetation index albedo and surface temperature and in situ meteorological observations air temperature airt relative air humidity rh global radiation gr and wind speed ws as provided by gauged networks et and water stress estimates computed from the instantaneous surface energy budget constrained by the surface temperature require meteorological observations acquired at the satellite overpass time to ensure precise timing with satellite information in situ meteorological observations must be available at sub daily resolution in this work we use satellite data provided by the latest modis collection http earthexplorer usgs gov that has a 1 km spatial resolution et and water stress are estimated over a region covered by several modis grid cells this region is defined so that it can be considered to be homogeneous in terms of climate and weather as a consequence a single multi variable meteorological series representative of the region is needed nevertheless there may be several gauged stations in the region with different observation periods and different gaps in the observation series moreover it is often the case that the observed meteorological series are available over too short periods of time therefore an important task is to develop a rigorous way to obtain a representative sub daily multi variable meteorological surrogate series in which gaps are filled and that extends in time the original series by exploiting the information provided by all the gauged stations in the region stochastic weather generators swgs are stochastic models based on statistical approaches for simulating at high spatial resolution surrogate meteorological series that are similar to observation series in terms of distributional properties preserving both systematic and random variations ailliot et al 2015 swgs are thus very useful models to perform coherent gap filling and to generate realistic surrogate series over periods for which no observations are available in the aforementioned surface water stress application the sub daily series of the four meteorological variables airt rh gr and ws display both annual and diurnal cycles once these primary systematic variations are accounted for there will likely remain some random variations there are two main strategies in swgs to model systematic and random variability which are applicable in the case of multiple meteorological variables at several gauged stations the weather type approach that breaks down weather into classes or types or states of typical recurring meteorological situations e g clear blue sky cloudy rainy etc and the stochastic regression approach that captures the variability of weather smoothly by using suitable covariates in the weather type approach the underlying assumption is that each time step belongs to one weather type and all the time steps belonging to the same weather type can be modeled with a relatively simple statistical approach in other words the temporal sequence is grouped into blocks each block being associated to one weather type ailliot et al 2015 wilks and wilby 1999 one of the earliest weather type swg proposed by katz 1977 models precipitation as a two state markov chain that corresponds to two simple weather types wet and dry types richardson 1981 builds on the latter model and represents the intensity of precipitation for the wet weather type as an exponential probability distribution the other three variables maximum minimum temperature and solar radiation are modeled with a multivariate normal distribution whose means and standard deviations change according to the wet or dry types in more recent approaches more general weather types may be obtained automatically as an unsupervised classification problem they thus are defined as the classes resulting from the clustering of time steps with each time step characterized by climatic or meteorological features flecher et al 2010 weather types may also be defined indirectly as the states of a latent variable using for instance hidden markov models ailliot et al 2009 the analog approach in swgs can be seen as pushing the weather type strategy to the limit where each time step constitutes a weather type oriani et al 2014 yiou 2014 analog based swgs may be entirely non parametric and they may succeed in reproducing complex patterns between variables and between sites however non parametric analog based swgs are essentially resampling schemes that are unable to simulate values and patterns that differ from those present in the observations this may be a serious drawback when observation periods are not long enough to contain all potential patterns to account for annual cycles weather types may be modeled separately for each season with the difficulty that the definition of seasons might be somewhat arbitrary flecher et al 2010 garavaglia et al 2010 except in the case of rainfall benoit et al 2020 sub daily weather typing for variables such as temperature humidity and solar radiation requires a suitable model of the daily cycle the interpretation of weather types at the sub daily scale may be less intuitive than at the daily scale it is not yet clear how to adapt the weather type strategy in order to account for the presence of diurnal cycles ailliot et al 2015 stochastic regression or equivalently conditional distribution modeling is another widely used strategy that can account for both systematic and random variability in swgs for instance williams 1998 links the two parameters of the gamma distribution that models the intensity of rainfall and the probability of rainfall to information on the rainfall pattern on the preceding day the time of the year etc with a one hidden layer feed forward neural network more generally instead of focusing on estimating the conditional mean as is the case in conventional regression stochastic regression seeks to estimate the full conditional distribution from which simulations can be drawn thereby allowing to reproduce the observed variability covariates that carry temporal and spatial information can be introduced letting the parameters of the conditional distribution vary in time and space as an alternative to neural networks generalized linear models glms have been used within swgs for the past 20 years or so see for instance chandler 2005 verdin et al 2018 and the references therein in glms the conditional distribution belongs to the exponential family that encompasses the gaussian distribution the link between the parameters and the covariates is established with a potentially transformed e g with a logarithm linear regression mccullagh and nelder 1989 routines to implement glms are readily available in standard statistical software e g r r core team 2020 spatial dependence may be accounted for by modeling the dependence structure of the residuals e g with gaussian processes verdin et al 2018 if informative enough covariates are used glm based swgs can reproduce very accurately both systematic and random variability chandler 2020 in particular despite that glm based swgs generally operate at the daily resolution sub daily resolution modeling may be achieved by introducing covariates carrying sub daily information most glm based swgs simulate only one or two meteorological variables very often precipitation and temperature as in verdin et al 2018 one notable exception is chandler 2020 who proposes a simple scheme to model jointly several meteorological variables based on the decomposition of the multivariate density into a product of conditional univariate densities in this work we introduce metgen an swg based on glm hence relying on stochastic regression that extends the approach described in chandler 2020 to the sub daily resolution its implementation is publicly and freely available as an r library https cran r project org package metgen in metgen the scheme proposed in chandler 2020 to model jointly several meteorological variables is adapted to the four meteorological variables airt rh gr and ws required for surface water stress estimation for which inter variable dependencies are rather strong in contrast to chandler 2020 who has proposed a way to model spatial dependence inter site dependence is not explicitly modeled in metgen the proposed swg works in a manner similar to the so called regional approach developed in hydrology hosking and wallis 2005 indeed several stations within the region of interest may be used to calibrate the swg to augment the size of the data set instead of relying on the homogeneity assumption of the regional approach spatial variability when present is modeled with covariates in addition to the covariates proposed in chandler 2015 special covariates are considered to enable the reproduction of diurnal cycles based on pairs of sines and cosines similarly as for annual cycles an important category of covariates used to carry sub daily information albeit at a large scale horizontal resolution of 31 km are the meteorological reanalyses provided by era5 available at hourly resolution hersbach et al 2020 since metgen makes use of reanalyses in its covariates and because there are no other to our knowledge publicly and freely available multi variable sub daily swg we resorted to two statistical downscaling methods as comparative approaches statistical downscaling aims to bridge the gap between low resolution and potentially biased simulations from global climate models and the high resolution series required for impact studies such as observation series from gauged networks ayar et al 2016 maraun et al 2010 although often used to obtain climate change scenarios over future periods statistical downscaling methods may be applied to reanalysis products in order to generate surrogate series over past periods ayar et al 2016 an active area of research in statistical downscaling concerns the so called bias correction methods françois et al 2020 bias correction aims at transforming the low resolution series of meteorological variables such as provided by reanalyses so as to match in terms of distributional properties e g in terms of means the high resolution series such as measured at gauged stations in order to assess whether explicitly accounting for inter variable dependencies is essential we include as comparative approaches a univariate cdf t developed in michelangeli et al 2009 and a multivariate bias correction method mbcn proposed by cannon 2018 both methods provide fast non parametric i e without strong distributional assumptions alternatives to metgen and are implemented as publicly available r libraries the paper is organized as follows section 2 presents our study area the merguellil plain in central tunisia together with the meteorological data provided by gauged stations and derived from era5 reanalyses the multi variable sub daily glm based swg metgen is described in section 3 along with the two aforementioned bias correction methods and their adaptation to enable their application at the sub daily resolution section 4 is dedicated to the comparison of the statistical methods at generating surrogate meteorological series both in terms of the ability to fill gaps in the observation series and in terms of the ability to extend in time the observation series section 5 reports an evaluation of the surrogate meteorological series in terms of surface water stress estimation in section 6 a discussion is presented followed by conclusions and research perspectives in section 7 2 study area and meteorological data 2 1 study area the merguellil plain the study area is part of the downstream plain of the merguellil catchment called the merguellil plain see fig 1 lying in a semi arid region located in central tunisia the catchment is characterized by a relatively mountainous upstream area 1200 km2 and by a downstream alluvial plain 676 km2 the upstream area presents a hilly topography altitude between 200 and 1200 m with a median elevation of 500 m leduc et al 2007 in the plain the landscape is mainly flat and the vegetation is typical of semi arid regions rainfed agriculture olive tree and cereals and summer vegetables melons peppers and tomatoes downstream farms are composed mainly of small cultivated areas molle and wester 2009 the upstream and downstream areas are separated by the el haouareb dam fig 1 which was built in 1989 to protect villages from inundations and to store irrigation water for the plain ben ammar et al 2006 the study area is influenced both by the mediterranean climate dry subhumid and the pre saharan climate arid baccour et al 2012 it is characterized by the inter annual irregularity of precipitation with an average of annual rainfall of about 300 mm per year and by a high evaporative demand of about 1600 mm per year water supply is by far insufficient to meet water demand which is rising steadily the rise is due to the increase in population and industrial development and most importantly to the intensification of agriculture which is the main water consumer around 80 leduc et al 2004 2 2 meteorological observations hourly observation series of the four meteorological variables air temperature airt relative air humidity rh global radiation gr and wind speed ws needed for surface water stress estimation are collected from the three gauged stations ben salem chebika and barrouta located in the merguellil plain see fig 1 the observation period approximate number of observations and approximate percentage of missing values are given in table 1 in addition fig 2 illustrates the observation period and the positions of the gaps in the series the three gauged stations provide similar meteorological information owing to their geographical proximity 7 9 km between ben salem and chebika 11 7 km between ben salem and barrouta and 11 km between chebika and barrouta as shown by the annual and diurnal cycles in fig 3 these stations share similar climatic behaviors with the exception of the wind speed observed at chebika lower wind speed values are caused by the presence of a windbreak in the vicinity of the station in addition inter station pair plots not shown confirmed the strong relationship in the meteorological information provided by the three stations the observation series at these three stations enter in the calibration of the stochastic generator metgen proposed in this work when building the statistical model spatial covariates are selected to account for differences in the distribution of the meteorological variables at each of the station in particular a special covariate is used for the wind to account for the presence of the windbreak see details in 3 1 ben salem is selected as the reference station as it complies best with the meteorological standards according to the wmo guidelines omm 2010 therefore only the simulations of surrogate series corresponding to ben salem station are used in the evaluation and comparison of metgen 2 3 meteorological reanalyses era5 reanalyses combine forecast models and observations through data assimilation schemes thereby providing a multivariate spatially complete and coherent record without gaps of atmospheric land and oceanic climate variables dee et al 2011 hersbach et al 2020 in particular era5 reanalyses are available for a long period in the past from 1950 till now hersbach et al 2020 despite being available at hourly resolution the era5 spatial resolution is low horizontal resolution of 31 km hersbach et al 2020 and thus local scale variability might not sufficiently be accounted for hooker et al 2018 besides the mismatch in spatial resolution several limitations affected the quality of previous reanalyses such as era interim which were improved with respect to most aspects for era5 hersbach et al 2020 the three gauged stations from the merguellil plain lie in the same era5 grid cell whose center is shown in fig 1 era5 reanalyses were extracted at this grid cell and were combined to obtain the six large scale meteorological variables listed in table 2 in most cases the large scale variables correspond to raw reanalysis products there are two exceptions the first one concerns the wind speed that was derived by taking the euclidean norm of the 10 m vertical and horizontal wind components the second exception concerns the relative humidity that was derived based on 2 m temperature and 2 m dewpoint temperature era5 products according to the procedures defined in allen et al 2005 the six large scale variables from table 2 serve as covariates in the statistical methods described in section 3 to obtain surrogate meteorological series for the merguellil plain among these the first four are the large scale counterpart of the meteorological variables needed for the surface water stress application to evaluate the quality of the reanalysis products these four large scale variables are used without further processing as one of the candidate surrogate meteorological series it is expected that the statistical methods should be able to correct departures in terms of distributional properties of the large scale variables therefore other large scale data whether reanalyses or remote sensed could be used instead of era5 3 statistical methods 3 1 metgen a regional multi variable sub daily glm based swg we focus on metgen implementation for the surface water stress application in central tunisia the workflow sequence summarized in fig 4 is the main contribution of this work and can be adapted in principle to any study area and to any other meteorological variables metgen is regional in the sense that the observations from several gauged stations can be used in the calibration to increase the sample size as discussed in 2 2 any spatial variability in terms of distribution is captured through dedicated covariates once calibrated metgen simulates series at all the gauged stations however for our surface water stress application a single series representative of the region is needed to this end we make use of the series corresponding to ben salem as it is our reference station see 2 2 3 1 1 multi variable modeling conditioning variables the first step to implement metgen consists in modeling the inter variable dependencies by means of conditioning variables see fig 4 this follows the proposal of rglimclim chandler 2015 by which a multivariate distribution can be decomposed with the product rule into conditional univariate distributions to determine the order of the decomposition in the product rule and to reduce the number of conditioning variables we rely on the dependence graph shown in fig 5 it has been adapted from the one made in the hydef project https www imperial ac uk media imperial college research centres and groups environmental and water resource engineering ucl15feb2012 pdf to apply rglimclim in the uk more precisely the multivariate distribution of the four meteorological variables needed for the surface water stress application boils down to modeling four conditional univariate distributions one for each meteorological variable and including in the covariates the appropriate conditioning variables 1 p w s x 2 p a i r t w s x 3 p r h a i r t w s x 4 p g r a i r t x where x are additional covariates to be described in 3 1 3 the choice of the conditional distribution model the selection of covariates and the calibration can be performed separately for each meteorological variable the simulation of the multi variable surrogate series proceeds following the order dictated by the dependence graph in fig 5 wind speed is simulated first then air temperature is simulated including among the covariates the series simulated for wind speed relative humidity is simulated afterwards with the previously simulated series for air temperature and wind speed included in the covariates and finally global radiation is simulated conditionally on the series simulated for air temperature 3 1 2 conditional univariate distribution models generalized linear models glms in the second step of metgen potential conditional univariate distribution models which are from the generalized linear model glm family for each meteorological variable must be defined see fig 4 at present three possible choices of probability distributions for the glms are available in metgen the gaussian distribution with constant homoscedastic or non constant variance heteroscedastic and the gamma distribution in the glms the parameters of the probability distributions may vary according to covariates in metgen we made the following choices to link the covariates to the probability distribution parameters let x μ and x σ be two covariate vectors let β μ and β σ be regression coefficient vectors of the same length as x μ and x σ respectively and let μ 0 σ 0 0 and ν 0 0 be three constants then the parameters of the conditional distributions are provided as follows for each of the three possible choices 5 homoscedastic gaussian μ x μ x μ β μ μ 0 location param σ 0 scale param 6 heteroscedastic gaussian μ x μ x μ β μ μ 0 location param σ x σ exp x σ β σ σ 0 scale param 7 gamma μ x μ exp x μ β μ μ 0 location param ν 0 shape param each of these models may be fitted by maximizing the log likelihood with the glm function in the base package of r for 5 and 7 and with the package lmvar for 6 some preprocessing is performed on the raw observed series before model fitting first time steps for which global radiation is assumed to be zero i e during the night are determined based on the time of the sunrise and of the sunset at the coordinates of the station and for the given day of the year see r package insol model fitting and simulation for global radiation is performed only on identified diurnal time steps second preliminary transformations are defined for three meteorological variables ws rh and gr so as to remove range constraints and make them more likely to be suitably modeled by the gaussian distribution see table 3 for each meteorological variable either two the homo or heteroscedastic gaussian see 5 6 or three choices of probability distributions including also the gamma see 7 for ws and gr that take only positive values with a preliminary transformation when necessary are considered as potential models for each meteorological variable see the complete list in table 3 3 1 3 mandatory and optional covariates this corresponds to steps 3 and 4 in fig 4 mandatory covariates which are always included in the models defined in 5 7 are set as follows for the location parameter μ x μ either of the gaussian distributions in 5 6 or of the gamma distribution in 7 x μ includes the conditioning variables and the large scale variables listed in the column 2 and 3 respectively of table 4 to limit model complexity the mandatory covariates included in x σ for the scale parameter of the heteroscedastic gaussian distribution σ x σ in 6 only include the large scale variables listed in the 3rd column of table 4 in addition to these mandatory covariates other covariates may be optionally included in x μ to model systematic temporal variability annual and diurnal cycles and to account for temporal persistence memory effects let y t s be the meteorological variable of interest either wind speed air temperature relative humidity or global radiation at time step t and at site s 1 s the following optional covariates are considered pairs of cosines and sines with annual oscillations 8 annual cycle covariates cos 2 π d k d sin 2 π d k d with 1 d 366 the day of the year associated to time step t and k d 365 183 91 30 pairs of cosines and sines with diurnal oscillations 9 diurnal cycle covariates cos 2 π h k h sin 2 π h k h with 1 h 24 the hour of the day associated to time step t and k h 24 12 6 lagged values of the meteorological variable 10 var lagk y t k s k 1 lagged values of the spatial average of the meteorological variable 11 sa lagk 1 s s 1 s y t k s k 1 lagged values of b moving averages with b 1 12 mab lagk 1 b j 1 b y t j 1 k s k 1 lagged values of spatial b moving averages with b 1 13 smab lagk 1 b s j 1 b s 1 s y t j 1 k s k 1 optional covariates that convey information on the spatial variability can also be considered besides the conventional x and y coordinates along with elevation we include a special binary covariate to account for the presence of the windbreak at chebika station 0 indicates no windbreak while 1 indicates the presence of a windbreak this binary covariate allows the intercept term in x μ β μ μ 0 see 5 7 to take on a different value according to whether there is a windbreak or not 3 1 4 selection of the conditional distribution model and of optional covariates this is the last step to implement metgen step 5 in fig 4 for each meteorological variable a conditional distribution model see table 3 must be selected and additional optional covariates may be included in the covariate set statistical tests that are conventionally used to perform model and covariate selection e g based on p values or likelihood ratios rely on the assumption that observations are conditionally independent which is likely not the case in our application to circumvent this issue model and covariate selection are performed with a calibration validation scheme in which the data is split into a calibration period used for model fitting and a validation period used to assess model performance the performance criteria are detailed in table 5 the first stage consists of selecting the conditional distribution model for a given meteorological variable all the potential models see table 3 with the covariate set restricted to the mandatory covariates see table 4 are fitted on the calibration period the choice of conditional distribution model yielding the best performance computed on the validation period is retained for the subsequent stages in the following stages the conditional distribution model is thus fixed and optional covariates among those in 8 13 along with spatial covariates are added to the covariate set gradually the model with the larger covariate set is fitted on the calibration period the optional covariates are retained when the performance criteria computed on the validation period are improved the strategy adopted for performance assessment is based on the minimization of 14 the mean squared error of quantile quantile plots qq plots that assesses how well the distribution is reproduced as long as no lacks of fit are detected from the plots see the visual criteria in table 5 an example of addition of optional covariates that arose was when a lack of auto correlation in the surrogate series was noticed that led to the inclusion of memory effects see 10 13 another example is the presence of the windbreak at chebika station that generates differences in the diurnal and annual cycles see fig 3a and fig 3e and led to the design and the inclusion of a special binary covariate indicating the presence of the windbreak as explained in 3 1 3 3 2 sub daily bias correction techniques 3 2 1 a univariate and a multivariate bias correction techniques initial bias correction techniques such as the quantile matching method d é qu é 2007 are univariate i e they seek to transform a single series representing a single meteorological variable at a single location the quantile matching method relies on a transformation that combines the cumulative distribution functions cdfs of the high resolution and the low resolution series estimated over a calibration period this transformation ensures that the cdf of the corrected series matches the high resolution series cdf accurately over the calibration period michelangeli et al 2009 built on the quantile matching method to propose a transformation called cdf t that incorporates additionally the cdf of the low resolution series over the study or validation period more recent bias correction methods are multivariate i e they correct jointly multiple series either from several locations or for several meteorological variables or both seeking to reproduce in addition to univariate distributional properties the dependence structures present in the series cannon 2018 vrac and friederichs 2015 one such recent multivariate bias correction method is the n dimensional probability density function transform mbcn proposed by cannon 2018 the mbcn method looks iteratively for linear combinations of the variables and performs bias correction with a univariate bias correction method such as quantile matching or cdf t on the linear combinations rather than on each variable separately these two bias correction techniques cdf t and mbcn are described next as previously let y t s be the meteorological variable of interest either wind speed air temperature relative humidity or global radiation at time step t and at given site s let x t m be its large scale counterpart provided by the era5 reanalyses as listed in the first four rows of table 2 at the same time step t and at the grid cell m that contains the site s in other words y t s is the high resolution meteorological variable from the gauged station and x t m is its large scale version obtained from the reanalyses let us assume that there is a period used for calibration for which both y t s and x t m are available and a period used for validation for which only x t m is available cdf t estimates y t s a single meteorological variable at a single site over the validation period as 15 y t s f x m f x m f y s f x m x t m where x t m is the value of the large scale variable x t m that actually occurred on time t of the validation period f z and f z denote respectively the empirical cumulative distribution function of the random variable z and its inverse the quantile function and f z f z indicates the empirical distribution function estimated over the calibration validation period see michelangeli et al 2009 for more details in contrast to cdf t which needs to be applied separately to each variable meteorological variable mbcn works directly with all the meteorological variables for which bias correction needs to be performed let y t s be the vector of four meteorological variables at site s and time step t similarly let x t m x t m be the 4 dimensional vector of large scale meteorological variables at the grid cell containing the gauged station for the calibration validation period mbcn relies on random orthogonal matrices r a univariate bias correction technique in the implementation of mbcn the quantile delta mapping is used see cannon 2012 for detailed explanations is applied separately by working on each element of the rotated vectors x t m r x t m r and y t s r then the bias corrected large scale variable vectors are rotated back these steps are summarized as follows with t denoting the univariate bias correction operator applied elementwise 16 x t m t x t m r r 1 17 x t m t x t m r r 1 this procedure is iterated with new random matrices r until the multivariate distribution of x t m matches the one of y t s then x t m contains the bias corrected series in the validation period 3 2 2 working with anomalies at a single station as our goal for the surface water stress application is to obtain a single surrogate series for each of the four meteorological variables that is representative of a homogeneous area the two bias correction techniques are applied to the observation series at a single station namely ben salem which is the reference station see 2 2 in other words the large scale variables obtained from era5 reanalyses are corrected either with cdf t or with mbcn to fill the gaps and extend in time the observation series at ben salem cdf t being a univariate approach is applied separately for each meteorological variable while mbcn is applied to all four meteorological variables at once the following procedure is adopted to apply the two bias correction techniques at the sub daily resolution a conventional way to deal with the presence of annual cycles is to split the year into seasons and to apply bias correction separately on each season see for instance ayar et al 2016 however the four meteorological variables used in the energy balance model ws airt rh gr also display clear diurnal cycles see fig 3 as this strategy splitting the year into seasons is not straightforward to extend to deal with diurnal cycles we propose instead to work on anomalies of diurnal cycles with the diurnal cycle that is allowed to change with the season more precisely diurnal cycles are computed for three seasons summer june to august winter november to march and inter season the remaining months observed large scale anomalies are computed by subtracting the observed large scale diurnal cycles from the observation large scale series working with anomalies allows to remove systematic fluctuations from the meteorological variables and to focus on random fluctuations around the diurnal cycles bias corrected meteorological series are obtained by adding the observed diurnal cycles for each season to the bias corrected anomalies 4 evaluation and comparison 4 1 cross validation scheme a cross validation scheme is used to evaluate the statistical methods described in section 3 in terms of their ability to extend in time the original series i e to simulate on periods for which no observations are available indeed cross validation is convenient for small data sets and is frequently used to evaluate out of sample performance bruce et al 2020 filzmoser et al 2009 the cross validation scheme is made of three temporal partitions of the observations at all the stations cv1 cv2 and cv3 as presented in fig 6 a in each partition the observation period from 2011 to 2016 is split into a calibration period made of four years used for model fitting and a validation period consisting of two years for model evaluation and comparison the statistical models thus simulate out of sample surrogate series over the 2 year validation set of each partition to account for the stochastic aspect of metgen the series are replicated 50 times i e for each time step 50 values are drawn from the conditional models the replications are limited to 50 to keep the computation times reasonable while still permitting to explore the uncertainty captured by the models out of sample surrogate series are generated over the complete observation period by putting together the validation periods of the three partitions for each partition of the cross validation scheme the conditional distribution model and optional covariates must be selected for metgen as described in 3 1 4 to this end the calibration set of the partition which has four years is split into a smaller calibration period of three years and a validation period of one year see fig 6b the selected conditional distribution model with the selected optional covariate set and the mandatory covariates is then calibrated anew over the whole calibration set four years of the partition note that different selections of model and of optional covariates may occur for each of the three partitions of the cross validation scheme the selections of conditional distribution models and optional covariates for each of the three partitions cv1 cv2 and cv3 are as indicated in table 6 for all the partitions the conditional distribution model selected for all four meteorological variables is the heteroscedastic gaussian distribution plus a preliminary transformation when needed models m 2 w s m 2 a i r t m 2 r h and m 2 g r from table 3 in addition to the mandatory covariates from table 4 the optional covariates included in the final set of covariates are listed in table 6 for a given meteorological variable different covariate sets may be selected for each partition of the cross validation scheme as the selection was performed separately for each partition nevertheless the covariate sets are very similar in most cases for the wind speed for example the special binary covariate indicating the presence of a windbreak at one of the station was deemed necessary for all partitions no other covariate conveying spatial information was retained besides no optional covariates related to seasonal or diurnal cycle for the relative humidity and no optional covariates related to memory effects for the global radiation were included for any partition as the improvement in performance was not significant some interactions among the covariates were tested but none of them brought significant performance improvements in our application so that none were retained 4 2 cross validation evaluation in what follows we report the evaluation and comparison of the three statistical models the swg metgen and the two bias correction techniques cdf t and mbcn see 3 based on the surrogate meteorological series simulated with the cross validation scheme described in 4 1 recall that for metgen the surrogate series is replicated 50 times and that only the series related to the ben salem station is kept in this evaluation in addition to the surrogate series from the statistical models the un processed large scale variables obtained from the era5 reanalyses see table 2 are included in the comparison 4 2 1 annual and diurnal cycles fig 7 presents the annual top row and diurnal bottom row cycles for each of the four meteorological variables annual cycles are computed by averaging values in each month over the observation period similarly diurnal cycles are obtained as hourly averages the cycles of the un processed large scale variables yellow diamonds in fig 7 accurately reproduce the observations cycles blue dots in fig 7 for most meteorological variables this is the case for air temperature fig 7f and fig 7b relative humidity fig 7g and fig 7c and global radiation fig 7h and fig 7d however the wind speed cycles are under estimated in the un processed large scale variables series the afternoon peak present in the diurnal cycle fig 7e is too low about 3 m s instead of about 4 5 m s in the observation cycle and the annual cycle values fig 7a are consistently below the observed ones despite relying on this information through its covariates metgen is able to correct fairly well the under estimation shown in the un processed large scale series and to reproduce much more accurately annual and diurnal cycles of the wind speed variable the series produced by the bias correction techniques are bias corrected anomalies to which observed diurnal cycles are added see 3 2 since the root mean squared errors not reported of the bias corrected anomalies are low it follows that the cycles of the corresponding series are well reproduced 4 2 2 goodness of fit of the whole distribution and of extreme values quantile quantile plots qq plots are used instead of scatter plots to assess whether the distribution of the observation series is well reproduced by the surrogate series indeed all the statistical methods considered to generate surrogate series aim at providing a correction of the distributional properties of the large scale variables rather than reproducing the chronology of the observed series similarly as in 14 let y i and y i with i 1 n be respectively the sorted observations and the sorted surrogates of a given meteorological variable for one of the approach either one of the three statistical methods or the un processed large scale variables corresponding to the ben salem station root mean squared errors rmses of the qq plots relative to the standard deviation of the observations are defined as follows 18 1 n i 1 n y i y i 2 1 n i 1 n y i 1 n i 1 n y i 2 the relative rmse is near zero when the qq plot is well aligned on the first bisector which means that the distribution of the observations is well reproduced by the surrogates it is below above one when the rmse is smaller greater than the standard deviation i e when the surrogate series is better worse than the empirical average at reproducing the observations the relative rmses for each meteorological variable and for each type of surrogate series are reported in table 7 for metgen the median of the relative rmses of the 50 replications is reported the relative rmse is computed either on all the quantiles indicated as 0 100 or the 1 highest quantiles to focus on how extreme values are reproduced on the complete distribution i e 0 100 all three statistical methods performed quite well relative rmses are all rather close to zero the multivariate bias correction technique mbcn performed best in general although often not by much the un processed large scale variables always have the poorest performance relative rmses are higher especially for the wind speed this indicates that all three statistical methods improved upon the distributional properties of the un processed large scale variables nevertheless their performance is rather decent as the relative rmses are always lower than one on the extreme values i e the 1 highest quantiles the relative rmses give a very different picture metgen outperforms the two bias correction techniques for the wind speed and the air temperature the un processed large scale variables relative rmses may be quite high higher than one in three instances and sometime by a rather large factor the extreme values of the relative humidity variable were the most difficult to reproduce in all surrogate series and is the only case in which all statistical methods do worst than the un processed large scale variables this might be caused by the upper bound on the values taken by the relative humidity 4 2 3 inter variable dependencies accurately reproducing inter variable dependencies is particularly important since surface water stress is generally triggered by a combination of meteorological factors in this evaluation inter variable dependence is summarized by kendall s τ a non parametric correlation coefficient based on ranks that is suitable for non gaussian distributions as opposed to the pearson correlation coefficient joe 1997 positive values of kendall s τ indicate that both variables tend to increase or decrease simultaneously while negative values indicate that they tend to vary in an opposite manner a value near zero signals a lack of dependence the comparison of correlation coefficients is carried out for each of the three seasons considered for the sub daily bias correction techniques see 3 2 summer june to august winter november to march and inter season the remaining months in fig 8 the correlation coefficients computed from the observations are on the x axis while those derived from the surrogate series are on the y axis there is an overall relatively good alignment along the first bisector the red line showing that the inter variable dependence strength is rather well preserved using the different surrogate series and for all three seasons nevertheless kendall s τ coefficients computed from the large scale variables series tend to be less tightly aligned especially when the wind speed ws variable is involved for example the correlation coefficient in winter fig 8c between the wind speed ws and the air temperature airt in the observation series is about 0 2 whereas it is about 0 1 for the un processed large scale variable series the correlation is always improved with the bias corrected series and in most cases with the metgen series a relatively strong negative dependence between the air temperature airt and the relative humidity rh is preserved in the different surrogate series and in the three seasons especially in summer as it reaches 0 65 fig 8a 4 3 gap filling exercise metgen can run in gap filling mode as the simulation proceeds step by step when observations are found missing surrogate values are simulated by metgen and the covariates introducing memory effects see 10 13 are updated based on the simulated values simulations of metgen in gap filling mode can be repeated to account for the uncertainty the two bias correction techniques may also be used to perform gap filling however in contrast to metgen the surrogate series are produced in the same way as for a validation period a gap filling exercise is carried out visually by inspecting the chronological plots of the surrogate series over one day december 26th 2014 for which the observed values of all four meteorological variables from the ben salem station were removed artificially fig 9 presents the observed series in blue at the ben salem station over the day selected for the gap filling exercise the surrogate series of metgen produced in gap filling mode 50 replications of the two bias correction techniques and of the un processed large scale variable are superimposed see color legend in fig 9 we observe that the simulated values from the three statistical methods reproduce rather well the original values observed at the ben salem station the 50 simulations from metgen are generally centered around the large scale series and most importantly their spread covers the observed series we also note that metgen is able to rectify values that are too low e g the wind speed in fig 9a or too high e g the relative humidity in fig 9c that are present in the un processed large scale variable series 5 surface water stress application 5 1 sparse a dual source energy balance model surface water stress may be deduced from evapotranspiration et using energy balance models at satellite overpass time energy balance models compute instantaneous latent heat flux le expressed in w m2 as the residual term of the land surface energy balance equation hoedjes et al 2008 norman et al 1995 timmermans et al 2007 in this application of surface water stress estimation we use the dual source model soil plant atmosphere and remote evapotranspiration sparse boulet et al 2015 which is based on the same rationale as tseb two source energy balance model norman et al 1995 sparse derives from the remotely sensed surface temperature tsurf separate estimates of the instantaneous fluxes of the soil subscript s and vegetation subscript v components of the energy budget at the satellite overpass time sparse can be run under the two following modes a prescribed mode which simulates evaporation and transpiration rates for known stress levels for instance the two extremes of the water status spectrum fully stressed or maximum moisture i e potential conditions the prescribed mode provides an estimate of the potential latent flux for the soil and the vegetation lespot and levpot respectively a retrieval mode which simulates actual evaporation and transpiration the respective stress levels between non evaporating transpiring and potential rates correspond to two unknown which are solved from the single piece of information tsurf boulet et al 2015 the surface water stress index si is derived from the actual and potential evapotranspiration rates simulated from the retrieval and the prescribed mode respectively at the time of the satellite overpass si can be defined so as to describe the water status of a single component either of the soil or of the vegetation using les or lev in this application we used rather the definition of si to describe the water status of the soil vegetation composite 19 s i 1 l e v l e s l e v p o t l e s p o t the stress index values obtained directly with 19 may contain negative values due to enhanced turbulence in unstable conditions as negative values cannot theoretically occur stress index values below 0 5 are replaced by zeros besides si daily evapotranspiration etd is computed from an extrapolation algorithm in order to reconstruct its sub daily variations by assuming the self preservation of the evaporative fraction delogu et al 2012 sparse is only ran when remote sensing data are available i e on clear sky days the implementation of sparse in the matlab environment is freely available online http tully ups tlse fr gilles boulet sparse 5 2 remote sensing data modis in addition to meteorological information air temperature relative air humidity global radiation and wind speed sparse uses as inputs satellite data normalized difference vegetation index ndvi albedo and surface temperature that provide a description of the initial conditions and of the characteristics of the surface cover to this end we relied on remotely sensed data from the latest collection 6 of modis http earthexplorer usgs gov that are available from the year 2000 more precisely we used the temporal 16 day composite series of modis ndvi mod13a2 daily land surface temperature lst surface emissivity and viewing angle from mod11a1 and 8 day of albedo series mcd43a3 having a spatial resolution of 500 m these data are acquired for the observation period available at the ben salem station 2012 2016 at the resolution of the modis sensor 1 km we extracted a sub image covering the whole merguellil plain see fig 1 in addition we performed a temporal interpolation of albedo and ndvi data to have daily information at the time of the satellite overpass last ndvi information is used to compute remotely sensed leaf area index 5 3 evaluation of the surrogate series in terms of si and etd estimates we compare various estimates of instantaneous surface water stress index si and of daily evapotranspiration etd by constraining sparse on one hand with the aforementioned modis data and on the other hand with different choices of meteorological information our ground truth is the estimates of si and etd obtained when the observed meteorological series at ben salem station are used other estimates of si and etd are obtained when these observed meteorological series are replaced by the surrogate meteorological series produced with the cross validation scheme see 4 1 for the three statistical methods metgen cdf t and mbcn and by the un processed large scale variable series out of the 50 replications of metgen we designed two surrogate series corresponding to meteorological conditions leading to lower or higher surface water stress the low stress conditions consist of high humidity levels set to the 97 5 quantile of rh and low levels of ws airt and gr set to 2 5 quantiles the quantile levels are reversed to obtain the high stress conditions in fig 10 the comparison between the various estimates of si and etd is first carried out in terms of distribution with qq plots on the x axis etd fig 10a and si fig 10b are the ground truth estimates i e when the observed meteorological series at ben salem station are used to constrain sparse on the y axis of both panels of fig 10 the estimates are obtained by replacing the observed meteorological series with one of the surrogate meteorological series the distribution of etd see fig 10a is overall well reproduced by all the estimates computed with the surrogate meteorological series the low and high stress condition surrogate series from metgen form a sort of confidence band around the first bisector more pronounced differences are observed in the comparison of the qq plots of si in fig 10b as explained in 5 1 a truncation of si estimates is performed to reduce the importance of negative values which are theoretically not realistic this creates an atom i e a concentration of values at zero in the distribution of si estimates which explains the shape of the qq plot close to zero the atom is especially important when the un processed large scale variable series are used as meteorological information to obtain the si estimates the atom makes a horizontal line of zero values starting at 0 in the yellow curve of the qq plot in fig 10b the atom is also present for the high stress condition series from metgen and the two bias corrected series the plotting symbols are covered partially the si estimates from the un processed large scale variables are globally too low the yellow curve of the qq plot is well under the first bisector the si estimates deduced with the surrogate series from the two bias correction techniques cdf t and mbcn display a much milder under estimation that concerns mostly the lower values indicative of high stress levels the si estimates computed with the low and high stress condition series from metgen can be thought of as forming a sort of confidence band for the lower index values however their behavior for the higher si values is much harder to interpret and would require further investigation in order to translate differences in distribution as visualized by discrepancies from the first bisector in the qq plot from fig 10b into a more hydrologically interpretable analysis we propose a comparison based on the probability that the si estimate exceeds a given threshold so called exceedance probability in fig 11 threshold values are represented on the x axis while the exceedance probabilities are on the y axis both ranging from 0 to 1 black dots represent the exceedance probabilities computed from the ground truth si estimates along with an empirical 95 confidence band in gray based on binomial proportion confidence intervals 20 p 1 p n 1 96 where p is the probability to have a si estimate value that exceeds the threshold and n is the number of the available time steps the exceedance probabilities computed from the other si estimates when relying on the surrogate series for the meteorological information constraining the sparse model are as indicated in the color legend in fig 11 with the si estimates obtained when using the un processed large scale variable series the exceedance probability yellow diamonds in fig 11 falls below the gray confidence band for almost all threshold values this is coherent with the under estimation detected from the qq plot in fig 11 with the si estimates obtained when using the surrogate series from the two bias correction techniques the exceedance probability also tend to fall slightly below the confidence band but only for the lower thresholds the si estimates based on the low and high stress condition series from metgen form a band that overlaps the empirical confidence band for most threshold values except the larger ones a final analysis is carried out to illustrate the fluctuations of the si estimates chronologically over a short period fig 12 presents an extract of the si estimates during one month may 2016 the estimates are derived at the satellite overpass times during this month with the different choices of meteorological information either the observed series or one of the surrogate series used to constrain the sparse energy balance model for metgen the high stress condition series was used as it follows more closely the ground truth estimates in fig 12 we observe that the si estimates derived with the surrogate series generated by the bias correction techniques or the un processed large scale variables tend to over or under estimate the ground truth estimates 6 discussion we proposed an adaptation of the glm based swg developed in chandler 2015 called metgen to the sub daily resolution indeed sub daily resolution is necessary when meteorological observations are used as inputs in energy balance models to estimate surface water stress in order to ensure a precise timing with satellite overpass time by decomposing the joint distribution into a product of conditional univariate distributions metgen can model any number of meteorological variables simultaneously glms are a flexible family to model the conditional distributions of meteorological variables from which surrogate series can be simulated stochastically thereby allowing to take into account uncertainty although the inter station dependence is not modeled metgen can exploit the observations of several gauged stations as long as spatial variability can be modeled through covariates this allows to increase the sample size similarly as in the regional approach frequently used in hydrology in addition for the surface water stress application surrogate series at a single station are needed hence inter site dependence is not an issue the framework of metgen available freely as an r library https cran r project org package metgen can be useful in many cases since consistent gap filling and the extension in time of a multi variable sub daily observation series is a frequent issue er raki et al 2010 leauthaud et al 2017 since observations at successive time steps and at neighboring gauged stations during the same time steps are likely to be dependent standard model selection procedure that relies for instance on the bayesian information criterion schwarz 1978 and covariate selection that relies on p values of the coefficients cannot be used for metgen for this reason we put forward a model selection procedure that relies entirely on out of sample data through a calibration validation scheme model selection proceeds iteratively by first selecting the type of conditional distribution model using the mandatory covariates only at the successive stages optional covariates may be introduced to improve goodness of fit criteria computed on the validation set in addition to the mean squared error from the qq plot we included several visual criteria that help to assess additional important features that may not be reflected in the mean squared error one notable example of such primary features are the annual and diurnal cycles that lead to the design and the inclusion of the special covariate used to account for the presence of a windbreak at one of the stations this covariate proved essential in order to reproduce correctly the different shapes of the annual and diurnal cycles of the wind speed at each station this shows that the metgen framework can handle a certain level of spatial variability among the stations used for calibration our analyses showed that relying directly on the un processed large variables obtained from era5 reanalyses hersbach et al 2020 as surrogate series may lead to some considerable biases this is especially true for the wind speed variable as can be seen in the comparison of the annual and diurnal cycles and in the relative rmse of the qq plots bias correction techniques are devised to correct this kind of systematic departures in terms of distribution of the large scale variables although based on a completely different kind of statistical approaches than swgs bias correction techniques can easily be adapted to the task of gap filling and temporal extension for which metgen is designed these techniques are more straightforward to apply since no model selection is needed nevertheless in contrast to metgen they are not stochastic a single replication of the surrogate series is produced in the gap filling exercise from the replicated surrogate series simulated by metgen confidence bands could be obtained by computing a low and a high quantiles such as 2 5 and 97 5 for a 95 confidence level for each time step in addition despite the development of multivariate approaches the bias correction techniques are not devised explicitly to exploit meteorological data from neighboring gauged stations lying in the same era5 grid cell in metgen we made the choice to include among the mandatory covariates in the glms the large scale variables in order to allow the simulated series to be guided by the non stationary behavior present in these covariates e g to follow trends and cycles owing to the inclusion of these large scale covariates metgen may also be thought of as performing in some sense a form of bias correction fodor et al 2013 chen et al 2010 the mechanisms that allow the swg metgen and the bias correction techniques to reproduce seasonal and diurnal behavior are very different the surrogate series produced by the two bias correction techniques are constructed by adding the observations diurnal cycles computed separately for three seasons to the bias corrected anomalies the fact that the cycles annual and diurnal computed from these surrogate series are very similar to the observations cycles for all meteorological variables essentially means that the bias corrected anomalies are almost zero mean the construction of these surrogate series also explains why inter variable correlations can suitably vary from one season to another this explains also likely why there are no large differences between the univariate and the multivariate bias correction techniques as there is probably not much residual inter variable dependencies left in the anomalies in most cases the plot symbols of cdf t are hidden by those of mbcn in contrast the surrogate series simulated by metgen are able to mimic seasonal and diurnal behavior by exploiting the information in the covariates included in the glms even when no covariates dedicated to the cycles are included such as for the relative humidity variable information on seasonal and diurnal behavior can be drawn from other covariates such as the large scale variables in particular the reproduction of the annual and diurnal cycles and of the seasonal variation of the inter variable correlations of the surrogate series simulated by metgen is only due to the information present in the covariates this mechanism is in our opinion more flexible as for instance it allows the starting and ending dates of each season to vary seamlessly from year to year instead of resorting to a specialized model carey smith et al 2014 in our analyses one of the main differences among the surrogate meteorological series concerns the wind speed variable this can be explained by the fact that the wind is the most turbulent of the meteorological variables being modeled the turbulence yields more frequent and important random fluctuations that are challenging for the statistical models as metgen relies on stochastic regression it can better cope with the wind speed fluctuations than the bias correction techniques these differences in the wind speed surrogate series might be related to the differences in the resulting surface water stress index estimates indeed the sparse energy balance model is particularly sensitive to wind speed especially high wind speed values that can lead to low si estimates these lower si values are particularly important as they are indicative of incipient water stress when the low and high stress condition series derived from the replicated series of metgen are used as inputs to the sparse model a form of confidence band is obtained that seems particularly reliable for the lower si values more work on the influence of each meteorological variable on the si estimates is required to improve these low and high stress condition series in order to yield proper confidence bands besides the daily evapotranspiration estimates etd are not very sensitive to which meteorological variables are used as inputs in the sparse model this lack of sensitivity may be explained by the fact that an extrapolation scheme that might act as a form of filter of the differences is applied to obtain daily values from instantaneous ones however the sensitivity of the sparse model should be explored further 7 conclusion the framework of metgen provides a solution to the task of obtaining a representative sub daily multi variable meteorological series which mimic the observation series but in which gaps are filled and whose simulation period can extend the observation period it relies on glms to model the conditional distributions with large scale variables used as covariates these large scale variables derived from era5 reanalyses in our application are useful to provide a temporal dynamic of the weather they may present in some cases important biases that can be reduced thanks to the glms and the introduction of other covariates metgen was compared to two bias correction techniques applied on the anomalies of seasonal diurnal cycles in contrast to the bias correction techniques metgen can exploit the observations series from nearby gauged stations having a relatively similar climatic behavior and can generate several replications of the same series to allow uncertainty assessment nevertheless metgen requires a careful selection of the conditional distribution models and of the covariate sets and the simulation can be slow while the bias correction techniques are more straightforward to implement and fast to run the analyses performed in this work provide a two pronged evaluation of the surrogate series generated by metgen and the two bias correction techniques firstly the evaluation is carried out in terms of the ability to reproduce several statistical properties of the meteorological variables and secondly in terms of surface water stress estimation when the series serve as inputs in the sparse energy balance model although the performance of the two bias correction techniques was similar for some criteria to metgen the evaluation in terms of the surface water stress application tends to indicate that the surrogate series generated by metgen may be used to deduce reliable confidence intervals especially for the lower si values that are important for early drought detection perspectives for this work include the study of the sensitivity of metgen to the choice of the dependence graph that serves to simplify the decomposition of the multivariate distribution into a product of conditional univariate distributions in addition the introduction of a mechanism to explicitly model spatial dependence would certainly be a useful development see verdin et al 2018 also it would be interesting to evaluate metgen in diverse climatic conditions e g a coastal area and for other meteorological variables e g precipitation the two main challenges for metgen implementation are to find representative enough large scale variables and to perform a thorough selection of the glms and of the optional covariates last more thorough analyses pertaining to agricultural drought monitoring should be performed declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors gratefully acknowledge financial support by the lmi naila the phc utique amande a french tunisian program eranetmed chaams and the european prima project altos we would also wish to thank all the technical teams of the ird and inat for their strong collaboration and support 
25566,the inowas decision support system dss is an open source web service for groundwater management applications in particular managed aquifer recharge it is accessible via the interactive graphical user interface gui running in a standard web browser and includes tools of varying complexity the platform provides the user with features to set up run and analyse numerical groundwater flow and transport models based on the usgs modflow family and the possibility to set up run and compare various groundwater management scenarios the applicability of the web based approach is demonstrated by the simulation of groundwater flow at a case study site in pirna germany keywords cloud computing modflow groundwater management scenario analysis abbreviations gui graphical user interface dss decision support system mar managed aquifer recharge crud create read update and delete rmse root mean square error 1 software availability software name inowas dss development team ralf junghanns robert schlick jana glass jana sallwey catalin stefan year first available 2017 software required web browser google chrome recommended or safari availability the inowas dss is accessible via web browser under https www inowas com free user registration required the inowas code can also be accessed through the github repository https github com inowas cost free license inowas dss is released under a gnu general public license version 2 june 1991 https www gnu org licenses oldlicenses gpl 2 0 en html 2 introduction groundwater systems are prone to overexploitation worldwide the rising water demand in combination with urbanization climate and land use change poses an increasing stress on the worldwide groundwater resources bouwer 2002 as a consequence groundwater levels are frequently declining leading e g to land subsidence salinization and water shortages hence a sustainable management of groundwater resources is necessary to mitigate the water related issues for that purpose suitable and innovative tools are needed numerical tools have been proven valuable to optimize the groundwater management and to evaluate various management and prediction scenarios for decades existing tools cover mainly desktop based software such as modflow harbaugh 2005 or feflow diersch and kolditz 2002 to evaluate the groundwater situation in recent years web based tools and online databases with focus on water resources have been considerably developed e g evans et al 2020 jones et al 2015 swain et al 2015 especially the usefulness of cloud based decision support systems based on recent and historic water data in an accessible web based interface with interactive mapping capabilities has been shown jones et al 2015 quite a range of systems exist for web based well or streamflow data mapping analysis and forecasting e g evans et al 2020 sit et al 2021 swain et al 2016 interactive web based simulation tools enable the configuration of model simulations and the simulation run on a server with subsequent visualization in a web browser and have been developed with various purposes these include e g the assessment of the groundwater pollution risk sege et al 2018 or the surface water quality walker and chapra 2014 the modelling of catchment hydrology rajib et al 2016 and the simulation of global water scarcity adaptation scenarios straatsma et al 2020 up to date the application of interactive web based numerical groundwater simulation tools is limited lui et al 2010 developed a service to run modflow models in a commercial cloud to enable easily scalable distributed computing of large batch runs or stochastic analysis mainly for educational purposes hu et al 2015 developed a web application to demonstrate the impact of groundwater pumping on the water table and streamflow with the help of the web based interface students can set up pumping wells invoke the execution of the simple modflow model and view the results in the web browser hu et al 2015 pedrazzini and kinzelbach 2016 developed an online groundwater model based on modflow and the shinyapplication of rstudio for the heihe basin china tailored to the needs of the local water authorities in the interactive shell the users were able to change irrigation parameters and by this means experiment with management alternatives pedrazzini and kinzelbach 2016 jones et al 2015 developed a web based interface for a modflow model in the northern utah valley usa to evaluate proposed well extraction changes in an automated way the cloud based application consists of the modflow model an arcgis geoprocessing workflow and the web based interface jones et al 2015 the interface provides the user with tools to modify the modflow wel input file initiate the execution of the modified model and analyse the model results allowing the user to evaluate new well permits in a simplified way jones et al 2015 all described applications have in common that the modflow model is developed desktop based and the users can only change selected input files before the execution of the model is initiated in contrast to that we present a new approach where the numerical groundwater flow and transport model can be completely set up run and analysed in a web based environment called the inowas dss the web based inowas dss was specifically developed to solve issues related to managed aquifer recharge but can also be used for general groundwater management aspects it consists of a toolbox with various empirical analytical and numerical tools glass 2019 glass et al 2018 sallwey et al 2018 stefan and ansems 2018 it is an open access web service where the whole workflow is directly managed in a web browser without the need to install additional software or plug ins on the computer this allows the easy collaboration among various users from remote locations and overcomes costly software license and high computer requirements the user specific projects and data can be accessed via a standard web browser from anywhere in the world this fosters the direct collaboration and simplifies the communication between researchers and decision makers by making the information quickly available everywhere needed this paper presents a new approach for numerical groundwater flow and transport modelling on the freely accessible web based inowas dss the platform provides users with features to set up run and analyse numerical groundwater flow and transport models based on the usgs modflow family and the possibility to set up run and compare various groundwater management scenarios the application of the web based approach is demonstrated by modelling the groundwater flow at a test field in pirna germany 3 inowas platform 3 1 technical details all functionalities of the inowas dss can be accessed by the user friendly graphical user interface gui utilizing a standard web browser google chrome recommended this in combination with powerful server capabilities enables the browser based and reliable simulations the platform is accessible under the following link www inowas com to access the full functionalities of the platform free user registration is required a special feature includes the account based simulations where the current work status can be saved at any time for later continuation and can be shared with other users comprehensive tool documentations including the theoretical background the restrictions and equations as well as default examples are accessible via a link in the navigation bar and the general web site for more details see glass et al 2018 3 1 1 user interface after user registration and login on the main web page the user can access the personal dashboard which provides an overview of all available tools and the personal projects which encompass saved tool runs of a specific user on the personal dashboard a new project can be started an existing project opened or a project can be cloned as a basis for a new project the confidentiality type can be either set to private where the project is only available to the project owner or public where any registered user can access and utilize the saved data hence on the one hand sensible or restricted data can be protected but on the other hand collaboration between different users is possible 3 1 2 technical infrastructure the infrastructure of the inowas dss consists of three main components client webserver and worker microservices fig 1 the front end part called client is based on the javascript library reactjs 1 1 https reactjs org 19 10 2021 which is used to create graphical user interfaces gui reactjs has been created by facebook and is a widely used open source library for creating user interfaces for complex web applications the core of reactjs is the concept of developing reusable components which can be connected with each other to form a bigger application redux 2 2 https redux js org license 19 10 2021 describes an architecture to connect multiple react components with each other the whole application state is held in a single immutable state tree the management of state data is happening only functional in so called reducers for visualization of data dynamic web content is rendered with react and d3 js 3 3 https d3js org 19 10 2021 d3 data driven documents the framework offers the creation of svg graphics and dynamic manipulation of data for the visualization of geodata inowas dss uses the open source library leaflet 4 4 https leafletjs com 19 10 2021 the webserver is a linux computer which can be accessed via the internet tcp ip it accepts connections from outside and understands the http and https protocols it is the central point of the inowas dss and manages requests from clients and workers it stores data runs calculations and delivers content and results the inowas dss uses nginx 5 5 https www nginx com 19 10 2021 a web server which is slim and agile in comparison to other technologies it was designed with the requirement to only need a few resources even under high loads symfony 6 6 https symfony com 19 10 2021 is an open source php web application framework it provides the rest representational state transfer interface and is the container for the domain logic of the project postgresql 7 7 https www postgresql org 19 10 2021 in combination with its postgis 8 8 https postgis net 19 10 2021 expansion is used as a relational database for processing and storing data including geodata worker microservices are a cloud based and scalable amount of linux computers which support different tools of the inowas dss the datadropper is a python based microservice to handle text files e g json or ascii files and stores them with a unique hash key to make them readable when needed users can access files via post requests and receive stored information the python based geoprocessing web service can read various raster file formats and is even able to scale them raster data can be accessed by post requests with the help of flopy bakker et al 2016 numerical groundwater models based on the usgs modflow harbaugh 2005 can be created and calculated the configuration file necessary for setting up a modflow model is created in the user interface client and sent to the microservice via post requests calculations are being handled by workers so it is possible to run multiple simulations at the same time the state of a simulation as well as simulation results can be requested via the api the microservices and the webserver are separate entities because of different requirements of the system the system setup was developed to facilitate the connection with external microservices which are specialised applications with manifold tasks realized in various different programming languages json as a common data transfer protocol is used between the application webserver and microservices the microservices can be scaled horizontally which means a high number of the same application is doing tasks in parallel e g calculation of a high number of parameterised models additional microservices can be added easily at a later stage allowing flexibility in the future development of the inowas dss 3 2 implemented tools in total 18 tools are currently implemented on the inowas dss ranging from empirical data driven tools to tools based on analytical or numerical equations the focus of the inowas dss is to provide tools for the assessment management and operation of managed aquifer recharge mar facilities mar is one of the most effective methods to augment groundwater resources and allows to maintain enhance and secure stressed groundwater systems and protect and improve water quality for ecological and human uses dillon et al 2020 2019 the implemented tools on the inowas platform provide the user with workflows of different levels of complexity to assess the user specific groundwater management issue implemented empirical tools cover mainly data driven query tools and integrate e g the global mar portal stefan 2015 the gis multi criteria decision analysis database for mar site selection sallwey et al 2018 a tool for mar method selection and a tool to select a suitable model to evaluate a specific mar related issue glass 2019 tools based on analytical equations encompass e g the analysis of the travel time through an unconfined aquifer pumping induced river drawdown groundwater mounding underneath an infiltration basin and 1d transport glass et al 2018 in addition the core of the inowas platform provides modflow based simulations using numerical groundwater flow and transport models 4 numerical inowas tools the numerical inowas dss tools provide features to setup run calibrate and visualise the results of a numerical density driven groundwater flow and transport model with the possibility to develop run and compare various simulation scenarios the numerical simulation core of the system is based on the open source software modflow 2005 harbaugh 2005 for groundwater flow mt3dms zheng and wang 1999 for solute transport and seawat langevin et al 2008 for density driven flow the implementation is based on the open source library flopy bakker et al 2016 a python library with the goal to script modflow related models with the help of flopy model input files are generated modflow executables including modflow 2005 modflow nwt niswonger et al 2011 modpath pollock 2016 seawat and mt3dms are run and simulation results are loaded and plotted bakker et al 2016 flopy helps to describe and calculate the models remotely on the inowas platform by providing a scriptable easy configurable object oriented abstraction layer for the application the modflow specific input data has to be transferred to the webserver where the size of the model and the internet connection of the user are time sensitive factors which increase the total calculation time on the inowas platform but not the runtime of the model itself to counter balance this limitation every part in the system can be scaled horizontally and vertically vertical scaling refers to the upgrade of the central computer power e g storage or cpu and horizontal scaling to the parallel calculation by a high number of workers the main webserver where data user data project data geodata is stored is a simple crud create read update and delete application which can take a lot of requests the client user browser maps and processes geodata into the modflow specific data format cell based data per modflow package and sends it to the modflow calculation service crud the modflow calculation service loads stores and processes modflow specific data to calculate and add a new job to the queue the queue is realized as a database table where all new and unfinished jobs are noted down the workers are checking this table regularly for new entries the number of calculation jobs can be increased with the number of users so there is no user limit longer processing times and even the break up of the processing of modflow packages can occur when big modflow models are created on older computers due to limited data conversion into modflow format by outdated browsers one solution is to reduce the number of cells and time steps e g by using local grid refinement at the moment there is no quantification of runtime available but it is planned to add a calculation time estimation in addition a performance test of the user s web browser or the processing of single modflow packages instead of all the project at once is planned to be added in the future to reduce the limitations associated with big models modflow packages were integrated modular on the inowas platform so that additional packages can be easily integrated in the future table 1 the default configuration comprises basic packages bas6 dis lpf bcf6 oc various boundary conditions chd fhb ghb rch wel riv evt drn and different solvers pcg de4 in addition solute transport as well as variable density flow can be added to the numerical groundwater flow model by using the corresponding mt3dms and seawat packages adv btn dsp gcg ssm vdf vsc the user interface on the inowas platform is divided into four main steps model setup calculation results and calibration fig 2 in addition scenarios can be setup run and compared a detailed description of the implemented tools and its functionalities is provided on the online documentation pages of the inowas platform https inowas com tools 4 1 model setup the menu model setup provides features to upload or define all relevant model parameters including the spatial and time discretization the soil layers top and bottom extent and soil parameters hydraulic conductivities specific storage specific yield the boundary conditions such as riv rch wel and chd the head observations for calibration and the transport and variable density parameters if applicable depending on the parameter type the values can be defined by direct value input uploaded from an existing raster file defined via zones or imported from other projects spatial extends of points wel lines riv chd or areal boundaries rch can be uploaded via json files a standard data interchange format or defined with the help of the gis functionalities of the platform time series data can be directly input or uploaded via csv files for more details see the online documentation 4 2 calculation the calculation section provides the user with the possibilities to change package related parameters for modflow and if enabled mt3dms and seawat in the sub section run calculation the simulation is started and the simulation status as well as the calculation logs and modflow input and output files are visualised an option to download all relevant model files is integrated too 4 3 results the results section provides the user with features to visualise the simulation results which include cross sections of groundwater head and drawdown and time series data at each stress period and layer in addition the user can evaluate all input and output fluxes of the model in the volumetric budget view as cumulative volumes or rates per stress period from here the user can start a scenario analysis see section 3 1 5 4 4 calibration in the case that head observation data was included in the model setup the calibration section provides the user with calculated statistics to evaluate the quality of the numerical simulation this includes e g residuals the coefficient of determination r2 and the standard error of determination but also graphs showing the simulated versus observed heads the weighted residuals versus the simulated heads and the ranked residuals against normal probability 4 5 scenario setup management and comparison a distinct feature of the inowas platform comprises the scenario manager which was developed to evaluate new management options such as future developments induced e g by land use change urbanization or climate change the features allow a user to easily modify and compare various model runs fig 3 a base model can be cloned edited and re run to create a new scenario and subsequently compared in the modflow model scenario manager the cross sectional view provides the user with features to visualise the groundwater levels of two or more simulation runs at a specific stress period the difference view helps to calculate the head difference of two specific scenarios in the time series view the hydraulic head or drawdown over time can be visualised for multiple scenarios the various analysis options help the user to evaluate several management options and identify the best management solution for his specific groundwater related issue 5 demonstration of the platform at pirna germany to demonstrate the capabilities of the web based software and to validate the results of the simulation versus a conventional offline calculation a numerical groundwater flow model was created calibrated and run on the inowas dss platform the modelled area covers the research test field of the technische universität dresden in pirna germany which has been used for various groundwater related investigations for over 15 years dietze and dietrich 2012 händel et al 2016 the model is based on preliminary works of bista 2015 and li 2014 who set up the initial groundwater flow model hence referred to as offline model using modelmuse winston 2009 a desktop based gui for u s geological survey usgs groundwater models the model was then adapted for the implementation on the web based inowas dss platform in order to test the capabilities of the platform and to validate the simulation results 5 1 pirna site description the test field is situated about 20 km southeast of the city of dresden germany coordinates 50 57 56 5 n 13 55 23 2 e right on the northern bank of the river elbe due to its close proximity to the river strong river groundwater interactions have been observed as well as high water table fluctuations within short time periods händel et al 2016 dietze and dietrich 2012 conducted a detailed characterized of the unconsolidated sediments in the upper 15 m below surface down to the quaternary bedrock the aquifer is comprised of interbedded strata of medium to coarse gravel followed by fine sands of varying thickness dietze and dietrich 2012 a high variability in hydraulic parameters and lithology is present due to the fluvial formation händel et al 2016 the upper layer up to approximately 2 4 m below surface consists of anthropogenic fillings mixed with fine alluvial deposits händel et al 2016 below that a low conductivity layer with silty sediments and a hydraulic conductivity of about 5 10 4 m s dietze and dietrich 2012 händel et al 2016 is underlain by about 4 5 m thick highly heterogeneous but in general highly conductive sands and gravels händel et al 2016 at about 15 m below surface lies the impervious basement formed of marine sediment rocks mainly sand and mudstone of the upper cretaceous age dietze and dietrich 2012 the lithological units as well as the aquifer dip slightly towards the riverbank li 2014 5 2 conceptual model to address the high heterogeneity the model area covers 25 200 m2 divided into 90 rows and 70 columns the cells of the offline model are 2 m high and 2 m wide the model consists of 5 layers with varying thickness reaching from the surface to the bedrock at a depth of about 15 m below surface the simulation time starts on june 2 2014 and ends on january 20 2015 the first stress period is simulated steady state followed by transient daily stress periods the flow in the model domain is mainly influenced by the elbe river specified as river boundary condition riv at the southern boundary of the model domain fig 4 data for the river stage was derived from a public monitoring station which is located about 500 m upstream of the pilot site considering a gradient of 0 25 m km the streambed conductance was assumed constant 43 2 m d using a riverbed thickness of 1 m and a streambed hydraulic conductivity of 0 864 m d the flow and head boundary condition fhb is specified along the northern boundary of the model domain to be able to consider the changing flow direction with the help of darcy s law the flux along the boundary was calculated using the measured piezometric head at the monitoring well g23 and an additional observation well g17 bista 2015 in addition recharge rch is applied to the highest active cell at a rate of 0 000432 m d derived from the precipitation during the study period and a coefficient of 0 2 for calibration the groundwater level measurements at the g wells were used to fit the hydraulic conductivity of the five layers the recharge and the riverbed conductance a simulation scenario included pumping from well g4 at a constant rate of 200 m³ day to evaluate the influence of anthropogenic activities on the groundwater heads 5 3 web based model implementation and results of the simulations the numerical groundwater flow model was implemented on the web based inowas dss platform by manually uploading spatial datasets and time series in json and csv formats or as raster image files this allowed the web based reconstruction of the model s spatial domain layers thickness defined by top and bottom elevations soil hydraulic parameters boundary conditions as well as location of observation wells and measured groundwater piezometric levels fig 2 after setting up and running the pirna model on the inowas dss the cumulative budget groundwater heads and calibration statistics were used to validate the tool implementation the statistical calibration parameters show only small deviations between the offline and online model the final offline model had a root mean square error rmse of 0 17 m a normalized rmse of 6 and a coefficient of determination r2 of 0 94 bista 2015 whereas the inowas model had a rmse of 0 18 m a nrmse of 6 5 and a r2 of 0 95 fig 5 the comparison of the cumulative budget of the offline and the web based model shows considerable deviations fig 6 the budget differences are due to the web based implementation on the inowas dss platform which required some adaptations of the offline model the coordinate system had to be transformed from gauβ krüger bessel potsdam zone 3 5685 to the coordinate system wgs84 4326 used in the inowas dss causing 1 the relative shift of some locations as well as displacements of boundaries and observation wells and 2 the change of grid size from 2 m 2m to 2 5 m 2 6 m especially the second point leads to the high discrepancies in the budget between the offline model and the inowas model the groundwater levels at the pirna test field closely follow the elbe river water level due to the low distance to the river fig 7 except during high discharge of the river at the end of the simulation period both the offline and online models are slightly underestimating the groundwater levels in the aquifer during time period of high flow the groundwater levels in the aquifer are overestimated by both groundwater flow models compared to the observations fig 8 this could be due to the very fast response of the aquifer to the rising water table in the river in combination with the low time resolution of the measurements daily the various monitoring wells show minor differences between each other which could be caused by local heterogeneities of the inhomogeneous aquifer depending on the river discharge the groundwater flow is either towards the river if the elbe river discharge is low or river water is infiltrating into the aquifer if the river discharge is high fig 9 5 4 scenario analysis a scenario was created to simulate water being pumped from the well g4 at a constant rate of 200 m³ day the scenario manager on the inowas dss platform helps to evaluate the scenario and compare it with the base model figs 3 and 10 the groundwater depression cone due to pumping is clearly visible although groundwater heads only decline by 0 2 m due to pumping 6 discussion and conclusions the application of the newly developed web based groundwater modelling tools of the inowas dss platform was demonstrated with the help of a modelling case study in pirna germany the developed scenario manager tool is to the knowledge of the authors unique in providing features for easy scenario analysis additional simulation scenarios can be created by duplication of the base model boundary conditions such as the pumping rate of a single well can be easily modified and the results compared in the scenario manager view where model results can be viewed side by side or in a time series graph and scenario differences can be calculated the web based tools provide a new framework for groundwater modelling overcoming the typical limitations of desktop based software including e g software installation manual updating system dependence and limited data accessibility the web based implementation fosters the international collaboration among various users and user groups including researchers and decision makers the open access approach reduces application barriers often occurring due to high licensing fees tutorials and online documentations allow new users to get easily familiar with the inowas dss platform which is further supported by frequent training activities in the form of online workshops in contrast to existing web based implementations which focus rather on education or a specific case study the workflow is flexible can be saved at any time for later continuation and also complex models can be set up and simulated the new framework is promising especially in times of travel restrictions and home office as data can be easily accessed remotely in the future a performance test of the user s web browser or the processing of single modflow packages instead of all the project at once is planned to reduce the limitations associated with big models high number of time steps and cells in combination with older user computers which can lead to long processing times of modflow packages or even the break up of data conversion also it is planned to extent the usability of the tool by implementing more modflow packages e g uzf to be able to apply the service to diverse case study settings also the implementation of additional post processing features such as modpath for particle tracking and the possibility to compare the budget for various scenarios in t07 is planned declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the german federal ministry of education and research bmbf grant no 01ln1311a junior research group inowas in addition the authors would like to thank the eu and the bmbf the french national research agency anr the paraíba research foundation fapesq the pernambuco research foundation facepe and the cyprus research promotion foundation rpf for funding provided in the frame of the collaborative international consortium smart control financed under the 2017 joint call developed by the water challenges for a changing world joint programme initiative water jpi also parts of this study were supported by the digires project funded within the eranet lac 3rd multi thematic joint call 2017 2018 by bmbf germany f r s fnrs belgium cnpq brazil concyt guatemala and fonci cuba 
25566,the inowas decision support system dss is an open source web service for groundwater management applications in particular managed aquifer recharge it is accessible via the interactive graphical user interface gui running in a standard web browser and includes tools of varying complexity the platform provides the user with features to set up run and analyse numerical groundwater flow and transport models based on the usgs modflow family and the possibility to set up run and compare various groundwater management scenarios the applicability of the web based approach is demonstrated by the simulation of groundwater flow at a case study site in pirna germany keywords cloud computing modflow groundwater management scenario analysis abbreviations gui graphical user interface dss decision support system mar managed aquifer recharge crud create read update and delete rmse root mean square error 1 software availability software name inowas dss development team ralf junghanns robert schlick jana glass jana sallwey catalin stefan year first available 2017 software required web browser google chrome recommended or safari availability the inowas dss is accessible via web browser under https www inowas com free user registration required the inowas code can also be accessed through the github repository https github com inowas cost free license inowas dss is released under a gnu general public license version 2 june 1991 https www gnu org licenses oldlicenses gpl 2 0 en html 2 introduction groundwater systems are prone to overexploitation worldwide the rising water demand in combination with urbanization climate and land use change poses an increasing stress on the worldwide groundwater resources bouwer 2002 as a consequence groundwater levels are frequently declining leading e g to land subsidence salinization and water shortages hence a sustainable management of groundwater resources is necessary to mitigate the water related issues for that purpose suitable and innovative tools are needed numerical tools have been proven valuable to optimize the groundwater management and to evaluate various management and prediction scenarios for decades existing tools cover mainly desktop based software such as modflow harbaugh 2005 or feflow diersch and kolditz 2002 to evaluate the groundwater situation in recent years web based tools and online databases with focus on water resources have been considerably developed e g evans et al 2020 jones et al 2015 swain et al 2015 especially the usefulness of cloud based decision support systems based on recent and historic water data in an accessible web based interface with interactive mapping capabilities has been shown jones et al 2015 quite a range of systems exist for web based well or streamflow data mapping analysis and forecasting e g evans et al 2020 sit et al 2021 swain et al 2016 interactive web based simulation tools enable the configuration of model simulations and the simulation run on a server with subsequent visualization in a web browser and have been developed with various purposes these include e g the assessment of the groundwater pollution risk sege et al 2018 or the surface water quality walker and chapra 2014 the modelling of catchment hydrology rajib et al 2016 and the simulation of global water scarcity adaptation scenarios straatsma et al 2020 up to date the application of interactive web based numerical groundwater simulation tools is limited lui et al 2010 developed a service to run modflow models in a commercial cloud to enable easily scalable distributed computing of large batch runs or stochastic analysis mainly for educational purposes hu et al 2015 developed a web application to demonstrate the impact of groundwater pumping on the water table and streamflow with the help of the web based interface students can set up pumping wells invoke the execution of the simple modflow model and view the results in the web browser hu et al 2015 pedrazzini and kinzelbach 2016 developed an online groundwater model based on modflow and the shinyapplication of rstudio for the heihe basin china tailored to the needs of the local water authorities in the interactive shell the users were able to change irrigation parameters and by this means experiment with management alternatives pedrazzini and kinzelbach 2016 jones et al 2015 developed a web based interface for a modflow model in the northern utah valley usa to evaluate proposed well extraction changes in an automated way the cloud based application consists of the modflow model an arcgis geoprocessing workflow and the web based interface jones et al 2015 the interface provides the user with tools to modify the modflow wel input file initiate the execution of the modified model and analyse the model results allowing the user to evaluate new well permits in a simplified way jones et al 2015 all described applications have in common that the modflow model is developed desktop based and the users can only change selected input files before the execution of the model is initiated in contrast to that we present a new approach where the numerical groundwater flow and transport model can be completely set up run and analysed in a web based environment called the inowas dss the web based inowas dss was specifically developed to solve issues related to managed aquifer recharge but can also be used for general groundwater management aspects it consists of a toolbox with various empirical analytical and numerical tools glass 2019 glass et al 2018 sallwey et al 2018 stefan and ansems 2018 it is an open access web service where the whole workflow is directly managed in a web browser without the need to install additional software or plug ins on the computer this allows the easy collaboration among various users from remote locations and overcomes costly software license and high computer requirements the user specific projects and data can be accessed via a standard web browser from anywhere in the world this fosters the direct collaboration and simplifies the communication between researchers and decision makers by making the information quickly available everywhere needed this paper presents a new approach for numerical groundwater flow and transport modelling on the freely accessible web based inowas dss the platform provides users with features to set up run and analyse numerical groundwater flow and transport models based on the usgs modflow family and the possibility to set up run and compare various groundwater management scenarios the application of the web based approach is demonstrated by modelling the groundwater flow at a test field in pirna germany 3 inowas platform 3 1 technical details all functionalities of the inowas dss can be accessed by the user friendly graphical user interface gui utilizing a standard web browser google chrome recommended this in combination with powerful server capabilities enables the browser based and reliable simulations the platform is accessible under the following link www inowas com to access the full functionalities of the platform free user registration is required a special feature includes the account based simulations where the current work status can be saved at any time for later continuation and can be shared with other users comprehensive tool documentations including the theoretical background the restrictions and equations as well as default examples are accessible via a link in the navigation bar and the general web site for more details see glass et al 2018 3 1 1 user interface after user registration and login on the main web page the user can access the personal dashboard which provides an overview of all available tools and the personal projects which encompass saved tool runs of a specific user on the personal dashboard a new project can be started an existing project opened or a project can be cloned as a basis for a new project the confidentiality type can be either set to private where the project is only available to the project owner or public where any registered user can access and utilize the saved data hence on the one hand sensible or restricted data can be protected but on the other hand collaboration between different users is possible 3 1 2 technical infrastructure the infrastructure of the inowas dss consists of three main components client webserver and worker microservices fig 1 the front end part called client is based on the javascript library reactjs 1 1 https reactjs org 19 10 2021 which is used to create graphical user interfaces gui reactjs has been created by facebook and is a widely used open source library for creating user interfaces for complex web applications the core of reactjs is the concept of developing reusable components which can be connected with each other to form a bigger application redux 2 2 https redux js org license 19 10 2021 describes an architecture to connect multiple react components with each other the whole application state is held in a single immutable state tree the management of state data is happening only functional in so called reducers for visualization of data dynamic web content is rendered with react and d3 js 3 3 https d3js org 19 10 2021 d3 data driven documents the framework offers the creation of svg graphics and dynamic manipulation of data for the visualization of geodata inowas dss uses the open source library leaflet 4 4 https leafletjs com 19 10 2021 the webserver is a linux computer which can be accessed via the internet tcp ip it accepts connections from outside and understands the http and https protocols it is the central point of the inowas dss and manages requests from clients and workers it stores data runs calculations and delivers content and results the inowas dss uses nginx 5 5 https www nginx com 19 10 2021 a web server which is slim and agile in comparison to other technologies it was designed with the requirement to only need a few resources even under high loads symfony 6 6 https symfony com 19 10 2021 is an open source php web application framework it provides the rest representational state transfer interface and is the container for the domain logic of the project postgresql 7 7 https www postgresql org 19 10 2021 in combination with its postgis 8 8 https postgis net 19 10 2021 expansion is used as a relational database for processing and storing data including geodata worker microservices are a cloud based and scalable amount of linux computers which support different tools of the inowas dss the datadropper is a python based microservice to handle text files e g json or ascii files and stores them with a unique hash key to make them readable when needed users can access files via post requests and receive stored information the python based geoprocessing web service can read various raster file formats and is even able to scale them raster data can be accessed by post requests with the help of flopy bakker et al 2016 numerical groundwater models based on the usgs modflow harbaugh 2005 can be created and calculated the configuration file necessary for setting up a modflow model is created in the user interface client and sent to the microservice via post requests calculations are being handled by workers so it is possible to run multiple simulations at the same time the state of a simulation as well as simulation results can be requested via the api the microservices and the webserver are separate entities because of different requirements of the system the system setup was developed to facilitate the connection with external microservices which are specialised applications with manifold tasks realized in various different programming languages json as a common data transfer protocol is used between the application webserver and microservices the microservices can be scaled horizontally which means a high number of the same application is doing tasks in parallel e g calculation of a high number of parameterised models additional microservices can be added easily at a later stage allowing flexibility in the future development of the inowas dss 3 2 implemented tools in total 18 tools are currently implemented on the inowas dss ranging from empirical data driven tools to tools based on analytical or numerical equations the focus of the inowas dss is to provide tools for the assessment management and operation of managed aquifer recharge mar facilities mar is one of the most effective methods to augment groundwater resources and allows to maintain enhance and secure stressed groundwater systems and protect and improve water quality for ecological and human uses dillon et al 2020 2019 the implemented tools on the inowas platform provide the user with workflows of different levels of complexity to assess the user specific groundwater management issue implemented empirical tools cover mainly data driven query tools and integrate e g the global mar portal stefan 2015 the gis multi criteria decision analysis database for mar site selection sallwey et al 2018 a tool for mar method selection and a tool to select a suitable model to evaluate a specific mar related issue glass 2019 tools based on analytical equations encompass e g the analysis of the travel time through an unconfined aquifer pumping induced river drawdown groundwater mounding underneath an infiltration basin and 1d transport glass et al 2018 in addition the core of the inowas platform provides modflow based simulations using numerical groundwater flow and transport models 4 numerical inowas tools the numerical inowas dss tools provide features to setup run calibrate and visualise the results of a numerical density driven groundwater flow and transport model with the possibility to develop run and compare various simulation scenarios the numerical simulation core of the system is based on the open source software modflow 2005 harbaugh 2005 for groundwater flow mt3dms zheng and wang 1999 for solute transport and seawat langevin et al 2008 for density driven flow the implementation is based on the open source library flopy bakker et al 2016 a python library with the goal to script modflow related models with the help of flopy model input files are generated modflow executables including modflow 2005 modflow nwt niswonger et al 2011 modpath pollock 2016 seawat and mt3dms are run and simulation results are loaded and plotted bakker et al 2016 flopy helps to describe and calculate the models remotely on the inowas platform by providing a scriptable easy configurable object oriented abstraction layer for the application the modflow specific input data has to be transferred to the webserver where the size of the model and the internet connection of the user are time sensitive factors which increase the total calculation time on the inowas platform but not the runtime of the model itself to counter balance this limitation every part in the system can be scaled horizontally and vertically vertical scaling refers to the upgrade of the central computer power e g storage or cpu and horizontal scaling to the parallel calculation by a high number of workers the main webserver where data user data project data geodata is stored is a simple crud create read update and delete application which can take a lot of requests the client user browser maps and processes geodata into the modflow specific data format cell based data per modflow package and sends it to the modflow calculation service crud the modflow calculation service loads stores and processes modflow specific data to calculate and add a new job to the queue the queue is realized as a database table where all new and unfinished jobs are noted down the workers are checking this table regularly for new entries the number of calculation jobs can be increased with the number of users so there is no user limit longer processing times and even the break up of the processing of modflow packages can occur when big modflow models are created on older computers due to limited data conversion into modflow format by outdated browsers one solution is to reduce the number of cells and time steps e g by using local grid refinement at the moment there is no quantification of runtime available but it is planned to add a calculation time estimation in addition a performance test of the user s web browser or the processing of single modflow packages instead of all the project at once is planned to be added in the future to reduce the limitations associated with big models modflow packages were integrated modular on the inowas platform so that additional packages can be easily integrated in the future table 1 the default configuration comprises basic packages bas6 dis lpf bcf6 oc various boundary conditions chd fhb ghb rch wel riv evt drn and different solvers pcg de4 in addition solute transport as well as variable density flow can be added to the numerical groundwater flow model by using the corresponding mt3dms and seawat packages adv btn dsp gcg ssm vdf vsc the user interface on the inowas platform is divided into four main steps model setup calculation results and calibration fig 2 in addition scenarios can be setup run and compared a detailed description of the implemented tools and its functionalities is provided on the online documentation pages of the inowas platform https inowas com tools 4 1 model setup the menu model setup provides features to upload or define all relevant model parameters including the spatial and time discretization the soil layers top and bottom extent and soil parameters hydraulic conductivities specific storage specific yield the boundary conditions such as riv rch wel and chd the head observations for calibration and the transport and variable density parameters if applicable depending on the parameter type the values can be defined by direct value input uploaded from an existing raster file defined via zones or imported from other projects spatial extends of points wel lines riv chd or areal boundaries rch can be uploaded via json files a standard data interchange format or defined with the help of the gis functionalities of the platform time series data can be directly input or uploaded via csv files for more details see the online documentation 4 2 calculation the calculation section provides the user with the possibilities to change package related parameters for modflow and if enabled mt3dms and seawat in the sub section run calculation the simulation is started and the simulation status as well as the calculation logs and modflow input and output files are visualised an option to download all relevant model files is integrated too 4 3 results the results section provides the user with features to visualise the simulation results which include cross sections of groundwater head and drawdown and time series data at each stress period and layer in addition the user can evaluate all input and output fluxes of the model in the volumetric budget view as cumulative volumes or rates per stress period from here the user can start a scenario analysis see section 3 1 5 4 4 calibration in the case that head observation data was included in the model setup the calibration section provides the user with calculated statistics to evaluate the quality of the numerical simulation this includes e g residuals the coefficient of determination r2 and the standard error of determination but also graphs showing the simulated versus observed heads the weighted residuals versus the simulated heads and the ranked residuals against normal probability 4 5 scenario setup management and comparison a distinct feature of the inowas platform comprises the scenario manager which was developed to evaluate new management options such as future developments induced e g by land use change urbanization or climate change the features allow a user to easily modify and compare various model runs fig 3 a base model can be cloned edited and re run to create a new scenario and subsequently compared in the modflow model scenario manager the cross sectional view provides the user with features to visualise the groundwater levels of two or more simulation runs at a specific stress period the difference view helps to calculate the head difference of two specific scenarios in the time series view the hydraulic head or drawdown over time can be visualised for multiple scenarios the various analysis options help the user to evaluate several management options and identify the best management solution for his specific groundwater related issue 5 demonstration of the platform at pirna germany to demonstrate the capabilities of the web based software and to validate the results of the simulation versus a conventional offline calculation a numerical groundwater flow model was created calibrated and run on the inowas dss platform the modelled area covers the research test field of the technische universität dresden in pirna germany which has been used for various groundwater related investigations for over 15 years dietze and dietrich 2012 händel et al 2016 the model is based on preliminary works of bista 2015 and li 2014 who set up the initial groundwater flow model hence referred to as offline model using modelmuse winston 2009 a desktop based gui for u s geological survey usgs groundwater models the model was then adapted for the implementation on the web based inowas dss platform in order to test the capabilities of the platform and to validate the simulation results 5 1 pirna site description the test field is situated about 20 km southeast of the city of dresden germany coordinates 50 57 56 5 n 13 55 23 2 e right on the northern bank of the river elbe due to its close proximity to the river strong river groundwater interactions have been observed as well as high water table fluctuations within short time periods händel et al 2016 dietze and dietrich 2012 conducted a detailed characterized of the unconsolidated sediments in the upper 15 m below surface down to the quaternary bedrock the aquifer is comprised of interbedded strata of medium to coarse gravel followed by fine sands of varying thickness dietze and dietrich 2012 a high variability in hydraulic parameters and lithology is present due to the fluvial formation händel et al 2016 the upper layer up to approximately 2 4 m below surface consists of anthropogenic fillings mixed with fine alluvial deposits händel et al 2016 below that a low conductivity layer with silty sediments and a hydraulic conductivity of about 5 10 4 m s dietze and dietrich 2012 händel et al 2016 is underlain by about 4 5 m thick highly heterogeneous but in general highly conductive sands and gravels händel et al 2016 at about 15 m below surface lies the impervious basement formed of marine sediment rocks mainly sand and mudstone of the upper cretaceous age dietze and dietrich 2012 the lithological units as well as the aquifer dip slightly towards the riverbank li 2014 5 2 conceptual model to address the high heterogeneity the model area covers 25 200 m2 divided into 90 rows and 70 columns the cells of the offline model are 2 m high and 2 m wide the model consists of 5 layers with varying thickness reaching from the surface to the bedrock at a depth of about 15 m below surface the simulation time starts on june 2 2014 and ends on january 20 2015 the first stress period is simulated steady state followed by transient daily stress periods the flow in the model domain is mainly influenced by the elbe river specified as river boundary condition riv at the southern boundary of the model domain fig 4 data for the river stage was derived from a public monitoring station which is located about 500 m upstream of the pilot site considering a gradient of 0 25 m km the streambed conductance was assumed constant 43 2 m d using a riverbed thickness of 1 m and a streambed hydraulic conductivity of 0 864 m d the flow and head boundary condition fhb is specified along the northern boundary of the model domain to be able to consider the changing flow direction with the help of darcy s law the flux along the boundary was calculated using the measured piezometric head at the monitoring well g23 and an additional observation well g17 bista 2015 in addition recharge rch is applied to the highest active cell at a rate of 0 000432 m d derived from the precipitation during the study period and a coefficient of 0 2 for calibration the groundwater level measurements at the g wells were used to fit the hydraulic conductivity of the five layers the recharge and the riverbed conductance a simulation scenario included pumping from well g4 at a constant rate of 200 m³ day to evaluate the influence of anthropogenic activities on the groundwater heads 5 3 web based model implementation and results of the simulations the numerical groundwater flow model was implemented on the web based inowas dss platform by manually uploading spatial datasets and time series in json and csv formats or as raster image files this allowed the web based reconstruction of the model s spatial domain layers thickness defined by top and bottom elevations soil hydraulic parameters boundary conditions as well as location of observation wells and measured groundwater piezometric levels fig 2 after setting up and running the pirna model on the inowas dss the cumulative budget groundwater heads and calibration statistics were used to validate the tool implementation the statistical calibration parameters show only small deviations between the offline and online model the final offline model had a root mean square error rmse of 0 17 m a normalized rmse of 6 and a coefficient of determination r2 of 0 94 bista 2015 whereas the inowas model had a rmse of 0 18 m a nrmse of 6 5 and a r2 of 0 95 fig 5 the comparison of the cumulative budget of the offline and the web based model shows considerable deviations fig 6 the budget differences are due to the web based implementation on the inowas dss platform which required some adaptations of the offline model the coordinate system had to be transformed from gauβ krüger bessel potsdam zone 3 5685 to the coordinate system wgs84 4326 used in the inowas dss causing 1 the relative shift of some locations as well as displacements of boundaries and observation wells and 2 the change of grid size from 2 m 2m to 2 5 m 2 6 m especially the second point leads to the high discrepancies in the budget between the offline model and the inowas model the groundwater levels at the pirna test field closely follow the elbe river water level due to the low distance to the river fig 7 except during high discharge of the river at the end of the simulation period both the offline and online models are slightly underestimating the groundwater levels in the aquifer during time period of high flow the groundwater levels in the aquifer are overestimated by both groundwater flow models compared to the observations fig 8 this could be due to the very fast response of the aquifer to the rising water table in the river in combination with the low time resolution of the measurements daily the various monitoring wells show minor differences between each other which could be caused by local heterogeneities of the inhomogeneous aquifer depending on the river discharge the groundwater flow is either towards the river if the elbe river discharge is low or river water is infiltrating into the aquifer if the river discharge is high fig 9 5 4 scenario analysis a scenario was created to simulate water being pumped from the well g4 at a constant rate of 200 m³ day the scenario manager on the inowas dss platform helps to evaluate the scenario and compare it with the base model figs 3 and 10 the groundwater depression cone due to pumping is clearly visible although groundwater heads only decline by 0 2 m due to pumping 6 discussion and conclusions the application of the newly developed web based groundwater modelling tools of the inowas dss platform was demonstrated with the help of a modelling case study in pirna germany the developed scenario manager tool is to the knowledge of the authors unique in providing features for easy scenario analysis additional simulation scenarios can be created by duplication of the base model boundary conditions such as the pumping rate of a single well can be easily modified and the results compared in the scenario manager view where model results can be viewed side by side or in a time series graph and scenario differences can be calculated the web based tools provide a new framework for groundwater modelling overcoming the typical limitations of desktop based software including e g software installation manual updating system dependence and limited data accessibility the web based implementation fosters the international collaboration among various users and user groups including researchers and decision makers the open access approach reduces application barriers often occurring due to high licensing fees tutorials and online documentations allow new users to get easily familiar with the inowas dss platform which is further supported by frequent training activities in the form of online workshops in contrast to existing web based implementations which focus rather on education or a specific case study the workflow is flexible can be saved at any time for later continuation and also complex models can be set up and simulated the new framework is promising especially in times of travel restrictions and home office as data can be easily accessed remotely in the future a performance test of the user s web browser or the processing of single modflow packages instead of all the project at once is planned to reduce the limitations associated with big models high number of time steps and cells in combination with older user computers which can lead to long processing times of modflow packages or even the break up of data conversion also it is planned to extent the usability of the tool by implementing more modflow packages e g uzf to be able to apply the service to diverse case study settings also the implementation of additional post processing features such as modpath for particle tracking and the possibility to compare the budget for various scenarios in t07 is planned declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the german federal ministry of education and research bmbf grant no 01ln1311a junior research group inowas in addition the authors would like to thank the eu and the bmbf the french national research agency anr the paraíba research foundation fapesq the pernambuco research foundation facepe and the cyprus research promotion foundation rpf for funding provided in the frame of the collaborative international consortium smart control financed under the 2017 joint call developed by the water challenges for a changing world joint programme initiative water jpi also parts of this study were supported by the digires project funded within the eranet lac 3rd multi thematic joint call 2017 2018 by bmbf germany f r s fnrs belgium cnpq brazil concyt guatemala and fonci cuba 
25567,coupled hydrologic hydraulic h h models have been widely applied for flood simulations yet the modern h h models suffer from one way and weak coupling and particularly disregarded run on infiltration which could compromise the model accuracy in this study we assess the h h model performance with and without re infiltration process in extreme flooding events results highlight that the re infiltration process should not be disregarded even in extreme flood simulations saturated hydraulic conductivity and antecedent soil moisture are found to be the prime contributors to such differences for the hurricane harvey event the model performance is verified against stream gauges and high water marks from which the re infiltration scheme increases the nash sutcliffe efficiency score by 140 on average and reduces maximum depth differences by 17 meanwhile the recent update of the crest imap model version 1 1 which incorporates two way coupling and re infiltration scheme is released for public access keywords flood inundation modeling run on infiltration re infiltration soil moisture hydrologic hydraulic modeling extreme flood events hurricane harvey bayou river basin 1 introduction flooding as a costly natural hazard has been increasingly threatening human lives and economies gourley et al 2017 hirabayashi et al 2013 li et al 2021a in the united states most billion dollar natural hazards are tied to either local or regional flooding making it the major cost to human society unfortunately under a warmer climate with anthropogenic pressure flood crises are likely to continue expanding as the flood frequency accelerates and flood magnitude rises bates et al 2021 hirabayashi et al 2013 li et al 2022a tabari 2020 triet et al 2020 swain et al 2020 viero et al 2019 to combat flood risks researchers have been developing hydrologic hydraulic models to deliver accurate and timely flood information for local communities and decision makers gourley et al 2017 in the united states two pronounced flood forecasting systems the nwm national water model cohen et al 2018 viterbo et al 2020 and flash flooded locations and simulated hydrographs project gourley et al 2017 yussouf et al 2020 are capable of both simulating real time floods and forecasting floods in a short range these large scale flood monitoring systems although claimed to offer inundation maps and predictions weaken their hydrodynamic simulation due to computational constraints for instance the nwm adopts the height above nearest drainage hand method to produce flood inundation maps along the river channels by mapping discharge to stage via rating curves johnson et al 2019 this conceptual method however overlooks the physics of floodwater propagation because of no flow dynamics being represented wing et al 2017 moreover it cannot simulate the pluvial flood which is a local effect caused by intense rainfall rates and does not normally occur along river channels bates et al 2021 more recently some emerging hydrodynamic models have been successfully deployed and evaluated at continental or global scales bates et al 2021 grimaldi et al 2019 sampson et al 2015 wing et al 2017 yamazaki et al 2011 these models simplify the full shallow water equation swe to speed up the flood simulation nevertheless they normally do not represent the hydrologic process well especially for the infiltration process which is proven to be critical in flood simulations li et al 2021b ni et al 2020 as such a coupled physically based hydrologic hydraulic h h model appears to be a better choice which takes the complementary advantages for accurate flood modeling dullo et al 2021 felder et al 2017 kim et al 2012 nguyen et al 2016 pontes et al 2017 sebastian et al 2021 readers are referred to teng et al 2017 and grimaldi et al 2019 for a detailed review of coupled models most of such models however adopt one way and weak coupling meaning that there is no interplay between the hydrologic component and hydraulic component bravo et al 2012 they normally produce surface runoff outputs first to drive the hydraulic model the recent development of the coupled routing and excess storage inundation mapping and prediction crest imap version 1 0 also uses this one way coupling strategy chen et al 2021 li et al 2021b two way coupling for the h h models has not hitherto been well recognized the accumulated surface water hydraulic feature along with excess surface runoff during a flood event in principle would alter infiltration rates whereby both the flood magnitudes and timings could differ therefore we activate the surface water infiltration along its way to downslope which is called run on infiltration or re infiltration in short smith and hebbert 1979 nahar et al 2004 zhang et al 2020 nahar et al 2004 defined this re infiltration as the infiltration of surface water that as it moves downslope encounters areas where moisture deficit has not yet been satisfied it is often ignored in rainfall runoff studies while it can be significant when the random nature of infiltration properties is taken into account corradini et al 1998 nahar et al 2004 smith and hebbert 1979 simulated the run on process with varying saturated hydraulic conductivity and rainfall rates and they reported that the effect of the run on process is to decrease the ponding time dramatically corradini et al 2002 compared models with and without re infiltration and they suggested that re infiltration greatly reduces surface flow and alters both rising and recession limbs of the hydrograph nahar et al 2004 emphasized the influence of re infiltration in hillslope hydrograph using the green ampt model with a 1d kinematic wave surface routing a recent study by zhang et al 2020 takes it one step further in which they applied the community model wrf hydro weather research forecasting model hydrological modeling system to explore the influence of rainfall rates topography soil types on the re infiltration process however none of these studies have considered the implication of re infiltration to hydrodynamic studies where the overland flow is driven by the 2d shallow water equation swe instead of 1d routing to do so we can obtain a more realistic view of how re infiltration plays a role in flood simulations during extreme flood events the infiltration process is often disregarded because the infiltration rates are relatively low compared to excess rainfall rates yet some studies claim that the infiltration process is critical to determine flood wave propagation such as arrival and dissipation especially in flat plain or regions with highly permeable soil media corradini et al 2002 mahapatra et al 2020 nahar et al 2004 li et al 2021b woolhiser et al 1996 a hydrodynamic model without infiltration is likely to overestimate flood depth kim et al 2012 li et al 2021b ni et al 2020 nevertheless it still remains unclear or at least not as clear as infiltration whether re infiltration is essential for h h models in extreme flood events in other words whether it is worth encapsulating such a scheme in modern flood simulation frameworks to our knowledge few studies have attempted to answer this question under the context of extreme flood events moreover further questions can arise as to what the determining factors are during such a process and how it interacts with both flood magnitude and dynamics in light of these questions the objectives of this study are to explore 1 the effectiveness and importance of the re infiltration scheme to an h h model 2 the contributing factors to the differences between with and without re infiltration and 3 whether and to what extent the re infiltration process can help improve flood inundation mapping and prediction of extreme events we first test its effectiveness and importance on a 100 year design extreme rainfall event during a sensitivity test and then apply it to a real case study hurricane harvey to validate the efficacy it is anticipated to provide insightful information for model developers and researchers to understand the importance of the re infiltration process to flood modeling in this study we also release our latest development of crest imap v1 1 which features a two way coupling and re infiltration scheme on top of the previous version chen et al 2021 li et al 2021b the rest of this paper is structured as follows section 2 introduces the study area and necessary datasets for the model setup followed by experimental designs section 3 presents the results from the sensitivity test and the hurricane harvey event section 4 discusses limitations of this study as well as recommendations for input data and future model development at last section 4 concludes the main findings of this study 2 methods 2 1 forcing data precipitation is the major driver of local or regional flooding and it is thus central to acquire an accurate and high resolution dataset in the u s the multi radar multi sensor mrms precipitation product developed at the national severe storms laboratory nssl provides 2 min and 1 km rainfall field making it suitable for flash flood forecasting yussouf et al 2016 it integrates 180 wsr 88d operational radars creating a seamless radar mosaic across the conus and southern canada recent studies e g chen et al 2020 li et al 2020 verified the efficacy of mrms data when compared to gauge based and satellite based products during the hurricane harvey event the advantage of using radar rainfall is obvious for flood inundation modeling as conventional rain gauges cannot readily represent the spatially variable rainfall fields the mrms data was downloaded at https mtarchive geol iastate edu besides precipitation data potential evapotranspiration pet is a common input into a hydrologic system when the surface radiation is not resolved in the modeling process in this real case study we obtain the pet data from the usgs fews data port https earlywarning usgs gov fews at daily temporal and 1 spatial resolution allen et al 1998 2 2 environmental data the modeling system requires inputs from the terrain land use land cover lulc and soil type and depth among these variables terrain data arguably plays the utmost important role in hydraulic simulation dullo et al 2021 schumann and bates 2018 there has been a thorough investigation of terrain data affecting flood inundation modeling since the early development of hydrologic hydraulic models kenward et al 2000 sanders 2007 lately with the increasing interest in deploying macro scale flood inundation simulations global terrain data assessment has been again brought up mohanty et al 2020 sampson et al 2015 generally three types of data are favored and available in the u s 1 airborne light ranging and detection lidar that resolves terrain with a high degree of vertical accuracy 0 05 0 2 m and comes with a high spatial resolution but limited areal coverage 2 spaceborne radar interferometry ifsar e g shuttle radar terrain mission that provides global coverage but poor vertical accuracy 10 m and spatial resolution 90 m and 3 a mixed product such as the national elevation dataset ned from the usgs that merges lidar surveys and the usgs quadrangle maps whose accuracy 5 7 m and resolution 5 10 30 m sit in between the former two products a general consensus from these studies is that lidar data is the most favorable dem owing to improved vertical accuracy in flood modeling mohanty et al 2020 sanders 2007 schumann and bates 2018 but they have to be accompanied by surveyed channel profile since channel depth is not reflected ifsar however degrades its quality because of poor vegetation penetration and speckle noise while the ned smooths some artifacts the ned 10 m data accurately represents the river channel morphology than high resolution lidar data that cannot penetrate water surface therefore in this study we select the 10 m dem data from the ned dataset in the study area to confirm the river channel bathymetry 13 surveyed river geometries from the harris country flood control are curated and compared to ned 10 m shown in table 1 the average difference is found to be small 0 55 m the lulc and impervious area data are acquired from the national land cover database nlcd at 30 m resolution to derive a priori parameter sets the soil type dataset is retrieved from the united states department of agriculture 2 3 study area greens bayou basin located in the north of the houston metropolitan region is one of the areas that are susceptible to regional flooding because firstly landfalling tropical cyclones and hurricanes bring torrential rainfall within a short period secondly the urban development in the recent years have altered the local ecosystem e g replacement of soil with built up structures the basin is relatively flat 1 5 with an average elevation of around 23 65 m and the total drainage area is 457 9 km2 three main streams flow across this region reinhardt bayou drainage area 86 3 km2 flows from north to south met with greens bayou to form the longest river in this area halls bayou drainage area 225 1 km2 the second longest river meets greens bayou at the basin outlet fig 1 a the five usgs stream gauges situated at each mainstream monitor instantaneous streamflow at a 15 min time interval nearly 90 of the area is well developed especially in the western portion forests and wetlands are present downstream close to the basin outlet fig 1b the soil types are dominated by a mixture of sand clay and loam fig 1c the typical runoff generation mechanism in this region is infiltration excess runoff when extreme rain rates surpass soil infiltration capacity indicated by relatively low hydraulic conductivity values buchanan et al 2018 meanwhile the correlation between rainfall and streamflow is above 0 6 pointing to a flashy hydrograph berghuijs et al 2016 during the 500 year hurricane harvey event this region is largely inundated due to record breaking 1600 mm rainfall over a one week storm lifespan chen et al 2020 li et al 2020 according to the harris country flood report both greens bayou and halls bayou experienced a 500 year water level downstream and 50 year to 100 year in between upstream greens bayou broke previous water level records in 2002 and observed flooding occurred along the entire channel 2 4 crest imap model hydrologic modeling is thus far a common approach to deliver timely flood information for the sake of scalability and efficiency gourley et al 2017 yet conventional hydrologic models bear large uncertainties in such developed regions which is mainly due to 1 simplified representation of terrain dullo et al 2021 and 2 one dimensional routing that raises issues in flat regions flamig et al 2020 getirana and paiva 2013 li et al 2021b on the other hand hydraulic models do not excel in representing hydrologic processes in light of these issues the newly developed coupled routing and excess storage inundation mapping and prediction crest imap model is used to investigate the importance of the re infiltration scheme in flood inundation models the crest imap integrates crest v2 1 for the hydrologic part that simulates vertical water distribution by land surface and anuga v2 1 for the hydraulic routing that distributes spatial water over terrain by solving 2d shallow water equation its performance has been evaluated in this region against the non coupled hydrologic models and other popular coupled models wrf hydro hand and lisflood fp chen et al 2021 li et al 2021b crest imap achieves similar performance with lisflood fp if not better and generally outperform wrf hydro hand however the previous version of crest imap v1 0 does not include the re infiltration scheme meaning that surface running water is not allowed to re enter the soil here we release the crest imap v1 1 an upgrade version which considers two way coupling via exchanging surface water between the hydraulic and hydrologic module and re infiltration two different schemes are illustrated schematically in fig 2 where the left panel represents the re infiltration scheme and the right does not the crest imap v 1 0 and v1 1 are openly accessible from https github com chrimerss crest imap crest imap inherits the previous version of the crest model which simulates saturation excess runoff as the primary runoff generation process wang et al 2011 xue et al 2013 flamig et al 2020 li et al 2022a b the schematic model structure is depicted in fig 2 the study area is discretized in variable triangular meshes which allow higher density in river channels to resolve high resolution river flow each modeling unit receives excess rainfall rainfall minus evaporation from forcing data then surface water is divided into overland flow and soil water according to the impervious area ratio through linear weighting overland flow is generated once soil water exceeds its holding capacity otherwise soil water is separated into the remaining amount and interflow based on the variable infiltration curve vic concept as shown in eq 1 the vic model is a widely recognized infiltration model that has been applied in several classic hydrologic models liang et al 1994 zhao 1995 overland flow combined with the impervious area and saturation excess flow is eventually fed into the 2d shallow water equation solver the finite volume scheme it solves water depth and momentum distributed at each grid cell and propagates across boundaries the outputs of the model include water depth velocity discharge and soil moisture at a desired time step the flexibility of the unstructured mesh in crest imap allows dense meshes in regions that reflect high terrain variability e g river channel and sparse meshes in other regions e g flood plain this study simulates the extreme flood events at 10 m resolution using the embedded unstructured mesh generator 1 i i max 1 1 a 1 b where i is the infiltration rates i m a x is the maximum infiltration capacity a is the fractional area of the curve and b is the exponent of the vic curve there are five hydrologic parameters and one hydraulic parameter for the crest imap which are listed in table 2 along with parameter ranges it is noteworthy that all these parameters are spatially distributed to account for the spatial heterogeneity of land cover and soil types the mean soil saturated hydraulic conductivity ksat f i i m a x 1 1 a 1 b rom 0 to 20 mm d indicates the soil infiltration capability higher ksat values imply higher infiltration rates if soils are not saturated while reaching plateau for the saturated soils the mean soil water capacity wm from 10 4 to 365 4 mm measures the total water content the soil can hold with lower value representing the impermeable soils the exponent of the variable infiltration curve vic b determines soil water partitioned to saturation excess runoff or interflow with a higher b value corresponding to higher infiltration rates ke is the ratio of the potential evapotranspiration to actual evapotranspiration similar to the concept of pan coefficient these soil related a priori parameters can be approximated from a look up table at an individual grid cell basis chow et al 1988 there are also conus wide optimized parameter sets that are configured for operational flood monitoring systems flamig et al 2020 the impervious area ratio im from 0 to 100 is obtained directly from the nlcd dataset the manning s n coefficient is derived from the lulc via a look up table both parameters determine water conveyance capacity meaning that higher values relate to faster and larger flood peaks the hydrologic parameters are configured at their optima based on previous study a conus wise calibrated parameters flamig et al 2020 and shown in fig s1 but for the hydraulic parameter manning coefficient we manually adjusted it in a preceding event to ensure generating timely and accurate possible flood peaks specifically we use 2017 08 20 to 2017 08 25 to calibrate the manning coefficient as well as in channel water stage 2 5 experiment 2 5 1 synthetic experiment the importance of re infiltration in principle is governed by 1 soil properties 2 soil water saturation and 3 excess rainfall rates to quantify the relative importance and generalize our results we decide to conduct a sensitivity test in this study area to mimic different environment while preserving other variables the sensitivity analysis addresses the following hypotheses 1 discernible differences exist when switching on and off re infiltration scheme 2 re infiltration alters flood inundation magnitude and dynamics 3 differences are amplified when increasing soil infiltration rates and drying antecedent soil saturation and 4 differences increase with more frequent rainfall storms of five hydrologic parameters we select two soil parameters i e ksat and b that have a direct interaction with infiltration rates increase in ksat and b promote re infiltration amount additionally the antecedent soil moisture sm 0 and roughness parameter n proven to be critical for flood generation li et al 2021b yang et al 2011 is another term to change infiltration dynamics we applied a multiplier to each parameter of interest ranging from 0 0 to 2 0 with 0 1 spacing except for sm0 that only ranges from 0 0 completely dry to 1 0 fully saturated with 0 1 spacing the initial values of parameters are found in fig s1 for the forcing data in this experiment we consider a 100 year extreme in the study area by looking up the local intensity duration frequency table this determined rainfall rates are uniform across 2 h without spatial heterogeneity to eliminate the impact of rainfall spatial structure because we solely consider the impact by soils we run the model for 24 h for each parameter totaling 50 runs 2 5 2 real case hurricane harvey hurricane harvey is one of the most destructive extreme weather events happened in this study area with substantial damaging winds and urban flooding the storm was stalled over the houston region for one week with continuous falling of extreme rains to develop pluvial and fluvial flooding compounded by costal surges according to the precipitation estimates by gauges and radars 1539 mm maximum rainfall was observed and most locations in the study area recorded at least 760 mm rainfall making it the wettest tropical cyclone on record as a result almost 25 30 percent of harris country was submerged during this event leading to at least 125 billion economic damage the second largest natural disasters in us history many stream gauges malfunctioned e g being flushed during high flows owing to the socioeconomic impact a variety of flood simulations were conducted in this region chen et al 2021 2022 dullo et al 2021 li et al 2021b sebastian et al 2021 the simulation in our study is conducted from 2017 08 26 to 2017 09 01 during which we did not vary model parameters between scenarios with and without re infiltration the parameter values are optimized from a previous study li et al 2021b the initial soil moisture states are obtained from the operational flash project flash ou edu new 2 6 computational metrics and results interpretation a set of computational metrics are selected for this study the binary assessment comparing the scenarios with and without re infiltration is considered with positive positives pp positive negatives pn and negative positives np the first notion indicates whether the model results with re infiltration detects floods while the second for model results without re infiltration scheme the rationale behind this is that flood extent observations e g witness reports watermark satellite derived flood extent and insurance claims are still uncertain without ground truth bates 2004 chen et al 2021 2022 for flood magnitude the depth area and volume are calculated as a basin integrated ratio for flood dynamics we inspect the initial inundation timings and total inundation duration that are often factored in flood risk assessments merz et al 2010 the first six metrics listed in table 3 are calculated at the maximum flood depth across the simulation period in the real case study we verify the performance of two schemes against stream gauge measurements which is so far the most conventional and trustworthy source during the verification the nash sutcliffe efficiency nse and correlation coefficient cc are the primary evaluation scores with each indicating the best value of 1 the detailed formulas for calculating these variables are listed in table 3 as well as their ranges the rmse can be further decomposed to reveal the systematic error and random error tang et al 2020 first we assume an additive error model by fitting a linear regression to our simulated stage to determine regression coefficients a and b we assign the new variable as f then the residual is calculated by the difference of observed river stage o and fitted river stage f 2 f a s b 3 r m s e s 1 n i 1 n s f 2 2 4 r m s e r 1 n i 1 n f o 2 2 where s is denoted as the simulated river stage and o is the observed in the results section we present it in two parts sensitivity analysis and real case study the former includes basin wise difference in an integral to reveal general differences regarding parameters section 3 1 1 and storm intensity section 3 1 2 in the real case study we focus on the efficacy of re infiltration scheme by comparing to river stage observations section 3 2 1 and the high water marks surveyed in the aftermath of the event section 3 2 2 in section 3 2 3 we investigate the importance of re infiltration scheme in real case study by cross comparing it to the synthetic results 3 results 3 1 sensitivity analysis 3 1 1 parameter sensitivity 3 1 1 1 basin average statistics the overall flood related differences between scenarios with and without re infiltration are shown in fig 3 calculated as basin integral change by averaging each metric over the whole grid cells that are wet water depth larger than 0 01 m first the differences are discernible comparing the two especially for the surface water volume ratio r v which varies from 0 7 to 1 it suggests the surface water with re infiltration scheme could only account for 70 of the condition without re infiltration previous studies agree that the re infiltration results in a substantial reduction of river flow discharge which can be translated to lower flood depth ratio r h nahar et al 2004 woolhiser et al 1996 for different conditions the antecedent soil moisture as expected exhibits the largest impact on flood inundation dynamics when comparing the two scenarios lower initial soil moisture leads to greater differences in flood depth ratio r h area ratio r f volume ratio r v and dynamics for instance when the initial soil is completely dry the average flood depth ratio r h area ratio r f and volume ratio r v ratios of the re infiltration scheme are 85 85 67 respectively the initial inundation timing for re infiltration delays around 0 5 h t init 0 5 and the total inundation duration is 2 5 h shorter than the scenario without re infiltration d 2 5 the total inundation duration is an important factor for flood risk management merz et al 2010 triet et al 2020 as soil gradually approaches saturation the differences diminish saturated hydraulic conductivity ksat ranked as the second most sensitive parameter during the test exponentially reduces flood depth area volume by 10 7 20 when its multiplier increasing from 0 0 to 2 0 furthermore the differences of inundation duration d range from 1 5 h to 3 5 h making ksat the most influential parameter however the initial inundation timing is relatively insensitive to it as opposed to initial soil saturation condition this is due to the fact that ksat only changes infiltration flux along the way while exerting less impact on the initial inundation timings for higher surface roughness n flood area ratio r f and volume ratio r v decrease it is expected because as water flows slowly more water is accumulated above the surface leaving higher potential to infiltrate one exception is for the flood depth ratio r h which increases with roughness it means differences in water depth are shrinking between two schemes for higher roughness this implies that with increasing roughness flood inundation calculated with re infiltration will result in more concentrated higher local water depth yet less widespread lower inundation areas flooding the infiltration parameter b however has the least impact on the flood inundation dynamics among the three these measures exhibit the greatest changes at small b multipliers 0 1 0 3 and then level out irrespective of increasing b multipliers a plateau is reached because of the constrain of the maximum infiltration capacity in summary this sensitivity analysis tests our three main hypotheses indicating the non negligible differences between the two schemes and how the soil type and condition influence the results 3 1 1 2 spatiotemporal relationships to explore the spatiotemporal differences of scenarios with and without re infiltration we set three parameters to their default values namely an initially dry soil condition and normal soil infiltration rates i e a priori setting fig 4 shows the difference in flood extent for the two schemes despite a considerable amount of grid cells showing positive agreements i e both detect floods pp 16 0 there are still 1 7 of the grid cells issuing nps amounting to 15 6k grid cells 1 56 km2 in this model configuration specifically those nps accumulate around upstream floodplains while the downstream such as areas near the basin outlet does not present discernible differences as the flood depth there due to accumulation is not sensitive to inundation thresholds for flooded cells moreover fig 5 portrays the spatial distribution of the differences with respect to maximum depth initial inundation timings and total inundation duration fig 5a depicts the major differences that are situated in floodplains and river channels where surface water is accumulated via routing and the maximum depth difference is up to 3 m in the river channel especially downstream of halls bayou however the initial inundation timings and durations are scattered sparsely over the study area with a majority of the grid cells showing earlier and longer inundations for the case without re infiltration scenario therefore it is likely that the flood is over predicted by models without the re infiltration scenario meanwhile we notice that there are some samples in the opposite distribution indicating delayed and or shorter inundation time fig 5 arguably this could be some local effects when the soil reaches earlier saturation in the re infiltration scenario thereby leading to earlier flooding a supporting material is found in fig 5d in which the basin average soil moistures of two schemes are compared notably evapotranspiration is not considered in this ideal test so the soil moisture does not deplete with time during the storm lifetime soil moisture surges from completely dry to 85 saturation for the scenario without re infiltration and to 95 saturation for scenario with re infiltration early saturation reduces infiltration rates later on and thus has pronounced effects on local flooding when surface water is not routed timely fig 5e presents the evolution of surface water volume by integrating surface water depth along with grid cells although both scenarios concurrently reach the maximum surface water volume their recession limbs show considerable differences the re infiltration scenario apparently has a steep exponential decay as both still water and running water infiltrates into the soil for the scenario without re infiltration in contrast there is a mild decay and even levels out at the end of the simulation the difference between the two increases with time as shown in the shaded area up to 0 4 10 8 m3 volume difference which equates to almost half of the total surface water volume in summary re infiltration scheme indeed influences flood magnitude and timings via surface water soil interaction and it possibly reduces flood magnitude and delays shortens flood timing duration flood magnitude differences are pronounced downstream or in depressions while flood timings are scattered such results are markedly tied to soil condition wet or dry and soil characteristics infiltration capacity 3 1 2 sensitivity to storm frequency for less intense storms yet more frequent the impact of re infiltration is expected to be larger than more intense storms as rainfall rates are less likely to exceed infiltration capacity we looked up 1 year 2 year 5 year 10 year 25 year 50 year and 100 year event rainfall from the noaa atlas 14 idf curve and simulated those events collectively with default parameters as shown in fig 6 three flood magnitude related metrics i e rf rh and rv verify our speculation for the 1 year storm specifically flood areas with re infiltration only account for 30 of that without re infiltration likewise flood depth and surface water volumes are reduced by 50 and 75 respectively however there is no monotonic trend for flood timing floods are delayed for all storms but the delayed time increases with return periods prior to peaking at 0 83 h for 10 year storm after which it decreases the flood duration differences again decrease with return periods 3 2 case study hurricane harvey the previous sensitivity test indicates that the re infiltration scheme is not only physically sound but it exerts considerable influences on model simulations although the crest imap tested under theoretical scenarios it is relevant to compare one another in a real case study against observations during hurricane harvey 3 2 1 verification against stream gauges gauged water heights from five usgs stream gauges within the model domain are retrieved during model simulations at the 15 min interval surface water levels from two simulation schemes are extracted at collocated stream gauge locations it is worth mentioning that the terrain elevation imposes great uncertainties when comparing model simulations to observations as the sub grid variation cannot be resolved in the current settings despite the resolution mismatch these gauge readings are still the most widely used source to verify the model performance table 4 shows the respective performance for with and without re infiltration scenarios with respect to observations the re infiltration scheme greatly improves the nse scores 139 9 and cc 7 24 while reducing rmse 18 2 especially for the gauge 08075900 there is more than a 400 increase in nse score jumping from 0 12 to 0 69 by breaking down the rmse into systematic error rmses and random error rmser we see the reduced errors are largely attributed to systematic error 31 2 relative to the random error 13 1 thus the systematic bias is much alleviated by considering the re infiltration scheme the reason for such a performance leap comes from better characterization of its flow recession limbs as shown in fig 7 both schemes are capable of simulating the peak water height values without delays but water in the scenario without re infiltration falls mildly in the recession stage resulting in much higher water level than the observations on the other hand flow for the re infiltration scenario follows the gauge readings closely especially after the first peak from 2017 08 26 to 2017 08 27 apart from this best performing gauge the re infiltration scheme improves capturing falling water across all the gauge stations thereby leading to significant performance gains consistently previous studies also highlighted that the re infiltration markedly reduces recession limbs in the hydrograph nahar et al 2004 to be noted water heights during flood recession period are both overestimated by two schemes pointing to a systematic error in our crest imap framework first the model is calibrated to capture the peaking water height while ignoring the recession period second surface runoff generated by the water balance model has been found to overestimate li et al 2022b third missing model physics such as subsurface exfiltration to channels and manmade structures complicate results interpretation 3 2 2 verification against high water marks because a direct assessment for flood inundation is not feasible some watermarks or stains in the aftermath of a flood event can be used as proxy data for model evaluation although with great uncertainties the usgs team routinely publishes their surveyed high water marks hwms after some major flood events which can subsequently be used for model evaluations chen et al 2021 li et al 2021b sebastian et al 2021 wing et al 2017 fig 8 shows the cell wise maximum flood depth of the two schemes compared to the hwms both schemes present better performance upstream of halls bayou with a difference smaller than 0 5 m however the model over predicts water depth in greens bayou up to 1 5 m this is consistent with the over prediction of in channel water level as shown in fig 7 the distribution of the differences is shown in fig 8c pointing to a generally better performance of the re infiltration scenario than without it as the absolute mean depth difference of the re infiltration 0 51 m is 17 2 smaller than that of the scenario without re infiltration 0 60 m it is worth noting that hwms themselves come with uncertainties that are due to the data quality and errors could be up to 0 2 m koenig et al 2016 for instance tranquil water represents a smooth trend that has small uncertainties there are also spurious errors that are related to human mistakes or values being rounded off this is particularly true for the recorded geographical coordinates which requires more floating points to pin down the location exactly in fig 8d and e the two hwms are marked with an absolute difference greater than 1 m but only several pixels away i e tens of meters from their true values despite this the re infiltration greatly alleviates the over prediction of the previous model 3 2 3 intercomparisons of flood magnitude and dynamics during hurricane harvey the intercomparison of flood magnitude and dynamics helps to understand the effects of re infiltration in a real 500 year event fig 9 similar to fig 5 depicts the basin integrated differences for flood dynamics the initial inundation timing t init and total inundation duration d could vary from 2 delayed to 4 earlier hours and 0 15 h respectively for the temporal evolution of the harvey event it is featured by two subsequent events the first event from 2017 08 26 to 2017 08 27 saturates the soils immediately during which the large differences of surface water volume and soil moisture are present between the two schemes the soil moisture content for the re infiltration scheme is about 10 more than without the re infiltration scheme the surface water volume however is 40 less the second event does not produce a large difference because of the saturated soils over the domain fig 9d as indicated by the sensitivity analysis this effect is highly dependent on soil condition soil types and rainfall characteristics the extreme rainfall from harvey leaves less room for water to infiltrate compared to other less intense events it is therefore expected to have more pronounced improvements for less intense rainfall or other regions with high soil infiltration capacity 4 discussion in this study only local variations of four parameters initial soil moisture manning s roughness hydraulic conductivity and the exponent of the vic model are tested independently however the interactions among these parameters are not explored herein global sensitivity analysis such as the morris method used in the previous study li et al 2021b can measure the variation of each parameter relative to other parameters so it provides a clearer picture of the parameter interactions needless to say initial soil saturation state is the dominant controller for the differences between the simulations with and without re infiltration process when the soils are fully saturated the with and without re infiltration scenarios are almost identical if other parameters are the same combined with our previous study that underlies the importance of infiltration and initial soil moisture for flood inundation modeling we highly recommend taking into consideration the initial soil moisture state as it has not been well recognized in the hydraulic model community this can be achieved via three ways 1 warm up the model for a relatively long period prior to the simulation period chen et al 2020 2 parameterize the initial soil moisture and calibrate it similar to the way we treat initial in channel water depth xue et al 2013 3 approximate it using observations or other model simulations like what has been done in the real case study in section 3 2 flamig et al 2020 the first approach is ideal because it eliminates uncertainties in parameterization such as equifinality or error propagation from observations simulations to models it is however the most computationally expensive approach for hydraulic modeling compared to the other two approach two and three are more pragmatic while both inherit uncertainties or errors we prefer the third approach if the data source is found to be trustworthy for instance in our case study we used the simulated soil moisture product from the operational crest ef5 model which shares the same land surface processes as the crest imap the results relating to different rainfall events are considered in this study i e 1 year 2 year 5 year 10 year 25 year 50 year and 100 year rainfall as for lower storm intensity the differences between the two schemes enlarge as rainfall rates are less likely to exceed the infiltration capacity and soils are not saturated other environmental factors pertaining to different topography and physiography are also likely to interact and change the results for instance an increase in slope will leave less room for surface water to re infiltrate which explains why re infiltration compromises its importance in hillslope hydrology corradini et al 2002 zhang et al 2020 5 conclusions this study focuses on the influence of the re infiltration process for 100 year and 500 year flood events which has so far not been well recognized by the hydrologic hydraulic modeling community the sensitivity experiment and a 500 year hurricane harvey example both highlight the discernible differences between the with and without re infiltration scheme the major conclusions are summarized as follows 1 in the 100 year design rainfall event re infiltration is found to make discernible differences with less flood extent 1 56 km2 depth 3 m and dynamics 4 h delayed flooding and 4 h shorter inundation duration compared to without re infiltration the differences are increasing with more frequent storms the 500 year hurricane harvey event shows a magnified difference in inundation duration up to 15 h because of the longer event duration however the flood depth difference is less in the harvey event due to the rapid saturation of the soils 2 the hydraulic conductivity and antecedent soil condition from the designed sensitivity test are found to be the prime contributors to the difference between with and without re infiltration and comparatively the antecedent soil moisture condition is the most sensitive among the four tested factors 3 for the harvey event the differences are verified with stream gauge observations on average a 139 9 increase in nse scores is found for re infiltration with respect to without it the improvements are mostly tied to better characterization of the recession limb after peak flow while the peak flows are well captured by both the proxy data usgs high water marks also indicate better performance with the inclusion of the re infiltration scheme as the re infiltration scheme presents a 17 2 less flood depth difference than the case without the re infiltration the differences are further expected to enlarge for less intensive events and regions with a higher percentage of permeable soil media this study aims to raise attention to the important re infiltration process in coupled h h flood modeling to provide more accurate flood information e g depth and timings for future work we will continue improving the current crest imap model framework by incorporating flood mitigation measures such as levees and dams into the system also it is critical to couple with the nwp model to advance flood prediction lead time which ensures more time for residents at risk to evacuate software availability software name crest imap version 1 1 programing language python and c the main package is developed using the python language version 2 7 some bottlenecks that are computationally expensive e g hydrologic component and mesh generator are written and compiled in c language for efficiency it has been tested both on linux and macos operating systems both crest imap version 1 0 without re infiltration and 1 1 with re infiltration can be accessed from the hydroshare http www hydroshare org resource 50ce0d7d80b642898f74d8bf56798565 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author is partially sponsored by the university of oklahoma hydrology and water security program the national science foundation pire project and the graduate college hoving fellowship the authors would like to thank the forcing data and validation data providers the national oceanic and atmospheric administration national severe storms laboratory team and the unites states geological survey appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105450 
25567,coupled hydrologic hydraulic h h models have been widely applied for flood simulations yet the modern h h models suffer from one way and weak coupling and particularly disregarded run on infiltration which could compromise the model accuracy in this study we assess the h h model performance with and without re infiltration process in extreme flooding events results highlight that the re infiltration process should not be disregarded even in extreme flood simulations saturated hydraulic conductivity and antecedent soil moisture are found to be the prime contributors to such differences for the hurricane harvey event the model performance is verified against stream gauges and high water marks from which the re infiltration scheme increases the nash sutcliffe efficiency score by 140 on average and reduces maximum depth differences by 17 meanwhile the recent update of the crest imap model version 1 1 which incorporates two way coupling and re infiltration scheme is released for public access keywords flood inundation modeling run on infiltration re infiltration soil moisture hydrologic hydraulic modeling extreme flood events hurricane harvey bayou river basin 1 introduction flooding as a costly natural hazard has been increasingly threatening human lives and economies gourley et al 2017 hirabayashi et al 2013 li et al 2021a in the united states most billion dollar natural hazards are tied to either local or regional flooding making it the major cost to human society unfortunately under a warmer climate with anthropogenic pressure flood crises are likely to continue expanding as the flood frequency accelerates and flood magnitude rises bates et al 2021 hirabayashi et al 2013 li et al 2022a tabari 2020 triet et al 2020 swain et al 2020 viero et al 2019 to combat flood risks researchers have been developing hydrologic hydraulic models to deliver accurate and timely flood information for local communities and decision makers gourley et al 2017 in the united states two pronounced flood forecasting systems the nwm national water model cohen et al 2018 viterbo et al 2020 and flash flooded locations and simulated hydrographs project gourley et al 2017 yussouf et al 2020 are capable of both simulating real time floods and forecasting floods in a short range these large scale flood monitoring systems although claimed to offer inundation maps and predictions weaken their hydrodynamic simulation due to computational constraints for instance the nwm adopts the height above nearest drainage hand method to produce flood inundation maps along the river channels by mapping discharge to stage via rating curves johnson et al 2019 this conceptual method however overlooks the physics of floodwater propagation because of no flow dynamics being represented wing et al 2017 moreover it cannot simulate the pluvial flood which is a local effect caused by intense rainfall rates and does not normally occur along river channels bates et al 2021 more recently some emerging hydrodynamic models have been successfully deployed and evaluated at continental or global scales bates et al 2021 grimaldi et al 2019 sampson et al 2015 wing et al 2017 yamazaki et al 2011 these models simplify the full shallow water equation swe to speed up the flood simulation nevertheless they normally do not represent the hydrologic process well especially for the infiltration process which is proven to be critical in flood simulations li et al 2021b ni et al 2020 as such a coupled physically based hydrologic hydraulic h h model appears to be a better choice which takes the complementary advantages for accurate flood modeling dullo et al 2021 felder et al 2017 kim et al 2012 nguyen et al 2016 pontes et al 2017 sebastian et al 2021 readers are referred to teng et al 2017 and grimaldi et al 2019 for a detailed review of coupled models most of such models however adopt one way and weak coupling meaning that there is no interplay between the hydrologic component and hydraulic component bravo et al 2012 they normally produce surface runoff outputs first to drive the hydraulic model the recent development of the coupled routing and excess storage inundation mapping and prediction crest imap version 1 0 also uses this one way coupling strategy chen et al 2021 li et al 2021b two way coupling for the h h models has not hitherto been well recognized the accumulated surface water hydraulic feature along with excess surface runoff during a flood event in principle would alter infiltration rates whereby both the flood magnitudes and timings could differ therefore we activate the surface water infiltration along its way to downslope which is called run on infiltration or re infiltration in short smith and hebbert 1979 nahar et al 2004 zhang et al 2020 nahar et al 2004 defined this re infiltration as the infiltration of surface water that as it moves downslope encounters areas where moisture deficit has not yet been satisfied it is often ignored in rainfall runoff studies while it can be significant when the random nature of infiltration properties is taken into account corradini et al 1998 nahar et al 2004 smith and hebbert 1979 simulated the run on process with varying saturated hydraulic conductivity and rainfall rates and they reported that the effect of the run on process is to decrease the ponding time dramatically corradini et al 2002 compared models with and without re infiltration and they suggested that re infiltration greatly reduces surface flow and alters both rising and recession limbs of the hydrograph nahar et al 2004 emphasized the influence of re infiltration in hillslope hydrograph using the green ampt model with a 1d kinematic wave surface routing a recent study by zhang et al 2020 takes it one step further in which they applied the community model wrf hydro weather research forecasting model hydrological modeling system to explore the influence of rainfall rates topography soil types on the re infiltration process however none of these studies have considered the implication of re infiltration to hydrodynamic studies where the overland flow is driven by the 2d shallow water equation swe instead of 1d routing to do so we can obtain a more realistic view of how re infiltration plays a role in flood simulations during extreme flood events the infiltration process is often disregarded because the infiltration rates are relatively low compared to excess rainfall rates yet some studies claim that the infiltration process is critical to determine flood wave propagation such as arrival and dissipation especially in flat plain or regions with highly permeable soil media corradini et al 2002 mahapatra et al 2020 nahar et al 2004 li et al 2021b woolhiser et al 1996 a hydrodynamic model without infiltration is likely to overestimate flood depth kim et al 2012 li et al 2021b ni et al 2020 nevertheless it still remains unclear or at least not as clear as infiltration whether re infiltration is essential for h h models in extreme flood events in other words whether it is worth encapsulating such a scheme in modern flood simulation frameworks to our knowledge few studies have attempted to answer this question under the context of extreme flood events moreover further questions can arise as to what the determining factors are during such a process and how it interacts with both flood magnitude and dynamics in light of these questions the objectives of this study are to explore 1 the effectiveness and importance of the re infiltration scheme to an h h model 2 the contributing factors to the differences between with and without re infiltration and 3 whether and to what extent the re infiltration process can help improve flood inundation mapping and prediction of extreme events we first test its effectiveness and importance on a 100 year design extreme rainfall event during a sensitivity test and then apply it to a real case study hurricane harvey to validate the efficacy it is anticipated to provide insightful information for model developers and researchers to understand the importance of the re infiltration process to flood modeling in this study we also release our latest development of crest imap v1 1 which features a two way coupling and re infiltration scheme on top of the previous version chen et al 2021 li et al 2021b the rest of this paper is structured as follows section 2 introduces the study area and necessary datasets for the model setup followed by experimental designs section 3 presents the results from the sensitivity test and the hurricane harvey event section 4 discusses limitations of this study as well as recommendations for input data and future model development at last section 4 concludes the main findings of this study 2 methods 2 1 forcing data precipitation is the major driver of local or regional flooding and it is thus central to acquire an accurate and high resolution dataset in the u s the multi radar multi sensor mrms precipitation product developed at the national severe storms laboratory nssl provides 2 min and 1 km rainfall field making it suitable for flash flood forecasting yussouf et al 2016 it integrates 180 wsr 88d operational radars creating a seamless radar mosaic across the conus and southern canada recent studies e g chen et al 2020 li et al 2020 verified the efficacy of mrms data when compared to gauge based and satellite based products during the hurricane harvey event the advantage of using radar rainfall is obvious for flood inundation modeling as conventional rain gauges cannot readily represent the spatially variable rainfall fields the mrms data was downloaded at https mtarchive geol iastate edu besides precipitation data potential evapotranspiration pet is a common input into a hydrologic system when the surface radiation is not resolved in the modeling process in this real case study we obtain the pet data from the usgs fews data port https earlywarning usgs gov fews at daily temporal and 1 spatial resolution allen et al 1998 2 2 environmental data the modeling system requires inputs from the terrain land use land cover lulc and soil type and depth among these variables terrain data arguably plays the utmost important role in hydraulic simulation dullo et al 2021 schumann and bates 2018 there has been a thorough investigation of terrain data affecting flood inundation modeling since the early development of hydrologic hydraulic models kenward et al 2000 sanders 2007 lately with the increasing interest in deploying macro scale flood inundation simulations global terrain data assessment has been again brought up mohanty et al 2020 sampson et al 2015 generally three types of data are favored and available in the u s 1 airborne light ranging and detection lidar that resolves terrain with a high degree of vertical accuracy 0 05 0 2 m and comes with a high spatial resolution but limited areal coverage 2 spaceborne radar interferometry ifsar e g shuttle radar terrain mission that provides global coverage but poor vertical accuracy 10 m and spatial resolution 90 m and 3 a mixed product such as the national elevation dataset ned from the usgs that merges lidar surveys and the usgs quadrangle maps whose accuracy 5 7 m and resolution 5 10 30 m sit in between the former two products a general consensus from these studies is that lidar data is the most favorable dem owing to improved vertical accuracy in flood modeling mohanty et al 2020 sanders 2007 schumann and bates 2018 but they have to be accompanied by surveyed channel profile since channel depth is not reflected ifsar however degrades its quality because of poor vegetation penetration and speckle noise while the ned smooths some artifacts the ned 10 m data accurately represents the river channel morphology than high resolution lidar data that cannot penetrate water surface therefore in this study we select the 10 m dem data from the ned dataset in the study area to confirm the river channel bathymetry 13 surveyed river geometries from the harris country flood control are curated and compared to ned 10 m shown in table 1 the average difference is found to be small 0 55 m the lulc and impervious area data are acquired from the national land cover database nlcd at 30 m resolution to derive a priori parameter sets the soil type dataset is retrieved from the united states department of agriculture 2 3 study area greens bayou basin located in the north of the houston metropolitan region is one of the areas that are susceptible to regional flooding because firstly landfalling tropical cyclones and hurricanes bring torrential rainfall within a short period secondly the urban development in the recent years have altered the local ecosystem e g replacement of soil with built up structures the basin is relatively flat 1 5 with an average elevation of around 23 65 m and the total drainage area is 457 9 km2 three main streams flow across this region reinhardt bayou drainage area 86 3 km2 flows from north to south met with greens bayou to form the longest river in this area halls bayou drainage area 225 1 km2 the second longest river meets greens bayou at the basin outlet fig 1 a the five usgs stream gauges situated at each mainstream monitor instantaneous streamflow at a 15 min time interval nearly 90 of the area is well developed especially in the western portion forests and wetlands are present downstream close to the basin outlet fig 1b the soil types are dominated by a mixture of sand clay and loam fig 1c the typical runoff generation mechanism in this region is infiltration excess runoff when extreme rain rates surpass soil infiltration capacity indicated by relatively low hydraulic conductivity values buchanan et al 2018 meanwhile the correlation between rainfall and streamflow is above 0 6 pointing to a flashy hydrograph berghuijs et al 2016 during the 500 year hurricane harvey event this region is largely inundated due to record breaking 1600 mm rainfall over a one week storm lifespan chen et al 2020 li et al 2020 according to the harris country flood report both greens bayou and halls bayou experienced a 500 year water level downstream and 50 year to 100 year in between upstream greens bayou broke previous water level records in 2002 and observed flooding occurred along the entire channel 2 4 crest imap model hydrologic modeling is thus far a common approach to deliver timely flood information for the sake of scalability and efficiency gourley et al 2017 yet conventional hydrologic models bear large uncertainties in such developed regions which is mainly due to 1 simplified representation of terrain dullo et al 2021 and 2 one dimensional routing that raises issues in flat regions flamig et al 2020 getirana and paiva 2013 li et al 2021b on the other hand hydraulic models do not excel in representing hydrologic processes in light of these issues the newly developed coupled routing and excess storage inundation mapping and prediction crest imap model is used to investigate the importance of the re infiltration scheme in flood inundation models the crest imap integrates crest v2 1 for the hydrologic part that simulates vertical water distribution by land surface and anuga v2 1 for the hydraulic routing that distributes spatial water over terrain by solving 2d shallow water equation its performance has been evaluated in this region against the non coupled hydrologic models and other popular coupled models wrf hydro hand and lisflood fp chen et al 2021 li et al 2021b crest imap achieves similar performance with lisflood fp if not better and generally outperform wrf hydro hand however the previous version of crest imap v1 0 does not include the re infiltration scheme meaning that surface running water is not allowed to re enter the soil here we release the crest imap v1 1 an upgrade version which considers two way coupling via exchanging surface water between the hydraulic and hydrologic module and re infiltration two different schemes are illustrated schematically in fig 2 where the left panel represents the re infiltration scheme and the right does not the crest imap v 1 0 and v1 1 are openly accessible from https github com chrimerss crest imap crest imap inherits the previous version of the crest model which simulates saturation excess runoff as the primary runoff generation process wang et al 2011 xue et al 2013 flamig et al 2020 li et al 2022a b the schematic model structure is depicted in fig 2 the study area is discretized in variable triangular meshes which allow higher density in river channels to resolve high resolution river flow each modeling unit receives excess rainfall rainfall minus evaporation from forcing data then surface water is divided into overland flow and soil water according to the impervious area ratio through linear weighting overland flow is generated once soil water exceeds its holding capacity otherwise soil water is separated into the remaining amount and interflow based on the variable infiltration curve vic concept as shown in eq 1 the vic model is a widely recognized infiltration model that has been applied in several classic hydrologic models liang et al 1994 zhao 1995 overland flow combined with the impervious area and saturation excess flow is eventually fed into the 2d shallow water equation solver the finite volume scheme it solves water depth and momentum distributed at each grid cell and propagates across boundaries the outputs of the model include water depth velocity discharge and soil moisture at a desired time step the flexibility of the unstructured mesh in crest imap allows dense meshes in regions that reflect high terrain variability e g river channel and sparse meshes in other regions e g flood plain this study simulates the extreme flood events at 10 m resolution using the embedded unstructured mesh generator 1 i i max 1 1 a 1 b where i is the infiltration rates i m a x is the maximum infiltration capacity a is the fractional area of the curve and b is the exponent of the vic curve there are five hydrologic parameters and one hydraulic parameter for the crest imap which are listed in table 2 along with parameter ranges it is noteworthy that all these parameters are spatially distributed to account for the spatial heterogeneity of land cover and soil types the mean soil saturated hydraulic conductivity ksat f i i m a x 1 1 a 1 b rom 0 to 20 mm d indicates the soil infiltration capability higher ksat values imply higher infiltration rates if soils are not saturated while reaching plateau for the saturated soils the mean soil water capacity wm from 10 4 to 365 4 mm measures the total water content the soil can hold with lower value representing the impermeable soils the exponent of the variable infiltration curve vic b determines soil water partitioned to saturation excess runoff or interflow with a higher b value corresponding to higher infiltration rates ke is the ratio of the potential evapotranspiration to actual evapotranspiration similar to the concept of pan coefficient these soil related a priori parameters can be approximated from a look up table at an individual grid cell basis chow et al 1988 there are also conus wide optimized parameter sets that are configured for operational flood monitoring systems flamig et al 2020 the impervious area ratio im from 0 to 100 is obtained directly from the nlcd dataset the manning s n coefficient is derived from the lulc via a look up table both parameters determine water conveyance capacity meaning that higher values relate to faster and larger flood peaks the hydrologic parameters are configured at their optima based on previous study a conus wise calibrated parameters flamig et al 2020 and shown in fig s1 but for the hydraulic parameter manning coefficient we manually adjusted it in a preceding event to ensure generating timely and accurate possible flood peaks specifically we use 2017 08 20 to 2017 08 25 to calibrate the manning coefficient as well as in channel water stage 2 5 experiment 2 5 1 synthetic experiment the importance of re infiltration in principle is governed by 1 soil properties 2 soil water saturation and 3 excess rainfall rates to quantify the relative importance and generalize our results we decide to conduct a sensitivity test in this study area to mimic different environment while preserving other variables the sensitivity analysis addresses the following hypotheses 1 discernible differences exist when switching on and off re infiltration scheme 2 re infiltration alters flood inundation magnitude and dynamics 3 differences are amplified when increasing soil infiltration rates and drying antecedent soil saturation and 4 differences increase with more frequent rainfall storms of five hydrologic parameters we select two soil parameters i e ksat and b that have a direct interaction with infiltration rates increase in ksat and b promote re infiltration amount additionally the antecedent soil moisture sm 0 and roughness parameter n proven to be critical for flood generation li et al 2021b yang et al 2011 is another term to change infiltration dynamics we applied a multiplier to each parameter of interest ranging from 0 0 to 2 0 with 0 1 spacing except for sm0 that only ranges from 0 0 completely dry to 1 0 fully saturated with 0 1 spacing the initial values of parameters are found in fig s1 for the forcing data in this experiment we consider a 100 year extreme in the study area by looking up the local intensity duration frequency table this determined rainfall rates are uniform across 2 h without spatial heterogeneity to eliminate the impact of rainfall spatial structure because we solely consider the impact by soils we run the model for 24 h for each parameter totaling 50 runs 2 5 2 real case hurricane harvey hurricane harvey is one of the most destructive extreme weather events happened in this study area with substantial damaging winds and urban flooding the storm was stalled over the houston region for one week with continuous falling of extreme rains to develop pluvial and fluvial flooding compounded by costal surges according to the precipitation estimates by gauges and radars 1539 mm maximum rainfall was observed and most locations in the study area recorded at least 760 mm rainfall making it the wettest tropical cyclone on record as a result almost 25 30 percent of harris country was submerged during this event leading to at least 125 billion economic damage the second largest natural disasters in us history many stream gauges malfunctioned e g being flushed during high flows owing to the socioeconomic impact a variety of flood simulations were conducted in this region chen et al 2021 2022 dullo et al 2021 li et al 2021b sebastian et al 2021 the simulation in our study is conducted from 2017 08 26 to 2017 09 01 during which we did not vary model parameters between scenarios with and without re infiltration the parameter values are optimized from a previous study li et al 2021b the initial soil moisture states are obtained from the operational flash project flash ou edu new 2 6 computational metrics and results interpretation a set of computational metrics are selected for this study the binary assessment comparing the scenarios with and without re infiltration is considered with positive positives pp positive negatives pn and negative positives np the first notion indicates whether the model results with re infiltration detects floods while the second for model results without re infiltration scheme the rationale behind this is that flood extent observations e g witness reports watermark satellite derived flood extent and insurance claims are still uncertain without ground truth bates 2004 chen et al 2021 2022 for flood magnitude the depth area and volume are calculated as a basin integrated ratio for flood dynamics we inspect the initial inundation timings and total inundation duration that are often factored in flood risk assessments merz et al 2010 the first six metrics listed in table 3 are calculated at the maximum flood depth across the simulation period in the real case study we verify the performance of two schemes against stream gauge measurements which is so far the most conventional and trustworthy source during the verification the nash sutcliffe efficiency nse and correlation coefficient cc are the primary evaluation scores with each indicating the best value of 1 the detailed formulas for calculating these variables are listed in table 3 as well as their ranges the rmse can be further decomposed to reveal the systematic error and random error tang et al 2020 first we assume an additive error model by fitting a linear regression to our simulated stage to determine regression coefficients a and b we assign the new variable as f then the residual is calculated by the difference of observed river stage o and fitted river stage f 2 f a s b 3 r m s e s 1 n i 1 n s f 2 2 4 r m s e r 1 n i 1 n f o 2 2 where s is denoted as the simulated river stage and o is the observed in the results section we present it in two parts sensitivity analysis and real case study the former includes basin wise difference in an integral to reveal general differences regarding parameters section 3 1 1 and storm intensity section 3 1 2 in the real case study we focus on the efficacy of re infiltration scheme by comparing to river stage observations section 3 2 1 and the high water marks surveyed in the aftermath of the event section 3 2 2 in section 3 2 3 we investigate the importance of re infiltration scheme in real case study by cross comparing it to the synthetic results 3 results 3 1 sensitivity analysis 3 1 1 parameter sensitivity 3 1 1 1 basin average statistics the overall flood related differences between scenarios with and without re infiltration are shown in fig 3 calculated as basin integral change by averaging each metric over the whole grid cells that are wet water depth larger than 0 01 m first the differences are discernible comparing the two especially for the surface water volume ratio r v which varies from 0 7 to 1 it suggests the surface water with re infiltration scheme could only account for 70 of the condition without re infiltration previous studies agree that the re infiltration results in a substantial reduction of river flow discharge which can be translated to lower flood depth ratio r h nahar et al 2004 woolhiser et al 1996 for different conditions the antecedent soil moisture as expected exhibits the largest impact on flood inundation dynamics when comparing the two scenarios lower initial soil moisture leads to greater differences in flood depth ratio r h area ratio r f volume ratio r v and dynamics for instance when the initial soil is completely dry the average flood depth ratio r h area ratio r f and volume ratio r v ratios of the re infiltration scheme are 85 85 67 respectively the initial inundation timing for re infiltration delays around 0 5 h t init 0 5 and the total inundation duration is 2 5 h shorter than the scenario without re infiltration d 2 5 the total inundation duration is an important factor for flood risk management merz et al 2010 triet et al 2020 as soil gradually approaches saturation the differences diminish saturated hydraulic conductivity ksat ranked as the second most sensitive parameter during the test exponentially reduces flood depth area volume by 10 7 20 when its multiplier increasing from 0 0 to 2 0 furthermore the differences of inundation duration d range from 1 5 h to 3 5 h making ksat the most influential parameter however the initial inundation timing is relatively insensitive to it as opposed to initial soil saturation condition this is due to the fact that ksat only changes infiltration flux along the way while exerting less impact on the initial inundation timings for higher surface roughness n flood area ratio r f and volume ratio r v decrease it is expected because as water flows slowly more water is accumulated above the surface leaving higher potential to infiltrate one exception is for the flood depth ratio r h which increases with roughness it means differences in water depth are shrinking between two schemes for higher roughness this implies that with increasing roughness flood inundation calculated with re infiltration will result in more concentrated higher local water depth yet less widespread lower inundation areas flooding the infiltration parameter b however has the least impact on the flood inundation dynamics among the three these measures exhibit the greatest changes at small b multipliers 0 1 0 3 and then level out irrespective of increasing b multipliers a plateau is reached because of the constrain of the maximum infiltration capacity in summary this sensitivity analysis tests our three main hypotheses indicating the non negligible differences between the two schemes and how the soil type and condition influence the results 3 1 1 2 spatiotemporal relationships to explore the spatiotemporal differences of scenarios with and without re infiltration we set three parameters to their default values namely an initially dry soil condition and normal soil infiltration rates i e a priori setting fig 4 shows the difference in flood extent for the two schemes despite a considerable amount of grid cells showing positive agreements i e both detect floods pp 16 0 there are still 1 7 of the grid cells issuing nps amounting to 15 6k grid cells 1 56 km2 in this model configuration specifically those nps accumulate around upstream floodplains while the downstream such as areas near the basin outlet does not present discernible differences as the flood depth there due to accumulation is not sensitive to inundation thresholds for flooded cells moreover fig 5 portrays the spatial distribution of the differences with respect to maximum depth initial inundation timings and total inundation duration fig 5a depicts the major differences that are situated in floodplains and river channels where surface water is accumulated via routing and the maximum depth difference is up to 3 m in the river channel especially downstream of halls bayou however the initial inundation timings and durations are scattered sparsely over the study area with a majority of the grid cells showing earlier and longer inundations for the case without re infiltration scenario therefore it is likely that the flood is over predicted by models without the re infiltration scenario meanwhile we notice that there are some samples in the opposite distribution indicating delayed and or shorter inundation time fig 5 arguably this could be some local effects when the soil reaches earlier saturation in the re infiltration scenario thereby leading to earlier flooding a supporting material is found in fig 5d in which the basin average soil moistures of two schemes are compared notably evapotranspiration is not considered in this ideal test so the soil moisture does not deplete with time during the storm lifetime soil moisture surges from completely dry to 85 saturation for the scenario without re infiltration and to 95 saturation for scenario with re infiltration early saturation reduces infiltration rates later on and thus has pronounced effects on local flooding when surface water is not routed timely fig 5e presents the evolution of surface water volume by integrating surface water depth along with grid cells although both scenarios concurrently reach the maximum surface water volume their recession limbs show considerable differences the re infiltration scenario apparently has a steep exponential decay as both still water and running water infiltrates into the soil for the scenario without re infiltration in contrast there is a mild decay and even levels out at the end of the simulation the difference between the two increases with time as shown in the shaded area up to 0 4 10 8 m3 volume difference which equates to almost half of the total surface water volume in summary re infiltration scheme indeed influences flood magnitude and timings via surface water soil interaction and it possibly reduces flood magnitude and delays shortens flood timing duration flood magnitude differences are pronounced downstream or in depressions while flood timings are scattered such results are markedly tied to soil condition wet or dry and soil characteristics infiltration capacity 3 1 2 sensitivity to storm frequency for less intense storms yet more frequent the impact of re infiltration is expected to be larger than more intense storms as rainfall rates are less likely to exceed infiltration capacity we looked up 1 year 2 year 5 year 10 year 25 year 50 year and 100 year event rainfall from the noaa atlas 14 idf curve and simulated those events collectively with default parameters as shown in fig 6 three flood magnitude related metrics i e rf rh and rv verify our speculation for the 1 year storm specifically flood areas with re infiltration only account for 30 of that without re infiltration likewise flood depth and surface water volumes are reduced by 50 and 75 respectively however there is no monotonic trend for flood timing floods are delayed for all storms but the delayed time increases with return periods prior to peaking at 0 83 h for 10 year storm after which it decreases the flood duration differences again decrease with return periods 3 2 case study hurricane harvey the previous sensitivity test indicates that the re infiltration scheme is not only physically sound but it exerts considerable influences on model simulations although the crest imap tested under theoretical scenarios it is relevant to compare one another in a real case study against observations during hurricane harvey 3 2 1 verification against stream gauges gauged water heights from five usgs stream gauges within the model domain are retrieved during model simulations at the 15 min interval surface water levels from two simulation schemes are extracted at collocated stream gauge locations it is worth mentioning that the terrain elevation imposes great uncertainties when comparing model simulations to observations as the sub grid variation cannot be resolved in the current settings despite the resolution mismatch these gauge readings are still the most widely used source to verify the model performance table 4 shows the respective performance for with and without re infiltration scenarios with respect to observations the re infiltration scheme greatly improves the nse scores 139 9 and cc 7 24 while reducing rmse 18 2 especially for the gauge 08075900 there is more than a 400 increase in nse score jumping from 0 12 to 0 69 by breaking down the rmse into systematic error rmses and random error rmser we see the reduced errors are largely attributed to systematic error 31 2 relative to the random error 13 1 thus the systematic bias is much alleviated by considering the re infiltration scheme the reason for such a performance leap comes from better characterization of its flow recession limbs as shown in fig 7 both schemes are capable of simulating the peak water height values without delays but water in the scenario without re infiltration falls mildly in the recession stage resulting in much higher water level than the observations on the other hand flow for the re infiltration scenario follows the gauge readings closely especially after the first peak from 2017 08 26 to 2017 08 27 apart from this best performing gauge the re infiltration scheme improves capturing falling water across all the gauge stations thereby leading to significant performance gains consistently previous studies also highlighted that the re infiltration markedly reduces recession limbs in the hydrograph nahar et al 2004 to be noted water heights during flood recession period are both overestimated by two schemes pointing to a systematic error in our crest imap framework first the model is calibrated to capture the peaking water height while ignoring the recession period second surface runoff generated by the water balance model has been found to overestimate li et al 2022b third missing model physics such as subsurface exfiltration to channels and manmade structures complicate results interpretation 3 2 2 verification against high water marks because a direct assessment for flood inundation is not feasible some watermarks or stains in the aftermath of a flood event can be used as proxy data for model evaluation although with great uncertainties the usgs team routinely publishes their surveyed high water marks hwms after some major flood events which can subsequently be used for model evaluations chen et al 2021 li et al 2021b sebastian et al 2021 wing et al 2017 fig 8 shows the cell wise maximum flood depth of the two schemes compared to the hwms both schemes present better performance upstream of halls bayou with a difference smaller than 0 5 m however the model over predicts water depth in greens bayou up to 1 5 m this is consistent with the over prediction of in channel water level as shown in fig 7 the distribution of the differences is shown in fig 8c pointing to a generally better performance of the re infiltration scenario than without it as the absolute mean depth difference of the re infiltration 0 51 m is 17 2 smaller than that of the scenario without re infiltration 0 60 m it is worth noting that hwms themselves come with uncertainties that are due to the data quality and errors could be up to 0 2 m koenig et al 2016 for instance tranquil water represents a smooth trend that has small uncertainties there are also spurious errors that are related to human mistakes or values being rounded off this is particularly true for the recorded geographical coordinates which requires more floating points to pin down the location exactly in fig 8d and e the two hwms are marked with an absolute difference greater than 1 m but only several pixels away i e tens of meters from their true values despite this the re infiltration greatly alleviates the over prediction of the previous model 3 2 3 intercomparisons of flood magnitude and dynamics during hurricane harvey the intercomparison of flood magnitude and dynamics helps to understand the effects of re infiltration in a real 500 year event fig 9 similar to fig 5 depicts the basin integrated differences for flood dynamics the initial inundation timing t init and total inundation duration d could vary from 2 delayed to 4 earlier hours and 0 15 h respectively for the temporal evolution of the harvey event it is featured by two subsequent events the first event from 2017 08 26 to 2017 08 27 saturates the soils immediately during which the large differences of surface water volume and soil moisture are present between the two schemes the soil moisture content for the re infiltration scheme is about 10 more than without the re infiltration scheme the surface water volume however is 40 less the second event does not produce a large difference because of the saturated soils over the domain fig 9d as indicated by the sensitivity analysis this effect is highly dependent on soil condition soil types and rainfall characteristics the extreme rainfall from harvey leaves less room for water to infiltrate compared to other less intense events it is therefore expected to have more pronounced improvements for less intense rainfall or other regions with high soil infiltration capacity 4 discussion in this study only local variations of four parameters initial soil moisture manning s roughness hydraulic conductivity and the exponent of the vic model are tested independently however the interactions among these parameters are not explored herein global sensitivity analysis such as the morris method used in the previous study li et al 2021b can measure the variation of each parameter relative to other parameters so it provides a clearer picture of the parameter interactions needless to say initial soil saturation state is the dominant controller for the differences between the simulations with and without re infiltration process when the soils are fully saturated the with and without re infiltration scenarios are almost identical if other parameters are the same combined with our previous study that underlies the importance of infiltration and initial soil moisture for flood inundation modeling we highly recommend taking into consideration the initial soil moisture state as it has not been well recognized in the hydraulic model community this can be achieved via three ways 1 warm up the model for a relatively long period prior to the simulation period chen et al 2020 2 parameterize the initial soil moisture and calibrate it similar to the way we treat initial in channel water depth xue et al 2013 3 approximate it using observations or other model simulations like what has been done in the real case study in section 3 2 flamig et al 2020 the first approach is ideal because it eliminates uncertainties in parameterization such as equifinality or error propagation from observations simulations to models it is however the most computationally expensive approach for hydraulic modeling compared to the other two approach two and three are more pragmatic while both inherit uncertainties or errors we prefer the third approach if the data source is found to be trustworthy for instance in our case study we used the simulated soil moisture product from the operational crest ef5 model which shares the same land surface processes as the crest imap the results relating to different rainfall events are considered in this study i e 1 year 2 year 5 year 10 year 25 year 50 year and 100 year rainfall as for lower storm intensity the differences between the two schemes enlarge as rainfall rates are less likely to exceed the infiltration capacity and soils are not saturated other environmental factors pertaining to different topography and physiography are also likely to interact and change the results for instance an increase in slope will leave less room for surface water to re infiltrate which explains why re infiltration compromises its importance in hillslope hydrology corradini et al 2002 zhang et al 2020 5 conclusions this study focuses on the influence of the re infiltration process for 100 year and 500 year flood events which has so far not been well recognized by the hydrologic hydraulic modeling community the sensitivity experiment and a 500 year hurricane harvey example both highlight the discernible differences between the with and without re infiltration scheme the major conclusions are summarized as follows 1 in the 100 year design rainfall event re infiltration is found to make discernible differences with less flood extent 1 56 km2 depth 3 m and dynamics 4 h delayed flooding and 4 h shorter inundation duration compared to without re infiltration the differences are increasing with more frequent storms the 500 year hurricane harvey event shows a magnified difference in inundation duration up to 15 h because of the longer event duration however the flood depth difference is less in the harvey event due to the rapid saturation of the soils 2 the hydraulic conductivity and antecedent soil condition from the designed sensitivity test are found to be the prime contributors to the difference between with and without re infiltration and comparatively the antecedent soil moisture condition is the most sensitive among the four tested factors 3 for the harvey event the differences are verified with stream gauge observations on average a 139 9 increase in nse scores is found for re infiltration with respect to without it the improvements are mostly tied to better characterization of the recession limb after peak flow while the peak flows are well captured by both the proxy data usgs high water marks also indicate better performance with the inclusion of the re infiltration scheme as the re infiltration scheme presents a 17 2 less flood depth difference than the case without the re infiltration the differences are further expected to enlarge for less intensive events and regions with a higher percentage of permeable soil media this study aims to raise attention to the important re infiltration process in coupled h h flood modeling to provide more accurate flood information e g depth and timings for future work we will continue improving the current crest imap model framework by incorporating flood mitigation measures such as levees and dams into the system also it is critical to couple with the nwp model to advance flood prediction lead time which ensures more time for residents at risk to evacuate software availability software name crest imap version 1 1 programing language python and c the main package is developed using the python language version 2 7 some bottlenecks that are computationally expensive e g hydrologic component and mesh generator are written and compiled in c language for efficiency it has been tested both on linux and macos operating systems both crest imap version 1 0 without re infiltration and 1 1 with re infiltration can be accessed from the hydroshare http www hydroshare org resource 50ce0d7d80b642898f74d8bf56798565 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author is partially sponsored by the university of oklahoma hydrology and water security program the national science foundation pire project and the graduate college hoving fellowship the authors would like to thank the forcing data and validation data providers the national oceanic and atmospheric administration national severe storms laboratory team and the unites states geological survey appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105450 
25568,to characterize complex biogeochemical systems results from multiple experiments where each targets a specific subprocess are commonly combined the resulting datasets are interpreted through the calibration of biogeochemical models for process inference and predictions commonly used calibration approaches of fitting datasets from individual experiments to subprocess models one at a time is prone to missing information shared between datasets and incomplete uncertainty propagation we propose a bayesian joint fitting scheme addressing the above mentioned concerns by jointly fitting all the available datasets thus calibrating the entire biogeochemical model in one go using markov chain monte carlo mcmc the identification of null spaces in the parameter distributions from mcmc guided the simplification of certain subprocess models for example fast kinetic sorption was replaced by equilibrium sorption and monod demethylation was replaced by first order demethylation joint fitting of datasets resulted in complete uncertainty propagation with parameter estimates informed by all available data keywords bayesian inference parameter uncertainty mercury methylation 1 introduction many stream ecosystems are under stress due to contamination driven by anthropogenic activities in particular trace metals such as mercury lead and cadmium released from industrial activities pose a serious threat to human and ecosystem health because of their toxicity even at minimum levels and potential of bioaccumulation chen and folt 2000 evers et al 2007 goodyear and mcneill 1999 horowitz 1991 mason et al 2000 ward et al 2010 a comprehensive understanding of their transformation in the environment requires considerations of the complex interplay between physical chemical and biological processes at different spatial and temporal scales carefully designed field and laboratory experiments gather critical data about these processes however the usefulness of these datasets depends on their reliable interpretations empirical relations can fit the data well but can struggle to extrapolate due to a lack of mechanistic underpinnings process based biogeochemical models are developed and calibrated using the experimental data for mechanistically rich process inferences and predictions there are generally multiple candidate biogeochemical models available to fit the data e g first order kinetics equilibrium monod and tessier all these models are upscaled phenomenological descriptions of complex biogeochemical processes and do not perfectly represent the biogeochemical system since the true model is unknown neumann and gujer 2008 typically based on preliminary data analysis and expert knowledge a model is proposed and calibrated to estimate model parameters however often initially proposed mechanistic models are overparameterized needing data driven methods for model simplification and unique parameter estimates commonly used approaches in biogeochemical modeling and calibration are prone to underutilization of the data available from multiple experiments incomplete uncertainty quantification and model overparameterization below we describe some of the challenges in interpreting data with commonly used biogeochemical modeling approaches and how we aim to address them characterization of biogeochemical systems often requires multiple experiments targeting a subset of processes in the system referred henceforth as subprocesses for example sorption experiments are performed separately from reaction focused experiments in various biogeochemical applications e g haggerty et al 2008 lemke et al 2014 olsen et al 2018 schwartz et al 2021a models for subprocesses like sorption are often calibrated first and the resulting best fit parameters are kept fixed in the subsequent estimation of reaction parameters in a broader model framework this sequential fitting scheme has two main shortcomings first it fails to make full use of all information information on subprocesses that are common to multiple experiments is not fully used when subprocess parameters are determined by analysis of one experiment and then held fixed in subsequent analyses second sequential fitting fails to propagate uncertainty from one experiment to the next for example fixing sorption parameters when analyzing reaction experiments ignores the fact that the reaction experiments contain information about both sorption and reactions moreover it fails to propagate uncertainty from the analyzed sorption experiment to the analysis of the reaction experiment fig 1 shows this sequential fitting scheme in a general biogeochemical model comprising three subprocesses that are calibrated sequentially we propose that the datasets from multiple experiments should be fitted simultaneously thus calibrating all subprocess models together by doing so we leverage information sharing about subprocesses among datasets additionally this joint fitting scheme also allows uncertainty propagation among subprocess models the surface kelleher et al 2019 knapp and cirpka 2017 lemke et al 2013 liao et al 2013 and subsurface luo et al 2006 zhao et al 2018 hydrologists have previously recognized merit in this joint fitting approach these studies fitted conservative and reactive tracer data from tracer experiments jointly rather than sequentially to leverage shared information and propagate uncertainty fully biogeochemical systems typically more complex with a larger number of highly intertwined subprocesses can particularly benefit from sharing information because of the different scales and interactions of these subprocesses captured in different experiments a more general problem and very relevant to biogeochemical modeling is the lack of parameter identifiability beck 1987 which essentially means that significantly different parameter values can reproduce the experimental data equally well this can result from insufficient information in the experimental data to match the model complexity or incorrect model structure marschmann et al 2019 although non linear least square schemes can yield parameters that produce a good match between simulations and experimental data in such cases the model structure deficiency can go undetected leading to non meaningful local uncertainty estimates and faulty process inferences marschmann et al 2019 neumann and gujer 2008 van turnhout et al 2016 neumann and gujer 2008 showed that bias related to model structure in environmental models can be diagnosed through analysis of residuals from non linear regression schemes marschmann et al 2019 used information geometry to identify model structure limitations and reduce the model complexity in mechanistically rich biogeochemical models suffering from the lack of parameter identifiability with the advancements in computational capabilities computationally intensive bayesian inverse modeling have become accessible to widespread applications including biogeochemical modeling mapping full joint distributions of parameters in bayesian approach can reveal parameter uncertainties sensitivities interactions and potential null spaces which can guide model improvements arhonditsis et al 2008 applied bayesian calibration using markov chain monte carlo mcmc to rigorously quantify parameter and predictive uncertainty in aquatic biogeochemical models zhang and arhonditsis 2009 using synthetic aquatic biogeochemistry datasets demonstrated the strength of the bayesian approach in transferring information across systems as priors in a hierarchical calibration scheme van oijen et al 2011 calibrated four different biogeochemical models for norway spruce forest using mcmc they compared models through integrated likelihood values of the estimated parameter distributions van turnhout et al 2016 developed a toolbox with bayesian inference based criteria for selecting optimal reaction network in municipal solid waste landfills davoudabadi et al 2021 used particle filter based advanced bayesian methods to calibrate high complexity state space models with soil carbon sequestration as an example in this paper we present a holistic framework of bayesian joint fitting for biogeochemical models leveraging information from multiple experiments treating uncertainty rigorously and detecting potential parameterization deficiencies this approach broadly has two facets first we propose that the datasets from multiple experiments should be fitted simultaneously thus calibrating all subprocess models together by doing so we are informing each subprocess model using all the available datasets involving that particular subprocess additionally this joint fitting scheme also allows uncertainty propagation among subprocess models and different datasets second we map the full joint distribution of parameters using the advanced markov chain monte carlo mcmc method this approach not only yields robust global uncertainty estimates and but can also identify model deficiencies causing the lack of parameter identifiability we apply the proposed workflow to a recently published mercury methylation dataset schwartz et al 2021b to improve data interpretation and advance process inferences characterization of mercury methylation processes a use case in this paper is of significantly high importance as methylmercury mehg poses a great threat to humans and wildlife eckley et al 2020 occurring in the environment as a natural and anthropogenic pollutant hg is methylated to form the neurotoxin mehg through microbially mediated processes clarkson et al 2003 mehg ingestion even at a low level has adverse impacts on the development of children mehg can get biomagnified in aquatic food webs which makes it particularly concerning mergler et al 2007 significant experimental and modeling efforts have been made to understand mercury methylation dynamics avramescu et al 2011 hintelmann et al 2000 jonsson et al 2012 liem nguyen et al 2016 mitchell and gilmour 2008 olsen et al 2018 rodrı guez martı n doimeadios et al 2004 schwartz et al 2019 traditionally first order reversible kinetics have been used to model mercury methylation hintelmann et al 2000 however apparent non first order kinetic behavior has often been observed in mercury methylation demethylation data avramescu et al 2011 jonsson et al 2012 olsen et al 2018 olsen et al 2018 studying mercury methylation in periphyton biofilms suggested that the apparent non first order behavior does not necessarily imply non first order kinetics but can result from other competing processes making hg and mehg unavailable for methylation and demethylation respectively they proposed the transient availability model tam accounting for competing processes like multisite kinetic sorption of hg and mehg and reduction of hg schwartz et al 2022 extended the tam framework to aquatic sediment systems to model mercury methylation on two disparate sediments from east fork poplar creek efpc in oak ridge tn usa tam application by schwartz et al 2022 considered fully kinetic competing processes for bioavailability with first order methylation demethylation for one sediment type and reversible monod kinetics for another with the sequential fitting of sorption and methylation datasets and a gradient based scheme for parameter estimation they reproduced experimental data well but obtained non meaningful error estimates for certain parameters using the novel mercury methylation dataset schwartz et al 2021b we implement the proposed parameter estimation workflow in the tam framework olsen et al 2018 and schwartz et al 2022 studied biologically mediated methylation of mercury hg to the neurotoxin methylmercury mehg on periphyton film and colonized sediments respectively both studies fitted sorption experiment datasets first to estimate sorption parameters which were then fixed while estimating methylation demethylation parameters fitting datasets from methylation experiments however it is important to recognize that these processes are not independent and methylation demethylation datasets also contain information about the sorption processes which is not leveraged in this sequential fitting approach additionally because of using the best fit values of sorption parameters the estimates of methylation demethylation parameters ignore uncertainties in sorption datasets in our approach sorption and methylation datasets are fitted simultaneously and respective parameters are jointly estimated thus addressing the above described concerns for the uncertainty aware parameter estimation olsen et al 2018 and schwartz et al 2022 adopted a gradient based local optimization scheme which works well in unimodal smooth parameter spaces but can suffer in multimodal or other atypical parameter spaces additionally under such schemes other model structure related issues like overparameterization may go undetected we adopt a bayesian approach using mcmc a global search scheme yielding a full joint distribution of parameters the obtained parameter distributions are expected to guide model improvements in addition to offering robust uncertainty estimates with the proposed parameter estimation workflow of bayesian joint fitting we aim to improve the interpretation of the rich mercury methylation dataset by schwartz et al 2021b the proposed workflow is easily transferable to other complex biogeochemical systems characterized through multiple experiments for reliable parameter estimates process inferences and predictions 2 overview of experiments and datasets here we provide a brief overview of the study site and experiments for more information about the site see brooks and southworth 2011 riscassi et al 2016 and for experimental details see schwartz et al 2022 schwartz et al 2022 collected sediments from efpc that have legacy mercury contamination and performed sorption and mercury methylation experiments to characterize and quantify their methylation and demethylation potential and associated physical chemical and microbial processes two contrasting sediment types were collected for use in laboratory studies sediment 1 rich in organic matter relatively anoxic fine sand predominant along the stream edge and sediment 2 a medium to coarse sand lower in organic carbon and less metabolically active than sediment 1 and predominant in the center of the channel for hg and mehg sorption experiments isotopically labeled 201hg or me202hg were added to sediment creek water suspension under air and placed on a reciprocating shaker the number of time points at which samples were collected for hg are 12 and 14 for sediment 1 and for sediment 2 respectively and for mehg 14 and 15 respectively triplicate samples were sacrificed at each time point in the hg sorption experiment hg in the aqueous phase total solid phase hg and hg 0 were quantified at different time points under the oxic conditions of the hg sorption experiments hg methylation an obligately anaerobic microbial process did not occur in the mehg sorption experiments aqueous me202hg quantified at different time points from triplicate samples and total me202hg in the system was quantified at select time points to verify the lack of demethylation of the added isotope schwartz et al 2022 mercury methylation experiments were conducted on carefully prepared anoxic sediment slurry microcosms three treatments were established one spiked with 201hg to monitor methylation via production of me201hg the second spiked with me202hg to monitor demethylation via loss of me202hg and treatment with no spike added to monitor other biogeochemical parameters of interest e g fe ii sulfide because the hg and mehg analyses were full consumptive and destructive the experiment for sediment 2 was run longer because of the low activity initially measured in those sediments the number of timepoints at which samples were collected is 4 and 3 for sediment 1 and sediment 2 respectively three microcosms from each treatment were destroyed at each time point for hg and mehg analysis the resulting datasets from mercury methylation experiments are total me201hg and me202hg at each time point schwartz et al 2022 3 bayesian joint fitting approach the data from the experiments by schwartz et al 2022 captures information about the biogeochemical processes controlling methylation of mercury on colonized river sediments models under the tam framework are proposed for these biogeochemical processes and calibrated to match the experimental data allowing us to make process inferences and develop prediction capabilities experience from the interpretation of mercury methylation datasets by schwartz et al 2022 reveals several shortcomings of commonly used biogeochemical model calibration practices which we discussed in the introduction section fig 1 our approach leverages information shared among experiments by joint calibration of subprocesses using all the available datasets additionally full joint distribution of parameters is mapped to get robust uncertainty estimates and detect potential model structure deficiencies in fig 2 we present a schematic of our bayesian joint fitting scheme and the advantages of different facets of the scheme in the following sub sections we describe different models considered for sorption and methylation provide bayesian formulation and details of mcmc application 3 1 transient availability model framework the tam model framework developed by olsen et al 2018 is used to describe mercury methylation under the tam model framework methylation and demethylation of mercury are strongly influenced by the availability of hg and mehg in the aqueous phase experiments suggest that the transient availability of hg in the aqueous phase is caused by competition with two site sorption and reversible reduction of aqueous mercury into elemental mercury while that of mehg is governed by competition with two site sorption fig 3 shows schematics of the general tam model framework for both types of sediments schwartz et al 2022 modeled two site sorption as fast and slow first order reversible kinetic sorption similarly the reduction of hg ii to hg 0 was also modeled as a first order reversible kinetic process for sediment 1 both methylation and demethylation were modeled using monod kinetics for sediment 2 methylation and demethylation were modeled using first order kinetics the uncertainty aware parameter estimation was performed using a gradient based scheme nlinfit in matlab yielding locally optimal parameter values with first order error estimates schwartz et al 2022 found large parameter uncertainties and non unique solution sensitive to initial values for monod kinetics in sediment 1 additionally when they applied monod kinetics in sediment 2 the parameter estimation scheme failed to converge without giving any clear insights into the defects in the considered parameterization this is not surprising as potential overparameterization related model structure deficiencies can go undetected with gradient based non linear fitting schemes in this study we first implement the proposed bayesian joint fitting scheme i e jointly calibrating sorption and methylation models using mcmc with tam model in schwartz et al 2022 thereafter based on the insights from the resulting joint posterior distributions we aim to improve the tam model framework for both sediment 1 and sediment 2 to explain the experimental data better and obtain robust uncertainty estimates the system of odes in tam reaction network are solved using odeint from scipy integrate package in python which uses isoda from fortran odepack library 3 2 bayesian formulation we estimate all parameters in the tam framework simultaneously by fitting datasets from sorption and methylation experiments together we adopt the bayesian approach to estimate distributions of tam parameters conditioned on the measurements measurements from hg sorption mehg sorption and methylation experiments are represented by vectors c 1 m e a s c 2 m e a s and c 3 m e a s respectively measurements for model calibration are triplicate averages at each time point collectively all model parameters can be represented by vector θ and measurements by c m e a s using bayes theorem conditional probability density function p θ c m e a s can be given as 1 p θ c m e a s p c m e a s θ p θ where p c m e a s θ is the likelihood of the measurements c m e a s given the parameter set θ which represents the constraints imposed by c m e a s on θ the p θ is the probability density function representing the prior knowledge about parameters probability p c m e a s θ is composed of contributions from individual experiments as 2 p c m e a s θ i 1 3 p c i m e a s θ c i m e a s for all experiment is assumed to follow multi gaussian distribution 3 p c i m e a s θ 1 2 π n σ i exp 1 2 c i m e a s c i s i m t σ i 1 c i m e a s c i s i m where σ i represents the covariance matrix of the measurements σ i represents its determinant and n is the number of time points in the dataset we assume uncorrelated measurement errors resulting in a diagonal σ i matrix with measurement variances as diagonal elements we assume uniform priors for all parameters with a reasonable range based on domain knowledge we do not have any additional information to assume non uniform priors for cases when parameter distribution hit the boundary of the parameter space analysis is repeated with expanded range a few times to ensure there is no additional mode beyond range we assume a homoscedastic error model and evaluate pooled variance for each dataset i e variance of different populations with different means with presumably same variance this is a reasonable choice as triplicate variances did not show consistent trend with time it is important to note however that pooled variance calculated this way is still uncertain because of small sample size we ignore the uncertainty in variance here but note it could be estimated in a more robust way by taking the variance as a hyperparameter that is sampled along with main model parameters for each case we plotted the distribution of residuals of model fit and found them to be unimodal almost symmetric and gaussian except at early timestep in one of the cases hence our assumption of gaussian likelihood is reasonable for these datasets the example plots of residual distributions for methylation and demethylation in both types of sediments are provided in the appendix b 3 3 mcmc implementation for most practical cases it is almost impossible to analytically derive the joint distributions of parameters hence we use the mcmc technique a family of algorithms designed to approximate posterior distributions of variables of interest by drawing samples from their derived distributions mcmc has benefitted many fields where posterior inferences about complex systems in the bayesian framework are desired vrugt et al 2008 mcmc offers globally optimal solutions with full joint distributions of parameters this allows for the quantification of parameter uncertainties rigorously assessment of model structure adequacy and potential for model improvements in this study we use pydream shockley et al 2018 a python implementation of the dream zs laloy and vrugt 2012 algorithm which is one of the most advanced adaptions of diffrential evolution adaptive metropolis dream vrugt et al 2008 dream is a multi chain mcmc with the automatic adaption of step size and direction of sampler movement dream zs stores past states in archives which are then used to propose new positions making it highly efficient with quick convergence and fewer number of chains needed with a range of proposal maneuvers dream zs can sample from challenging parameter spaces like multimodality and highly correlated parameters key inputs to dream zs algorithm includes number of differential pairs equals to 3 gamma levels to 4 probability of unity gamma to 0 2 and probability of snooker step to 0 1 for more details about these options refer to shockley et al 2018 and laloy and vrugt 2012 we provide in the supporting information a modular python workflow in which each experiment is modeled in a separate python script and is accessed by the main python script performing mcmc therefore datasets from additional experiments can be conveniently included in the mcmc analyses the mcmc results are post processed analyzed and visualized in jupyter notebooks enabling an efficient and self documenting workflow the model data files are archived at rathore and painter 2021 4 results and discussion mcmc runs were performed using 10 parallel communicating chains with at least 25 000 generations per chain after chains had converged the convergence of chain is tested using reduction factor r ˆ criteria by gelman and rubin 1992 which when below 1 2 indicates the chain convergence plots for the evolution of r ˆ with the progression of chains for each mcmc run can be found in the si the posterior distributions of estimated parameters were summarized using at least 250 000 parameter sets yielding a robust estimation first we present parameter estimation with models of schwartz et al 2022 based on the insights into the parameter space we improve the model structure and discuss process inferences equations for final improved models are provided in the appendix a 4 1 bayesian joint fitting for transient availability model by schwartz et al 2022 4 1 1 model description fig 4 a and 5 a depicts models considered by schwartz et al 2022 for fitting sorption and methylation experiment data sequentially for both sediments 1 and 2 respectively hg sorption experiment is modeled using three first order reversible kinetic processes namely sorption desorption on fast sites h g f a s t i i with parameters k 1 t 1 and k 2 t 1 sorption desorption on slow sites h g s l o w i i with parameters k 3 t 1 and k 4 t 1 and first order reversible kinetic reduction to h g a q 0 with parameters k 5 t 1 and k 6 t 1 mehg sorption is modeled using first order reversible kinetic fast site sorption with parameters k 7 t 1 and k 10 t 1 and slow site sorption with parameters k 9 t 1 and k 10 t 1 they fitted hg and mehg models to respective experimental data and reported best fit parameters and quantified uncertainty as 5th and 95th confidence intervals thereafter the mercury methylation model under the tam framework was calibrated to estimate methylation and demethylation parameters keeping sorption parameters fixed as best fit values estimated previously in sediment 1 monod kinetics was used to model methylation and demethylation for methylation k m m a x m t 1 and k m h s m denote maximum reaction rate and half saturation constant respectively similarly for demethylation monod parameters are denoted as k d m a x m t 1 and k d h s m monod reactions exhibit first order behavior at concentrations significantly smaller than k h s and zeroth order behavior at concentrations significantly greater than k h s in sediment 2 reversible first order kinetics was used for methylation and demethylation with rate constants k m t 1 and k d t 1 respectively we estimate all parameters together by fitting seven datasets from three experiments described in section 2 simultaneously using mcmc to obtain full posterior joint distributions figs 4 b and 5 b maps experimental datasets to the parameters they are informing this allows for a rigorous treatment of uncertainty in all parameters and full utilization of the information shared among available datasets in section 4 1 2 joint distributions of parameters of the model by schwartz et al 2022 obtained from mcmc are presented for both sediment types marginal distributions and predictive uncertainty plots are provided in the si 4 1 2 joint distribution of parameters fig 6 and fig 7 present mcmc results for sediment 1 and sediment 2 respectively as individual parameter histograms and pairwise joint distributions r ˆ for the converged chains for all the parameters was approximately equal to 1 in both cases of sediment types for sediment 1 the distributions of parameters are unimodal with reasonable uncertainty except for hg reduction and the monod parameters the marginal distributions of the hg reduction parameters are almost non informative their joint distribution appears to be unconstrained with a strong linear correlation suggesting that the parameter space contains a null space defined by a distinctive k 6 k 5 ratio distribution presented in fig 6 this strongly suggests that for the considered system hg reduction should be modeled as an equilibrium reaction similarly monod parameters for demethylation k d m a x and k d h s also span a null space defined by their distinct ratio suggesting that the experimental data can be explained by a simpler first order kinetic model for demethylation monod methylation parameters k m m a x and k m h s also exhibit strong linear correlation and high uncertainty with a thick tail but with some central tendency as unique peaks because of this null space in demethylation parameter distributions and highly skewed non gaussian methylation parameter distributions schwartz et al 2022 obtained very large uncertainty estimates and non unique solutions for monod methylation demethylation parameters for sediment 2 the parameters for fast sorption for hg k 1 and k 2 hg reduction k 5 and k 6 and fast sorption of mehg k 7 and k 8 were non informative and unconstrained spanning a null space characterized by a distinct ratio of forward to reverse rate constants distributions of ratios shown in fig 7 this favors replacing the reversible kinetic models for hg reduction and fast sorption of hg and mehg with corresponding equilibrium models the non linear least square fitting scheme as adopted in schwartz et al 2022 does not offer such insights into parameter space the best fit parameters for these subprocess models by schwartz et al 2021a are one of the many equally good possibilities in the null space and their local error estimates stem from perturbations around the best fit parameters in section 4 2 based on the insights obtained from mcmc results improved models are proposed and parameter estimation is performed for new models 4 2 bayesian joint fitting for updated models based on the insights gained from the mcmc analysis of the schwartz et al 2022 model we propose improved models and perform bayesian parameter estimation to obtain joint parameter distributions and quantify uncertainties for both sediment 1 and sediment 2 r ˆ for all the parameters was approximately equal to 1 in both cases of sediment types except for k d in sediment 1 for which final r ˆ is equal to 1 07 model descriptions estimated parameters distributions summary statistics and predictive uncertainties are presented in the following subsections 4 2 1 sediment 1 under the improved model structure for sediment 1 mercury reduction is modeled as an equilibrium process parameterized by a single parameter the dimensionless equilibrium constant k 5 6 note that the subscripts of the equilibrium constant are related to the corresponding kinetic models in fig 4 for the sake of clarity and convenience additionally demethylation is modeled using first order kinetics with a rate constant k d t 1 the model structure and estimated joint parameter distributions are presented in fig 8 marginal distributions are provided in si all parameters are estimated as unimodal distributions monod parameters for methylation exhibit high uncertainty with a thick tail this ambiguity is potentially due to time varying microbial activity due to evolving redox conditions in the sediments schwartz et al 2022 the summary statistics of parameters are presented in table 1 in the form of 5th 25th median 75th and 95th percentiles fig 9 presents the predictive uncertainty plots obtained by generating an ensemble of sorption and methylation model runs using the parameters sets from mcmc model reduction from kinetic to equilibrium for hg reduction and from monod for first order kinetics for demethylation was found to be adequate as the model simulation results explain the experimental data well for both sorption and methylation experiments the parameter estimates and thus predictions benefit from the joint fitting of sorption and methylation datasets and estimating all parameters simultaneously 4 2 2 sediment 2 parameters for the fast site sorption for hg and mehg considered by schwartz et al 2022 exhibited high ambiguity as they contain a null space with a distinctive ratio of forward and reverse rate constants in the improved model sorption of hg and mehg on sediment 2 is modeled as two site sorption i e equilibrium and kinetic sorption equilibrium sorption constants for hg and mehg are represented by k 1 2 and k 7 8 respectively similar to sediment 1 hg reduction in sediment 2 is modeled as an equilibrium process with the equilibrium constant as k 5 6 guo et al 2019 showed that the uncertainty in the thermodynamic constants for equilibrium processes has an outsized impact on the output uncertainties hence it is critical to obtain global uncertainty estimates for these constants the joint distribution of estimated parameters in fig 10 are well constrained and unimodal the summary statistics of estimated parameters are presented in table 2 predictive uncertainty plots in fig 11 show good agreement between model simulations and experimental data for both sorption and methylation experiments 5 summary and conclusions in this paper we proposed bayesian joint fitting a parameter estimation workflow for biogeochemical systems with two main advancements from traditional methods 1 simultaneous fitting of datasets from multiple experiments characterizing competing subprocesses within the system 2 mapping full joint distribution of parameters using mcmc the efficacy of the proposed method was demonstrated with mercury methylation as a use case the method was deployed to reinterpret recently published datasets schwartz et al 2021b characterizing mercury methylation on aquatic sediments under the transient availability model tam framework olsen et al 2018 schwartz et al 2022 rigorous estimation of parameter uncertainties and improvement in the tam framework were achieved simultaneous fitting of datasets from sorption and methylation experiments to estimate all tam parameters together comprehensively captured interactions between sorption and methylation processes this approach allowed the inclusion of the measurement uncertainties in the sorption experiments for both hg and mehg into methylation parameters in the tam model framework another advantage of simultaneously fitting datasets is sorption parameters were informed by datasets from both sorption and methylation experiments mapping full joint distributions of parameters using mcmc yielded global optimal parameters and robust uncertainty estimates additionally it revealed deficiencies in the existing model structure which eluded first order error estimates for example the identification of null spaces in the original sorption model schwartz et al 2022 in sediment 2 and the hg reduction model for both sediments proved valuable in simplifying the model from kinetic to equilibrium thus reducing the parameter uncertainty arising due to over parameterization similarly null space in parameters for monod demethylation in schwartz et al 2022 model suggested replacing it with first order kinetics note that lumping null space parameters into a single parameter is one of the many ways of model reduction this inverse modeling based approach simplifies the model with null space identification without compromising on the goodness of fit and prediction uncertainties the different fast sorption dynamics of the finer grained sediment 1 and coarser grained sediment 2 is consistent with reduced access to sorption sites due to low diffusion coefficients in sediment 1 such insights can be helpful to guide future experiments for better resolving processes that exert control on hg methylation mehg demethylation dynamics in environmental systems for example the mehg concentrations measured in samples is the net result of the opposing processes of hg methylation and mehg demethylation the simultaneous fitting approach discriminated among competing models describing these processes and identified parameters with greater relative uncertainty which in turn will inform future data gathering efforts to design experiments that will decrease uncertainty in those parameters other approaches include using theoretical measures like akaike information criterion or bayesian information criterion to trade off the goodness of fit for the model simplicity bayesian inference from experimental datasets schwartz et al 2021b revealed different methylation and sorption dynamics for sediment 1 and sediment 2 in sediment 1 two site kinetic sorption fast and slow explained the datasets well in sediment 2 data did not capture kinetic behavior for fast sorption making the associated kinetic parameters unidentifiable the equilibrium sorption along with slow kinetic sorption resulted in a good fit between the model and data for the methylation experiment in sediment 1 monod methylation and first order demethylation gave a decent fit to the data with unique parameter estimates the dataset was unable to uniquely identify monod parameters for demethylation in sediment 2 first order kinetics for both methylation and demethylation offered a good match with the data in summary the proposed method of joint fitting of multiple experiment datasets in the bayesian framework and mapping full joint distributions using mcmc were shown to be effective in improving the parameterization of biogeochemical models quantifying the uncertainties in parameters and outputs and evaluating different model structures this approach can be beneficial particularly in a system with multiple competing reactions taking place at different timescales insights into parameter space through joint probability distributions coupled with domain knowledge guides model improvements full quantification of uncertainties informs the design of future experiments targeted at constraining poorly constrained parameters eliminating multimodality or dealing with parameter degeneracy improved model structures and robust uncertainty estimates offer better understanding of underlying mechanisms and reliable predictions of fate of metals and nutrients in the environment parameter estimation using mcmc will not be suitable for models with high runtime as mcmc requires large number of model runs raising the computational cost however typical reaction networks are odes and even a complex reaction network is relatively fast to solve additionally with an increased access to computational resources there is no reason why researcher should not opt for robust uncertainty estimates using mcmc which offers flexibility of analyzing multiple experiments together and also guide domain knowledge based model improvements in the process the workflow for this approach is coded in python and scripts are provided in the model data archive at rathore and painter 2021 the modular nature of the workflow allows for inclusion of additional experimental data or a new reaction to the reaction network conveniently software and data availability the original experimental dataset is archived at schwartz et al 2021b the model data and workflow for this manuscript is archived at rathore and painter 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the u s department of energy office of science biological and environmental research subsurface biogeochemical research sbr program and is a product of the critical interfaces science focus area sfa at ornl and the ideas watersheds project this research used resources of the compute and data environment for science cades at the oak ridge national laboratory which is supported by the office of science of the u s department of energy under contract no de ac05 00or22725 we thank the editor and anonymous reviewer for their constructive feedback on the paper appendix a here we present mathematical equations for improved models for mercury methylation on aquatic sediments presented in this manuscript variables are described in the main text of the manuscript sediment 1 1 d h g d t k m max k m h s h g h g k d m e h g k 1 h g k 2 h g f k 3 h g k 4 h g s 2 d m e h g d t k m max k m h s h g h g k d m e h g k 7 m e h g k 8 m e h g f k 9 m e h g k 10 m e h g s 3 d h g f d t k 1 h g k 2 h g f 4 d h g s d t k 3 h g k 4 h g s 5 d m e h g f d t k 7 m e h g k 8 m e h g f 6 d m e h g s d t k 9 m e h g k 10 m e h g s 7 h g 0 1 k 5 6 h g sediment 2 8 d h g d t k m h g k d m e h g k 3 h g k 4 h g s 9 d m e h g d t k m h g k d m e h g k 9 m e h g k 10 m e h g s 10 d h g s d t k 3 h g k 4 h g s 11 d m e h g s d t k 9 m e h g k 10 m e h g s 12 h g f 1 k 1 2 h g 13 h g 0 1 k 5 6 h g 14 m e h g f 1 k 7 8 m e h g appendix b example plots of residual distribution for the time series of me201hg and me202hg in methylation and demethylation experiments for both sediment 1 and sediment 2 are shown below fig b 1 distribution of residuals for me201hg predictions are different timepoints for methylation experiment in sediment 1 fig b 1 fig b 2 distribution of residuals for me202hg predictions are different timepoints for demethylation experiment in sediment 1 fig b 2 fig b 3 distribution of residuals for me201hg predictions are different timepoints for methylation experiment in sediment 2 fig b 3 fig b 4 distribution of residuals for me202hg predictions are different timepoints for demethylation experiment in sediment 2 fig b 4 
25568,to characterize complex biogeochemical systems results from multiple experiments where each targets a specific subprocess are commonly combined the resulting datasets are interpreted through the calibration of biogeochemical models for process inference and predictions commonly used calibration approaches of fitting datasets from individual experiments to subprocess models one at a time is prone to missing information shared between datasets and incomplete uncertainty propagation we propose a bayesian joint fitting scheme addressing the above mentioned concerns by jointly fitting all the available datasets thus calibrating the entire biogeochemical model in one go using markov chain monte carlo mcmc the identification of null spaces in the parameter distributions from mcmc guided the simplification of certain subprocess models for example fast kinetic sorption was replaced by equilibrium sorption and monod demethylation was replaced by first order demethylation joint fitting of datasets resulted in complete uncertainty propagation with parameter estimates informed by all available data keywords bayesian inference parameter uncertainty mercury methylation 1 introduction many stream ecosystems are under stress due to contamination driven by anthropogenic activities in particular trace metals such as mercury lead and cadmium released from industrial activities pose a serious threat to human and ecosystem health because of their toxicity even at minimum levels and potential of bioaccumulation chen and folt 2000 evers et al 2007 goodyear and mcneill 1999 horowitz 1991 mason et al 2000 ward et al 2010 a comprehensive understanding of their transformation in the environment requires considerations of the complex interplay between physical chemical and biological processes at different spatial and temporal scales carefully designed field and laboratory experiments gather critical data about these processes however the usefulness of these datasets depends on their reliable interpretations empirical relations can fit the data well but can struggle to extrapolate due to a lack of mechanistic underpinnings process based biogeochemical models are developed and calibrated using the experimental data for mechanistically rich process inferences and predictions there are generally multiple candidate biogeochemical models available to fit the data e g first order kinetics equilibrium monod and tessier all these models are upscaled phenomenological descriptions of complex biogeochemical processes and do not perfectly represent the biogeochemical system since the true model is unknown neumann and gujer 2008 typically based on preliminary data analysis and expert knowledge a model is proposed and calibrated to estimate model parameters however often initially proposed mechanistic models are overparameterized needing data driven methods for model simplification and unique parameter estimates commonly used approaches in biogeochemical modeling and calibration are prone to underutilization of the data available from multiple experiments incomplete uncertainty quantification and model overparameterization below we describe some of the challenges in interpreting data with commonly used biogeochemical modeling approaches and how we aim to address them characterization of biogeochemical systems often requires multiple experiments targeting a subset of processes in the system referred henceforth as subprocesses for example sorption experiments are performed separately from reaction focused experiments in various biogeochemical applications e g haggerty et al 2008 lemke et al 2014 olsen et al 2018 schwartz et al 2021a models for subprocesses like sorption are often calibrated first and the resulting best fit parameters are kept fixed in the subsequent estimation of reaction parameters in a broader model framework this sequential fitting scheme has two main shortcomings first it fails to make full use of all information information on subprocesses that are common to multiple experiments is not fully used when subprocess parameters are determined by analysis of one experiment and then held fixed in subsequent analyses second sequential fitting fails to propagate uncertainty from one experiment to the next for example fixing sorption parameters when analyzing reaction experiments ignores the fact that the reaction experiments contain information about both sorption and reactions moreover it fails to propagate uncertainty from the analyzed sorption experiment to the analysis of the reaction experiment fig 1 shows this sequential fitting scheme in a general biogeochemical model comprising three subprocesses that are calibrated sequentially we propose that the datasets from multiple experiments should be fitted simultaneously thus calibrating all subprocess models together by doing so we leverage information sharing about subprocesses among datasets additionally this joint fitting scheme also allows uncertainty propagation among subprocess models the surface kelleher et al 2019 knapp and cirpka 2017 lemke et al 2013 liao et al 2013 and subsurface luo et al 2006 zhao et al 2018 hydrologists have previously recognized merit in this joint fitting approach these studies fitted conservative and reactive tracer data from tracer experiments jointly rather than sequentially to leverage shared information and propagate uncertainty fully biogeochemical systems typically more complex with a larger number of highly intertwined subprocesses can particularly benefit from sharing information because of the different scales and interactions of these subprocesses captured in different experiments a more general problem and very relevant to biogeochemical modeling is the lack of parameter identifiability beck 1987 which essentially means that significantly different parameter values can reproduce the experimental data equally well this can result from insufficient information in the experimental data to match the model complexity or incorrect model structure marschmann et al 2019 although non linear least square schemes can yield parameters that produce a good match between simulations and experimental data in such cases the model structure deficiency can go undetected leading to non meaningful local uncertainty estimates and faulty process inferences marschmann et al 2019 neumann and gujer 2008 van turnhout et al 2016 neumann and gujer 2008 showed that bias related to model structure in environmental models can be diagnosed through analysis of residuals from non linear regression schemes marschmann et al 2019 used information geometry to identify model structure limitations and reduce the model complexity in mechanistically rich biogeochemical models suffering from the lack of parameter identifiability with the advancements in computational capabilities computationally intensive bayesian inverse modeling have become accessible to widespread applications including biogeochemical modeling mapping full joint distributions of parameters in bayesian approach can reveal parameter uncertainties sensitivities interactions and potential null spaces which can guide model improvements arhonditsis et al 2008 applied bayesian calibration using markov chain monte carlo mcmc to rigorously quantify parameter and predictive uncertainty in aquatic biogeochemical models zhang and arhonditsis 2009 using synthetic aquatic biogeochemistry datasets demonstrated the strength of the bayesian approach in transferring information across systems as priors in a hierarchical calibration scheme van oijen et al 2011 calibrated four different biogeochemical models for norway spruce forest using mcmc they compared models through integrated likelihood values of the estimated parameter distributions van turnhout et al 2016 developed a toolbox with bayesian inference based criteria for selecting optimal reaction network in municipal solid waste landfills davoudabadi et al 2021 used particle filter based advanced bayesian methods to calibrate high complexity state space models with soil carbon sequestration as an example in this paper we present a holistic framework of bayesian joint fitting for biogeochemical models leveraging information from multiple experiments treating uncertainty rigorously and detecting potential parameterization deficiencies this approach broadly has two facets first we propose that the datasets from multiple experiments should be fitted simultaneously thus calibrating all subprocess models together by doing so we are informing each subprocess model using all the available datasets involving that particular subprocess additionally this joint fitting scheme also allows uncertainty propagation among subprocess models and different datasets second we map the full joint distribution of parameters using the advanced markov chain monte carlo mcmc method this approach not only yields robust global uncertainty estimates and but can also identify model deficiencies causing the lack of parameter identifiability we apply the proposed workflow to a recently published mercury methylation dataset schwartz et al 2021b to improve data interpretation and advance process inferences characterization of mercury methylation processes a use case in this paper is of significantly high importance as methylmercury mehg poses a great threat to humans and wildlife eckley et al 2020 occurring in the environment as a natural and anthropogenic pollutant hg is methylated to form the neurotoxin mehg through microbially mediated processes clarkson et al 2003 mehg ingestion even at a low level has adverse impacts on the development of children mehg can get biomagnified in aquatic food webs which makes it particularly concerning mergler et al 2007 significant experimental and modeling efforts have been made to understand mercury methylation dynamics avramescu et al 2011 hintelmann et al 2000 jonsson et al 2012 liem nguyen et al 2016 mitchell and gilmour 2008 olsen et al 2018 rodrı guez martı n doimeadios et al 2004 schwartz et al 2019 traditionally first order reversible kinetics have been used to model mercury methylation hintelmann et al 2000 however apparent non first order kinetic behavior has often been observed in mercury methylation demethylation data avramescu et al 2011 jonsson et al 2012 olsen et al 2018 olsen et al 2018 studying mercury methylation in periphyton biofilms suggested that the apparent non first order behavior does not necessarily imply non first order kinetics but can result from other competing processes making hg and mehg unavailable for methylation and demethylation respectively they proposed the transient availability model tam accounting for competing processes like multisite kinetic sorption of hg and mehg and reduction of hg schwartz et al 2022 extended the tam framework to aquatic sediment systems to model mercury methylation on two disparate sediments from east fork poplar creek efpc in oak ridge tn usa tam application by schwartz et al 2022 considered fully kinetic competing processes for bioavailability with first order methylation demethylation for one sediment type and reversible monod kinetics for another with the sequential fitting of sorption and methylation datasets and a gradient based scheme for parameter estimation they reproduced experimental data well but obtained non meaningful error estimates for certain parameters using the novel mercury methylation dataset schwartz et al 2021b we implement the proposed parameter estimation workflow in the tam framework olsen et al 2018 and schwartz et al 2022 studied biologically mediated methylation of mercury hg to the neurotoxin methylmercury mehg on periphyton film and colonized sediments respectively both studies fitted sorption experiment datasets first to estimate sorption parameters which were then fixed while estimating methylation demethylation parameters fitting datasets from methylation experiments however it is important to recognize that these processes are not independent and methylation demethylation datasets also contain information about the sorption processes which is not leveraged in this sequential fitting approach additionally because of using the best fit values of sorption parameters the estimates of methylation demethylation parameters ignore uncertainties in sorption datasets in our approach sorption and methylation datasets are fitted simultaneously and respective parameters are jointly estimated thus addressing the above described concerns for the uncertainty aware parameter estimation olsen et al 2018 and schwartz et al 2022 adopted a gradient based local optimization scheme which works well in unimodal smooth parameter spaces but can suffer in multimodal or other atypical parameter spaces additionally under such schemes other model structure related issues like overparameterization may go undetected we adopt a bayesian approach using mcmc a global search scheme yielding a full joint distribution of parameters the obtained parameter distributions are expected to guide model improvements in addition to offering robust uncertainty estimates with the proposed parameter estimation workflow of bayesian joint fitting we aim to improve the interpretation of the rich mercury methylation dataset by schwartz et al 2021b the proposed workflow is easily transferable to other complex biogeochemical systems characterized through multiple experiments for reliable parameter estimates process inferences and predictions 2 overview of experiments and datasets here we provide a brief overview of the study site and experiments for more information about the site see brooks and southworth 2011 riscassi et al 2016 and for experimental details see schwartz et al 2022 schwartz et al 2022 collected sediments from efpc that have legacy mercury contamination and performed sorption and mercury methylation experiments to characterize and quantify their methylation and demethylation potential and associated physical chemical and microbial processes two contrasting sediment types were collected for use in laboratory studies sediment 1 rich in organic matter relatively anoxic fine sand predominant along the stream edge and sediment 2 a medium to coarse sand lower in organic carbon and less metabolically active than sediment 1 and predominant in the center of the channel for hg and mehg sorption experiments isotopically labeled 201hg or me202hg were added to sediment creek water suspension under air and placed on a reciprocating shaker the number of time points at which samples were collected for hg are 12 and 14 for sediment 1 and for sediment 2 respectively and for mehg 14 and 15 respectively triplicate samples were sacrificed at each time point in the hg sorption experiment hg in the aqueous phase total solid phase hg and hg 0 were quantified at different time points under the oxic conditions of the hg sorption experiments hg methylation an obligately anaerobic microbial process did not occur in the mehg sorption experiments aqueous me202hg quantified at different time points from triplicate samples and total me202hg in the system was quantified at select time points to verify the lack of demethylation of the added isotope schwartz et al 2022 mercury methylation experiments were conducted on carefully prepared anoxic sediment slurry microcosms three treatments were established one spiked with 201hg to monitor methylation via production of me201hg the second spiked with me202hg to monitor demethylation via loss of me202hg and treatment with no spike added to monitor other biogeochemical parameters of interest e g fe ii sulfide because the hg and mehg analyses were full consumptive and destructive the experiment for sediment 2 was run longer because of the low activity initially measured in those sediments the number of timepoints at which samples were collected is 4 and 3 for sediment 1 and sediment 2 respectively three microcosms from each treatment were destroyed at each time point for hg and mehg analysis the resulting datasets from mercury methylation experiments are total me201hg and me202hg at each time point schwartz et al 2022 3 bayesian joint fitting approach the data from the experiments by schwartz et al 2022 captures information about the biogeochemical processes controlling methylation of mercury on colonized river sediments models under the tam framework are proposed for these biogeochemical processes and calibrated to match the experimental data allowing us to make process inferences and develop prediction capabilities experience from the interpretation of mercury methylation datasets by schwartz et al 2022 reveals several shortcomings of commonly used biogeochemical model calibration practices which we discussed in the introduction section fig 1 our approach leverages information shared among experiments by joint calibration of subprocesses using all the available datasets additionally full joint distribution of parameters is mapped to get robust uncertainty estimates and detect potential model structure deficiencies in fig 2 we present a schematic of our bayesian joint fitting scheme and the advantages of different facets of the scheme in the following sub sections we describe different models considered for sorption and methylation provide bayesian formulation and details of mcmc application 3 1 transient availability model framework the tam model framework developed by olsen et al 2018 is used to describe mercury methylation under the tam model framework methylation and demethylation of mercury are strongly influenced by the availability of hg and mehg in the aqueous phase experiments suggest that the transient availability of hg in the aqueous phase is caused by competition with two site sorption and reversible reduction of aqueous mercury into elemental mercury while that of mehg is governed by competition with two site sorption fig 3 shows schematics of the general tam model framework for both types of sediments schwartz et al 2022 modeled two site sorption as fast and slow first order reversible kinetic sorption similarly the reduction of hg ii to hg 0 was also modeled as a first order reversible kinetic process for sediment 1 both methylation and demethylation were modeled using monod kinetics for sediment 2 methylation and demethylation were modeled using first order kinetics the uncertainty aware parameter estimation was performed using a gradient based scheme nlinfit in matlab yielding locally optimal parameter values with first order error estimates schwartz et al 2022 found large parameter uncertainties and non unique solution sensitive to initial values for monod kinetics in sediment 1 additionally when they applied monod kinetics in sediment 2 the parameter estimation scheme failed to converge without giving any clear insights into the defects in the considered parameterization this is not surprising as potential overparameterization related model structure deficiencies can go undetected with gradient based non linear fitting schemes in this study we first implement the proposed bayesian joint fitting scheme i e jointly calibrating sorption and methylation models using mcmc with tam model in schwartz et al 2022 thereafter based on the insights from the resulting joint posterior distributions we aim to improve the tam model framework for both sediment 1 and sediment 2 to explain the experimental data better and obtain robust uncertainty estimates the system of odes in tam reaction network are solved using odeint from scipy integrate package in python which uses isoda from fortran odepack library 3 2 bayesian formulation we estimate all parameters in the tam framework simultaneously by fitting datasets from sorption and methylation experiments together we adopt the bayesian approach to estimate distributions of tam parameters conditioned on the measurements measurements from hg sorption mehg sorption and methylation experiments are represented by vectors c 1 m e a s c 2 m e a s and c 3 m e a s respectively measurements for model calibration are triplicate averages at each time point collectively all model parameters can be represented by vector θ and measurements by c m e a s using bayes theorem conditional probability density function p θ c m e a s can be given as 1 p θ c m e a s p c m e a s θ p θ where p c m e a s θ is the likelihood of the measurements c m e a s given the parameter set θ which represents the constraints imposed by c m e a s on θ the p θ is the probability density function representing the prior knowledge about parameters probability p c m e a s θ is composed of contributions from individual experiments as 2 p c m e a s θ i 1 3 p c i m e a s θ c i m e a s for all experiment is assumed to follow multi gaussian distribution 3 p c i m e a s θ 1 2 π n σ i exp 1 2 c i m e a s c i s i m t σ i 1 c i m e a s c i s i m where σ i represents the covariance matrix of the measurements σ i represents its determinant and n is the number of time points in the dataset we assume uncorrelated measurement errors resulting in a diagonal σ i matrix with measurement variances as diagonal elements we assume uniform priors for all parameters with a reasonable range based on domain knowledge we do not have any additional information to assume non uniform priors for cases when parameter distribution hit the boundary of the parameter space analysis is repeated with expanded range a few times to ensure there is no additional mode beyond range we assume a homoscedastic error model and evaluate pooled variance for each dataset i e variance of different populations with different means with presumably same variance this is a reasonable choice as triplicate variances did not show consistent trend with time it is important to note however that pooled variance calculated this way is still uncertain because of small sample size we ignore the uncertainty in variance here but note it could be estimated in a more robust way by taking the variance as a hyperparameter that is sampled along with main model parameters for each case we plotted the distribution of residuals of model fit and found them to be unimodal almost symmetric and gaussian except at early timestep in one of the cases hence our assumption of gaussian likelihood is reasonable for these datasets the example plots of residual distributions for methylation and demethylation in both types of sediments are provided in the appendix b 3 3 mcmc implementation for most practical cases it is almost impossible to analytically derive the joint distributions of parameters hence we use the mcmc technique a family of algorithms designed to approximate posterior distributions of variables of interest by drawing samples from their derived distributions mcmc has benefitted many fields where posterior inferences about complex systems in the bayesian framework are desired vrugt et al 2008 mcmc offers globally optimal solutions with full joint distributions of parameters this allows for the quantification of parameter uncertainties rigorously assessment of model structure adequacy and potential for model improvements in this study we use pydream shockley et al 2018 a python implementation of the dream zs laloy and vrugt 2012 algorithm which is one of the most advanced adaptions of diffrential evolution adaptive metropolis dream vrugt et al 2008 dream is a multi chain mcmc with the automatic adaption of step size and direction of sampler movement dream zs stores past states in archives which are then used to propose new positions making it highly efficient with quick convergence and fewer number of chains needed with a range of proposal maneuvers dream zs can sample from challenging parameter spaces like multimodality and highly correlated parameters key inputs to dream zs algorithm includes number of differential pairs equals to 3 gamma levels to 4 probability of unity gamma to 0 2 and probability of snooker step to 0 1 for more details about these options refer to shockley et al 2018 and laloy and vrugt 2012 we provide in the supporting information a modular python workflow in which each experiment is modeled in a separate python script and is accessed by the main python script performing mcmc therefore datasets from additional experiments can be conveniently included in the mcmc analyses the mcmc results are post processed analyzed and visualized in jupyter notebooks enabling an efficient and self documenting workflow the model data files are archived at rathore and painter 2021 4 results and discussion mcmc runs were performed using 10 parallel communicating chains with at least 25 000 generations per chain after chains had converged the convergence of chain is tested using reduction factor r ˆ criteria by gelman and rubin 1992 which when below 1 2 indicates the chain convergence plots for the evolution of r ˆ with the progression of chains for each mcmc run can be found in the si the posterior distributions of estimated parameters were summarized using at least 250 000 parameter sets yielding a robust estimation first we present parameter estimation with models of schwartz et al 2022 based on the insights into the parameter space we improve the model structure and discuss process inferences equations for final improved models are provided in the appendix a 4 1 bayesian joint fitting for transient availability model by schwartz et al 2022 4 1 1 model description fig 4 a and 5 a depicts models considered by schwartz et al 2022 for fitting sorption and methylation experiment data sequentially for both sediments 1 and 2 respectively hg sorption experiment is modeled using three first order reversible kinetic processes namely sorption desorption on fast sites h g f a s t i i with parameters k 1 t 1 and k 2 t 1 sorption desorption on slow sites h g s l o w i i with parameters k 3 t 1 and k 4 t 1 and first order reversible kinetic reduction to h g a q 0 with parameters k 5 t 1 and k 6 t 1 mehg sorption is modeled using first order reversible kinetic fast site sorption with parameters k 7 t 1 and k 10 t 1 and slow site sorption with parameters k 9 t 1 and k 10 t 1 they fitted hg and mehg models to respective experimental data and reported best fit parameters and quantified uncertainty as 5th and 95th confidence intervals thereafter the mercury methylation model under the tam framework was calibrated to estimate methylation and demethylation parameters keeping sorption parameters fixed as best fit values estimated previously in sediment 1 monod kinetics was used to model methylation and demethylation for methylation k m m a x m t 1 and k m h s m denote maximum reaction rate and half saturation constant respectively similarly for demethylation monod parameters are denoted as k d m a x m t 1 and k d h s m monod reactions exhibit first order behavior at concentrations significantly smaller than k h s and zeroth order behavior at concentrations significantly greater than k h s in sediment 2 reversible first order kinetics was used for methylation and demethylation with rate constants k m t 1 and k d t 1 respectively we estimate all parameters together by fitting seven datasets from three experiments described in section 2 simultaneously using mcmc to obtain full posterior joint distributions figs 4 b and 5 b maps experimental datasets to the parameters they are informing this allows for a rigorous treatment of uncertainty in all parameters and full utilization of the information shared among available datasets in section 4 1 2 joint distributions of parameters of the model by schwartz et al 2022 obtained from mcmc are presented for both sediment types marginal distributions and predictive uncertainty plots are provided in the si 4 1 2 joint distribution of parameters fig 6 and fig 7 present mcmc results for sediment 1 and sediment 2 respectively as individual parameter histograms and pairwise joint distributions r ˆ for the converged chains for all the parameters was approximately equal to 1 in both cases of sediment types for sediment 1 the distributions of parameters are unimodal with reasonable uncertainty except for hg reduction and the monod parameters the marginal distributions of the hg reduction parameters are almost non informative their joint distribution appears to be unconstrained with a strong linear correlation suggesting that the parameter space contains a null space defined by a distinctive k 6 k 5 ratio distribution presented in fig 6 this strongly suggests that for the considered system hg reduction should be modeled as an equilibrium reaction similarly monod parameters for demethylation k d m a x and k d h s also span a null space defined by their distinct ratio suggesting that the experimental data can be explained by a simpler first order kinetic model for demethylation monod methylation parameters k m m a x and k m h s also exhibit strong linear correlation and high uncertainty with a thick tail but with some central tendency as unique peaks because of this null space in demethylation parameter distributions and highly skewed non gaussian methylation parameter distributions schwartz et al 2022 obtained very large uncertainty estimates and non unique solutions for monod methylation demethylation parameters for sediment 2 the parameters for fast sorption for hg k 1 and k 2 hg reduction k 5 and k 6 and fast sorption of mehg k 7 and k 8 were non informative and unconstrained spanning a null space characterized by a distinct ratio of forward to reverse rate constants distributions of ratios shown in fig 7 this favors replacing the reversible kinetic models for hg reduction and fast sorption of hg and mehg with corresponding equilibrium models the non linear least square fitting scheme as adopted in schwartz et al 2022 does not offer such insights into parameter space the best fit parameters for these subprocess models by schwartz et al 2021a are one of the many equally good possibilities in the null space and their local error estimates stem from perturbations around the best fit parameters in section 4 2 based on the insights obtained from mcmc results improved models are proposed and parameter estimation is performed for new models 4 2 bayesian joint fitting for updated models based on the insights gained from the mcmc analysis of the schwartz et al 2022 model we propose improved models and perform bayesian parameter estimation to obtain joint parameter distributions and quantify uncertainties for both sediment 1 and sediment 2 r ˆ for all the parameters was approximately equal to 1 in both cases of sediment types except for k d in sediment 1 for which final r ˆ is equal to 1 07 model descriptions estimated parameters distributions summary statistics and predictive uncertainties are presented in the following subsections 4 2 1 sediment 1 under the improved model structure for sediment 1 mercury reduction is modeled as an equilibrium process parameterized by a single parameter the dimensionless equilibrium constant k 5 6 note that the subscripts of the equilibrium constant are related to the corresponding kinetic models in fig 4 for the sake of clarity and convenience additionally demethylation is modeled using first order kinetics with a rate constant k d t 1 the model structure and estimated joint parameter distributions are presented in fig 8 marginal distributions are provided in si all parameters are estimated as unimodal distributions monod parameters for methylation exhibit high uncertainty with a thick tail this ambiguity is potentially due to time varying microbial activity due to evolving redox conditions in the sediments schwartz et al 2022 the summary statistics of parameters are presented in table 1 in the form of 5th 25th median 75th and 95th percentiles fig 9 presents the predictive uncertainty plots obtained by generating an ensemble of sorption and methylation model runs using the parameters sets from mcmc model reduction from kinetic to equilibrium for hg reduction and from monod for first order kinetics for demethylation was found to be adequate as the model simulation results explain the experimental data well for both sorption and methylation experiments the parameter estimates and thus predictions benefit from the joint fitting of sorption and methylation datasets and estimating all parameters simultaneously 4 2 2 sediment 2 parameters for the fast site sorption for hg and mehg considered by schwartz et al 2022 exhibited high ambiguity as they contain a null space with a distinctive ratio of forward and reverse rate constants in the improved model sorption of hg and mehg on sediment 2 is modeled as two site sorption i e equilibrium and kinetic sorption equilibrium sorption constants for hg and mehg are represented by k 1 2 and k 7 8 respectively similar to sediment 1 hg reduction in sediment 2 is modeled as an equilibrium process with the equilibrium constant as k 5 6 guo et al 2019 showed that the uncertainty in the thermodynamic constants for equilibrium processes has an outsized impact on the output uncertainties hence it is critical to obtain global uncertainty estimates for these constants the joint distribution of estimated parameters in fig 10 are well constrained and unimodal the summary statistics of estimated parameters are presented in table 2 predictive uncertainty plots in fig 11 show good agreement between model simulations and experimental data for both sorption and methylation experiments 5 summary and conclusions in this paper we proposed bayesian joint fitting a parameter estimation workflow for biogeochemical systems with two main advancements from traditional methods 1 simultaneous fitting of datasets from multiple experiments characterizing competing subprocesses within the system 2 mapping full joint distribution of parameters using mcmc the efficacy of the proposed method was demonstrated with mercury methylation as a use case the method was deployed to reinterpret recently published datasets schwartz et al 2021b characterizing mercury methylation on aquatic sediments under the transient availability model tam framework olsen et al 2018 schwartz et al 2022 rigorous estimation of parameter uncertainties and improvement in the tam framework were achieved simultaneous fitting of datasets from sorption and methylation experiments to estimate all tam parameters together comprehensively captured interactions between sorption and methylation processes this approach allowed the inclusion of the measurement uncertainties in the sorption experiments for both hg and mehg into methylation parameters in the tam model framework another advantage of simultaneously fitting datasets is sorption parameters were informed by datasets from both sorption and methylation experiments mapping full joint distributions of parameters using mcmc yielded global optimal parameters and robust uncertainty estimates additionally it revealed deficiencies in the existing model structure which eluded first order error estimates for example the identification of null spaces in the original sorption model schwartz et al 2022 in sediment 2 and the hg reduction model for both sediments proved valuable in simplifying the model from kinetic to equilibrium thus reducing the parameter uncertainty arising due to over parameterization similarly null space in parameters for monod demethylation in schwartz et al 2022 model suggested replacing it with first order kinetics note that lumping null space parameters into a single parameter is one of the many ways of model reduction this inverse modeling based approach simplifies the model with null space identification without compromising on the goodness of fit and prediction uncertainties the different fast sorption dynamics of the finer grained sediment 1 and coarser grained sediment 2 is consistent with reduced access to sorption sites due to low diffusion coefficients in sediment 1 such insights can be helpful to guide future experiments for better resolving processes that exert control on hg methylation mehg demethylation dynamics in environmental systems for example the mehg concentrations measured in samples is the net result of the opposing processes of hg methylation and mehg demethylation the simultaneous fitting approach discriminated among competing models describing these processes and identified parameters with greater relative uncertainty which in turn will inform future data gathering efforts to design experiments that will decrease uncertainty in those parameters other approaches include using theoretical measures like akaike information criterion or bayesian information criterion to trade off the goodness of fit for the model simplicity bayesian inference from experimental datasets schwartz et al 2021b revealed different methylation and sorption dynamics for sediment 1 and sediment 2 in sediment 1 two site kinetic sorption fast and slow explained the datasets well in sediment 2 data did not capture kinetic behavior for fast sorption making the associated kinetic parameters unidentifiable the equilibrium sorption along with slow kinetic sorption resulted in a good fit between the model and data for the methylation experiment in sediment 1 monod methylation and first order demethylation gave a decent fit to the data with unique parameter estimates the dataset was unable to uniquely identify monod parameters for demethylation in sediment 2 first order kinetics for both methylation and demethylation offered a good match with the data in summary the proposed method of joint fitting of multiple experiment datasets in the bayesian framework and mapping full joint distributions using mcmc were shown to be effective in improving the parameterization of biogeochemical models quantifying the uncertainties in parameters and outputs and evaluating different model structures this approach can be beneficial particularly in a system with multiple competing reactions taking place at different timescales insights into parameter space through joint probability distributions coupled with domain knowledge guides model improvements full quantification of uncertainties informs the design of future experiments targeted at constraining poorly constrained parameters eliminating multimodality or dealing with parameter degeneracy improved model structures and robust uncertainty estimates offer better understanding of underlying mechanisms and reliable predictions of fate of metals and nutrients in the environment parameter estimation using mcmc will not be suitable for models with high runtime as mcmc requires large number of model runs raising the computational cost however typical reaction networks are odes and even a complex reaction network is relatively fast to solve additionally with an increased access to computational resources there is no reason why researcher should not opt for robust uncertainty estimates using mcmc which offers flexibility of analyzing multiple experiments together and also guide domain knowledge based model improvements in the process the workflow for this approach is coded in python and scripts are provided in the model data archive at rathore and painter 2021 the modular nature of the workflow allows for inclusion of additional experimental data or a new reaction to the reaction network conveniently software and data availability the original experimental dataset is archived at schwartz et al 2021b the model data and workflow for this manuscript is archived at rathore and painter 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the u s department of energy office of science biological and environmental research subsurface biogeochemical research sbr program and is a product of the critical interfaces science focus area sfa at ornl and the ideas watersheds project this research used resources of the compute and data environment for science cades at the oak ridge national laboratory which is supported by the office of science of the u s department of energy under contract no de ac05 00or22725 we thank the editor and anonymous reviewer for their constructive feedback on the paper appendix a here we present mathematical equations for improved models for mercury methylation on aquatic sediments presented in this manuscript variables are described in the main text of the manuscript sediment 1 1 d h g d t k m max k m h s h g h g k d m e h g k 1 h g k 2 h g f k 3 h g k 4 h g s 2 d m e h g d t k m max k m h s h g h g k d m e h g k 7 m e h g k 8 m e h g f k 9 m e h g k 10 m e h g s 3 d h g f d t k 1 h g k 2 h g f 4 d h g s d t k 3 h g k 4 h g s 5 d m e h g f d t k 7 m e h g k 8 m e h g f 6 d m e h g s d t k 9 m e h g k 10 m e h g s 7 h g 0 1 k 5 6 h g sediment 2 8 d h g d t k m h g k d m e h g k 3 h g k 4 h g s 9 d m e h g d t k m h g k d m e h g k 9 m e h g k 10 m e h g s 10 d h g s d t k 3 h g k 4 h g s 11 d m e h g s d t k 9 m e h g k 10 m e h g s 12 h g f 1 k 1 2 h g 13 h g 0 1 k 5 6 h g 14 m e h g f 1 k 7 8 m e h g appendix b example plots of residual distribution for the time series of me201hg and me202hg in methylation and demethylation experiments for both sediment 1 and sediment 2 are shown below fig b 1 distribution of residuals for me201hg predictions are different timepoints for methylation experiment in sediment 1 fig b 1 fig b 2 distribution of residuals for me202hg predictions are different timepoints for demethylation experiment in sediment 1 fig b 2 fig b 3 distribution of residuals for me201hg predictions are different timepoints for methylation experiment in sediment 2 fig b 3 fig b 4 distribution of residuals for me202hg predictions are different timepoints for demethylation experiment in sediment 2 fig b 4 
25569,efforts to combine satellite images from different sources are particularly needed in land surface temperature based lst studies this research proposes for the first time to our knowledge a google earth engine based gee 10 m lst system named ten st gee it is based on both landsat 8 and sentinel 2 bands ten st gee has the ability to automatically transform 30 m to 10 m lst at landsat 8 overpass time machine learning and regression methods i e ols rls distrad rf and svm are embedded within this system ten st gee was applied over two agricultural lands and two urban regions in the united states of america and in lebanon ols and rls showed an rmse of 1 1 c compared to 2 4 c for distrad and 2 5 c for rf and svm the open source and automated ten st gee can generate information at the building level and within the agricultural parcels it has the potential to be portable to any region across the globe aiming at better management of environmental resources keywords crop temperature vegetation temperature volcano remote sensing open source small scale 1 introduction land surface temperature lst is one of the most critical parameters as it is incorporated in many disciplines such as wildfire outbreak and propagation e g mhawej et al 2016 quintano et al 2017 crop water consumption and evapotranspiration rate e g sun et al 2016 mhawej et al 2020c drought and desertification e g faour et al 2015 ghaleb et al 2015 hu et al 2020 urban heat island effect and microclimate e g abunnasr et al 2022 tran et al 2017 peng et al 2018 volcanic and geothermal areas mapping e g silvestri et al 2019 silvestri et al 2020 land use and land cover change e g tran et al 2017 wang et al 2019 abunnasr and mhawej 2021a and climate change research e g abunnasr and mhawej 2021b halder et al 2021 improving the spatial resolution of the lst products would then assist in a better outputs representation and characterization as well as a more undertanding of the studied natural and environmental phenomena downscaling lst began with a combination of information generated from one panchromatic band only price 1999 afterwards approaches were broadly divided into two main categories the physical and the statistical methods in the first the dual band approach based on the subpixel thermal anomaly monitoring is used dozier 1981 giglio et al 2003 along the emissivity based approach founded on the isothermal hypothesis liu and pu 2008 zhan et al 2013 the second major category the statistical downscaling approach is based on deriving a relationship between lst and auxiliary data zakšek and oštir 2012 more particularly two directions were observed in this category the first related to the use of the visible and near infrared satellite bands e g kustas et al 2003 agam et al 2007 zhang et al 2019 along coarser spatial resolution lst images the second combines lst images with lower spatial but higher temporal resolutions for more recurrent observations e g hutengs and vohland 2016 guzinski and nieto 2019 in the physical based methods the requirement of too many independent measurements is regarded as a major limitation dozier 1981 on the other hand the implementation of the much sophisticated statistical based approaches hutengs and vohland 2016 as well as their portability to other regions and settings pan et al 2018 might pose transferability issues in this context several automated algorithms were proposed based on these methods including disaggregation procedure for radiometric surface temperature distrad kustas et al 2003 thermal sharpening tsharp agam et al 2007 pixel block intensity modulation pbim algorithm stathopoulou and cartalis 2009 high resolution urban thermal sharpener huts dominguez et al 2011 data mining sharpener dms gao et al 2012 over the last five decades the landsat program has enabled a successful monitoring of the earth s surface using its multiple optical satellites e g faour et al 2018 mhawej et al 2020a using its various spectral bands it enables the assessment of multiple parameters and indices maybe the most well known are the normalized difference vegetation index ndvi albedo and land surface temperature lst landsat 8 has the ability to produce 30 m ndvi 30 m albedo and resampled from 100 m a 30 m lst data mhawej et al 2020b with the advancement in the sensors spatial and temporal resolutions many previous studies have tried to merge landsat 8 images with sentinel 2 e g storey et al 2016 claverie et al 2017 allam et al 2021 sentinel 3 e g manzo et al 2015 huryna et al 2019 guzinski and nieto 2019 moderate resolution imaging spectroradiometer modis e g zhou et al 2016 dong et al 2016 liao et al 2019 and many other sensors e g mouginot et al 2017 garcía llamas et al 2019 zawadzka et al 2020 this data fusion aims at improving the spatial resolution of an input image based on auxiliary data with better resolution gao et al 2012 several previous studies have tried to downscale lst using diverse methods for instance filgueiras et al 2019 proposed a derivation of lst for the evapotranspiration ratio estimation using s2 imagery and the thermal infrared sensor tirs l8 these authors compared different methodologies of pattern recognition algorithms to establish the best model including support vector machine linear svmlinear support vector machine radial svmradial linear regression lm ridge regression random forest rf cubist partial least squares pls principal components regression pcr generalized boosted modeling gbm and bayesian regularized neural network brnn the best model was found to be the rf pu and bonafoni 2021 tried to address the scaling effect in downscaling land surface temperature studies using a correction term after the application of a modified thermal component based thermal spectral unmixing tsu model and a multiple regression approach the combination was between coarse resolution 90 m advanced spaceborne thermal emission and reflection radiometer aster lst and 2 m thermal data they were able to reduce the root mean square error by over 30 in abunnasr and mhawej 2022 a data fusion were made automated under the python language named hsr lst between 100 m l8 lst data and high spatial resolution hsr 2 m red green and blue rgb bands in comparison to an airborne lst image over elkhorn river in nebraska usa the hsr lst product showed a rmse of 1 73 c and ame of 1 34 c even when using only these three rgb bands mhawej and abunnasr 2022 proposed a daily 10 m lst downscaling system based on the robust least squares statistical approach yielding an accuracy of 2 27 c ame in comparison to two airborne thermal infrared images over the hat creek region in california usa still only limited previous studies were based on the gee and that hindered their replicability and large applicability worldwide with the massive supercomputer power and using only some of the twenty petabytes of geospatial data nested within the google earth engine gee platform gorelick et al 2017 this study combined both gee and remote sensing techniques to produce a fully automated open source and user friendly system the ten meter surface temperature google earth engine ten st gee system is producing 10 m lst datasets based on the combination of 30 m landsat 8 and 10 m sentinel 2 images using the gee platform ten st gee has the ability to be promptly implemented over any area across the globe the selected study area in this research is located in two agricultural lands and two urban agglomerations in the united states of america and in lebanon showing the wide usage of this system e g air pollution impact in urban settings water needs in agricultural areas drought monitoring and volcano temperature assessments ten st gee can be applied and enhanced when necessary by any interested user largely assisting scientists regional planners and decision makers across a wide range of related disciplines 2 materials and methods 2 1 study area the study area fig 1 encompasses two agricultural lands and two urban agglomerations in both the united states of america and in lebanon the prevailing climates is warm summer mediterranean climate in the pacific palisades 34 0356 n 118 5156 w and cold semi arid climate in lamont 35 2597 n 118 9143 w california united states of america it is hot summer mediterranean climate in badaro 33 8835 n 35 5166 e and bar elias 33 7728 n 35 8979 e lebanon according to the koppen classification the selected study sites could be considered as typical for each region where the pacific palisades and barado were chosen in urban settings but near an urban green infrastructure the selected agricultural areas represent the distinctive large plots in lamont and small and heterogeneous parcels in bar elias 2 2 the ten st gee system gee is a cloud based planetary scale platform consisting of a multi petabyte analysis ready data catalog it has also an unparalleled computation service due to the usage of google s massive computational capabilities its main objective is to empower not only traditional remote sensing scientists but also a much wider audience from a variety of high impact societal issues including deforestation drought disease food security water management and environmental protection among many others gee offers any user access and processing of data from the public or their own private catalogs thus accelerating scientific discovery and advancements further details on the gee platform can be found in gorelick et al 2017 both satellite images data sources i e landsat 8 and sentinel 2 are directly accessible within the gee usgs surface reflectance along atmospherically corrected lst tier 1 level 2 product is accessible at https developers google com earth engine datasets catalog landsat lc08 c02 t1 l2 whereas the atmospherically corrected level 2a sentinel 2 can be found at https developers google com earth engine datasets catalog copernicus s2 sr the sentinel 2 level 2a covers datasets since march 2017 and thus if user requires older dates the alternative sentinel 2 msi multispectral instrument level 1c i e https developers google com earth engine datasets catalog copernicus s2 can be used but requires further atmospheric corrections embedded within ten st gee system a simple cloud masking approach was applied to each used satellite i e landsat 8 and sentinel 2 based on their respective cloud detection quality assessment bands i e qa pixel for landsat 8 and qa60 for sentinel 2 any pixel classified as cloud or cloud shadow was eliminated it was followed by a monthly median assessment of each satellite image to capture any change within that specified month and eliminate any biased values l8 lst data were directly retrieved from the cloud free gee product by applying a scale i e 0 00341802 and an offset i e 149 factors these datasets are based on the band 10 thermal radiances from which the lst is retrieved with a single channel algorithm jointly created by the rochester institute of technology rit and national aeronautics and space administration nasa jet propulsion laboratory jpl lst values cannot be retrieved for sentinel 2 as no thermal bands are available within this satellite a bandpass adjustment for the red green blue near infrared nir shortwave infrared 1 swir 1 and shortwave infrared 2 swir 2 cloud free bands was made between the monthly median datasets of both satellites as suggested by claverie et al 2018 this approach produces a linear fit relationship between each of the six bands in sentinel 2 satellite and their correspondence in landsat 8 satellite weighting factors are then produced to relate the s2 bands values with l8 bands values the result is a new monthly 10 m six band image where the s2 bands are corrected by the l8 bands five statistical based approaches were used in this study including three regressions i e ordinary least square ols robust least squares rls and distrad and two machine learning methods i e random forests rf and support vector machine svm the first regression approach is the ols method which aim towards estimating the unknown parameters in the linear regression model the general equation is as follows y β x β x β x et c ε where y is the dependent variable β β β are the independent variables x x x are the generated weights ϵ is the interception value the second regression approach is the robust least squares rls regression analysis this regression overcomes several limitations of the standard traditional parametric and non parametric approaches it is designed to be unaffected by violations of assumptions which is found for instance in the ols analysis further information can be found in davies 1993 in each of these two regression tests the twelve reflective l8 and s2 bands i e six for each satellite as well as three time upscaled 90 m l8 lst image are used as the independent variables whereas the 30 m monthly based l8 lst is used as the dependent variable the three time upscaling factor used corresponds to the same scaling factor between the 30 m l8 and 10 m s2 datasets weighting coefficients are then generated at the end of the ols and rls regression tests these coefficients are multiplied by the correspondent s2 bands to compute two 10 m lst images namely the ols 10 m lst and rls 10 m lst the used equation is as follows ls t fr bluex greenx redx nirx swir 1 x swir 2 x ls t cr x ε where lstfr is the dependent variable reflecting the fine resolution 10 m lst image blue green red nir swir1 swir2 corrected bands are the independent variables lstcr is also an independent variable reflecting the coarse resolution 30 m l8 lst image x x x x x x and x are the generated weights ϵ is the interception value the third regression approach used in this study is the original distrad kustas et al 2003 which was first developed over homogenous vegetated landscape this method uses higher resolution vegetation indices more commonly the normalized difference vegetation index ndvi and correlates it using an ols approach with the low resolution thermal information eswar et al 2016 as distrad approach is in continuous improvements e g essa et al 2017 pu and bonafoni 2021 in this study the original version was used for the sole purpose of comparison against ols and rls approaches even though it is now included within the ten st gee system the used formula is as follows ls t fr ndvix ndv i 2 x δ t ε where lstfr is the dependent variable reflecting the fine resolution 10 m lst image ndvi and ndvi2 are the independent variables x and x are the generated weights from the correction between the coarse resolution 30 m l8 lst and 30 m ndvi δt is the temperature estimation residuals computed via a simple subtraction between the 30 m l8 lst and the upscaled to 30 m lstfr ϵ is the interception value the widely known ndvi is computed as follows n d v i r e d n e a r i n f r a r e d r e d n e a r i n f r a r e d where red and near infrared are bands 4 and 5 for l8 and bands 4 and 8 for s2 respectively the first machine learning approach used is the rf with 130 decision trees created rf is a nonparametric statistical ensemble learning method for classification and regression it is designed to overcome the problem of over fitting by introducing randomness into the individual regression trees along averaging a large collection of these de correlated individual trees chen et al 2020 more information can be found in breiman 2001 svm is the second machine learning approach used for downscaling in this research while it was first developed by vapnik vladimir 1995 svm is based on the vapnikchervonenkis vc dimension of statistical learning theory it aims at solving the classification problems and had been mainly used for regression and classification issues with small and high dimensional samples further information on svm can be found in chang and lin 2001 in each of these two machine learning methods the six l8 reflective bands as well as three time upscaled 90 m l8 lst image and the 30 m monthly based l8 lst were used to train the rf and svm classifiers later these classifiers were applied over the six s2 reflective bands and the 30 m monthly based l8 lst to compute two 10 m lst images namely the rf 10 m lst and svm 10 m lst finally the mean deviation in percentage is computed to highlight the change between l8 and the 10 m lst images the mean deviation is the subtraction between the l8 lst and 10 m lst over the actual l8 lst at 30 m furthermore the root mean square error rmse is produced as well within the ten st gee system a simplified flowchart excluding distrad is available in fig 2 2 3 the ten st gee over the gee platform ten st gee is fully functional within the gee platform it enables an automatic collection of the needed inputs processing of the datasets and generation of the final 10 m lst products without any interference from the user this later requires only to include the studied month and year for instance the month 5 or 05 of the year 2020 as well as to draw or import the study region ten st gee can export the l8 lst and the final products ols rls distrad rf and svm 10 m s2 based lst into the google drive to cloud storage or to the user s asset moreover the mean deviation in percentage between the two products as well as the rmse are shown in the console tab ten st gee can be freely accessed at https bit ly 3i9mvzr a snapshot of the ten st gee system is available in fig 3 2 4 data evaluation in terms of the selection of the evaluation approach previous studies found in the literature have tried to validate their lst values based on one of three approaches the first is based on ground truth datasets such as using local flux towers e g cristóbal et al 2018 or the widely available surface radiation budget monitoring surfrad and heihe watershed allied telemetry experimental research hiwater in situ radiative data e g yu et al 2014 meng et al 2019 in both cases the derivation of actual lst values from the measured radiation might generate inherited errors in each required step of the transformation more importantly the validity of extrapolating flux values outside the location of the station also known as its representativeness is already discussed in many previous studies e g guillevic et al 2013 allam et al 2021 known as tower footprint or source area they showed an ambiguity of defining a clear representative boundary with changing land features types and microclimate conditions particularly visible in an urban region velasco and roth 2010 this can also be attributed to the directional effects discussed in göttsche et al 2016 and ermida et al 2017 moreover using the generated ground based lst values might not be adequate for in situ validation in our case where these towers field of view is approximately 70 m seven time larger than the 10 m lst produced products the second approach is the radiance based method validating the lst outputs based on the usage of a surface energy balance model or a radiative model such as moderate resolution atmospheric transmission modtran e g rozenstein et al 2014 in this method channel specific surface emissivity values as well as atmospheric temperature and water vapor profiles coincident with the satellite overpass are required guillevic et al 2018 no in situ measurements are needed still such models require a continuous update to improve their accuracies particularly where they might be missing or spatially improving a parameter affecting even slightly the end result e g berk et al 1998 acharya et al 1999 berk et al 2000 berk et al 2014 rosas et al 2017 berk and hawes 2017 the third approach of validation uses another product even with another spatial resolution such as what was produced in li and jiang 2018 where 30 m l8 lst datasets were compared to 1 km modis lst product also this approach has its limitations particularly related to other sensing time date and sensors as well as the inclusion of sometimes different land feature types within a 30 m pixel not seen at the 1 km scale faour et al 2016 as a result this approach does not yield absolute validation results and the inter comparisons alone are insufficient to validate a new product guillevic et al 2018 in this context the evaluation applied in this research tried to overcome these issues by cross evaluating over a 1 km2 region the cloud free upscaled nine time 10 m lst values with the cloud free upscaled three time 30 m l8 lst values using the same multiplier here 3 used for the downscaling process it was implemented over the whole year 2019 to account for different seasonal impacts thus the comparison can be considered more adequate as the same product with the same wavelengths is used but also with the same land feature composition at the pixel level furthermore a comparison between the widely known distrad method with its output upscaled to 90 m from 10 m and the 90 m l8 lst was also made it is important to note that the upscaling in the ols rls distrad rf and svm are based on a mean aggregation of pixels also an inter comparison over gee with another lst product namely the 1 km modis lst product i e mod11a1 was made following the best practices described in guillevic et al 2018 this is done through a bandpass adjustment between the upscaled 33 time l8 and the 1 km modis lst when adjusted this latter is compared to the 33 time l8 and 100 time ols rls and rf lst products rmse in each scenario was computed it is important to note that only cloud free datasets were used to reduce the effect of cloud contamination furthermore to account for different seasonal impacts the comparison was made over the whole year of 2019 at l8 overpass time the considered buffered region is 7 km2 to collect 49 samples at each comparison 3 results 3 1 data evaluation rmse for ols rls distrad rf and svm products were computed for the year of 2019 and over a 1 km2 region very promising rmse were shown when comparing upscaled nine time ols and rls 10 m lst products and upscaled three time 30 m l8 lst values table 1 the lowest rmse based on the ols method were presented in the urban regions of pacific palisades and badaro with 0 415 c and 0 387 c respectively rls outputs over urban regions were slightly higher the agricultural lands presented less accuracy with an average rmse value of 1 761 c for both study areas in the united states of america and lebanon and using both ols and rls approaches the rls method implemented over agricultural lands showed improved accuracy over the ols method the total average among the considered regions showed a rmse value of 1 102 c using the ols method and 1 09 c using the rls method distrad rmse values were larger than ols and rls on any occasion the average rmse over the selected regions using distrad is 2 359 c rmse of rf and svm were the largest among the used methods with a total average of 2 54 c fig 4 highlights a monthly average rmse between the upscaled nine time 10 m ols rls distrad rf and svm lst compared to the upscaled three time 30 m l8 lst over 1 km2 regions in the united states of america and lebanon the lowest rmse were shown in december and january the highest rmse values were seen in summer months with rmse exceeding 1 5 c based on the ols and rls approaches 2 5 c when using the original distrad method and 4 c in the machine learning methods anyhow both ols and rls products performed similarly except for a few months i e january and april distrad showed an improved rmse value only in march in comparison to both ols and rls approaches both rf and svm products appear to be higher than the other products in almost every month the intercomparison to the 1 km modis lst product has shown comparable results between the l8 rmse and those of the ols and rls approaches table 2 in comparison to modis lst rls is best performing with an rmse of 0 253 c followed by ols and then the l8 lst product it is also noticeable that the urban regions showed slightly lower rmse values than the agricultural areas machine learning approaches i e rf appear to be less performing than regression based methods i e ols and rls 3 2 ten st gee implemented in the united states of america and lebanon it is first important to note that the studied regions were randomly selected but are considered characteristic for each region more particularly a random house is selected in the pacific palisades area in the united states of america fig 5 no pixels could be selected for the l8 lst as the boundaries did not fully cover any pixel whereas seven pixels could be assessed in the 10 m lst products for badaro a major roundabout was selected connecting main roads in the city of beirut lebanon it could be a surrogate to the local air pollution impact as this roundabout has a diameter of nearly 90 m nine pixels could be selected for the l8 lst but 63 pixels for the 10 m lst fig 5 a square field was selected in lamont united states of america with 42 pixels in l8 lst and 399 pixels in 10 lst fig 5 in bar elias area lebanon the number of pixels used is 27 pixels for l8 lst and 244 for 10 m lst fig 5 4 discussion ten st gee ols and rls products showed promising results in comparison to the upscaled three time 30 m l8 lst and to the 1 km modis lst products the best accuracy was presented in urban settings with limited monthly variations inversely the agricultural lands with sometimes large monthly changes due to diverse crop development phases showed slightly larger rmse values these values were also large in summer months where temperatures are more intense and more scattered both ols and rls showed comparable results overall while not yielding absolute validation results this evaluation was made according to the best practices suggested by guillevic et al 2018 the presented ols and rls results indicate that ten st gee can be implemented in the three different climatic regions studied in this research i e warm summer mediterranean cold semi arid and hot summer mediterranean climates with accuracy and uncertainty tests already embedded within the system the choice is then up to the user to select the most adequate method in its study area distrad rf and svm on the other hand showed acceptable but more than doubled rmse values compared to ols and rls methods this finding can be related to the scaling effect added within distrad which is under continuous improvement in the literature pu and bonafoni 2021 furthermore as distrad was first built over agricultural areas multiple indices other than the ndvi found in the original distrad should be selected to capture the heterogeneity in urban areas essa et al 2017 this is also under development in the literature at the moment while the machine learning methods have performed poorly in comparison to ols and rls regression approaches these biases are normal and aligned with many previous studies signaling a rmse of 2 5 c e g li et al 2019 bartkowiak et al 2019 ebrahimy and azadbakht 2019 wu and wan 2019 the proposed ten st gee was implemented in four randomly selected sub regions within the considered study areas this system is providing monthly downscaled 10 m lst products based on the lst l8 overpass times and thus the difference of acquisition time between l8 and s2 would not pose any issue one important aspect of this system was to enable the monitoring of a single house in pacific palisades not possible with the standard l8 lst product thus the increased spatial resolution is particularly important at building and block scales the changing lst monthly values in badaro depicting the variation in temperatures but also related to the existent traffic and air pollution enabled a larger monitoring of lst values with more than 60 pixels were included versus the only nine pixels in l8 lst also over agricultural areas the three fold increase in pixels number enables the monitoring of the heterogeneity within parcels and to conduct better assessment for their water needs or vegetation health status such an improvement would be hugely important in for instance surface energy balance seb models aiming towards increasing water productivity while conserving actual resources allam et al 2021 in this context the proposal of an open source user friendly and easily accessible ten st gee system with an unprecedent accuracy along five 10 m lst products computed from the most common and recent literature based approaches over urban and agricultural areas would be vital for future research in diverse disciplines and destined for professionals from different backgrounds it uses the massive computation power and huge geospatial databases provided by the gee platform by combining both remote sensing and cloud processing techniques users will save on time and resources while having the ability to store 10 m lst layers over the cloud and accessing these layers in a timely manner furthermore any user with prior knowledge of javascript programming language can freely enhance the existing code this shall also improve the available environmental and agricultural models requiring accurate 10 m lst values future studies can focus on deriving 10 m lst values from different available satellite sensors including for instance landsat 5 landsat 7 the recently launched landsat 9 and advanced spaceborne thermal emission and reflectance radiometer aster thus enabling an historical lst assessment the addition of other satellite products missing thermal bands e g sentinel 1 as independent variables in the statistical regression might also improve the outputs accuracy designing a more appropriate validation approach that is able to validate via airborne thermal images the generated 10 m lst products is advisable still the proposed approach excelled in improving the spatial resolution of the 30 m l8 lst images over the gee which is to our knowledge not explicitly found in any previous study implementing ten st gee in other climatic regions and land cover use types is recommended finally including ten st gee within other environmental models particularly those existents within the gee platform is advisable 5 conclusion ten st gee is a user friendly much needed open access 10 m lst retrieval system it applies straightforward approaches while combining two satellite sensors namely the landsat 8 and sentinel 2 while being hosted within the gee users has the ability to implement the proposed system in any region worldwide in a matter of few seconds storing information locally or on the cloud and finally incorporating such information in other environmental models including wildfires drought evapotranspiration urban heat and volcano among many others ols and rls methods showed an rmse of 1 1 c compared to 2 4 c using the original distrad method and 2 5 c using rf and svm approaches in three different climatic regions and four different locations thus it can be implemented in other areas particularly where different performance tests are already embedded within the system the much improved spatial resolution ten st gee system is freely available from the authors for research and educational purposes at https bit ly 3i9mvzr users are welcome to send their feedback and suggestions to the corresponding author software availability name of the software monthly ten st gee e mail mm278 aub edu lb first available 2022 minimum requirements any device with a web browsing capability platform any platform with a web browsing capability availability through gee platform at https bit ly 3i9mvzr https code earthengine google com 5512a9d63524adc0030c683b9c61b6a3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25569,efforts to combine satellite images from different sources are particularly needed in land surface temperature based lst studies this research proposes for the first time to our knowledge a google earth engine based gee 10 m lst system named ten st gee it is based on both landsat 8 and sentinel 2 bands ten st gee has the ability to automatically transform 30 m to 10 m lst at landsat 8 overpass time machine learning and regression methods i e ols rls distrad rf and svm are embedded within this system ten st gee was applied over two agricultural lands and two urban regions in the united states of america and in lebanon ols and rls showed an rmse of 1 1 c compared to 2 4 c for distrad and 2 5 c for rf and svm the open source and automated ten st gee can generate information at the building level and within the agricultural parcels it has the potential to be portable to any region across the globe aiming at better management of environmental resources keywords crop temperature vegetation temperature volcano remote sensing open source small scale 1 introduction land surface temperature lst is one of the most critical parameters as it is incorporated in many disciplines such as wildfire outbreak and propagation e g mhawej et al 2016 quintano et al 2017 crop water consumption and evapotranspiration rate e g sun et al 2016 mhawej et al 2020c drought and desertification e g faour et al 2015 ghaleb et al 2015 hu et al 2020 urban heat island effect and microclimate e g abunnasr et al 2022 tran et al 2017 peng et al 2018 volcanic and geothermal areas mapping e g silvestri et al 2019 silvestri et al 2020 land use and land cover change e g tran et al 2017 wang et al 2019 abunnasr and mhawej 2021a and climate change research e g abunnasr and mhawej 2021b halder et al 2021 improving the spatial resolution of the lst products would then assist in a better outputs representation and characterization as well as a more undertanding of the studied natural and environmental phenomena downscaling lst began with a combination of information generated from one panchromatic band only price 1999 afterwards approaches were broadly divided into two main categories the physical and the statistical methods in the first the dual band approach based on the subpixel thermal anomaly monitoring is used dozier 1981 giglio et al 2003 along the emissivity based approach founded on the isothermal hypothesis liu and pu 2008 zhan et al 2013 the second major category the statistical downscaling approach is based on deriving a relationship between lst and auxiliary data zakšek and oštir 2012 more particularly two directions were observed in this category the first related to the use of the visible and near infrared satellite bands e g kustas et al 2003 agam et al 2007 zhang et al 2019 along coarser spatial resolution lst images the second combines lst images with lower spatial but higher temporal resolutions for more recurrent observations e g hutengs and vohland 2016 guzinski and nieto 2019 in the physical based methods the requirement of too many independent measurements is regarded as a major limitation dozier 1981 on the other hand the implementation of the much sophisticated statistical based approaches hutengs and vohland 2016 as well as their portability to other regions and settings pan et al 2018 might pose transferability issues in this context several automated algorithms were proposed based on these methods including disaggregation procedure for radiometric surface temperature distrad kustas et al 2003 thermal sharpening tsharp agam et al 2007 pixel block intensity modulation pbim algorithm stathopoulou and cartalis 2009 high resolution urban thermal sharpener huts dominguez et al 2011 data mining sharpener dms gao et al 2012 over the last five decades the landsat program has enabled a successful monitoring of the earth s surface using its multiple optical satellites e g faour et al 2018 mhawej et al 2020a using its various spectral bands it enables the assessment of multiple parameters and indices maybe the most well known are the normalized difference vegetation index ndvi albedo and land surface temperature lst landsat 8 has the ability to produce 30 m ndvi 30 m albedo and resampled from 100 m a 30 m lst data mhawej et al 2020b with the advancement in the sensors spatial and temporal resolutions many previous studies have tried to merge landsat 8 images with sentinel 2 e g storey et al 2016 claverie et al 2017 allam et al 2021 sentinel 3 e g manzo et al 2015 huryna et al 2019 guzinski and nieto 2019 moderate resolution imaging spectroradiometer modis e g zhou et al 2016 dong et al 2016 liao et al 2019 and many other sensors e g mouginot et al 2017 garcía llamas et al 2019 zawadzka et al 2020 this data fusion aims at improving the spatial resolution of an input image based on auxiliary data with better resolution gao et al 2012 several previous studies have tried to downscale lst using diverse methods for instance filgueiras et al 2019 proposed a derivation of lst for the evapotranspiration ratio estimation using s2 imagery and the thermal infrared sensor tirs l8 these authors compared different methodologies of pattern recognition algorithms to establish the best model including support vector machine linear svmlinear support vector machine radial svmradial linear regression lm ridge regression random forest rf cubist partial least squares pls principal components regression pcr generalized boosted modeling gbm and bayesian regularized neural network brnn the best model was found to be the rf pu and bonafoni 2021 tried to address the scaling effect in downscaling land surface temperature studies using a correction term after the application of a modified thermal component based thermal spectral unmixing tsu model and a multiple regression approach the combination was between coarse resolution 90 m advanced spaceborne thermal emission and reflection radiometer aster lst and 2 m thermal data they were able to reduce the root mean square error by over 30 in abunnasr and mhawej 2022 a data fusion were made automated under the python language named hsr lst between 100 m l8 lst data and high spatial resolution hsr 2 m red green and blue rgb bands in comparison to an airborne lst image over elkhorn river in nebraska usa the hsr lst product showed a rmse of 1 73 c and ame of 1 34 c even when using only these three rgb bands mhawej and abunnasr 2022 proposed a daily 10 m lst downscaling system based on the robust least squares statistical approach yielding an accuracy of 2 27 c ame in comparison to two airborne thermal infrared images over the hat creek region in california usa still only limited previous studies were based on the gee and that hindered their replicability and large applicability worldwide with the massive supercomputer power and using only some of the twenty petabytes of geospatial data nested within the google earth engine gee platform gorelick et al 2017 this study combined both gee and remote sensing techniques to produce a fully automated open source and user friendly system the ten meter surface temperature google earth engine ten st gee system is producing 10 m lst datasets based on the combination of 30 m landsat 8 and 10 m sentinel 2 images using the gee platform ten st gee has the ability to be promptly implemented over any area across the globe the selected study area in this research is located in two agricultural lands and two urban agglomerations in the united states of america and in lebanon showing the wide usage of this system e g air pollution impact in urban settings water needs in agricultural areas drought monitoring and volcano temperature assessments ten st gee can be applied and enhanced when necessary by any interested user largely assisting scientists regional planners and decision makers across a wide range of related disciplines 2 materials and methods 2 1 study area the study area fig 1 encompasses two agricultural lands and two urban agglomerations in both the united states of america and in lebanon the prevailing climates is warm summer mediterranean climate in the pacific palisades 34 0356 n 118 5156 w and cold semi arid climate in lamont 35 2597 n 118 9143 w california united states of america it is hot summer mediterranean climate in badaro 33 8835 n 35 5166 e and bar elias 33 7728 n 35 8979 e lebanon according to the koppen classification the selected study sites could be considered as typical for each region where the pacific palisades and barado were chosen in urban settings but near an urban green infrastructure the selected agricultural areas represent the distinctive large plots in lamont and small and heterogeneous parcels in bar elias 2 2 the ten st gee system gee is a cloud based planetary scale platform consisting of a multi petabyte analysis ready data catalog it has also an unparalleled computation service due to the usage of google s massive computational capabilities its main objective is to empower not only traditional remote sensing scientists but also a much wider audience from a variety of high impact societal issues including deforestation drought disease food security water management and environmental protection among many others gee offers any user access and processing of data from the public or their own private catalogs thus accelerating scientific discovery and advancements further details on the gee platform can be found in gorelick et al 2017 both satellite images data sources i e landsat 8 and sentinel 2 are directly accessible within the gee usgs surface reflectance along atmospherically corrected lst tier 1 level 2 product is accessible at https developers google com earth engine datasets catalog landsat lc08 c02 t1 l2 whereas the atmospherically corrected level 2a sentinel 2 can be found at https developers google com earth engine datasets catalog copernicus s2 sr the sentinel 2 level 2a covers datasets since march 2017 and thus if user requires older dates the alternative sentinel 2 msi multispectral instrument level 1c i e https developers google com earth engine datasets catalog copernicus s2 can be used but requires further atmospheric corrections embedded within ten st gee system a simple cloud masking approach was applied to each used satellite i e landsat 8 and sentinel 2 based on their respective cloud detection quality assessment bands i e qa pixel for landsat 8 and qa60 for sentinel 2 any pixel classified as cloud or cloud shadow was eliminated it was followed by a monthly median assessment of each satellite image to capture any change within that specified month and eliminate any biased values l8 lst data were directly retrieved from the cloud free gee product by applying a scale i e 0 00341802 and an offset i e 149 factors these datasets are based on the band 10 thermal radiances from which the lst is retrieved with a single channel algorithm jointly created by the rochester institute of technology rit and national aeronautics and space administration nasa jet propulsion laboratory jpl lst values cannot be retrieved for sentinel 2 as no thermal bands are available within this satellite a bandpass adjustment for the red green blue near infrared nir shortwave infrared 1 swir 1 and shortwave infrared 2 swir 2 cloud free bands was made between the monthly median datasets of both satellites as suggested by claverie et al 2018 this approach produces a linear fit relationship between each of the six bands in sentinel 2 satellite and their correspondence in landsat 8 satellite weighting factors are then produced to relate the s2 bands values with l8 bands values the result is a new monthly 10 m six band image where the s2 bands are corrected by the l8 bands five statistical based approaches were used in this study including three regressions i e ordinary least square ols robust least squares rls and distrad and two machine learning methods i e random forests rf and support vector machine svm the first regression approach is the ols method which aim towards estimating the unknown parameters in the linear regression model the general equation is as follows y β x β x β x et c ε where y is the dependent variable β β β are the independent variables x x x are the generated weights ϵ is the interception value the second regression approach is the robust least squares rls regression analysis this regression overcomes several limitations of the standard traditional parametric and non parametric approaches it is designed to be unaffected by violations of assumptions which is found for instance in the ols analysis further information can be found in davies 1993 in each of these two regression tests the twelve reflective l8 and s2 bands i e six for each satellite as well as three time upscaled 90 m l8 lst image are used as the independent variables whereas the 30 m monthly based l8 lst is used as the dependent variable the three time upscaling factor used corresponds to the same scaling factor between the 30 m l8 and 10 m s2 datasets weighting coefficients are then generated at the end of the ols and rls regression tests these coefficients are multiplied by the correspondent s2 bands to compute two 10 m lst images namely the ols 10 m lst and rls 10 m lst the used equation is as follows ls t fr bluex greenx redx nirx swir 1 x swir 2 x ls t cr x ε where lstfr is the dependent variable reflecting the fine resolution 10 m lst image blue green red nir swir1 swir2 corrected bands are the independent variables lstcr is also an independent variable reflecting the coarse resolution 30 m l8 lst image x x x x x x and x are the generated weights ϵ is the interception value the third regression approach used in this study is the original distrad kustas et al 2003 which was first developed over homogenous vegetated landscape this method uses higher resolution vegetation indices more commonly the normalized difference vegetation index ndvi and correlates it using an ols approach with the low resolution thermal information eswar et al 2016 as distrad approach is in continuous improvements e g essa et al 2017 pu and bonafoni 2021 in this study the original version was used for the sole purpose of comparison against ols and rls approaches even though it is now included within the ten st gee system the used formula is as follows ls t fr ndvix ndv i 2 x δ t ε where lstfr is the dependent variable reflecting the fine resolution 10 m lst image ndvi and ndvi2 are the independent variables x and x are the generated weights from the correction between the coarse resolution 30 m l8 lst and 30 m ndvi δt is the temperature estimation residuals computed via a simple subtraction between the 30 m l8 lst and the upscaled to 30 m lstfr ϵ is the interception value the widely known ndvi is computed as follows n d v i r e d n e a r i n f r a r e d r e d n e a r i n f r a r e d where red and near infrared are bands 4 and 5 for l8 and bands 4 and 8 for s2 respectively the first machine learning approach used is the rf with 130 decision trees created rf is a nonparametric statistical ensemble learning method for classification and regression it is designed to overcome the problem of over fitting by introducing randomness into the individual regression trees along averaging a large collection of these de correlated individual trees chen et al 2020 more information can be found in breiman 2001 svm is the second machine learning approach used for downscaling in this research while it was first developed by vapnik vladimir 1995 svm is based on the vapnikchervonenkis vc dimension of statistical learning theory it aims at solving the classification problems and had been mainly used for regression and classification issues with small and high dimensional samples further information on svm can be found in chang and lin 2001 in each of these two machine learning methods the six l8 reflective bands as well as three time upscaled 90 m l8 lst image and the 30 m monthly based l8 lst were used to train the rf and svm classifiers later these classifiers were applied over the six s2 reflective bands and the 30 m monthly based l8 lst to compute two 10 m lst images namely the rf 10 m lst and svm 10 m lst finally the mean deviation in percentage is computed to highlight the change between l8 and the 10 m lst images the mean deviation is the subtraction between the l8 lst and 10 m lst over the actual l8 lst at 30 m furthermore the root mean square error rmse is produced as well within the ten st gee system a simplified flowchart excluding distrad is available in fig 2 2 3 the ten st gee over the gee platform ten st gee is fully functional within the gee platform it enables an automatic collection of the needed inputs processing of the datasets and generation of the final 10 m lst products without any interference from the user this later requires only to include the studied month and year for instance the month 5 or 05 of the year 2020 as well as to draw or import the study region ten st gee can export the l8 lst and the final products ols rls distrad rf and svm 10 m s2 based lst into the google drive to cloud storage or to the user s asset moreover the mean deviation in percentage between the two products as well as the rmse are shown in the console tab ten st gee can be freely accessed at https bit ly 3i9mvzr a snapshot of the ten st gee system is available in fig 3 2 4 data evaluation in terms of the selection of the evaluation approach previous studies found in the literature have tried to validate their lst values based on one of three approaches the first is based on ground truth datasets such as using local flux towers e g cristóbal et al 2018 or the widely available surface radiation budget monitoring surfrad and heihe watershed allied telemetry experimental research hiwater in situ radiative data e g yu et al 2014 meng et al 2019 in both cases the derivation of actual lst values from the measured radiation might generate inherited errors in each required step of the transformation more importantly the validity of extrapolating flux values outside the location of the station also known as its representativeness is already discussed in many previous studies e g guillevic et al 2013 allam et al 2021 known as tower footprint or source area they showed an ambiguity of defining a clear representative boundary with changing land features types and microclimate conditions particularly visible in an urban region velasco and roth 2010 this can also be attributed to the directional effects discussed in göttsche et al 2016 and ermida et al 2017 moreover using the generated ground based lst values might not be adequate for in situ validation in our case where these towers field of view is approximately 70 m seven time larger than the 10 m lst produced products the second approach is the radiance based method validating the lst outputs based on the usage of a surface energy balance model or a radiative model such as moderate resolution atmospheric transmission modtran e g rozenstein et al 2014 in this method channel specific surface emissivity values as well as atmospheric temperature and water vapor profiles coincident with the satellite overpass are required guillevic et al 2018 no in situ measurements are needed still such models require a continuous update to improve their accuracies particularly where they might be missing or spatially improving a parameter affecting even slightly the end result e g berk et al 1998 acharya et al 1999 berk et al 2000 berk et al 2014 rosas et al 2017 berk and hawes 2017 the third approach of validation uses another product even with another spatial resolution such as what was produced in li and jiang 2018 where 30 m l8 lst datasets were compared to 1 km modis lst product also this approach has its limitations particularly related to other sensing time date and sensors as well as the inclusion of sometimes different land feature types within a 30 m pixel not seen at the 1 km scale faour et al 2016 as a result this approach does not yield absolute validation results and the inter comparisons alone are insufficient to validate a new product guillevic et al 2018 in this context the evaluation applied in this research tried to overcome these issues by cross evaluating over a 1 km2 region the cloud free upscaled nine time 10 m lst values with the cloud free upscaled three time 30 m l8 lst values using the same multiplier here 3 used for the downscaling process it was implemented over the whole year 2019 to account for different seasonal impacts thus the comparison can be considered more adequate as the same product with the same wavelengths is used but also with the same land feature composition at the pixel level furthermore a comparison between the widely known distrad method with its output upscaled to 90 m from 10 m and the 90 m l8 lst was also made it is important to note that the upscaling in the ols rls distrad rf and svm are based on a mean aggregation of pixels also an inter comparison over gee with another lst product namely the 1 km modis lst product i e mod11a1 was made following the best practices described in guillevic et al 2018 this is done through a bandpass adjustment between the upscaled 33 time l8 and the 1 km modis lst when adjusted this latter is compared to the 33 time l8 and 100 time ols rls and rf lst products rmse in each scenario was computed it is important to note that only cloud free datasets were used to reduce the effect of cloud contamination furthermore to account for different seasonal impacts the comparison was made over the whole year of 2019 at l8 overpass time the considered buffered region is 7 km2 to collect 49 samples at each comparison 3 results 3 1 data evaluation rmse for ols rls distrad rf and svm products were computed for the year of 2019 and over a 1 km2 region very promising rmse were shown when comparing upscaled nine time ols and rls 10 m lst products and upscaled three time 30 m l8 lst values table 1 the lowest rmse based on the ols method were presented in the urban regions of pacific palisades and badaro with 0 415 c and 0 387 c respectively rls outputs over urban regions were slightly higher the agricultural lands presented less accuracy with an average rmse value of 1 761 c for both study areas in the united states of america and lebanon and using both ols and rls approaches the rls method implemented over agricultural lands showed improved accuracy over the ols method the total average among the considered regions showed a rmse value of 1 102 c using the ols method and 1 09 c using the rls method distrad rmse values were larger than ols and rls on any occasion the average rmse over the selected regions using distrad is 2 359 c rmse of rf and svm were the largest among the used methods with a total average of 2 54 c fig 4 highlights a monthly average rmse between the upscaled nine time 10 m ols rls distrad rf and svm lst compared to the upscaled three time 30 m l8 lst over 1 km2 regions in the united states of america and lebanon the lowest rmse were shown in december and january the highest rmse values were seen in summer months with rmse exceeding 1 5 c based on the ols and rls approaches 2 5 c when using the original distrad method and 4 c in the machine learning methods anyhow both ols and rls products performed similarly except for a few months i e january and april distrad showed an improved rmse value only in march in comparison to both ols and rls approaches both rf and svm products appear to be higher than the other products in almost every month the intercomparison to the 1 km modis lst product has shown comparable results between the l8 rmse and those of the ols and rls approaches table 2 in comparison to modis lst rls is best performing with an rmse of 0 253 c followed by ols and then the l8 lst product it is also noticeable that the urban regions showed slightly lower rmse values than the agricultural areas machine learning approaches i e rf appear to be less performing than regression based methods i e ols and rls 3 2 ten st gee implemented in the united states of america and lebanon it is first important to note that the studied regions were randomly selected but are considered characteristic for each region more particularly a random house is selected in the pacific palisades area in the united states of america fig 5 no pixels could be selected for the l8 lst as the boundaries did not fully cover any pixel whereas seven pixels could be assessed in the 10 m lst products for badaro a major roundabout was selected connecting main roads in the city of beirut lebanon it could be a surrogate to the local air pollution impact as this roundabout has a diameter of nearly 90 m nine pixels could be selected for the l8 lst but 63 pixels for the 10 m lst fig 5 a square field was selected in lamont united states of america with 42 pixels in l8 lst and 399 pixels in 10 lst fig 5 in bar elias area lebanon the number of pixels used is 27 pixels for l8 lst and 244 for 10 m lst fig 5 4 discussion ten st gee ols and rls products showed promising results in comparison to the upscaled three time 30 m l8 lst and to the 1 km modis lst products the best accuracy was presented in urban settings with limited monthly variations inversely the agricultural lands with sometimes large monthly changes due to diverse crop development phases showed slightly larger rmse values these values were also large in summer months where temperatures are more intense and more scattered both ols and rls showed comparable results overall while not yielding absolute validation results this evaluation was made according to the best practices suggested by guillevic et al 2018 the presented ols and rls results indicate that ten st gee can be implemented in the three different climatic regions studied in this research i e warm summer mediterranean cold semi arid and hot summer mediterranean climates with accuracy and uncertainty tests already embedded within the system the choice is then up to the user to select the most adequate method in its study area distrad rf and svm on the other hand showed acceptable but more than doubled rmse values compared to ols and rls methods this finding can be related to the scaling effect added within distrad which is under continuous improvement in the literature pu and bonafoni 2021 furthermore as distrad was first built over agricultural areas multiple indices other than the ndvi found in the original distrad should be selected to capture the heterogeneity in urban areas essa et al 2017 this is also under development in the literature at the moment while the machine learning methods have performed poorly in comparison to ols and rls regression approaches these biases are normal and aligned with many previous studies signaling a rmse of 2 5 c e g li et al 2019 bartkowiak et al 2019 ebrahimy and azadbakht 2019 wu and wan 2019 the proposed ten st gee was implemented in four randomly selected sub regions within the considered study areas this system is providing monthly downscaled 10 m lst products based on the lst l8 overpass times and thus the difference of acquisition time between l8 and s2 would not pose any issue one important aspect of this system was to enable the monitoring of a single house in pacific palisades not possible with the standard l8 lst product thus the increased spatial resolution is particularly important at building and block scales the changing lst monthly values in badaro depicting the variation in temperatures but also related to the existent traffic and air pollution enabled a larger monitoring of lst values with more than 60 pixels were included versus the only nine pixels in l8 lst also over agricultural areas the three fold increase in pixels number enables the monitoring of the heterogeneity within parcels and to conduct better assessment for their water needs or vegetation health status such an improvement would be hugely important in for instance surface energy balance seb models aiming towards increasing water productivity while conserving actual resources allam et al 2021 in this context the proposal of an open source user friendly and easily accessible ten st gee system with an unprecedent accuracy along five 10 m lst products computed from the most common and recent literature based approaches over urban and agricultural areas would be vital for future research in diverse disciplines and destined for professionals from different backgrounds it uses the massive computation power and huge geospatial databases provided by the gee platform by combining both remote sensing and cloud processing techniques users will save on time and resources while having the ability to store 10 m lst layers over the cloud and accessing these layers in a timely manner furthermore any user with prior knowledge of javascript programming language can freely enhance the existing code this shall also improve the available environmental and agricultural models requiring accurate 10 m lst values future studies can focus on deriving 10 m lst values from different available satellite sensors including for instance landsat 5 landsat 7 the recently launched landsat 9 and advanced spaceborne thermal emission and reflectance radiometer aster thus enabling an historical lst assessment the addition of other satellite products missing thermal bands e g sentinel 1 as independent variables in the statistical regression might also improve the outputs accuracy designing a more appropriate validation approach that is able to validate via airborne thermal images the generated 10 m lst products is advisable still the proposed approach excelled in improving the spatial resolution of the 30 m l8 lst images over the gee which is to our knowledge not explicitly found in any previous study implementing ten st gee in other climatic regions and land cover use types is recommended finally including ten st gee within other environmental models particularly those existents within the gee platform is advisable 5 conclusion ten st gee is a user friendly much needed open access 10 m lst retrieval system it applies straightforward approaches while combining two satellite sensors namely the landsat 8 and sentinel 2 while being hosted within the gee users has the ability to implement the proposed system in any region worldwide in a matter of few seconds storing information locally or on the cloud and finally incorporating such information in other environmental models including wildfires drought evapotranspiration urban heat and volcano among many others ols and rls methods showed an rmse of 1 1 c compared to 2 4 c using the original distrad method and 2 5 c using rf and svm approaches in three different climatic regions and four different locations thus it can be implemented in other areas particularly where different performance tests are already embedded within the system the much improved spatial resolution ten st gee system is freely available from the authors for research and educational purposes at https bit ly 3i9mvzr users are welcome to send their feedback and suggestions to the corresponding author software availability name of the software monthly ten st gee e mail mm278 aub edu lb first available 2022 minimum requirements any device with a web browsing capability platform any platform with a web browsing capability availability through gee platform at https bit ly 3i9mvzr https code earthengine google com 5512a9d63524adc0030c683b9c61b6a3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
