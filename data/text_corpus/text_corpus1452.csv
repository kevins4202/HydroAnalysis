index,text
7260,the long term change of evapotranspiration et is crucial for managing water resources in areas with extreme climates such as the tibetan plateau tp this study proposed a modified algorithm for estimating et based on the mod16 algorithm on a global scale over alpine meadow on the tp in china wind speed and vegetation height were integrated to estimate aerodynamic resistance while the temperature and moisture constraints for stomatal conductance were revised based on the technique proposed by fisher et al 2008 moreover fisher s method for soil evaporation was adopted to reduce the uncertainty in soil evaporation estimation five representative alpine meadow sites on the tp were selected to investigate the performance of the modified algorithm comparisons were made between the et observed using the eddy covariance ec and estimated using both the original and modified algorithms the results revealed that the modified algorithm performed better than the original mod16 algorithm with the coefficient of determination r2 increasing from 0 26 to 0 68 and root mean square error rmse decreasing from 1 56 to 0 78 mm d 1 the modified algorithm performed slightly better with a higher r2 0 70 and lower rmse 0 61 mm d 1 for after precipitation days than for non precipitation days at suli site contrarily better results were obtained for non precipitation days than for after precipitation days at arou tanggula and hulugou sites indicating that the modified algorithm may be more suitable for estimating et for non precipitation days with higher accuracy than for after precipitation days which had large observation errors the comparisons between the modified algorithm and two mainstream methods suggested that the modified algorithm could produce high accuracy et over the alpine meadow sites on the tp keywords evapotranspiration alpine meadow modified mod16 algorithm resistance parameterization tibetan plateau 1 introduction evapotranspiration et is an important component for the global terrestrial energy budget balance and water cycle jia et al 2012 di et al 2015 liu et al 2016 therefore the long term change of regional et is of significance for improving water management and monitoring climate change di et al 2015 cleugh et al 2007 kustas et al 2007 mu et al 2007 kim et al 2012 conventional et estimation methods such as the lysimeter eddy covariance ec and bowen ratio are based on in situ measurements that cannot acquire regional or global et existing remote sensing models that provide the potential for regional et estimation can be divided into three categories 1 empirical regression models wang et al 2007 wang et al 2010 2 surface energy balance seb models such as the surface energy balance algorithm for land sebal bastiaanssen et al 1998 mapping evapotranspiration at high resolution with internalized calibration metric allen et al 2007 and surface energy balance system sebs su 2002 and 3 traditional methods combined with remote sensing data such as the penman monteith pm method with moderate resolution imaging spectroradiometer modis imagery cleugh et al 2007 mu et al 2007 2011 fisher et al 2011 and the priestly taylor equation with advanced very high resolution spectroradiometer avhrr fisher et al 2008 empirical regression models have been developed by establishing the relationship between et and satellite based net radiation temperature and vegetation indices wang et al 2007 2010 although these models can quickly capture long term regional et it is difficult to ascertain the empirical coefficients for different ecosystems due to the heterogeneity of land use types feng et al 2015 further seb models have been applied successfully in many countries with varying climates gao and long 2008 paiva et al 2011 tang et al 2013 chang et al 2017 and have been proven to produce regional et with high spatial resolution which is useful for monitoring water resources however these models cannot capture long term change of regional et due to the availability of continuous remote sensing data to address these problems cleugh et al 2007 first developed a remote sensing model based on the pm equation with modis vegetation products and meteorological data mu et al 2007 modified cleugh s model and produced the first global modis et datasets with a spatial resolution of 1 km and temporal resolutions of 8 day monthly and yearly later mu et al 2011 further improved the modis et algorithm to produce a more accurate global modis et product mod16 from 2000 to 2014 the mod16 et product has been validated and applied in many countries with various climates jia et al 2012 kim et al 2012 tang et al 2015 chen et al 2014 kim et al 2012 found that the mod16 product performed best at five forest sites and mismatched with observed et at two grassland sites in asia chen et al 2014 demonstrated that the mod16 product showed substantial differences with pt jpl fisher et al 2008 and yuan s yuan et al 2010 algorithms in china tang et al 2015 pointed out that the mod16 et product produced good agreement with ec values at one cropland site but underestimated et at four irrigated cropland sites in china feng et al 2015 found the mod16 algorithm showed reduced performance at grassland savannas and shrubland sites over semi arid ecosystems compared with pt jpl fisher et al 2008 and yao s yao et al 2013 algorithms it is pertinent to note that the performance of the mod16 et product varies under different climates and different surfaces feng et al 2016 yet few studies focus on the performance of the mod16 et product on the tibetan plateau tp which is recognized as the world s third pole and one of most sensitive areas for climate change qiu 2008 long term regional et variations on the tp are significant to understand local hydrological processes and global climate change although et on the tp has been studied using the water balance method liu et al 2016 xue et al 2013 li et al 2014 hydrological models yang et al 2011 zhou et al 2014 and satellite based models chen et al 2013a b peng et al 2016 these methods are inadequate to evaluate the accuracy of et estimation over alpine meadow due to lack of observation data which is an important land cover type on the tp further there are substantial differences among different satellite based models on the tp peng et al 2016 therefore it is important to evaluate and improve the current satellite based et algorithm mod16 algorithm for long term et estimation over alpine meadow on the tp in this study the objective was to evaluate and improve the mod16 algorithm for daily et estimation over alpine meadow on the tp by modifying evaluating and then finalizing the improved algorithm first modification of the canopy resistance parameterization scheme was introduced by integrating wind speed and vegetation height in the mod16 algorithm the algorithms for temperature and moisture constraints for canopy stomatal conductance were also improved and the soil evaporation algorithm was modified by adopting fisher s equation second the performance of original and modified mod16 algorithms were evaluated at five representative sites on the tp the results of the two algorithms under non precipitation and after precipitation days were also analyzed and the performance of the mod16 product original mod16 algorithm and modified algorithm at four representative sites with an 8 day time window were compared third the uncertainties and limitations of the modified mod16 algorithm were determined the study will help to obtain more accurate long term and continuous et over alpine meadow on the tp 2 materials and methods 2 1 study sites observed data at five flux tower sites on the tp during the growing season were collected liu et al 2011 li et al 2013 chen et al 2015 2017 observation data at arou site can be downloaded from the internet http westdc westgis ac cn data the vegetation type was alpine meadow with semiarid climate and elevation ranges from 3033 to 5100 m at the observed area the land classification type was grassland provided by mod12q1 for all sites the locations of the flux tower sites are shown in fig 1 and details of the tower sites are listed in table 1 only one growing season was available for each location with different years due to lack of data the observed half hourly meteorological data which included air temperature ta relative humidity rh and wind speed u at five sites were aggregated into daily data and used as forcing data for the models in this study the forcing data also included net radiation rn and soil heat flux g the net radiation was measured by a net radiometer cnr while the surface soil heat flux was measured by soil heat flux plate buried 5 cm underground and was corrected using the thermal diffusion equation and correction method with soil temperature and soil water content yang and wang 2008 the le measured by ec was used as validation the results of the original mod16 and modified mod16 algorithms were both driven by observed meteorological data fraction of photosynthetically active radiation fpar and leaf area index lai data were extracted from mod15a2 with spatial resolution of 1 km and temporal resolution of 8 day the unreliable fpar and lai data were linearly filled with the nearest reliable value before and after the missing data mu et al 2011 the mean value of the nine surrounding pixels around each site was considered to be the site point scale value mu et al 2007 daily fpar and lai data were linearly interpolated by the 8 day mod15a2 product we also used meteorological data from the modern era retrospective analysis research and applications merra product by nasa s global modeling and assimilation office gmao to explore the limitations of the algorithm section 4 4 the mod16 8 day product data were downloaded from the internet https e4ftl01 cr usgs gov molt which were driven by merra reanalysis data 2 2 data pre processing edire software http www geos ed ac uk homes jbm micromet edire was employed for ec data processing with observation correction and quality assessment the observation correction included the removal of spikes coordinate rotation 2 d rotation frequency response correction sonic virtual temperature correction and corrections for density fluctuation webb pearman leuning wpl correction data quality assessment was performed using the turbulence stationary test and integrated turbulence characteristics test the flag system 0 1 and 2 mauder and foken 2015 was used for quality assessment where a flag of 0 represented the best data data were discarded when precipitation occurred within 1 h before and after data collection or when the data quality control flag was marked as 2 as well as data at night when the friction velocity was below 0 1 m s 1 blanken 1998 liu et al 2011 li et al 2013 if the number n of reliable 30 min measurements was less than 40 per day the daily measurements were set as null values the missing half hourly latent heat flux data were filled by the nonlinear regression method establish the relationship between the latent heat flux and net radiation le ar2 n brn c which was considered a better interpolation method for half hourly data xu et al 2009 2017 then the daily et was calculated by summing the half hourly gap filled values fig 2 displays the nonlinear regression equation at arou site although ec was recognized as the best method for observed et estimation compared to the bowen ratio and lysimeter feng et al 2015 baldocchi et al 2001 energy enclosure issues generally exist feng et al 2015 in which turbulent flux measured by the flux tower is less than the available energy over alpine meadow gu et al 2005 wu et al 2015 the energy closure ratios h le rn g goulden et al 1996 leuning et al 2012 at suli arou nagqu tanggula and hulugou sites were 0 80 0 98 0 89 0 97 and 0 68 respectively since it was essential to address this issue before calculating the actual daily et the following correcting method was applied which was derived from twine et al 2000 1 et cor r n g h uncor le uncor et uncor where etcor is the corrected evapotranspiration mm d 1 rn is net radiation w m 2 g is soil heat flux w m 2 and huncor leuncor and etuncor are the uncorrected sensible heat flux w m 2 latent heat flux w m 2 and evapotranspiration mm d 1 respectively to make comparisons with the mod16 8 day product 8 day et from ec was integrated from 30 min measurements if the number n of reliable 30 min measurements from the mod16 8 day period is greater than 352 the 8 day measured et can be determined using mu s equation mu et al 2011 2 et 8 d i 1 n et i 384 n considering the absence of observed meteorological or ec data the actual number of available days for suli arou nagqu tanggula and hulugou sites during the growing season were 146 77 149 100 and 117 respectively 2 3 modified mod16 algorithm the original mod16 algorithm developed by mu et al 2007 2011 is based on the penman monteith equation monteith 1965 generally daily et can be divided into four parts 1 evaporation from wet canopy surface 2 canopy transpiration 3 evaporation from wet soil and 4 and evaporation from dry soil surface daytime and nighttime results were obtained for each part where nighttime and daytime were distinguished by hourly downward shortwave radiations less and equal to greater than 10 w m 2 respectively the biophysical parameters used for each site referred to the parameters of grassland in the study by running et al 2017 table 2 the detailed information for the mod16 algorithm can be seen in the publications by mu et al 2007 2011 and running et al 2017 3 λ e sa ρ c p e sat e r a s γ 1 r s r a where λe is the latent heat flux w m 2 a is the available energy partitioned between sensible heat and latent heat fluxes on land surface w m 2 s is the slope of saturated water vapor pressure versus temperature kpa c 1 cp is the specific heat capacity of air j kg 1 k 1 ρ is the air density kg m 3 esat e is the water vapor pressure deficit at reference height pa γ is the psychometric constant pa k 1 ra and rs are the aerodynamic and surface resistances s m 1 respectively in our study the algorithms for plant temperature and moisture constraints aerodynamic resistance for transpiration and soil evaporation were modified to fit a single algorithm 2 3 1 canopy surface resistance estimation resistance parameterization is significant to estimate et based on the penman monteith method shuttleworth and wallace 1985 mccabe et al 2005 mu et al 2007 2011 ershadi et al 2015 and canopy transpiration is an important partition for et in the original mod16 algorithm canopy surface resistance is expressed as a function of air temperature lai and rh shuttleworth and wallace 1985 however the canopy surface resistance is also affected by the microclimate and vegetation physiological characteristics vörösmarty et al 1998 jacquemin and noilhan 1990 proposed the jarvis scheme to estimate canopy resistance which represented the combination effects of plant stress in terms of solar radiation humidity air temperature and soil moisture as been implemented in the noah model similarly fisher et al 2008 introduced an eco physiological theory that considered multiple stresses on plant function through biophysical remote sensing metrics the canopy surface resistance in the original mod16 algorithm was estimated as mu et al 2007 2011 4 c ci g s 2 g si 1 g cu g s 2 g si 1 g cu lai 0 1 fwet 0 0 lai 0 1 fwet 0 g cu g cu r corr g s 2 gl sh g si 1 c l m t min m vpd r corr i daytime 0 i nighttime r si 1 c ci 5 m t min 1 0 t min t min open t min t min close t min open t min close t min close t min t min open 0 1 t min t min close 6 m vpd 1 0 vpd vpd open vpd close vpd vpd close vpd open vpd open vpd vpd close 0 1 vpd vpd close where i means the variable value at daytime or nighttime g1 s is the stomatal conductance g2 s is the leaf boundary layer conductance gcu is leaf cuticular conductance gcu is cuticular conductance per unit lai 0 00001 m s 1 glsh is leaf conductance to sensible heat per unit lai table 2 rcorr is a corrector for atmospheric temperature and pressure with standard conditions m tmin and m vpd are constraints for plant temperature and moisture close means nearly complete inhibition full stomatal closure and open refers to no inhibition to transpiration and tmin is the minimum air temperature c the minimum temperature for full stomatal closure of the grassland was set to 8 c however the tmin close for alpine meadow may be different with that for grassland due to alpine characteristic the lower temperature constraint probably induces the underestimation of the stomatal conductance and transpiration fisher et al 2008 considered that when leaf area light and temperature were high and vpd was low optimal stomata conductance occurred and thus updating the approach developed by june et al 2004 while several studies running and nemani 1988 granger and gray 1989 mu et al 2007 2011 regarded vpd as an indicator of plant moisture it may fail to capture water stress in semi arid and arid regions song et al 2012 in our modified algorithm the temperature and moisture constraint for stomatal conductance in the modified algorithm was expressed using the equations adopted by fisher et al 2008 7 m t exp t max t opt t opt 2 8 m m f apar f apar max where mt and mm are the plant temperature and moisture constraints corresponding to m tmin and m vpd in the original algorithm respectively tmax is the maximum temperature during the day topt is the optimum plant growth temperature 25 c which has been proven to obtain good results in high altitude areas yuan et al 2010 fapar is the fraction of par absorbed by green vegetation cover and faparmax is the maximum fapar during the growing season 2 3 2 aerodynamic resistance estimation for transpiration in the original mod16 algorithm canopy aerodynamic resistance was obtained from the parallel resistance to convective and radiative heat transfer which is a function of air temperature that does not need wind speed and soil moisture data mu et al 2011 9 r a rh rr rh rr rh 1 gl bl rr ρ c p 4 σ t i 273 15 3 where rh and rr are the resistances to convective and radiative heat transfer respectively glbl is leaf scale boundary layer conductance m s 1 and σ is the stefan boltzmann constant 5 67 10 8 w m 2k 4 however wind speed has been recognized as a critical variable of aerodynamic resistance that influences et sellers et al 1997 based on ershadi et al 2015 suggesting that thom s equation thom 1975 was better than mu s equation 2011 for estimating aerodynamic resistance we applied thom s method to estimate aerodynamic resistance with wind speed for canopy transpiration thom 1975 10 r a ln z d z om ln z d z oh k 2 u z where z is the reference height m d is the zero displacement height m uz is the wind speed at the height of z m s 1 and k is von karman s constant 0 41 zom and zoh are the roughness heights of momentum and water vapor transfer m respectively and are calculated by eq 11 brutsaert 2005 11 z om 0 123 h c z oh 0 1 z om where hc is the vegetation height m we set constant values for each month according to observed vegetation height 2 3 3 soil evaporation soil surface resistance is expressed as a function of air temperature and vpd in the original mod16 algorithm which is questionable due to the ignorance of soil moisture for different soil texture types di et al 2015 thus the uncertainty of soil surface resistance induces large uncertainty for soil evaporation in the original mod16 algorithm moreover there is little knowledge regarding the boundary layer resistance for soil evaporation mu et al 2011 due to the difficulty of surface and aerodynamic resistances estimation fisher et al 2008 proposed a simple bio meteorological method based on the priestley taylor for et estimation therefore fisher s method fisher et al 2008 for soil evaporation was adopted in the modified algorithm 12 λ e wet s α f wet sa s s γ λ e dry s α f sm 1 f wet sa s s γ where α is empirical coefficient 1 26 as is the component of available energy on soil surface 1 fc rn g fc is the vegetation cover fraction fwet is the relative surface wetness rh4 and fsm is soil moisture constraint which is equal to rhvpd β fisher et al 2008 2 4 statistical index we used the coefficient of determination r2 root mean square error rmse mean absolute error mae mean bias mb and nash sutcliffe efficiency coefficient nse for statistical evaluation table 3 3 results 3 1 estimated daily et comparison between original mod16 and modified algorithms to make comparisons observed air temperature relative humidity wind speed net radiation and soil heat flux data at the five sites were used to force the original mod16 and modified algorithms fig 3 to analyze the performance of two algorithms at different sites r2 rmse mae and nse between the observed et and simulated et via different algorithms were used as statistical indices fig 4 et estimated by the original mod16 algorithm had no significant correlation with measured et at suli tanggula and nagqu sites while et estimated using the original algorithm were in good agreement with measured values at arou and hulugou sites however the original mod16 algorithm underestimated et for all sites fig 3 the modified mod16 algorithm improved the accuracy of estimated et with higher r2 and nse and lower rmse and mae fig 4 the et estimated by the modified algorithm at suli site was slightly underestimated after july 9 2010 and overestimated after august 1 2010 until the end of august which may be due to the influence of the permafrost thawing nse of the modified algorithm at arou site was 0 18 which demonstrates that the simulation does not perform well at areas with high vegetation cover it was clear that the modified algorithm underestimated et before july 3 and overestimated et after september 20 2014 at arou site the modified algorithm underestimated et with a relative error of 12 1 at nagqu site while it performed better than the original algorithm the modified algorithm reduced the variation amplitude and improved the accuracy of the estimated et at tanggula site both algorithms underestimated et during the growing season while the modified mod16 algorithm overestimated et during the end of april the average r2 increased from 0 26 using the original algorithm to 0 68 using the modified algorithm for five sites the rmse for five sites ranged from 1 09 to 1 98 mm d 1 with a mean value of 1 56 mm d 1 using the original mod16 algorithm while rmse ranged from 0 41 to 1 11 mm d 1 with a mean value of 0 78 mm d 1 using the modified algorithm fig 4b mae decreased from 1 28 using the original algorithm to 0 61 mm d 1 using the modified algorithm nse values increased significantly using the modified algorithm at four sites except hulugou site fig 4d and the mean nse of the modified mod16 algorithm for all sites was 0 57 which was higher than that of the original mod16 algorithm 0 71 rmse and mb in the modified mod16 algorithm were 0 78 and 0 09 mm d 1 while those were nearly 0 88 and 0 24 mm d 1 at the qomolangma station chen et al 2013a b the monthly r2 and rmse between the high resolution land atmosphere surface parameters from space holaps dataset and the landflux eval dataset were 0 98 and 2 69 w m 2 over the whole tp respectively peng et al 2016 although it seems that the holaps datasets provided better et estimation this suggestion was based on cross comparison of the existing datasets with a monthly scale these results indicate that more validation with in situ data and remote sensing data are still needed to improve the accuracy of daily et estimation on the tp moreover the original mod16 algorithm tended to underestimate et at high values and overestimate at low values which induced substantial uncertainties fig 3 table 4 the modified mod16 algorithm reduced deviations by integrating wind speed and vegetation height into canopy aerodynamic resistance and substituting the canopy surface resistance and soil evaporation calculation method proposed by fisher et al 2008 the mean bias of peak et and minimal et using the modified mod16 algorithm for all sites were 0 42 and 0 12 mm d 1 respectively which were lower than those of the original mod16 algorithm 1 5 and 0 3 mm d 1 respectively 3 2 daily et comparison of after precipitation days and non precipitation days in addition to the strong correlation between net radiation and et wang et al 2007 precipitation is another important factor sun et al 2011 since water is a major constraint on et in semi arid areas tang et al 2010 due to the unavailability of the precipitation data at nagqu site we compared the et estimation by the original mod16 and modified algorithms for non precipitation and after precipitation days at suli arou tanggula and hulugou sites fig 5 the day after precipitation events was defined as the day with short duration 1 h precipitation such as at arou and hulugou sites or the next day after one with longer precipitation duration or more precipitation events such as at suli and tanggula sites it can be found that the modified algorithm had better performance than the original mod16 algorithm for both non precipitation and after precipitation days fig 5 the modified algorithm performed slightly better with higher r2 0 70 and lower rmse 0 61 mm d 1 for after precipitation days than for non precipitation days at suli site while it had better results for non precipitation days than for after precipitation days at arou tanggula and hulugou sites based on such performance the modified algorithm may be more suitable for estimating et for non precipitation days with higher accuracy than et for after precipitation days which had large observation errors 4 discussion 4 1 comparison 8 day estimated et using the original mod16 algorithm and modified mod16 algorithm to test the effect of different time windows on et estimation by the original mod16 and modified mod16 algorithms four sites suli nagqu tanggula and hulugou were selected for comparison fig 6 the arou site was not analyzed due to the unavailability of daily data aggregating into 8 day value for forcing data the original and modified mod16 algorithms implemented in situ data while the 8 day mod16 et used gmao reanalysis data the underestimation of the mod16 et product was significant at suli fig 6a nagqu fig 6b and tanggula fig 6c stations during the growing season the mod16 product overestimated 8 day et in april and underestimated 8 day et from may at hulugou site fig 6d the mae of the mod16 product at suli nagqu and tanggula sites were 12 64 14 18 and 17 69 mm 8 d 1 respectively and the rmse values at these sites were all greater than 10 mm 8 d 1 indicating the existence of large errors table 5 moreover the mod16 product performed worse with higher rmse 8 67 mm 8 d 1 higher mae 6 78 mm 8 d 1 and lower nse 0 55 at hulugou site which may be due to the bias of gmao reanalysis meteorological data the input data in the mod16 product including downward shortwave radiation air temperature and relative humidity extracted from the gmao reanalysis data had large biases with higher rmse and mae fig 7 a c this was in agreement with mu et al 2011 who stated that large errors exist on a local scale especially for complex terrain compared to the mod16 et product the rmse of the original mod16 algorithm decreased by substituting the observed meteorological data at four sites table 5 the et obtained from the modified mod16 algorithm had the highest correlations with measured et compared to et of the mod16 product and original mod16 algorithm at the four sites furthermore the mae values of the modified mod16 algorithm were lower than those of the mod16 product and original mod16 algorithm the nse values of the modified mod16 algorithm at suli and tanggula sites were 0 68 and 0 58 respectively while nse values of the mod16 product and original mod16 algorithm were negative nse of the modified mod16 algorithm at nagqu site was only 0 02 which was higher than the negative nse values of the mod16 product and original mod16 algorithm therefore it can be concluded that the modified mod16 algorithm performed best with higher r2 and nse values and lower rmse and mae with an 8 day time window compared to the original mod16 algorithm and mod16 et product 4 2 daily et comparisons of the modified algorithm pt jpl and regress methods re parameterizations of the original algorithm have been reported in several studies for instance yuan et al 2010 modified mod16 algorithm by substituting the temperature constraint for stomatal conductance and energy partition equations and recalibrated some parameters to set invariant values for all biome types peng et al 2016 implemented the modified aerodynamic resistance parameterization from sebs into the mod16 algorithm pmsrb pu to estimate et with seasonal and annual variations on the tp in the latter study it was found that substantial spatial temporal differences existed for the pmsrb pu and other four datasets on the tp for the modified algorithm in this study and peng s work wind speed and vegetation height were considered into the aerodynamic resistance parameterization the advantage of the modified algorithm in this study was to introduce temperature and moisture constraints suggested by fisher et al 2008 and yuan et al 2010 into the stomatal conductance and to reduce the uncertainty of boundary layer resistance for soil evaporation although we obtained high accuracy et over the alpine meadow sites using the parameters of the grassland the parameters for different species within the same biome type were different considering that we have not yet calibrated the parameters for alpine meadow which may induce uncertainty further study is needed to calibrate these parameters and improve the parameterization scheme 4 3 modular analysis of the modified mod16 algorithm to understand the relative importance of each part of the modified mod16 algorithm a modular analysis was performed by making a single change in aerodynamic resistance canopy resistance and soil evaporation table 6 the fractions of interception transpiration and soil evaporation relative to et with the original mod16 algorithm for each tower site are listed in table 7 the soil evaporation accounted for the largest proportion followed by plant transpiration for all sites interception accounted for the lowest proportion where the negative interception even occurred at nagqu site in the original mod16 algorithm nearly zero interception occurred at daytime due to the lower rh 70 and relative higher air temperature while negative interception or condensation occurred at nighttime due to the negative energy partition higher rh 70 and lower air temperature at nagqu site therefore the modification of the soil evaporation and plant transpiration could improve the accuracy of et estimation the algorithm with the aerodynamic resistance change performed better with higher r2 lower mae lower mb and higher nse values than the original mod16 algorithm at the five sites table 6 implying that the aerodynamic resistance change implemented into the modified mod16 algorithm could obtain good results in addition the algorithm with the canopy resistance change improved the results insignificantly the algorithm with the soil evaporation change performed better than that with the aerodynamic change canopy surface change and original mod16 algorithm with a higher r2 lower mae and higher nse at four sites except hulugou site mae of the algorithm with the soil evaporation change at hulugou site increased from 0 86 to 0 91 mm d 1 while the mb increased from 0 64 to 0 89 mm d 1 using the original algorithm the modified algorithm with soil evaporation change had the highest accuracy with higher r2 0 60 lower mae 0 83 mm d 1 and mb 0 69 mm d 1 and higher nse 0 26 compared to the changes of aerodynamic resistance and canopy surface resistance and original mod16 algorithm which suggests that the modified algorithm was the most sensitive to soil evaporation change the modified algorithm improved the accuracy most compared with the original algorithm the algorithm with the aerodynamic resistance change canopy surface resistance change and soil evaporation change at four sites except arou site the algorithm with the soil evaporation change performed better than the modified algorithm at arou site among the three changes the algorithm was the most sensitive to soil evaporation change followed by aerodynamic resistance change and was the least sensitive to canopy resistance change the modified mod16 algorithm performed better with the highest r2 0 68 lowest rsme 0 78 mm d 1 lowest mae 0 61 mm d 1 and highest nse 0 57 value than algorithms with a single change and the original algorithm table 6 thus the most effective algorithm should implement all changes 4 4 uncertainty and limitation of the modified algorithm the modified mod16 algorithm produced more accurate results for daily et and 8 day et estimations than the original mod16 algorithm and mod16 product based on higher r2 and lower rmse and mae values however many uncertainties still exist for the original and the modified mod16 algorithms one source of uncertainty of the original and modified mod16 algorithms may be due to the forcing data from gmao reanalysis data so the modified mod16 algorithm forced by observation fig 8 a and gmao reanalysis data fig 9 were compared the modified algorithm driven by the gmao reanalysis data showed poorer performance with a lower r2 0 47 higher rmse 1 19 mm d 1 and lower nse 0 01 than the observation force algorithm which suggests that the bias of the gmao reanalysis data led to substantial errors for et estimation aside from the forcing data uncertainties from land cover misclassification may induce biases for the mod16 product mu et al 2011 mu et al 2011 developed original mod16 et with 12 classifying biome types where each type had the same biophysical parameter globally that would produce uncertainties although there was no mismatched land cover at the five sites all were alpine meadows the characteristics and parameters of the alpine meadow were different from those of grassland which have been used in the algorithm there were no parameters for alpine meadow in the mod16 algorithm that would induce uncertainty for et estimation in addition some limitations exist in the physical mechanism of the mod16 algorithm the soil moisture constraint has been validated by soil volumetric water with good results fisher et al 2008 however the flux sites on the tp covered with permafrost which has thawing and melting processes have not been verified thus the constant value β defined as the relative sensitivity to vpd needs to be studied in future work for areas with permafrost moreover the soil moisture constraint has a large uncertainty when net radiation is large yang et al 2016 the relative surface wetness was used for the distinction of wet and dry surfaces and no water covering the surface was identified when rh was less than 70 which is unreasonable for the judgment of surface wetness considering that precipitation at the observation sites occurred with short duration on the tp which changed the soil moisture for a short time daytime or nighttime rh may not reflect the real status of soil moisture and probably introduce errors for et estimation another type of uncertainty stemming from modis fpar and lai most likely influenced the et estimation the mean value of the nine pixels around each site was considered to represent the site point scale however for the flux sites located at high altitude and mountainous terrain on the tp the mean value may not represent the site condition due to the heterogeneous land cover over the complex terrain mu et al 2011 proposed that the underestimation of lai led to the underestimation of et through the overestimation of surface resistance moreover fc influenced the available energy portion between the canopy and soil surface thus uncertainties in fc and lai may be one reason for the obvious underestimation of the mod16 product during the growing season at representative site fig 10 therefore we compared the simulated with observed fc and fpar values at suli site which had monthly observations of fc and lai qin et al 2014 it was clear that monthly fpar and lai retrieved from the modis product were underestimated when compared with the measured value causing a decrease in the available energy for the canopy and an underestimation of canopy transpiration aside from the above causes errors from the modified algorithm may be attributable to the use of α as a constant value for instance fisher et al 2008 set α equal to 1 26 to maintain the potential latent heat flux equation intact however the impact of the use of constant α influenced the estimation of the soil evaporation and affected the et estimation in the modified mod16 algorithm fig 11 increased α led to an increase in r2 and decreased in rmse and mae which means that the soil evaporation was underestimated with low α value and vice versa although the modified mod16 algorithm performed better at five sites there were more uncertainties when applied across the whole tp or to other areas with different surfaces and climates among which the most uncertainty was the spatial distribution of wind speed and the dynamic of vegetation height although many reanalysis data products have provided wind speed data these data have a coarse resolution that would induce large uncertainties under heterogeneous surfaces the wind speed extracted from the gmao reanalysis data had a lower r2 higher rmse and lower nse compared to observed values fig 7d there was no vegetation height dataset for the whole tp region except for the middle reaches of the heihe river basin li et al 2017 to validate the applicability of the modified algorithm four sites with different surfaces and different climates from ameriflux was selected and compared table 8 and 9 fig 12 it was found that the modified algorithm had better performances than the original algorithm at sites with different climates and surfaces this was especially evident at grassland sites where the modified algorithm improved the accuracy of et estimation with an increase in r2 from 0 2 to 0 58 decrease in rmse from 1 28 to 0 74 mm d 1 and increase in nse from 1 33 to 0 21 indicating that the modified algorithm improved the accuracy of et estimation over the grassland under different climates the modified algorithm slightly improved the performance at evergreen needleleaf forest site which had relatively higher r2 lower rmse and higher nse compared with the original algorithm the modified algorithm resulted in an increased correlation r2 with measured values for closed shrubland site these results indicate that the modified algorithm could estimate et with higher accuracy for several land covers compared to the original mod16 algorithm for which it is more pronounced for grassland sites however the improvement is not obvious for evergreen needleleaf forests and closed shrubland ecosystems the reason for the better performance of the modified algorithm than needleleaf forests and shrublands may be that the aerodynamic resistance parameterization with wind speed and vegetation height is more suitable for short canopy and bare soil rather than tall vegetation yang et al 2002 therefore the proposed algorithm may still need to be improved in the future to fit more land cover types which we expect to focus on in future studies 5 conclusions this paper presented a modified mod16 algorithm that incorporated recalculation of the canopy surface and aerodynamic resistances and adopted other soil evaporation equations the performances of both the original mod16 and modified mod16 algorithms were compared at five flux sites over alpine meadow on the tp the following conclusions were based on the obtained results the modified mod16 algorithm presented higher accuracy with higher r2 0 68 lower rmse 0 78 mm d 1 lower mae 0 61 mm d 1 and higher nse 0 57 values than the original mod16 algorithm at the five sites during the growing season indicating that the modified mod16 algorithm is more effective in simulating the water and energy balance on the tp the modified algorithm performed slightly better with higher r2 0 70 and lower rmse 0 61 mm d 1 values for after precipitation days than for non precipitation days at suli site while it had better results for non precipitation days than after precipitation days at arou tanggula and hulugou sites this finding suggests that the modified algorithm might be more suitable for estimating et for non precipitation days with higher accuracy than et for after precipitation days which had large observation errors in our study the modular analysis suggested that the modified algorithm was the most sensitive to change in soil evaporation followed by change in aerodynamic resistance comparisons of the modified mod16 algorithm with pt jpl fisher et al 2008 and regress wang et al 2010 indicate that the modified algorithm could produce et with high accuracy over the alpine meadow sites on the tp during the growing season overall the modified mod16 algorithm improved some equations under the principle of the mod16 algorithm we will perform more intense and extensive research in future studies to improve the mod16 algorithm from point to point by starting with theoretical physics principles in aim to develop a new scheme for et estimation over the tp acknowledgements the authors would like to thank the heihe watershed allied telemetry experimental research hiwater the nagqu station of plateau climate and environment the cryosphere research station on the tibetan plateau and the qilian alpine ecology and hydrology research station northwest institute of eco environment and resources nieer chinese academy of sciences cas for providing meteorological and eddy covariance flux data thanks for ameriflux data resources funded by the u s department of energy s office of science this work is supported by the national key research and development plan 2017yfc0404302 china national natural science foundation grants nos 41730751 41671056 41421061 
7260,the long term change of evapotranspiration et is crucial for managing water resources in areas with extreme climates such as the tibetan plateau tp this study proposed a modified algorithm for estimating et based on the mod16 algorithm on a global scale over alpine meadow on the tp in china wind speed and vegetation height were integrated to estimate aerodynamic resistance while the temperature and moisture constraints for stomatal conductance were revised based on the technique proposed by fisher et al 2008 moreover fisher s method for soil evaporation was adopted to reduce the uncertainty in soil evaporation estimation five representative alpine meadow sites on the tp were selected to investigate the performance of the modified algorithm comparisons were made between the et observed using the eddy covariance ec and estimated using both the original and modified algorithms the results revealed that the modified algorithm performed better than the original mod16 algorithm with the coefficient of determination r2 increasing from 0 26 to 0 68 and root mean square error rmse decreasing from 1 56 to 0 78 mm d 1 the modified algorithm performed slightly better with a higher r2 0 70 and lower rmse 0 61 mm d 1 for after precipitation days than for non precipitation days at suli site contrarily better results were obtained for non precipitation days than for after precipitation days at arou tanggula and hulugou sites indicating that the modified algorithm may be more suitable for estimating et for non precipitation days with higher accuracy than for after precipitation days which had large observation errors the comparisons between the modified algorithm and two mainstream methods suggested that the modified algorithm could produce high accuracy et over the alpine meadow sites on the tp keywords evapotranspiration alpine meadow modified mod16 algorithm resistance parameterization tibetan plateau 1 introduction evapotranspiration et is an important component for the global terrestrial energy budget balance and water cycle jia et al 2012 di et al 2015 liu et al 2016 therefore the long term change of regional et is of significance for improving water management and monitoring climate change di et al 2015 cleugh et al 2007 kustas et al 2007 mu et al 2007 kim et al 2012 conventional et estimation methods such as the lysimeter eddy covariance ec and bowen ratio are based on in situ measurements that cannot acquire regional or global et existing remote sensing models that provide the potential for regional et estimation can be divided into three categories 1 empirical regression models wang et al 2007 wang et al 2010 2 surface energy balance seb models such as the surface energy balance algorithm for land sebal bastiaanssen et al 1998 mapping evapotranspiration at high resolution with internalized calibration metric allen et al 2007 and surface energy balance system sebs su 2002 and 3 traditional methods combined with remote sensing data such as the penman monteith pm method with moderate resolution imaging spectroradiometer modis imagery cleugh et al 2007 mu et al 2007 2011 fisher et al 2011 and the priestly taylor equation with advanced very high resolution spectroradiometer avhrr fisher et al 2008 empirical regression models have been developed by establishing the relationship between et and satellite based net radiation temperature and vegetation indices wang et al 2007 2010 although these models can quickly capture long term regional et it is difficult to ascertain the empirical coefficients for different ecosystems due to the heterogeneity of land use types feng et al 2015 further seb models have been applied successfully in many countries with varying climates gao and long 2008 paiva et al 2011 tang et al 2013 chang et al 2017 and have been proven to produce regional et with high spatial resolution which is useful for monitoring water resources however these models cannot capture long term change of regional et due to the availability of continuous remote sensing data to address these problems cleugh et al 2007 first developed a remote sensing model based on the pm equation with modis vegetation products and meteorological data mu et al 2007 modified cleugh s model and produced the first global modis et datasets with a spatial resolution of 1 km and temporal resolutions of 8 day monthly and yearly later mu et al 2011 further improved the modis et algorithm to produce a more accurate global modis et product mod16 from 2000 to 2014 the mod16 et product has been validated and applied in many countries with various climates jia et al 2012 kim et al 2012 tang et al 2015 chen et al 2014 kim et al 2012 found that the mod16 product performed best at five forest sites and mismatched with observed et at two grassland sites in asia chen et al 2014 demonstrated that the mod16 product showed substantial differences with pt jpl fisher et al 2008 and yuan s yuan et al 2010 algorithms in china tang et al 2015 pointed out that the mod16 et product produced good agreement with ec values at one cropland site but underestimated et at four irrigated cropland sites in china feng et al 2015 found the mod16 algorithm showed reduced performance at grassland savannas and shrubland sites over semi arid ecosystems compared with pt jpl fisher et al 2008 and yao s yao et al 2013 algorithms it is pertinent to note that the performance of the mod16 et product varies under different climates and different surfaces feng et al 2016 yet few studies focus on the performance of the mod16 et product on the tibetan plateau tp which is recognized as the world s third pole and one of most sensitive areas for climate change qiu 2008 long term regional et variations on the tp are significant to understand local hydrological processes and global climate change although et on the tp has been studied using the water balance method liu et al 2016 xue et al 2013 li et al 2014 hydrological models yang et al 2011 zhou et al 2014 and satellite based models chen et al 2013a b peng et al 2016 these methods are inadequate to evaluate the accuracy of et estimation over alpine meadow due to lack of observation data which is an important land cover type on the tp further there are substantial differences among different satellite based models on the tp peng et al 2016 therefore it is important to evaluate and improve the current satellite based et algorithm mod16 algorithm for long term et estimation over alpine meadow on the tp in this study the objective was to evaluate and improve the mod16 algorithm for daily et estimation over alpine meadow on the tp by modifying evaluating and then finalizing the improved algorithm first modification of the canopy resistance parameterization scheme was introduced by integrating wind speed and vegetation height in the mod16 algorithm the algorithms for temperature and moisture constraints for canopy stomatal conductance were also improved and the soil evaporation algorithm was modified by adopting fisher s equation second the performance of original and modified mod16 algorithms were evaluated at five representative sites on the tp the results of the two algorithms under non precipitation and after precipitation days were also analyzed and the performance of the mod16 product original mod16 algorithm and modified algorithm at four representative sites with an 8 day time window were compared third the uncertainties and limitations of the modified mod16 algorithm were determined the study will help to obtain more accurate long term and continuous et over alpine meadow on the tp 2 materials and methods 2 1 study sites observed data at five flux tower sites on the tp during the growing season were collected liu et al 2011 li et al 2013 chen et al 2015 2017 observation data at arou site can be downloaded from the internet http westdc westgis ac cn data the vegetation type was alpine meadow with semiarid climate and elevation ranges from 3033 to 5100 m at the observed area the land classification type was grassland provided by mod12q1 for all sites the locations of the flux tower sites are shown in fig 1 and details of the tower sites are listed in table 1 only one growing season was available for each location with different years due to lack of data the observed half hourly meteorological data which included air temperature ta relative humidity rh and wind speed u at five sites were aggregated into daily data and used as forcing data for the models in this study the forcing data also included net radiation rn and soil heat flux g the net radiation was measured by a net radiometer cnr while the surface soil heat flux was measured by soil heat flux plate buried 5 cm underground and was corrected using the thermal diffusion equation and correction method with soil temperature and soil water content yang and wang 2008 the le measured by ec was used as validation the results of the original mod16 and modified mod16 algorithms were both driven by observed meteorological data fraction of photosynthetically active radiation fpar and leaf area index lai data were extracted from mod15a2 with spatial resolution of 1 km and temporal resolution of 8 day the unreliable fpar and lai data were linearly filled with the nearest reliable value before and after the missing data mu et al 2011 the mean value of the nine surrounding pixels around each site was considered to be the site point scale value mu et al 2007 daily fpar and lai data were linearly interpolated by the 8 day mod15a2 product we also used meteorological data from the modern era retrospective analysis research and applications merra product by nasa s global modeling and assimilation office gmao to explore the limitations of the algorithm section 4 4 the mod16 8 day product data were downloaded from the internet https e4ftl01 cr usgs gov molt which were driven by merra reanalysis data 2 2 data pre processing edire software http www geos ed ac uk homes jbm micromet edire was employed for ec data processing with observation correction and quality assessment the observation correction included the removal of spikes coordinate rotation 2 d rotation frequency response correction sonic virtual temperature correction and corrections for density fluctuation webb pearman leuning wpl correction data quality assessment was performed using the turbulence stationary test and integrated turbulence characteristics test the flag system 0 1 and 2 mauder and foken 2015 was used for quality assessment where a flag of 0 represented the best data data were discarded when precipitation occurred within 1 h before and after data collection or when the data quality control flag was marked as 2 as well as data at night when the friction velocity was below 0 1 m s 1 blanken 1998 liu et al 2011 li et al 2013 if the number n of reliable 30 min measurements was less than 40 per day the daily measurements were set as null values the missing half hourly latent heat flux data were filled by the nonlinear regression method establish the relationship between the latent heat flux and net radiation le ar2 n brn c which was considered a better interpolation method for half hourly data xu et al 2009 2017 then the daily et was calculated by summing the half hourly gap filled values fig 2 displays the nonlinear regression equation at arou site although ec was recognized as the best method for observed et estimation compared to the bowen ratio and lysimeter feng et al 2015 baldocchi et al 2001 energy enclosure issues generally exist feng et al 2015 in which turbulent flux measured by the flux tower is less than the available energy over alpine meadow gu et al 2005 wu et al 2015 the energy closure ratios h le rn g goulden et al 1996 leuning et al 2012 at suli arou nagqu tanggula and hulugou sites were 0 80 0 98 0 89 0 97 and 0 68 respectively since it was essential to address this issue before calculating the actual daily et the following correcting method was applied which was derived from twine et al 2000 1 et cor r n g h uncor le uncor et uncor where etcor is the corrected evapotranspiration mm d 1 rn is net radiation w m 2 g is soil heat flux w m 2 and huncor leuncor and etuncor are the uncorrected sensible heat flux w m 2 latent heat flux w m 2 and evapotranspiration mm d 1 respectively to make comparisons with the mod16 8 day product 8 day et from ec was integrated from 30 min measurements if the number n of reliable 30 min measurements from the mod16 8 day period is greater than 352 the 8 day measured et can be determined using mu s equation mu et al 2011 2 et 8 d i 1 n et i 384 n considering the absence of observed meteorological or ec data the actual number of available days for suli arou nagqu tanggula and hulugou sites during the growing season were 146 77 149 100 and 117 respectively 2 3 modified mod16 algorithm the original mod16 algorithm developed by mu et al 2007 2011 is based on the penman monteith equation monteith 1965 generally daily et can be divided into four parts 1 evaporation from wet canopy surface 2 canopy transpiration 3 evaporation from wet soil and 4 and evaporation from dry soil surface daytime and nighttime results were obtained for each part where nighttime and daytime were distinguished by hourly downward shortwave radiations less and equal to greater than 10 w m 2 respectively the biophysical parameters used for each site referred to the parameters of grassland in the study by running et al 2017 table 2 the detailed information for the mod16 algorithm can be seen in the publications by mu et al 2007 2011 and running et al 2017 3 λ e sa ρ c p e sat e r a s γ 1 r s r a where λe is the latent heat flux w m 2 a is the available energy partitioned between sensible heat and latent heat fluxes on land surface w m 2 s is the slope of saturated water vapor pressure versus temperature kpa c 1 cp is the specific heat capacity of air j kg 1 k 1 ρ is the air density kg m 3 esat e is the water vapor pressure deficit at reference height pa γ is the psychometric constant pa k 1 ra and rs are the aerodynamic and surface resistances s m 1 respectively in our study the algorithms for plant temperature and moisture constraints aerodynamic resistance for transpiration and soil evaporation were modified to fit a single algorithm 2 3 1 canopy surface resistance estimation resistance parameterization is significant to estimate et based on the penman monteith method shuttleworth and wallace 1985 mccabe et al 2005 mu et al 2007 2011 ershadi et al 2015 and canopy transpiration is an important partition for et in the original mod16 algorithm canopy surface resistance is expressed as a function of air temperature lai and rh shuttleworth and wallace 1985 however the canopy surface resistance is also affected by the microclimate and vegetation physiological characteristics vörösmarty et al 1998 jacquemin and noilhan 1990 proposed the jarvis scheme to estimate canopy resistance which represented the combination effects of plant stress in terms of solar radiation humidity air temperature and soil moisture as been implemented in the noah model similarly fisher et al 2008 introduced an eco physiological theory that considered multiple stresses on plant function through biophysical remote sensing metrics the canopy surface resistance in the original mod16 algorithm was estimated as mu et al 2007 2011 4 c ci g s 2 g si 1 g cu g s 2 g si 1 g cu lai 0 1 fwet 0 0 lai 0 1 fwet 0 g cu g cu r corr g s 2 gl sh g si 1 c l m t min m vpd r corr i daytime 0 i nighttime r si 1 c ci 5 m t min 1 0 t min t min open t min t min close t min open t min close t min close t min t min open 0 1 t min t min close 6 m vpd 1 0 vpd vpd open vpd close vpd vpd close vpd open vpd open vpd vpd close 0 1 vpd vpd close where i means the variable value at daytime or nighttime g1 s is the stomatal conductance g2 s is the leaf boundary layer conductance gcu is leaf cuticular conductance gcu is cuticular conductance per unit lai 0 00001 m s 1 glsh is leaf conductance to sensible heat per unit lai table 2 rcorr is a corrector for atmospheric temperature and pressure with standard conditions m tmin and m vpd are constraints for plant temperature and moisture close means nearly complete inhibition full stomatal closure and open refers to no inhibition to transpiration and tmin is the minimum air temperature c the minimum temperature for full stomatal closure of the grassland was set to 8 c however the tmin close for alpine meadow may be different with that for grassland due to alpine characteristic the lower temperature constraint probably induces the underestimation of the stomatal conductance and transpiration fisher et al 2008 considered that when leaf area light and temperature were high and vpd was low optimal stomata conductance occurred and thus updating the approach developed by june et al 2004 while several studies running and nemani 1988 granger and gray 1989 mu et al 2007 2011 regarded vpd as an indicator of plant moisture it may fail to capture water stress in semi arid and arid regions song et al 2012 in our modified algorithm the temperature and moisture constraint for stomatal conductance in the modified algorithm was expressed using the equations adopted by fisher et al 2008 7 m t exp t max t opt t opt 2 8 m m f apar f apar max where mt and mm are the plant temperature and moisture constraints corresponding to m tmin and m vpd in the original algorithm respectively tmax is the maximum temperature during the day topt is the optimum plant growth temperature 25 c which has been proven to obtain good results in high altitude areas yuan et al 2010 fapar is the fraction of par absorbed by green vegetation cover and faparmax is the maximum fapar during the growing season 2 3 2 aerodynamic resistance estimation for transpiration in the original mod16 algorithm canopy aerodynamic resistance was obtained from the parallel resistance to convective and radiative heat transfer which is a function of air temperature that does not need wind speed and soil moisture data mu et al 2011 9 r a rh rr rh rr rh 1 gl bl rr ρ c p 4 σ t i 273 15 3 where rh and rr are the resistances to convective and radiative heat transfer respectively glbl is leaf scale boundary layer conductance m s 1 and σ is the stefan boltzmann constant 5 67 10 8 w m 2k 4 however wind speed has been recognized as a critical variable of aerodynamic resistance that influences et sellers et al 1997 based on ershadi et al 2015 suggesting that thom s equation thom 1975 was better than mu s equation 2011 for estimating aerodynamic resistance we applied thom s method to estimate aerodynamic resistance with wind speed for canopy transpiration thom 1975 10 r a ln z d z om ln z d z oh k 2 u z where z is the reference height m d is the zero displacement height m uz is the wind speed at the height of z m s 1 and k is von karman s constant 0 41 zom and zoh are the roughness heights of momentum and water vapor transfer m respectively and are calculated by eq 11 brutsaert 2005 11 z om 0 123 h c z oh 0 1 z om where hc is the vegetation height m we set constant values for each month according to observed vegetation height 2 3 3 soil evaporation soil surface resistance is expressed as a function of air temperature and vpd in the original mod16 algorithm which is questionable due to the ignorance of soil moisture for different soil texture types di et al 2015 thus the uncertainty of soil surface resistance induces large uncertainty for soil evaporation in the original mod16 algorithm moreover there is little knowledge regarding the boundary layer resistance for soil evaporation mu et al 2011 due to the difficulty of surface and aerodynamic resistances estimation fisher et al 2008 proposed a simple bio meteorological method based on the priestley taylor for et estimation therefore fisher s method fisher et al 2008 for soil evaporation was adopted in the modified algorithm 12 λ e wet s α f wet sa s s γ λ e dry s α f sm 1 f wet sa s s γ where α is empirical coefficient 1 26 as is the component of available energy on soil surface 1 fc rn g fc is the vegetation cover fraction fwet is the relative surface wetness rh4 and fsm is soil moisture constraint which is equal to rhvpd β fisher et al 2008 2 4 statistical index we used the coefficient of determination r2 root mean square error rmse mean absolute error mae mean bias mb and nash sutcliffe efficiency coefficient nse for statistical evaluation table 3 3 results 3 1 estimated daily et comparison between original mod16 and modified algorithms to make comparisons observed air temperature relative humidity wind speed net radiation and soil heat flux data at the five sites were used to force the original mod16 and modified algorithms fig 3 to analyze the performance of two algorithms at different sites r2 rmse mae and nse between the observed et and simulated et via different algorithms were used as statistical indices fig 4 et estimated by the original mod16 algorithm had no significant correlation with measured et at suli tanggula and nagqu sites while et estimated using the original algorithm were in good agreement with measured values at arou and hulugou sites however the original mod16 algorithm underestimated et for all sites fig 3 the modified mod16 algorithm improved the accuracy of estimated et with higher r2 and nse and lower rmse and mae fig 4 the et estimated by the modified algorithm at suli site was slightly underestimated after july 9 2010 and overestimated after august 1 2010 until the end of august which may be due to the influence of the permafrost thawing nse of the modified algorithm at arou site was 0 18 which demonstrates that the simulation does not perform well at areas with high vegetation cover it was clear that the modified algorithm underestimated et before july 3 and overestimated et after september 20 2014 at arou site the modified algorithm underestimated et with a relative error of 12 1 at nagqu site while it performed better than the original algorithm the modified algorithm reduced the variation amplitude and improved the accuracy of the estimated et at tanggula site both algorithms underestimated et during the growing season while the modified mod16 algorithm overestimated et during the end of april the average r2 increased from 0 26 using the original algorithm to 0 68 using the modified algorithm for five sites the rmse for five sites ranged from 1 09 to 1 98 mm d 1 with a mean value of 1 56 mm d 1 using the original mod16 algorithm while rmse ranged from 0 41 to 1 11 mm d 1 with a mean value of 0 78 mm d 1 using the modified algorithm fig 4b mae decreased from 1 28 using the original algorithm to 0 61 mm d 1 using the modified algorithm nse values increased significantly using the modified algorithm at four sites except hulugou site fig 4d and the mean nse of the modified mod16 algorithm for all sites was 0 57 which was higher than that of the original mod16 algorithm 0 71 rmse and mb in the modified mod16 algorithm were 0 78 and 0 09 mm d 1 while those were nearly 0 88 and 0 24 mm d 1 at the qomolangma station chen et al 2013a b the monthly r2 and rmse between the high resolution land atmosphere surface parameters from space holaps dataset and the landflux eval dataset were 0 98 and 2 69 w m 2 over the whole tp respectively peng et al 2016 although it seems that the holaps datasets provided better et estimation this suggestion was based on cross comparison of the existing datasets with a monthly scale these results indicate that more validation with in situ data and remote sensing data are still needed to improve the accuracy of daily et estimation on the tp moreover the original mod16 algorithm tended to underestimate et at high values and overestimate at low values which induced substantial uncertainties fig 3 table 4 the modified mod16 algorithm reduced deviations by integrating wind speed and vegetation height into canopy aerodynamic resistance and substituting the canopy surface resistance and soil evaporation calculation method proposed by fisher et al 2008 the mean bias of peak et and minimal et using the modified mod16 algorithm for all sites were 0 42 and 0 12 mm d 1 respectively which were lower than those of the original mod16 algorithm 1 5 and 0 3 mm d 1 respectively 3 2 daily et comparison of after precipitation days and non precipitation days in addition to the strong correlation between net radiation and et wang et al 2007 precipitation is another important factor sun et al 2011 since water is a major constraint on et in semi arid areas tang et al 2010 due to the unavailability of the precipitation data at nagqu site we compared the et estimation by the original mod16 and modified algorithms for non precipitation and after precipitation days at suli arou tanggula and hulugou sites fig 5 the day after precipitation events was defined as the day with short duration 1 h precipitation such as at arou and hulugou sites or the next day after one with longer precipitation duration or more precipitation events such as at suli and tanggula sites it can be found that the modified algorithm had better performance than the original mod16 algorithm for both non precipitation and after precipitation days fig 5 the modified algorithm performed slightly better with higher r2 0 70 and lower rmse 0 61 mm d 1 for after precipitation days than for non precipitation days at suli site while it had better results for non precipitation days than for after precipitation days at arou tanggula and hulugou sites based on such performance the modified algorithm may be more suitable for estimating et for non precipitation days with higher accuracy than et for after precipitation days which had large observation errors 4 discussion 4 1 comparison 8 day estimated et using the original mod16 algorithm and modified mod16 algorithm to test the effect of different time windows on et estimation by the original mod16 and modified mod16 algorithms four sites suli nagqu tanggula and hulugou were selected for comparison fig 6 the arou site was not analyzed due to the unavailability of daily data aggregating into 8 day value for forcing data the original and modified mod16 algorithms implemented in situ data while the 8 day mod16 et used gmao reanalysis data the underestimation of the mod16 et product was significant at suli fig 6a nagqu fig 6b and tanggula fig 6c stations during the growing season the mod16 product overestimated 8 day et in april and underestimated 8 day et from may at hulugou site fig 6d the mae of the mod16 product at suli nagqu and tanggula sites were 12 64 14 18 and 17 69 mm 8 d 1 respectively and the rmse values at these sites were all greater than 10 mm 8 d 1 indicating the existence of large errors table 5 moreover the mod16 product performed worse with higher rmse 8 67 mm 8 d 1 higher mae 6 78 mm 8 d 1 and lower nse 0 55 at hulugou site which may be due to the bias of gmao reanalysis meteorological data the input data in the mod16 product including downward shortwave radiation air temperature and relative humidity extracted from the gmao reanalysis data had large biases with higher rmse and mae fig 7 a c this was in agreement with mu et al 2011 who stated that large errors exist on a local scale especially for complex terrain compared to the mod16 et product the rmse of the original mod16 algorithm decreased by substituting the observed meteorological data at four sites table 5 the et obtained from the modified mod16 algorithm had the highest correlations with measured et compared to et of the mod16 product and original mod16 algorithm at the four sites furthermore the mae values of the modified mod16 algorithm were lower than those of the mod16 product and original mod16 algorithm the nse values of the modified mod16 algorithm at suli and tanggula sites were 0 68 and 0 58 respectively while nse values of the mod16 product and original mod16 algorithm were negative nse of the modified mod16 algorithm at nagqu site was only 0 02 which was higher than the negative nse values of the mod16 product and original mod16 algorithm therefore it can be concluded that the modified mod16 algorithm performed best with higher r2 and nse values and lower rmse and mae with an 8 day time window compared to the original mod16 algorithm and mod16 et product 4 2 daily et comparisons of the modified algorithm pt jpl and regress methods re parameterizations of the original algorithm have been reported in several studies for instance yuan et al 2010 modified mod16 algorithm by substituting the temperature constraint for stomatal conductance and energy partition equations and recalibrated some parameters to set invariant values for all biome types peng et al 2016 implemented the modified aerodynamic resistance parameterization from sebs into the mod16 algorithm pmsrb pu to estimate et with seasonal and annual variations on the tp in the latter study it was found that substantial spatial temporal differences existed for the pmsrb pu and other four datasets on the tp for the modified algorithm in this study and peng s work wind speed and vegetation height were considered into the aerodynamic resistance parameterization the advantage of the modified algorithm in this study was to introduce temperature and moisture constraints suggested by fisher et al 2008 and yuan et al 2010 into the stomatal conductance and to reduce the uncertainty of boundary layer resistance for soil evaporation although we obtained high accuracy et over the alpine meadow sites using the parameters of the grassland the parameters for different species within the same biome type were different considering that we have not yet calibrated the parameters for alpine meadow which may induce uncertainty further study is needed to calibrate these parameters and improve the parameterization scheme 4 3 modular analysis of the modified mod16 algorithm to understand the relative importance of each part of the modified mod16 algorithm a modular analysis was performed by making a single change in aerodynamic resistance canopy resistance and soil evaporation table 6 the fractions of interception transpiration and soil evaporation relative to et with the original mod16 algorithm for each tower site are listed in table 7 the soil evaporation accounted for the largest proportion followed by plant transpiration for all sites interception accounted for the lowest proportion where the negative interception even occurred at nagqu site in the original mod16 algorithm nearly zero interception occurred at daytime due to the lower rh 70 and relative higher air temperature while negative interception or condensation occurred at nighttime due to the negative energy partition higher rh 70 and lower air temperature at nagqu site therefore the modification of the soil evaporation and plant transpiration could improve the accuracy of et estimation the algorithm with the aerodynamic resistance change performed better with higher r2 lower mae lower mb and higher nse values than the original mod16 algorithm at the five sites table 6 implying that the aerodynamic resistance change implemented into the modified mod16 algorithm could obtain good results in addition the algorithm with the canopy resistance change improved the results insignificantly the algorithm with the soil evaporation change performed better than that with the aerodynamic change canopy surface change and original mod16 algorithm with a higher r2 lower mae and higher nse at four sites except hulugou site mae of the algorithm with the soil evaporation change at hulugou site increased from 0 86 to 0 91 mm d 1 while the mb increased from 0 64 to 0 89 mm d 1 using the original algorithm the modified algorithm with soil evaporation change had the highest accuracy with higher r2 0 60 lower mae 0 83 mm d 1 and mb 0 69 mm d 1 and higher nse 0 26 compared to the changes of aerodynamic resistance and canopy surface resistance and original mod16 algorithm which suggests that the modified algorithm was the most sensitive to soil evaporation change the modified algorithm improved the accuracy most compared with the original algorithm the algorithm with the aerodynamic resistance change canopy surface resistance change and soil evaporation change at four sites except arou site the algorithm with the soil evaporation change performed better than the modified algorithm at arou site among the three changes the algorithm was the most sensitive to soil evaporation change followed by aerodynamic resistance change and was the least sensitive to canopy resistance change the modified mod16 algorithm performed better with the highest r2 0 68 lowest rsme 0 78 mm d 1 lowest mae 0 61 mm d 1 and highest nse 0 57 value than algorithms with a single change and the original algorithm table 6 thus the most effective algorithm should implement all changes 4 4 uncertainty and limitation of the modified algorithm the modified mod16 algorithm produced more accurate results for daily et and 8 day et estimations than the original mod16 algorithm and mod16 product based on higher r2 and lower rmse and mae values however many uncertainties still exist for the original and the modified mod16 algorithms one source of uncertainty of the original and modified mod16 algorithms may be due to the forcing data from gmao reanalysis data so the modified mod16 algorithm forced by observation fig 8 a and gmao reanalysis data fig 9 were compared the modified algorithm driven by the gmao reanalysis data showed poorer performance with a lower r2 0 47 higher rmse 1 19 mm d 1 and lower nse 0 01 than the observation force algorithm which suggests that the bias of the gmao reanalysis data led to substantial errors for et estimation aside from the forcing data uncertainties from land cover misclassification may induce biases for the mod16 product mu et al 2011 mu et al 2011 developed original mod16 et with 12 classifying biome types where each type had the same biophysical parameter globally that would produce uncertainties although there was no mismatched land cover at the five sites all were alpine meadows the characteristics and parameters of the alpine meadow were different from those of grassland which have been used in the algorithm there were no parameters for alpine meadow in the mod16 algorithm that would induce uncertainty for et estimation in addition some limitations exist in the physical mechanism of the mod16 algorithm the soil moisture constraint has been validated by soil volumetric water with good results fisher et al 2008 however the flux sites on the tp covered with permafrost which has thawing and melting processes have not been verified thus the constant value β defined as the relative sensitivity to vpd needs to be studied in future work for areas with permafrost moreover the soil moisture constraint has a large uncertainty when net radiation is large yang et al 2016 the relative surface wetness was used for the distinction of wet and dry surfaces and no water covering the surface was identified when rh was less than 70 which is unreasonable for the judgment of surface wetness considering that precipitation at the observation sites occurred with short duration on the tp which changed the soil moisture for a short time daytime or nighttime rh may not reflect the real status of soil moisture and probably introduce errors for et estimation another type of uncertainty stemming from modis fpar and lai most likely influenced the et estimation the mean value of the nine pixels around each site was considered to represent the site point scale however for the flux sites located at high altitude and mountainous terrain on the tp the mean value may not represent the site condition due to the heterogeneous land cover over the complex terrain mu et al 2011 proposed that the underestimation of lai led to the underestimation of et through the overestimation of surface resistance moreover fc influenced the available energy portion between the canopy and soil surface thus uncertainties in fc and lai may be one reason for the obvious underestimation of the mod16 product during the growing season at representative site fig 10 therefore we compared the simulated with observed fc and fpar values at suli site which had monthly observations of fc and lai qin et al 2014 it was clear that monthly fpar and lai retrieved from the modis product were underestimated when compared with the measured value causing a decrease in the available energy for the canopy and an underestimation of canopy transpiration aside from the above causes errors from the modified algorithm may be attributable to the use of α as a constant value for instance fisher et al 2008 set α equal to 1 26 to maintain the potential latent heat flux equation intact however the impact of the use of constant α influenced the estimation of the soil evaporation and affected the et estimation in the modified mod16 algorithm fig 11 increased α led to an increase in r2 and decreased in rmse and mae which means that the soil evaporation was underestimated with low α value and vice versa although the modified mod16 algorithm performed better at five sites there were more uncertainties when applied across the whole tp or to other areas with different surfaces and climates among which the most uncertainty was the spatial distribution of wind speed and the dynamic of vegetation height although many reanalysis data products have provided wind speed data these data have a coarse resolution that would induce large uncertainties under heterogeneous surfaces the wind speed extracted from the gmao reanalysis data had a lower r2 higher rmse and lower nse compared to observed values fig 7d there was no vegetation height dataset for the whole tp region except for the middle reaches of the heihe river basin li et al 2017 to validate the applicability of the modified algorithm four sites with different surfaces and different climates from ameriflux was selected and compared table 8 and 9 fig 12 it was found that the modified algorithm had better performances than the original algorithm at sites with different climates and surfaces this was especially evident at grassland sites where the modified algorithm improved the accuracy of et estimation with an increase in r2 from 0 2 to 0 58 decrease in rmse from 1 28 to 0 74 mm d 1 and increase in nse from 1 33 to 0 21 indicating that the modified algorithm improved the accuracy of et estimation over the grassland under different climates the modified algorithm slightly improved the performance at evergreen needleleaf forest site which had relatively higher r2 lower rmse and higher nse compared with the original algorithm the modified algorithm resulted in an increased correlation r2 with measured values for closed shrubland site these results indicate that the modified algorithm could estimate et with higher accuracy for several land covers compared to the original mod16 algorithm for which it is more pronounced for grassland sites however the improvement is not obvious for evergreen needleleaf forests and closed shrubland ecosystems the reason for the better performance of the modified algorithm than needleleaf forests and shrublands may be that the aerodynamic resistance parameterization with wind speed and vegetation height is more suitable for short canopy and bare soil rather than tall vegetation yang et al 2002 therefore the proposed algorithm may still need to be improved in the future to fit more land cover types which we expect to focus on in future studies 5 conclusions this paper presented a modified mod16 algorithm that incorporated recalculation of the canopy surface and aerodynamic resistances and adopted other soil evaporation equations the performances of both the original mod16 and modified mod16 algorithms were compared at five flux sites over alpine meadow on the tp the following conclusions were based on the obtained results the modified mod16 algorithm presented higher accuracy with higher r2 0 68 lower rmse 0 78 mm d 1 lower mae 0 61 mm d 1 and higher nse 0 57 values than the original mod16 algorithm at the five sites during the growing season indicating that the modified mod16 algorithm is more effective in simulating the water and energy balance on the tp the modified algorithm performed slightly better with higher r2 0 70 and lower rmse 0 61 mm d 1 values for after precipitation days than for non precipitation days at suli site while it had better results for non precipitation days than after precipitation days at arou tanggula and hulugou sites this finding suggests that the modified algorithm might be more suitable for estimating et for non precipitation days with higher accuracy than et for after precipitation days which had large observation errors in our study the modular analysis suggested that the modified algorithm was the most sensitive to change in soil evaporation followed by change in aerodynamic resistance comparisons of the modified mod16 algorithm with pt jpl fisher et al 2008 and regress wang et al 2010 indicate that the modified algorithm could produce et with high accuracy over the alpine meadow sites on the tp during the growing season overall the modified mod16 algorithm improved some equations under the principle of the mod16 algorithm we will perform more intense and extensive research in future studies to improve the mod16 algorithm from point to point by starting with theoretical physics principles in aim to develop a new scheme for et estimation over the tp acknowledgements the authors would like to thank the heihe watershed allied telemetry experimental research hiwater the nagqu station of plateau climate and environment the cryosphere research station on the tibetan plateau and the qilian alpine ecology and hydrology research station northwest institute of eco environment and resources nieer chinese academy of sciences cas for providing meteorological and eddy covariance flux data thanks for ameriflux data resources funded by the u s department of energy s office of science this work is supported by the national key research and development plan 2017yfc0404302 china national natural science foundation grants nos 41730751 41671056 41421061 
7261,floodplain mapping using hydrodynamic models is difficult in data scarce regions additionally using hydrodynamic models to map floodplain over large stream network can be computationally challenging some of these limitations of floodplain mapping using hydrodynamic modeling can be overcome by developing computationally efficient statistical methods to identify floodplains in large and ungauged watersheds using publicly available data this paper proposes a geomorphic model to generate probabilistic 100 year floodplain maps for the conterminous united states conus the proposed model first categorizes the watersheds in the conus into three classes based on the height of the water surface corresponding to the 100 year flood from the streambed next the probability that any watershed in the conus belongs to one of these three classes is computed through supervised classification using watershed characteristics related to topography hydrography land use and climate the result of this classification is then fed into a probabilistic threshold binary classifier ptbc to generate the probabilistic 100 year floodplain maps the supervised classification algorithm is trained by using the 100 year flood insurance rated maps firm from the u s federal emergency management agency fema fema firms are also used to validate the performance of the proposed model in areas not included in the training additionally hec ras model generated flood inundation extents are used to validate the model performance at fifteen sites that lack fema maps validation results show that the probabilistic 100 year floodplain maps generated by proposed model match well with both fema and hec ras generated maps on average the error of predicted flood extents is around 14 across the conus the high accuracy of the validation results shows the reliability of the geomorphic model as an alternative approach for fast and cost effective delineation of 100 year floodplains for the conus keywords floodplain mapping geomorphic model watershed characteristics watershed classification data scarce regions 1 introduction digital elevation models dems play a critical role in flood inundation mapping by providing floodplain topography as input to hydrodynamic models and then enabling the mapping of the floodplain by using the resulting water surface elevations bates and de roo 2000 casas et al 2006 merwade et al 2008a noman et al 2001 tate et al 2002 townsend and walsh 1998 most commonly the hydrodynamic modeling approach is used to create flood hazard maps corresponding to a rare high flood magnitude of 100 year return period or higher although this approach can provide very accurate floodplain maps it is computationally demanding as a result the modeling approach to flood hazard mapping works well for individual streams but its efficiency drops significantly when used to map floodplains over a large stream network cobby et al 2003 although there are ongoing efforts to use hydrodynamic models for large scale floodplain mapping sampson et al 2015 wing et al 2017 the issue related to high computational demand still exists in the recent years geomorphic methods that use topography data in the form of digital elevation model dem and its derivatives such as wetness index and slope have been used to map floodplains geomorphic methods are not only used to delineate the geomorphic floodplain riparian area just above the bank full discharge corresponding to a 1 5 2 year flow but they can be trained using 100 year hazard maps to provide 100 year flood inundation extent while the accurate hydrodynamics resulting from river structures and complex geometry cannot be accounted by the geomorphic methods they provide an optimal solution by providing the required accuracy in large scale floodplain mapping at a much lower cost bates 2004 bradley et al 1996 considering the importance of flood hazard it is important to understand the role of uncertainty and incorporate that information in flood hazard maps the hydrodynamic modeling approach is suitable for accounting various uncertainties and thus lends itself to creating probabilistic floodplain maps merwade et al 2008b conducted a detailed analysis on the potential sources of uncertainty arising in floodplain mapping problems they showed that uncertainty in design flow train datasets and modeling approach are three major components affecting the inundation extents alfonso and tefferi 2015 di baldassarre et al 2010 yan et al 2013 to generate a probabilistic floodplain map a large number of hydrodynamic model configurations corresponding to a distinct combination of uncertain data input and or model parameters are executed to generate an ensemble of flood inundation maps this ensemble is then used to assign the probability of flooding to any given point within the floodplain to get a probabilistic floodplain map aronica et al 2002 domeneghetti et al 2013 neal et al 2013 purvis et al 2008 sarhadi et al 2012 besides providing a robust prediction for flood inundation probabilistic presentation of floodplain areas is also beneficial for decision making and risk analysis alfonso et al 2016 again this process is time consuming and computationally demanding the objective of this paper is to propose a method to avoid this computational burden in the hydrodynamic modeling approach by developing a geomorphic model based probabilistic floodplain mapping approach that relates the flood extent to watershed characteristics wolman 1971 conducted one of the first studies to explore floodplain mapping using alternative approaches in which flood mapping methods were compared by dividing them into several groups including physiographic pedologic vegetation occasional floods regional floods of selected frequency and flood profiles and backwater curves while this study did not focus on the details of any specific method it provided a general insight on these alternative floodplain mapping methods williams et al 2000 suggested a simple method to delineate floodplains by subtracting the dem from an assumed constant water level for the entire stream network the main limitation of this method was the assumption of constant water level and the lack of a reliable method to find the actual water depth in the rivers later a series of methods to identify low lying valleys based on dem were developed dodov and foufoula georgiou 2005 gallant and dowling 2003 mcglynn and seibert 2003 for example gallant and dowling 2003 proposed a multiresolution index to estimate the valley bottoms although distinguishing valley bottoms from hillslopes is a valuable task for hydrologic purposes these areas do not account for a particular flood magnitude or frequency nardi et al 2013 2006 proposed a hydrogeomorphic method for mapping floodplains in which the hydrologic characteristics of a flood event were also incorporated into the modeling process therefore the method was able to map floodplains corresponding to specific flood frequencies in addition to dem methods based on soil information have also been proposed for floodplain mapping sangwan and merwade 2015 some recently developed alternative methods for floodplain mapping are based on supervised and unsupervised classification clustering of data unsupervised methods attempt to group data points into several clusters based on similarity of their attributes a common form of clustering in hydrological problems is termed regionalization where a large heterogeneous area is divided into smaller homogeneous regions based on multiple watershed characteristics shih min et al 2002 rao 2004 rao and srinivas 2006a 2006b razavi and coulibaly 2013 ridolfi et al 2016 watershed characteristics have been widely used as reliable descriptors of hydrologic variables in ungauged basins berger and entekhabi 2001 ganora et al 2009 patton and baker 1976 sankarasubramanian and vogel 2002 sefton and howarth 1998 thomas and benson 1970 specifically several regional regression models have been developed in the past few decades to relate streamflow statistics e g 100 year flood mean annual flow 7 day low flow frequencies with watershed characteristics acreman 1985 crippen and bue 1977 ries 2007 sauer et al 1983 thomas and benson 1970 turnipseed and ries iii 2007 besides regionalization clustering methods can also be used to map flood risk areas in one study papaioannou et al 2014 employed a clustering method to classify a raster into different levels of flood risk areas they used multi criteria evaluation methods to select and find the weights of the most significant factors for clustering selection of proper factors and weights can add huge uncertainties in the results of unsupervised classification methods largely due to the high sensitivity of clustering results to the unknown weight of factors spatial supervised classification methods attempt to find a pattern in the attributes of some labeled data training data and utilize that pattern to classify the unknown data test data these methods have been successfully used in floorplain mapping by dividing a watershed into a grid of cells where each cell can be classified as flood or non flood binary classification an observed or reliable floodplain map is required as a reference to train the classifier and then the trained classifier predicts the class labels of unknown cells de risi et al 2014 degiorgis et al 2013 manfreda et al 2015 2014 samela et al 2016 in order to find the best classifier different morphological features have been proposed including the modified topographic index manfreda et al 2011 manfreda et al 2008 and topographic wetness index de risi et al 2014 degiorgis et al 2012 compared the performance of several single morphologic features and suggested that feature h defined as the difference in elevation between a given cell and the lowest elevation in the nearest stream as represented in a dem nobre et al 2011 rennó et al 2008 plays the most significant role in these methods their results showed that using several features and or more complicated classification methods such as support vector machine is not necessary in these past studies a binary threshold classification is used so that a threshold on the morphologic feature is chosen based on minimizing the error between the reference and predicted flood extents the supervised binary classification based on finding a threshold on morphologic feature h trh is a reliable approach for floodplain mapping over large areas because it is simple fast and accurate in addition it can identify the floodplains associated with a particular flood frequency such as 100 year floods however this method like any supervised classification problem needs some training based on a reference map the reference maps are usually provided by collecting detailed survey data from field measurements and running hydrodynamic models the dependency of this method on the reference map and hydraulic data limits its application for ungauged basins where no reference maps are available samela et al 2017 used supervised classification methods to identify floodplains for both gauged and ungauged basins by assuming that the threshold determined from training watersheds can be used for other ungauged watersheds in a large region this threshold transferability assumption considers the entire study area as a homogenous region where hydrological and morphological factors in the training and test areas are considered identical watershed characteristics have been widely used in hydrology to convert flood magnitudes from gauged sites to ungauged sites using this concept jafarzadegan and merwade 2017 developed a regression model which used watershed characteristics to predict trh corresponding to the 100 year floodplain for north carolina the predicted trh from the regression model was then used to identify floodplains although the method worked it was not able to satisfactory predict trh for flat and mountainous watersheds because the regression model was site dependent thus limiting its application in areas with different topographic climatic and land use settings in order to overcome this limitation this study proposes a geomorphic model in which the classification method is used to classify watersheds based on watershed characteristics and then a range of trh values are used to map probabilistic floodplains for a given watershed the proposed method also overcomes the threshold transferability assumption of samela et al 2017 by acknowledging spatial heterogeneity in the landscape to relate the spatial trh variability to watershed characteristics considering the generality of the proposed model it is developed and applied for stream networks across the conus 2 dataset and study area when a single trh is used for an entire stream network in a watershed it is assumed that all rivers and tributaries in the watershed have the same hydrological and morphological characteristics the assumption of hydrological and morphological homogeneity and unique trh can generate unreal results with high uncertainties for a large watershed but working with smaller watersheds can provide relatively accurate results in this study a hydrologic unit code 12 huc12 is used as the computational unit for floodplain mapping the united states geological survey usgs has divided the u s using six levels of hydrologic unit codes huc watershed boundaries from the largest huc2 called regions to the smallest huc12 called subwatersheds according to the huc classification the u s is divided into 22 regions and each region is subdivided into around 7600 subwatersheds u s geological survey national hydrography dataset 2014 a total of 216 huc12 units refereed hereafter as just watersheds across the conus are selected fig 1 in this study the watersheds are selected to capture the variability in topography climate land use and geography to develop and test the proposed model additionally training of the classification algorithm requires reference floodplain maps which are available from fema for these watersheds in addition to the 216 training watersheds 145 watersheds are chosen to validate the performance of the proposed model in the first phase the same criteria used for selection of training watersheds is considered for choosing the validation watersheds the second phase of validation is performed by generating probabilistic 100 year floodplain maps for 15 more watersheds that do not have any reference fema maps considering the lack of fema references maps for these 15 sites the predicted flood extents are validated against the results obtained from hec ras modeling at these sites fig 1 depicts the spatial distribution of the watersheds selected for training and validation fig 1 clearly shows uneven distribution of areas between the eastern and western part of the u s due to the absence of reliable reference maps fema for some states such as washington utah idaho and wyoming in the west other datasets including stream networks dems land use and climatic rasters are also used in this study to compute watershed characteristics the sources for these data include the usgs 30 m horizontal resolution national elevation dataset dem usgs s national hydrography dataset nhd for the stream networks the national land cover database nlcd 2011 for land use and worldclim global climate data for the average annual precipitation and temperature flood insurance rate maps firms provided by fema are used as reference for training and validation of the proposed methodology 3 methodology in this section the geomorphic model for probabilistic mapping of 100 year floodplains for conus is introduced the proposed model consists of two classification modules in the first module all study watersheds within the conus are classified into three different classes of trh range based on multiple watershed characteristics using supervised classification in the second module each study watershed represented using a raster grid is classified into flooded and non flooded cells using the probabilistic threshold binary classification ptbc it should be noted that both classifications are applied in the probabilistic mode in the first module the probability that a given watershed belongs to one of the three trh classes is determined by using watershed characteristics derived from dem land use and climate data the second module uses h raster a lookup table and the class probabilities derived from the first module as input to determine the probability of each grid cell within the watershed getting inundated from a 100 year flood event a flowchart of the proposed model is presented in fig 2 and more specific details are provided below 3 1 supervised watershed classification in a supervised classification problem each data point is defined as a pair consisting of attributes and target class the first component attribute is a vector of input features describing the status of data point while the second component target class is a discrete label assigned to a data point as the output of classification the objective is to establish a rule and find a relation between these two components train the classifier to predict the class label of unknown data points in order to find this relation data points with known target classes are chosen as training dataset in this study each watershed represents a data point which has watersheds characteristics as attributes and the target classes include discrete labels associated with three trh ranges from a lookup table to compile the attributes of training dataset multiple watersheds from different geographical locations are selected and several watershed characteristics based on hydrography topography climate and land use are calculated table 1 the main stream slope drainage density and drainage area of a watershed are three features related to the hydrography because the stream network and flow path in the watershed are required to calculate the main stream slope the stream with the highest strahler s stream order is selected as the main stream and then the dem is used to find the slope of this river two climatic features including precipitation and temperature are also calculated to include climate variability topography features including average slope and average elevation of the watershed are determined from the dem another possible effective variable in floodplain mapping is the surface roughness in the form of manning s coefficient the surface roughness is computed by taking the mean of the manning s n value for the watershed as found from different land use types in the watershed kalyanapu et al 2010 percent urban cover and water are calculated from the land use raster the list of potential watershed characteristics in table 1 includes some features that may not be strongly correlated with trh thus to increase the efficiency of the classifier watershed characteristics that are poorly correlated with trh are removed from the analysis two commonly used correlation coefficients namely pearsons s r and kendall s tau are used in this study to test the correlation of watershed characteristics with the trh ranges kendall 1948 pearson 1904 while the pearson s r tests the linear relation kendall s tau is a rank based coefficient that tests non linear monotonic correlations in order to find the correlation coefficients between a vector of distinct values a given watershed characteristic such as average slope and a vector of interval numbers trh range a uniformly distributed random number is generated over the trh range for all watersheds each time then the correlation coefficient between the random trh vector and the watershed characteristic vector is determined this process is repeated 10 000 times and the maximum correlation coefficient is reported as the correlation coefficient between the given watershed characteristic and the trh range to assign the target classes to the training watersheds two important variables namely optimum trh and trh range are calculated for each watershed a fema map is required as a reference map to find these values for a given watershed if one assumes raster h for a watershed all cells with h less than trh are labeled as flood and others will be non flood cells this simple if and else rule is used for floodplain mapping based on trh in general each instance of a binary classification problem is positive or negative which can be renamed with flood and non flood cells in a flood mapping problem the optimum trh is determined by minimizing the total error between predicted and reference maps where the total error is the summation of all misclassified cells flood predicted as non flood and vice versa in order to find the trh ranges two indices namely c and f are used eq 1 and 2 these indices have been widely used in literature to estimate the performance of a predicted flood inundation extents alfieri et al 2014 bates and de roo 2000 horritt and bates 2002 sangwan and merwade 2015 while c index only recognizes the underpredictions in a model f gives more information about both underpredictions and overpredictions in this study trh range is defined as an interval of the trh values where any threshold inside this interval can generate an acceptable flood map with c α and f β eq 3 1 c floodcells predicted correctly flood cells 2 f flood cells predicted correctly flood cells nonflood cells predicted as flood 3 trh i trh range if c trh i α and f trh i β in this study α and β for trh range calculation are 0 8 and 0 5 respectively jafarzadegan and merwade 2017 used α 0 9 and β 0 6 for trh range calculation in north carolina but considering the broader applicability of the proposed work the criteria for α and β is slightly relaxed in this study by using lower values for α and β the lookup table is created by looking into the variability of trh range and optimum trh for the training watersheds this table defines three classes of trh ranges and assumes that any watershed in conus belongs to one of these three classes based on this table the calculated trh range and optimum trh a target class label is assigned to each training watershed the significant watershed characteristics and the assigned class labels of training watersheds are the major inputs for developing a supervised classifier in this study four common classifiers namely logistic regression support vector machine decision tree and random forest are fit to the data the performance of these classifiers is compared using k fold cross validation and the root mean square error rmse the best classifier is selected to perform the supervised classification for the proposed model module1 the selected classifier creates the probability that a watershed belongs to each class as defined by a trh range presented in the lookup table table 2 these class probabilities as well as their corresponding trh range from the lookup table and the h raster are used in ptbc to generate the probabilistic 100 year floodplain maps 3 2 probabilistic threshold binary classifier ptbc ptbc is the second classification module used in the proposed model to generate the 100 year floodplains the essence of this classifier is similar to the threshold binary classifiers used in the literature for floodplain mapping degiorgis et al 2013 2012 those simple threshold classifiers use raster h as input and generate deterministic floodplain maps based on a threshold trh the ptbc proposed in this study uses additional information including the class probabilities from module 1 and a set of trh ranges lookup table instead of a single trh to generate probabilistic floodplain maps in order to employ ptbc and generate the probabilistic 100 year floodplain maps first the trh ranges from lookup tables are discretized into eleven trh values table 3 considering the trh range as a set of trh values between two endpoints as a trh b ten equal increments are defined to discretize the trh range as follows 4 δ b a 10 5 trh range a a δ a 2 δ a 9 δ b for each discretized trh value the raster is classified into flood and non flood areas using a simple conditional function eq 6 in order to use this function raster h for a given watershed should be computed and all cells with corresponding h values less than trh are labeled as flood and others are labeled as non flood cells this process is repeated for all eleven discretized trh values and the mean of flood and non flood cells are calculated eq 7 a weighted average of probabilistic flood maps for each class is calculated to find the final floodplain maps eq 8 the weight of each class defined as the probability of watershed belonging to a given class p m from module 1 is used as input to ptbc 6 f k s i j 1 h i j trh k s 0 h i j trh k s 7 pr s i j k 1 k f k s i j k 8 pr i j s 1 s p s pr s i j in these equations k refers to the total eleven discretized trh values inside a trh range where index k is the counter of these eleven numbers k 1 2 11 s is total number of classes where index s is the counter of classes s 1 2 3 trh k s is the kth discretized trh in class s pr i j is the probability of 100 year flood for a given cell i j p s is the probability that watershed belongs to class s pr s i j is the probability of 100 year flood for given cell i j if the watershed belongs to class s f k s i j is a conditional function for kth discretized trh in class s for a given cell i j h i j is morphologic feature h for a given cell i j to understand the approach consider a hypothetical example where the probability of flooding for two cells a and b within a watershed needs to be determined cell a is near a stream with h 2 and b is away from the stream with h 4 2 first seven watershed characteristics for the watershed are calculated using the dem land use and climate data the watershed characteristics are used as input to the classifier module 1 that has already been trained for the conus assume that module 1 classifier predicts the class probabilities as p s 0 1 0 7 0 2 which means the given watershed most likely belongs to the second class probability 0 7 of lookup table table 2 the discretized trh range for this class is available in table 3 using eq 6 the conditional function f k s is calculated for both points a and b table 4 the probability of flooding for each class pr s is determined by taking the average of conditional functions at each column eq 7 table 5 finally the numbers from table 5 together with output of module one p s 0 1 0 7 0 2 are used in eq 8 to find the probability of 100 year floodplain for cell a and b as follows pr a 0 1 0 273 0 7 1 0 2 1 0 93 pr b 0 1 0 0 7 0 273 0 2 0 909 0 37 these values show that point a with a probability of 0 93 is very likely to get inundated while point b with a 0 37 chance of flooding is less likely point b and other points with a probability of flooding around 0 5 refer to areas with highest uncertainty near the floodplain boundary that need further evaluation to decide whether they will get inundated or not 3 3 validation phase 1 comparison with fema in order to validate the effectiveness and reliability of the geomorphic model probabilistic 100 year floodplain maps are generated for multiple watersheds across the conus and their overlap with fema maps is examined to compare a deterministic map reference map with a probabilistic map predicted map two methods are used in the first method the overestimation flood index ofi and the underestimation flood index ufi are defined using eqs 9 and 10 respectively 9 ufi i 1 n 1 p i n 100 i f 10 ofi j 1 m p j m 100 j nf in these equations f and nf refer to the flood and non flood areas of reference map respectively p i and p j are the probability of flooding for cell i and j obtained from the predicted probabilistic map cell i represents a cell inside the fema floodplains f whereas cell j represents a cell outside of fema floodplains inside the non flood areas nf n and m are the total number of cells inside the f and nf respectively after finding these two indices for each validating watershed the performance of watershed is presented as a point in the ofi ufi space performance of the geomorphic model for estimating the extent of floodplains is also evaluated using the receiver operating characteristic roc graphs which are one of the most commonly used methods for validation of probabilistic classifiers for a given threshold between 0 and 1 the probabilistic map is converted to deterministic one and the rate of true positive rtp and rate of false positive rfp are calculated fawcett 2006 11 rtp true positive instances total positives 12 rfp false positive instances total negatives here positive and negative refer to the flood and non flood cells respectively roc graph is a curve showing the relation of rtp and rfp for different thresholds in order to quantify the performance of such a graph the area under the curve auc is calculated fig 3 for a random classification auc value is 0 5 but in this study watersheds with flood maps having auc more than 0 9 are considered good and flood maps with auc less than 0 8 are considered poor the auc values calculated in similar geomorphic floodplain modeling studies vary from 0 55 to 0 95 manfreda et al 2014 samela et al 2016 therefore regarding the continental extent of this study the selected constraints for auc are considered reasonable 3 4 validation phase 2 comparison with hec ras results many areas in the conus do not have fema maps and creating these maps would require hydrologic and hydraulic modeling to evaluate the reliability of the geomorphic model the probabilistic 100 year floodplain maps are also compared with hec ras generated inundation extents in areas fig 1 where fema maps do not exist the 100 year flow magnitude for some gauged streams are found using flood frequency analysis by fitting the log pearson type 3 distribution to the annual maximum series griffis and stedinger 2007 for ungauged reaches the 100 year flow magnitude is estimated using streamstats which is a web interface developed by united states geological survey usgs to estimate 100 year flood magnitudes at any location along the ungauged reaches u s geological survey 2012 streamstats uses watershed characteristics e g drainage area stream slope basin length average precipitation fraction of urban area in a regression model to estimate the target flood magnitudes the estimated 100 year flow rate as well as the geometry data generated using hec georas ackerman 2005 are used to create the hec ras model and the inundation extent the inundation from hec ras model and the predicted flood extents by the proposed model are compared the same way as fema maps considering that hec ras modeling was performed only on the main reach the comparison is conducted on a single reach instead of the entire network 4 results 4 1 geomorphic model setup a total of 216 watersheds with various climate land use and topography from 43 different geographical regions are selected fig 1 to perform supervised classification significant watershed characteristics from a set of ten are selected by using the correlation coefficients between these characteristics and the trh range as presented in table 6 two land use characteristics namely pu and pw as well as ae show low correlation with trh range and thus are removed from further analysis in order to assign the class labels to these watersheds a lookup table including three trh ranges is created table 2 supervised classification is then performed using four methods namely logistic regression support vector machine decision tree and random forest comparison of these methods using k fold cross validation with k 10 demonstrates that random forest classifier with an accuracy of 0 776 performs the best for the study data followed by logistic regression decision tree and support vector machine with an accuracy of 0 736 0 735 and 0 529 respectively thus random forest classifier is used to classify the watersheds in this study random forest classifier is an ensemble of multiple tree classifiers which combine the decisions of all tree classifiers by weighted or unweighted voting to classify the unknown examples pal 2005 each tree casts a unit vote for the most popular class to classify an input vector breiman 1999 in this study ten sub samples of the training dataset are generated by replacement bootstrapping where the sub sample size is the same as the original dataset then ten tree classifiers are fitted to sub samples finally for each given vector of watershed characteristics the decisions of these ten trees is averaged to find the probability of all three class labels p s some additional feature analysis on the developed random forest indicates that average slope as with the weight of 0 33 is the most significant factor for the classification annual temperature tr and roughness coefficient rc have weights of 0 15 and 0 14 respectively and main stream slope mss and drainage area da have weights of 0 11 and 0 1 annual precipitation ap and drainage density dd have the lowest weights with values of 0 09 and 0 07 in this classifier all these low weight variables have a relatively similar role in the classification after determining the most significant watershed characteristics and the best classifier for watershed classification the geomorphic model is then used for floodplain mapping using ptbc to generate a probabilistic 100 year floodplain map for a given watershed h raster is calculated from a dem and stream network furthermore seven watershed characteristics table 6 are calculated and used as input to the trained random forest classifier the random forest classifier estimates three class probabilities for three class labels the three trh ranges corresponding to the class labels in lookup table table 2 the three probabilities from the random forest result as well as h raster are used as the main inputs to ptbc to generate the final floodplain maps fig 2 4 2 validation phase 1 comparison with fema maps in order to validate the effectiveness of the geomorphic model for floodplain mapping the floodplain of 145 watersheds from various geographical regions is mapped by using the proposed model fig 4 illustrates the position of the validating watersheds in the ofi ufi space the performance of predicted flood extents for each watershed can be evaluated by using the distance of the watershed position in the ofi ufi from the origin the high density of points near the origin in fig 4 shows that the predicted flood extents by the proposed model is satisfactory compared to the fema reference maps in order to quantify the validation results the average ofi and ufi with 95 percent confidence interval is determined the results show that the average of overprediction and underprediction for watersheds in conus vary from 12 6 to 16 and 12 2 to 15 2 respectively in addition to the ofi ufi plot the high frequency of auc values around 1 for all watersheds as shown in fig 5 demonstrates the ability of the geomorphic model to reliably create 100 year floodplain maps based on the results 81 of predicted maps have an auc 0 9 and 14 fall in the range of 0 8 0 9 only 5 of watersheds with auc less than 0 8 have poor estimation of flood extent in order to check the overall fit between the probabilistic maps and the fema maps the flood probability values for all cells in 145 validating watersheds are rounded to one decimal digit numbers 0 0 1 0 2 0 9 1 and their occurrence inside the fema floodplains and fema non flood areas are presented in fig 6 this figure shows that 75 of reference non flood areas include cells with zero probability of flooding moreover around 75 of reference floodplain area includes cells with probability of 0 9 or 1 this proves that almost 75 of entire validating watersheds has a complete fit with fema map the advantage of probabilistic map can be explained by looking at the 25 remaining cells in a deterministic map if 75 of cells predict truly the remaining 25 are definitely the errors however these probabilistic maps show that less than 5 of cells have been predicted wrongly cells with probability of zero inside the flood area or cells with probability of one inside the non flood area and more than 20 of cells show probability of flooding between zero and one these 20 of cells are areas with some level of uncertainty that need further investigation before deciding their flooding status the uncertainty for making a decision will increase as the probabilities move to the middle of the range 0 5 on the contrary recognizing the flood and non flood areas for small or large probabilities would be easier therefore a probabilistic presentation of flood extent helps decision makers to recognize that areas near the boundary of floodplains need further evaluation to decide their flooding status spatial distribution of poorly predicted areas among validating watersheds in conus in fig 7 shows that results are not affected by the location some of the poorly predicted watershed lie next to a good performing watershed as illustrated by examples for new york tennessee and texas in fig 7 the seven watershed characteristics of these poorly predicted watersheds are also compared with those of the training watersheds to examine any pattern in their characteristics fig 8 shows that the watershed characteristics for poorly predicted areas lie randomly with a wide variability without any peculiar pattern further investigation of these poorly predicted watersheds reveal that i their topography is heterogeneous e g two cases in texas and one case in tennessee ii they are located in coastal areas with nested stream networks e g two cases in california and they are located in urban areas with artificial channels and many riverine structures e g one case in indiana 4 3 validation phase 2 comparison with hec ras the floodplain maps are also compared with hec ras generated inundation maps in 15 data scarce regions that do not have fema maps according to the watershed characteristics for these 15 watersheds as presented in table 7 seven watersheds fall in the mid latitude regions and the remaining eight watersheds fall in the flat and mountainous areas with respect to the average slope the corresponding class probabilities generated from random forest classifier for these watersheds are listed in table 8 the ofi and ufi for these areas presented in table 9 and fig 9 calculated by comparing the predicted flood maps with hec ras generated inundation for a single river reach show that ten out of 15 areas have good prediction two reaches show underprediction ufi 30 and three show overprediction ofi 30 the average of ofi and ufi for these fifteen watersheds considering the 95 percent confidence interval are in the range of 7 7 24 8 and 2 9 20 3 respectively the larger confidence interval of results at validation phase 2 compared to phase 1 can be explained by the smaller sample size in phase 2 15 watersheds compared to 145 watersheds used in phase 1 in figs 10 12 the probabilistic 100 year floodplain maps for three watersheds in wyoming south dakota and idaho are presented the results for wyoming and south dakota are an example of well predicted watersheds fifty percent of flood extent is underpredicted for the idaho reach fig 12 because the estimated trh from the geomorphic model is lower than what it should be the idaho reach should belong to class 1 with average slope as 40 42 due to its hilly terrain but the random forest classifier puts the idaho watershed belonging in both class 1 and 2 with probability of 0 6 and 0 4 respectively table 8 this example demonstrates the limitation of the random forest in correct classification of this watershed the performance of random forest can be improved by adding more training data to capture the variation of watershed characteristics and generate a better model fit to data also there should be other factors in addition to the seven selected watershed characteristics affecting the trh which have been neglected in classification these factors can be more dominant in areas such as idaho 5 discussion and conclusions in this study a geomorphic model for probabilistic mapping of 100 year floodplains in conus is proposed by using attributes derived from freely available topography landuse and climate data overall results computed in terms of auc and ufi ofi show that the proposed model provides a relatively reliable and robust alternative to generate probabilistic 100 year floodplain maps for an entire stream network in a huc12 unit the proposed model is scalable to identify floodplains for all stream reaches in the conus by delineating floodplains for each huc12 unit the proposed model is a fast and cost effective method for primary estimation of floodplain areas for an entire stream network in any gauged or ungauged watershed for example for a huc 12 unit used in this study with the combined stream lengths in the range of 50 150 km the proposed approach created the probabilistic floodplain map in 5 10 min using a computer desktop with core i7 3 6 ghz processor and 16 gb memory ram the computing time also included time of downloading dem and nhd stream network for the unit around 2 3 min creating a probabilistic flood inundation map for the same length of stream network using a hydrodynamic model would take hours or days including both model setup and running time depending on the model used in addition to the actual computing time of a conventional hydrodynamic model the lack of reliable data for all tributaries including 100 year flow and the bathymetry data and the high cost of field measurement to derive accurate data pose additional challenges in applying conventional probabilistic floodplain modeling approaches for large areas the validation results illustrated that around 80 of watersheds are predicted well by the proposed approach in comparison to the fema maps it should be noted that fema maps are used as reference only to train and validate the proposed geomorphic approach it is unrealistic to expect an exact overlap between the fema maps and the geomorphic model predicted maps because fema maps are generated by hydrologic and hydraulic models that account for accurate hydrodynamics and geometric details it is also true that the modeling approach used in fema mapping has uncertainties related to 100 year flow estimation model structure and assumptions merwade et al 2008b saksena and merwade 2017 2015 so some of the discrepancies between flood extent predicted by the proposed approach and fema maps could be related to these uncertainties similar arguments can be made about the comparison between the proposed approach and hec ras predicted outputs that show around 67 of satisfactory prediction 20 overprediction and 13 underprediction the higher rate of overprediction compared to underprediction and the lower fitness with hec ras maps compared to fema maps can be explained by two arguments first the geomorphic approach makes prediction for the entire stream network including the tributaries but hec ras maps are created only for a single reach of a river this scale difference is the major reason of overprediction in most of the selected watersheds second the bathymetry data used in hec ras has more uncertainties than ones used by fema in our hec ras modeling approach the bathymetry data is generated using dem and digitizing cross sections on the river however most fema maps include accurate bathymetry using field measurements furthermore the poor results for the extreme case of idaho demonstrates the limitation of random forest classifier in true classification of all watersheds the performance of random forest classifier can be improved by increasing the number of training data to capture more variability in the watershed characteristics while floodplain mapping is traditionally being done using computational models the data and resources need to undertake modeling studies make the task of floodplain mapping difficult in data scarce low income rural areas this study is motivated by the desire to make floodplain maps more accessible in such regions using machine learning techniques however the quantity and quality of training data is critical in developing any methods using machine learning in the proposed approach the use of fema maps which are not true observations and somewhat uncertain products can significantly affect the model performance therefore one of the major limitations of the geomorphic model developed in this study is its dependence on the fema maps accuracy the lack of detailed information in urban areas and exclusion of riverine structures in mapping the floodplains are some limitations of the geomorphic model these details are not easy to incorporate at river network scale detailed hydrodynamic models are more useful for local regions of importance but large scale methods such as the geomorphic method proposed here can be more effective for estimating the flood extents in data scarce regions and rural areas the probabilistic watershed classification by random forest classifier the range of trh values used in the lookup table instead of one certain value and the ptbc module used to convert these uncertain data to a probabilistic map demonstrate that the model structure is the only source of uncertainty considered in the proposed approach the other potential sources of uncertainty in the proposed model are associated with two major inputs topography data dem and the reference maps as a suggestion for future studies the uncertainty in topography data can be incorporated into the proposed model by using several dems the quantification of uncertainty in reference maps is a challenging task because the modeling approach used in creating a flood inundation map has several uncertainties including data sources model structure and its parameters however if the uncertainty in the reference maps is known it can be incorporated by rearranging the optimum trh and the trh range values determined based on reference maps for training watersheds the new values could affect the class labels of training watersheds which will produce new watershed classification results in the first module of the proposed model it should be stressed that the continental scale floodplain mapping for the conus has been performed by different studies recently including sangwan and merwade 2015 wing et al 2017 samela et al 2017 the soil based approach by sangwan and merwade 2015 can generate 100 year floodplain maps for conus but the soil based approach ignores topographic attributes which play an important role in forming floodplains the recent proposed hydrodynamic approach by wing et al 2017 is a significant contribution in continental scale floodplain mapping as it relies on freely available open sourced data for numerical hydrodynamic modeling in such a large scale domain however the potential source of uncertainties in simplifying the channel geometries obtained from a dem without any detailed field measurement and the errors in estimating the flow rate from regional regression equations significantly reduce the model accuracy a detailed comparison of accuracy between some of these related studies is not easy as some of them use different performance measures such as the c and f indices compared to ufi ofi and auc used in this study additionally the computational units or domains for these studies also vary for example this study uses huc12 as one computational unit but other studies use county or climate regions or the whole conus for creating floodplain maps however a simple comparison can be made by considering the fact that our proposed model is developed based on the criteria of c 0 8 and f 0 5 and 80 of watersheds have been predicted well considering the validation results from other studies that have an average c and f of around 0 8 and 0 5 respectively the results from this study are reasonable the major advantage of the proposed model compared to the conventional approach is the fact that one can get an acceptable floodplain map in a data scarce region without investing considerable amount of computational and monetary resources samela et al 2017 proposed a continental scale geomorphic approach similar to the one proposed here for the conus their approach yielded an average auc of 83 3 for the conus with majority of the areas having an auc ranging between 80 and 90 in this study the average auc is 93 3 for the validating watersheds with most areas giving an auc ranging between 90 and 100 the increase in accuracy in our approach is attributed to the consideration of heterogeneity in the topography by using huc 12 for computations compared to huc 2 by samela et al who also assumed constant gfi for the entire huc2 finally the proposed geomorphic method is able to create probabilistic presentation of floodplains which is not possible at such a scale from other related studies the probabilistic presentation of floodplains is more realistic because of the stochastic nature of flood events and the huge uncertainties associated with their predictions while the probabilistic maps do not account for uncertainties related to rainfall and flow predictions they consider the uncertainties in the model structure by assigning a range of trh instead and finding the probability of a watershed belonging to different trh classes in addition the probabilistic maps would be more useful for generating flood risk maps and decision making alfonso et al 2016 the proposed model like any geomorphic method considers topography as the key factor in defining the floodplains in addition the higher impact of average slope derived from dem on trh variability in conus confirms the dominant role of topography in the utilization and success of the proposed model consequently it is expected that the quality of dem including its horizontal resolution and vertical accuracy can highly affect the model results manfreda et al 2011 rexer and hirt 2014 saksena and merwade 2015 sanders 2007 yamazaki et al 2012 in this study the usgs ned was used to generate the floodplain maps for conus because of its higher quality compared to dems provided by shuttle radar topography mission srtm gesch et al 2002 sanders 2007 the availability of higher quality dems such as ned with 1 9 arc second resolution or lidar data in the future will certainly improve the proposed model performance significantly looking into the characteristics of the poorly predicted watersheds it is found that proposed model is not influenced by any particular topographic climatic or land use setting a uniform distribution of poorly predicted watersheds across the conus also shows that the proposed model is not affected by the geographic locations of the watersheds however a closer look into poorly predicted watersheds revealed that watersheds with extreme topographic heterogeneity performed relatively poorly for future studies defining and adding a new morphologic index which explains the level of topographic heterogeneity to the current seven watershed characteristics may improve the performance of the proposed model in such regions in addition mapping the floodplain in coastal and urban areas needs additional considerations because of different parameters affecting the floodplain in these areas therefore it is highly recommended to exclude coastal and urban watershed from the proposed model application and use separate models developed exclusively for these watersheds overall the findings from this study suggest that the approach may be extended to floodplain mapping at the global scale because of the strong dependence of trh on topography and its attributes while good topography data is available in developed nations developing nations rely on globally available dataset such as srtm and aster dem it is known that the accuracy of globally available dems is not as good as the dem used in this study so the proposed approach will require some modifications to account for the lower accuracy of data at the global scale additionally data scarce regions will also not have access to 100 year hazard maps for training and in such cases other resources including the global flood map repositories e g the floodplain maps created in 19 european countries and japan van alphen and passchier 2007 and satellite derived flood inundation information may be used to train and validate the geomorphic model 
7261,floodplain mapping using hydrodynamic models is difficult in data scarce regions additionally using hydrodynamic models to map floodplain over large stream network can be computationally challenging some of these limitations of floodplain mapping using hydrodynamic modeling can be overcome by developing computationally efficient statistical methods to identify floodplains in large and ungauged watersheds using publicly available data this paper proposes a geomorphic model to generate probabilistic 100 year floodplain maps for the conterminous united states conus the proposed model first categorizes the watersheds in the conus into three classes based on the height of the water surface corresponding to the 100 year flood from the streambed next the probability that any watershed in the conus belongs to one of these three classes is computed through supervised classification using watershed characteristics related to topography hydrography land use and climate the result of this classification is then fed into a probabilistic threshold binary classifier ptbc to generate the probabilistic 100 year floodplain maps the supervised classification algorithm is trained by using the 100 year flood insurance rated maps firm from the u s federal emergency management agency fema fema firms are also used to validate the performance of the proposed model in areas not included in the training additionally hec ras model generated flood inundation extents are used to validate the model performance at fifteen sites that lack fema maps validation results show that the probabilistic 100 year floodplain maps generated by proposed model match well with both fema and hec ras generated maps on average the error of predicted flood extents is around 14 across the conus the high accuracy of the validation results shows the reliability of the geomorphic model as an alternative approach for fast and cost effective delineation of 100 year floodplains for the conus keywords floodplain mapping geomorphic model watershed characteristics watershed classification data scarce regions 1 introduction digital elevation models dems play a critical role in flood inundation mapping by providing floodplain topography as input to hydrodynamic models and then enabling the mapping of the floodplain by using the resulting water surface elevations bates and de roo 2000 casas et al 2006 merwade et al 2008a noman et al 2001 tate et al 2002 townsend and walsh 1998 most commonly the hydrodynamic modeling approach is used to create flood hazard maps corresponding to a rare high flood magnitude of 100 year return period or higher although this approach can provide very accurate floodplain maps it is computationally demanding as a result the modeling approach to flood hazard mapping works well for individual streams but its efficiency drops significantly when used to map floodplains over a large stream network cobby et al 2003 although there are ongoing efforts to use hydrodynamic models for large scale floodplain mapping sampson et al 2015 wing et al 2017 the issue related to high computational demand still exists in the recent years geomorphic methods that use topography data in the form of digital elevation model dem and its derivatives such as wetness index and slope have been used to map floodplains geomorphic methods are not only used to delineate the geomorphic floodplain riparian area just above the bank full discharge corresponding to a 1 5 2 year flow but they can be trained using 100 year hazard maps to provide 100 year flood inundation extent while the accurate hydrodynamics resulting from river structures and complex geometry cannot be accounted by the geomorphic methods they provide an optimal solution by providing the required accuracy in large scale floodplain mapping at a much lower cost bates 2004 bradley et al 1996 considering the importance of flood hazard it is important to understand the role of uncertainty and incorporate that information in flood hazard maps the hydrodynamic modeling approach is suitable for accounting various uncertainties and thus lends itself to creating probabilistic floodplain maps merwade et al 2008b conducted a detailed analysis on the potential sources of uncertainty arising in floodplain mapping problems they showed that uncertainty in design flow train datasets and modeling approach are three major components affecting the inundation extents alfonso and tefferi 2015 di baldassarre et al 2010 yan et al 2013 to generate a probabilistic floodplain map a large number of hydrodynamic model configurations corresponding to a distinct combination of uncertain data input and or model parameters are executed to generate an ensemble of flood inundation maps this ensemble is then used to assign the probability of flooding to any given point within the floodplain to get a probabilistic floodplain map aronica et al 2002 domeneghetti et al 2013 neal et al 2013 purvis et al 2008 sarhadi et al 2012 besides providing a robust prediction for flood inundation probabilistic presentation of floodplain areas is also beneficial for decision making and risk analysis alfonso et al 2016 again this process is time consuming and computationally demanding the objective of this paper is to propose a method to avoid this computational burden in the hydrodynamic modeling approach by developing a geomorphic model based probabilistic floodplain mapping approach that relates the flood extent to watershed characteristics wolman 1971 conducted one of the first studies to explore floodplain mapping using alternative approaches in which flood mapping methods were compared by dividing them into several groups including physiographic pedologic vegetation occasional floods regional floods of selected frequency and flood profiles and backwater curves while this study did not focus on the details of any specific method it provided a general insight on these alternative floodplain mapping methods williams et al 2000 suggested a simple method to delineate floodplains by subtracting the dem from an assumed constant water level for the entire stream network the main limitation of this method was the assumption of constant water level and the lack of a reliable method to find the actual water depth in the rivers later a series of methods to identify low lying valleys based on dem were developed dodov and foufoula georgiou 2005 gallant and dowling 2003 mcglynn and seibert 2003 for example gallant and dowling 2003 proposed a multiresolution index to estimate the valley bottoms although distinguishing valley bottoms from hillslopes is a valuable task for hydrologic purposes these areas do not account for a particular flood magnitude or frequency nardi et al 2013 2006 proposed a hydrogeomorphic method for mapping floodplains in which the hydrologic characteristics of a flood event were also incorporated into the modeling process therefore the method was able to map floodplains corresponding to specific flood frequencies in addition to dem methods based on soil information have also been proposed for floodplain mapping sangwan and merwade 2015 some recently developed alternative methods for floodplain mapping are based on supervised and unsupervised classification clustering of data unsupervised methods attempt to group data points into several clusters based on similarity of their attributes a common form of clustering in hydrological problems is termed regionalization where a large heterogeneous area is divided into smaller homogeneous regions based on multiple watershed characteristics shih min et al 2002 rao 2004 rao and srinivas 2006a 2006b razavi and coulibaly 2013 ridolfi et al 2016 watershed characteristics have been widely used as reliable descriptors of hydrologic variables in ungauged basins berger and entekhabi 2001 ganora et al 2009 patton and baker 1976 sankarasubramanian and vogel 2002 sefton and howarth 1998 thomas and benson 1970 specifically several regional regression models have been developed in the past few decades to relate streamflow statistics e g 100 year flood mean annual flow 7 day low flow frequencies with watershed characteristics acreman 1985 crippen and bue 1977 ries 2007 sauer et al 1983 thomas and benson 1970 turnipseed and ries iii 2007 besides regionalization clustering methods can also be used to map flood risk areas in one study papaioannou et al 2014 employed a clustering method to classify a raster into different levels of flood risk areas they used multi criteria evaluation methods to select and find the weights of the most significant factors for clustering selection of proper factors and weights can add huge uncertainties in the results of unsupervised classification methods largely due to the high sensitivity of clustering results to the unknown weight of factors spatial supervised classification methods attempt to find a pattern in the attributes of some labeled data training data and utilize that pattern to classify the unknown data test data these methods have been successfully used in floorplain mapping by dividing a watershed into a grid of cells where each cell can be classified as flood or non flood binary classification an observed or reliable floodplain map is required as a reference to train the classifier and then the trained classifier predicts the class labels of unknown cells de risi et al 2014 degiorgis et al 2013 manfreda et al 2015 2014 samela et al 2016 in order to find the best classifier different morphological features have been proposed including the modified topographic index manfreda et al 2011 manfreda et al 2008 and topographic wetness index de risi et al 2014 degiorgis et al 2012 compared the performance of several single morphologic features and suggested that feature h defined as the difference in elevation between a given cell and the lowest elevation in the nearest stream as represented in a dem nobre et al 2011 rennó et al 2008 plays the most significant role in these methods their results showed that using several features and or more complicated classification methods such as support vector machine is not necessary in these past studies a binary threshold classification is used so that a threshold on the morphologic feature is chosen based on minimizing the error between the reference and predicted flood extents the supervised binary classification based on finding a threshold on morphologic feature h trh is a reliable approach for floodplain mapping over large areas because it is simple fast and accurate in addition it can identify the floodplains associated with a particular flood frequency such as 100 year floods however this method like any supervised classification problem needs some training based on a reference map the reference maps are usually provided by collecting detailed survey data from field measurements and running hydrodynamic models the dependency of this method on the reference map and hydraulic data limits its application for ungauged basins where no reference maps are available samela et al 2017 used supervised classification methods to identify floodplains for both gauged and ungauged basins by assuming that the threshold determined from training watersheds can be used for other ungauged watersheds in a large region this threshold transferability assumption considers the entire study area as a homogenous region where hydrological and morphological factors in the training and test areas are considered identical watershed characteristics have been widely used in hydrology to convert flood magnitudes from gauged sites to ungauged sites using this concept jafarzadegan and merwade 2017 developed a regression model which used watershed characteristics to predict trh corresponding to the 100 year floodplain for north carolina the predicted trh from the regression model was then used to identify floodplains although the method worked it was not able to satisfactory predict trh for flat and mountainous watersheds because the regression model was site dependent thus limiting its application in areas with different topographic climatic and land use settings in order to overcome this limitation this study proposes a geomorphic model in which the classification method is used to classify watersheds based on watershed characteristics and then a range of trh values are used to map probabilistic floodplains for a given watershed the proposed method also overcomes the threshold transferability assumption of samela et al 2017 by acknowledging spatial heterogeneity in the landscape to relate the spatial trh variability to watershed characteristics considering the generality of the proposed model it is developed and applied for stream networks across the conus 2 dataset and study area when a single trh is used for an entire stream network in a watershed it is assumed that all rivers and tributaries in the watershed have the same hydrological and morphological characteristics the assumption of hydrological and morphological homogeneity and unique trh can generate unreal results with high uncertainties for a large watershed but working with smaller watersheds can provide relatively accurate results in this study a hydrologic unit code 12 huc12 is used as the computational unit for floodplain mapping the united states geological survey usgs has divided the u s using six levels of hydrologic unit codes huc watershed boundaries from the largest huc2 called regions to the smallest huc12 called subwatersheds according to the huc classification the u s is divided into 22 regions and each region is subdivided into around 7600 subwatersheds u s geological survey national hydrography dataset 2014 a total of 216 huc12 units refereed hereafter as just watersheds across the conus are selected fig 1 in this study the watersheds are selected to capture the variability in topography climate land use and geography to develop and test the proposed model additionally training of the classification algorithm requires reference floodplain maps which are available from fema for these watersheds in addition to the 216 training watersheds 145 watersheds are chosen to validate the performance of the proposed model in the first phase the same criteria used for selection of training watersheds is considered for choosing the validation watersheds the second phase of validation is performed by generating probabilistic 100 year floodplain maps for 15 more watersheds that do not have any reference fema maps considering the lack of fema references maps for these 15 sites the predicted flood extents are validated against the results obtained from hec ras modeling at these sites fig 1 depicts the spatial distribution of the watersheds selected for training and validation fig 1 clearly shows uneven distribution of areas between the eastern and western part of the u s due to the absence of reliable reference maps fema for some states such as washington utah idaho and wyoming in the west other datasets including stream networks dems land use and climatic rasters are also used in this study to compute watershed characteristics the sources for these data include the usgs 30 m horizontal resolution national elevation dataset dem usgs s national hydrography dataset nhd for the stream networks the national land cover database nlcd 2011 for land use and worldclim global climate data for the average annual precipitation and temperature flood insurance rate maps firms provided by fema are used as reference for training and validation of the proposed methodology 3 methodology in this section the geomorphic model for probabilistic mapping of 100 year floodplains for conus is introduced the proposed model consists of two classification modules in the first module all study watersheds within the conus are classified into three different classes of trh range based on multiple watershed characteristics using supervised classification in the second module each study watershed represented using a raster grid is classified into flooded and non flooded cells using the probabilistic threshold binary classification ptbc it should be noted that both classifications are applied in the probabilistic mode in the first module the probability that a given watershed belongs to one of the three trh classes is determined by using watershed characteristics derived from dem land use and climate data the second module uses h raster a lookup table and the class probabilities derived from the first module as input to determine the probability of each grid cell within the watershed getting inundated from a 100 year flood event a flowchart of the proposed model is presented in fig 2 and more specific details are provided below 3 1 supervised watershed classification in a supervised classification problem each data point is defined as a pair consisting of attributes and target class the first component attribute is a vector of input features describing the status of data point while the second component target class is a discrete label assigned to a data point as the output of classification the objective is to establish a rule and find a relation between these two components train the classifier to predict the class label of unknown data points in order to find this relation data points with known target classes are chosen as training dataset in this study each watershed represents a data point which has watersheds characteristics as attributes and the target classes include discrete labels associated with three trh ranges from a lookup table to compile the attributes of training dataset multiple watersheds from different geographical locations are selected and several watershed characteristics based on hydrography topography climate and land use are calculated table 1 the main stream slope drainage density and drainage area of a watershed are three features related to the hydrography because the stream network and flow path in the watershed are required to calculate the main stream slope the stream with the highest strahler s stream order is selected as the main stream and then the dem is used to find the slope of this river two climatic features including precipitation and temperature are also calculated to include climate variability topography features including average slope and average elevation of the watershed are determined from the dem another possible effective variable in floodplain mapping is the surface roughness in the form of manning s coefficient the surface roughness is computed by taking the mean of the manning s n value for the watershed as found from different land use types in the watershed kalyanapu et al 2010 percent urban cover and water are calculated from the land use raster the list of potential watershed characteristics in table 1 includes some features that may not be strongly correlated with trh thus to increase the efficiency of the classifier watershed characteristics that are poorly correlated with trh are removed from the analysis two commonly used correlation coefficients namely pearsons s r and kendall s tau are used in this study to test the correlation of watershed characteristics with the trh ranges kendall 1948 pearson 1904 while the pearson s r tests the linear relation kendall s tau is a rank based coefficient that tests non linear monotonic correlations in order to find the correlation coefficients between a vector of distinct values a given watershed characteristic such as average slope and a vector of interval numbers trh range a uniformly distributed random number is generated over the trh range for all watersheds each time then the correlation coefficient between the random trh vector and the watershed characteristic vector is determined this process is repeated 10 000 times and the maximum correlation coefficient is reported as the correlation coefficient between the given watershed characteristic and the trh range to assign the target classes to the training watersheds two important variables namely optimum trh and trh range are calculated for each watershed a fema map is required as a reference map to find these values for a given watershed if one assumes raster h for a watershed all cells with h less than trh are labeled as flood and others will be non flood cells this simple if and else rule is used for floodplain mapping based on trh in general each instance of a binary classification problem is positive or negative which can be renamed with flood and non flood cells in a flood mapping problem the optimum trh is determined by minimizing the total error between predicted and reference maps where the total error is the summation of all misclassified cells flood predicted as non flood and vice versa in order to find the trh ranges two indices namely c and f are used eq 1 and 2 these indices have been widely used in literature to estimate the performance of a predicted flood inundation extents alfieri et al 2014 bates and de roo 2000 horritt and bates 2002 sangwan and merwade 2015 while c index only recognizes the underpredictions in a model f gives more information about both underpredictions and overpredictions in this study trh range is defined as an interval of the trh values where any threshold inside this interval can generate an acceptable flood map with c α and f β eq 3 1 c floodcells predicted correctly flood cells 2 f flood cells predicted correctly flood cells nonflood cells predicted as flood 3 trh i trh range if c trh i α and f trh i β in this study α and β for trh range calculation are 0 8 and 0 5 respectively jafarzadegan and merwade 2017 used α 0 9 and β 0 6 for trh range calculation in north carolina but considering the broader applicability of the proposed work the criteria for α and β is slightly relaxed in this study by using lower values for α and β the lookup table is created by looking into the variability of trh range and optimum trh for the training watersheds this table defines three classes of trh ranges and assumes that any watershed in conus belongs to one of these three classes based on this table the calculated trh range and optimum trh a target class label is assigned to each training watershed the significant watershed characteristics and the assigned class labels of training watersheds are the major inputs for developing a supervised classifier in this study four common classifiers namely logistic regression support vector machine decision tree and random forest are fit to the data the performance of these classifiers is compared using k fold cross validation and the root mean square error rmse the best classifier is selected to perform the supervised classification for the proposed model module1 the selected classifier creates the probability that a watershed belongs to each class as defined by a trh range presented in the lookup table table 2 these class probabilities as well as their corresponding trh range from the lookup table and the h raster are used in ptbc to generate the probabilistic 100 year floodplain maps 3 2 probabilistic threshold binary classifier ptbc ptbc is the second classification module used in the proposed model to generate the 100 year floodplains the essence of this classifier is similar to the threshold binary classifiers used in the literature for floodplain mapping degiorgis et al 2013 2012 those simple threshold classifiers use raster h as input and generate deterministic floodplain maps based on a threshold trh the ptbc proposed in this study uses additional information including the class probabilities from module 1 and a set of trh ranges lookup table instead of a single trh to generate probabilistic floodplain maps in order to employ ptbc and generate the probabilistic 100 year floodplain maps first the trh ranges from lookup tables are discretized into eleven trh values table 3 considering the trh range as a set of trh values between two endpoints as a trh b ten equal increments are defined to discretize the trh range as follows 4 δ b a 10 5 trh range a a δ a 2 δ a 9 δ b for each discretized trh value the raster is classified into flood and non flood areas using a simple conditional function eq 6 in order to use this function raster h for a given watershed should be computed and all cells with corresponding h values less than trh are labeled as flood and others are labeled as non flood cells this process is repeated for all eleven discretized trh values and the mean of flood and non flood cells are calculated eq 7 a weighted average of probabilistic flood maps for each class is calculated to find the final floodplain maps eq 8 the weight of each class defined as the probability of watershed belonging to a given class p m from module 1 is used as input to ptbc 6 f k s i j 1 h i j trh k s 0 h i j trh k s 7 pr s i j k 1 k f k s i j k 8 pr i j s 1 s p s pr s i j in these equations k refers to the total eleven discretized trh values inside a trh range where index k is the counter of these eleven numbers k 1 2 11 s is total number of classes where index s is the counter of classes s 1 2 3 trh k s is the kth discretized trh in class s pr i j is the probability of 100 year flood for a given cell i j p s is the probability that watershed belongs to class s pr s i j is the probability of 100 year flood for given cell i j if the watershed belongs to class s f k s i j is a conditional function for kth discretized trh in class s for a given cell i j h i j is morphologic feature h for a given cell i j to understand the approach consider a hypothetical example where the probability of flooding for two cells a and b within a watershed needs to be determined cell a is near a stream with h 2 and b is away from the stream with h 4 2 first seven watershed characteristics for the watershed are calculated using the dem land use and climate data the watershed characteristics are used as input to the classifier module 1 that has already been trained for the conus assume that module 1 classifier predicts the class probabilities as p s 0 1 0 7 0 2 which means the given watershed most likely belongs to the second class probability 0 7 of lookup table table 2 the discretized trh range for this class is available in table 3 using eq 6 the conditional function f k s is calculated for both points a and b table 4 the probability of flooding for each class pr s is determined by taking the average of conditional functions at each column eq 7 table 5 finally the numbers from table 5 together with output of module one p s 0 1 0 7 0 2 are used in eq 8 to find the probability of 100 year floodplain for cell a and b as follows pr a 0 1 0 273 0 7 1 0 2 1 0 93 pr b 0 1 0 0 7 0 273 0 2 0 909 0 37 these values show that point a with a probability of 0 93 is very likely to get inundated while point b with a 0 37 chance of flooding is less likely point b and other points with a probability of flooding around 0 5 refer to areas with highest uncertainty near the floodplain boundary that need further evaluation to decide whether they will get inundated or not 3 3 validation phase 1 comparison with fema in order to validate the effectiveness and reliability of the geomorphic model probabilistic 100 year floodplain maps are generated for multiple watersheds across the conus and their overlap with fema maps is examined to compare a deterministic map reference map with a probabilistic map predicted map two methods are used in the first method the overestimation flood index ofi and the underestimation flood index ufi are defined using eqs 9 and 10 respectively 9 ufi i 1 n 1 p i n 100 i f 10 ofi j 1 m p j m 100 j nf in these equations f and nf refer to the flood and non flood areas of reference map respectively p i and p j are the probability of flooding for cell i and j obtained from the predicted probabilistic map cell i represents a cell inside the fema floodplains f whereas cell j represents a cell outside of fema floodplains inside the non flood areas nf n and m are the total number of cells inside the f and nf respectively after finding these two indices for each validating watershed the performance of watershed is presented as a point in the ofi ufi space performance of the geomorphic model for estimating the extent of floodplains is also evaluated using the receiver operating characteristic roc graphs which are one of the most commonly used methods for validation of probabilistic classifiers for a given threshold between 0 and 1 the probabilistic map is converted to deterministic one and the rate of true positive rtp and rate of false positive rfp are calculated fawcett 2006 11 rtp true positive instances total positives 12 rfp false positive instances total negatives here positive and negative refer to the flood and non flood cells respectively roc graph is a curve showing the relation of rtp and rfp for different thresholds in order to quantify the performance of such a graph the area under the curve auc is calculated fig 3 for a random classification auc value is 0 5 but in this study watersheds with flood maps having auc more than 0 9 are considered good and flood maps with auc less than 0 8 are considered poor the auc values calculated in similar geomorphic floodplain modeling studies vary from 0 55 to 0 95 manfreda et al 2014 samela et al 2016 therefore regarding the continental extent of this study the selected constraints for auc are considered reasonable 3 4 validation phase 2 comparison with hec ras results many areas in the conus do not have fema maps and creating these maps would require hydrologic and hydraulic modeling to evaluate the reliability of the geomorphic model the probabilistic 100 year floodplain maps are also compared with hec ras generated inundation extents in areas fig 1 where fema maps do not exist the 100 year flow magnitude for some gauged streams are found using flood frequency analysis by fitting the log pearson type 3 distribution to the annual maximum series griffis and stedinger 2007 for ungauged reaches the 100 year flow magnitude is estimated using streamstats which is a web interface developed by united states geological survey usgs to estimate 100 year flood magnitudes at any location along the ungauged reaches u s geological survey 2012 streamstats uses watershed characteristics e g drainage area stream slope basin length average precipitation fraction of urban area in a regression model to estimate the target flood magnitudes the estimated 100 year flow rate as well as the geometry data generated using hec georas ackerman 2005 are used to create the hec ras model and the inundation extent the inundation from hec ras model and the predicted flood extents by the proposed model are compared the same way as fema maps considering that hec ras modeling was performed only on the main reach the comparison is conducted on a single reach instead of the entire network 4 results 4 1 geomorphic model setup a total of 216 watersheds with various climate land use and topography from 43 different geographical regions are selected fig 1 to perform supervised classification significant watershed characteristics from a set of ten are selected by using the correlation coefficients between these characteristics and the trh range as presented in table 6 two land use characteristics namely pu and pw as well as ae show low correlation with trh range and thus are removed from further analysis in order to assign the class labels to these watersheds a lookup table including three trh ranges is created table 2 supervised classification is then performed using four methods namely logistic regression support vector machine decision tree and random forest comparison of these methods using k fold cross validation with k 10 demonstrates that random forest classifier with an accuracy of 0 776 performs the best for the study data followed by logistic regression decision tree and support vector machine with an accuracy of 0 736 0 735 and 0 529 respectively thus random forest classifier is used to classify the watersheds in this study random forest classifier is an ensemble of multiple tree classifiers which combine the decisions of all tree classifiers by weighted or unweighted voting to classify the unknown examples pal 2005 each tree casts a unit vote for the most popular class to classify an input vector breiman 1999 in this study ten sub samples of the training dataset are generated by replacement bootstrapping where the sub sample size is the same as the original dataset then ten tree classifiers are fitted to sub samples finally for each given vector of watershed characteristics the decisions of these ten trees is averaged to find the probability of all three class labels p s some additional feature analysis on the developed random forest indicates that average slope as with the weight of 0 33 is the most significant factor for the classification annual temperature tr and roughness coefficient rc have weights of 0 15 and 0 14 respectively and main stream slope mss and drainage area da have weights of 0 11 and 0 1 annual precipitation ap and drainage density dd have the lowest weights with values of 0 09 and 0 07 in this classifier all these low weight variables have a relatively similar role in the classification after determining the most significant watershed characteristics and the best classifier for watershed classification the geomorphic model is then used for floodplain mapping using ptbc to generate a probabilistic 100 year floodplain map for a given watershed h raster is calculated from a dem and stream network furthermore seven watershed characteristics table 6 are calculated and used as input to the trained random forest classifier the random forest classifier estimates three class probabilities for three class labels the three trh ranges corresponding to the class labels in lookup table table 2 the three probabilities from the random forest result as well as h raster are used as the main inputs to ptbc to generate the final floodplain maps fig 2 4 2 validation phase 1 comparison with fema maps in order to validate the effectiveness of the geomorphic model for floodplain mapping the floodplain of 145 watersheds from various geographical regions is mapped by using the proposed model fig 4 illustrates the position of the validating watersheds in the ofi ufi space the performance of predicted flood extents for each watershed can be evaluated by using the distance of the watershed position in the ofi ufi from the origin the high density of points near the origin in fig 4 shows that the predicted flood extents by the proposed model is satisfactory compared to the fema reference maps in order to quantify the validation results the average ofi and ufi with 95 percent confidence interval is determined the results show that the average of overprediction and underprediction for watersheds in conus vary from 12 6 to 16 and 12 2 to 15 2 respectively in addition to the ofi ufi plot the high frequency of auc values around 1 for all watersheds as shown in fig 5 demonstrates the ability of the geomorphic model to reliably create 100 year floodplain maps based on the results 81 of predicted maps have an auc 0 9 and 14 fall in the range of 0 8 0 9 only 5 of watersheds with auc less than 0 8 have poor estimation of flood extent in order to check the overall fit between the probabilistic maps and the fema maps the flood probability values for all cells in 145 validating watersheds are rounded to one decimal digit numbers 0 0 1 0 2 0 9 1 and their occurrence inside the fema floodplains and fema non flood areas are presented in fig 6 this figure shows that 75 of reference non flood areas include cells with zero probability of flooding moreover around 75 of reference floodplain area includes cells with probability of 0 9 or 1 this proves that almost 75 of entire validating watersheds has a complete fit with fema map the advantage of probabilistic map can be explained by looking at the 25 remaining cells in a deterministic map if 75 of cells predict truly the remaining 25 are definitely the errors however these probabilistic maps show that less than 5 of cells have been predicted wrongly cells with probability of zero inside the flood area or cells with probability of one inside the non flood area and more than 20 of cells show probability of flooding between zero and one these 20 of cells are areas with some level of uncertainty that need further investigation before deciding their flooding status the uncertainty for making a decision will increase as the probabilities move to the middle of the range 0 5 on the contrary recognizing the flood and non flood areas for small or large probabilities would be easier therefore a probabilistic presentation of flood extent helps decision makers to recognize that areas near the boundary of floodplains need further evaluation to decide their flooding status spatial distribution of poorly predicted areas among validating watersheds in conus in fig 7 shows that results are not affected by the location some of the poorly predicted watershed lie next to a good performing watershed as illustrated by examples for new york tennessee and texas in fig 7 the seven watershed characteristics of these poorly predicted watersheds are also compared with those of the training watersheds to examine any pattern in their characteristics fig 8 shows that the watershed characteristics for poorly predicted areas lie randomly with a wide variability without any peculiar pattern further investigation of these poorly predicted watersheds reveal that i their topography is heterogeneous e g two cases in texas and one case in tennessee ii they are located in coastal areas with nested stream networks e g two cases in california and they are located in urban areas with artificial channels and many riverine structures e g one case in indiana 4 3 validation phase 2 comparison with hec ras the floodplain maps are also compared with hec ras generated inundation maps in 15 data scarce regions that do not have fema maps according to the watershed characteristics for these 15 watersheds as presented in table 7 seven watersheds fall in the mid latitude regions and the remaining eight watersheds fall in the flat and mountainous areas with respect to the average slope the corresponding class probabilities generated from random forest classifier for these watersheds are listed in table 8 the ofi and ufi for these areas presented in table 9 and fig 9 calculated by comparing the predicted flood maps with hec ras generated inundation for a single river reach show that ten out of 15 areas have good prediction two reaches show underprediction ufi 30 and three show overprediction ofi 30 the average of ofi and ufi for these fifteen watersheds considering the 95 percent confidence interval are in the range of 7 7 24 8 and 2 9 20 3 respectively the larger confidence interval of results at validation phase 2 compared to phase 1 can be explained by the smaller sample size in phase 2 15 watersheds compared to 145 watersheds used in phase 1 in figs 10 12 the probabilistic 100 year floodplain maps for three watersheds in wyoming south dakota and idaho are presented the results for wyoming and south dakota are an example of well predicted watersheds fifty percent of flood extent is underpredicted for the idaho reach fig 12 because the estimated trh from the geomorphic model is lower than what it should be the idaho reach should belong to class 1 with average slope as 40 42 due to its hilly terrain but the random forest classifier puts the idaho watershed belonging in both class 1 and 2 with probability of 0 6 and 0 4 respectively table 8 this example demonstrates the limitation of the random forest in correct classification of this watershed the performance of random forest can be improved by adding more training data to capture the variation of watershed characteristics and generate a better model fit to data also there should be other factors in addition to the seven selected watershed characteristics affecting the trh which have been neglected in classification these factors can be more dominant in areas such as idaho 5 discussion and conclusions in this study a geomorphic model for probabilistic mapping of 100 year floodplains in conus is proposed by using attributes derived from freely available topography landuse and climate data overall results computed in terms of auc and ufi ofi show that the proposed model provides a relatively reliable and robust alternative to generate probabilistic 100 year floodplain maps for an entire stream network in a huc12 unit the proposed model is scalable to identify floodplains for all stream reaches in the conus by delineating floodplains for each huc12 unit the proposed model is a fast and cost effective method for primary estimation of floodplain areas for an entire stream network in any gauged or ungauged watershed for example for a huc 12 unit used in this study with the combined stream lengths in the range of 50 150 km the proposed approach created the probabilistic floodplain map in 5 10 min using a computer desktop with core i7 3 6 ghz processor and 16 gb memory ram the computing time also included time of downloading dem and nhd stream network for the unit around 2 3 min creating a probabilistic flood inundation map for the same length of stream network using a hydrodynamic model would take hours or days including both model setup and running time depending on the model used in addition to the actual computing time of a conventional hydrodynamic model the lack of reliable data for all tributaries including 100 year flow and the bathymetry data and the high cost of field measurement to derive accurate data pose additional challenges in applying conventional probabilistic floodplain modeling approaches for large areas the validation results illustrated that around 80 of watersheds are predicted well by the proposed approach in comparison to the fema maps it should be noted that fema maps are used as reference only to train and validate the proposed geomorphic approach it is unrealistic to expect an exact overlap between the fema maps and the geomorphic model predicted maps because fema maps are generated by hydrologic and hydraulic models that account for accurate hydrodynamics and geometric details it is also true that the modeling approach used in fema mapping has uncertainties related to 100 year flow estimation model structure and assumptions merwade et al 2008b saksena and merwade 2017 2015 so some of the discrepancies between flood extent predicted by the proposed approach and fema maps could be related to these uncertainties similar arguments can be made about the comparison between the proposed approach and hec ras predicted outputs that show around 67 of satisfactory prediction 20 overprediction and 13 underprediction the higher rate of overprediction compared to underprediction and the lower fitness with hec ras maps compared to fema maps can be explained by two arguments first the geomorphic approach makes prediction for the entire stream network including the tributaries but hec ras maps are created only for a single reach of a river this scale difference is the major reason of overprediction in most of the selected watersheds second the bathymetry data used in hec ras has more uncertainties than ones used by fema in our hec ras modeling approach the bathymetry data is generated using dem and digitizing cross sections on the river however most fema maps include accurate bathymetry using field measurements furthermore the poor results for the extreme case of idaho demonstrates the limitation of random forest classifier in true classification of all watersheds the performance of random forest classifier can be improved by increasing the number of training data to capture more variability in the watershed characteristics while floodplain mapping is traditionally being done using computational models the data and resources need to undertake modeling studies make the task of floodplain mapping difficult in data scarce low income rural areas this study is motivated by the desire to make floodplain maps more accessible in such regions using machine learning techniques however the quantity and quality of training data is critical in developing any methods using machine learning in the proposed approach the use of fema maps which are not true observations and somewhat uncertain products can significantly affect the model performance therefore one of the major limitations of the geomorphic model developed in this study is its dependence on the fema maps accuracy the lack of detailed information in urban areas and exclusion of riverine structures in mapping the floodplains are some limitations of the geomorphic model these details are not easy to incorporate at river network scale detailed hydrodynamic models are more useful for local regions of importance but large scale methods such as the geomorphic method proposed here can be more effective for estimating the flood extents in data scarce regions and rural areas the probabilistic watershed classification by random forest classifier the range of trh values used in the lookup table instead of one certain value and the ptbc module used to convert these uncertain data to a probabilistic map demonstrate that the model structure is the only source of uncertainty considered in the proposed approach the other potential sources of uncertainty in the proposed model are associated with two major inputs topography data dem and the reference maps as a suggestion for future studies the uncertainty in topography data can be incorporated into the proposed model by using several dems the quantification of uncertainty in reference maps is a challenging task because the modeling approach used in creating a flood inundation map has several uncertainties including data sources model structure and its parameters however if the uncertainty in the reference maps is known it can be incorporated by rearranging the optimum trh and the trh range values determined based on reference maps for training watersheds the new values could affect the class labels of training watersheds which will produce new watershed classification results in the first module of the proposed model it should be stressed that the continental scale floodplain mapping for the conus has been performed by different studies recently including sangwan and merwade 2015 wing et al 2017 samela et al 2017 the soil based approach by sangwan and merwade 2015 can generate 100 year floodplain maps for conus but the soil based approach ignores topographic attributes which play an important role in forming floodplains the recent proposed hydrodynamic approach by wing et al 2017 is a significant contribution in continental scale floodplain mapping as it relies on freely available open sourced data for numerical hydrodynamic modeling in such a large scale domain however the potential source of uncertainties in simplifying the channel geometries obtained from a dem without any detailed field measurement and the errors in estimating the flow rate from regional regression equations significantly reduce the model accuracy a detailed comparison of accuracy between some of these related studies is not easy as some of them use different performance measures such as the c and f indices compared to ufi ofi and auc used in this study additionally the computational units or domains for these studies also vary for example this study uses huc12 as one computational unit but other studies use county or climate regions or the whole conus for creating floodplain maps however a simple comparison can be made by considering the fact that our proposed model is developed based on the criteria of c 0 8 and f 0 5 and 80 of watersheds have been predicted well considering the validation results from other studies that have an average c and f of around 0 8 and 0 5 respectively the results from this study are reasonable the major advantage of the proposed model compared to the conventional approach is the fact that one can get an acceptable floodplain map in a data scarce region without investing considerable amount of computational and monetary resources samela et al 2017 proposed a continental scale geomorphic approach similar to the one proposed here for the conus their approach yielded an average auc of 83 3 for the conus with majority of the areas having an auc ranging between 80 and 90 in this study the average auc is 93 3 for the validating watersheds with most areas giving an auc ranging between 90 and 100 the increase in accuracy in our approach is attributed to the consideration of heterogeneity in the topography by using huc 12 for computations compared to huc 2 by samela et al who also assumed constant gfi for the entire huc2 finally the proposed geomorphic method is able to create probabilistic presentation of floodplains which is not possible at such a scale from other related studies the probabilistic presentation of floodplains is more realistic because of the stochastic nature of flood events and the huge uncertainties associated with their predictions while the probabilistic maps do not account for uncertainties related to rainfall and flow predictions they consider the uncertainties in the model structure by assigning a range of trh instead and finding the probability of a watershed belonging to different trh classes in addition the probabilistic maps would be more useful for generating flood risk maps and decision making alfonso et al 2016 the proposed model like any geomorphic method considers topography as the key factor in defining the floodplains in addition the higher impact of average slope derived from dem on trh variability in conus confirms the dominant role of topography in the utilization and success of the proposed model consequently it is expected that the quality of dem including its horizontal resolution and vertical accuracy can highly affect the model results manfreda et al 2011 rexer and hirt 2014 saksena and merwade 2015 sanders 2007 yamazaki et al 2012 in this study the usgs ned was used to generate the floodplain maps for conus because of its higher quality compared to dems provided by shuttle radar topography mission srtm gesch et al 2002 sanders 2007 the availability of higher quality dems such as ned with 1 9 arc second resolution or lidar data in the future will certainly improve the proposed model performance significantly looking into the characteristics of the poorly predicted watersheds it is found that proposed model is not influenced by any particular topographic climatic or land use setting a uniform distribution of poorly predicted watersheds across the conus also shows that the proposed model is not affected by the geographic locations of the watersheds however a closer look into poorly predicted watersheds revealed that watersheds with extreme topographic heterogeneity performed relatively poorly for future studies defining and adding a new morphologic index which explains the level of topographic heterogeneity to the current seven watershed characteristics may improve the performance of the proposed model in such regions in addition mapping the floodplain in coastal and urban areas needs additional considerations because of different parameters affecting the floodplain in these areas therefore it is highly recommended to exclude coastal and urban watershed from the proposed model application and use separate models developed exclusively for these watersheds overall the findings from this study suggest that the approach may be extended to floodplain mapping at the global scale because of the strong dependence of trh on topography and its attributes while good topography data is available in developed nations developing nations rely on globally available dataset such as srtm and aster dem it is known that the accuracy of globally available dems is not as good as the dem used in this study so the proposed approach will require some modifications to account for the lower accuracy of data at the global scale additionally data scarce regions will also not have access to 100 year hazard maps for training and in such cases other resources including the global flood map repositories e g the floodplain maps created in 19 european countries and japan van alphen and passchier 2007 and satellite derived flood inundation information may be used to train and validate the geomorphic model 
7262,lakes provide enormous economic recreational and aesthetic benefits to citizens these ecosystem services may be adversely impacted by climate change in the twin cities metropolitan area of minnesota usa many lakes have been at historic low levels and water augmentation strategies have been proposed to alleviate the problem white bear lake wbl is a notable example its water level declined 1 5 m during 2003 2013 for reasons that are not fully understood this study examined current past and future lake evaporation to better understand how climate will impact the water balance of lakes within this region evaporation from wbl was measured from july 2014 to february 2017 using two eddy covariance ec systems to provide better constraints on the water budget and to investigate the impact of evaporation on lake level the estimated annual evaporation losses for years 2014 through 2016 were 559 22 mm 779 81 mm and 766 11 mm respectively the higher evaporation in 2015 and 2016 was caused by the combined effects of larger average daily evaporation and a longer ice free season the ec measurements were used to tune the community land model 4 lake ice snow and sediment simulator clm4 lisss to estimate lake evaporation over the period 1979 2016 retrospective analyses indicate that wbl evaporation increased during this time by about 3 8 mm year 1 which was driven by increased wind speed and lake surface vapor pressure gradient using a business as usual greenhouse gas emission scenario rcp8 5 lake evaporation was modeled forward in time from 2017 to 2100 annual evaporation is expected to increase by 1 4 mm year 1 over this century largely driven by lengthening ice free periods these changes in ice phenology and evaporation will have important implications for the regional water balance and water management and water augmentation strategies that are being proposed for these metropolitan lakes keywords evaporation lake eddy covariance modeling ice phenology water level 1 introduction the physical aspects of temperate closed basin lakes such as water level water temperature and ice phenology are highly sensitive to variations in climate adrian et al 2009 schindler 2009 since their contributing watersheds are often relatively small streamflow is typically a minor component of their water balance which is primarily controlled by precipitation evaporation and groundwater exchange almendinger 1990 winter 1995 short term water level change of closed basin lakes is most influenced by changes in precipitation and evaporation van der kamp et al 2008 while the interaction with groundwater tends to impact lake water level at longer time scales and acts to dampen seasonal and inter annual lake level changes kirillin et al 2013 in general the water levels of closed basin lakes are subject to larger variations than flow through lakes and in some cases water level can vary several meters within a decade e g almendinger 1990 ayenew and becht 2008 attribution of the cause s of such changes is challenging since the only data typically available are precipitation records evaporation is the major sink loss term in the lake water budget of temperate closed basin lakes better constraints on lake evaporation are needed to improve our understanding of present past and future changes in water levels key results of relevant studies on lake evaporation and water budgets for closed basin lakes are summarized in table 1 in most of these studies lake evaporation has been estimated using indirect methods such as the penman or energy balance methods e g lenters et al 2005 shanahan et al 2007 direct measurement of lake evaporation using the eddy covariance approach has been applied in relatively few studies it has been used on several large flow through lakes e g blanken et al 2000 blanken et al 2011 wang et al 2014 but only a few closed basin lakes e g li et al 2016 wang et al 2017 eddy covariance has relatively few theoretical assumptions and can reveal detailed physical characteristics of lake evaporation at high temporal resolution i e hourly providing important information needed for the development and optimization of physical models for the broader study and estimation of lake evaporation e g granger and hedstrom 2011 hu et al 2017 lake evaporation is influenced by atmospheric and lake surface processes acting at different time scales at hourly to daily scales lake evaporation rate is highly correlated to wind speed over the lake and vapor pressure difference between the lake surface and the atmosphere blanken et al 2000 wang et al 2017 the mass transfer method uses these two key variables to estimate lake evaporation singh and xu 1997 at daily to weekly scales evaporation is influenced by synoptic scale patterns modulated by the significant thermal lag that results from the high heat capacity of water for example cold fronts bringing cold and dry air over a warm lake in fall can significantly enhance evaporation blanken et al 2000 spence et al 2013 seasonal variation in evaporation is mainly forced by changes in net radiation and the air lake temperature gradient lenters et al 2005 annual evaporation from temperate closed basin lakes is particularly sensitive to lake ice phenology typically a longer ice free period can potentially increase the integrated amount of seasonal evaporation rouse et al 2008 however observation over large lakes such as lake superior and lake michigan indicated that temperate lake evaporation is not always positively correlated with ice free duration blanken et al 2011 gronewold et al 2015 for instance a cold and dry fall season over lake superior can significantly enhance the shoulder season evaporation cause a large annual total evaporation and induce a short ice free season with early ice formation while a mild winter with a late ice formation may have less cumulative evaporation spence et al 2013 the extent to which these observations apply to much smaller lakes is unclear and needs to be addressed lake evaporation rates are also influenced by physical properties such as lake geometry i e shape depth surface area and water transparency e g subin et al 2012 deng et al 2013 lake depth is an important factor that influences the change in water heat storage and the seasonal phase lag between energy input and evaporation rate for example the monthly mean evaporation of a subtropical large shallow lake was closely coupled with seasonal variations in net radiation lake taihu in china average depth 2 m wang et al 2014 whereas deep lakes maximum depth 10 m show a 2 to 3 month phase lag between the net radiation and evaporation regardless of the area of the lake e g li et al 2016 blanken et al 2000 the transparency or turbidity which is related to the chemical composition and plankton population distribution within a lake affects the penetration depth of shortwave radiation and therefore the available energy for evaporation deng et al 2013 wang et al 2014 changes in climate including air temperature humidity wind speed and available energy can have an important influence on long term trends in lake evaporation the globally average wind speed decreased by 0 7 m s 1 from 1950 to 2000 which coincided with an observed decrease in global pan evaporation and calculated crop reference evapotranspiration mcvicar et al 2012 the global decrease in surface solar radiation i e global dimming between the 1950s and 1980s could also have potentially decreased lake evaporation by reducing the available energy the more recent brightening trend over much of the globe could have acted to enhance evaporation wild 2009 further the global mean lake summer surface water temperature was found to increase by 0 34 c decade 1 between 1985 and 2009 correlated with the global brightening and air temperature trends o reilly et al 2015 over the period 1973 to 2003 observed specific humidity over land increased by 0 11 g kg 1 per decade while the relative humidity remained quasi constant willett et al 2008 given the observed increases in air temperature and lake surface temperature this implies that the vapor pressure gradient at the lake surface has also increased the increase in vapor pressure gradient for the most rapidly warming lakes around the globe was estimated to be 15 20 from 1985 to 2009 friedrich et al 2018 these trends in key climate variables have likely interacted and influenced lake evaporation resulting in significant inter annual variability in lake evaporation and fluctuations in lake water level given that multiple trends in atmospheric variables affect the long term trend of lake evaporation understanding these trends and projecting future changes requires a physically based model white bear lake wbl is a typical temperate closed basin lake in the mississippi river twin cities watershed minnesota usa in this watershed water levels of many lakes have declined significantly during the first decade of this century a study conducted by the united states geological survey usgs on 96 lakes in the northeastern twin cities metropolitan area found that the water levels of 51 flow through lakes declined an average of 0 2 m from 2002 to 2010 while 45 closed basin lakes declined by about 0 7 m on average jones et al 2016 wbl s water level declined 1 5 m during a 10 year period 2003 2013 and the cause of the decline has been the subject of considerable debate proposals to augment the lake water levels have raised serious economic and environmental concerns a study on wbl s interaction with groundwater was recently completed by the usgs and minnesota department of natural resources mndnr jones et al 2013 the annual evaporation was estimated by applying a coefficient of 0 75 to a class a evaporation pan 18 km away from wbl jones et al 2013 however estimating annual lake evaporation from a general evaporation pan is a challenging and dubious endeavor brustaert 1982 the single coefficient method is associated with several potential uncertainties first the heat capacity of the evaporation pan is much smaller than that of a large lake second pan evaporation cannot reflect temporal and spatial distributions and vertical profile of lake temperature third although lake evaporation and pan evaporation are both highly correlated with wind speed and vapor pressure gradient above the water surface these relations differ as the wind driven mixing influences the evaporation by incorporating the effects of vertical temperature gradient at the lake surface while a pan is easily well mixed and the wind driven mixing maintains temperature homogeneity of the pan blanken et al 2000 martínez et al 2006 roderick et al 2007 in this research we measured and modeled the evaporation at wbl to address the following questions 1 what is the magnitude of evaporation from a temperate lake and how much does it vary seasonally and inter annually 2 how sensitive is annual evaporation to meteorology and climate and to what extent has evaporation from a temperate closed basin lake changed over the past 30 years 3 how are changes in climate expected to impact evaporation and water level of a temperate closed basin lake through the 21st century and 4 what are the potential implications for other lakes within the mississippi river twin cities watershed 2 study site wbl is the second largest lake in terms of surface area 9 7 km2 in the twin cities metropolitan area of minnesota usa it is a glacially formed and trefoil shaped lake an island in the west manitou island and a peninsula in the east divide the lake into three bays north west and southeast fig 1 c the deepest point is 25 m in the southeast bay however the lake depth is generally 10 m in the north and west bays there is no natural inlet or outlet around wbl however there is a control outlet at a water level of 281 7 m above the sea level which allows an outflow rate of about 0 12 m3 s 1 for a water level of 282 m mndnr 1998 three aquifer layers lie below wbl from top to bottom these layers include a glacial water table aquifer in the quaternary deposits the st peter sandstone bedrock aquifer and the prairie du chien jordan bedrock aquifer lake water and groundwater exchange directly in the glacial water table aquifer and the water in the glacial water table aquifer can seep downward to the bedrock aquifers jones et al 2013 since wbl is a closed basin and has a relatively small catchment to lake area ratio of 2 1 with a catchment area of 19 km2 its water level is very sensitive to changes in climate and or hydrologic conditions the recent study on wbl s interaction with groundwater suggested that the lake water discharge to glacial buried and bedrock aquifers was a critical hydrologic sink for wbl jones et al 2013 they found that from 1978 to 2002 precipitation and evaporation could explain the lake level variation but additional increasing municipal groundwater withdrawals needed to be considered to accurately simulate changes in water level from 2003 to 2011 their analyses imply that increased groundwater pumping within the region could be an emerging threat to numerous lakes within the region furthermore changes in climate higher air temperature and humidity longer ice free periods changes in wind speed and net radiation could significantly enhance evaporation and thereby amplify declining lake levels during relatively dry or drought periods 3 methods 3 1 observations 3 1 1 eddy covariance measurements we installed one eddy covariance ec tower in 2014 and two ec towers in 2015 and 2016 fig 1 in 2014 one tower t2014 was first set up south of the east peninsula 45 4 38 0 n 92 58 34 6 w on july 18 it was removed on november 14 to prevent damage from lake ice formation the tower was located at the edge of the lake terrace where the water depth was about 1 m relatively light colored sand covers the terrace and dominant aquatic plants such as coontail ceratophyllum demersum and eurasian watermilfoil m spicatum mccomas 2011 that were present in deeper water beyond the terrace in 2015 one tower t2015a was installed about 14 m away from the 2014 tower location 45 4 37 6 n 92 58 34 9 w the other tower t2015b was set up to the west edge of the peninsula terrace 45 4 42 1 n 92 58 43 1 w these two towers were operational from may 8 to october 31 in 2016 we installed ec towers at the same locations as in 2015 the 2016 observations started a week after the ice out date on march 25 and continued through the frozen period until february 17 2017 a three dimensional sonic anemometer model csat3 campbell scientific logan ut and an open path co2 h2o gas analyzer model li 7500 li cor lincoln ne were installed about 3 m above the water surface at t2014 the sonic anemometer was oriented to the southeast to ensure a relatively broad fetch from the south the instrument settings on t2015a were nearly identical to t2014 however a krypton hygrometer model kh20 campbell scientific logan ut was added for comparison to the li 7500 t2015b was equipped with a csat3 and a kh20 which were installed about 3 m above the water surface the sonic anemometer was oriented to the northwest in order to observe the fetch from the north the towers in 2016 used the same instrumentation as in 2015 all ec signals wind speed sonic temperature and water vapor density were recorded at 10 hz other variables including net radiation water skin temperature and 1 m water temperature were measured every 5 s flux calculations and corrections followed the methods described in lee et al 2004 fluxes were calculated from the 10 hz data using block averaging every 30 min and rotated into the natural wind coordinate system using a double rotation lee et al 2004 a correction term for uv absorption by oxygen tanner et al 1993 was applied to water vapor flux estimates of the kh20 the webb pearman and leuning wpl density terms webb and leuning 1980 were applied to all the turbulent heat flux estimates other ancillary meteorological variables such as net radiation air pressure water skin temperature 1 m water temperature were measured every 5 s detailed information of instrumentation for each micrometeorological tower is listed in table s1 3 1 2 filtering and gap filling the raw turbulent heat fluxes were filtered using a double difference function following papale et al 2006 and griffis et al 2016 as another quality control measure to remove outliers we compared the water vapor concentrations between the li7500 and the relative humidity sensor model cs215 l campbell scientific logan ut and discarded fluxes if the difference was larger than 5 both kh20 sensors exhibited detectable drifts in signal strength related to dirt building up on the lenses and aging detectors therefore we calibrated the kh20 sensors every 10 days by independently comparing their water vapor concentration measurement with two other higher precision moisture sensors li7500 and cs215 l on each tower finally since the highest ground elevation of the peninsula is about 15 m above the lake surface the near surface wind tends to veer along the shoreline the ec data were filtered to eliminate any fetch associated with nearby land according to wind rose and flux footprint analyses due to power failure and filtering about 20 of the half hourly latent heat flux data were missing over the whole observation period we applied the neural net fitting in the neural network toolbox in matlab 2016a to fit all data from 2014 to 2016 and then used the fitting function to fill gaps we set 100 hidden nodes and used the levenberg marquardt method demuth et al 2014 the critical input variables for fitting included meteorological variables air temperature ta vapor pressure ea wind speed u and friction velocity u lake water variables water skin temperature ts 1 m water temperature tw and lake surface saturation vapor pressure es and energy terms net radiation rn and sensible heat flux h given that the evaporation was influenced by thermal and moisture gradients at the water air interface the lake surface vapor pressure difference es ea and temperature difference ts ta were included a non linear term u es ea was also treated as an input variable because evaporation was found to be highly correlated to it in previous studies singh and xu 1997 blanken et al 2000 3 1 3 ice phenology the ice out date of wbl has been well documented by mndnr since 1928 however there are no long term records for ice in dates two daily 500 m reflectance products mod09ga and myd09ga from the moderate resolution imaging spectroradiometer modis instruments on the terra and aqua satellites vermote and wolfe 2015 vermote 2015 were used to determine the ice cover over the lake the retrieved ice period data were used to test the ability of clm4 lisss to simulate the ice cover over wbl described further below the averaged time series of the two products were used to calculate the daily normalized difference snow index ndsi following irish 2000 1 ndsi band 2 band 5 band 2 band 5 where bands 2 and 5 are reflectances in the near infrared 841 876 nm and shortwave infrared 1230 1250 nm ranges respectively the ice phenology over wbl was characterized by the ndsi difference between water and land first the ndsi of each grid cell within an area of 5 km 4 5 km around wbl was calculated then the grid cells were classified as water or land according to the modis 500 m land cover type product mcd12q1 friedl et al 2010 on clear days the ensemble average of ndsi over the ice free water was generally smaller than that over land while the frozen lake ndsi was larger than the land ndsi on cloudy days the differences of ndsis between land and water were close to zero therefore the ndsi difference between water and land is generally negative when the lake is open and positive when the lake is frozen ice in and ice out dates were determined by the inflection points in the yearly cumulative series of the daily ndsi difference the ice in dates fall near the yearly minimum of the accumulation while the maximum indicates the ice out events the major sources of uncertainty are cloud cover snow events and the enduring time of freezing and thawing clouds snow on land and partial ice cover over the lake can minimize the ndsi difference causing the ice in out dates to differ from the actual minima maxima points because the ice in process is generally longer than for ice out there is greater potential for a bias in estimating the ice in date 3 2 modeling 3 2 1 model description the community land model version 4 lake ice snow and sediment simulator clm4 lisss is the offline lake model in clm 4 5 subin et al 2012 oleson et al 2013 although this model is a one dimensional lake model that only considers mixing in the vertical direction it has shown reasonable performance in simulating lake temperature and surface energy fluxes for several different sized lakes around the world e g subin et al 2012 deng et al 2013 stepanenko et al 2014 hu et al 2017 the version of clm4 lisss we used here was based on deng et al 2013 and hu et al 2017 the model was tuned and validated to reproduce the observed wbl evaporation from 2014 through 2016 in order to perform retrospective analyses of evaporation extending from 2016 back to 1979 and to predict future evaporation from 2017 to 2100 3 2 2 forcing data and model spin up clm4 lisss is forced by seven variables including air temperature air pressure air humidity precipitation wind speed down welling shortwave radiation and down welling longwave radiation in the model validation test clm4 lisss was primarily forced by climate data obtained from the ec systems and nearby local weather stations air temperature pressure humidity and wind speed were obtained from 2014 to 2016 using the ec measurement systems table s2 the gaps in these variables were filled using simple linear regression with data from a nearby citizen weather observer program cwop weather station ew2811 45 7 58 n 93 0 39 w which is located about 6 8 km to the northwest of t2014 the observations of air temperature pressure humidity and net radiation in 2015 and 2016 showed that these values were highly consistent between the towers so the averages of these variables from both towers were used to drive the model the wind speed was chosen from representative fetches of the towers determined by the wind roses and flux footprints precipitation and down welling shortwave radiation were measured in 2015 and 2016 but not in 2014 and the down welling longwave radiation was not measured during the three years periods without observations were gap filled using the reanalysis product of the hourly 0 125 0 125 north american land data assimilation system nldas primary forcing data nldas fora0125 h xia et al 2012 due to the coarse resolution of the nldas data the grid cell containing wbl mainly reflected features over land rather than water the diurnal features of air temperature and specific humidity in the nldas data were not consistent with our observations from 2014 to 2016 overall the daily peak of the air temperature in the nldas data was roughly 3 h earlier than that over wbl the amplitudes of the diurnal variation in air temperature were larger than the observations the daily averages of specific humidity in the nldas data were generally smaller than the observations consequently we used the neural net fitting in the neural network toolbox in matlab 2016a with 10 hidden nodes and the levenberg marquardt method demuth et al 2014 to fit the observations with the nldas data and obtain the fitting function the fitting function was then used to correct the air temperature and specific humidity in the entire dataset the correction did not introduce or change long term trends of the original data furthermore piecewise cubic hermitian polynomial interpolation fritscht and carlson 1980 was applied on the nldas data to obtain half hourly values the water temperature profile in the upper lake levels can be obtained with a spin up time of about 1 month however we found that the temperature in the deepest part of the lake required 3 5 years to reach thermal equilibrium therefore nldas data from 2004 to 2013 were applied for the model spin up in the validation simulations 3 2 3 model tuning in the model validation we optimized the model by tuning its parameters and assessing if clm4 lisss produces similar variability in evaporation due to climate when compared to the observations clm4 lisss simulates the sensible heat flux and the latent heat flux using the aerodynamic method subin et al 2012 the sensible heat flux is computed from the thermal resistance and the temperature difference between surface air and the lake surface similarly the latent heat flux is determined by the moisture resistance and the vapor pressure difference at the lake surface the aerodynamic resistance terms are computed from monin obukhov similarity theory with prescribed roughness lengths of momentum heat and moisture since the atmospheric temperature and moisture are driven by the forcing data the modeled lake skin temperature will critically determine the temperature and the vapor pressure differences at the lake surface the lake skin temperature is iteratively solved to close the energy balance within a conceptual lake surface layer the model assumes that this layer absorbs the penetrated shortwave radiation with an absorption fraction β which is related to the light extinction coefficient η of the lake and the thickness of this surface layer za by beer s law deng et al 2013 2 β 1 e z a η the extinction coefficient can be estimated as 1 7 sd where sd is secchi disk transparency boyd 2015 the minnesota pollution control agency mpca occasionally recorded the transparency at wbl during the ice free seasons from 1974 to 2015 using an all white 20 cm diameter secchi disk the observed transparency varied between 2 and 10 m and the annual median value ranged between 1 5 and 4 5 m no significant annual trend was found within the whole observation period the 40 year average of the annual median is about 3 m in our simulations the extinction coefficient was thus set as a constant of 0 57 m 1 a surface layer thickness of 0 6 m is generally set in clm4 lisss and this value has been validated for various lakes around the world e g subin et al 2012 deng et al 2013 reported that for a shallow about 2 m and turbid lake η 5 m 1 the conceptual thickness can be as small as 0 2 m because wbl is generally deeper and relatively clear we assumed that the general setting of 0 6 m was reasonable for our simulations while β is set fixed the lake skin temperature is sensitive to the thermal diffusivity of the water this diffusivity is a critical parameter to tune because it represents both vertical and horizontal mixing in the model the diffusivity is the sum of molecular wind driven eddy and enhanced eddy diffusivities here the dominant term is wind driven eddy diffusivity which is large at the surface and decreases exponentially with depth henderson sellers 1985 the enhanced eddy diffusivity is a parameterization of the unresolved 3d mixing fang and stefan 1996 sensitivity analyses revealed that the lake skin temperature was most sensitive to the wind driven eddy diffusivity and that the amplitude of the simulated diurnal variation of lake skin temperature was too small in the original scheme simulations were substantially improved after reducing the wind driven eddy diffusivity to 0 5 of its original scheme this yielded a surface wind driven eddy diffusivity that typically varied between 0 1 10 5 and 1 8 10 5 m2 s 1 deng et al 2013 reported a similar feature and they scaled the original scheme by 2 i e eddy diffusivity ranged from 0 1 10 5 to 4 10 5 m2 s 1 for a large shallow lake they found that the large wind driven eddy diffusivity erodes the stratification and causes smaller diurnal variation in the lake skin temperature furthermore the lake skin temperature in the fall is very sensitive to the diffusivity in the deeper part of the lake the analyses revealed that the original enhanced eddy diffusivity was appropriate for wbl although subin et al 2012 proposed that the enhanced eddy diffusivity should scale by a factor of 10 for a deep lake the tunings of diffusivity implied that although the deepest point of wbl is 25 m the deep lake setting cannot be entirely applied to wbl wbl is a v shaped glacial lake and its uneven topology causes a range of mixing properties the wind driven mixing is not efficient in shallow regions of wbl while the deep parts of the lake store significant heat and impact the seasonal and annual temperature variability further given the relatively small fetch of wbl the surface wind and internal boundary layer might not be fully equilibrated above the lake surface and therefore might not effectively drive the eddy mixing within the lake surface layer after tuning the diffusivity the aerodynamic roughness lengths were then tuned to better reproduce the turbulent heat fluxes our tuning yielded optimized values for the roughness lengths of momentum z0m heat z0h and moisture z0q in the dynamic ranges of 10 5 10 4 m 10 7 10 8 m and 10 7 10 8 m respectively furthermore ln z0m z0h and ln z0m z0q were kept constant as 7 36 and 7 48 respectively 3 2 4 retrospective modeling in the retrospective simulations the optimized clm4 lisss was driven by the nldas data from 1979 to 2016 the air temperature and specific humidity in the nldas was corrected by the method described in the section 3 2 2 the data from 1980 were cycled over a ten year period to initialize the model 3 2 5 projected modeling to investigate future changes in lake evaporation and water level in response to anticipated climate change we used the tuned clm4 lisss to predict the wbl evaporation under the representative concentration pathways rcp8 5 business as usual greenhouse gas emission scenario riahi et al 2011 from 2017 to 2100 the forcing data were 3 hourly outputs from the gfdl esm2g climate model dunne et al 2012 in the fifth phase of the coupled model inter comparison project cmip5 these simulations were performed for years 2006 to 2100 data from 2006 to 2016 were used for model initialization and spin up 4 results 4 1 observed evaporation 4 1 1 climatology monthly temperature and precipitation data for the twin cities metro area from 2014 to 2016 are shown in fig 2 and compared with the climate normal for the period 1981 2010 national weather service forecast office s now data noaa online weather data http w2 weather gov climate xmacis php wfo mpx the mean annual air temperature in 2014 was 1 6 c cooler than the climate normal 7 8 c and 2015 was 1 2 c warmer than the climate normal each month in 2016 was warmer than normal except december the mean annual temperature in 2016 was 2 c warmer than normal the annual precipitation totals from 2014 to 2016 were 899 mm 918 mm and 1024 mm respectively each year had greater than normal 777 mm precipitation in 2014 the precipitation was mainly concentrated in late spring and early summer while both 2015 and 2016 experienced higher than normal precipitation in summer and fall due to the warmer spring and fall 2015 and 2016 both had significantly earlier ice out dates and later ice in dates than in 2014 according to the mndnr ice phenology data there was no significant trend in the ice out dates of wbl from 1979 to 2016 slope 0 156 day year 1 p 0 319 fig 3 the ice out dates retrieved from modis were in good agreement with the mndnr observations modis vs mndnr slope 1 097 r2 0 98 p 0 01 in the period 2000 2016 both ice in dates and ice free days from modis indicate no statistically significant trends from 2000 to 2016 slope 0 561 day year 1 p 0 189 for ice in dates and slope 0 390 day year 1 p 0 676 for ice free days 4 1 2 energy budgets the mean monthly energy balance partitioning is shown in fig 4 the net radiation peaked at 160 w m 2 in june and july and dropped below 20 w m 2 in november except for a short period after ice out e g march 2016 the sensible heat flux was positive upward the sensible heat flux reached its annual maximum 21 25 w m 2 in october in 2015 and 2016 or november in 2014 the latent heat flux was always larger than the sensible heat flux in the ice free season it gradually increased from spring to mid summer and reached a maximum of 110 129 w m 2 between july and august and slowly decreased from 80 to 90 w m 2 in the summer to 30 40 w m 2 in the fall we were not able to measure lake water temperature profiles to adequately calculate changes in the wbl heat storage instead we calculated the energy residual by subtracting the turbulent heat fluxes from the net radiation i e δs rn h le and used it to estimate the heat storage change a positive energy residual indicates an increase in lake heat storage while a negative residual indicates that energy was released and transformed into turbulent heat fluxes here we assumed that our ec measurements underestimated the total turbulent heat flux by 20 based on energy balance closure analyses wilson et al 2002 nordbo et al 2011 from spring to mid summer the positive residuals indicated that the net radiation warmed the lake and increased the heat storage from late summer to fall while the net radiation gradually reduced to zero the increased negative residuals indicated that the heat released from the lake became the energy source driving the turbulent fluxes 4 1 3 daily evaporation patterns the pattern of the 24 h total daily evaporation i e the daily evaporation rate is shown in fig 5 daily evaporation rates were impacted by synoptic scale variabilities in air temperature humidity and wind speed during summer the distinct spikes in evaporation were induced by the passing of cold fronts blanken et al 2000 spence et al 2013 the peaks of evaporation with daily wind speeds 2 m s 1 positive temperature gradients and vapor pressure gradients were typically triggered by the passage of cold fronts during the observation period such events were identified 7 times in 2014 14 times in 2015 and 21 times in 2016 these events occurred in october most frequently 3 times in 2014 4 times in 2015 and 5 times in 2016 a pronounced two week cycle of evaporation in the fall coincided with synoptic scale systems as identified by wavelet time series analyses in 2015 and 2016 figs s4 s6 4 1 4 annual evaporation to quantify the annual evaporation from 2014 to 2016 the primary challenge is to estimate the evaporation that occurred outside of the observation period four approaches were used to help constrain the annual evaporation from 2014 to 2016 table 2 the first estimate simply multiplied the observed length of the ice free period by the measured daily mean evaporation rate the daily mean of the observation period was calculated from the gap filled half hourly data and the ice in and ice out dates were retrieved from the mndnr and the modis data the second approach applied monthly weighting factors to extrapolate the evaporation to the spring and fall periods when observations were not available here we averaged three years of measurements from march to november and calculated the monthly factors weighted by the total evaporation in august through october when our observations were nearly continuous among the three years the other two methods applied clm4 lisss to extrapolate the evaporation beyond the observation period the third estimate used the optimized clm4 lisss model which was forced with local observations the fourth method was derived from the retrospective modeling approach with clm4 lisss forced only with the reanalysis data product the first method resulted in evaporation estimates of 543 mm 915 mm and 778 mm for 2014 to 2016 respectively this simple annual estimate represents an upper bound on evaporation and contains two main uncertainties the observation errors and the errors associated with the extrapolation of the observation data to the early and late ice free periods for example in 2015 our observations captured much of the summer season but missed the shoulder seasons which likely had less evaporation due to lower available energy the second approach estimated evaporation in the three years to be 535 mm 763 mm and 748 mm respectively the annual evaporation estimated from method three was 567 mm 709 mm and 773 mm for the three years finally method four resulted in annual estimates of 590 mm 731 mm and 764 mm overall these estimates were relatively consistent excluding method one for 2015 which resulted in a large overestimate the major difference between the modeled and weighted estimates was caused by the ice phenology the model overestimated the length of the ice free days in 2014 and underestimated the length in 2015 after averaging the four estimates for each year and applying the standard variance as the error of uncertainty we estimated the annual evaporation in the three years at wbl to be 559 22 mm 779 81 mm and 766 11 mm respectively the annual evaporation in 2014 was least among the three years because of a relatively short ice free period combined with lower daily evaporation rates these estimates did not account for sublimation occurring during the ice cover period in winter 2016 2017 we made ec measurements over the ice from the end of january to early february which showed that the monthly latent heat flux was about 5 w m 2 this is equivalent to about 20 mm of evaporation when assuming a 4 month ice cover period sublimation therefore represented 5 of the annual evaporation budget 4 2 retrospective evaporation 4 2 1 model validation the tuned model forced by local observations reproduced the latent heat flux reasonably well from half hourly to monthly time scales fig 6 at the half hourly scale the model explained 74 of the variance and the root mean square error rmse was 35 w m 2 at the daily scale the model performed slightly better with r2 81 and rmse 23 w m 2 the errors are mainly contributed by four summer days with high evaporation on aug 19 2015 aug 24 2015 jun 20 2016 and aug 21 2016 underestimating the daily mean latent heat flux by over 80 w m 2 overall the modeled monthly latent heat flux showed good agreement with observations in spring and fall however modeled evaporation was biased low in most summers and it was underestimated in summer 2016 by about 10 4 2 2 retrospective results the annual water budget terms were calculated based on the retrospective model outputs and the wbl precipitation and water level recorded by mndnr over the period 1979 to 2016 fig 7 the mean annual precipitation was 868 mm with no significant trends during this period the mean annual total evaporation was 688 mm and it increased 3 8 mm year 1 p 0 01 the trend in the annual evaporation was mainly attributed to the increased daily mean evaporation slope 0 01 mm day 1 year 1 p 0 01 during this period the increase of daily mean evaporation was driven by both increased wind speed slope 0 011 m s 1 year 1 p 0 01 and the lake surface vapor pressure difference slope 0 013 hpa year 1 p 0 095 the increase of the vapor pressure difference was mainly driven by the increased air temperature during the ice free period slope 0 020 c year 1 p 0 075 while no significant trends were shown in the forcing specific humidity slope 0 0032 g kg 1 year 1 p 0 51 and the modeled lake surface temperature slope 0 0021 k year 1 p 0 81 the modeling results also show that the length of ice free period was extended slope 0 40 day year 1 p 0 067 over the period 1979 to 2016 with earlier ice out dates slope 0 294 day year 1 p 0 0734 and insignificant earlier ice in dates slope 0 156 day year 1 p 0 319 fig 3 the extension of the ice free period was mainly driven by the increased whole year air temperature slope 0 038 c year 1 p 0 015 the modeled ice out dates are largely matched with the mndnr record modis vs mndnr slope 0 883 r2 0 68 p 0 01 the trend of the measured ice out dates in the mndnr record while it has the same sign as the modeled trend is smaller and of lesser significance slope 1 29 day year 1 p 0 124 the difference between observed precipitation and modeled evaporation p e showed a similar pattern to the yearly wbl water level change linear regression showed that 77 of the lake water level change could be explained by p e slope 1 36 intercept 247 mm year 1 p 0 01 the groundwater inflow to the lake was estimated from the budget residual by assuming a simplified water balance g inflow δl p e where δl is the change in water level and a positive g inflow indicates a flux of groundwater to the lake the residual analyses provided a rough estimate of the exchange between lake water and groundwater the mean annual residual was 211 mm year 1 for the period 1981 to 1990 which is in close agreement 283 mm year 1 r 0 69 p 0 016 with estimates from a hydrologic model that had comprehensive runoff and groundwater components but a rough evaporation component mndnr 1998 the mean annual residual was 183 mm year 1 for the period 1979 to 2016 this indicates that lake water was generally seeping into the aquifer 4 3 projected evaporation the future wbl evaporation under the rcp8 5 greenhouse gas emission scenario is shown in fig 8 under this scenario the annual evaporation is expected to increase mainly due to the longer ice free period in the forcing data the air temperature increased by 0 049 c year 1 p 0 01 and the specific humidity increased by 0 021 g kg 1 year 1 p 0 01 these trends are consistent with the ongoing and expected intensification of the hydrological cycle under a warming climate santer et al 2007 trenberth and asrar 2014 the modeled increase in the ice free period was 0 50 d year 1 p 0 01 with no significant trend in the daily mean evaporation rate the combined effect will result in an increase in annual total evaporation of 1 4 mm year 1over this century by the end of this century the ice cover days are expected to decrease by more than a month and the annual evaporation will increase by about 15 100 mm year 1 equivalent to 9 8 105 m3 of water the lack of a significant trend in daily mean evaporation may seem counterintuitive but most climate models assume supported to this point by evidence that as global temperature increases mean relative humidity is conserved allen and ingram 2002 thus expected increases in mean lake surface vapor pressure driven by the increases in lake surface temperature with slope 0 018 k year 1 and p 0 01 will be accompanied by increases in mean atmospheric vapor pressure so that the lake surface vapor pressure gradient is projected to increase by only 0 0043 hpa year 1 p 0 17 further wind speed is expected to decrease by 0 0019 m s 1 year 1 p 0 01 and the average net radiation during the ice free period is forecast to decrease by 0 039 w m 2 year 1 p 0 099 5 discussion 5 1 observed evaporation 5 1 1 evaporation rates and patterns during the observation period the estimated average daily evaporation rate during the ice free season ranged from 2 6 to 3 4 mm day 1 from wbl table 2 similar to values reported for small temperate closed basin lakes such as sparkling lake and williams lake listed in table 1 the wbl annual evaporation rate ranged from 559 to 779 mm year 1 similar to lake ahnejärv and lake martiska in europe with average annual evaporation rates of 648 mm year 1 from 1970 to 2009 vainu and terasmaa 2014 the patterns of evaporation at wbl showed many features common to other closed basin lakes table 1 similar dynamics between heat storage changes and turbulent heat fluxes have been reported previously for a broad range of lake types e g blanken et al 2000 lenters et al 2005 li et al 2007 gianniou and antonopoulos 2007 spence et al 2013 wang et al 2014 wang et al 2017 a one month phase lag between latent heat flux and net radiation was also observed over sparkling lake a small temperate closed basin in the upper midwest us lenters et al 2005 while a lag of two to three months has been observed over large deep lakes such as lake superior in the us blanken et al 2000 and lake qinghai in china li et al 2007 the energy stored in wbl from may to july and released from september to november was similar to that observed from a small plateau closed basin lake in china wang et al 2017 however the influence of the length of the ice free period on annual and inter annual variation in evaporation from temperate closed basin lakes was not well characterized in the studies summarized in table 1 our study found that the ice phenology is an important factor for estimating the annual total evaporation and it is an important causal factor of the inter annual differences in annual evaporation the annual total evaporation eannual is the product of daily mean evaporation rate in the ice free period eday and the length of ice free days lice free here we define p q and r as the increase ratios of the daily mean evaporation rate the length of ice free period and the total annual evaporation respectively 3 e annual e day l ice free 4 1 r e annual 1 p e day 1 q l ice free 1 p q p q e day l ice free the term pq is the increased rate caused by the interaction of daily mean evaporation and length of ice free period normally it is a second order term and can be ignored compared with 2014 the annual total evaporation at wbl increased by about 40 in 2015 while the daily mean evaporation increased by about 10 and the length of ice free period increased by about 25 therefore the ice free period contributed about 60 to the increase in the annual evaporation given a typical daily mean evaporation rate of 2 6 mm day 1 and a length of ice free period of 250 days a 1 error represent a 0 026 mm day 1 in the daily mean evaporation and 2 5 days in the length of ice free period without accurate in situ record of ice phenology of a lake it is very likely to have a 2 5 day error in the length of ice free period 5 1 2 lake evaporation and pan evaporation fig 9 shows the comparison between monthly evaporation at wbl and the evaporation from a class a evaporation pan 18 km away from wbl on the st paul campus at the university of minnesota the evaporation measured by ec at wbl was greatest in july and august and the fall evaporation was more active than spring i e the shape of the distribution was unimodal and left skewed the pan evaporation was greatest in july while the spring evaporation was larger than fall i e unimodal but right skewed and it exhibited a larger seasonal variability compared to the lake these patterns indicate that the pan evaporation was mainly driven by variability in solar radiation the lake to pan evaporation ratio showed large monthly and inter annual variability for example the ratio increased from 0 5 to 0 9 from may to september and the ratio was larger than 1 in april and october the august ratio varied between 0 66 and 0 96 for the three years the annual total pan evaporation was 864 mm 853 mm and 877 mm from 2014 to 2016 respectively these values were considerably larger than the gap filled ec measurements and showed relatively little inter annual variability the lake to pan evaporation ratio for the annual total was 0 65 0 91 and 0 87 for these three years respectively these analyses demonstrate that it is difficult to derive a single coefficient between annual pan evaporation and wbl evaporation it should be noted that the variability in the length of the ice free period can have a nonlinear effect on these ratios making it impractical to obtain a representative annual value for a temperate lake 5 2 retrospective evaporation in the past 40 years wbl has experienced two significant water level declines 1986 to 1990 and 2003 to 2012 fig 8 both periods coincided with years with less precipitation and greater evaporation than the average our residual analyses suggest that the flux of lake water entering groundwater was larger than the average over the same period this indicates that the groundwater aquifer was also at relatively low levels due to the low precipitation further while the entire region was in a drought during these periods groundwater pumping was also estimated to be greater due to agricultural and municipal usage jones et al 2013 for example over the 10 year period 2003 to 2012 mean annual precipitation was 822 83 mm year 1 mean standard deviation the modeled annual evaporation was 756 79 mm and the derived mean seepage to groundwater was 244 132 mm year 1 for regional context we compared the modeled retrospective annual evaporation at wbl to our long term ec measurements of evapotranspiration et at corn soybean sites in rosemount mn at the southern edge of the twin cities metropolitan area baker and griffis 2005 baker et al 2012 griffis et al 2016 over the same period 2003 to 2012 the mean annual et was 522 52 mm over croplands the mean annual et for corn and soybean systems were not statistically different with mean annual values of 507 53 mm and 534 52 mm respectively year 2003 and 2012 were two notable drought years within the region the modeled lake evaporation for 2003 and 2012 was 723 mm and 917 mm respectively while the pan evaporation for these two years was 879 mm and 1031 mm respectively the annual et from crops was about 100 mm lower than the 10 year average for corn soybean with minimum values of 423 and 472 mm in 2003 and 2012 respectively these relatively low et rates highlight how declining soil water content and drought stress likely increased the demand for irrigation it is likely that such drought periods have exacerbated groundwater usage and have contributed to the decline in groundwater and lake water level within the region jones et al 2013 5 3 projected evaporation given that lake evaporation is affected by multiple atmospheric variables and processes long term trends in those variables are critical in determining the trend in daily evaporation rate and annual total evaporation the forcing data under rcp8 5 scenario indicate that air temperature and humidity will rise while wind speed will decrease fig 8 correspondingly the lake water surface temperature is expected to enhance the vertical vapor pressure gradient these recent regional trends are consistent with the observed global trends over the past decades willett et al 2008 mcvicar et al 2012 o reilly et al 2015 although the projected modeling results indicate that the effects of changes in wind speed vapor pressure difference and net radiation might be compensatory with respect to their influence on the daily mean evaporation rate the change in length of the ice free period will be a major factor driving increases in the annual total evaporation the relation between evaporation and ice phenology deserves more research to understand the feedback processes and the potential amount of inter annual variability in evaporation from temperate closed basin lakes we note that wind speed forcing data has opposite trends in the retrospective modeling and the projected modeling these trends are consistent with previous studies holt and wang 2012 kulkarni and huang 2014 ashtine et al 2016 the mechanism underlying these trends is largely driven by changes in the atmospheric circulation and require further investigation hobbins et al 2012 found that the most important drivers of the atmospheric evaporative demand for our study area were air temperature specific humidity downwelling shortwave radiation downwelling longwave radiation and wind speed this indicates that compared with the wind speed the vapor pressure difference at the lake surface which is mainly influenced by both air temperature and specific humidity is more critical in affecting the daily lake evaporation rate in the region the projected modeling results indicate that increasing rate of evaporation is likely to outpace changes in precipitation it is estimated that annual evaporation at wbl will increase by 1 4 mm year 1 year 1 p 0 01 over this century while the annual precipitation is expected to increase by about 1 1 mm year 1 year 1 p 0 06 furthermore there will be increasing probability of more extreme large evaporation years compared with current climate status overall we expect decreases in the moving average of wbl lake water level and increases in lake level variability 5 4 regional implications the mississippi river twin cities watershed has more than 1 8 million people and over 250 lakes anderson et al 2013 many of these lakes have shown high sensitivity to variations in climate and water use our observations and model analyses support that lake levels within the region are closely coupled to evaporation and that future scenarios show a tendency for increased likelihood of lower water levels and more extreme fluctuations in lake levels unfortunately detailed observations and modeling for such lakes remains rare and therefore their utility in effectively managing these water resources remains limited there is an increasing need for developing comprehensive water management strategies to address the potential impacts of extreme climate events and climate change in light of increasing population and growing demand for these water resources understanding and forecasting changes in the magnitude of evaporation cannot be ignored for wbl the water loss from a typical evaporation rate of 5 mm per day during the summer is equivalent to 4 9 104 m3 of water or roughly 0 5 m3 s 1 of continuous 24 h pumping an advanced ice out date or postponed ice in date of just one day is likely to result in an additional water loss of 2 104 m3 thus small changes in the evaporation rate or ice phenology can have significant impacts on available water for these communities proposed water augmentation strategies that are designed to compensate for declining lake levels within the region for the present climate must be aware of the potential changes in supply and demand as climate continues to warm further according to recent data from the usgs per capita water use in minnesota is about 0 23 m3 per day maupin et al 2010 the additional 100 mm of evaporation at wbl resulting from the long term change in climate is equivalent to the annual water use of over 11 000 people and further highlights the need for sound water use management strategies the combination of long term evaporation measurements and modeling for sentinel lakes should be adopted as a strategy to help protect lakes and the ecosystem services that they provide the annual changes in water level of wbl showed strong coherence to changes in the regional lake water levels from 1925 to 2016 fig 10 the changes in wbl water level and the median changes in the regional lake water levels exhibited very similar statistical characteristics including the overall sign they were also significantly correlated with a coefficient of 0 68 p 0 01 as a closed basin lake wbl also showed a larger variation in the water level than the regional median value but the water level changes of wbl were usually within the ranges bounded by the whiskers of the box plot such coherence implies that the regional lake water levels were collectively impacted by the large scale synoptic and climatic drivers influencing the regional precipitation and evaporation previous studies have shown that decadal changes in water levels of closed basin lakes in north america were correlated to large scale climate teleconnections such as the atlantic multidecadal oscillation amo and the north atlantic oscillation nao hanrahan et al 2010 hanrahan et al 2014 watras et al 2014 these teleconnections can help us further understand the inter annual relationship between climate and regional water balance for example previous research has shown that the 2009 2010 el niño winter induced higher than normal air temperature in north america and mild ice conditions over the great lakes bai et al 2011 in our study period the increases in mean annual air temperature and precipitation from 2014 to 2016 in the twin cities and the changes in their patterns were potentially related to a phase shift in the el niño southern oscillation enso from a neutral to el niño state correspondingly we observed a relative longer ice free season and larger annual evaporation at wbl in 2015 and 2016 than 2014 thus long term projections of lake evaporation will also be influenced by the frequency and persistence of large scale atmospheric circulations however the influence of climate change on such teleconnections is poorly understood hanrahan et al 2010 therefore more observations and modeling studies are needed to improve our understanding of these factors 6 conclusion 1 daily evaporation at wbl was strongly influenced by synoptic scale variability the heat released from the lake was the main energy source for evaporation in the fall the annual evaporation at wbl from 2014 to 2016 was 559 22 mm 779 81 mm and 766 11 mm respectively the annual evaporation in 2014 was least among the three years due to its relatively short ice free period and its relatively lower daily evaporation rate 2 the retrospective analyses indicated that wbl annual total evaporation increased by about 3 8 mm year 1 from 1979 to 2016 which was attributed to increased daily mean evaporation during the ice free period the increase of daily mean evaporation was driven by the increased wind speed and lake surface vapor pressure gradient the lake level declines at wbl during 1986 1990 and 2003 2012 were caused by the coupled low precipitation and high evaporation this finding implies that a regional drought and potential intensified groundwater use can have a dramatic impact on water level at a closed basin lake 3 model results suggest that annual evaporation at wbl will increase 1 4 mm year 1 over this century under the rcp 8 5 scenario which is largely driven by the extended ice free periods at the end of this century the ice cover period will shorten by more than a month and the annual evaporation will increase by an equivalent 9 8 105 m3 of water 4 our observations and model analyses support that lake levels within the region are closely coupled to evaporation lake evaporation is expected to increase due to the extended ice free period as climate continues to warm a tendency for increased likelihood of lower water levels and greater fluctuations in water level for wbl and other lakes within the region is expected acknowledgements we acknowledge the nicholson family and the becker family for allowing us to access white bear lake through their private property financial support and scientific instruments were provided by ameriflux the minnesota corn research and promotion council grant number 4101 15sp and the minnesota dnr usda ars provided maintenance and logistical support for this project finally we acknowledge the white bear lake conservation district for their interest and continued support we express our since thanks to editor tim r mcvicar associate editor joshua larsen and two reviewers peter blanken university of colorado boulder colorado united states and one anonymous reviewer for their helpful comments and suggestions all data from this study will be made available at https www biometeorology umn edu research data archives appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 03 059 appendix a supplementary data supplementary data 
7262,lakes provide enormous economic recreational and aesthetic benefits to citizens these ecosystem services may be adversely impacted by climate change in the twin cities metropolitan area of minnesota usa many lakes have been at historic low levels and water augmentation strategies have been proposed to alleviate the problem white bear lake wbl is a notable example its water level declined 1 5 m during 2003 2013 for reasons that are not fully understood this study examined current past and future lake evaporation to better understand how climate will impact the water balance of lakes within this region evaporation from wbl was measured from july 2014 to february 2017 using two eddy covariance ec systems to provide better constraints on the water budget and to investigate the impact of evaporation on lake level the estimated annual evaporation losses for years 2014 through 2016 were 559 22 mm 779 81 mm and 766 11 mm respectively the higher evaporation in 2015 and 2016 was caused by the combined effects of larger average daily evaporation and a longer ice free season the ec measurements were used to tune the community land model 4 lake ice snow and sediment simulator clm4 lisss to estimate lake evaporation over the period 1979 2016 retrospective analyses indicate that wbl evaporation increased during this time by about 3 8 mm year 1 which was driven by increased wind speed and lake surface vapor pressure gradient using a business as usual greenhouse gas emission scenario rcp8 5 lake evaporation was modeled forward in time from 2017 to 2100 annual evaporation is expected to increase by 1 4 mm year 1 over this century largely driven by lengthening ice free periods these changes in ice phenology and evaporation will have important implications for the regional water balance and water management and water augmentation strategies that are being proposed for these metropolitan lakes keywords evaporation lake eddy covariance modeling ice phenology water level 1 introduction the physical aspects of temperate closed basin lakes such as water level water temperature and ice phenology are highly sensitive to variations in climate adrian et al 2009 schindler 2009 since their contributing watersheds are often relatively small streamflow is typically a minor component of their water balance which is primarily controlled by precipitation evaporation and groundwater exchange almendinger 1990 winter 1995 short term water level change of closed basin lakes is most influenced by changes in precipitation and evaporation van der kamp et al 2008 while the interaction with groundwater tends to impact lake water level at longer time scales and acts to dampen seasonal and inter annual lake level changes kirillin et al 2013 in general the water levels of closed basin lakes are subject to larger variations than flow through lakes and in some cases water level can vary several meters within a decade e g almendinger 1990 ayenew and becht 2008 attribution of the cause s of such changes is challenging since the only data typically available are precipitation records evaporation is the major sink loss term in the lake water budget of temperate closed basin lakes better constraints on lake evaporation are needed to improve our understanding of present past and future changes in water levels key results of relevant studies on lake evaporation and water budgets for closed basin lakes are summarized in table 1 in most of these studies lake evaporation has been estimated using indirect methods such as the penman or energy balance methods e g lenters et al 2005 shanahan et al 2007 direct measurement of lake evaporation using the eddy covariance approach has been applied in relatively few studies it has been used on several large flow through lakes e g blanken et al 2000 blanken et al 2011 wang et al 2014 but only a few closed basin lakes e g li et al 2016 wang et al 2017 eddy covariance has relatively few theoretical assumptions and can reveal detailed physical characteristics of lake evaporation at high temporal resolution i e hourly providing important information needed for the development and optimization of physical models for the broader study and estimation of lake evaporation e g granger and hedstrom 2011 hu et al 2017 lake evaporation is influenced by atmospheric and lake surface processes acting at different time scales at hourly to daily scales lake evaporation rate is highly correlated to wind speed over the lake and vapor pressure difference between the lake surface and the atmosphere blanken et al 2000 wang et al 2017 the mass transfer method uses these two key variables to estimate lake evaporation singh and xu 1997 at daily to weekly scales evaporation is influenced by synoptic scale patterns modulated by the significant thermal lag that results from the high heat capacity of water for example cold fronts bringing cold and dry air over a warm lake in fall can significantly enhance evaporation blanken et al 2000 spence et al 2013 seasonal variation in evaporation is mainly forced by changes in net radiation and the air lake temperature gradient lenters et al 2005 annual evaporation from temperate closed basin lakes is particularly sensitive to lake ice phenology typically a longer ice free period can potentially increase the integrated amount of seasonal evaporation rouse et al 2008 however observation over large lakes such as lake superior and lake michigan indicated that temperate lake evaporation is not always positively correlated with ice free duration blanken et al 2011 gronewold et al 2015 for instance a cold and dry fall season over lake superior can significantly enhance the shoulder season evaporation cause a large annual total evaporation and induce a short ice free season with early ice formation while a mild winter with a late ice formation may have less cumulative evaporation spence et al 2013 the extent to which these observations apply to much smaller lakes is unclear and needs to be addressed lake evaporation rates are also influenced by physical properties such as lake geometry i e shape depth surface area and water transparency e g subin et al 2012 deng et al 2013 lake depth is an important factor that influences the change in water heat storage and the seasonal phase lag between energy input and evaporation rate for example the monthly mean evaporation of a subtropical large shallow lake was closely coupled with seasonal variations in net radiation lake taihu in china average depth 2 m wang et al 2014 whereas deep lakes maximum depth 10 m show a 2 to 3 month phase lag between the net radiation and evaporation regardless of the area of the lake e g li et al 2016 blanken et al 2000 the transparency or turbidity which is related to the chemical composition and plankton population distribution within a lake affects the penetration depth of shortwave radiation and therefore the available energy for evaporation deng et al 2013 wang et al 2014 changes in climate including air temperature humidity wind speed and available energy can have an important influence on long term trends in lake evaporation the globally average wind speed decreased by 0 7 m s 1 from 1950 to 2000 which coincided with an observed decrease in global pan evaporation and calculated crop reference evapotranspiration mcvicar et al 2012 the global decrease in surface solar radiation i e global dimming between the 1950s and 1980s could also have potentially decreased lake evaporation by reducing the available energy the more recent brightening trend over much of the globe could have acted to enhance evaporation wild 2009 further the global mean lake summer surface water temperature was found to increase by 0 34 c decade 1 between 1985 and 2009 correlated with the global brightening and air temperature trends o reilly et al 2015 over the period 1973 to 2003 observed specific humidity over land increased by 0 11 g kg 1 per decade while the relative humidity remained quasi constant willett et al 2008 given the observed increases in air temperature and lake surface temperature this implies that the vapor pressure gradient at the lake surface has also increased the increase in vapor pressure gradient for the most rapidly warming lakes around the globe was estimated to be 15 20 from 1985 to 2009 friedrich et al 2018 these trends in key climate variables have likely interacted and influenced lake evaporation resulting in significant inter annual variability in lake evaporation and fluctuations in lake water level given that multiple trends in atmospheric variables affect the long term trend of lake evaporation understanding these trends and projecting future changes requires a physically based model white bear lake wbl is a typical temperate closed basin lake in the mississippi river twin cities watershed minnesota usa in this watershed water levels of many lakes have declined significantly during the first decade of this century a study conducted by the united states geological survey usgs on 96 lakes in the northeastern twin cities metropolitan area found that the water levels of 51 flow through lakes declined an average of 0 2 m from 2002 to 2010 while 45 closed basin lakes declined by about 0 7 m on average jones et al 2016 wbl s water level declined 1 5 m during a 10 year period 2003 2013 and the cause of the decline has been the subject of considerable debate proposals to augment the lake water levels have raised serious economic and environmental concerns a study on wbl s interaction with groundwater was recently completed by the usgs and minnesota department of natural resources mndnr jones et al 2013 the annual evaporation was estimated by applying a coefficient of 0 75 to a class a evaporation pan 18 km away from wbl jones et al 2013 however estimating annual lake evaporation from a general evaporation pan is a challenging and dubious endeavor brustaert 1982 the single coefficient method is associated with several potential uncertainties first the heat capacity of the evaporation pan is much smaller than that of a large lake second pan evaporation cannot reflect temporal and spatial distributions and vertical profile of lake temperature third although lake evaporation and pan evaporation are both highly correlated with wind speed and vapor pressure gradient above the water surface these relations differ as the wind driven mixing influences the evaporation by incorporating the effects of vertical temperature gradient at the lake surface while a pan is easily well mixed and the wind driven mixing maintains temperature homogeneity of the pan blanken et al 2000 martínez et al 2006 roderick et al 2007 in this research we measured and modeled the evaporation at wbl to address the following questions 1 what is the magnitude of evaporation from a temperate lake and how much does it vary seasonally and inter annually 2 how sensitive is annual evaporation to meteorology and climate and to what extent has evaporation from a temperate closed basin lake changed over the past 30 years 3 how are changes in climate expected to impact evaporation and water level of a temperate closed basin lake through the 21st century and 4 what are the potential implications for other lakes within the mississippi river twin cities watershed 2 study site wbl is the second largest lake in terms of surface area 9 7 km2 in the twin cities metropolitan area of minnesota usa it is a glacially formed and trefoil shaped lake an island in the west manitou island and a peninsula in the east divide the lake into three bays north west and southeast fig 1 c the deepest point is 25 m in the southeast bay however the lake depth is generally 10 m in the north and west bays there is no natural inlet or outlet around wbl however there is a control outlet at a water level of 281 7 m above the sea level which allows an outflow rate of about 0 12 m3 s 1 for a water level of 282 m mndnr 1998 three aquifer layers lie below wbl from top to bottom these layers include a glacial water table aquifer in the quaternary deposits the st peter sandstone bedrock aquifer and the prairie du chien jordan bedrock aquifer lake water and groundwater exchange directly in the glacial water table aquifer and the water in the glacial water table aquifer can seep downward to the bedrock aquifers jones et al 2013 since wbl is a closed basin and has a relatively small catchment to lake area ratio of 2 1 with a catchment area of 19 km2 its water level is very sensitive to changes in climate and or hydrologic conditions the recent study on wbl s interaction with groundwater suggested that the lake water discharge to glacial buried and bedrock aquifers was a critical hydrologic sink for wbl jones et al 2013 they found that from 1978 to 2002 precipitation and evaporation could explain the lake level variation but additional increasing municipal groundwater withdrawals needed to be considered to accurately simulate changes in water level from 2003 to 2011 their analyses imply that increased groundwater pumping within the region could be an emerging threat to numerous lakes within the region furthermore changes in climate higher air temperature and humidity longer ice free periods changes in wind speed and net radiation could significantly enhance evaporation and thereby amplify declining lake levels during relatively dry or drought periods 3 methods 3 1 observations 3 1 1 eddy covariance measurements we installed one eddy covariance ec tower in 2014 and two ec towers in 2015 and 2016 fig 1 in 2014 one tower t2014 was first set up south of the east peninsula 45 4 38 0 n 92 58 34 6 w on july 18 it was removed on november 14 to prevent damage from lake ice formation the tower was located at the edge of the lake terrace where the water depth was about 1 m relatively light colored sand covers the terrace and dominant aquatic plants such as coontail ceratophyllum demersum and eurasian watermilfoil m spicatum mccomas 2011 that were present in deeper water beyond the terrace in 2015 one tower t2015a was installed about 14 m away from the 2014 tower location 45 4 37 6 n 92 58 34 9 w the other tower t2015b was set up to the west edge of the peninsula terrace 45 4 42 1 n 92 58 43 1 w these two towers were operational from may 8 to october 31 in 2016 we installed ec towers at the same locations as in 2015 the 2016 observations started a week after the ice out date on march 25 and continued through the frozen period until february 17 2017 a three dimensional sonic anemometer model csat3 campbell scientific logan ut and an open path co2 h2o gas analyzer model li 7500 li cor lincoln ne were installed about 3 m above the water surface at t2014 the sonic anemometer was oriented to the southeast to ensure a relatively broad fetch from the south the instrument settings on t2015a were nearly identical to t2014 however a krypton hygrometer model kh20 campbell scientific logan ut was added for comparison to the li 7500 t2015b was equipped with a csat3 and a kh20 which were installed about 3 m above the water surface the sonic anemometer was oriented to the northwest in order to observe the fetch from the north the towers in 2016 used the same instrumentation as in 2015 all ec signals wind speed sonic temperature and water vapor density were recorded at 10 hz other variables including net radiation water skin temperature and 1 m water temperature were measured every 5 s flux calculations and corrections followed the methods described in lee et al 2004 fluxes were calculated from the 10 hz data using block averaging every 30 min and rotated into the natural wind coordinate system using a double rotation lee et al 2004 a correction term for uv absorption by oxygen tanner et al 1993 was applied to water vapor flux estimates of the kh20 the webb pearman and leuning wpl density terms webb and leuning 1980 were applied to all the turbulent heat flux estimates other ancillary meteorological variables such as net radiation air pressure water skin temperature 1 m water temperature were measured every 5 s detailed information of instrumentation for each micrometeorological tower is listed in table s1 3 1 2 filtering and gap filling the raw turbulent heat fluxes were filtered using a double difference function following papale et al 2006 and griffis et al 2016 as another quality control measure to remove outliers we compared the water vapor concentrations between the li7500 and the relative humidity sensor model cs215 l campbell scientific logan ut and discarded fluxes if the difference was larger than 5 both kh20 sensors exhibited detectable drifts in signal strength related to dirt building up on the lenses and aging detectors therefore we calibrated the kh20 sensors every 10 days by independently comparing their water vapor concentration measurement with two other higher precision moisture sensors li7500 and cs215 l on each tower finally since the highest ground elevation of the peninsula is about 15 m above the lake surface the near surface wind tends to veer along the shoreline the ec data were filtered to eliminate any fetch associated with nearby land according to wind rose and flux footprint analyses due to power failure and filtering about 20 of the half hourly latent heat flux data were missing over the whole observation period we applied the neural net fitting in the neural network toolbox in matlab 2016a to fit all data from 2014 to 2016 and then used the fitting function to fill gaps we set 100 hidden nodes and used the levenberg marquardt method demuth et al 2014 the critical input variables for fitting included meteorological variables air temperature ta vapor pressure ea wind speed u and friction velocity u lake water variables water skin temperature ts 1 m water temperature tw and lake surface saturation vapor pressure es and energy terms net radiation rn and sensible heat flux h given that the evaporation was influenced by thermal and moisture gradients at the water air interface the lake surface vapor pressure difference es ea and temperature difference ts ta were included a non linear term u es ea was also treated as an input variable because evaporation was found to be highly correlated to it in previous studies singh and xu 1997 blanken et al 2000 3 1 3 ice phenology the ice out date of wbl has been well documented by mndnr since 1928 however there are no long term records for ice in dates two daily 500 m reflectance products mod09ga and myd09ga from the moderate resolution imaging spectroradiometer modis instruments on the terra and aqua satellites vermote and wolfe 2015 vermote 2015 were used to determine the ice cover over the lake the retrieved ice period data were used to test the ability of clm4 lisss to simulate the ice cover over wbl described further below the averaged time series of the two products were used to calculate the daily normalized difference snow index ndsi following irish 2000 1 ndsi band 2 band 5 band 2 band 5 where bands 2 and 5 are reflectances in the near infrared 841 876 nm and shortwave infrared 1230 1250 nm ranges respectively the ice phenology over wbl was characterized by the ndsi difference between water and land first the ndsi of each grid cell within an area of 5 km 4 5 km around wbl was calculated then the grid cells were classified as water or land according to the modis 500 m land cover type product mcd12q1 friedl et al 2010 on clear days the ensemble average of ndsi over the ice free water was generally smaller than that over land while the frozen lake ndsi was larger than the land ndsi on cloudy days the differences of ndsis between land and water were close to zero therefore the ndsi difference between water and land is generally negative when the lake is open and positive when the lake is frozen ice in and ice out dates were determined by the inflection points in the yearly cumulative series of the daily ndsi difference the ice in dates fall near the yearly minimum of the accumulation while the maximum indicates the ice out events the major sources of uncertainty are cloud cover snow events and the enduring time of freezing and thawing clouds snow on land and partial ice cover over the lake can minimize the ndsi difference causing the ice in out dates to differ from the actual minima maxima points because the ice in process is generally longer than for ice out there is greater potential for a bias in estimating the ice in date 3 2 modeling 3 2 1 model description the community land model version 4 lake ice snow and sediment simulator clm4 lisss is the offline lake model in clm 4 5 subin et al 2012 oleson et al 2013 although this model is a one dimensional lake model that only considers mixing in the vertical direction it has shown reasonable performance in simulating lake temperature and surface energy fluxes for several different sized lakes around the world e g subin et al 2012 deng et al 2013 stepanenko et al 2014 hu et al 2017 the version of clm4 lisss we used here was based on deng et al 2013 and hu et al 2017 the model was tuned and validated to reproduce the observed wbl evaporation from 2014 through 2016 in order to perform retrospective analyses of evaporation extending from 2016 back to 1979 and to predict future evaporation from 2017 to 2100 3 2 2 forcing data and model spin up clm4 lisss is forced by seven variables including air temperature air pressure air humidity precipitation wind speed down welling shortwave radiation and down welling longwave radiation in the model validation test clm4 lisss was primarily forced by climate data obtained from the ec systems and nearby local weather stations air temperature pressure humidity and wind speed were obtained from 2014 to 2016 using the ec measurement systems table s2 the gaps in these variables were filled using simple linear regression with data from a nearby citizen weather observer program cwop weather station ew2811 45 7 58 n 93 0 39 w which is located about 6 8 km to the northwest of t2014 the observations of air temperature pressure humidity and net radiation in 2015 and 2016 showed that these values were highly consistent between the towers so the averages of these variables from both towers were used to drive the model the wind speed was chosen from representative fetches of the towers determined by the wind roses and flux footprints precipitation and down welling shortwave radiation were measured in 2015 and 2016 but not in 2014 and the down welling longwave radiation was not measured during the three years periods without observations were gap filled using the reanalysis product of the hourly 0 125 0 125 north american land data assimilation system nldas primary forcing data nldas fora0125 h xia et al 2012 due to the coarse resolution of the nldas data the grid cell containing wbl mainly reflected features over land rather than water the diurnal features of air temperature and specific humidity in the nldas data were not consistent with our observations from 2014 to 2016 overall the daily peak of the air temperature in the nldas data was roughly 3 h earlier than that over wbl the amplitudes of the diurnal variation in air temperature were larger than the observations the daily averages of specific humidity in the nldas data were generally smaller than the observations consequently we used the neural net fitting in the neural network toolbox in matlab 2016a with 10 hidden nodes and the levenberg marquardt method demuth et al 2014 to fit the observations with the nldas data and obtain the fitting function the fitting function was then used to correct the air temperature and specific humidity in the entire dataset the correction did not introduce or change long term trends of the original data furthermore piecewise cubic hermitian polynomial interpolation fritscht and carlson 1980 was applied on the nldas data to obtain half hourly values the water temperature profile in the upper lake levels can be obtained with a spin up time of about 1 month however we found that the temperature in the deepest part of the lake required 3 5 years to reach thermal equilibrium therefore nldas data from 2004 to 2013 were applied for the model spin up in the validation simulations 3 2 3 model tuning in the model validation we optimized the model by tuning its parameters and assessing if clm4 lisss produces similar variability in evaporation due to climate when compared to the observations clm4 lisss simulates the sensible heat flux and the latent heat flux using the aerodynamic method subin et al 2012 the sensible heat flux is computed from the thermal resistance and the temperature difference between surface air and the lake surface similarly the latent heat flux is determined by the moisture resistance and the vapor pressure difference at the lake surface the aerodynamic resistance terms are computed from monin obukhov similarity theory with prescribed roughness lengths of momentum heat and moisture since the atmospheric temperature and moisture are driven by the forcing data the modeled lake skin temperature will critically determine the temperature and the vapor pressure differences at the lake surface the lake skin temperature is iteratively solved to close the energy balance within a conceptual lake surface layer the model assumes that this layer absorbs the penetrated shortwave radiation with an absorption fraction β which is related to the light extinction coefficient η of the lake and the thickness of this surface layer za by beer s law deng et al 2013 2 β 1 e z a η the extinction coefficient can be estimated as 1 7 sd where sd is secchi disk transparency boyd 2015 the minnesota pollution control agency mpca occasionally recorded the transparency at wbl during the ice free seasons from 1974 to 2015 using an all white 20 cm diameter secchi disk the observed transparency varied between 2 and 10 m and the annual median value ranged between 1 5 and 4 5 m no significant annual trend was found within the whole observation period the 40 year average of the annual median is about 3 m in our simulations the extinction coefficient was thus set as a constant of 0 57 m 1 a surface layer thickness of 0 6 m is generally set in clm4 lisss and this value has been validated for various lakes around the world e g subin et al 2012 deng et al 2013 reported that for a shallow about 2 m and turbid lake η 5 m 1 the conceptual thickness can be as small as 0 2 m because wbl is generally deeper and relatively clear we assumed that the general setting of 0 6 m was reasonable for our simulations while β is set fixed the lake skin temperature is sensitive to the thermal diffusivity of the water this diffusivity is a critical parameter to tune because it represents both vertical and horizontal mixing in the model the diffusivity is the sum of molecular wind driven eddy and enhanced eddy diffusivities here the dominant term is wind driven eddy diffusivity which is large at the surface and decreases exponentially with depth henderson sellers 1985 the enhanced eddy diffusivity is a parameterization of the unresolved 3d mixing fang and stefan 1996 sensitivity analyses revealed that the lake skin temperature was most sensitive to the wind driven eddy diffusivity and that the amplitude of the simulated diurnal variation of lake skin temperature was too small in the original scheme simulations were substantially improved after reducing the wind driven eddy diffusivity to 0 5 of its original scheme this yielded a surface wind driven eddy diffusivity that typically varied between 0 1 10 5 and 1 8 10 5 m2 s 1 deng et al 2013 reported a similar feature and they scaled the original scheme by 2 i e eddy diffusivity ranged from 0 1 10 5 to 4 10 5 m2 s 1 for a large shallow lake they found that the large wind driven eddy diffusivity erodes the stratification and causes smaller diurnal variation in the lake skin temperature furthermore the lake skin temperature in the fall is very sensitive to the diffusivity in the deeper part of the lake the analyses revealed that the original enhanced eddy diffusivity was appropriate for wbl although subin et al 2012 proposed that the enhanced eddy diffusivity should scale by a factor of 10 for a deep lake the tunings of diffusivity implied that although the deepest point of wbl is 25 m the deep lake setting cannot be entirely applied to wbl wbl is a v shaped glacial lake and its uneven topology causes a range of mixing properties the wind driven mixing is not efficient in shallow regions of wbl while the deep parts of the lake store significant heat and impact the seasonal and annual temperature variability further given the relatively small fetch of wbl the surface wind and internal boundary layer might not be fully equilibrated above the lake surface and therefore might not effectively drive the eddy mixing within the lake surface layer after tuning the diffusivity the aerodynamic roughness lengths were then tuned to better reproduce the turbulent heat fluxes our tuning yielded optimized values for the roughness lengths of momentum z0m heat z0h and moisture z0q in the dynamic ranges of 10 5 10 4 m 10 7 10 8 m and 10 7 10 8 m respectively furthermore ln z0m z0h and ln z0m z0q were kept constant as 7 36 and 7 48 respectively 3 2 4 retrospective modeling in the retrospective simulations the optimized clm4 lisss was driven by the nldas data from 1979 to 2016 the air temperature and specific humidity in the nldas was corrected by the method described in the section 3 2 2 the data from 1980 were cycled over a ten year period to initialize the model 3 2 5 projected modeling to investigate future changes in lake evaporation and water level in response to anticipated climate change we used the tuned clm4 lisss to predict the wbl evaporation under the representative concentration pathways rcp8 5 business as usual greenhouse gas emission scenario riahi et al 2011 from 2017 to 2100 the forcing data were 3 hourly outputs from the gfdl esm2g climate model dunne et al 2012 in the fifth phase of the coupled model inter comparison project cmip5 these simulations were performed for years 2006 to 2100 data from 2006 to 2016 were used for model initialization and spin up 4 results 4 1 observed evaporation 4 1 1 climatology monthly temperature and precipitation data for the twin cities metro area from 2014 to 2016 are shown in fig 2 and compared with the climate normal for the period 1981 2010 national weather service forecast office s now data noaa online weather data http w2 weather gov climate xmacis php wfo mpx the mean annual air temperature in 2014 was 1 6 c cooler than the climate normal 7 8 c and 2015 was 1 2 c warmer than the climate normal each month in 2016 was warmer than normal except december the mean annual temperature in 2016 was 2 c warmer than normal the annual precipitation totals from 2014 to 2016 were 899 mm 918 mm and 1024 mm respectively each year had greater than normal 777 mm precipitation in 2014 the precipitation was mainly concentrated in late spring and early summer while both 2015 and 2016 experienced higher than normal precipitation in summer and fall due to the warmer spring and fall 2015 and 2016 both had significantly earlier ice out dates and later ice in dates than in 2014 according to the mndnr ice phenology data there was no significant trend in the ice out dates of wbl from 1979 to 2016 slope 0 156 day year 1 p 0 319 fig 3 the ice out dates retrieved from modis were in good agreement with the mndnr observations modis vs mndnr slope 1 097 r2 0 98 p 0 01 in the period 2000 2016 both ice in dates and ice free days from modis indicate no statistically significant trends from 2000 to 2016 slope 0 561 day year 1 p 0 189 for ice in dates and slope 0 390 day year 1 p 0 676 for ice free days 4 1 2 energy budgets the mean monthly energy balance partitioning is shown in fig 4 the net radiation peaked at 160 w m 2 in june and july and dropped below 20 w m 2 in november except for a short period after ice out e g march 2016 the sensible heat flux was positive upward the sensible heat flux reached its annual maximum 21 25 w m 2 in october in 2015 and 2016 or november in 2014 the latent heat flux was always larger than the sensible heat flux in the ice free season it gradually increased from spring to mid summer and reached a maximum of 110 129 w m 2 between july and august and slowly decreased from 80 to 90 w m 2 in the summer to 30 40 w m 2 in the fall we were not able to measure lake water temperature profiles to adequately calculate changes in the wbl heat storage instead we calculated the energy residual by subtracting the turbulent heat fluxes from the net radiation i e δs rn h le and used it to estimate the heat storage change a positive energy residual indicates an increase in lake heat storage while a negative residual indicates that energy was released and transformed into turbulent heat fluxes here we assumed that our ec measurements underestimated the total turbulent heat flux by 20 based on energy balance closure analyses wilson et al 2002 nordbo et al 2011 from spring to mid summer the positive residuals indicated that the net radiation warmed the lake and increased the heat storage from late summer to fall while the net radiation gradually reduced to zero the increased negative residuals indicated that the heat released from the lake became the energy source driving the turbulent fluxes 4 1 3 daily evaporation patterns the pattern of the 24 h total daily evaporation i e the daily evaporation rate is shown in fig 5 daily evaporation rates were impacted by synoptic scale variabilities in air temperature humidity and wind speed during summer the distinct spikes in evaporation were induced by the passing of cold fronts blanken et al 2000 spence et al 2013 the peaks of evaporation with daily wind speeds 2 m s 1 positive temperature gradients and vapor pressure gradients were typically triggered by the passage of cold fronts during the observation period such events were identified 7 times in 2014 14 times in 2015 and 21 times in 2016 these events occurred in october most frequently 3 times in 2014 4 times in 2015 and 5 times in 2016 a pronounced two week cycle of evaporation in the fall coincided with synoptic scale systems as identified by wavelet time series analyses in 2015 and 2016 figs s4 s6 4 1 4 annual evaporation to quantify the annual evaporation from 2014 to 2016 the primary challenge is to estimate the evaporation that occurred outside of the observation period four approaches were used to help constrain the annual evaporation from 2014 to 2016 table 2 the first estimate simply multiplied the observed length of the ice free period by the measured daily mean evaporation rate the daily mean of the observation period was calculated from the gap filled half hourly data and the ice in and ice out dates were retrieved from the mndnr and the modis data the second approach applied monthly weighting factors to extrapolate the evaporation to the spring and fall periods when observations were not available here we averaged three years of measurements from march to november and calculated the monthly factors weighted by the total evaporation in august through october when our observations were nearly continuous among the three years the other two methods applied clm4 lisss to extrapolate the evaporation beyond the observation period the third estimate used the optimized clm4 lisss model which was forced with local observations the fourth method was derived from the retrospective modeling approach with clm4 lisss forced only with the reanalysis data product the first method resulted in evaporation estimates of 543 mm 915 mm and 778 mm for 2014 to 2016 respectively this simple annual estimate represents an upper bound on evaporation and contains two main uncertainties the observation errors and the errors associated with the extrapolation of the observation data to the early and late ice free periods for example in 2015 our observations captured much of the summer season but missed the shoulder seasons which likely had less evaporation due to lower available energy the second approach estimated evaporation in the three years to be 535 mm 763 mm and 748 mm respectively the annual evaporation estimated from method three was 567 mm 709 mm and 773 mm for the three years finally method four resulted in annual estimates of 590 mm 731 mm and 764 mm overall these estimates were relatively consistent excluding method one for 2015 which resulted in a large overestimate the major difference between the modeled and weighted estimates was caused by the ice phenology the model overestimated the length of the ice free days in 2014 and underestimated the length in 2015 after averaging the four estimates for each year and applying the standard variance as the error of uncertainty we estimated the annual evaporation in the three years at wbl to be 559 22 mm 779 81 mm and 766 11 mm respectively the annual evaporation in 2014 was least among the three years because of a relatively short ice free period combined with lower daily evaporation rates these estimates did not account for sublimation occurring during the ice cover period in winter 2016 2017 we made ec measurements over the ice from the end of january to early february which showed that the monthly latent heat flux was about 5 w m 2 this is equivalent to about 20 mm of evaporation when assuming a 4 month ice cover period sublimation therefore represented 5 of the annual evaporation budget 4 2 retrospective evaporation 4 2 1 model validation the tuned model forced by local observations reproduced the latent heat flux reasonably well from half hourly to monthly time scales fig 6 at the half hourly scale the model explained 74 of the variance and the root mean square error rmse was 35 w m 2 at the daily scale the model performed slightly better with r2 81 and rmse 23 w m 2 the errors are mainly contributed by four summer days with high evaporation on aug 19 2015 aug 24 2015 jun 20 2016 and aug 21 2016 underestimating the daily mean latent heat flux by over 80 w m 2 overall the modeled monthly latent heat flux showed good agreement with observations in spring and fall however modeled evaporation was biased low in most summers and it was underestimated in summer 2016 by about 10 4 2 2 retrospective results the annual water budget terms were calculated based on the retrospective model outputs and the wbl precipitation and water level recorded by mndnr over the period 1979 to 2016 fig 7 the mean annual precipitation was 868 mm with no significant trends during this period the mean annual total evaporation was 688 mm and it increased 3 8 mm year 1 p 0 01 the trend in the annual evaporation was mainly attributed to the increased daily mean evaporation slope 0 01 mm day 1 year 1 p 0 01 during this period the increase of daily mean evaporation was driven by both increased wind speed slope 0 011 m s 1 year 1 p 0 01 and the lake surface vapor pressure difference slope 0 013 hpa year 1 p 0 095 the increase of the vapor pressure difference was mainly driven by the increased air temperature during the ice free period slope 0 020 c year 1 p 0 075 while no significant trends were shown in the forcing specific humidity slope 0 0032 g kg 1 year 1 p 0 51 and the modeled lake surface temperature slope 0 0021 k year 1 p 0 81 the modeling results also show that the length of ice free period was extended slope 0 40 day year 1 p 0 067 over the period 1979 to 2016 with earlier ice out dates slope 0 294 day year 1 p 0 0734 and insignificant earlier ice in dates slope 0 156 day year 1 p 0 319 fig 3 the extension of the ice free period was mainly driven by the increased whole year air temperature slope 0 038 c year 1 p 0 015 the modeled ice out dates are largely matched with the mndnr record modis vs mndnr slope 0 883 r2 0 68 p 0 01 the trend of the measured ice out dates in the mndnr record while it has the same sign as the modeled trend is smaller and of lesser significance slope 1 29 day year 1 p 0 124 the difference between observed precipitation and modeled evaporation p e showed a similar pattern to the yearly wbl water level change linear regression showed that 77 of the lake water level change could be explained by p e slope 1 36 intercept 247 mm year 1 p 0 01 the groundwater inflow to the lake was estimated from the budget residual by assuming a simplified water balance g inflow δl p e where δl is the change in water level and a positive g inflow indicates a flux of groundwater to the lake the residual analyses provided a rough estimate of the exchange between lake water and groundwater the mean annual residual was 211 mm year 1 for the period 1981 to 1990 which is in close agreement 283 mm year 1 r 0 69 p 0 016 with estimates from a hydrologic model that had comprehensive runoff and groundwater components but a rough evaporation component mndnr 1998 the mean annual residual was 183 mm year 1 for the period 1979 to 2016 this indicates that lake water was generally seeping into the aquifer 4 3 projected evaporation the future wbl evaporation under the rcp8 5 greenhouse gas emission scenario is shown in fig 8 under this scenario the annual evaporation is expected to increase mainly due to the longer ice free period in the forcing data the air temperature increased by 0 049 c year 1 p 0 01 and the specific humidity increased by 0 021 g kg 1 year 1 p 0 01 these trends are consistent with the ongoing and expected intensification of the hydrological cycle under a warming climate santer et al 2007 trenberth and asrar 2014 the modeled increase in the ice free period was 0 50 d year 1 p 0 01 with no significant trend in the daily mean evaporation rate the combined effect will result in an increase in annual total evaporation of 1 4 mm year 1over this century by the end of this century the ice cover days are expected to decrease by more than a month and the annual evaporation will increase by about 15 100 mm year 1 equivalent to 9 8 105 m3 of water the lack of a significant trend in daily mean evaporation may seem counterintuitive but most climate models assume supported to this point by evidence that as global temperature increases mean relative humidity is conserved allen and ingram 2002 thus expected increases in mean lake surface vapor pressure driven by the increases in lake surface temperature with slope 0 018 k year 1 and p 0 01 will be accompanied by increases in mean atmospheric vapor pressure so that the lake surface vapor pressure gradient is projected to increase by only 0 0043 hpa year 1 p 0 17 further wind speed is expected to decrease by 0 0019 m s 1 year 1 p 0 01 and the average net radiation during the ice free period is forecast to decrease by 0 039 w m 2 year 1 p 0 099 5 discussion 5 1 observed evaporation 5 1 1 evaporation rates and patterns during the observation period the estimated average daily evaporation rate during the ice free season ranged from 2 6 to 3 4 mm day 1 from wbl table 2 similar to values reported for small temperate closed basin lakes such as sparkling lake and williams lake listed in table 1 the wbl annual evaporation rate ranged from 559 to 779 mm year 1 similar to lake ahnejärv and lake martiska in europe with average annual evaporation rates of 648 mm year 1 from 1970 to 2009 vainu and terasmaa 2014 the patterns of evaporation at wbl showed many features common to other closed basin lakes table 1 similar dynamics between heat storage changes and turbulent heat fluxes have been reported previously for a broad range of lake types e g blanken et al 2000 lenters et al 2005 li et al 2007 gianniou and antonopoulos 2007 spence et al 2013 wang et al 2014 wang et al 2017 a one month phase lag between latent heat flux and net radiation was also observed over sparkling lake a small temperate closed basin in the upper midwest us lenters et al 2005 while a lag of two to three months has been observed over large deep lakes such as lake superior in the us blanken et al 2000 and lake qinghai in china li et al 2007 the energy stored in wbl from may to july and released from september to november was similar to that observed from a small plateau closed basin lake in china wang et al 2017 however the influence of the length of the ice free period on annual and inter annual variation in evaporation from temperate closed basin lakes was not well characterized in the studies summarized in table 1 our study found that the ice phenology is an important factor for estimating the annual total evaporation and it is an important causal factor of the inter annual differences in annual evaporation the annual total evaporation eannual is the product of daily mean evaporation rate in the ice free period eday and the length of ice free days lice free here we define p q and r as the increase ratios of the daily mean evaporation rate the length of ice free period and the total annual evaporation respectively 3 e annual e day l ice free 4 1 r e annual 1 p e day 1 q l ice free 1 p q p q e day l ice free the term pq is the increased rate caused by the interaction of daily mean evaporation and length of ice free period normally it is a second order term and can be ignored compared with 2014 the annual total evaporation at wbl increased by about 40 in 2015 while the daily mean evaporation increased by about 10 and the length of ice free period increased by about 25 therefore the ice free period contributed about 60 to the increase in the annual evaporation given a typical daily mean evaporation rate of 2 6 mm day 1 and a length of ice free period of 250 days a 1 error represent a 0 026 mm day 1 in the daily mean evaporation and 2 5 days in the length of ice free period without accurate in situ record of ice phenology of a lake it is very likely to have a 2 5 day error in the length of ice free period 5 1 2 lake evaporation and pan evaporation fig 9 shows the comparison between monthly evaporation at wbl and the evaporation from a class a evaporation pan 18 km away from wbl on the st paul campus at the university of minnesota the evaporation measured by ec at wbl was greatest in july and august and the fall evaporation was more active than spring i e the shape of the distribution was unimodal and left skewed the pan evaporation was greatest in july while the spring evaporation was larger than fall i e unimodal but right skewed and it exhibited a larger seasonal variability compared to the lake these patterns indicate that the pan evaporation was mainly driven by variability in solar radiation the lake to pan evaporation ratio showed large monthly and inter annual variability for example the ratio increased from 0 5 to 0 9 from may to september and the ratio was larger than 1 in april and october the august ratio varied between 0 66 and 0 96 for the three years the annual total pan evaporation was 864 mm 853 mm and 877 mm from 2014 to 2016 respectively these values were considerably larger than the gap filled ec measurements and showed relatively little inter annual variability the lake to pan evaporation ratio for the annual total was 0 65 0 91 and 0 87 for these three years respectively these analyses demonstrate that it is difficult to derive a single coefficient between annual pan evaporation and wbl evaporation it should be noted that the variability in the length of the ice free period can have a nonlinear effect on these ratios making it impractical to obtain a representative annual value for a temperate lake 5 2 retrospective evaporation in the past 40 years wbl has experienced two significant water level declines 1986 to 1990 and 2003 to 2012 fig 8 both periods coincided with years with less precipitation and greater evaporation than the average our residual analyses suggest that the flux of lake water entering groundwater was larger than the average over the same period this indicates that the groundwater aquifer was also at relatively low levels due to the low precipitation further while the entire region was in a drought during these periods groundwater pumping was also estimated to be greater due to agricultural and municipal usage jones et al 2013 for example over the 10 year period 2003 to 2012 mean annual precipitation was 822 83 mm year 1 mean standard deviation the modeled annual evaporation was 756 79 mm and the derived mean seepage to groundwater was 244 132 mm year 1 for regional context we compared the modeled retrospective annual evaporation at wbl to our long term ec measurements of evapotranspiration et at corn soybean sites in rosemount mn at the southern edge of the twin cities metropolitan area baker and griffis 2005 baker et al 2012 griffis et al 2016 over the same period 2003 to 2012 the mean annual et was 522 52 mm over croplands the mean annual et for corn and soybean systems were not statistically different with mean annual values of 507 53 mm and 534 52 mm respectively year 2003 and 2012 were two notable drought years within the region the modeled lake evaporation for 2003 and 2012 was 723 mm and 917 mm respectively while the pan evaporation for these two years was 879 mm and 1031 mm respectively the annual et from crops was about 100 mm lower than the 10 year average for corn soybean with minimum values of 423 and 472 mm in 2003 and 2012 respectively these relatively low et rates highlight how declining soil water content and drought stress likely increased the demand for irrigation it is likely that such drought periods have exacerbated groundwater usage and have contributed to the decline in groundwater and lake water level within the region jones et al 2013 5 3 projected evaporation given that lake evaporation is affected by multiple atmospheric variables and processes long term trends in those variables are critical in determining the trend in daily evaporation rate and annual total evaporation the forcing data under rcp8 5 scenario indicate that air temperature and humidity will rise while wind speed will decrease fig 8 correspondingly the lake water surface temperature is expected to enhance the vertical vapor pressure gradient these recent regional trends are consistent with the observed global trends over the past decades willett et al 2008 mcvicar et al 2012 o reilly et al 2015 although the projected modeling results indicate that the effects of changes in wind speed vapor pressure difference and net radiation might be compensatory with respect to their influence on the daily mean evaporation rate the change in length of the ice free period will be a major factor driving increases in the annual total evaporation the relation between evaporation and ice phenology deserves more research to understand the feedback processes and the potential amount of inter annual variability in evaporation from temperate closed basin lakes we note that wind speed forcing data has opposite trends in the retrospective modeling and the projected modeling these trends are consistent with previous studies holt and wang 2012 kulkarni and huang 2014 ashtine et al 2016 the mechanism underlying these trends is largely driven by changes in the atmospheric circulation and require further investigation hobbins et al 2012 found that the most important drivers of the atmospheric evaporative demand for our study area were air temperature specific humidity downwelling shortwave radiation downwelling longwave radiation and wind speed this indicates that compared with the wind speed the vapor pressure difference at the lake surface which is mainly influenced by both air temperature and specific humidity is more critical in affecting the daily lake evaporation rate in the region the projected modeling results indicate that increasing rate of evaporation is likely to outpace changes in precipitation it is estimated that annual evaporation at wbl will increase by 1 4 mm year 1 year 1 p 0 01 over this century while the annual precipitation is expected to increase by about 1 1 mm year 1 year 1 p 0 06 furthermore there will be increasing probability of more extreme large evaporation years compared with current climate status overall we expect decreases in the moving average of wbl lake water level and increases in lake level variability 5 4 regional implications the mississippi river twin cities watershed has more than 1 8 million people and over 250 lakes anderson et al 2013 many of these lakes have shown high sensitivity to variations in climate and water use our observations and model analyses support that lake levels within the region are closely coupled to evaporation and that future scenarios show a tendency for increased likelihood of lower water levels and more extreme fluctuations in lake levels unfortunately detailed observations and modeling for such lakes remains rare and therefore their utility in effectively managing these water resources remains limited there is an increasing need for developing comprehensive water management strategies to address the potential impacts of extreme climate events and climate change in light of increasing population and growing demand for these water resources understanding and forecasting changes in the magnitude of evaporation cannot be ignored for wbl the water loss from a typical evaporation rate of 5 mm per day during the summer is equivalent to 4 9 104 m3 of water or roughly 0 5 m3 s 1 of continuous 24 h pumping an advanced ice out date or postponed ice in date of just one day is likely to result in an additional water loss of 2 104 m3 thus small changes in the evaporation rate or ice phenology can have significant impacts on available water for these communities proposed water augmentation strategies that are designed to compensate for declining lake levels within the region for the present climate must be aware of the potential changes in supply and demand as climate continues to warm further according to recent data from the usgs per capita water use in minnesota is about 0 23 m3 per day maupin et al 2010 the additional 100 mm of evaporation at wbl resulting from the long term change in climate is equivalent to the annual water use of over 11 000 people and further highlights the need for sound water use management strategies the combination of long term evaporation measurements and modeling for sentinel lakes should be adopted as a strategy to help protect lakes and the ecosystem services that they provide the annual changes in water level of wbl showed strong coherence to changes in the regional lake water levels from 1925 to 2016 fig 10 the changes in wbl water level and the median changes in the regional lake water levels exhibited very similar statistical characteristics including the overall sign they were also significantly correlated with a coefficient of 0 68 p 0 01 as a closed basin lake wbl also showed a larger variation in the water level than the regional median value but the water level changes of wbl were usually within the ranges bounded by the whiskers of the box plot such coherence implies that the regional lake water levels were collectively impacted by the large scale synoptic and climatic drivers influencing the regional precipitation and evaporation previous studies have shown that decadal changes in water levels of closed basin lakes in north america were correlated to large scale climate teleconnections such as the atlantic multidecadal oscillation amo and the north atlantic oscillation nao hanrahan et al 2010 hanrahan et al 2014 watras et al 2014 these teleconnections can help us further understand the inter annual relationship between climate and regional water balance for example previous research has shown that the 2009 2010 el niño winter induced higher than normal air temperature in north america and mild ice conditions over the great lakes bai et al 2011 in our study period the increases in mean annual air temperature and precipitation from 2014 to 2016 in the twin cities and the changes in their patterns were potentially related to a phase shift in the el niño southern oscillation enso from a neutral to el niño state correspondingly we observed a relative longer ice free season and larger annual evaporation at wbl in 2015 and 2016 than 2014 thus long term projections of lake evaporation will also be influenced by the frequency and persistence of large scale atmospheric circulations however the influence of climate change on such teleconnections is poorly understood hanrahan et al 2010 therefore more observations and modeling studies are needed to improve our understanding of these factors 6 conclusion 1 daily evaporation at wbl was strongly influenced by synoptic scale variability the heat released from the lake was the main energy source for evaporation in the fall the annual evaporation at wbl from 2014 to 2016 was 559 22 mm 779 81 mm and 766 11 mm respectively the annual evaporation in 2014 was least among the three years due to its relatively short ice free period and its relatively lower daily evaporation rate 2 the retrospective analyses indicated that wbl annual total evaporation increased by about 3 8 mm year 1 from 1979 to 2016 which was attributed to increased daily mean evaporation during the ice free period the increase of daily mean evaporation was driven by the increased wind speed and lake surface vapor pressure gradient the lake level declines at wbl during 1986 1990 and 2003 2012 were caused by the coupled low precipitation and high evaporation this finding implies that a regional drought and potential intensified groundwater use can have a dramatic impact on water level at a closed basin lake 3 model results suggest that annual evaporation at wbl will increase 1 4 mm year 1 over this century under the rcp 8 5 scenario which is largely driven by the extended ice free periods at the end of this century the ice cover period will shorten by more than a month and the annual evaporation will increase by an equivalent 9 8 105 m3 of water 4 our observations and model analyses support that lake levels within the region are closely coupled to evaporation lake evaporation is expected to increase due to the extended ice free period as climate continues to warm a tendency for increased likelihood of lower water levels and greater fluctuations in water level for wbl and other lakes within the region is expected acknowledgements we acknowledge the nicholson family and the becker family for allowing us to access white bear lake through their private property financial support and scientific instruments were provided by ameriflux the minnesota corn research and promotion council grant number 4101 15sp and the minnesota dnr usda ars provided maintenance and logistical support for this project finally we acknowledge the white bear lake conservation district for their interest and continued support we express our since thanks to editor tim r mcvicar associate editor joshua larsen and two reviewers peter blanken university of colorado boulder colorado united states and one anonymous reviewer for their helpful comments and suggestions all data from this study will be made available at https www biometeorology umn edu research data archives appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 03 059 appendix a supplementary data supplementary data 
7263,the high solubility and conservative behaviour of chloride make it ideal for use as an environmental tracer of water and salt movement through the hydrologic cycle for such use the spatial distribution of chloride deposition in rainfall at a suitable scale must be known a number of authors have used point data acquired from field studies of chloride deposition around australia to construct relationships to characterise chloride deposition as a function of distance from the coast these relationships have allowed chloride deposition to be interpolated in different regions around australia in this paper we took this a step further and developed a chloride deposition map for all of australia which includes a quantification of uncertainty a previously developed four parameter model of chloride deposition as a function of distance from the coast for australia was used as the basis for producing a continental scale chloride deposition map each of the four model parameters were made spatially variable by creating parameter surfaces that were interpolated using a pilot point regularisation approach within a parameter estimation software the observations of chloride deposition were drawn from a literature review that identified 291 point measurements of chloride deposition over a period of 80 years spread unevenly across all australian states and territories a best estimate chloride deposition map was developed from the resulting surfaces on a 0 05 degree grid the uncertainty in the chloride deposition map was quantified as the 5th and 95th percentile of 1000 calibrated models produced via null space monte carlo analysis and the spatial variability of chloride deposition across the continent was consistent with landscape morphology the temporal variability in chloride deposition on a decadal scale was investigated in the murray darling basin this highlighted the need for long term monitoring of chloride deposition if the uncertainty of the continental scale map is to be reduced use of the derived chloride deposition map was demonstrated for a probabilistic estimation of groundwater recharge for the southeast of south australia using the chloride mass balance method keywords atmospheric chemistry rainfall groundwater recharge spatial variability uncertainty analysis 1 introduction chloride deposition occurs in the landscape by the mechanisms of rainfall washout of dissolved chloride ions cl and dry deposition of ions attached to dust particles gustafsson and franzen 1996 previous work e g hutton 1976 keywood et al 1997 has shown that distance from the coast wind and rainfall are all factors which contribute to the amount of cl deposition on land surfaces the high solubility chemical stability and environmentally conservative nature of the cl ion make it an ideal tracer for estimating the average diffuse net recharge to aquifers from rainfall using the chloride mass balance method the chloride mass balance method was first used by anderson 1945 near mount gambier in south australia and is the most widely used method for estimating recharge in australia crosbie et al 2010 and globally at least in semi arid and arid areas scanlon et al 2006 in addition to estimating recharge data concerning salt mass in rainfall are essential for numerous other applications these include i both point scale and basin scale salinity modelling ruprecht and sivapalan 1991 walker et al 2002 ii calculations of salt mass balances in groundwater within catchments herczeg et al 2001 tolmie et al 2004 iii investigating hydrologic equilibrium following land clearing peck and hurle 1973 jolly et al 2001 iv prediction of atmospheric corrosion meira et al 2008 suzuki and robertson 2011 and v quantifying air pollution delalieux et al 2006 there have been many investigations of major ions in rainfall carried out in australia over the last 80 years these have included studies into the chloride analysis of rainfall concentration and deposition to investigate impacts on stream water quality douglas 1968 soil salinity hingston and gailitis 1976 and atmospheric corrosion slamova et al 2012 previous investigations concluded that the source of salts in soils and groundwater was atmospheric rather than from the weathering of rocks willsmore and wood 1929 teakle 1937 herczeg et al 2001 distance from the coast keywood et al 1997 orographic guan et al 2010 and atmospheric factors have more recently been examined in order to try and explain the spatial variability of chloride deposition in the landscape the role of vegetation in enhancing chloride deposition rate ulrich 1983 deng et al 2013 has also been studied the work of deng et al 2013 and bresciani et al 2014 was able to quantify the chloride deposition enhancement for a coastal forested environment at a local scale these effects are less obvious when considering chloride deposition at the broader continental scale regional scale investigations have shown the input of chloride varied within years at coastal localities and was most dependent upon localised conditions such as wind speed and direction hingston and gailitis 1976 farrington et al 1993 several authors have highlighted the need for seasonally averaged long term data to prevent these short term variances from biasing estimates of chloride deposition blackburn and mcleod 1983 hingston and galbraith 1990 ideally rates of chloride deposition would be measured by a long term monitoring program such as the national atmospheric deposition program in the usa lamb and bowersox 2000 unfortunately most of the rest of the world has to rely on upscaling sparse infrequent field studies eriksson and khunakasem 1969 allison and hughes 1978 claassen and halm 1996 sami and hughes 1996 minor et al 2007 these upscaling methods assume a uniform distribution of chloride deposition rate over a broad spatial resolution and do not take into account the highly variable nature and the associated uncertainty in deposition a number of studies have been conducted to model the chloride deposition using a variety of relationships keywood et al 1997 gustafsson and hallgren larsson 2000 more recently guan et al 2010 attributed 70 of spatial variability in bulk chloride deposition to the effect of coastal distance in a study located in the mt lofty ranges south australia there have been numerous models created for predicting the chloride deposition rate as a function of distance from the coast in australia hutton 1976 hingston and gailitis 1976 keywood et al 1997 these relationships are regional in nature and they have often been created using datasets specific to one region the relationships are subsequently used in other data poor regions by other researchers without considering whether this is appropriate thus highlighting the need for a continental chloride deposition map keywood et al 1997 identified that their model fit the data well in a west east array across western australia and the southern half of a south north array across western australia but did not fit the northern section of this array this observation was attributed to lower wind velocities in the north mcvicar et al 2008 not carrying the aerosols as far inland meaning that the fast decay was not as dominant as it was in the south this limitation of the model was partially overcome by vleeshouwer et al 2009 who split australia into six climate zones and fitted individual models for each climate zone an alternative approach in predicting the distribution of chloride deposition is to apply kriging models to field sampled data sets delalieux et al 2006 used kriging methods matheron 1965 cressie 1993 to map the cl and na contents of total wet dry deposition on a grid cell mesh 5 5 km based on some 91 observations over southwest europe alcalá and custodio 2008 mapped the bulk deposition of chloride across spain using ordinary kriging delalieux et al 2006 noted the reduction in chloride deposition with distance from the coast but warned on using their results very close to the coast as their data was inland and the kriging model did not incorporate this relationship a selection of these studies that have produced chloride deposition maps is shown in table 1 this shows that both regression and kriging approaches have been previously used at a continental scale but they have not been combined and uncertainties characterised at this scale the objective of this study is to map the spatial distribution of the average annual bulk chloride deposition rate across australia combining the best features of the regression and interpolation approaches and to provide a measure of its uncertainty specifically this study will i collate the existing measured data on chloride deposition across australia ii use this data to create a continental scale chloride deposition map with its associated uncertainty and iii provide an example of using this chloride deposition map to estimate groundwater recharge in the southeast of south australia these objectives provide the structural sub headings used in the following methods results and discussions sections 2 methods 2 1 observations of chloride deposition across australia a review of the available literature produced a total of 291 point measurements of chloride deposition in rainfall across australia ranging in dates from 1933 to 2010 and for sampling periods ranging from 2 months to 5 years fig 1 a where the chloride flux load was not reported it was calculated as the rainfall weighted average 1 d cl i p i p i 10 2 where d is chloride mass kg ha 1 yr 1 cl i is rainfall chloride concentration for time period i mg l 1 and pi is rainfall in time period i mm there were a few studies that reported the rainfall weighted average chloride concentration but not the rainfall in these cases the mean annual rainfall jeffrey et al 2001 was used to calculate the chloride flux from the rainfall weighted average chloride concentration where 2 d cl p 10 2 where d is chloride mass kg ha 1 yr 1 cl is average chloride concentration of the rainfall mg l 1 and p is annual average rainfall mm yr 1 the details of the 291 point measurements of the chloride deposition in rainfall are listed in the supplementary material an additional 20 measurement locations of chloride deposition in rainfall reported in crosbie et al 2012 were omitted from this dataset and subsequently used to provide an independent evaluation of model results fig 2 previous studies have shown that distance from the coast yaalon and katz 1962 hutton 1976 naranjo et al 2015 rainfall neal and kirchner 2000 windspeed alcalá and custodio 2008 castañeda 2016 elevation slope and aspect guan et al 2010 are useful predictors of chloride deposition these factors which are available from continental datasets jeffrey et al 2001 mcvicar et al 2008 gallant et al 2011 gallant and austin 2012a b are assessed through correlation analysis to determine if they are useful predictors of chloride deposition on a continental scale 2 2 a continental scale chloride deposition map the regression method of spatial interpolation used by vleeshouwer et al 2009 incorporated the relationship between chloride deposition and distance from the coast for multiple climatic zones but had limitations in edge effects where climatic boundaries were used to account for spatial trends kriging is useful in spatial interpolation in that it can account for the spatial variability at both the large scale and the small scale by modelling both the spatial trend and spatial correlation of the data however the kriging method implemented by delalieux et al 2006 was limited by not incorporating the relationship with distance from the coast explicitly resulting in questionable results in areas without data this study will combine the best of these two approaches by spatially interpolating the parameters of the regression model to enable a seamless chloride deposition map to be created that respects both the local and regional spatial trends in the observed chloride deposition data the method used here to create a continental map of chloride deposition is shown diagrammatically in fig 3 and can be summarised into several steps i development of the regression model ii calibration of the regression model parameters iii assessing the uncertainty in the continental scale map of chloride deposition iv an assessment of the model performance against an independent validation data set and v an assessment of how the temporal variation decadal in chloride deposition effects the long term average chloride deposition map produced here these steps will be addressed in turn in the following sub sections 2 2 1 regression model a number of studies have been conducted exploring the distribution of chloride deposition using regression modelling to describe the relationship between deposition of chloride and distance from the coast both in australia hutton 1976 farrington et al 1993 biggs 2004 and elsewhere yaalon and katz 1962 neal and kirchner 2000 meira et al 2006 these have largely been developed from local observations of chloride deposition e g catchment scale keywood et al 1997 suggested that the primary deposition of chloride across australia could be described in terms of fast and slow deposition rates with the rate that dominates being dependent upon distance from the coast the fast portion of this relationship is characterised by rapid removal of chloride near the coast and has an exponential decay constant of 60 km the slow portion is characterised by a much slower decline in chloride deposition with distance and has a decay constant of 700 km the total chloride mass deposited by the fast fraction of deposition is generally double that deposited by the slow fraction the keywood et al 1997 model has some theoretical justification for the use of the sum of two exponential decay functions with distance from the coast namely a fast and slow decay component the fast component is conceptualised as the wet and dry deposition of chloride in aerosols sourced from the ocean the slow component is the deposition of gaseous chloride that is formed by volatilisation of the chloride in sea salt at a ph below 3 the model of keywood et al 1997 is 3 d a 1 e d λ 1 a 2 e d λ 2 where d is chloride deposition rate kg ha 1 yr 1 a 1 and a 2 are coefficients kg ha 1 yr 1 λ 1 and λ 2 are decay constants km and d is distance from the coast km 2 2 2 model calibration combining the best of previous approaches to map chloride deposition the observations of chloride deposition are fitted to a surface generated using eq 3 in which the four parameters a 1 a 2 λ 1 and λ 2 are themselves surfaces that are calibrated using the pilot points method details given below this strategy avoids the edge effects of zones and allows for the spatial distribution of the fitting parameters to be data driven rather than prescribed a priori the calibration of the model to the measured rates of chloride deposition was conducted using pilot points regularisation in pest doherty 2010 the use of pilot points allows for spatial variability in the four model parameters which in a highly parameterised inversion can lead to an almost perfect fit between the model and the observations however a perfect fit between the model and the observations would almost certainly result in unrealistic parameter surfaces because the uncertainty in the observations is not taken into account nor are the mechanisms responsible for the changes in the parameter surface regularisation applies an additional constraint into the objective function whereby the difference in the parameter values between pilot points is minimised but allowed to vary only to the point of realising an acceptable degree of misfit between the observations and simulations this ensures that any spatial variability in the parameter surface is there because it needs to be to fit the data and ensures that the resulting parameter surface is as realistic as possible the location of the pilot points were selected to cover the geographical domain of the model and were biased to areas with a higher density of observations fig 1b a total of 150 pilot points were used in the modelling approximately half the number of observations which results in 600 parameters to be calibrated ordinary kriging was used to interpolate the parameter values at the pilot points to a regular grid at a resolution of 0 05 degree approximately 5 5 km the exponential semivariogram parameters used in the ordinary kriging were optimised iteratively with the model parameters at the pilot points using esri s arcgis geostatistical analyst this enabled interpolation of the parameters to a regular grid the optimised parameter values at the pilot points were then again fitted to the semivariogram to ensure that the parameters were still appropriate after the pest optimisation the pest optimisation aimed for a specified sum of squared residuals ssr between the weighted and log transformed fitted and observed values of chloride deposition subject to the constraint that the parameter surfaces be as smooth as possible using a tikhonov regularisation the weights used on the observations were based on the assessment of the field data with an assigned weighting from 0 to 1 0 according to their reliability and length of study with 0 being assigned to short term measurements of uncertain derivation and 1 0 assigned to long term measurements of verifiable derivation through trial and error an acceptable level of misfit was determined to be a ssr of 7 a lower ssr is possible with pest but resulted in parameter surfaces that contained bullseyes to enable a fit to observed data that has its own uncertainty a higher ssr allows for smoother parameter surfaces that begin to lose some of the spatial structure that can be explained through physical processes see discussion section 4 2 there is a subjective choice to be made in the acceptable level of misfit that is ideally informed by the uncertainty in the observations of chloride deposition a ssr of 7 translates to an average misfit of 0 15 log kg ha 1 yr 1 which is comparable to the differences in chloride deposition measured at the same location in different studies see results section 3 2 3 this calibration procedure resulted in a best estimate map of chloride deposition across australia 2 2 3 uncertainty in predicted chloride deposition the highly parameterised inversion used by pest doherty 2010 results in a non unique solution in the parameter space this can be used to our advantage for investigating the predictive uncertainty in the map of chloride deposition this is achieved using the null space monte carlo technique tonkin and doherty 2009 within pest this technique splits the parameter space into the solution space and the null space the null space is parameter combinations that have no effect upon the calibration but contribute to predictive uncertainty parameter combinations in the null space can be generated stochastically to produce multiple replicates of equally well calibrated models for this study we have generated 1000 replicates of the continental scale chloride deposition these are reported as 95 confidence limits around our best estimate and also the descriptive statistics of the mean standard deviation coefficient of variation and skewness of the 1000 replicates 2 2 4 model validation work by crosbie et al 2012 reported new data for the chemical composition of rainfall at 20 sites across australia over a period of 4 years fig 2 these data points were used to provide an independent evaluation of model results the values of predicted chloride deposition from the continental scale map and its uncertainty were extracted at each of the 20 locations monitored by crosbie et al 2012 and plotted in a scatterplot to assess the accuracy of the continental scale mapping the validation dataset was also compared to the more conventional approach of producing a chloride deposition map using ordinary kriging gustafsson and hallgren larsson 2000 delalieux et al 2006 alcalá and custodio 2008 ordinary kriging of the 260 points with a weighting of greater than 0 2 was performed to produce a map on the same 0 05 degrees grid as the map produced using the keywood et al 1997 model 2 2 5 temporal variations in chloride deposition it is commonly assumed that collecting chloride deposition data over a complete year is the minimum requirement to estimate a chloride deposition rate blackburn and mcleod 1983 biggs 2006 sweeney et al 2016 this can be tested where there are data collected at different times in the same location katherine in the northern territory is the best example from our dataset which has been the site of five independent recorded studies of deposition of chloride ranging in duration from 1 to 5 years wetselaar and hutton 1963 galloway et al 1982 likens et al 1987 keywood 1995 wilson et al 2006 and with a range of values of deposition from 2 46 to 7 30 kg ha 1 yr 1 this compares to a value of 4 06 kg ha 1 yr 1 for the modelled chloride deposition in this study it is clear that there is substantial temporal variation between studies at this location over many locations and independent studies these differences may be averaged out by fitting to a continental scale model to test how this will affect the fitting of the keywood et al 1997 model over the continent the murray darling basin fig 2 was selected for investigation the 94 estimates of chloride deposition were split into those collected in years with above average rainfall wet and below average rainfall dry these wet and dry periods were compared to the model fitted to the whole dataset 2 3 example of estimating groundwater recharge the chloride mass balance method of estimating recharge is the most frequently used method in australia crosbie et al 2010 this is because it is simple to implement and the data requirements are not onerous the method works by comparing the concentration of chloride in groundwater to that of rainfall the ratio of the concentration of chloride in rainfall to groundwater gives the proportion of rainfall that becomes recharge this can be reformulated as 4 r 100 d cl gw where r is groundwater recharge mm yr 1 d is the chloride deposition due to rainfall kg ha 1 yr 1 and clgw is the chloride concentration of groundwater mg l 1 the assumptions behind this methodology have been explored in detail by wood 1999 australian state and territory governments have collected vast amounts of data on the chloride concentration of groundwater but what has previously been lacking is a chloride deposition map with associated uncertainty in an easily accessible format an example of using the new chloride deposition map for estimating recharge in a probabilistic manner is presented for the southeast of south australia while the chloride mass balance method has been widely used to estimate spatial distributions of recharge e g eriksson and khunakasem 1969 scanlon et al 2012 the uncertainty associated with both chloride deposition rates and groundwater chloride concentrations are rarely propagated to quantify the uncertainty associated with estimated rates of recharge alcalá and custodio 2015 crosbie et al 2018 in practice stochastic methods can be used to generate many replicates of an estimated recharge map through random sampling of the input probability distributions in the present study 10 000 replicates were created through sampling of a pearson type iii probability distribution of chloride deposition rates and a log normal distribution of groundwater chloride concentrations the chloride in groundwater data was obtained from the databases held by state governments in south australia and victoria in locations with multiple measurements the geometric mean was used it was assumed that the chloride concentration of groundwater measured in a bore is representative of the chloride concentration at the water table ordinary kriging was used to interpolate log transformed point measurements to a regular grid this interpolated grid and its associated standard error were used to create 10 000 replicates of the spatial distribution of chloride in groundwater 3 results 3 1 observations of chloride deposition across australia there were a total of 291 data points available for developing the continental scale chloride deposition map of these 88 had a measurement period of at least 12 months and none had a measurement period longer than 5 years fig 4 shows scatterplots of the chloride deposition against the predictors i distance from the coast ii elevation iii difference in angle between the aspect of the slope and direction to the coast iv mean annual rainfall v mean annual windspeed and vi slope from the data that we have available it seems as though distance from the coast is the only significant predictor of chloride deposition this supports the use of the keywood et al 1997 model and so only the distance from the coast will be used in the regression model 3 2 a continental scale chloride deposition map 3 2 1 regression model analysis of our dataset showed that the only significant predictor of the chloride deposition is the distance from the coast therefore the regression model of keywood et al 1997 will be used in this study 3 2 2 model calibration the weighted sum of squared residuals between observed and fitted chloride deposition for the best calibrated model was 6 998 this corresponds to an rms error of 0 16 log transformed kg ha 1 yr 1 14 of the mean which is comparable to the difference in observations between sites that have multiple studies at the same location the constraint imposed by the regularisation process was that the ssr had be less than 7 whilst minimising the difference in values of the parameters at the pilot points so there is a penalty imposed on the calibration for minimising the ssr any further a comparison of measured and modelled chloride deposition for the 291 observations across australia are shown in fig 5 overall the modelled chloride deposition appears to account well for the variation of the observed data with an r2 0 91 the pest parameterisation of the 4 keywood et al 1997 model parameters are shown in fig 6 the best estimate of the chloride deposition across australia is presented in fig 7 along with the 5th and 95th percentiles these datasets are available for download via the csiro data access portal davies and crosbie 2014 the mean bulk deposition rate of chloride varies from 8 3 to 139 7 kg ha 1 yr 1 in coastal areas defined here as being within 50 km from the nearest coastline in the centre of australia chloride deposition rates vary from 0 7 to 2 6 kg ha 1 yr 1 3 2 3 uncertainty in predicted chloride deposition the quantified uncertainty in the modelled chloride deposition is presented as the mean standard deviation coefficient of variation and skewness of 1000 equally well calibrated models fig 8 these descriptive statistics can then be used to generate a probability distribution of chloride deposition for any location in australia that can be used for further studies the standard deviation is not a particularly useful measure of the comparative spatial uncertainty in the predictions of chloride deposition in this study because of the large range in the mean values 3 orders of magnitude the coefficient of variation i e the standard deviation divided by the mean is the more useful statistic in this case this plot shows that the greatest relative uncertainty is in locations with the least data central australia inland tasmania east and north queensland and locations with a high density of data show the lowest relative uncertainty southwest western australia southeast south australia and victoria this is to be expected as the modelling process is data driven the areas with a high coefficient of variation also tend to have a skewed distribution to the right this is probably caused by the lack of data and calibration being performed on log transformed data 3 2 4 model validation as part of a study looking at the chemical and isotopic composition of rainfall across australia crosbie et al 2012 analysed rainfall samples at 20 locations over a period of 4 years of the 20 measurements 9 sites are determined to have a higher chloride deposition and 11 sites are determined to have a lower chloride deposition when comparing the modelled chloride deposition to the measured chloride deposition table 2 and fig 9 the rms error across the 20 sites is 0 35 log transformed kg ha 1 yr 1 26 which is higher than the calibration statistics this could be indicative of over fitting the data and therefore under estimating the uncertainty the two worst fits for the validation data are for cape grim and alice springs cape grim rainfall collector was only 20 m from the coast on the cliff top facing the roaring forties westerly winds across the southern ocean this is an extreme case of chloride deposition coupled with the limited resolution of the 0 05 degrees 5 km grid cells used in the model in a previous study in alice springs which was part of the calibration dataset the chloride deposition was recorded as 1 1 kg ha 1 yr 1 keywood et al 1997 which is much lower than the 3 1 kg ha 1 yr 1 used in the validation dataset table 2 this could be indicative of needing a longer term monitoring program to enable a better estimate of the average chloride deposition the chloride deposition map produced using ordinary kriging fig 10 is similar to the map produced using the keywood et al 1997 model fig 7 in the areas of dense observation data in the se and sw of the continent fig 1 in areas without dense data the map produced using ordinary kriging does not fit with our understanding of the physical processes there are bullseyes on the east coast where there is some data and particularly the east coast of tasmania does not have an increase in chloride deposition toward the coast when compared to the validation dataset of crosbie et al 2012 fig 10 the ordinary kriging map does not perform as well as the map produced using the keywood et al 1997 model fig 9 the ordinary kriging map has a low bias with 4 of the 20 points being greater than the observations and having an overall rms error of 0 40 log transformed kg ha 1 yr 1 3 2 5 temporal variations in chloride deposition our dataset includes several locations for which there has been more than one study of chloride deposition conducted and which show a temporal variation between studies we investigated this further using 94 estimates of chloride deposition across the murray darling basin which were split into those collected in years with above average rainfall wet and those collected in years with below average rainfall dry these wet and dry subsets were fitted to the model with a single parameter set each the wet subset has greater chloride deposition than the entire dataset and the dry subset has less chloride deposition than the entire dataset fig 11 the difference in the subsets fitted values compared to the entire dataset is greatest at the coast with a difference of 12 kg ha 1 yr 1 for both subsets with the difference becoming smaller with increasing distance inland but the percentage difference increases this demonstrates that using many independent studies to create a continental scale chloride deposition map will average out the wet and dry studies however this is dependent upon having a dense spatial network this only occurs in the southwest and southeast of the continent fig 1 the uncertainty in the chloride deposition map fig 7 is probably under estimated in areas outside of this dense network of observed data 3 3 example of estimating groundwater recharge the study by crosbie et al 2015 compared three methods of estimating recharge over part of the otway and murray basins in southeastern australia including the chloride mass balance method the previous study used point estimates of recharge this will be extended to estimate recharge on a regular grid there are 3901 point locations with measurements of chloride in groundwater within the area of interest fig 12 the estimated recharge was reported as the 5th 50th and 95th percentiles at each grid cell from the 10 000 replicates results indicate that areas of highest recharge are located in the south of the study area i e along the coast conversely areas of lowest recharge are located toward the north of the study area i e inland using the chloride mass balance approach the average recharge rate across the study area is estimated at 21 mm yr 1 for the 50th percentile with a plausible range of 13 34 mm yr 1 based upon the 5th and 95th percentiles with the 95th percentile being nearly three times the magnitude of the 5th percentile the uncertainty associated with the best estimate of recharge is quite significant and should be important for water resource managers to understand 4 discussion 4 1 observations of chloride deposition across australia gains in reducing the uncertainty of predicting the chloride deposition across the continent could be made by instigating a long term monitoring program such as the national atmospheric deposition program in the usa lamb and bowersox 2000 it has been shown here that wet and dry periods on a decadal scale do produce a different relationship between chloride deposition and distance from the coast a more representative long term average chloride deposition rate at a point scale would be a major improvement in reducing the uncertainty in a continental scale chloride deposition map orographic effects have been studied by other researchers for its effect on chloride deposition a positive correlation has been observed in the mt lofty ranges australia guan et al 2010 and the greater tamar catchment australia sweeney et al 2016 in contrast a negative correlation has been observed in hawaii miller et al 1984 and no correlation has been observed in allt a mharcaidh catchment scotland ferrier et al 1990 from the data collated here there is a very weak negative correlation between chloride deposition and elevation elevation may be a useful predictor of chloride deposition at the local to regional scale but from the data collated here that is not the case at the continental scale 4 2 a continental scale chloride deposition map the chloride deposition rate is a minimum in central australia which is as expected due to the insulation of any effects of marine air masses supplying chloride keywood et al 1997 farrington and bartle 1988 recorded a considerable variation in chloride deposition from rainfall between years of a 5 year study at gnangara mound they noted that this was likely due to the variation in number or intensity of storm events during the winter and spring the transport of chloride as aerosols from oceanic spray by prevailing onshore westerly winds was noted as increasing during such storm events farrington and bartle 1988 the effect of prevailing westerly winds is seen in the spatial distribution of the fast component a 1 fitting parameter fig 6 noticeably inland from southwest western australia southeast south australia and western victoria as well as inland tasmania this effect is also seen in the spatial distribution of the fast component λ1 decay parameter fig 6 with decay constants of 75 km it is uncertain if an associated prevailing onshore wind effect is the cause of increased a 1 inland from the northeast coast of queensland and northeast new south wales with biggs 2004 noting no obvious variation but other studies did see an effect from prevailing southeasterly winds probert 1976 douglas 1968 prevailing westerly winds in eastern victoria and southern new south wales that blow from the interior of the continental landmass probably explain the lower values of a 1 in those areas douglas 1968 hingston and gailitis 1976 noted a chloride deposition rate approximately one order of magnitude lower in the north of western australia compared to the southwest of the state which is reproduced here they suggested that this difference in chloride deposition could be due the influence of tropical monsoonal rainfall patterns resulting in dilution of atmospheric chloride available for entrainment and an overall lower equilibrium concentration of chloride at tropical latitudes it is clear that the spatial variability in the regression parameters introduced through the use of pilot points is compensating for all the other factors that influence chloride deposition that are not the distance from the coast from the existing literature and the analysis of the continental scale chloride deposition map produced here the direction of wind and possibly windspeed would be a useful addition into the regression equation used here the first step could be the distance from the coast in the prevailing wind direction the model used here is purely empirical and does not purport to represent actual physical processes while a physically based model that incorporates a mass balance of chloride with a source at the coast and diffuse losses downwind could be created it is doubtful that we currently have enough knowledge of the processes to conceptualise or enough data to parameterise such a model 4 3 example of estimating groundwater recharge the example presented here of using the chloride deposition map to probabilistically estimate groundwater recharge is a step forward as it allows for the characterisation of the uncertainty of the chloride deposition input into the chloride mass balance method when the other inputs into the chloride mass balance calculations are also presented in a probabilistic manner then a better appreciation of the true uncertainty in our estimates of recharge will be understood this is already occurring see crosbie et al 2018 and should lead to better information being used to develop water resources policy the improved estimates of chloride deposition presented here will also be useful to other areas of research such as those identified in the introduction salinity modelling catchment salt mass balances investigating hydrologic equilibrium following land clearing prediction of atmospheric corrosion and quantifying air pollution 5 conclusion based on 291 point measurements of chloride deposition across australia over a period of 80 years and using a pilot point regularisation approach within a parameter estimation software we have been able to model a relationship between chloride deposition and distance from the coast for australia the result was to produce a best estimate continental scale chloride deposition map in contrast to previously derived chloride deposition maps we have quantified the uncertainty as the 5th and 95th percentile of 1000 calibrated models produced via null space monte carlo analysis a comparison of the chloride deposition map to an independent data set of 20 sites across australia demonstrated the significant geographic spatial variability across the australian landmass has been replicated in the model but there is the possibility that the model has been over fitted and is underestimating the uncertainty acknowledgements this paper is a result of work undertaken as part of the project a consistent approach to groundwater recharge and discharge estimation in data poor areas funded by the australian national water commission the authors also gratefully acknowledge the support of the australian government department of the environment through the bioregional assessment programme tim ransley jeffrey turner and saad mustafa are acknowledged for providing additional site data we are also grateful for the comprehensive reviews of rebecca doble chris turnadge and two anonymous reviewers and the helpful comments of the joh editorial team huade guan and tim mcvicar the gridded datasets are available via the csiro data access portal http doi org 10 4225 08 545bee54cd4fc appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 03 051 appendix a supplementary data supplementary data 1 
7263,the high solubility and conservative behaviour of chloride make it ideal for use as an environmental tracer of water and salt movement through the hydrologic cycle for such use the spatial distribution of chloride deposition in rainfall at a suitable scale must be known a number of authors have used point data acquired from field studies of chloride deposition around australia to construct relationships to characterise chloride deposition as a function of distance from the coast these relationships have allowed chloride deposition to be interpolated in different regions around australia in this paper we took this a step further and developed a chloride deposition map for all of australia which includes a quantification of uncertainty a previously developed four parameter model of chloride deposition as a function of distance from the coast for australia was used as the basis for producing a continental scale chloride deposition map each of the four model parameters were made spatially variable by creating parameter surfaces that were interpolated using a pilot point regularisation approach within a parameter estimation software the observations of chloride deposition were drawn from a literature review that identified 291 point measurements of chloride deposition over a period of 80 years spread unevenly across all australian states and territories a best estimate chloride deposition map was developed from the resulting surfaces on a 0 05 degree grid the uncertainty in the chloride deposition map was quantified as the 5th and 95th percentile of 1000 calibrated models produced via null space monte carlo analysis and the spatial variability of chloride deposition across the continent was consistent with landscape morphology the temporal variability in chloride deposition on a decadal scale was investigated in the murray darling basin this highlighted the need for long term monitoring of chloride deposition if the uncertainty of the continental scale map is to be reduced use of the derived chloride deposition map was demonstrated for a probabilistic estimation of groundwater recharge for the southeast of south australia using the chloride mass balance method keywords atmospheric chemistry rainfall groundwater recharge spatial variability uncertainty analysis 1 introduction chloride deposition occurs in the landscape by the mechanisms of rainfall washout of dissolved chloride ions cl and dry deposition of ions attached to dust particles gustafsson and franzen 1996 previous work e g hutton 1976 keywood et al 1997 has shown that distance from the coast wind and rainfall are all factors which contribute to the amount of cl deposition on land surfaces the high solubility chemical stability and environmentally conservative nature of the cl ion make it an ideal tracer for estimating the average diffuse net recharge to aquifers from rainfall using the chloride mass balance method the chloride mass balance method was first used by anderson 1945 near mount gambier in south australia and is the most widely used method for estimating recharge in australia crosbie et al 2010 and globally at least in semi arid and arid areas scanlon et al 2006 in addition to estimating recharge data concerning salt mass in rainfall are essential for numerous other applications these include i both point scale and basin scale salinity modelling ruprecht and sivapalan 1991 walker et al 2002 ii calculations of salt mass balances in groundwater within catchments herczeg et al 2001 tolmie et al 2004 iii investigating hydrologic equilibrium following land clearing peck and hurle 1973 jolly et al 2001 iv prediction of atmospheric corrosion meira et al 2008 suzuki and robertson 2011 and v quantifying air pollution delalieux et al 2006 there have been many investigations of major ions in rainfall carried out in australia over the last 80 years these have included studies into the chloride analysis of rainfall concentration and deposition to investigate impacts on stream water quality douglas 1968 soil salinity hingston and gailitis 1976 and atmospheric corrosion slamova et al 2012 previous investigations concluded that the source of salts in soils and groundwater was atmospheric rather than from the weathering of rocks willsmore and wood 1929 teakle 1937 herczeg et al 2001 distance from the coast keywood et al 1997 orographic guan et al 2010 and atmospheric factors have more recently been examined in order to try and explain the spatial variability of chloride deposition in the landscape the role of vegetation in enhancing chloride deposition rate ulrich 1983 deng et al 2013 has also been studied the work of deng et al 2013 and bresciani et al 2014 was able to quantify the chloride deposition enhancement for a coastal forested environment at a local scale these effects are less obvious when considering chloride deposition at the broader continental scale regional scale investigations have shown the input of chloride varied within years at coastal localities and was most dependent upon localised conditions such as wind speed and direction hingston and gailitis 1976 farrington et al 1993 several authors have highlighted the need for seasonally averaged long term data to prevent these short term variances from biasing estimates of chloride deposition blackburn and mcleod 1983 hingston and galbraith 1990 ideally rates of chloride deposition would be measured by a long term monitoring program such as the national atmospheric deposition program in the usa lamb and bowersox 2000 unfortunately most of the rest of the world has to rely on upscaling sparse infrequent field studies eriksson and khunakasem 1969 allison and hughes 1978 claassen and halm 1996 sami and hughes 1996 minor et al 2007 these upscaling methods assume a uniform distribution of chloride deposition rate over a broad spatial resolution and do not take into account the highly variable nature and the associated uncertainty in deposition a number of studies have been conducted to model the chloride deposition using a variety of relationships keywood et al 1997 gustafsson and hallgren larsson 2000 more recently guan et al 2010 attributed 70 of spatial variability in bulk chloride deposition to the effect of coastal distance in a study located in the mt lofty ranges south australia there have been numerous models created for predicting the chloride deposition rate as a function of distance from the coast in australia hutton 1976 hingston and gailitis 1976 keywood et al 1997 these relationships are regional in nature and they have often been created using datasets specific to one region the relationships are subsequently used in other data poor regions by other researchers without considering whether this is appropriate thus highlighting the need for a continental chloride deposition map keywood et al 1997 identified that their model fit the data well in a west east array across western australia and the southern half of a south north array across western australia but did not fit the northern section of this array this observation was attributed to lower wind velocities in the north mcvicar et al 2008 not carrying the aerosols as far inland meaning that the fast decay was not as dominant as it was in the south this limitation of the model was partially overcome by vleeshouwer et al 2009 who split australia into six climate zones and fitted individual models for each climate zone an alternative approach in predicting the distribution of chloride deposition is to apply kriging models to field sampled data sets delalieux et al 2006 used kriging methods matheron 1965 cressie 1993 to map the cl and na contents of total wet dry deposition on a grid cell mesh 5 5 km based on some 91 observations over southwest europe alcalá and custodio 2008 mapped the bulk deposition of chloride across spain using ordinary kriging delalieux et al 2006 noted the reduction in chloride deposition with distance from the coast but warned on using their results very close to the coast as their data was inland and the kriging model did not incorporate this relationship a selection of these studies that have produced chloride deposition maps is shown in table 1 this shows that both regression and kriging approaches have been previously used at a continental scale but they have not been combined and uncertainties characterised at this scale the objective of this study is to map the spatial distribution of the average annual bulk chloride deposition rate across australia combining the best features of the regression and interpolation approaches and to provide a measure of its uncertainty specifically this study will i collate the existing measured data on chloride deposition across australia ii use this data to create a continental scale chloride deposition map with its associated uncertainty and iii provide an example of using this chloride deposition map to estimate groundwater recharge in the southeast of south australia these objectives provide the structural sub headings used in the following methods results and discussions sections 2 methods 2 1 observations of chloride deposition across australia a review of the available literature produced a total of 291 point measurements of chloride deposition in rainfall across australia ranging in dates from 1933 to 2010 and for sampling periods ranging from 2 months to 5 years fig 1 a where the chloride flux load was not reported it was calculated as the rainfall weighted average 1 d cl i p i p i 10 2 where d is chloride mass kg ha 1 yr 1 cl i is rainfall chloride concentration for time period i mg l 1 and pi is rainfall in time period i mm there were a few studies that reported the rainfall weighted average chloride concentration but not the rainfall in these cases the mean annual rainfall jeffrey et al 2001 was used to calculate the chloride flux from the rainfall weighted average chloride concentration where 2 d cl p 10 2 where d is chloride mass kg ha 1 yr 1 cl is average chloride concentration of the rainfall mg l 1 and p is annual average rainfall mm yr 1 the details of the 291 point measurements of the chloride deposition in rainfall are listed in the supplementary material an additional 20 measurement locations of chloride deposition in rainfall reported in crosbie et al 2012 were omitted from this dataset and subsequently used to provide an independent evaluation of model results fig 2 previous studies have shown that distance from the coast yaalon and katz 1962 hutton 1976 naranjo et al 2015 rainfall neal and kirchner 2000 windspeed alcalá and custodio 2008 castañeda 2016 elevation slope and aspect guan et al 2010 are useful predictors of chloride deposition these factors which are available from continental datasets jeffrey et al 2001 mcvicar et al 2008 gallant et al 2011 gallant and austin 2012a b are assessed through correlation analysis to determine if they are useful predictors of chloride deposition on a continental scale 2 2 a continental scale chloride deposition map the regression method of spatial interpolation used by vleeshouwer et al 2009 incorporated the relationship between chloride deposition and distance from the coast for multiple climatic zones but had limitations in edge effects where climatic boundaries were used to account for spatial trends kriging is useful in spatial interpolation in that it can account for the spatial variability at both the large scale and the small scale by modelling both the spatial trend and spatial correlation of the data however the kriging method implemented by delalieux et al 2006 was limited by not incorporating the relationship with distance from the coast explicitly resulting in questionable results in areas without data this study will combine the best of these two approaches by spatially interpolating the parameters of the regression model to enable a seamless chloride deposition map to be created that respects both the local and regional spatial trends in the observed chloride deposition data the method used here to create a continental map of chloride deposition is shown diagrammatically in fig 3 and can be summarised into several steps i development of the regression model ii calibration of the regression model parameters iii assessing the uncertainty in the continental scale map of chloride deposition iv an assessment of the model performance against an independent validation data set and v an assessment of how the temporal variation decadal in chloride deposition effects the long term average chloride deposition map produced here these steps will be addressed in turn in the following sub sections 2 2 1 regression model a number of studies have been conducted exploring the distribution of chloride deposition using regression modelling to describe the relationship between deposition of chloride and distance from the coast both in australia hutton 1976 farrington et al 1993 biggs 2004 and elsewhere yaalon and katz 1962 neal and kirchner 2000 meira et al 2006 these have largely been developed from local observations of chloride deposition e g catchment scale keywood et al 1997 suggested that the primary deposition of chloride across australia could be described in terms of fast and slow deposition rates with the rate that dominates being dependent upon distance from the coast the fast portion of this relationship is characterised by rapid removal of chloride near the coast and has an exponential decay constant of 60 km the slow portion is characterised by a much slower decline in chloride deposition with distance and has a decay constant of 700 km the total chloride mass deposited by the fast fraction of deposition is generally double that deposited by the slow fraction the keywood et al 1997 model has some theoretical justification for the use of the sum of two exponential decay functions with distance from the coast namely a fast and slow decay component the fast component is conceptualised as the wet and dry deposition of chloride in aerosols sourced from the ocean the slow component is the deposition of gaseous chloride that is formed by volatilisation of the chloride in sea salt at a ph below 3 the model of keywood et al 1997 is 3 d a 1 e d λ 1 a 2 e d λ 2 where d is chloride deposition rate kg ha 1 yr 1 a 1 and a 2 are coefficients kg ha 1 yr 1 λ 1 and λ 2 are decay constants km and d is distance from the coast km 2 2 2 model calibration combining the best of previous approaches to map chloride deposition the observations of chloride deposition are fitted to a surface generated using eq 3 in which the four parameters a 1 a 2 λ 1 and λ 2 are themselves surfaces that are calibrated using the pilot points method details given below this strategy avoids the edge effects of zones and allows for the spatial distribution of the fitting parameters to be data driven rather than prescribed a priori the calibration of the model to the measured rates of chloride deposition was conducted using pilot points regularisation in pest doherty 2010 the use of pilot points allows for spatial variability in the four model parameters which in a highly parameterised inversion can lead to an almost perfect fit between the model and the observations however a perfect fit between the model and the observations would almost certainly result in unrealistic parameter surfaces because the uncertainty in the observations is not taken into account nor are the mechanisms responsible for the changes in the parameter surface regularisation applies an additional constraint into the objective function whereby the difference in the parameter values between pilot points is minimised but allowed to vary only to the point of realising an acceptable degree of misfit between the observations and simulations this ensures that any spatial variability in the parameter surface is there because it needs to be to fit the data and ensures that the resulting parameter surface is as realistic as possible the location of the pilot points were selected to cover the geographical domain of the model and were biased to areas with a higher density of observations fig 1b a total of 150 pilot points were used in the modelling approximately half the number of observations which results in 600 parameters to be calibrated ordinary kriging was used to interpolate the parameter values at the pilot points to a regular grid at a resolution of 0 05 degree approximately 5 5 km the exponential semivariogram parameters used in the ordinary kriging were optimised iteratively with the model parameters at the pilot points using esri s arcgis geostatistical analyst this enabled interpolation of the parameters to a regular grid the optimised parameter values at the pilot points were then again fitted to the semivariogram to ensure that the parameters were still appropriate after the pest optimisation the pest optimisation aimed for a specified sum of squared residuals ssr between the weighted and log transformed fitted and observed values of chloride deposition subject to the constraint that the parameter surfaces be as smooth as possible using a tikhonov regularisation the weights used on the observations were based on the assessment of the field data with an assigned weighting from 0 to 1 0 according to their reliability and length of study with 0 being assigned to short term measurements of uncertain derivation and 1 0 assigned to long term measurements of verifiable derivation through trial and error an acceptable level of misfit was determined to be a ssr of 7 a lower ssr is possible with pest but resulted in parameter surfaces that contained bullseyes to enable a fit to observed data that has its own uncertainty a higher ssr allows for smoother parameter surfaces that begin to lose some of the spatial structure that can be explained through physical processes see discussion section 4 2 there is a subjective choice to be made in the acceptable level of misfit that is ideally informed by the uncertainty in the observations of chloride deposition a ssr of 7 translates to an average misfit of 0 15 log kg ha 1 yr 1 which is comparable to the differences in chloride deposition measured at the same location in different studies see results section 3 2 3 this calibration procedure resulted in a best estimate map of chloride deposition across australia 2 2 3 uncertainty in predicted chloride deposition the highly parameterised inversion used by pest doherty 2010 results in a non unique solution in the parameter space this can be used to our advantage for investigating the predictive uncertainty in the map of chloride deposition this is achieved using the null space monte carlo technique tonkin and doherty 2009 within pest this technique splits the parameter space into the solution space and the null space the null space is parameter combinations that have no effect upon the calibration but contribute to predictive uncertainty parameter combinations in the null space can be generated stochastically to produce multiple replicates of equally well calibrated models for this study we have generated 1000 replicates of the continental scale chloride deposition these are reported as 95 confidence limits around our best estimate and also the descriptive statistics of the mean standard deviation coefficient of variation and skewness of the 1000 replicates 2 2 4 model validation work by crosbie et al 2012 reported new data for the chemical composition of rainfall at 20 sites across australia over a period of 4 years fig 2 these data points were used to provide an independent evaluation of model results the values of predicted chloride deposition from the continental scale map and its uncertainty were extracted at each of the 20 locations monitored by crosbie et al 2012 and plotted in a scatterplot to assess the accuracy of the continental scale mapping the validation dataset was also compared to the more conventional approach of producing a chloride deposition map using ordinary kriging gustafsson and hallgren larsson 2000 delalieux et al 2006 alcalá and custodio 2008 ordinary kriging of the 260 points with a weighting of greater than 0 2 was performed to produce a map on the same 0 05 degrees grid as the map produced using the keywood et al 1997 model 2 2 5 temporal variations in chloride deposition it is commonly assumed that collecting chloride deposition data over a complete year is the minimum requirement to estimate a chloride deposition rate blackburn and mcleod 1983 biggs 2006 sweeney et al 2016 this can be tested where there are data collected at different times in the same location katherine in the northern territory is the best example from our dataset which has been the site of five independent recorded studies of deposition of chloride ranging in duration from 1 to 5 years wetselaar and hutton 1963 galloway et al 1982 likens et al 1987 keywood 1995 wilson et al 2006 and with a range of values of deposition from 2 46 to 7 30 kg ha 1 yr 1 this compares to a value of 4 06 kg ha 1 yr 1 for the modelled chloride deposition in this study it is clear that there is substantial temporal variation between studies at this location over many locations and independent studies these differences may be averaged out by fitting to a continental scale model to test how this will affect the fitting of the keywood et al 1997 model over the continent the murray darling basin fig 2 was selected for investigation the 94 estimates of chloride deposition were split into those collected in years with above average rainfall wet and below average rainfall dry these wet and dry periods were compared to the model fitted to the whole dataset 2 3 example of estimating groundwater recharge the chloride mass balance method of estimating recharge is the most frequently used method in australia crosbie et al 2010 this is because it is simple to implement and the data requirements are not onerous the method works by comparing the concentration of chloride in groundwater to that of rainfall the ratio of the concentration of chloride in rainfall to groundwater gives the proportion of rainfall that becomes recharge this can be reformulated as 4 r 100 d cl gw where r is groundwater recharge mm yr 1 d is the chloride deposition due to rainfall kg ha 1 yr 1 and clgw is the chloride concentration of groundwater mg l 1 the assumptions behind this methodology have been explored in detail by wood 1999 australian state and territory governments have collected vast amounts of data on the chloride concentration of groundwater but what has previously been lacking is a chloride deposition map with associated uncertainty in an easily accessible format an example of using the new chloride deposition map for estimating recharge in a probabilistic manner is presented for the southeast of south australia while the chloride mass balance method has been widely used to estimate spatial distributions of recharge e g eriksson and khunakasem 1969 scanlon et al 2012 the uncertainty associated with both chloride deposition rates and groundwater chloride concentrations are rarely propagated to quantify the uncertainty associated with estimated rates of recharge alcalá and custodio 2015 crosbie et al 2018 in practice stochastic methods can be used to generate many replicates of an estimated recharge map through random sampling of the input probability distributions in the present study 10 000 replicates were created through sampling of a pearson type iii probability distribution of chloride deposition rates and a log normal distribution of groundwater chloride concentrations the chloride in groundwater data was obtained from the databases held by state governments in south australia and victoria in locations with multiple measurements the geometric mean was used it was assumed that the chloride concentration of groundwater measured in a bore is representative of the chloride concentration at the water table ordinary kriging was used to interpolate log transformed point measurements to a regular grid this interpolated grid and its associated standard error were used to create 10 000 replicates of the spatial distribution of chloride in groundwater 3 results 3 1 observations of chloride deposition across australia there were a total of 291 data points available for developing the continental scale chloride deposition map of these 88 had a measurement period of at least 12 months and none had a measurement period longer than 5 years fig 4 shows scatterplots of the chloride deposition against the predictors i distance from the coast ii elevation iii difference in angle between the aspect of the slope and direction to the coast iv mean annual rainfall v mean annual windspeed and vi slope from the data that we have available it seems as though distance from the coast is the only significant predictor of chloride deposition this supports the use of the keywood et al 1997 model and so only the distance from the coast will be used in the regression model 3 2 a continental scale chloride deposition map 3 2 1 regression model analysis of our dataset showed that the only significant predictor of the chloride deposition is the distance from the coast therefore the regression model of keywood et al 1997 will be used in this study 3 2 2 model calibration the weighted sum of squared residuals between observed and fitted chloride deposition for the best calibrated model was 6 998 this corresponds to an rms error of 0 16 log transformed kg ha 1 yr 1 14 of the mean which is comparable to the difference in observations between sites that have multiple studies at the same location the constraint imposed by the regularisation process was that the ssr had be less than 7 whilst minimising the difference in values of the parameters at the pilot points so there is a penalty imposed on the calibration for minimising the ssr any further a comparison of measured and modelled chloride deposition for the 291 observations across australia are shown in fig 5 overall the modelled chloride deposition appears to account well for the variation of the observed data with an r2 0 91 the pest parameterisation of the 4 keywood et al 1997 model parameters are shown in fig 6 the best estimate of the chloride deposition across australia is presented in fig 7 along with the 5th and 95th percentiles these datasets are available for download via the csiro data access portal davies and crosbie 2014 the mean bulk deposition rate of chloride varies from 8 3 to 139 7 kg ha 1 yr 1 in coastal areas defined here as being within 50 km from the nearest coastline in the centre of australia chloride deposition rates vary from 0 7 to 2 6 kg ha 1 yr 1 3 2 3 uncertainty in predicted chloride deposition the quantified uncertainty in the modelled chloride deposition is presented as the mean standard deviation coefficient of variation and skewness of 1000 equally well calibrated models fig 8 these descriptive statistics can then be used to generate a probability distribution of chloride deposition for any location in australia that can be used for further studies the standard deviation is not a particularly useful measure of the comparative spatial uncertainty in the predictions of chloride deposition in this study because of the large range in the mean values 3 orders of magnitude the coefficient of variation i e the standard deviation divided by the mean is the more useful statistic in this case this plot shows that the greatest relative uncertainty is in locations with the least data central australia inland tasmania east and north queensland and locations with a high density of data show the lowest relative uncertainty southwest western australia southeast south australia and victoria this is to be expected as the modelling process is data driven the areas with a high coefficient of variation also tend to have a skewed distribution to the right this is probably caused by the lack of data and calibration being performed on log transformed data 3 2 4 model validation as part of a study looking at the chemical and isotopic composition of rainfall across australia crosbie et al 2012 analysed rainfall samples at 20 locations over a period of 4 years of the 20 measurements 9 sites are determined to have a higher chloride deposition and 11 sites are determined to have a lower chloride deposition when comparing the modelled chloride deposition to the measured chloride deposition table 2 and fig 9 the rms error across the 20 sites is 0 35 log transformed kg ha 1 yr 1 26 which is higher than the calibration statistics this could be indicative of over fitting the data and therefore under estimating the uncertainty the two worst fits for the validation data are for cape grim and alice springs cape grim rainfall collector was only 20 m from the coast on the cliff top facing the roaring forties westerly winds across the southern ocean this is an extreme case of chloride deposition coupled with the limited resolution of the 0 05 degrees 5 km grid cells used in the model in a previous study in alice springs which was part of the calibration dataset the chloride deposition was recorded as 1 1 kg ha 1 yr 1 keywood et al 1997 which is much lower than the 3 1 kg ha 1 yr 1 used in the validation dataset table 2 this could be indicative of needing a longer term monitoring program to enable a better estimate of the average chloride deposition the chloride deposition map produced using ordinary kriging fig 10 is similar to the map produced using the keywood et al 1997 model fig 7 in the areas of dense observation data in the se and sw of the continent fig 1 in areas without dense data the map produced using ordinary kriging does not fit with our understanding of the physical processes there are bullseyes on the east coast where there is some data and particularly the east coast of tasmania does not have an increase in chloride deposition toward the coast when compared to the validation dataset of crosbie et al 2012 fig 10 the ordinary kriging map does not perform as well as the map produced using the keywood et al 1997 model fig 9 the ordinary kriging map has a low bias with 4 of the 20 points being greater than the observations and having an overall rms error of 0 40 log transformed kg ha 1 yr 1 3 2 5 temporal variations in chloride deposition our dataset includes several locations for which there has been more than one study of chloride deposition conducted and which show a temporal variation between studies we investigated this further using 94 estimates of chloride deposition across the murray darling basin which were split into those collected in years with above average rainfall wet and those collected in years with below average rainfall dry these wet and dry subsets were fitted to the model with a single parameter set each the wet subset has greater chloride deposition than the entire dataset and the dry subset has less chloride deposition than the entire dataset fig 11 the difference in the subsets fitted values compared to the entire dataset is greatest at the coast with a difference of 12 kg ha 1 yr 1 for both subsets with the difference becoming smaller with increasing distance inland but the percentage difference increases this demonstrates that using many independent studies to create a continental scale chloride deposition map will average out the wet and dry studies however this is dependent upon having a dense spatial network this only occurs in the southwest and southeast of the continent fig 1 the uncertainty in the chloride deposition map fig 7 is probably under estimated in areas outside of this dense network of observed data 3 3 example of estimating groundwater recharge the study by crosbie et al 2015 compared three methods of estimating recharge over part of the otway and murray basins in southeastern australia including the chloride mass balance method the previous study used point estimates of recharge this will be extended to estimate recharge on a regular grid there are 3901 point locations with measurements of chloride in groundwater within the area of interest fig 12 the estimated recharge was reported as the 5th 50th and 95th percentiles at each grid cell from the 10 000 replicates results indicate that areas of highest recharge are located in the south of the study area i e along the coast conversely areas of lowest recharge are located toward the north of the study area i e inland using the chloride mass balance approach the average recharge rate across the study area is estimated at 21 mm yr 1 for the 50th percentile with a plausible range of 13 34 mm yr 1 based upon the 5th and 95th percentiles with the 95th percentile being nearly three times the magnitude of the 5th percentile the uncertainty associated with the best estimate of recharge is quite significant and should be important for water resource managers to understand 4 discussion 4 1 observations of chloride deposition across australia gains in reducing the uncertainty of predicting the chloride deposition across the continent could be made by instigating a long term monitoring program such as the national atmospheric deposition program in the usa lamb and bowersox 2000 it has been shown here that wet and dry periods on a decadal scale do produce a different relationship between chloride deposition and distance from the coast a more representative long term average chloride deposition rate at a point scale would be a major improvement in reducing the uncertainty in a continental scale chloride deposition map orographic effects have been studied by other researchers for its effect on chloride deposition a positive correlation has been observed in the mt lofty ranges australia guan et al 2010 and the greater tamar catchment australia sweeney et al 2016 in contrast a negative correlation has been observed in hawaii miller et al 1984 and no correlation has been observed in allt a mharcaidh catchment scotland ferrier et al 1990 from the data collated here there is a very weak negative correlation between chloride deposition and elevation elevation may be a useful predictor of chloride deposition at the local to regional scale but from the data collated here that is not the case at the continental scale 4 2 a continental scale chloride deposition map the chloride deposition rate is a minimum in central australia which is as expected due to the insulation of any effects of marine air masses supplying chloride keywood et al 1997 farrington and bartle 1988 recorded a considerable variation in chloride deposition from rainfall between years of a 5 year study at gnangara mound they noted that this was likely due to the variation in number or intensity of storm events during the winter and spring the transport of chloride as aerosols from oceanic spray by prevailing onshore westerly winds was noted as increasing during such storm events farrington and bartle 1988 the effect of prevailing westerly winds is seen in the spatial distribution of the fast component a 1 fitting parameter fig 6 noticeably inland from southwest western australia southeast south australia and western victoria as well as inland tasmania this effect is also seen in the spatial distribution of the fast component λ1 decay parameter fig 6 with decay constants of 75 km it is uncertain if an associated prevailing onshore wind effect is the cause of increased a 1 inland from the northeast coast of queensland and northeast new south wales with biggs 2004 noting no obvious variation but other studies did see an effect from prevailing southeasterly winds probert 1976 douglas 1968 prevailing westerly winds in eastern victoria and southern new south wales that blow from the interior of the continental landmass probably explain the lower values of a 1 in those areas douglas 1968 hingston and gailitis 1976 noted a chloride deposition rate approximately one order of magnitude lower in the north of western australia compared to the southwest of the state which is reproduced here they suggested that this difference in chloride deposition could be due the influence of tropical monsoonal rainfall patterns resulting in dilution of atmospheric chloride available for entrainment and an overall lower equilibrium concentration of chloride at tropical latitudes it is clear that the spatial variability in the regression parameters introduced through the use of pilot points is compensating for all the other factors that influence chloride deposition that are not the distance from the coast from the existing literature and the analysis of the continental scale chloride deposition map produced here the direction of wind and possibly windspeed would be a useful addition into the regression equation used here the first step could be the distance from the coast in the prevailing wind direction the model used here is purely empirical and does not purport to represent actual physical processes while a physically based model that incorporates a mass balance of chloride with a source at the coast and diffuse losses downwind could be created it is doubtful that we currently have enough knowledge of the processes to conceptualise or enough data to parameterise such a model 4 3 example of estimating groundwater recharge the example presented here of using the chloride deposition map to probabilistically estimate groundwater recharge is a step forward as it allows for the characterisation of the uncertainty of the chloride deposition input into the chloride mass balance method when the other inputs into the chloride mass balance calculations are also presented in a probabilistic manner then a better appreciation of the true uncertainty in our estimates of recharge will be understood this is already occurring see crosbie et al 2018 and should lead to better information being used to develop water resources policy the improved estimates of chloride deposition presented here will also be useful to other areas of research such as those identified in the introduction salinity modelling catchment salt mass balances investigating hydrologic equilibrium following land clearing prediction of atmospheric corrosion and quantifying air pollution 5 conclusion based on 291 point measurements of chloride deposition across australia over a period of 80 years and using a pilot point regularisation approach within a parameter estimation software we have been able to model a relationship between chloride deposition and distance from the coast for australia the result was to produce a best estimate continental scale chloride deposition map in contrast to previously derived chloride deposition maps we have quantified the uncertainty as the 5th and 95th percentile of 1000 calibrated models produced via null space monte carlo analysis a comparison of the chloride deposition map to an independent data set of 20 sites across australia demonstrated the significant geographic spatial variability across the australian landmass has been replicated in the model but there is the possibility that the model has been over fitted and is underestimating the uncertainty acknowledgements this paper is a result of work undertaken as part of the project a consistent approach to groundwater recharge and discharge estimation in data poor areas funded by the australian national water commission the authors also gratefully acknowledge the support of the australian government department of the environment through the bioregional assessment programme tim ransley jeffrey turner and saad mustafa are acknowledged for providing additional site data we are also grateful for the comprehensive reviews of rebecca doble chris turnadge and two anonymous reviewers and the helpful comments of the joh editorial team huade guan and tim mcvicar the gridded datasets are available via the csiro data access portal http doi org 10 4225 08 545bee54cd4fc appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 03 051 appendix a supplementary data supplementary data 1 
7264,many rivers in the pacific northwest region of north america are anthropogenically manipulated via dam operations leading to system wide impacts on hydrodynamic conditions and aquatic communities understanding how dam operations alter abiotic and biotic variables is important for designing management actions for example in the klamath river dam outflows could be manipulated to alter water age and temperature to reduce risk of parasite infections in salmon by diluting or altering viability of parasite spores however sensitivity of water age and temperature to the riverine conditions such as bathymetry can affect outcomes from dam operations to examine this issue in detail we conducted a global sensitivity analysis of water age and temperature to a comprehensive set of hydraulics and meteorological parameters in the klamath river california where management of salmonid disease is a high priority we applied an analysis technique which combined latin hypercube and one at a time sampling methods and included simulation runs with the hydrodynamic numerical model of the lower klamath we found that flow rate and bottom roughness were the two most important parameters that influence water age water temperature was more sensitive to inflow temperature air temperature solar radiation wind speed flow rate and wet bulb temperature respectively our results are relevant for managers because they provide a framework for predicting how water within high infection risk sections of the river will respond to dam water low infection risk input moreover these data will be useful for prioritizing the use of water age dilution versus temperature spore viability under certain contexts when considering flow manipulation as a method to reduce risk of infection and disease in klamath river salmon keywords latin hypercube salmonid disease management sensitivity analysis water temperature water age 1 introduction the flow regimes of most major rivers in the pacific northwest region of north america are managed via dam operations flow management alters the natural hydrograph and has been associated with economic and ecological impacts including the health of native fishes bunn and arthington 2002 mea 2005 the klamath river which flows through oregon and california to the pacific ocean is managed by a series of reservoirs and dams that have modified the original natural flows and temperature regimes flow modification has the potential to affect ceratonova shasta c shasta a myxosporean parasite that is a significant cause of mortality for salmonid fishes in the klamath river fujiwara et al 2011 ray et al 2012 true et al 2016 because of impacts on discharge and temperature the parasite is fully aquatic and has non motile waterborne infectious stages myxospores and actinospores that alternate between an invertebrate host polychaete and salmonids annual prevalence of c shasta infection in out migrating juvenile chinook salmon has been estimated at up to 91 percent in some years true et al 2016 high mortality in juvenile salmonids has been observed following low magnitude peak winter and spring flows when river temperatures reach 15 18 c in late spring bartholomew and foott 2010 hallett et al 2012 in contrast reduced c shasta infection and related mortality in salmonids have been observed following high magnitude winter and spring discharge true et al 2011 hallett et al 2012 the addition of cool parasite free water from iron gate dam the lowermost dam to the mainstem klamath river may be an effective management action when disease risk to fish is high water temperature is correlated with disease progression and mortality in infected fish hosts ray et al 2012 parasite density is also correlated with mortality 5 and 10 spores per l of river water cause 40 mortality in klamath river coho and chinook respectively hallett et al 2012 thus managed water releases could reduce disease risk to fish by reducing downstream river water temperature or by diluting waterborne parasite stages knowledge of water age or the travel time required for a parcel of water released from the dam to reach specific location is necessary for predicting how the timing and volume of water released will affect parasite density and water temperature within high risk for fish zones located downstream in addition water age is relevant for i determining timing and volume of water to be released to achieve decreases in parasite density or water temperature and ii designing monitoring plans to measure effects on disease risk for fish e g measure parasite density before during and after a managed water release event thus a model predicting water temperature and age would be useful for managers considering scenarios of flow manipulation strategies models for predicting water temperature have been developed for many river systems including the klamath river e g perry et al 2011 previous studies in other systems have demonstrated that atmospheric conditions are significant drivers of river temperature edinger et al 1974 ward 1985 stefan and preud homme 1993 other researchers have also recognized river discharge as a significant driver of the river water temperature morse 1972 the water temperature is expected to be lower for higher flow rates grant 1977 there are several studies that have evaluated the sensitivity of river temperature to hydraulic and meteorological conditions wu 1992 sinokrot and stefan 1994 gu and li 2002 sinokrot and stefan 1994 introduced the air temperature and shortwave solar radiation as the most important parameters that affect river water temperature gu and li 2002 found that sensitivity of river temperature to discharge and inflow temperature was similar to sensitivity of temperature to atmospheric conditions in their study they reported that air temperature flowrate relative humidity and inflow temperature are the most sensitive parameters in calculating the maximum temperature slope and bottom roughness have no effect on the daily mean temperature and have only a small effect on maximum water temperature in their study however they considered the river slope to be a constant value for the entire river which is not a reasonable assumption for rivers moving through significant topographic changes for such rivers the numerical model is divided into several meshes that represent the physical domain of the river and hence the bottom elevation and slope of each cell is specified separately estimating spatially varying slopes requires an accurate bathymetry dataset for the entire length of the river light detection and ranging lidar is one approach to get high resolution topographic data to represent the land surface for the river bathymetry a specific type of lidar data called green lidar is used since it can penetrate the water and detect the bathymetry boundaries in absence of green lidar bathymetry could be collected by in situ data collection techniques such as computerized hydrographic survey system land based positioning survey and etc rogala 1999 however in situ bathymetry data has lower spatial resolution comparing to lidar another limitation is that when acoustical sounding is used to determine the water depth in shallow rivers with high vegetation the acoustic signal reaches the vegetation instead of riverbed rogala 1999 hence in situ bathymetry data can have uncertainty in its values thereby requiring its effects to be considered in hydrodynamic models in contrast to water temperature sensitivity of water age travel time particle movement to riverine conditions and parameters has not been extensively studied and has not been developed at all for the klamath river the time elapsed from when a water particle moves from its boundary to a specific location is called water age delhez et al 1999 water age represents the travel time when the initial water age is set to zero shen and haas 2004 calculated the mean age and residence time of a released substance from the tributaries to the tidal york river and found that water age was primarily dependent on river discharge gong et al 2009 developed a three dimensional model to evaluate the effect of wind on particle travel time using the concept of water age in the tidal rappahannock river they found that water age distribution is highly affected by the local wind in estuaries and depended significantly on the interaction of wind buoyancy forcings and on the prior status of the circulation gong et al 2009 however all of these previous studies examined tidal rivers that interacted with estuaries and hence sensitivity of water age to riverine and meteorological parameters in shallow inland rivers is still not clear additionally the sensitivity of water age to the bathymetry data has not been evaluated in previous studies and there is a critical need to investigate the effect of a comprehensive set of parameters on the water age in shallow rivers sensitivity analysis sa is divided into two broad categories local sensitivity analysis and global sensitivity analysis pappenberger et al 2008 local sa considers the local impact of input parameters variation on model response by concentrating on thesensitivityin vicinity of a set of parameter values borgonovo and plischke 2016 unlike local sa global sa addresses the sensitivity relative to the entire range of parameters distribution dobler and pappenberger 2013 therefore local sa is not a robust approach in many cases and global sa is the preferred sensitivity analysis method cloke et al 2008 global sensitivity analysis is a valuable tool in understanding the model structures the main sources of model output uncertainty and deriving decisions to reduce model uncertainty ratto et al 2001 the most common global methods are the i regional sensitivity analysis rsa hornberger and spear 1981 ii variance based methods sobol 1993 and iii latin hypercube one at a time van griensven et al 2006 lh oat has been used by multiple studies and has been found to be an efficient sensitivity analysis technique holvoet et al 2005 li et al 2009 in this study we combined lh oat as a global sensitivity analysis tool with a three dimensional hydrodynamic model of an inland river klamath river in california to investigate the most sensitive model inputs and parameters that influence water age and water temperature water temperature and age both influence disease risk for klamath river salmon hence disease management actions stand to significantly benefit from a clear understanding of the factors that drive them 2 study site the klamath river basin is located in southern oregon and northern california it is divided into upper basin and lower basin at iron gate dam igd at river kilometer 304 the modeled reach for this study is located in california begins at igd and continues 100 km downstream to seiad valley river kilometers 304 204 this reach includes a zone of high infection risk for salmon that is likely to be influenced by a prescribed release of reservoir water via dam operations the lower klamath basin is generally covered by forest except the shasta and scott river drainages which are used primarily for agriculture and rangeland unlike the upper basin the lower basin is steep and rocky with a complex terrain two main tributaries join the main stem the shasta and scott rivers draining from the shasta and scott basins respectively in addition several smaller but still significant tributaries drain into the river including beaver creek horse creek and grider creek fig 1 3 methodology 3 1 data collection data required for building this model included flow measurements obtained from the united states geological survey usgs gauges fig 1 with stations ids 11516530 at igd 11517500 at shasta river 11519500 at scott river and 11520500 at seiad valley atmospheric data including the precipitation air temperature relative humidity wind speed and wind direction collected at collins baldy clb and slater butte srb stations the water temperature measurements were collected from karuk tribe water resources measurements at igd seiad valley and shasta river bathymetry data were obtained from a survey conducted by department of the interior bureau of reclamation usbor with support from the usgs they used two boats equipped with a multibeam acoustic doppler current profiler adcp interfaced with global positioning system gps however there are 6 areas with total length of 11 5 km where boats could not collect the data due to gaps in gps coverage and data collection issues aeration shallow depth etc woolpert inc gathered lidar data from the link dam upstream of igd and not shown in fig 1 to happy camp downstream of seiad and not shown in fig 1 with 0 91 m 3 ft resolution with coverage over the entire study area greimann et al 2011 since lidar used for collecting these data cannot penetrate the water the data represent the water surface elevation instead of the bathymetry water depth was also gathered by usbor for the entire length of study site in this study we used the lidar and water depth measurements to estimate bathymetry at locations where bottom bathymetry was not known these estimates were generated by subtracting the water depth from water surface elevation for areas with missing data however the water elevation and water depth were not collected on the same date so the generated bathymetry needed to be adjusted by comparison with measured bathymetry data at monitored locations these locations include tree of heaven beaver creek and community center sites wright et al 2014 where accurate bathymetry with error tolerance of approximately 0 03 m were collected using gps and echo sounder in 2014 woolpert also collected a set of bathymetric cross sections for 4 8 km starting from i 5 to the shasta river these two sets of monitored sites were used for bathymetry adjustment of unmonitored areas along the length of the river furthermore the data was collected by boats that moved along two longitudinal paths down the river with at the most only two bathymetry measurements at every cross section hence at cross sections the nearest neighbor interpolation was used to generate the bathymetry data for unmonitored locations in lateral direction table 1 summarizes the data used for bathymetry generation and adjustment 3 2 numerical model 3 2 1 hydrodynamics we used the environmental fluid dynamic code efdc to create a three dimensional hydrodynamic model of the lower klamath river this model has been applied to several studies to simulate the flow ji et al 2001 seo et al 2010 efdc contains three functional modules including hydrodynamics water quality and sediments contaminants it supports cartesian and curvilinear orthogonal horizontal coordinates and stretched or sigma vertical coordinates efdc solves the following continuity and momentum equations eqs 1 3 given by 1 h t hu x hv y w z q h 2 t hu x huu y hvu z wu fhv h x g ξ p p atm x h z x h z p z h 1 a v z u q u 3 t hv x huv y hvw z wv fhu h y g ξ p p atm y h z y h z p z h 1 a v z u q v where h is water depth u and v are horizontal velocity components in x and y direction respectively w is vertical velocity component in z direction qh is the volumetric source and sink term concerning evaporation and rainfall f is coriolis factor p is the water column hydro static pressure patm is the kinematic atmospheric pressure av is vertical turbulent momentum diffusion coefficients and qu and qv are momentum source sink terms the transport equation for temperature is tetra tech 2007 4 t ht x hut y hvt z wt z h 1 a v z t q t where t is temperature and qt is the source and since term 3 2 2 heat exchange surface and sediment heat exchange can be formulated as eqs 5 and 6 respectively 5 h n h s h a h c h e h sr h ar h br 6 h sw k sw t w t s where hn is the net rate of heat exchange hs is the short wave solar radiation ha is the long wave solar radiation hc is the heat conduction he is evaporative heat loss hsr the reflected short wave radiation har is the reflected long wave radiation hbr is the back radiation from the water surface hsw is the rate of sediment water heat exchange ksw is the coefficient of sediment water heat exchange tw is the water temperature and ts is the sediment temperature 3 2 3 water age water age can be calculated based on tracer and age concentration deleersnijder et al 2001 7 α t x t u α t x k α t x c t x where x is a coordinate u is the velocity in space and time c is the tracer concentration k is the diffusivity tensor and α is the age concentration 3 3 model set up curvilinear model grids were used in this study with five cells across the river and three vertical layers the longitudinal grid resolution ranged from 85 m to 150 m boundaries include the flows from igd the shasta and scott rivers beaver horse and grider creek and the open boundary at seiad valley the model was run for three days with one second time steps on intel r xeon r cpu e3 1240 v2 3 40 ghz processors 3 4 sensitivity method and effective parameters a complete set of parameters used in efdc are listed in tables 2 and 3 these parameters were used in the sensitivity analysis in order to investigate the model representation of water age and water temperature we applied lh oat van griensven et al 2006 a sampling method that combines latin hypercube and one factor at a time sampling approach this method is global because it scans the entire range of possible parameter values and possible parameter sets this stratified sampling of input parameters enables a robust estimation of output statistics this method involves taking n latin hypercube sample points user defined with a probability of occurrence equal to 1 n and varying each lh sample point by changing the parameters one at a time this method requires a total of n p 1 runs where p is the total number of parameters the partial effect si j in percent of each parameter is estimated as 8 s i j 100 m e 1 e i 1 f i e p m e 1 e i e p m e 1 e i 1 f i e p m e 1 e i e p 2 f i where m is the model function i and j refer to the parameters and lh point respectively ei refers to model parameters and fi is a fraction by which parameters are changed finally each parameter can be ranked as the largest effect to the smallest effect by giving rank 1 to a rank equal to the total number of parameters respectively the parameters listed in tables 2 and 3 were changed randomly over the range of 50 with five lh sampling points for each parameter range 3 5 model sensitivity to bathymetry data the calibrated model of the klamath river for 2015 was obtained from a previous model developed by javaheri et al 2016 the monte carlo sampling approach was used to assess the sensitivity of the model to bathymetry data this method samples from the possible range of the input values the samples are usually generated randomly based on the estimated error of bathymetry from section 4 1 the bottom elevation of each cell was perturbed among the range of error to create the ensembles the advantage of this method is that sensitivity and uncertainty analysis can be accomplished simultaneously to evaluate the sensitivity of water temperature and water age to bathymetry data we tested the responses of each model different scenarios since this approach is computationally intensive three i iii scenarios during 2015 were selected to evaluate the sensitivity of water temperature and two i ii were selected for estimating the sensitivity of water age these scenarios represent periods of i high water temperature and base flow summer ii low water temperature and peak flow winter and iii high salmon and parasite densities spring we selected the first two scenarios to capture the extremes in temperature and discharge and included the third for the temperature model because it represents the peak period of c shasta related infection and mortality in salmon 4 results and discussions 4 1 bathymetry data generation after subtracting the water depth from water surface elevation we concluded that water surface elevation provided by lidar data needed to be adjusted by 31 cm in order to minimize the error after water surface elevation adjustment and interpolation of data across the river the overall root mean square error between generated bathymetry and actual data was found to be less than 40 cm fig 2 4 2 water age travel time sensitivity based on the partial effects parameters were classified into three importance groups most important very important and slightly important fig 3 flow rate was classified as the most important parameter with partial effect of 22 global sensitivity rank equal to 1 table 4 bottom roughness with partial effect of 7 6 was classified as very important global sensitivity rank equal to 2 wind speed with partial effect of 0 12 was classified as slightly important global sensitivity rank equal to 3 wind direction and vertical eddy viscosity with partial effects of 0 05 0 09 were classified as not important 4 global sensitivity rank 5 horizontal mass diffusivity and vertical molecular diffusivity have no significant influence on the water age because their partial effects was found to be zero hence they were ranked 7 total number of parameters water age was slightly affected by bathymetry and flow rate did not impact the sensitivity of water age to bathymetry for the two scenarios selected for evaluating the sensitivity of water age to bathymetry we found that average variance decreases when number of bathymetry realizations generated via monte carlo increase when the sample size of a set of bathymetry realizations is more than 200 the average variance converges to a constant value fig 4 this suggests that this model with more than a 200 sample size could be representative of the bathymetry error in the system since flow rate is the most sensitive parameter our selection of two different periods during 2015 one with a high flow rate 100 m3 s and one with a low flow rate 28 m3 s to investigate the water age sensitivity to bathymetry data was appropriate the average travel time water age from iron gate dam to seiad valley is 19 5 h at 100 m3 s and 37 2 h at 28 m3 s fig 5 the average difference between lower bound and upper bound is about 20 min and the maximum difference between bounds are 25 min for low flow and 40 min for high flow 4 3 water temperature sensitivity based on the partial effects we classified parameters effective on water temperature into five groups fig 6 inflow temperature with partial effect of 27 was classified as the most important parameter global sensitivity rank equal to 1 table 5 air temperature solar radiation wind speed flowrate and wet bulb temperature with partial effects of 3 8 were classified as very important 2 global sensitivity rank 6 bottom roughness and fast scale solar short wave radiation attenuation coefficient with partial effects of 1 2 were classified as important 7 global sensitivity rank 8 fraction of solar short wave radiation absorbed in the top layer evaporation rate and heat transfer coefficient between bed and bottom water layer with partial effect of 0 13 0 6 were classified as slightly important 9 global sensitivity rank 11 finally vertical eddy viscosity convective heat coefficient between bed and bottom water layer wind direction thickness of active bed temperature layer fractional cloud cover atmospheric pressure horizontal mass diffusivity and vertical molecular diffusivity with partial effect less than 0 07 were classified as not important global sensitivity rank 12 the correlation between forcing data was calculated to estimate the interaction between the various parameters fig 7 it is seen that only dry temperature and wet bulb temperature are highly correlated correlation value of 0 91 since these two parameters are positively correlated the uncertainty in the model will be greater than if they were not correlated table 6 lists the most effective parameters on water temperature and their range for three periods the warmest period the coldest period and the period with highest fish mortality during 2015 to investigate the water temperature sensitivity to bathymetry the timely averaged variance versus the sample size was calculated to evaluate the model convergence fig 8 by increasing the sample size from 50 to 100 the average variance decreases however there is no significant change for sample size more than 100 fig 9 illustrates the average water temperature at saied valley with the minimum and maximum bounds of possible results it is seen that with 40 cm error in bathymetry the average and maximum difference between upper bound and lower bound are 0 48 c and 0 59 c respectively during the winter fig 9a however these differences are bigger for fig 9b the average and maximum difference between upper bound and lower bound are 1 2 c and 1 7 c respectively during the summer fig 9c the average and maximum difference between upper bound and lower bound are 1 65 c and 2 3 c respectively it demonstrates that water temperature is more sensitive to bathymetry for low flows and warm meteorological conditions 5 conclusion a three dimensional hydrodynamic model of the lower klamath river was developed to evaluate the most sensitive parameters in calculating the river water age and water temperature both water age and temperature influence salmon disease risk in this section of the klamath river and a better understanding of how these factors may respond to managed flow events is critical to evaluating the efficacy of such actions we used a global sensitivity analysis tool a combination of latin hypercube and one factor at a time sampling to estimate the partial sensitivity of each parameter and rank them from the most important to not important the analysis highlighted the importance of bathymetry and bottom topography bathymetry were generated using lidar data and water depth measurements collected at different times these data were then adjusted using bathymetry data collected at tree of heaven beaver creek and community center with overall error less than 40 cm this technique was used to estimate data for bathymetry gap due to gps coverage and data collection difficulties in addition flow rate and bottom roughness were found to be the most sensitive parameters in estimating the water age even though wind speed wind direction and vertical eddy viscosity have been found to be sensitive parameters for water age in estuaries gong et al 2009 water age was not found to be sensitive to these parameters in the relatively shallow klamath river horizontal mass diffusivity and vertical molecular diffusivity also had no impact on the water age this result may be applicable to other shallow rivers travel time was slightly affected by bathymetry considering the error in generated bathymetry data the average error between lower bound and upper bound of possible results was found to be less than 20 min also flowrate was not found to affect the travel time sensitivity to bathymetry inflow temperature was found to be the most influential parameter that affects water temperature air temperature solar radiation wind speed flowrate wet bulb temperature bottom roughness and fast scale solar short wave radiation attenuation coefficient are the other important parameters in calculating the water temperature respectively in contrast to gu and li 2002 we found that water temperature is sensitive to bathymetry especially during warm periods considering the estimate error in generated bathymetry the average difference between lower bound and upper bound of possible results are 1 65 c and 0 48 c during the summer and winter respectively finally this study is relevant for managers interested in using flow manipulation as a disease mitigation measure in the klamath river because we provide tools models for predicting 1 water age travel time and 2 water temperature managers need to know how long it takes for a parcel of water released from the dam to reach specific location in order to know when to monitor e g before during and after such data are necessary for determining how the timing and volume of water released affected parasite density and disease risk to fish at specific sites water age travel time is of interest because the addition of water from iron gate dam reservoir would result an input of parasite free water assumption water temperature is of interest because the addition of water from iron gate dam reservoir would result an input of cooler water during spring or summer to the mainstem previously managed water releases from igd have occurred when parasite densities and water temperatures are high with the aim to reduce both however managers and researchers have previously lacked simulation models to predict the arrival of the parcel of water at monitoring locations which has constrained monitoring in addition predicting the impacts on water temperature has not been examined the latter is critical for justifying the use of stored water particularly during droughts our results demonstrate that flow rate and bottom roughness were the two most important parameters that influence water age and that travel time from the dam to sv ranges from 37 2 to 19 5 h over 28 to 100 m3 s discharge thus monitoring activities should occur within that window we also demonstrate that water temperature was sensitive to inflow reservoir temperature which suggests flow management to manipulate water temperature is very depending on reservoir conditions in summary our results provide a framework for predicting how water within high infection risk sections of the river will respond to dam water low infection risk inputs this information will be useful for prioritizing the use of water age dilution versus temperature spore viability under certain contexts when considering flow manipulation as a method to reduce risk of infection and disease in klamath river salmon acknowledgements this publication was funded by the bureau of reclamation reclamation u s department of interior funding was provided by reclamation as part of it mission to manage develop and protect water and related resources in an environmentally and economically sound manner in the interest of the american public funding was provided through interagency agreement r15pg00065 the views in this report are the author s and do not necessarily represent the views of reclamation 
7264,many rivers in the pacific northwest region of north america are anthropogenically manipulated via dam operations leading to system wide impacts on hydrodynamic conditions and aquatic communities understanding how dam operations alter abiotic and biotic variables is important for designing management actions for example in the klamath river dam outflows could be manipulated to alter water age and temperature to reduce risk of parasite infections in salmon by diluting or altering viability of parasite spores however sensitivity of water age and temperature to the riverine conditions such as bathymetry can affect outcomes from dam operations to examine this issue in detail we conducted a global sensitivity analysis of water age and temperature to a comprehensive set of hydraulics and meteorological parameters in the klamath river california where management of salmonid disease is a high priority we applied an analysis technique which combined latin hypercube and one at a time sampling methods and included simulation runs with the hydrodynamic numerical model of the lower klamath we found that flow rate and bottom roughness were the two most important parameters that influence water age water temperature was more sensitive to inflow temperature air temperature solar radiation wind speed flow rate and wet bulb temperature respectively our results are relevant for managers because they provide a framework for predicting how water within high infection risk sections of the river will respond to dam water low infection risk input moreover these data will be useful for prioritizing the use of water age dilution versus temperature spore viability under certain contexts when considering flow manipulation as a method to reduce risk of infection and disease in klamath river salmon keywords latin hypercube salmonid disease management sensitivity analysis water temperature water age 1 introduction the flow regimes of most major rivers in the pacific northwest region of north america are managed via dam operations flow management alters the natural hydrograph and has been associated with economic and ecological impacts including the health of native fishes bunn and arthington 2002 mea 2005 the klamath river which flows through oregon and california to the pacific ocean is managed by a series of reservoirs and dams that have modified the original natural flows and temperature regimes flow modification has the potential to affect ceratonova shasta c shasta a myxosporean parasite that is a significant cause of mortality for salmonid fishes in the klamath river fujiwara et al 2011 ray et al 2012 true et al 2016 because of impacts on discharge and temperature the parasite is fully aquatic and has non motile waterborne infectious stages myxospores and actinospores that alternate between an invertebrate host polychaete and salmonids annual prevalence of c shasta infection in out migrating juvenile chinook salmon has been estimated at up to 91 percent in some years true et al 2016 high mortality in juvenile salmonids has been observed following low magnitude peak winter and spring flows when river temperatures reach 15 18 c in late spring bartholomew and foott 2010 hallett et al 2012 in contrast reduced c shasta infection and related mortality in salmonids have been observed following high magnitude winter and spring discharge true et al 2011 hallett et al 2012 the addition of cool parasite free water from iron gate dam the lowermost dam to the mainstem klamath river may be an effective management action when disease risk to fish is high water temperature is correlated with disease progression and mortality in infected fish hosts ray et al 2012 parasite density is also correlated with mortality 5 and 10 spores per l of river water cause 40 mortality in klamath river coho and chinook respectively hallett et al 2012 thus managed water releases could reduce disease risk to fish by reducing downstream river water temperature or by diluting waterborne parasite stages knowledge of water age or the travel time required for a parcel of water released from the dam to reach specific location is necessary for predicting how the timing and volume of water released will affect parasite density and water temperature within high risk for fish zones located downstream in addition water age is relevant for i determining timing and volume of water to be released to achieve decreases in parasite density or water temperature and ii designing monitoring plans to measure effects on disease risk for fish e g measure parasite density before during and after a managed water release event thus a model predicting water temperature and age would be useful for managers considering scenarios of flow manipulation strategies models for predicting water temperature have been developed for many river systems including the klamath river e g perry et al 2011 previous studies in other systems have demonstrated that atmospheric conditions are significant drivers of river temperature edinger et al 1974 ward 1985 stefan and preud homme 1993 other researchers have also recognized river discharge as a significant driver of the river water temperature morse 1972 the water temperature is expected to be lower for higher flow rates grant 1977 there are several studies that have evaluated the sensitivity of river temperature to hydraulic and meteorological conditions wu 1992 sinokrot and stefan 1994 gu and li 2002 sinokrot and stefan 1994 introduced the air temperature and shortwave solar radiation as the most important parameters that affect river water temperature gu and li 2002 found that sensitivity of river temperature to discharge and inflow temperature was similar to sensitivity of temperature to atmospheric conditions in their study they reported that air temperature flowrate relative humidity and inflow temperature are the most sensitive parameters in calculating the maximum temperature slope and bottom roughness have no effect on the daily mean temperature and have only a small effect on maximum water temperature in their study however they considered the river slope to be a constant value for the entire river which is not a reasonable assumption for rivers moving through significant topographic changes for such rivers the numerical model is divided into several meshes that represent the physical domain of the river and hence the bottom elevation and slope of each cell is specified separately estimating spatially varying slopes requires an accurate bathymetry dataset for the entire length of the river light detection and ranging lidar is one approach to get high resolution topographic data to represent the land surface for the river bathymetry a specific type of lidar data called green lidar is used since it can penetrate the water and detect the bathymetry boundaries in absence of green lidar bathymetry could be collected by in situ data collection techniques such as computerized hydrographic survey system land based positioning survey and etc rogala 1999 however in situ bathymetry data has lower spatial resolution comparing to lidar another limitation is that when acoustical sounding is used to determine the water depth in shallow rivers with high vegetation the acoustic signal reaches the vegetation instead of riverbed rogala 1999 hence in situ bathymetry data can have uncertainty in its values thereby requiring its effects to be considered in hydrodynamic models in contrast to water temperature sensitivity of water age travel time particle movement to riverine conditions and parameters has not been extensively studied and has not been developed at all for the klamath river the time elapsed from when a water particle moves from its boundary to a specific location is called water age delhez et al 1999 water age represents the travel time when the initial water age is set to zero shen and haas 2004 calculated the mean age and residence time of a released substance from the tributaries to the tidal york river and found that water age was primarily dependent on river discharge gong et al 2009 developed a three dimensional model to evaluate the effect of wind on particle travel time using the concept of water age in the tidal rappahannock river they found that water age distribution is highly affected by the local wind in estuaries and depended significantly on the interaction of wind buoyancy forcings and on the prior status of the circulation gong et al 2009 however all of these previous studies examined tidal rivers that interacted with estuaries and hence sensitivity of water age to riverine and meteorological parameters in shallow inland rivers is still not clear additionally the sensitivity of water age to the bathymetry data has not been evaluated in previous studies and there is a critical need to investigate the effect of a comprehensive set of parameters on the water age in shallow rivers sensitivity analysis sa is divided into two broad categories local sensitivity analysis and global sensitivity analysis pappenberger et al 2008 local sa considers the local impact of input parameters variation on model response by concentrating on thesensitivityin vicinity of a set of parameter values borgonovo and plischke 2016 unlike local sa global sa addresses the sensitivity relative to the entire range of parameters distribution dobler and pappenberger 2013 therefore local sa is not a robust approach in many cases and global sa is the preferred sensitivity analysis method cloke et al 2008 global sensitivity analysis is a valuable tool in understanding the model structures the main sources of model output uncertainty and deriving decisions to reduce model uncertainty ratto et al 2001 the most common global methods are the i regional sensitivity analysis rsa hornberger and spear 1981 ii variance based methods sobol 1993 and iii latin hypercube one at a time van griensven et al 2006 lh oat has been used by multiple studies and has been found to be an efficient sensitivity analysis technique holvoet et al 2005 li et al 2009 in this study we combined lh oat as a global sensitivity analysis tool with a three dimensional hydrodynamic model of an inland river klamath river in california to investigate the most sensitive model inputs and parameters that influence water age and water temperature water temperature and age both influence disease risk for klamath river salmon hence disease management actions stand to significantly benefit from a clear understanding of the factors that drive them 2 study site the klamath river basin is located in southern oregon and northern california it is divided into upper basin and lower basin at iron gate dam igd at river kilometer 304 the modeled reach for this study is located in california begins at igd and continues 100 km downstream to seiad valley river kilometers 304 204 this reach includes a zone of high infection risk for salmon that is likely to be influenced by a prescribed release of reservoir water via dam operations the lower klamath basin is generally covered by forest except the shasta and scott river drainages which are used primarily for agriculture and rangeland unlike the upper basin the lower basin is steep and rocky with a complex terrain two main tributaries join the main stem the shasta and scott rivers draining from the shasta and scott basins respectively in addition several smaller but still significant tributaries drain into the river including beaver creek horse creek and grider creek fig 1 3 methodology 3 1 data collection data required for building this model included flow measurements obtained from the united states geological survey usgs gauges fig 1 with stations ids 11516530 at igd 11517500 at shasta river 11519500 at scott river and 11520500 at seiad valley atmospheric data including the precipitation air temperature relative humidity wind speed and wind direction collected at collins baldy clb and slater butte srb stations the water temperature measurements were collected from karuk tribe water resources measurements at igd seiad valley and shasta river bathymetry data were obtained from a survey conducted by department of the interior bureau of reclamation usbor with support from the usgs they used two boats equipped with a multibeam acoustic doppler current profiler adcp interfaced with global positioning system gps however there are 6 areas with total length of 11 5 km where boats could not collect the data due to gaps in gps coverage and data collection issues aeration shallow depth etc woolpert inc gathered lidar data from the link dam upstream of igd and not shown in fig 1 to happy camp downstream of seiad and not shown in fig 1 with 0 91 m 3 ft resolution with coverage over the entire study area greimann et al 2011 since lidar used for collecting these data cannot penetrate the water the data represent the water surface elevation instead of the bathymetry water depth was also gathered by usbor for the entire length of study site in this study we used the lidar and water depth measurements to estimate bathymetry at locations where bottom bathymetry was not known these estimates were generated by subtracting the water depth from water surface elevation for areas with missing data however the water elevation and water depth were not collected on the same date so the generated bathymetry needed to be adjusted by comparison with measured bathymetry data at monitored locations these locations include tree of heaven beaver creek and community center sites wright et al 2014 where accurate bathymetry with error tolerance of approximately 0 03 m were collected using gps and echo sounder in 2014 woolpert also collected a set of bathymetric cross sections for 4 8 km starting from i 5 to the shasta river these two sets of monitored sites were used for bathymetry adjustment of unmonitored areas along the length of the river furthermore the data was collected by boats that moved along two longitudinal paths down the river with at the most only two bathymetry measurements at every cross section hence at cross sections the nearest neighbor interpolation was used to generate the bathymetry data for unmonitored locations in lateral direction table 1 summarizes the data used for bathymetry generation and adjustment 3 2 numerical model 3 2 1 hydrodynamics we used the environmental fluid dynamic code efdc to create a three dimensional hydrodynamic model of the lower klamath river this model has been applied to several studies to simulate the flow ji et al 2001 seo et al 2010 efdc contains three functional modules including hydrodynamics water quality and sediments contaminants it supports cartesian and curvilinear orthogonal horizontal coordinates and stretched or sigma vertical coordinates efdc solves the following continuity and momentum equations eqs 1 3 given by 1 h t hu x hv y w z q h 2 t hu x huu y hvu z wu fhv h x g ξ p p atm x h z x h z p z h 1 a v z u q u 3 t hv x huv y hvw z wv fhu h y g ξ p p atm y h z y h z p z h 1 a v z u q v where h is water depth u and v are horizontal velocity components in x and y direction respectively w is vertical velocity component in z direction qh is the volumetric source and sink term concerning evaporation and rainfall f is coriolis factor p is the water column hydro static pressure patm is the kinematic atmospheric pressure av is vertical turbulent momentum diffusion coefficients and qu and qv are momentum source sink terms the transport equation for temperature is tetra tech 2007 4 t ht x hut y hvt z wt z h 1 a v z t q t where t is temperature and qt is the source and since term 3 2 2 heat exchange surface and sediment heat exchange can be formulated as eqs 5 and 6 respectively 5 h n h s h a h c h e h sr h ar h br 6 h sw k sw t w t s where hn is the net rate of heat exchange hs is the short wave solar radiation ha is the long wave solar radiation hc is the heat conduction he is evaporative heat loss hsr the reflected short wave radiation har is the reflected long wave radiation hbr is the back radiation from the water surface hsw is the rate of sediment water heat exchange ksw is the coefficient of sediment water heat exchange tw is the water temperature and ts is the sediment temperature 3 2 3 water age water age can be calculated based on tracer and age concentration deleersnijder et al 2001 7 α t x t u α t x k α t x c t x where x is a coordinate u is the velocity in space and time c is the tracer concentration k is the diffusivity tensor and α is the age concentration 3 3 model set up curvilinear model grids were used in this study with five cells across the river and three vertical layers the longitudinal grid resolution ranged from 85 m to 150 m boundaries include the flows from igd the shasta and scott rivers beaver horse and grider creek and the open boundary at seiad valley the model was run for three days with one second time steps on intel r xeon r cpu e3 1240 v2 3 40 ghz processors 3 4 sensitivity method and effective parameters a complete set of parameters used in efdc are listed in tables 2 and 3 these parameters were used in the sensitivity analysis in order to investigate the model representation of water age and water temperature we applied lh oat van griensven et al 2006 a sampling method that combines latin hypercube and one factor at a time sampling approach this method is global because it scans the entire range of possible parameter values and possible parameter sets this stratified sampling of input parameters enables a robust estimation of output statistics this method involves taking n latin hypercube sample points user defined with a probability of occurrence equal to 1 n and varying each lh sample point by changing the parameters one at a time this method requires a total of n p 1 runs where p is the total number of parameters the partial effect si j in percent of each parameter is estimated as 8 s i j 100 m e 1 e i 1 f i e p m e 1 e i e p m e 1 e i 1 f i e p m e 1 e i e p 2 f i where m is the model function i and j refer to the parameters and lh point respectively ei refers to model parameters and fi is a fraction by which parameters are changed finally each parameter can be ranked as the largest effect to the smallest effect by giving rank 1 to a rank equal to the total number of parameters respectively the parameters listed in tables 2 and 3 were changed randomly over the range of 50 with five lh sampling points for each parameter range 3 5 model sensitivity to bathymetry data the calibrated model of the klamath river for 2015 was obtained from a previous model developed by javaheri et al 2016 the monte carlo sampling approach was used to assess the sensitivity of the model to bathymetry data this method samples from the possible range of the input values the samples are usually generated randomly based on the estimated error of bathymetry from section 4 1 the bottom elevation of each cell was perturbed among the range of error to create the ensembles the advantage of this method is that sensitivity and uncertainty analysis can be accomplished simultaneously to evaluate the sensitivity of water temperature and water age to bathymetry data we tested the responses of each model different scenarios since this approach is computationally intensive three i iii scenarios during 2015 were selected to evaluate the sensitivity of water temperature and two i ii were selected for estimating the sensitivity of water age these scenarios represent periods of i high water temperature and base flow summer ii low water temperature and peak flow winter and iii high salmon and parasite densities spring we selected the first two scenarios to capture the extremes in temperature and discharge and included the third for the temperature model because it represents the peak period of c shasta related infection and mortality in salmon 4 results and discussions 4 1 bathymetry data generation after subtracting the water depth from water surface elevation we concluded that water surface elevation provided by lidar data needed to be adjusted by 31 cm in order to minimize the error after water surface elevation adjustment and interpolation of data across the river the overall root mean square error between generated bathymetry and actual data was found to be less than 40 cm fig 2 4 2 water age travel time sensitivity based on the partial effects parameters were classified into three importance groups most important very important and slightly important fig 3 flow rate was classified as the most important parameter with partial effect of 22 global sensitivity rank equal to 1 table 4 bottom roughness with partial effect of 7 6 was classified as very important global sensitivity rank equal to 2 wind speed with partial effect of 0 12 was classified as slightly important global sensitivity rank equal to 3 wind direction and vertical eddy viscosity with partial effects of 0 05 0 09 were classified as not important 4 global sensitivity rank 5 horizontal mass diffusivity and vertical molecular diffusivity have no significant influence on the water age because their partial effects was found to be zero hence they were ranked 7 total number of parameters water age was slightly affected by bathymetry and flow rate did not impact the sensitivity of water age to bathymetry for the two scenarios selected for evaluating the sensitivity of water age to bathymetry we found that average variance decreases when number of bathymetry realizations generated via monte carlo increase when the sample size of a set of bathymetry realizations is more than 200 the average variance converges to a constant value fig 4 this suggests that this model with more than a 200 sample size could be representative of the bathymetry error in the system since flow rate is the most sensitive parameter our selection of two different periods during 2015 one with a high flow rate 100 m3 s and one with a low flow rate 28 m3 s to investigate the water age sensitivity to bathymetry data was appropriate the average travel time water age from iron gate dam to seiad valley is 19 5 h at 100 m3 s and 37 2 h at 28 m3 s fig 5 the average difference between lower bound and upper bound is about 20 min and the maximum difference between bounds are 25 min for low flow and 40 min for high flow 4 3 water temperature sensitivity based on the partial effects we classified parameters effective on water temperature into five groups fig 6 inflow temperature with partial effect of 27 was classified as the most important parameter global sensitivity rank equal to 1 table 5 air temperature solar radiation wind speed flowrate and wet bulb temperature with partial effects of 3 8 were classified as very important 2 global sensitivity rank 6 bottom roughness and fast scale solar short wave radiation attenuation coefficient with partial effects of 1 2 were classified as important 7 global sensitivity rank 8 fraction of solar short wave radiation absorbed in the top layer evaporation rate and heat transfer coefficient between bed and bottom water layer with partial effect of 0 13 0 6 were classified as slightly important 9 global sensitivity rank 11 finally vertical eddy viscosity convective heat coefficient between bed and bottom water layer wind direction thickness of active bed temperature layer fractional cloud cover atmospheric pressure horizontal mass diffusivity and vertical molecular diffusivity with partial effect less than 0 07 were classified as not important global sensitivity rank 12 the correlation between forcing data was calculated to estimate the interaction between the various parameters fig 7 it is seen that only dry temperature and wet bulb temperature are highly correlated correlation value of 0 91 since these two parameters are positively correlated the uncertainty in the model will be greater than if they were not correlated table 6 lists the most effective parameters on water temperature and their range for three periods the warmest period the coldest period and the period with highest fish mortality during 2015 to investigate the water temperature sensitivity to bathymetry the timely averaged variance versus the sample size was calculated to evaluate the model convergence fig 8 by increasing the sample size from 50 to 100 the average variance decreases however there is no significant change for sample size more than 100 fig 9 illustrates the average water temperature at saied valley with the minimum and maximum bounds of possible results it is seen that with 40 cm error in bathymetry the average and maximum difference between upper bound and lower bound are 0 48 c and 0 59 c respectively during the winter fig 9a however these differences are bigger for fig 9b the average and maximum difference between upper bound and lower bound are 1 2 c and 1 7 c respectively during the summer fig 9c the average and maximum difference between upper bound and lower bound are 1 65 c and 2 3 c respectively it demonstrates that water temperature is more sensitive to bathymetry for low flows and warm meteorological conditions 5 conclusion a three dimensional hydrodynamic model of the lower klamath river was developed to evaluate the most sensitive parameters in calculating the river water age and water temperature both water age and temperature influence salmon disease risk in this section of the klamath river and a better understanding of how these factors may respond to managed flow events is critical to evaluating the efficacy of such actions we used a global sensitivity analysis tool a combination of latin hypercube and one factor at a time sampling to estimate the partial sensitivity of each parameter and rank them from the most important to not important the analysis highlighted the importance of bathymetry and bottom topography bathymetry were generated using lidar data and water depth measurements collected at different times these data were then adjusted using bathymetry data collected at tree of heaven beaver creek and community center with overall error less than 40 cm this technique was used to estimate data for bathymetry gap due to gps coverage and data collection difficulties in addition flow rate and bottom roughness were found to be the most sensitive parameters in estimating the water age even though wind speed wind direction and vertical eddy viscosity have been found to be sensitive parameters for water age in estuaries gong et al 2009 water age was not found to be sensitive to these parameters in the relatively shallow klamath river horizontal mass diffusivity and vertical molecular diffusivity also had no impact on the water age this result may be applicable to other shallow rivers travel time was slightly affected by bathymetry considering the error in generated bathymetry data the average error between lower bound and upper bound of possible results was found to be less than 20 min also flowrate was not found to affect the travel time sensitivity to bathymetry inflow temperature was found to be the most influential parameter that affects water temperature air temperature solar radiation wind speed flowrate wet bulb temperature bottom roughness and fast scale solar short wave radiation attenuation coefficient are the other important parameters in calculating the water temperature respectively in contrast to gu and li 2002 we found that water temperature is sensitive to bathymetry especially during warm periods considering the estimate error in generated bathymetry the average difference between lower bound and upper bound of possible results are 1 65 c and 0 48 c during the summer and winter respectively finally this study is relevant for managers interested in using flow manipulation as a disease mitigation measure in the klamath river because we provide tools models for predicting 1 water age travel time and 2 water temperature managers need to know how long it takes for a parcel of water released from the dam to reach specific location in order to know when to monitor e g before during and after such data are necessary for determining how the timing and volume of water released affected parasite density and disease risk to fish at specific sites water age travel time is of interest because the addition of water from iron gate dam reservoir would result an input of parasite free water assumption water temperature is of interest because the addition of water from iron gate dam reservoir would result an input of cooler water during spring or summer to the mainstem previously managed water releases from igd have occurred when parasite densities and water temperatures are high with the aim to reduce both however managers and researchers have previously lacked simulation models to predict the arrival of the parcel of water at monitoring locations which has constrained monitoring in addition predicting the impacts on water temperature has not been examined the latter is critical for justifying the use of stored water particularly during droughts our results demonstrate that flow rate and bottom roughness were the two most important parameters that influence water age and that travel time from the dam to sv ranges from 37 2 to 19 5 h over 28 to 100 m3 s discharge thus monitoring activities should occur within that window we also demonstrate that water temperature was sensitive to inflow reservoir temperature which suggests flow management to manipulate water temperature is very depending on reservoir conditions in summary our results provide a framework for predicting how water within high infection risk sections of the river will respond to dam water low infection risk inputs this information will be useful for prioritizing the use of water age dilution versus temperature spore viability under certain contexts when considering flow manipulation as a method to reduce risk of infection and disease in klamath river salmon acknowledgements this publication was funded by the bureau of reclamation reclamation u s department of interior funding was provided by reclamation as part of it mission to manage develop and protect water and related resources in an environmentally and economically sound manner in the interest of the american public funding was provided through interagency agreement r15pg00065 the views in this report are the author s and do not necessarily represent the views of reclamation 
