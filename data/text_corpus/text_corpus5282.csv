index,text
26410,in this article we present the software panga which is a new open source tool for the evaluation of noble gas data from ground water samples in addition to most of the features of traditionally used software like noble it includes among others easy to use methods for monte carlo simulations and analyses as well as support for newer excess air models and ensemble fits we verify panga s results for two data sets by comparing them with the results of noble using different excess air models we conclude this study with a set of recommendations for the evaluation of noble gas data sets giving detailed step by step instructions including descriptions of the monte carlo patterns most commonly observed graphical abstract image 1 keywords groundwater recharge conditions noble gas temperature excess air parameter estimation software availability name of software panga program for the analysis of noble gas data developer michael jung e mail panga mjung org year first available 2013 software required operating systems windows xp or newer macos 10 8 or newer or linux hardware required any hardware that runs one of the above operating systems program language c program size about 9 mb availability and cost compiled binaries are available freely for download on http www iup uni heidelberg de tools panga a link to the github repository of the source code is also provided there panga is free software it can be redistributed and or modified under the terms of the gnu general public license version 3 as published by the free software foundation http www gnu org licenses gpl 3 0 en html 1 introduction noble gases being chemically inert and having a well known atmospheric source are useful tracers in many geochemical environmental and in particular aquatic systems burnard 2013 dissolved noble gases in water have become widely used tools in environmental systems such as oceans lakes groundwater sediment pore water or even speleothem fluid inclusions stanley and jenkins 2013 brennwald et al 2013 kluge et al 2008 with two main applications the first concerns age dating of the water where several methods based on noble gases are available including the accumulation of tritiogenic 3he 3h 3he method and radiogenic 4he kipfer et al 2002 the second major application is noble gas thermometry stute and schlosser 1993 aeschbach hertig and solomon 2013 where the concentrations of dissolved atmospheric noble gases provide a thermometer via the temperature dependence of gas solubilities in water these applications require an accurate separation of the noble gas components originating from equilibration with the atmosphere and radiogenic production further complications may arise in some environmental systems from the presence of additional phases such as ice hydrocarbons or other gases utting et al 2013 visser et al 2007 the quantitative interpretation of noble gases in water must thus be based on an accurate model of their partitioning between air water and possible additional phases this applies in particular for the noble gas paleothermometer in groundwater it has long been recognized that the entrapment of air bubbles during infiltration of groundwater leads to an additional dissolved noble gas component of atmospheric origin the so called excess air heaton and vogel 1981 correctly accounting for this component is crucial for the determination of reliable noble gas paleotemperature records therefore various models for the origin and composition of excess air and numerical tools to evaluate the resulting noble gas concentrations in groundwater have been developed in the field of noble gas thermometry in groundwater hydrology aeschbach hertig and solomon 2013 some of these models can also describe the less frequently encountered case of degassed samples which exhibit dissolved gas concentrations below atmospheric equilibrium aeschbach hertig et al 2008 in 1999 two groups independently presented an inverse modeling approach for the interpretation of dissolved noble gas concentrations in natural waters aeschbach hertig et al 1999 ballentine and hall 1999 this approach consists of a minimization of the error weighted deviation denoted as χ 2 between observed noble gas concentrations and those derived from models designed to describe these concentrations with few parameters such as the equilibration temperature the excess air content or the extent of degassing for example the goal is to derive quantitative estimates of the model parameters in particular the equilibration or recharge temperature and their uncertainties in principle the inverse approach is applicable to noble gas concentrations in any aquatic system as long as a model for the expected concentrations with few parameters less than the number of measured gases is available in practice it has mainly been applied to interpret atmospheric noble gas ne ar kr xe data sets from groundwater in terms of past recharge temperatures aeschbach hertig and solomon 2013 noble gases in lake sediment porewater have been used to reconstruct paleosalinity brennwald et al 2004 in a recent study by chatton et al 2016 the inverse approach has been adapted to a reduced set of noble gases ne ar combined with nitrogen as an additional quasi conservative gas for groundwater recharge temperature estimation another recent study has extended the principle of the noble gas thermometer to estimate recharge properties of the deep ocean loose et al 2016 at the time when the inverse approach was introduced only few models for the atmospheric noble gas components in groundwater were available and implemented in the first fitting tools these were the unfractionated excess air ua partial re equilibration pr and partial degassing pd models see e g kipfer et al 2002 aeschbach hertig and solomon 2013 shortly thereafter the closed system equilibration ce model was developed aeschbach hertig et al 2000 this new model along with other extensions was implemented in the software noble presented by peeters et al 2003 which became widely used for the purpose of parameter estimation from noble gas data sets derived from groundwater and sometimes also other systems in the meantime several new models for atmospheric noble gas components in groundwater have been presented in the literature most notably the oxygen depletion od hall et al 2005 and the gas diffusion relaxation gr sun et al 2008 models furthermore the reliability and numerical stability of the different models has been critically evaluated sun et al 2010 raising some questions with regard to a possible bias of the temperature estimates obtained from the ce model jung et al 2013 have shown the usefulness of monte carlo simulations in assessing and resolving such problems that sometimes afflict the parameter estimation based on the ce model using the noble software in this study we present panga a new stand alone software for the analysis of noble gas data from ground water samples it updates on noble by including newer excess air models and it is flexibly designed to enable easy incorporation of new models as they may be developed in the future it furthermore provides user friendly tools to conduct and evaluate monte carlo simulations and thus apply the procedures suggested by jung et al 2013 2 physical background noble gas concentrations in groundwater are assumed to consist of two components the equilibrium component i e the gas concentrations in water when it is in equilibrium with the atmosphere and the so called excess air which is an additional component caused by air entrapment during water table fluctuations in groundwater holocher et al 2002 in addition interaction of the groundwater with a gas phase may lead to a loss of dissolved gases usually referred to as degassing aeschbach hertig et al 2008 if degassing occurs the dissolved noble gas concentrations are lowered and may fall below the equilibrium concentrations a comprehensive review of the currently available models to describe the effects of excess air and degassing has recently been provided by aeschbach hertig and solomon 2013 in the following we only give a short summary of the different models however in contrast to aeschbach hertig and solomon 2013 throughout this article and in the software we do not use molar units for concentrations even though they lead to more elegant equations instead we use cm3 stp g for dissolved noble gas concentrations and express gas phase concentrations as partial pressures choosing these units is consistent with the model equations as given by kipfer et al 2002 and as used in the software noble this choice also avoids problems with unit conversion since most laboratories report their noble gas results in cm3 stp g which is an absolute and experimentally easily accessible measure for gas concentrations per water amount similarly partial pressures in the gas phase can directly be related to known atmospheric abundances expressing concentrations in mol l in both phases is advantageous for formal calculations but the conversion from these units to the experimentally accessible forms used here depends on environmental conditions like temperature pressure and salinity the equilibrium components of he ne ar and kr are calculated using the solubilities determined by weiss 1970 1971 weiss and kyser 1978 the equations given by clever 1979 were used for xe the implementation follows the recommendations of kipfer et al 2002 so far these are the only solubility equations implemented the program can however easily be extended to include others as well the following section provides a list of all available excess air or degassing models and their equations they all share three parameters which are needed to calculate the equilibrium component the temperature t measured in c the salinity s in g kg and the pressure p in atm another quantity used in all formulas is z i which stands for the volume or mole fraction of the specific noble gas in dry air unfractionated excess air ua model this traditional and most simple model of pure air addition can be expressed as follows c i t s p a c i e q t s p a z i a denotes the concentration of dissolved excess air in cm3 stp g partial re equilibration pr model this model assuming diffusive loss of excess air has been introduced by stute et al 1995 and can be written as c i t s p a f p r β c i e q t s p a z i exp f p r d i d ne β f pr is the dimensionless parameter for excess air loss due to re equilibration β is the dimensionless exponent in the relationship of the gas transfer velocity to the diffusivity in water the diffusion coefficients d i are calculated following jähne et al 1987 except for d ar which was interpolated from the values of the other noble gases assuming that d is inversely proportional to the square root of the atomic mass a is the same as in the ua model partial degassing pd model this model assuming diffusive loss of all noble gas components can be formulated as follows compare aeschbach hertig et al 2008 c i t s p a f p d β c i e q t s p a z i exp f p d d i d ne β f pd is the dimensionless parameter for diffusive gas loss from the dissolved phase into a noble gas free gas phase β d i and a are the same as in the pr model oxygen depletion od model this model assuming an increase of noble gas partial pressures in soil air in response to oxygen depletion has been introduced by hall et al 2005 and may be written as c i t s p a p o d c i e q t s p p o d a z i in this model p od denotes the pressure increase factor which is dimensionless a is again the same as in the previous models gas diffusion relaxation gr model this extension of the od model assumes partial loss of excess air similar to the pr model but governed by diffusion in the gas phase it has been formulated by sun et al 2008 c i t s p a p o d f g r c i e q t s p p o d a z i exp f g r d i β f gr is the dimensionless parameter for excess air loss due to gas phase diffusion β is the dimensionless exponent in the relationship of the gas transfer velocity to the gas phase diffusion coefficients d i which are calculated according to benítez 1948 closed system equilibration ce model this model assuming equilibration between groundwater and trapped air bubbles has been formulated by aeschbach hertig et al 2000 c i t s p a f c i e q t s p 1 f a z i 1 f a z i c i e q here f denotes the dimensionless fractionation factor by which the size of the gas phase has changed during re equilibration the parameter a has a different meaning than in the other models it describes the initial amount of entrapped air per unit mass of water and is measured in cm3 stp g note that this parameter becomes a dimensionless ratio of two volumes if all concentrations are expressed in molar units as used in the most recent representations of the ce model given by aeschbach hertig et al 2008 and aeschbach hertig and solomon 2013 the two representations can be converted into each other using the following formula a c m 3 s t p g a ρ g cm 3 p e p 0 t 0 t t 0 and p 0 are standard temperature and pressure t and p are temperature and pressure governing atmospheric equilibrium e is the saturation vapor pressure and ρ the density of water 3 the panga application panga stands for program for the analysis of noble gas data and is a new open source software for the evaluation of noble gas data from water samples its main functionality is comparable to the software noble by peeters et al 2003 including ensemble and monte carlo fits it includes however the more recent od and gr excess air models and can calculate monte carlo fits much faster additionally it is possible to perform a graphical evaluation of monte carlo data as needed e g for applying the method proposed by jung et al 2013 to solve problems which occasionally arise when the ce model is applied to groundwater data sets it also provides means for the interactive exploration of the χ 2 space of a fit 3 1 algorithms panga calculates best estimates of the parameters of the above described models using typical methods of inverse modeling such as employed e g by mcgrail 2001 the specific implementation chosen here follows essentially the approach described by aeschbach hertig et al 1999 in this method the quantity χ 2 which is the error weighted square sum of the deviations between measured and modeled noble gas concentrations is minimized 1 χ 2 i c i c i mod 2 σ i 2 the minimization is carried out using a modern implementation of the levenberg marquardt algorithm marquardt 1963 as suggested by moré 1978 the uncertainties of the model parameters and of derived quantities are calculated from the covariance matrix v p of the fit parameters p which is estimated by the fitter according to the following equation bard 1974 2 v p j t v c 1 j 1 v c is the covariance matrix of the measured noble gas concentrations the jacobian matrix j consists of the derivatives of the modeled noble gas concentrations with respect to the fit parameters j i j c i mod p j the parameter uncertainties are determined by taking the square root of the variances which are located on the main diagonal of v p the uncertainties of quantities derived from the fit parameters such as the modeled concentrations of all noble gases including he isotopes denoted by y p are calculated according to bard 1974 3 v y j v p j t here the jacobian matrix j contains the derivatives of the quantities y with respect to the fit parameters j i j y i p p j note that the covariance matrices as given in 2 and 3 are only approximations most of the time they yield good values for the uncertainties but if the model equations are too nonlinear in the vicinity of the parameter estimates the values may deviate considerably from the true ones better and more robust error estimates can be obtained by using monte carlo simulations as explained below the idea of monte carlo simulations is to directly explore the range and distribution of parameter values that corresponds to the range of noble gas concentrations in accordance with the measurement results and their uncertainty the calculation of monte carlo data is done as follows compare e g checchi et al 2007 at first a set of monte carlo realizations of the sample under consideration is created by randomly altering the sample concentrations according to normal distributions with standard deviations equal to the uncertainties of the input data then a fit is performed for each of these monte carlo realizations afterwards the results of these fits are used for statistical analysis of the resulting parameter distributions this procedure represents an explicit assessment of the parameter uncertainties resulting from equation 2 3 2 features the main user interface of panga shows a list of samples with their measured noble gas concentrations on the right and a panel to select fit options on the left fig 1 in its most basic mode of operation panga finds a set of model parameters in such a way that the modeled noble gas concentrations reproduce the measured sample concentrations as closely as possible i e with minimal χ 2 this is done for all samples independently of each other the user selects the noble gas concentrations to be included in the fitting procedure the model to be applied and the parameters to be fitted model parameters which are not supposed to be varied can be set to fixed values either globally or independently for each sample in the same manner initial parameter values for the fitting algorithm may be specified either globally or individually individually set fixed or initial parameter values are listed in the lower right panel of the interface fig 1 users can easily copy and paste values from data files into this table if desired the range of possible values of the parameters may be constrained checkbox constrained fit to only include physically meaningful values this option currently constrains the parameters a and f in the ce model to positive values and the parameter p o d in the od model to the range of 1 p o d 1 264 which can be produced by removal of oxygen from air freundt et al 2013 as another option the software can also perform a so called ensemble fit i e it does not treat the samples individually but combines all of them in a single fit in this mode each fit parameter can either be varied independently for each sample or it can be fitted to the ensemble of samples i e it has the same value for every sample in both modes the user can choose to additionally carry out a specific number of monte carlo calculations which can be used to determine more accurate estimates for the fit parameters and their errors and to check whether or not the fit shows any kind of abnormal behavior all the values calculated for normal fits are also saved for every monte carlo realization and one dimensional histograms of their frequencies of occurrence can be created for all of them additionally any two parameters may be combined into a two dimensional histogram both kinds of histograms can be used to select a subset of the monte carlo realizations which can then be applied as a mask to the monte carlo results of the sample s other parameters this can be used for example to restrict the analysis of the sample to one of multiple clusters as suggested by jung et al 2013 for certain problematic cases the program performs a basic statistical analysis consisting of mean values standard deviations and correlation coefficients for the remaining monte carlo realizations and summarizes these statistical characteristics in a table a few 100 000 up to several millions of monte carlo realizations can be fitted in a few minutes depending on computation power and on the properties of the samples such a large number of simulations is not needed in order to obtain stable estimates of the mean and standard deviation of the parameter distributions some ten thousand simulations suffice for that purpose however considering that calculation time will usually not be a strong limitation for the monte carlo analysis a larger number of simulations is recommended in order to improve the visual representation of the data in two dimensional histograms the output of the fitting procedure consists of the following quantities for every single fit degrees of freedom of the fit the final χ 2 value together with the probability of obtaining this or a higher value based on the χ 2 distribution under the assumption that the model is correct and deviations are due to statistical measurement errors the best estimates of the fit parameters together with their uncertainties derived from the covariance matrix as explained above the off diagonal elements of the covariance matrix in the form of correlation coefficients which need to be used for error propagation if further quantities are derived from the fit parameters the residuals of the fit the equilibrium components of the modeled concentrations the final modeled concentrations because χ 2 may have multiple local minima it can sometimes be helpful to analyze the structure of the χ 2 surface in the parameter space in order to find irregularities or to verify that the obtained minimum also is the global one jung et al 2013 for these cases panga provides the χ 2 explorer mode which shows contour plots of two dimensional cross sections of the χ 2 surface the user can freely choose the parameters on the plot s axes and can either fix the remaining parameters to a certain value or leave them to be fitted this feature allows for an interactive exploration of the χ 2 surface as the results are calculated on the fly while the user pans or zooms the plot 3 3 implementation panga is written completely in c and runs on microsoft windows linux and os x for its calculations and user interface it makes use of several libraries the eigen template library http eigen tuxfamily org is used for all algebraic computations eigen comes with a port of minpack which is a reliable and robust implementation of the levenberg marquardt algorithm the user interface was implemented using the qt framework http qt project org plots are created with the qwt library http qwt sourceforge net the boost libraries http www boost org are used for many other aspects like multi threading serialization of data and compression 4 comparison with existing results in order to verify the accuracy of the fits produced by panga we compared its output to the results generated by noble using two published noble gas data sets the first data set consists of 44 samples from the ledo paniselian aquifer in belgium and was taken from the study of blaser et al 2010 the second data set was taken from the study of castro et al 2007 and consists of 41 samples from the carrizo aquifer in texas usa for the comparison both noble and panga were run without constraints on the range of parameter values 4 1 ledo paniselian aquifer belgium all 44 samples from 39 wells reported in the study of blaser et al 2010 were fitted with the ce model 34 had excess air whereas degassing occurred in 10 of them we label the samples with be followed by the well number or name as used in the original publication the fitting was carried out with the following initial values for the parameters see fig 1 t was set to 2 c a to 0 01 cm3 stp g f has to be chosen according to the type of sample to prevent the fitter from falling into a wrong local minimum for the samples with excess air and with degassing the initial values 0 5 and 3 were used respectively the fixed parameter p was set to 0 9976 atm corresponding to an altitude of 20 m asl salinity s was set individually for each sample the complete input values for the belgium samples are made available on the download page of the software as a test data set except for five special cases which will be discussed later on most results were very close to those provided by noble i e the best parameter estimates showed typical deviations of about 0 002 0 017 c for t 0 03 10 3 to 1 50 10 3 cm3 stp g for a and 0 02 10 3 to 2 50 10 3 for f which correspond to relative deviations of around 0 1 0 3 for t 0 04 1 7 for a and 0 004 0 3 for f the samples be 7 and be 9 however showed significant deviations of the f parameter of almost 0 1 for sample be 29 the deviation for f was 0 02 these three samples have in common that their f and a parameters are ill determined their errors are between 5 and almost 150 times as high as their values if their deviations are compared to the estimated parameter errors they are however still smaller than 1 3 for all parameters also the error estimates of the two programs agree well the deviations of estimated parameter errors are typically of the same order of magnitude as the deviations of the parameter values if the parameters are however not very well determined i e their uncertainties become about the same size as their values the deviations of the estimated errors can increase drastically a detailed comparison of the results led to the conclusion that discrepancies in the modeled xe equilibrium concentrations of about 0 01 caused these deviations further investigation showed that these discrepancies were due to noble using an additional factor for the conversion of molar units derived from clever 1979 instead of the simple factor x xe as recommended by kipfer et al 2002 noble uses x xe 1 x xe with x xe being the mole fraction solubility of xe which is small compared to 1 for a better comparison the additional 1 1 x xe factor was temporarily incorporated into panga and the analysis was repeated now all the samples except for the five aforementioned special cases were in very good agreement with noble i e the deviations of the parameters were in the majority of cases smaller than 5 10 4 c 10 5 cm3 stp g and 10 5 for the parameters t a and f respectively even for samples with comparatively high parameter uncertainties the deviations of the parameter estimates did not exceed 0 005 c 4 10 4 cm3 stp g and 7 10 4 in the three above mentioned cases with ill determined parameters the absolute deviations of the parameter errors were still higher but they were always lower than 0 7 of the estimated errors the five aforementioned special samples be 7new be 9new be 12 be 30 be 32 exhibit unusual and unexpected values of the f parameter i e highly negative ranging from 1200 to 2200 or in one case be 12 highly positive about 4200 the corresponding uncertainties of f are very large and range from 10 7 to 10 8 the a values are quite low and range from 3 10 7 to 10 6 cm3 stp g with uncertainties of 6 10 3 up to 3 10 2 cm3 stp g obviously a and f are ill defined in these cases nevertheless the fit results obtained with panga and noble of these samples show good agreement for the t parameter with differences of 0 13 at most the temperature uncertainties however deviate from each other by up to 47 for the ill defined parameters the discrepancies are large the results of noble for the absolute values of f are much lower roughly by an order of magnitude whereas the values for a are higher again by about one order of magnitude within the extremely large errors however these results still agree remarkably despite the seemingly erratic results for a and f the product a f is the same for both fitters with differences of less than 0 8 these results can be understood if one considers the following limiting case of the ce model 4 c i c i e q 1 f a z i 1 f a z i c i e q f 1 c i e q f a z i 1 f a z i c i e q this equation shows that in the case of f 1 the modeled concentrations only depend on the product a f therefore one can expect that a and f individually can only be determined with a high degree of uncertainty and that small numeric differences in the calculations can have a large impact on the estimates of these two parameters the t parameter however is not influenced by this effect and for this reason the temperature estimates don t show any significant deviations it is interesting to note that if additionally f a z i c i e q 1 which generally is the case for f a 0 01 c m 3 s t p g equation 4 can further be simplified to 5 c i c i e q f a z i this is identical to the ua model with a being replaced by f a for the samples just discussed f a lies between 4 10 4 cm3 stp g and 0 002 cm3 stp g i e the latter approximation is not fully justified in all cases nevertheless equation 5 shows that large negative values of f in combination with small positive values of a can lead to a reasonable description of the observed noble gas concentrations which approximates the simple case of unfractionated excess air the one case with a large positive f and small positive a actually corresponds to a degassed sample that approximates a ua model with a hypothetical negative excess air parameter 4 2 carrizo aquifer in texas usa the study of the carrizo aquifer by castro et al 2007 contains 49 samples from 20 different wells labeled by tx followed by a well number and a number for the replicate analyses in the original paper eight of the 49 samples were removed from the analysis because of their χ ua 2 values being too high the remaining 41 samples were evaluated as an ensemble using the od model with one p od parameter common for all wells by varying this parameter and fitting ua models with correspondingly adjusted pressure to each sample individually the authors found a minimal total χ od 2 value of 113 0 at p od 1 14 table 1 we repeated this analysis using panga to fit the same selected 41 samples with the od model in an ensemble fit i e the parameters a and t were fitted independently for each sample but only a single p od value was determined for the whole data set the value obtained in this way is p od 1 114 0 020 at χ od 2 73 9 table 1 this value differs from the original result of castro et al 2007 although the two values are probably compatible within error no formal uncertainty estimate for p od was given in the original paper a comparison of ua model results of panga with those reported by castro et al 2007 also showed deviations we determined a total χ ua 2 of 109 2 whereas castro et al 2007 found χ ua 2 201 6 furthermore our temperature estimates where systematically lower roughly by 0 1 c these deviations are likely due to the use of different evaluation methods incorporating for example different solubility equations on the one hand and the use of slightly different input data on the other hand differences in the input may include the assumed fixed values for pressure and salinity which we were unable to reproduce from the information given in the article and the uncertainties of the measured noble gas concentrations which were only specified as general percentages in the paper but might have been available in more detail for the original analysis in order to test the reliability of our fitter we compared with noble also for the carizzo data set again using the aforementioned additional factor 1 1 x xe so as to match noble s solubility equilibrium concentrations in the best possible way the ua model results are in very good agreement with each other none of the samples including the eight samples with high χ ua 2 showed relative deviations in the model parameters or their uncertainties which exceeded 10 ppm because noble cannot natively fit the od model we determined the best estimate of p od in the same way as castro et al 2007 i e by running multiple ua fits with pressure p p p od for different values of p od p is the pressure estimated from the recharge altitude in this way we found a minimal χ od 2 of 73 8 at p od 1 112 table 1 this compares well with the value obtained by the ensemble fit of panga which with the xe modifications in place slightly increased to p od 1 115 0 020 at χ od 2 73 9 table 1 the deviation between the two values corresponds to 13 of the estimated error 5 evaluation process the evaluation of groundwater noble gas data by inverse modeling tools requires a certain expertise to judge the obtained results and choose a reasonable evaluation strategy as sun et al 2010 have demonstrated in detail there are systematic offsets between the noble gas temperatures estimated with different excess air models but the temperature variation within a data set is robust with regard to the model choice it is therefore strongly recommended to evaluate coherent data sets from which past changes in recharge temperatures are to be estimated with only one model for all samples as our new software panga adds additional ways to treat individual samples even more care should be taken to obtain a consistent and appropriate evaluation of any given data set we therefore recommend to start with simple traditional evaluation methods and only use the advanced models and features such as monte carlo simulations and ensemble fitting when clearly indicated in the following we describe our recommended step by step evaluation approach for groundwater samples with a particular emphasis on the application of the ce model which probably has been the most frequently used model in the past the ce model is also most flexible and its parameters have a clear physical interpretation aeschbach hertig et al 2008 but sometimes it produces unexpected or even unphysical fit results a fact that needs special attention in the evaluation process before starting the data evaluation care should be taken to check the quality of the input data and in particular the reliability of the estimated analytical errors of the noble gas concentrations the assigned uncertainties obviously have a strong influence on the fit target χ 2 equation 1 and they also define the range of variation of monte carlo realizations of a sample the analytical uncertainties should represent the reproducibility of repeated measurements and also account for possible systematic offsets compared to the reference concentrations for air equilibrated water if they cannot be avoided or corrected for very important is furthermore a reliable estimation of model parameters that are prescribed such as atmospheric pressure from recharge altitude or salinity it also makes sense to consider any additional information on the samples such as the hydrogeological setting and sampling conditions although in our experience it is often difficult to relate fitting outcomes to such information the different special cases occurring in ce model fits as discussed in the preceding section are usually not related to other properties of the samples except maybe for the degassing case which is often related to bubble formation during sampling due to high gas contents of the groundwater 5 1 step 1 ua model fits in the beginning fitting the ua model which is the most simple and also numerically most stable excess air model can give helpful information about the samples for example very small or very large values of the parameter a may indicate unusual excess air conditions possibly due to equilibration or air contamination during sampling whereas negative a values indicate that the samples were probably affected by degassing the χ 2 values obtained by the ua model and the corresponding probabilities from the χ 2 distribution also provide some orientation based on our experience with many data sets we recommend to reject fits with probabilities below 0 01 aeschbach hertig et al 2000 2002 2008 aeschbach hertig and solomon 2013 therefore low χ 2 values with corresponding probabilities larger than 1 may in principle supersede any further evaluations but this is rarely the case for entire data sets samples with higher χ 2 values p 0 01 call for more complex models accounting for excess air fractionation with an additional parameter which in general can improve on the ua model fits very high χ 2 values from the ua model however may indicate some problem with the data that no model will be able to explain monte carlo analysis is unlikely to yield any improvements for the ua model where parameter correlations are low 5 2 step 2 ce model fits as a next step we suggest fitting the ce model which can describe both excess air and degassing and contains other models as limiting cases aeschbach hertig et al 2008 information from fitting the ua model in the first step should be taken into account for the selection of initial fit parameters initial values of a 0 01 cm 3 stp g and f 0 5 usually worked very well for samples with excess air whereas f 3 seems to be a good choice for degassed samples in order to find the minimum with f 1 corresponding to degassing the temperature initial values should be close to the expected paleotemperatures or somewhat lower to prevent the fitter from falling into a second local χ 2 minimum at higher temperatures and unrealistically high a values which is present in most samples cf jung et al 2013 for many samples standard ce model fitting works very well and provides sound parameter and uncertainty estimates as well as reasonable χ 2 values as a guidance we consider the following ranges of the ce model parameters a and f and the temperature uncertainty δ t as reasonable a 0 05 cm 3 stp g 0 1 f 1 and δ t 2 c the χ 2 fit probability should be greater than 1 in order to work out whether or not monte carlo fits would significantly improve the parameter estimates and their uncertainty in case of such samples with well behaved ce model fits we analyzed around 250 samples from five different data sets a comparison of fit results with parameter estimates in the standard ce model range with their respective monte carlo counterparts showed good agreement of the temperature parameter in almost all cases in two cases the mean monte carlo temperature was about 0 7 0 8 c lower than the fit temperature in five cases it was about 0 4 c lower for three other cases the temperature estimates agreed well but the monte carlo temperature error was about 0 4 c lower so for most well behaved samples monte carlo analysis does not improve parameter estimation and may be omitted in such cases the analysis may be terminated at this point unless there are reasons such as suspected temperature biases or oxygen depletion that motivate the exploration of further models 5 3 step 3 monte carlos fits we recommend carrying out monte carlo fits if the ce model fit results do not meet the criteria given above among these samples two special patterns are quite common first comparatively high estimates of a in conjunction with large temperature uncertainties and second highly negative f values typically between 1000 and 5000 combined with low a values less than 10 6 cm3 stp g or in rare cases with interchanged signs i e high positive f together with small negative a the first case has been discussed in detail by jung et al 2013 the second case was discussed above for some samples from belgium in both cases monte carlo analyses and their appropriate evaluation as demonstrated later in this section can help to obtain better results it is recommended to begin with unconstrained fits to avoid any problems in the boundary area which could be caused by the constraints only if the results indicate it the fits should be repeated in constrained mode 5 4 examples of common monte carlo cases in the following sections we will list the most common patterns appearing in the evaluation process and give recommendations on how to handle them to illustrate the different cases we will use the following set of samples their noble gas concentrations and the values used for p and s are summarized in table 2 be 8 and tx48 2 were taken from the belgium and texas data sets which we already used in the previous section ind 25 3 ind 19 2 are two samples from a sedimentary aquifer system in the north cambay basin in northwest india wieser 2011 the last sample cn 16 2 is from a well in the beijing area and part of a recent study in the north china plain schneider 2014 table 3 shows the results of an initial ua model fit the a estimates indicate that all samples but cn 16 2 have excess air cn 16 2 however has negative a meaning that degassing has probably occurred for the samples ind 25 3 ind 19 2 and cn 16 2 the fit probabilities are very low the χ 2 probability of be 8 is almost acceptable tx48 2 is one of the rare cases where the probability is very good so that further fitting might not be necessary nevertheless all samples will be further analyzed here to show their different behaviors with respect to ce model monte carlo fitting as they have excess air all samples but the last one should be fitted with an initial f smaller than 1 the remaining sample cn 16 2 should have an initial f greater than 1 the ce model fit can now be used to distinguish between the different cases 5 4 1 normal case be 8 fig 2 shows the standard case all parameter estimates and uncertainties look realistic and are in the above mentioned range monte carlo analysis is usually not necessary in this case and will probably only show small deviations cf table 4 5 4 2 high a in combination with large temperature uncertainties the two samples from india ind 25 3 and ind 19 2 both show high values of a in combination with large temperature uncertainties this case is usually accompanied by too high temperature estimates which may however still be in the realistic range this case has been analyzed in detail by jung et al 2013 it appears to be triggered by a combination of high ar and low xe cocentrations which however is not easily recognizable from the concentration data alone samples of this type need to be analyzed using monte carlo fits their histograms normally show a split up of the a and t parameters into two peaks or in the two dimensional a t histogram the formation of two more or less separate clusters one of which is at unrealistic parameter values in particular high values of a and contains the original fit result the other cluster shows more realistic a values and as described in more detail by jung et al 2013 corresponds to the true parameter values therefore the evaluation needs to be restricted to the realistic cluster whereas the monte carlo realizations in the unrealistic cluster are dropped in fig 3 a this process is illustrated for the sample ind 25 3 however in rare cases like ind 19 2 which is shown in fig 3b this approach is not feasible because all or almost all monte carlo results lie in the cluster at unrealistic values for these samples it is impossible to retrieve the correct results as the realistic cluster is not visible 5 4 3 ua limit case the ce model includes two different ua model limit cases the first one is f 0 in which case the ce model equation simplifies to c e q i a z i which is the ua model the other less obvious case is the one mentioned above in equations 4 and 5 if unconstrained fits are used the latter case is the one appearing most of the time however because the approximation in equation 5 is not correct in all cases the results correspond to some sort of mathematical mixing of the ua and ce models which does not need to have any physical meaning it is therefore advisable to use constrained fits for these types of samples and thereby prevent the fit from falling into the cases with highly positive or highly negative f fig 4 shows constrained monte carlo results of the sample tx48 2 which is an example for the ua limit case monte carlo statistics typically reproduce the fit temperature very well but lead to lower temperature uncertainties and should therefore be carried out whenever this limit case occurs even though this is a ua model limit case the ua model should not be used for the statistics as not all ce model monte carlo results fall into this limit case and therefore contribute to a higher temperature error which is still lower than the ce model fit error 5 4 4 combination of high a and ua limit cases the ua limit case sometimes appears together with the case of high a i e the direct fit of the sample shows either behavior but the monte carlo histograms show clusters at high a as well as in the ua limit case for this type of sample the analysis should be restricted to the normal and the ua limit case clusters dropping the cluster at high a values 5 4 5 degassed samples degassed samples exhibit f values larger than 1 and the corresponding ce model solutions are obtained by using an initial value of f 1 besides this they show the same special monte carlo patterns as samples with excess air in addition to the normal case as shown in fig 5 they can have high a values be in the ua limit case here normally with large positive f or show a combination of both this means that the same evaluation methods as for samples with excess air may be tried we did however not analyze these cases and therefore recommend the more conservative approach of using all monte carlo realizations for the statistical analysis in addition in the case of degassed samples the pd model based on diffusion controlled degassing should also be tried this model may be appropriate for fast gas stripping due to depressurizing during sampling whereas gas loss by partitioning into a subsurface gas phase is likely controlled by solubility equilibrium visser et al 2007 as described by the ce model in our experience and in agreement with the findings of aeschbach hertig et al 2008 the ce model tends to provide the better fits to degassed samples in some cases nevertheless the pd model can describe the measured noble gas concentrations more accurately 6 conclusions and future plans with panga we provide a new easy to use tool for the analysis of noble gas samples we have demonstrated the correct functioning of our implementation by showing that the results obtained by panga are nearly identical to those derived with the widely used program noble except for some deviations in special unphysical cases of the ce model these deviations in the ua limit case are of little practical relevance as we anyway recommend to use constrained fitting in these cases essentially reducing the model to the simpler and more robust ua model an advantage of panga is that it allows for a fast evaluation workflow for well behaved samples as well as problematic cases which need special treatment like statistical analysis over a subset of the monte carlo results with its support for ensemble fits and more recent excess air models like the od model it can also be used for most special cases that might occur in order to guide the user through the many features and possible evaluation steps provided by panga we have given a step by step recommendation for the evaluation process for various cases that according to our experience typically occur when the ce model is used to interpret noble gas concentrations from groundwater samples another strength of panga is its flexible structure and availability as open source software enabling us and other users to add extensions if desired for example new excess air models might be incorporated in the future as well as other equations for the calculation of noble gas equilibrium concentrations users may want to create a plugin interface for data input which will allow for a better integration with existing infrastructure like sample databases for some applications it might make sense to include dissolved nitrogen as a nearly conservative gas in addition to the noble gases a more sophisticated extension could be the incorporation of expert knowledge to better constrain some parameters e g a soft upper boundary for the parameter a in the ce model instead of the purely physical constraints available now acknowledgments we thank all members of our research group who contributed to the testing and optimization of panga during its development we also thank bryant jurgens and two anonymous reviewers for constructive comments this work was supported by the german research foundation dfg through grant ae 93 7 
26410,in this article we present the software panga which is a new open source tool for the evaluation of noble gas data from ground water samples in addition to most of the features of traditionally used software like noble it includes among others easy to use methods for monte carlo simulations and analyses as well as support for newer excess air models and ensemble fits we verify panga s results for two data sets by comparing them with the results of noble using different excess air models we conclude this study with a set of recommendations for the evaluation of noble gas data sets giving detailed step by step instructions including descriptions of the monte carlo patterns most commonly observed graphical abstract image 1 keywords groundwater recharge conditions noble gas temperature excess air parameter estimation software availability name of software panga program for the analysis of noble gas data developer michael jung e mail panga mjung org year first available 2013 software required operating systems windows xp or newer macos 10 8 or newer or linux hardware required any hardware that runs one of the above operating systems program language c program size about 9 mb availability and cost compiled binaries are available freely for download on http www iup uni heidelberg de tools panga a link to the github repository of the source code is also provided there panga is free software it can be redistributed and or modified under the terms of the gnu general public license version 3 as published by the free software foundation http www gnu org licenses gpl 3 0 en html 1 introduction noble gases being chemically inert and having a well known atmospheric source are useful tracers in many geochemical environmental and in particular aquatic systems burnard 2013 dissolved noble gases in water have become widely used tools in environmental systems such as oceans lakes groundwater sediment pore water or even speleothem fluid inclusions stanley and jenkins 2013 brennwald et al 2013 kluge et al 2008 with two main applications the first concerns age dating of the water where several methods based on noble gases are available including the accumulation of tritiogenic 3he 3h 3he method and radiogenic 4he kipfer et al 2002 the second major application is noble gas thermometry stute and schlosser 1993 aeschbach hertig and solomon 2013 where the concentrations of dissolved atmospheric noble gases provide a thermometer via the temperature dependence of gas solubilities in water these applications require an accurate separation of the noble gas components originating from equilibration with the atmosphere and radiogenic production further complications may arise in some environmental systems from the presence of additional phases such as ice hydrocarbons or other gases utting et al 2013 visser et al 2007 the quantitative interpretation of noble gases in water must thus be based on an accurate model of their partitioning between air water and possible additional phases this applies in particular for the noble gas paleothermometer in groundwater it has long been recognized that the entrapment of air bubbles during infiltration of groundwater leads to an additional dissolved noble gas component of atmospheric origin the so called excess air heaton and vogel 1981 correctly accounting for this component is crucial for the determination of reliable noble gas paleotemperature records therefore various models for the origin and composition of excess air and numerical tools to evaluate the resulting noble gas concentrations in groundwater have been developed in the field of noble gas thermometry in groundwater hydrology aeschbach hertig and solomon 2013 some of these models can also describe the less frequently encountered case of degassed samples which exhibit dissolved gas concentrations below atmospheric equilibrium aeschbach hertig et al 2008 in 1999 two groups independently presented an inverse modeling approach for the interpretation of dissolved noble gas concentrations in natural waters aeschbach hertig et al 1999 ballentine and hall 1999 this approach consists of a minimization of the error weighted deviation denoted as χ 2 between observed noble gas concentrations and those derived from models designed to describe these concentrations with few parameters such as the equilibration temperature the excess air content or the extent of degassing for example the goal is to derive quantitative estimates of the model parameters in particular the equilibration or recharge temperature and their uncertainties in principle the inverse approach is applicable to noble gas concentrations in any aquatic system as long as a model for the expected concentrations with few parameters less than the number of measured gases is available in practice it has mainly been applied to interpret atmospheric noble gas ne ar kr xe data sets from groundwater in terms of past recharge temperatures aeschbach hertig and solomon 2013 noble gases in lake sediment porewater have been used to reconstruct paleosalinity brennwald et al 2004 in a recent study by chatton et al 2016 the inverse approach has been adapted to a reduced set of noble gases ne ar combined with nitrogen as an additional quasi conservative gas for groundwater recharge temperature estimation another recent study has extended the principle of the noble gas thermometer to estimate recharge properties of the deep ocean loose et al 2016 at the time when the inverse approach was introduced only few models for the atmospheric noble gas components in groundwater were available and implemented in the first fitting tools these were the unfractionated excess air ua partial re equilibration pr and partial degassing pd models see e g kipfer et al 2002 aeschbach hertig and solomon 2013 shortly thereafter the closed system equilibration ce model was developed aeschbach hertig et al 2000 this new model along with other extensions was implemented in the software noble presented by peeters et al 2003 which became widely used for the purpose of parameter estimation from noble gas data sets derived from groundwater and sometimes also other systems in the meantime several new models for atmospheric noble gas components in groundwater have been presented in the literature most notably the oxygen depletion od hall et al 2005 and the gas diffusion relaxation gr sun et al 2008 models furthermore the reliability and numerical stability of the different models has been critically evaluated sun et al 2010 raising some questions with regard to a possible bias of the temperature estimates obtained from the ce model jung et al 2013 have shown the usefulness of monte carlo simulations in assessing and resolving such problems that sometimes afflict the parameter estimation based on the ce model using the noble software in this study we present panga a new stand alone software for the analysis of noble gas data from ground water samples it updates on noble by including newer excess air models and it is flexibly designed to enable easy incorporation of new models as they may be developed in the future it furthermore provides user friendly tools to conduct and evaluate monte carlo simulations and thus apply the procedures suggested by jung et al 2013 2 physical background noble gas concentrations in groundwater are assumed to consist of two components the equilibrium component i e the gas concentrations in water when it is in equilibrium with the atmosphere and the so called excess air which is an additional component caused by air entrapment during water table fluctuations in groundwater holocher et al 2002 in addition interaction of the groundwater with a gas phase may lead to a loss of dissolved gases usually referred to as degassing aeschbach hertig et al 2008 if degassing occurs the dissolved noble gas concentrations are lowered and may fall below the equilibrium concentrations a comprehensive review of the currently available models to describe the effects of excess air and degassing has recently been provided by aeschbach hertig and solomon 2013 in the following we only give a short summary of the different models however in contrast to aeschbach hertig and solomon 2013 throughout this article and in the software we do not use molar units for concentrations even though they lead to more elegant equations instead we use cm3 stp g for dissolved noble gas concentrations and express gas phase concentrations as partial pressures choosing these units is consistent with the model equations as given by kipfer et al 2002 and as used in the software noble this choice also avoids problems with unit conversion since most laboratories report their noble gas results in cm3 stp g which is an absolute and experimentally easily accessible measure for gas concentrations per water amount similarly partial pressures in the gas phase can directly be related to known atmospheric abundances expressing concentrations in mol l in both phases is advantageous for formal calculations but the conversion from these units to the experimentally accessible forms used here depends on environmental conditions like temperature pressure and salinity the equilibrium components of he ne ar and kr are calculated using the solubilities determined by weiss 1970 1971 weiss and kyser 1978 the equations given by clever 1979 were used for xe the implementation follows the recommendations of kipfer et al 2002 so far these are the only solubility equations implemented the program can however easily be extended to include others as well the following section provides a list of all available excess air or degassing models and their equations they all share three parameters which are needed to calculate the equilibrium component the temperature t measured in c the salinity s in g kg and the pressure p in atm another quantity used in all formulas is z i which stands for the volume or mole fraction of the specific noble gas in dry air unfractionated excess air ua model this traditional and most simple model of pure air addition can be expressed as follows c i t s p a c i e q t s p a z i a denotes the concentration of dissolved excess air in cm3 stp g partial re equilibration pr model this model assuming diffusive loss of excess air has been introduced by stute et al 1995 and can be written as c i t s p a f p r β c i e q t s p a z i exp f p r d i d ne β f pr is the dimensionless parameter for excess air loss due to re equilibration β is the dimensionless exponent in the relationship of the gas transfer velocity to the diffusivity in water the diffusion coefficients d i are calculated following jähne et al 1987 except for d ar which was interpolated from the values of the other noble gases assuming that d is inversely proportional to the square root of the atomic mass a is the same as in the ua model partial degassing pd model this model assuming diffusive loss of all noble gas components can be formulated as follows compare aeschbach hertig et al 2008 c i t s p a f p d β c i e q t s p a z i exp f p d d i d ne β f pd is the dimensionless parameter for diffusive gas loss from the dissolved phase into a noble gas free gas phase β d i and a are the same as in the pr model oxygen depletion od model this model assuming an increase of noble gas partial pressures in soil air in response to oxygen depletion has been introduced by hall et al 2005 and may be written as c i t s p a p o d c i e q t s p p o d a z i in this model p od denotes the pressure increase factor which is dimensionless a is again the same as in the previous models gas diffusion relaxation gr model this extension of the od model assumes partial loss of excess air similar to the pr model but governed by diffusion in the gas phase it has been formulated by sun et al 2008 c i t s p a p o d f g r c i e q t s p p o d a z i exp f g r d i β f gr is the dimensionless parameter for excess air loss due to gas phase diffusion β is the dimensionless exponent in the relationship of the gas transfer velocity to the gas phase diffusion coefficients d i which are calculated according to benítez 1948 closed system equilibration ce model this model assuming equilibration between groundwater and trapped air bubbles has been formulated by aeschbach hertig et al 2000 c i t s p a f c i e q t s p 1 f a z i 1 f a z i c i e q here f denotes the dimensionless fractionation factor by which the size of the gas phase has changed during re equilibration the parameter a has a different meaning than in the other models it describes the initial amount of entrapped air per unit mass of water and is measured in cm3 stp g note that this parameter becomes a dimensionless ratio of two volumes if all concentrations are expressed in molar units as used in the most recent representations of the ce model given by aeschbach hertig et al 2008 and aeschbach hertig and solomon 2013 the two representations can be converted into each other using the following formula a c m 3 s t p g a ρ g cm 3 p e p 0 t 0 t t 0 and p 0 are standard temperature and pressure t and p are temperature and pressure governing atmospheric equilibrium e is the saturation vapor pressure and ρ the density of water 3 the panga application panga stands for program for the analysis of noble gas data and is a new open source software for the evaluation of noble gas data from water samples its main functionality is comparable to the software noble by peeters et al 2003 including ensemble and monte carlo fits it includes however the more recent od and gr excess air models and can calculate monte carlo fits much faster additionally it is possible to perform a graphical evaluation of monte carlo data as needed e g for applying the method proposed by jung et al 2013 to solve problems which occasionally arise when the ce model is applied to groundwater data sets it also provides means for the interactive exploration of the χ 2 space of a fit 3 1 algorithms panga calculates best estimates of the parameters of the above described models using typical methods of inverse modeling such as employed e g by mcgrail 2001 the specific implementation chosen here follows essentially the approach described by aeschbach hertig et al 1999 in this method the quantity χ 2 which is the error weighted square sum of the deviations between measured and modeled noble gas concentrations is minimized 1 χ 2 i c i c i mod 2 σ i 2 the minimization is carried out using a modern implementation of the levenberg marquardt algorithm marquardt 1963 as suggested by moré 1978 the uncertainties of the model parameters and of derived quantities are calculated from the covariance matrix v p of the fit parameters p which is estimated by the fitter according to the following equation bard 1974 2 v p j t v c 1 j 1 v c is the covariance matrix of the measured noble gas concentrations the jacobian matrix j consists of the derivatives of the modeled noble gas concentrations with respect to the fit parameters j i j c i mod p j the parameter uncertainties are determined by taking the square root of the variances which are located on the main diagonal of v p the uncertainties of quantities derived from the fit parameters such as the modeled concentrations of all noble gases including he isotopes denoted by y p are calculated according to bard 1974 3 v y j v p j t here the jacobian matrix j contains the derivatives of the quantities y with respect to the fit parameters j i j y i p p j note that the covariance matrices as given in 2 and 3 are only approximations most of the time they yield good values for the uncertainties but if the model equations are too nonlinear in the vicinity of the parameter estimates the values may deviate considerably from the true ones better and more robust error estimates can be obtained by using monte carlo simulations as explained below the idea of monte carlo simulations is to directly explore the range and distribution of parameter values that corresponds to the range of noble gas concentrations in accordance with the measurement results and their uncertainty the calculation of monte carlo data is done as follows compare e g checchi et al 2007 at first a set of monte carlo realizations of the sample under consideration is created by randomly altering the sample concentrations according to normal distributions with standard deviations equal to the uncertainties of the input data then a fit is performed for each of these monte carlo realizations afterwards the results of these fits are used for statistical analysis of the resulting parameter distributions this procedure represents an explicit assessment of the parameter uncertainties resulting from equation 2 3 2 features the main user interface of panga shows a list of samples with their measured noble gas concentrations on the right and a panel to select fit options on the left fig 1 in its most basic mode of operation panga finds a set of model parameters in such a way that the modeled noble gas concentrations reproduce the measured sample concentrations as closely as possible i e with minimal χ 2 this is done for all samples independently of each other the user selects the noble gas concentrations to be included in the fitting procedure the model to be applied and the parameters to be fitted model parameters which are not supposed to be varied can be set to fixed values either globally or independently for each sample in the same manner initial parameter values for the fitting algorithm may be specified either globally or individually individually set fixed or initial parameter values are listed in the lower right panel of the interface fig 1 users can easily copy and paste values from data files into this table if desired the range of possible values of the parameters may be constrained checkbox constrained fit to only include physically meaningful values this option currently constrains the parameters a and f in the ce model to positive values and the parameter p o d in the od model to the range of 1 p o d 1 264 which can be produced by removal of oxygen from air freundt et al 2013 as another option the software can also perform a so called ensemble fit i e it does not treat the samples individually but combines all of them in a single fit in this mode each fit parameter can either be varied independently for each sample or it can be fitted to the ensemble of samples i e it has the same value for every sample in both modes the user can choose to additionally carry out a specific number of monte carlo calculations which can be used to determine more accurate estimates for the fit parameters and their errors and to check whether or not the fit shows any kind of abnormal behavior all the values calculated for normal fits are also saved for every monte carlo realization and one dimensional histograms of their frequencies of occurrence can be created for all of them additionally any two parameters may be combined into a two dimensional histogram both kinds of histograms can be used to select a subset of the monte carlo realizations which can then be applied as a mask to the monte carlo results of the sample s other parameters this can be used for example to restrict the analysis of the sample to one of multiple clusters as suggested by jung et al 2013 for certain problematic cases the program performs a basic statistical analysis consisting of mean values standard deviations and correlation coefficients for the remaining monte carlo realizations and summarizes these statistical characteristics in a table a few 100 000 up to several millions of monte carlo realizations can be fitted in a few minutes depending on computation power and on the properties of the samples such a large number of simulations is not needed in order to obtain stable estimates of the mean and standard deviation of the parameter distributions some ten thousand simulations suffice for that purpose however considering that calculation time will usually not be a strong limitation for the monte carlo analysis a larger number of simulations is recommended in order to improve the visual representation of the data in two dimensional histograms the output of the fitting procedure consists of the following quantities for every single fit degrees of freedom of the fit the final χ 2 value together with the probability of obtaining this or a higher value based on the χ 2 distribution under the assumption that the model is correct and deviations are due to statistical measurement errors the best estimates of the fit parameters together with their uncertainties derived from the covariance matrix as explained above the off diagonal elements of the covariance matrix in the form of correlation coefficients which need to be used for error propagation if further quantities are derived from the fit parameters the residuals of the fit the equilibrium components of the modeled concentrations the final modeled concentrations because χ 2 may have multiple local minima it can sometimes be helpful to analyze the structure of the χ 2 surface in the parameter space in order to find irregularities or to verify that the obtained minimum also is the global one jung et al 2013 for these cases panga provides the χ 2 explorer mode which shows contour plots of two dimensional cross sections of the χ 2 surface the user can freely choose the parameters on the plot s axes and can either fix the remaining parameters to a certain value or leave them to be fitted this feature allows for an interactive exploration of the χ 2 surface as the results are calculated on the fly while the user pans or zooms the plot 3 3 implementation panga is written completely in c and runs on microsoft windows linux and os x for its calculations and user interface it makes use of several libraries the eigen template library http eigen tuxfamily org is used for all algebraic computations eigen comes with a port of minpack which is a reliable and robust implementation of the levenberg marquardt algorithm the user interface was implemented using the qt framework http qt project org plots are created with the qwt library http qwt sourceforge net the boost libraries http www boost org are used for many other aspects like multi threading serialization of data and compression 4 comparison with existing results in order to verify the accuracy of the fits produced by panga we compared its output to the results generated by noble using two published noble gas data sets the first data set consists of 44 samples from the ledo paniselian aquifer in belgium and was taken from the study of blaser et al 2010 the second data set was taken from the study of castro et al 2007 and consists of 41 samples from the carrizo aquifer in texas usa for the comparison both noble and panga were run without constraints on the range of parameter values 4 1 ledo paniselian aquifer belgium all 44 samples from 39 wells reported in the study of blaser et al 2010 were fitted with the ce model 34 had excess air whereas degassing occurred in 10 of them we label the samples with be followed by the well number or name as used in the original publication the fitting was carried out with the following initial values for the parameters see fig 1 t was set to 2 c a to 0 01 cm3 stp g f has to be chosen according to the type of sample to prevent the fitter from falling into a wrong local minimum for the samples with excess air and with degassing the initial values 0 5 and 3 were used respectively the fixed parameter p was set to 0 9976 atm corresponding to an altitude of 20 m asl salinity s was set individually for each sample the complete input values for the belgium samples are made available on the download page of the software as a test data set except for five special cases which will be discussed later on most results were very close to those provided by noble i e the best parameter estimates showed typical deviations of about 0 002 0 017 c for t 0 03 10 3 to 1 50 10 3 cm3 stp g for a and 0 02 10 3 to 2 50 10 3 for f which correspond to relative deviations of around 0 1 0 3 for t 0 04 1 7 for a and 0 004 0 3 for f the samples be 7 and be 9 however showed significant deviations of the f parameter of almost 0 1 for sample be 29 the deviation for f was 0 02 these three samples have in common that their f and a parameters are ill determined their errors are between 5 and almost 150 times as high as their values if their deviations are compared to the estimated parameter errors they are however still smaller than 1 3 for all parameters also the error estimates of the two programs agree well the deviations of estimated parameter errors are typically of the same order of magnitude as the deviations of the parameter values if the parameters are however not very well determined i e their uncertainties become about the same size as their values the deviations of the estimated errors can increase drastically a detailed comparison of the results led to the conclusion that discrepancies in the modeled xe equilibrium concentrations of about 0 01 caused these deviations further investigation showed that these discrepancies were due to noble using an additional factor for the conversion of molar units derived from clever 1979 instead of the simple factor x xe as recommended by kipfer et al 2002 noble uses x xe 1 x xe with x xe being the mole fraction solubility of xe which is small compared to 1 for a better comparison the additional 1 1 x xe factor was temporarily incorporated into panga and the analysis was repeated now all the samples except for the five aforementioned special cases were in very good agreement with noble i e the deviations of the parameters were in the majority of cases smaller than 5 10 4 c 10 5 cm3 stp g and 10 5 for the parameters t a and f respectively even for samples with comparatively high parameter uncertainties the deviations of the parameter estimates did not exceed 0 005 c 4 10 4 cm3 stp g and 7 10 4 in the three above mentioned cases with ill determined parameters the absolute deviations of the parameter errors were still higher but they were always lower than 0 7 of the estimated errors the five aforementioned special samples be 7new be 9new be 12 be 30 be 32 exhibit unusual and unexpected values of the f parameter i e highly negative ranging from 1200 to 2200 or in one case be 12 highly positive about 4200 the corresponding uncertainties of f are very large and range from 10 7 to 10 8 the a values are quite low and range from 3 10 7 to 10 6 cm3 stp g with uncertainties of 6 10 3 up to 3 10 2 cm3 stp g obviously a and f are ill defined in these cases nevertheless the fit results obtained with panga and noble of these samples show good agreement for the t parameter with differences of 0 13 at most the temperature uncertainties however deviate from each other by up to 47 for the ill defined parameters the discrepancies are large the results of noble for the absolute values of f are much lower roughly by an order of magnitude whereas the values for a are higher again by about one order of magnitude within the extremely large errors however these results still agree remarkably despite the seemingly erratic results for a and f the product a f is the same for both fitters with differences of less than 0 8 these results can be understood if one considers the following limiting case of the ce model 4 c i c i e q 1 f a z i 1 f a z i c i e q f 1 c i e q f a z i 1 f a z i c i e q this equation shows that in the case of f 1 the modeled concentrations only depend on the product a f therefore one can expect that a and f individually can only be determined with a high degree of uncertainty and that small numeric differences in the calculations can have a large impact on the estimates of these two parameters the t parameter however is not influenced by this effect and for this reason the temperature estimates don t show any significant deviations it is interesting to note that if additionally f a z i c i e q 1 which generally is the case for f a 0 01 c m 3 s t p g equation 4 can further be simplified to 5 c i c i e q f a z i this is identical to the ua model with a being replaced by f a for the samples just discussed f a lies between 4 10 4 cm3 stp g and 0 002 cm3 stp g i e the latter approximation is not fully justified in all cases nevertheless equation 5 shows that large negative values of f in combination with small positive values of a can lead to a reasonable description of the observed noble gas concentrations which approximates the simple case of unfractionated excess air the one case with a large positive f and small positive a actually corresponds to a degassed sample that approximates a ua model with a hypothetical negative excess air parameter 4 2 carrizo aquifer in texas usa the study of the carrizo aquifer by castro et al 2007 contains 49 samples from 20 different wells labeled by tx followed by a well number and a number for the replicate analyses in the original paper eight of the 49 samples were removed from the analysis because of their χ ua 2 values being too high the remaining 41 samples were evaluated as an ensemble using the od model with one p od parameter common for all wells by varying this parameter and fitting ua models with correspondingly adjusted pressure to each sample individually the authors found a minimal total χ od 2 value of 113 0 at p od 1 14 table 1 we repeated this analysis using panga to fit the same selected 41 samples with the od model in an ensemble fit i e the parameters a and t were fitted independently for each sample but only a single p od value was determined for the whole data set the value obtained in this way is p od 1 114 0 020 at χ od 2 73 9 table 1 this value differs from the original result of castro et al 2007 although the two values are probably compatible within error no formal uncertainty estimate for p od was given in the original paper a comparison of ua model results of panga with those reported by castro et al 2007 also showed deviations we determined a total χ ua 2 of 109 2 whereas castro et al 2007 found χ ua 2 201 6 furthermore our temperature estimates where systematically lower roughly by 0 1 c these deviations are likely due to the use of different evaluation methods incorporating for example different solubility equations on the one hand and the use of slightly different input data on the other hand differences in the input may include the assumed fixed values for pressure and salinity which we were unable to reproduce from the information given in the article and the uncertainties of the measured noble gas concentrations which were only specified as general percentages in the paper but might have been available in more detail for the original analysis in order to test the reliability of our fitter we compared with noble also for the carizzo data set again using the aforementioned additional factor 1 1 x xe so as to match noble s solubility equilibrium concentrations in the best possible way the ua model results are in very good agreement with each other none of the samples including the eight samples with high χ ua 2 showed relative deviations in the model parameters or their uncertainties which exceeded 10 ppm because noble cannot natively fit the od model we determined the best estimate of p od in the same way as castro et al 2007 i e by running multiple ua fits with pressure p p p od for different values of p od p is the pressure estimated from the recharge altitude in this way we found a minimal χ od 2 of 73 8 at p od 1 112 table 1 this compares well with the value obtained by the ensemble fit of panga which with the xe modifications in place slightly increased to p od 1 115 0 020 at χ od 2 73 9 table 1 the deviation between the two values corresponds to 13 of the estimated error 5 evaluation process the evaluation of groundwater noble gas data by inverse modeling tools requires a certain expertise to judge the obtained results and choose a reasonable evaluation strategy as sun et al 2010 have demonstrated in detail there are systematic offsets between the noble gas temperatures estimated with different excess air models but the temperature variation within a data set is robust with regard to the model choice it is therefore strongly recommended to evaluate coherent data sets from which past changes in recharge temperatures are to be estimated with only one model for all samples as our new software panga adds additional ways to treat individual samples even more care should be taken to obtain a consistent and appropriate evaluation of any given data set we therefore recommend to start with simple traditional evaluation methods and only use the advanced models and features such as monte carlo simulations and ensemble fitting when clearly indicated in the following we describe our recommended step by step evaluation approach for groundwater samples with a particular emphasis on the application of the ce model which probably has been the most frequently used model in the past the ce model is also most flexible and its parameters have a clear physical interpretation aeschbach hertig et al 2008 but sometimes it produces unexpected or even unphysical fit results a fact that needs special attention in the evaluation process before starting the data evaluation care should be taken to check the quality of the input data and in particular the reliability of the estimated analytical errors of the noble gas concentrations the assigned uncertainties obviously have a strong influence on the fit target χ 2 equation 1 and they also define the range of variation of monte carlo realizations of a sample the analytical uncertainties should represent the reproducibility of repeated measurements and also account for possible systematic offsets compared to the reference concentrations for air equilibrated water if they cannot be avoided or corrected for very important is furthermore a reliable estimation of model parameters that are prescribed such as atmospheric pressure from recharge altitude or salinity it also makes sense to consider any additional information on the samples such as the hydrogeological setting and sampling conditions although in our experience it is often difficult to relate fitting outcomes to such information the different special cases occurring in ce model fits as discussed in the preceding section are usually not related to other properties of the samples except maybe for the degassing case which is often related to bubble formation during sampling due to high gas contents of the groundwater 5 1 step 1 ua model fits in the beginning fitting the ua model which is the most simple and also numerically most stable excess air model can give helpful information about the samples for example very small or very large values of the parameter a may indicate unusual excess air conditions possibly due to equilibration or air contamination during sampling whereas negative a values indicate that the samples were probably affected by degassing the χ 2 values obtained by the ua model and the corresponding probabilities from the χ 2 distribution also provide some orientation based on our experience with many data sets we recommend to reject fits with probabilities below 0 01 aeschbach hertig et al 2000 2002 2008 aeschbach hertig and solomon 2013 therefore low χ 2 values with corresponding probabilities larger than 1 may in principle supersede any further evaluations but this is rarely the case for entire data sets samples with higher χ 2 values p 0 01 call for more complex models accounting for excess air fractionation with an additional parameter which in general can improve on the ua model fits very high χ 2 values from the ua model however may indicate some problem with the data that no model will be able to explain monte carlo analysis is unlikely to yield any improvements for the ua model where parameter correlations are low 5 2 step 2 ce model fits as a next step we suggest fitting the ce model which can describe both excess air and degassing and contains other models as limiting cases aeschbach hertig et al 2008 information from fitting the ua model in the first step should be taken into account for the selection of initial fit parameters initial values of a 0 01 cm 3 stp g and f 0 5 usually worked very well for samples with excess air whereas f 3 seems to be a good choice for degassed samples in order to find the minimum with f 1 corresponding to degassing the temperature initial values should be close to the expected paleotemperatures or somewhat lower to prevent the fitter from falling into a second local χ 2 minimum at higher temperatures and unrealistically high a values which is present in most samples cf jung et al 2013 for many samples standard ce model fitting works very well and provides sound parameter and uncertainty estimates as well as reasonable χ 2 values as a guidance we consider the following ranges of the ce model parameters a and f and the temperature uncertainty δ t as reasonable a 0 05 cm 3 stp g 0 1 f 1 and δ t 2 c the χ 2 fit probability should be greater than 1 in order to work out whether or not monte carlo fits would significantly improve the parameter estimates and their uncertainty in case of such samples with well behaved ce model fits we analyzed around 250 samples from five different data sets a comparison of fit results with parameter estimates in the standard ce model range with their respective monte carlo counterparts showed good agreement of the temperature parameter in almost all cases in two cases the mean monte carlo temperature was about 0 7 0 8 c lower than the fit temperature in five cases it was about 0 4 c lower for three other cases the temperature estimates agreed well but the monte carlo temperature error was about 0 4 c lower so for most well behaved samples monte carlo analysis does not improve parameter estimation and may be omitted in such cases the analysis may be terminated at this point unless there are reasons such as suspected temperature biases or oxygen depletion that motivate the exploration of further models 5 3 step 3 monte carlos fits we recommend carrying out monte carlo fits if the ce model fit results do not meet the criteria given above among these samples two special patterns are quite common first comparatively high estimates of a in conjunction with large temperature uncertainties and second highly negative f values typically between 1000 and 5000 combined with low a values less than 10 6 cm3 stp g or in rare cases with interchanged signs i e high positive f together with small negative a the first case has been discussed in detail by jung et al 2013 the second case was discussed above for some samples from belgium in both cases monte carlo analyses and their appropriate evaluation as demonstrated later in this section can help to obtain better results it is recommended to begin with unconstrained fits to avoid any problems in the boundary area which could be caused by the constraints only if the results indicate it the fits should be repeated in constrained mode 5 4 examples of common monte carlo cases in the following sections we will list the most common patterns appearing in the evaluation process and give recommendations on how to handle them to illustrate the different cases we will use the following set of samples their noble gas concentrations and the values used for p and s are summarized in table 2 be 8 and tx48 2 were taken from the belgium and texas data sets which we already used in the previous section ind 25 3 ind 19 2 are two samples from a sedimentary aquifer system in the north cambay basin in northwest india wieser 2011 the last sample cn 16 2 is from a well in the beijing area and part of a recent study in the north china plain schneider 2014 table 3 shows the results of an initial ua model fit the a estimates indicate that all samples but cn 16 2 have excess air cn 16 2 however has negative a meaning that degassing has probably occurred for the samples ind 25 3 ind 19 2 and cn 16 2 the fit probabilities are very low the χ 2 probability of be 8 is almost acceptable tx48 2 is one of the rare cases where the probability is very good so that further fitting might not be necessary nevertheless all samples will be further analyzed here to show their different behaviors with respect to ce model monte carlo fitting as they have excess air all samples but the last one should be fitted with an initial f smaller than 1 the remaining sample cn 16 2 should have an initial f greater than 1 the ce model fit can now be used to distinguish between the different cases 5 4 1 normal case be 8 fig 2 shows the standard case all parameter estimates and uncertainties look realistic and are in the above mentioned range monte carlo analysis is usually not necessary in this case and will probably only show small deviations cf table 4 5 4 2 high a in combination with large temperature uncertainties the two samples from india ind 25 3 and ind 19 2 both show high values of a in combination with large temperature uncertainties this case is usually accompanied by too high temperature estimates which may however still be in the realistic range this case has been analyzed in detail by jung et al 2013 it appears to be triggered by a combination of high ar and low xe cocentrations which however is not easily recognizable from the concentration data alone samples of this type need to be analyzed using monte carlo fits their histograms normally show a split up of the a and t parameters into two peaks or in the two dimensional a t histogram the formation of two more or less separate clusters one of which is at unrealistic parameter values in particular high values of a and contains the original fit result the other cluster shows more realistic a values and as described in more detail by jung et al 2013 corresponds to the true parameter values therefore the evaluation needs to be restricted to the realistic cluster whereas the monte carlo realizations in the unrealistic cluster are dropped in fig 3 a this process is illustrated for the sample ind 25 3 however in rare cases like ind 19 2 which is shown in fig 3b this approach is not feasible because all or almost all monte carlo results lie in the cluster at unrealistic values for these samples it is impossible to retrieve the correct results as the realistic cluster is not visible 5 4 3 ua limit case the ce model includes two different ua model limit cases the first one is f 0 in which case the ce model equation simplifies to c e q i a z i which is the ua model the other less obvious case is the one mentioned above in equations 4 and 5 if unconstrained fits are used the latter case is the one appearing most of the time however because the approximation in equation 5 is not correct in all cases the results correspond to some sort of mathematical mixing of the ua and ce models which does not need to have any physical meaning it is therefore advisable to use constrained fits for these types of samples and thereby prevent the fit from falling into the cases with highly positive or highly negative f fig 4 shows constrained monte carlo results of the sample tx48 2 which is an example for the ua limit case monte carlo statistics typically reproduce the fit temperature very well but lead to lower temperature uncertainties and should therefore be carried out whenever this limit case occurs even though this is a ua model limit case the ua model should not be used for the statistics as not all ce model monte carlo results fall into this limit case and therefore contribute to a higher temperature error which is still lower than the ce model fit error 5 4 4 combination of high a and ua limit cases the ua limit case sometimes appears together with the case of high a i e the direct fit of the sample shows either behavior but the monte carlo histograms show clusters at high a as well as in the ua limit case for this type of sample the analysis should be restricted to the normal and the ua limit case clusters dropping the cluster at high a values 5 4 5 degassed samples degassed samples exhibit f values larger than 1 and the corresponding ce model solutions are obtained by using an initial value of f 1 besides this they show the same special monte carlo patterns as samples with excess air in addition to the normal case as shown in fig 5 they can have high a values be in the ua limit case here normally with large positive f or show a combination of both this means that the same evaluation methods as for samples with excess air may be tried we did however not analyze these cases and therefore recommend the more conservative approach of using all monte carlo realizations for the statistical analysis in addition in the case of degassed samples the pd model based on diffusion controlled degassing should also be tried this model may be appropriate for fast gas stripping due to depressurizing during sampling whereas gas loss by partitioning into a subsurface gas phase is likely controlled by solubility equilibrium visser et al 2007 as described by the ce model in our experience and in agreement with the findings of aeschbach hertig et al 2008 the ce model tends to provide the better fits to degassed samples in some cases nevertheless the pd model can describe the measured noble gas concentrations more accurately 6 conclusions and future plans with panga we provide a new easy to use tool for the analysis of noble gas samples we have demonstrated the correct functioning of our implementation by showing that the results obtained by panga are nearly identical to those derived with the widely used program noble except for some deviations in special unphysical cases of the ce model these deviations in the ua limit case are of little practical relevance as we anyway recommend to use constrained fitting in these cases essentially reducing the model to the simpler and more robust ua model an advantage of panga is that it allows for a fast evaluation workflow for well behaved samples as well as problematic cases which need special treatment like statistical analysis over a subset of the monte carlo results with its support for ensemble fits and more recent excess air models like the od model it can also be used for most special cases that might occur in order to guide the user through the many features and possible evaluation steps provided by panga we have given a step by step recommendation for the evaluation process for various cases that according to our experience typically occur when the ce model is used to interpret noble gas concentrations from groundwater samples another strength of panga is its flexible structure and availability as open source software enabling us and other users to add extensions if desired for example new excess air models might be incorporated in the future as well as other equations for the calculation of noble gas equilibrium concentrations users may want to create a plugin interface for data input which will allow for a better integration with existing infrastructure like sample databases for some applications it might make sense to include dissolved nitrogen as a nearly conservative gas in addition to the noble gases a more sophisticated extension could be the incorporation of expert knowledge to better constrain some parameters e g a soft upper boundary for the parameter a in the ce model instead of the purely physical constraints available now acknowledgments we thank all members of our research group who contributed to the testing and optimization of panga during its development we also thank bryant jurgens and two anonymous reviewers for constructive comments this work was supported by the german research foundation dfg through grant ae 93 7 
26411,coupled 1d 2d modelling is a widely used approach to predict water movement in complicated surface and subsurface drainage systems in urban or peri urban areas in this study a hybrid parallel code h12 is developed for 1d 2d coupled urban flood modelling hybrid 1d 2d or h12 enables street resolving hyper resolution simulation over a large area by combining open multi processing openmp and message passing interface mpi parallelization variable grid sizing is adopted for detailed geometric representation of urban surfaces as well as efficient computation to assess the capability of h12 simulation experiments were carried for the johnson creek catchment 40 km2 in arlington texas the lidar derived digital elevation model dem and detailed land cover map at 1 m resolution are used to represent the terrain and urban features in flood modelling hybrid parallelization achieves up to a 79 fold reduction in simulation time compared to the serial run and is more efficient than either openmp or mpi alone especially in hyper resolution simulations keywords coupled 1d 2d modelling hybrid parallelization hyper resolution lidar 1 introduction floods are one of the most destructive natural hazards michel kerjan and kunreuther 2011 with urbanization and climate change the frequency and magnitude of floods are changing in many parts of the world hirabayashi et al 2013 karen r ryberg et al 2014 mallakpour and villarini 2015 in particular urban flooding is becoming increasingly costly and difficult to manage due to a greater concentration of population and assets in urban centers urban flooding is a phenomenon affected by various factors such as rainfall topography and hydrologic and hydraulic processes on land surface and in subsurface dual drainage i e the concurrent water flow not only in sewer pipes but also on land surface at the same location is a unique and important aspect of urban hydrology and has been studied by many researchers bazin et al 2014 djordjević et al 1999 2014 fraga et al 2015 leandro et al 2009 teng et al 2017 among the numerous modelling approaches coupled 1d 2d modelling is one of the most widely used to simulate dual drainage through one dimensional sewer flow two dimensional surface flow and the exchange between the 1d and 2d domains in which the above flows occur adeogun et al 2015 chen et al 2015 djordjević et al 2014 fraga et al 2015 leandro et al 2009 noh et al 2016a since surface flow is strongly influenced by topography availability of accurate digital elevation models dem at appropriate resolutions is central to accurately simulating flooding abily et al 2016 bierkens et al 2015 bates et al 2003 leitão et al 2009 mark et al 2004 also pointed out that the use of high resolution geomorphological data is critical to accounting for the effects of man made structures such as buildings roads and curbs on the urban surface recently remote sensing technology has revolutionized high resolution water modelling fewtrell et al 2008 liu et al 2015 in particular light detection and ranging lidar systems have been widely used for flood inundation modelling which can capture floodplain topography at 1 m or finer resolution with high horizontal and vertical accuracy gallegos et al 2009 marks and bates 2000 mason et al 2007 2014 neal et al 2009a ozdemir et al 2013 schubert et al 2008 however as discussed by dottori et al 2013 overconfidence can be placed in such high resolution data when in reality accuracy is not necessarily improved by higher resolution due to uncertainty sources in addition since the laser beam measures the distance to the first object on its path the dems from lidar may be different from the actual surface boundary for water movement for instance if vegetation canopy and man made structures such as bridge or elevated roads are not properly treated in the raw lidar data the resulting flood simulation is not likely to be realistic although there remain various issues on the tradeoff among model resolution model complexity data and parameter uncertainty and computational feasibility abily et al 2016 bates et al 2003 fewtrell et al 2008 mignot et al 2006 neal et al 2012 hyper or high resolution data sets are radically changing computer models and their use increasing their complexity and range of applications beven 2007 dottori et al 2013 although there is no formal distinction between hyper and high resolutions we use the term hyper resolution for grid sizes of 1 m or finer and high resolution for grid sizes which are coarser than 1 m in addition within the manuscript we use the terms fine and coarse to differentiate grid sizes in relative terms within high and hyper resolutions hyper or high resolution 2d inundation modelling is however computationally expensive for many real world applications neal et al 2010 for example 1 m resolution modelling of a 40 km2 area 40 106 grids requires more computational grids than 500 m resolution modelling of the contiguous united states conus 32 106 grids in addition to the dimensionality of the computational grid a small time step of 0 05 s or less is required for convergence e g courant friedrichs lewy cfl condition for dynamic or diffusive wave modelling which renders hyper resolution modelling more challenging for practical implementation as such some form of parallelization is necessary for hyper resolution flood modelling parallel computing methods include message passing interface mpi in a distributed memory system gropp et al 1996 open multi processing openmp in a shared memory chapman et al 2007 and co processor parallelism which utilizes graphics processing units gpu ament et al 2011 or many core processors such as intel xeon phi intel 2017 there have been significant advances in parallel 2d inundation modelling gallegos et al 2009 neal et al 2010 2009b sampson et al 2015 sanders et al 2010 shen et al 2015 sanders et al 2010 developed a parallel godunov type shallow water code parbrezo using mpi and evaluated it for urban dam break flood and storm surge inundation cases neal et al 2010 compared three parallelization methods based on openmp mpi and accelerator cards and found that mpi is slightly more efficient than openmp and shows high scalability with a large number of cores leandro et al 2014 developed a 2d parallel diffusive wave model for flood plain inundation with the matlab parallel computing toolbox and fortran openmp sampson et al 2015 used openmp parallelized lisflood fp and reach decomposition to simulate inundation over the entire globe at 90 m resolution each parallelization method has positives and negatives though straightforward to implement openmp is limited in scalability by the size of the shared memory system mpi is scalable for high dimensional problems but at the expense of reduced computational efficiency due to increasing communications among the cpus despite large potential co processor parallelism is highly device dependent and improves performance only for specific types of calculations in recent years a number of studies have demonstrated that hybrid parallelization that combines the above may be an effective solution for drastically reducing computational time for high dimensional problems in various research areas borges et al 2014 gorobets et al 2013 ibanez et al 2016 mininni et al 2011 satarić et al 2016 wan and lin 2013 however to the best of the authors knowledge there has been no investigation into hybrid parallelization for 1d 2d flood modelling presumably due to the convoluted nature of dual drainage which requires extensive coupling in this study a hybrid parallel code hybrid 1d 2d or h12 for short is developed for coupled 1d 2d urban flood modelling to enable street resolving hyper resolution simulation for a large area by combining openmp and mpi the code developed uses variable grid sizing for detailed geometric representation of urban land surfaces and computational efficiency in order to assess the capability of hyper resolution 1d 2d modelling in a realistic setting h12 is applied for the johnson creek catchment 40 km2 in arlington texas we used lidar derived dem with 1 m resolution for topography the lidar derived dem is post processed to remove vegetation canopy and to capture flow paths below urban objects such as elevated roads overpasses and bridges the value of hyper resolution 1d 2d modelling is discussed in comparison with low resolution modelling results in addition performance of hybrid parallelization over openmp and mpi is analyzed the rest of this paper is organized as follows section 2 describes the study area and data used section 3 describes the coupled 1d 2d urban flood model and development of the parallel code h12 including the variable sized scheme profiling of the serial code and structure of the hybrid parallel code section 4 presents simulation results at two different resolutions compares run times and efficiency with different parallelization methods and discusses challenges of hyper resolution urban flood modelling and parallel computing section 5 summarizes the main findings 2 study area and data used in this section we describe hydrologic and hydraulic characteristics of the study domain and the procedure of lidar data processing for hyper resolution 1d 2d urban flood modelling 2 1 study area the study domain is the johnson creek catchment 40 2 km2 located in arlington texas fig 1 the johnson creek length 21 km originates near interstate 20 in eastern tarrant county tx and flows northeasterly to drain into the trinity river in grand prairie in dallas county the major land cover types include residential area road parking lot and lawn there are 7017 sewer pipes connected with 3615 inlets 3259 storm fittings 549 manholes 286 culverts and 415 outfalls in the model domain the total lengths of sewer pipes and culverts are 190 7 and 5 8 km respectively according to the previous study asquith and roussel 2004 500 year return period rainfall for 1 and 2 h durations is about 117 and 71 mm h respectively which was used for code evaluation and benchmarking 2 2 hyper resolution lidar derived land surface hyper resolution dem was obtained from lidar point data the lidar data used in this study was created in april 2009 with the leica als 50 system the system was flown at an average flying altitude of 1400 m above ground with an average point spacing of 0 57 m the resulting lidar data have a vertical accuracy of 0 07 m and a horizontal accuracy of 1 m at 95 confidence level given the density of the raw data 1 m was considered the finest resolution retrievable for the study area initially a 1 m bare earth dem was created from the ground return lidar points using a natural neighbor interpolation method sibson 1981 with arcgis 10 4 www esri com then solid urban features such as houses and buildings which obstruct flow of storm water were added to the bare earth dem for numerical flood modelling we obtained the building footprint gis layer from the city of arlington and identified positions of buildings and houses in the dem by increasing the elevation of the pixels inside of the building foot print polygons due to the fact that certain urban features such as elevated roads and bridges could not be removed correctly by using the provided lidar classification codes we adjusted the dem manually to remove the erroneous elevated roads and bridges over water bodies which unrealistically obstruct the flow of water in the ground surface model abdullah et al 2012 meesuk et al 2015 the 1 m resolution dem generated through the above processes is shown in fig 2 mixed effects of natural and artificial urban components are found in the hyper resolution dem within the 40 2 km2 area the maximum difference of elevation within the dem is about 70 m fig 3 enlarges the dem and land cover maps in the red circle in fig 2 it is also found that the hyper resolution dem represents variations of elevation at road and building scale in this work infiltration and evapotranspiration were not modeled for runoff simulation we used the city provided gis data to create the land cover type map the six land cover types used in modelling include road building parking lot vegetation bare ground and water body different runoff coefficients were assigned to different land cover types as shown in table 1 calibration of runoff coefficients was beyond the scope of this study 3 numerical model this section describes the coupled urban flood model used and development of the hybrid parallel code the coupled 1d 2d urban flood model consists of a horizontal 2d inundation flow model kawaike et al 2011 a 1d preissmann slot model chaudhry 1979 and an interaction model between the 1d and 2d domains this model has been used in a number of previous studies to improve understanding of urban flooding lee et al 2012 lee 2013 noh et al 2016a although the overall model structure is similar to other hydrodynamical models one of the unique features of the proposed model is to incorporate physically based representation of the exchange of flow not only between the land surface and sewer pipes but also between the sewer pipes and manholes noh et al 2016a in order to develop an efficient parallel code the serial code is profiled in a hyper resolution case in terms of computation run times then the hybrid parallel code which combines mpi and openmp is described 3 1 coupled 1d 2d urban flood model two dimensional 2d shallow water equations which consist of continuity eq 1 and momentum eqs 2 and 3 equations are used as governing equations for the 2d surface flow model as follows 1 h t m x n y α r e q e x s 2 m t u m x v m y g h h x g n 2 m u 2 v 2 h 4 3 3 n t u n x v n y g h h y g n 2 n u 2 v 2 h 4 3 where h is the water depth h is the water level u and v are the x and y directional velocities respectively m uh and n vh are the x and y directional flow fluxes respectively q ex s q ex a sur is the interacting discharge between the surface and the manhole m s the interacting discharge q ex is calculated from the water depths in surface and manhole grids using weir and orifice equations noh et al 2016a a sur is the area of surface grid g is the gravitational acceleration r e is the rainfall intensity α is the runoff coefficient for each land use and n is manning s roughness coefficient in this study we used n 0 012 the preissmann slot model chaudhry 1979 is used to simulate sewer pipe flow this model assumes a hypothetically narrow slot on the pipe crown the governing equations are as follows 4 a t q x q m a n 5 q t u q x g a h p x g n 2 q q r 4 3 a where a is the wetted area of the cross section q is the flow discharge u is the velocity r is the hydraulic radius and n is the roughness coefficient h p is the piezometric head h p z p h p z p is the bottom elevation of the sewer pipe q man q man dx sw is the flow discharge from the manhole to the sewer pipe per unit pipe length q man is interactive flow discharge from the manhole to the sewer pipe and h p is the water depth the water depth in the sewer pipe can be calculated from wetted area of pipe based on hydraulic characteristic curve lee et al 2016 it is also worth noting that in h12 a sewer pipe longer than a threshold grid length 10 m in this simulation is divided into multiple subsections for numerical simulation e g a 150 m long sewer pipe is divided into 15 subsections each with a length of 10 m the lateral inflow outflow between manhole and sewer pipes q man m2 s in eq 4 occurs only at the subsections which are connected to a manhole and for other subsections in pipes q man is zero the water depth in a manhole h man is updated based on the discharge interactions between the surface and the manhole q ex m as well as between the manhole and the sewer pipe q man the governing equation is as follows 6 h m a n t q e x m i 1 n q m a n i a m a n where q ex m q ex a man is the interacting discharge between surface and manhole m s positive value means water flow from surface to manhole and negative value means water flow from manhole to surface a man is the bottom area of manhole h man is the water depth in the manhole n is the number of sewer pipes which are connected to the manhole the 2d surface flow model and 1d sewer network model are coupled by the interaction model to exchange flow under various hydraulic conditions modified orifice and weir equations are used to calculate flow between surface and manhole as well as between manhole and sewer pipe in this study flow exchange between the 2d surface and 1d sewer domains is assumed to occur at all inlet or manhole grids while inlet is conceptualized as a type of manhole the details of the interaction model can be found in noh et al 2016a for mesh generation we used variable grid sizing adopted and extended from the non uniform but structured quadrilateral grid scheme liang 2011 this technique is known to reduce computation time while maintaining accuracy where the topography is relatively steep and the flow pattern is complicated kesserwani and liang 2012 an example mesh from variable grid sizing is shown in fig 4 the figure assumes that a 4 way intersection is located in the middle of the study domain and that more detailed simulation is required along the road with variable grid sizing the grids along or in the vicinity of the road is selectively defined at hyper resolution while coarser resolution grids are used for other types of land cover for numerical computation on coarse grids that abut fine grids the values on the find grids are aggregated onto the coarse grids by averaging them which results in a virtual parent grid of the fine grids for fine grids that abut coarse grids the coarse grid is divided into multiple virtual child grids that share the same value with the coarse grid although the original scheme allows multiple grid sizes that may vary in time according to computational conditions only two grid sizes were used in this work for two different land uses road fine grid other land covers coarse grid without temporal variation a major benefit of the variable grid sizing scheme is reduced computation cost as shown in table 2 the number of the surface grids with the scheme using 1 and 8 m resolutions is 13 998 516 and 284 768 respectively which means 35 and 45 reduction compared to that of uniformly sized grids the reduced dimension contributes to a reduction not only in run time but also in memory fig 5 shows the flowchart of coupled 1d 2d modelling the module rdat reads input data for topography land use sewer pipes and manholes a time loop is then iterated at every time step dt until the simulation is complete modules that deal with gridded rainfall and artificial pumping at multiple locations are available but not explicitly described in the flowchart for simplicity at each time loop the momentum and continuity equations for the 2d and 1d domains are solved alternately and then the exchange of flow between two domains is calculated by the module connect mh in the right part of fig 5 the run time in sequential processing is profiled to develop a hybrid parallelization strategy the relative run time i e the run time of each module normalized by the total run time was estimated in a serial 1 m resolution run the relative run time can vary according to different spatiotemporal forcings and configurations of the 1d 2d modelling domain in fig 5 the run time for input and output i o operations was not included in order to examine the time requirement for each process related module only it was found that the solver for the momentum equations for the 2d domain flux module accounted for most run time 47 4 followed by the module that calculates values of each flux and water depth variables in eqs 1 3 for the next time step mdvel 30 9 and the module that solves the continuity equations for the 2d domain suisin 12 4 not surprisingly the momentum and continuity equation solvers for 1d sewer flow flux sw suisin sw accounted for less than 0 1 of the run time due to the differences in the computational cost between surface and sewer domains multiple approaches are required to parallelize the code and integrate two domains which will be discussed in the next section 3 2 hybrid parallel code hybrid 1d 2d h12 the hybrid parallel code h12 was developed for the coupled 1d 2d urban flood model described above by combining the strengths of two parallelization methods of openmp and mpi openmp is an application programming interface api that supports multi platform shared memory multiprocessing since openmp is implemented through compiler directives that instruct segments of code to be run as parallel threads without major changes in serial code it is one of the least labor intensive parallelism methods neal et al 2010 despite the computational overhead to create and merge multiple threads in the beginning and end of each openmp statement communication among threads is not necessary since system memory is shared the scalability of openmp however is explicitly limited by the number of cores in the shared system for instance in this study the maximum number of cores available for openmp was sixteen because each node includes two 8 core processors mpi is a standardized and portable message passing specification to run a parallel program across distributed memory systems and therefore is scalable for large scale parallel applications if there are appropriate computing resources available such as supercomputers the hybrid approach developed in this work maximizes computational efficiency through leveraging shared memory based parallelism openmp in each mpi task in the distributed memory system fig 6 illustrates the workflow of h12 a hyper resolution surface domain is decomposed into smaller multiple domains each of which is solved by an mpi task the domain was partitioned unidirectionally to simplify the parallelization procedure for balanced and efficient parallel computation the same number of grids is assigned to each mpi task by dividing the total grids by the number of mpi tasks the surface grids in the 2 d domain are re ordered in a 1 d computation space according to the geometry of the model domain e g from the upper left corner to the lower right corner and the domain decomposition in the surface grids by mpi is also implemented in a 1 d space therefore the irregular boundary of the catchment in the 2 d domain does not affect the efficiency of parallel computation by mpi for each decomposed 2d domain openmp directives were implemented for additional parallelization using multiple openmp threads in a shared memory system i e a node in a pc or supercomputer for forward calculation of shallow water equations in a decomposed domain updated information i e water depth of connected grids which are being calculated at other mpi tasks is necessary in other words each decomposed domain has their own new boundary conditions as whole domain was divided into several segments ghost and effective grids overlap each other between two adjacent decomposed domains non blocking communication was used to exchange state variables between ghost and effective grids for structured uniform grid a complete solution can be obtained by exchanging three columns in the new virtual boundaries between the two mpi tasks for unstructured variable grid however it is necessary to exchange additional grids during the mpi communications in this study 3 of the grids are set to be exchanged based on sensitivity and integrity analyses however the number of exchanging grids may be further reduced through investigating the exact range of grids affected by virtual boundaries and also vary with respect to catchment geometry it is also worth noting that the gains from variable grid sizing far outweigh the additional cost of increasing the number of grids for communication with virtual boundaries in terms of runtime in addition since computational cost of the sewer domain is far less than that of the surface domain see fig 5 openmp parallelization was implemented for the sewer domain without domain decomposition one potential bottleneck in parallelization in coupled 1d 2d modelling is that the state variables for surface and sewer domains need synchronization at every time step to reduce the computational overhead for coupling the two domains communication among mpi tasks was minimized through synchronizing the states at the inlet manhole grids only which account for less than 0 001 of the total surface grids at 1 m resolution fig 7 shows a simplified example of hybrid parallelization applied to the momentum equation solver for the 2d domain flux module the red lines are associated with part of the mpi parallelization for each time step for a decomposed domain and communication between the ghost and effective grids the blue lines are associated with part of the openmp directives for a loop for a decomposed domain except for the lines for parallelization the other part of the code is the same as the serial code it was verified that the simulation results by h12 are entirely consistent with those by the serial code in all simulation cases 4 results and discussion 4 1 analysis of hyper resolution 1d 2d modelling in order to assess the realism of hyper resolution 1d 2d modelling simulations at two different resolutions were made with grid sizes of 1 and 8 m except for the grid size for the surface domain the same conditions were applied for all other components such as the interaction model and the 1d sewer model with variable grid sizing the resolutions of the road grids were 1 and 8 m while those of all other land cover types were twice as large at 2 and 16 m respectively the total number of grids in the surface domain at varying resolutions is summarized in table 2 each case was simulated for 2 h with uniform rainfall of 71 mm h which is equivalent to 500 year return period precipitation for 2 h duration asquith and roussel 2004 fig 8 shows the spatial distribution of water depth at the end of the 2 h simulation overall the simulated water depth at the two resolutions shows a similar pattern at the catchment scale at smaller scales however considerable differences exists compared to the 1 m resolution result fig 8 a the simulated water depth at 8 m resolution shows more dispersion and numerous artificial puddles small shallow puddles may temporally form on the surface formation of deep puddles which are frequently found in the 8 m resolution result fig 8 b appears to be affected by the coarse resolution because the 8 m resolution grid is obtained from averaging the 1 m resolution grid 64 8 rows 8 columns the terrain depicted at the coarse resolution could be prone to distortion especially when the width of flow pass e g road street and stream is smaller than the grid resolution differences in simulation results between the two resolutions are more clearly observed in fig 9 which shows enlarged views for the circled areas in fig 8 for larger contrast results valid at different simulation times were selected for comparison in fig 9 fig 9 a and b show the water depth at the southern part of the catchment at 90 min into simulation in the 1 m resolution result fig 9 a deep water shown in dark blue is consistently found along streets and streams in the 8 m resolution map fig 9 b it is found that the flow along small streams and streets is disconnected and the storm water is locally trapped in multiple locations comparison of water depth in the main stem of johnson creek is shown in fig 9 c and d the simulated water depth in the 1 m resolution is deeper than that in the 8 m resolution which renders the higher resolution simulation to convey more storm water along the creek accordingly the inundation extent in the 1 m resolution is narrower than that in the 8 m resolution along the creek whereas ground validation is necessary to verify the above it is readily seen from the visual examination that hyper resolution modelling is able to depict storm water flow at small scales more realistically and that coarse resolution modelling is likely to lose details of the terrain such as channel width and depth and ground elevation and hence distort flow dynamics and patterns particularly in urban areas the difference between 1 m and 8 m resolutions does not however directly indicate improvement in accuracy but illustrates potential benefits from high resolution urban inundation modelling as teng et al 2017 noted there is no such thing as a perfect model and the selection of model complexity including resolution must be balanced against various factors such as computation demand and the requirements of the end user a more rigorous analysis on the relation between model accuracy and grid resolution is left as a future endeavor we also note here that the study area of the johnson creek catchment is currently being developed into a hydroinformatics testbed noh et al 2016b seo et al 2015 to support validation of h12 and other research activities the study area is under the casawx network of x band radars from which real time products of high resolution quantitative precipitation estimates are also available chandrasekar et al 2012 4 2 comparison of run time this subsection assesses the computational performance of the hybrid parallel code h12 model grids in the 1d and 2d domains are identical to those in subsection 4 1 except that run time was evaluated for 1 h simulation forced by the uniform rainfall of 117 mm h or 500 year return period precipitation for 1 h duration all cases were run on the stampede supercomputer at the texas advanced computing center tacc https www tacc utexas edu the majority of the 6400 nodes in stampede include two intel xeon e5 sandy bridge processors in each node and the nodes are interconnected by mellanox fdr infiniband technology these compute nodes are configured with 32 gb of memory fig 10 compares run times for 1 h simulation at 1 m resolution using the serial code and the two parallelization methods of openmp and hybrid the run time of 48 8 h of the serial code is shown in the red bar openmp parallelization is shown in blue bars with run times ranging from 24 9 to 7 3 h as the number of openmp threads increases from 2 to 16 the gain from openmp parallelization became smaller as the number of threads increased specifically when the number of openmp threads doubled from 8 to 16 the run time was reduced only by 18 from 9 0 to 7 3 h presumably due to the configuration of the computing nodes as explained below since an intel xeon e5 processor includes eight cpu cores additional overhead may be required to implement openmp with 16 threads in two processors in a single node in hybrid parallelization shown in green bars run times were assessed with varying mpi tasks and a fixed number of openmp threads 8 threads as the number of mpi tasks increased from 8 to 16 32 and 48 run times of the hybrid cases were reduced to 1 3 0 79 0 67 and 0 61 h respectively which demonstrates the efficacy of hybrid parallelization in hyper resolution 1d 2d urban flood modelling fig 11 compares the impact of varying the number of openmp threads on run time in hybrid parallelization when the number of mpi tasks was fixed at 8 as expected a reduction in run time was observed when the number of openmp threads increased from 2 to 8 counterintuitively however the run time increased by 26 as the number of openmp threads in the hybrid parallelization increased from 8 to 16 as with the reduced performance seen in openmp parallelization in fig 10 it appears that the hybrid parallelization remains efficient when the number of openmp threads does not exceed the total number of cpu cores in a processor a similar run time pattern was also observed when the impact of the number of openmp threads was assessed with a large number of mpi tasks not shown performance of the parallel code was assessed through two metrics speedup eq 7 and efficiency eq 8 speedup indicates increased computational speed achieved by parallelization over the serial run while efficiency refers to the effectiveness of computational resources for speedup a perfect efficiency equals 1 efficiency is typically less than 1 due to the overhead associated with parallel computing such as communication and synchronization 7 s p e e d u p s e r i a l r u n t i m e p a r a l l e l r u n t i m e 8 e f f i c i e n c y s p e e d u p n u m b e r o f c p u c o r e s fig 12 a shows the speedup result by the three different parallelization methods of openmp blue line mpi grey line and hybrid green line note that mpi parallelization was implemented by assigning one openmp thread per mpi task i e omp num threads 1 in the hybrid parallel code the maximum speedup obtained by hybrid parallelization green line was 79 0 using 48 mpi tasks and 8 openmp threads 384 cores in total with hybrid parallelization green line speedup also increased from 37 2 to 61 2 as the number of total cores doubled from 64 to 128 from 8 to 16 mpi tasks with 8 openmp threads the rate of speedup in hybrid parallelization green line showed a declining pattern as the number of total cores doubled from 192 to 384 the speedup increased 18 from 67 2 to 79 0 speedup of mpi parallelization grey line showed a maximum of 29 0 using 64 cores and a decrease in spite of increased cores from 128 to 256 the decrease in speedup in mpi parallelization with a larger number of cores may be understood in terms of computational granularity of the tasks i e the amount of computational work assigned to each task the larger the number of cpu cores is the greater the potential for speedup is due to small granularity but at the expense of greater overhead for synchronization and communication among the mpi tasks with the number of mpi tasks over 64 the amount of communication per mpi task e g ghost grids in the surface domain does not change but the total communication increases linearly which negates the gain from computational granularity in hybrid parallelization on the other hand speedup is improved by balancing mpi and openmp parallelization even with a large number of cores from the perspective of the size of each mpi domain granularity of hybrid parallelization is coarser than that of mpi parallelization the overhead for openmp parallelization however does not increase linearly with increasing mpi tasks because in hybrid parallelization communication by openmp occurs only within each mpi task speedup by openmp parallelization ranged from 1 9 to 6 8 as the number of openmp threads increased from 2 to 16 in fig 12 b the parallelization methods were assessed in terms of efficiency overall the most efficient method varied according to the number of cores used when the number of cores was less than 4 efficiency of openmp parallelization blue line was higher than 0 9 between 8 and 16 mpi parallelization grey line outperformed openmp parallelization when the number of cores was larger than 64 hybrid parallelization green line outperformed mpi parallelization grey line speedup by openmp parallelization blue line increased but attenuated as the number of threads increased efficiency of mpi parallelization grey line deteriorated sharply as the number of cores exceeded 100 efficiency of mpi parallelization with 128 and 256 tasks was 0 16 and 0 07 respectively in hybrid parallelization green line on the other hand efficiency still remained higher than 0 45 until the number of total cores reached 128 in figs 10 12 the performance of parallelization methods was analyzed at 1 m resolution a question may arise whether hybrid parallelization is computationally efficient only in hyper resolution modelling fig 13 compares performance of hybrid parallelization in coarser but still high resolution cases where 4 and 8 m resolution simulations were made using different combinations of mpi tasks and openmp threads except for the grid size the simulation conditions were identical to those used in the 1 m resolution cases of 1 h simulation forced by rainfall amount of 117 mm even though the same number of cpu cores was used hybrid parallelization showed a reduction in run time over mpi parallelization in both 4 and 8 m resolution cases in addition among hybrid runs the run time was reduced as more openmp threads were assigned although the difference in run time between mpi and the best hybrid n8n8 runs decreased for coarser resolution 8 m the normalized reduction ratio of run time between the mpi and hybrid parallelization was similar as 51 and 57 for 4 and 8 m resolutions respectively the above suggests that the hybrid parallelization developed in this work is applicable to real world implementation of h12 at high to hyper resolutions 4 3 discussion in the era of hyper resolution data parallel computing plays central roles in the development of environmental modelling approaches including urban flood models however due to the rapid development of high performance computing hpc technologies significant changes have been made in recent years including heterogeneous architectures increasing core counts greater vector widths and sophisticated memory hierarchies which may challenge the development of parallel code mcintoshi smith 2016 especially the use of hundreds of cores running at lower frequency namely many core is an emerging trend in hpc for instance a considerable number of leadership class supercomputers rely on many core processors e g intel xeon phi or co processors such as gpu www top500 org it is worth noting that hybrid parallelization is flexible to speed up a wide range of high dimensional hydrologic and hydraulic modelling in various hpc environments without a significant change of the existing code the major challenge is to find optimal combinations of mpi tasks and openmp threads in different many core systems h12 which is based on hybrid parallelization is designed to be implemented in both multi core and many core computing systems we are currently pursuing improving h12 for heterogeneous parallel computing environments and the results will be reported in the near future it is worth noting that additional performance improvements could be achieved by vectorization and in line functions the key idea of vectorization is to increase the number of computations by increasing the number of operations per cycle i e applying the same operation to multiple items in parallel recently some hardware supports registration of a 512 bit width vector e g avx 512 which means an 8 fold reduction of the run time in double precision computation if the code is perfectly vectorized tacc 2017 the use of compiler flags is one of the widely selected options to vectorize the code automatically however complicated loops and frequent usage of functions and subroutines in the code are major obstacles to automatic vectorization the use of in lined functions and manual simplification of the code may further improve performance by vectorization one of the key challenges for developing hyper resolution modelling is collecting in situ reference data to validate the accuracy of the model traditional observations of street flooding from the existing flash flood survey systems are far too sparse to resolve the spatiotemporal variations that hyper resolution models such as h12 can represent toward that end non traditional observational approaches such as crowdsourcing habibi et al 2017 and unmanned aerial vehicle sensing perks et al 2016 as well as development of intensely instrumented testbeds are necessary 5 conclusions in this study a hybrid parallelization code h12 was developed for 1d 2d coupled urban flood modelling the code combines the open multi processing openmp and message passing interface mpi parallelization methods to enable street resolving hyper resolution simulation of movement of water over a large urban area h12 uses variable grid sizing for detailed geometric representation of urban land surfaces and computational efficiency to assess h12 the code developed was applied for the johnson creek catchment 40 km2 in arlington texas model topography is based on 1 m lidar data the major findings of this study are as follows hyper resolution modelling depicts storm water flow in urban areas realistically whereas coarser resolution modelling lead to locally isolated and distorted water depth distribution due to lack of topographic details hybrid parallelization for h12 achieves a 79 fold reduction in computing time compared to the serial run of the same model at 1 m resolution for a 40 km2 catchment variable grid sizing reduces the number of the surface grids by 35 45 compared to that by uniform grid sizing which greatly reduces computational cost while maintaining accuracy speedup by hybrid parallelization consistently increases up to 384 cores in usage whereas that by mpi parallelization decreases as the number of cores exceeds 128 despite the linearly increase in speedup with a smaller number of cores openmp parallelization is not effective in hyper resolution modelling due to the limited number of cores in the shared memory system improved performance of hybrid parallelization over mpi only parallelization also holds at coarser resolutions of 4 and 8 m grids acknowledgments this material is based upon work supported by the national science foundation under grant no cybersees 1442735 and by noaa under grant no na17oar4590194 dong jun seo university of texas at arlington pi s lee acknowledges the support from the apec climate center the authors acknowledge the texas advanced computing center tacc at the university of texas at austin for providing hpc resources that have contributed to the research results reported within this paper url http www tacc utexas edu the authors are grateful for constructive comments by two anonymous reviewers and dr haksu lee of noaa usa appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 02 008 
26411,coupled 1d 2d modelling is a widely used approach to predict water movement in complicated surface and subsurface drainage systems in urban or peri urban areas in this study a hybrid parallel code h12 is developed for 1d 2d coupled urban flood modelling hybrid 1d 2d or h12 enables street resolving hyper resolution simulation over a large area by combining open multi processing openmp and message passing interface mpi parallelization variable grid sizing is adopted for detailed geometric representation of urban surfaces as well as efficient computation to assess the capability of h12 simulation experiments were carried for the johnson creek catchment 40 km2 in arlington texas the lidar derived digital elevation model dem and detailed land cover map at 1 m resolution are used to represent the terrain and urban features in flood modelling hybrid parallelization achieves up to a 79 fold reduction in simulation time compared to the serial run and is more efficient than either openmp or mpi alone especially in hyper resolution simulations keywords coupled 1d 2d modelling hybrid parallelization hyper resolution lidar 1 introduction floods are one of the most destructive natural hazards michel kerjan and kunreuther 2011 with urbanization and climate change the frequency and magnitude of floods are changing in many parts of the world hirabayashi et al 2013 karen r ryberg et al 2014 mallakpour and villarini 2015 in particular urban flooding is becoming increasingly costly and difficult to manage due to a greater concentration of population and assets in urban centers urban flooding is a phenomenon affected by various factors such as rainfall topography and hydrologic and hydraulic processes on land surface and in subsurface dual drainage i e the concurrent water flow not only in sewer pipes but also on land surface at the same location is a unique and important aspect of urban hydrology and has been studied by many researchers bazin et al 2014 djordjević et al 1999 2014 fraga et al 2015 leandro et al 2009 teng et al 2017 among the numerous modelling approaches coupled 1d 2d modelling is one of the most widely used to simulate dual drainage through one dimensional sewer flow two dimensional surface flow and the exchange between the 1d and 2d domains in which the above flows occur adeogun et al 2015 chen et al 2015 djordjević et al 2014 fraga et al 2015 leandro et al 2009 noh et al 2016a since surface flow is strongly influenced by topography availability of accurate digital elevation models dem at appropriate resolutions is central to accurately simulating flooding abily et al 2016 bierkens et al 2015 bates et al 2003 leitão et al 2009 mark et al 2004 also pointed out that the use of high resolution geomorphological data is critical to accounting for the effects of man made structures such as buildings roads and curbs on the urban surface recently remote sensing technology has revolutionized high resolution water modelling fewtrell et al 2008 liu et al 2015 in particular light detection and ranging lidar systems have been widely used for flood inundation modelling which can capture floodplain topography at 1 m or finer resolution with high horizontal and vertical accuracy gallegos et al 2009 marks and bates 2000 mason et al 2007 2014 neal et al 2009a ozdemir et al 2013 schubert et al 2008 however as discussed by dottori et al 2013 overconfidence can be placed in such high resolution data when in reality accuracy is not necessarily improved by higher resolution due to uncertainty sources in addition since the laser beam measures the distance to the first object on its path the dems from lidar may be different from the actual surface boundary for water movement for instance if vegetation canopy and man made structures such as bridge or elevated roads are not properly treated in the raw lidar data the resulting flood simulation is not likely to be realistic although there remain various issues on the tradeoff among model resolution model complexity data and parameter uncertainty and computational feasibility abily et al 2016 bates et al 2003 fewtrell et al 2008 mignot et al 2006 neal et al 2012 hyper or high resolution data sets are radically changing computer models and their use increasing their complexity and range of applications beven 2007 dottori et al 2013 although there is no formal distinction between hyper and high resolutions we use the term hyper resolution for grid sizes of 1 m or finer and high resolution for grid sizes which are coarser than 1 m in addition within the manuscript we use the terms fine and coarse to differentiate grid sizes in relative terms within high and hyper resolutions hyper or high resolution 2d inundation modelling is however computationally expensive for many real world applications neal et al 2010 for example 1 m resolution modelling of a 40 km2 area 40 106 grids requires more computational grids than 500 m resolution modelling of the contiguous united states conus 32 106 grids in addition to the dimensionality of the computational grid a small time step of 0 05 s or less is required for convergence e g courant friedrichs lewy cfl condition for dynamic or diffusive wave modelling which renders hyper resolution modelling more challenging for practical implementation as such some form of parallelization is necessary for hyper resolution flood modelling parallel computing methods include message passing interface mpi in a distributed memory system gropp et al 1996 open multi processing openmp in a shared memory chapman et al 2007 and co processor parallelism which utilizes graphics processing units gpu ament et al 2011 or many core processors such as intel xeon phi intel 2017 there have been significant advances in parallel 2d inundation modelling gallegos et al 2009 neal et al 2010 2009b sampson et al 2015 sanders et al 2010 shen et al 2015 sanders et al 2010 developed a parallel godunov type shallow water code parbrezo using mpi and evaluated it for urban dam break flood and storm surge inundation cases neal et al 2010 compared three parallelization methods based on openmp mpi and accelerator cards and found that mpi is slightly more efficient than openmp and shows high scalability with a large number of cores leandro et al 2014 developed a 2d parallel diffusive wave model for flood plain inundation with the matlab parallel computing toolbox and fortran openmp sampson et al 2015 used openmp parallelized lisflood fp and reach decomposition to simulate inundation over the entire globe at 90 m resolution each parallelization method has positives and negatives though straightforward to implement openmp is limited in scalability by the size of the shared memory system mpi is scalable for high dimensional problems but at the expense of reduced computational efficiency due to increasing communications among the cpus despite large potential co processor parallelism is highly device dependent and improves performance only for specific types of calculations in recent years a number of studies have demonstrated that hybrid parallelization that combines the above may be an effective solution for drastically reducing computational time for high dimensional problems in various research areas borges et al 2014 gorobets et al 2013 ibanez et al 2016 mininni et al 2011 satarić et al 2016 wan and lin 2013 however to the best of the authors knowledge there has been no investigation into hybrid parallelization for 1d 2d flood modelling presumably due to the convoluted nature of dual drainage which requires extensive coupling in this study a hybrid parallel code hybrid 1d 2d or h12 for short is developed for coupled 1d 2d urban flood modelling to enable street resolving hyper resolution simulation for a large area by combining openmp and mpi the code developed uses variable grid sizing for detailed geometric representation of urban land surfaces and computational efficiency in order to assess the capability of hyper resolution 1d 2d modelling in a realistic setting h12 is applied for the johnson creek catchment 40 km2 in arlington texas we used lidar derived dem with 1 m resolution for topography the lidar derived dem is post processed to remove vegetation canopy and to capture flow paths below urban objects such as elevated roads overpasses and bridges the value of hyper resolution 1d 2d modelling is discussed in comparison with low resolution modelling results in addition performance of hybrid parallelization over openmp and mpi is analyzed the rest of this paper is organized as follows section 2 describes the study area and data used section 3 describes the coupled 1d 2d urban flood model and development of the parallel code h12 including the variable sized scheme profiling of the serial code and structure of the hybrid parallel code section 4 presents simulation results at two different resolutions compares run times and efficiency with different parallelization methods and discusses challenges of hyper resolution urban flood modelling and parallel computing section 5 summarizes the main findings 2 study area and data used in this section we describe hydrologic and hydraulic characteristics of the study domain and the procedure of lidar data processing for hyper resolution 1d 2d urban flood modelling 2 1 study area the study domain is the johnson creek catchment 40 2 km2 located in arlington texas fig 1 the johnson creek length 21 km originates near interstate 20 in eastern tarrant county tx and flows northeasterly to drain into the trinity river in grand prairie in dallas county the major land cover types include residential area road parking lot and lawn there are 7017 sewer pipes connected with 3615 inlets 3259 storm fittings 549 manholes 286 culverts and 415 outfalls in the model domain the total lengths of sewer pipes and culverts are 190 7 and 5 8 km respectively according to the previous study asquith and roussel 2004 500 year return period rainfall for 1 and 2 h durations is about 117 and 71 mm h respectively which was used for code evaluation and benchmarking 2 2 hyper resolution lidar derived land surface hyper resolution dem was obtained from lidar point data the lidar data used in this study was created in april 2009 with the leica als 50 system the system was flown at an average flying altitude of 1400 m above ground with an average point spacing of 0 57 m the resulting lidar data have a vertical accuracy of 0 07 m and a horizontal accuracy of 1 m at 95 confidence level given the density of the raw data 1 m was considered the finest resolution retrievable for the study area initially a 1 m bare earth dem was created from the ground return lidar points using a natural neighbor interpolation method sibson 1981 with arcgis 10 4 www esri com then solid urban features such as houses and buildings which obstruct flow of storm water were added to the bare earth dem for numerical flood modelling we obtained the building footprint gis layer from the city of arlington and identified positions of buildings and houses in the dem by increasing the elevation of the pixels inside of the building foot print polygons due to the fact that certain urban features such as elevated roads and bridges could not be removed correctly by using the provided lidar classification codes we adjusted the dem manually to remove the erroneous elevated roads and bridges over water bodies which unrealistically obstruct the flow of water in the ground surface model abdullah et al 2012 meesuk et al 2015 the 1 m resolution dem generated through the above processes is shown in fig 2 mixed effects of natural and artificial urban components are found in the hyper resolution dem within the 40 2 km2 area the maximum difference of elevation within the dem is about 70 m fig 3 enlarges the dem and land cover maps in the red circle in fig 2 it is also found that the hyper resolution dem represents variations of elevation at road and building scale in this work infiltration and evapotranspiration were not modeled for runoff simulation we used the city provided gis data to create the land cover type map the six land cover types used in modelling include road building parking lot vegetation bare ground and water body different runoff coefficients were assigned to different land cover types as shown in table 1 calibration of runoff coefficients was beyond the scope of this study 3 numerical model this section describes the coupled urban flood model used and development of the hybrid parallel code the coupled 1d 2d urban flood model consists of a horizontal 2d inundation flow model kawaike et al 2011 a 1d preissmann slot model chaudhry 1979 and an interaction model between the 1d and 2d domains this model has been used in a number of previous studies to improve understanding of urban flooding lee et al 2012 lee 2013 noh et al 2016a although the overall model structure is similar to other hydrodynamical models one of the unique features of the proposed model is to incorporate physically based representation of the exchange of flow not only between the land surface and sewer pipes but also between the sewer pipes and manholes noh et al 2016a in order to develop an efficient parallel code the serial code is profiled in a hyper resolution case in terms of computation run times then the hybrid parallel code which combines mpi and openmp is described 3 1 coupled 1d 2d urban flood model two dimensional 2d shallow water equations which consist of continuity eq 1 and momentum eqs 2 and 3 equations are used as governing equations for the 2d surface flow model as follows 1 h t m x n y α r e q e x s 2 m t u m x v m y g h h x g n 2 m u 2 v 2 h 4 3 3 n t u n x v n y g h h y g n 2 n u 2 v 2 h 4 3 where h is the water depth h is the water level u and v are the x and y directional velocities respectively m uh and n vh are the x and y directional flow fluxes respectively q ex s q ex a sur is the interacting discharge between the surface and the manhole m s the interacting discharge q ex is calculated from the water depths in surface and manhole grids using weir and orifice equations noh et al 2016a a sur is the area of surface grid g is the gravitational acceleration r e is the rainfall intensity α is the runoff coefficient for each land use and n is manning s roughness coefficient in this study we used n 0 012 the preissmann slot model chaudhry 1979 is used to simulate sewer pipe flow this model assumes a hypothetically narrow slot on the pipe crown the governing equations are as follows 4 a t q x q m a n 5 q t u q x g a h p x g n 2 q q r 4 3 a where a is the wetted area of the cross section q is the flow discharge u is the velocity r is the hydraulic radius and n is the roughness coefficient h p is the piezometric head h p z p h p z p is the bottom elevation of the sewer pipe q man q man dx sw is the flow discharge from the manhole to the sewer pipe per unit pipe length q man is interactive flow discharge from the manhole to the sewer pipe and h p is the water depth the water depth in the sewer pipe can be calculated from wetted area of pipe based on hydraulic characteristic curve lee et al 2016 it is also worth noting that in h12 a sewer pipe longer than a threshold grid length 10 m in this simulation is divided into multiple subsections for numerical simulation e g a 150 m long sewer pipe is divided into 15 subsections each with a length of 10 m the lateral inflow outflow between manhole and sewer pipes q man m2 s in eq 4 occurs only at the subsections which are connected to a manhole and for other subsections in pipes q man is zero the water depth in a manhole h man is updated based on the discharge interactions between the surface and the manhole q ex m as well as between the manhole and the sewer pipe q man the governing equation is as follows 6 h m a n t q e x m i 1 n q m a n i a m a n where q ex m q ex a man is the interacting discharge between surface and manhole m s positive value means water flow from surface to manhole and negative value means water flow from manhole to surface a man is the bottom area of manhole h man is the water depth in the manhole n is the number of sewer pipes which are connected to the manhole the 2d surface flow model and 1d sewer network model are coupled by the interaction model to exchange flow under various hydraulic conditions modified orifice and weir equations are used to calculate flow between surface and manhole as well as between manhole and sewer pipe in this study flow exchange between the 2d surface and 1d sewer domains is assumed to occur at all inlet or manhole grids while inlet is conceptualized as a type of manhole the details of the interaction model can be found in noh et al 2016a for mesh generation we used variable grid sizing adopted and extended from the non uniform but structured quadrilateral grid scheme liang 2011 this technique is known to reduce computation time while maintaining accuracy where the topography is relatively steep and the flow pattern is complicated kesserwani and liang 2012 an example mesh from variable grid sizing is shown in fig 4 the figure assumes that a 4 way intersection is located in the middle of the study domain and that more detailed simulation is required along the road with variable grid sizing the grids along or in the vicinity of the road is selectively defined at hyper resolution while coarser resolution grids are used for other types of land cover for numerical computation on coarse grids that abut fine grids the values on the find grids are aggregated onto the coarse grids by averaging them which results in a virtual parent grid of the fine grids for fine grids that abut coarse grids the coarse grid is divided into multiple virtual child grids that share the same value with the coarse grid although the original scheme allows multiple grid sizes that may vary in time according to computational conditions only two grid sizes were used in this work for two different land uses road fine grid other land covers coarse grid without temporal variation a major benefit of the variable grid sizing scheme is reduced computation cost as shown in table 2 the number of the surface grids with the scheme using 1 and 8 m resolutions is 13 998 516 and 284 768 respectively which means 35 and 45 reduction compared to that of uniformly sized grids the reduced dimension contributes to a reduction not only in run time but also in memory fig 5 shows the flowchart of coupled 1d 2d modelling the module rdat reads input data for topography land use sewer pipes and manholes a time loop is then iterated at every time step dt until the simulation is complete modules that deal with gridded rainfall and artificial pumping at multiple locations are available but not explicitly described in the flowchart for simplicity at each time loop the momentum and continuity equations for the 2d and 1d domains are solved alternately and then the exchange of flow between two domains is calculated by the module connect mh in the right part of fig 5 the run time in sequential processing is profiled to develop a hybrid parallelization strategy the relative run time i e the run time of each module normalized by the total run time was estimated in a serial 1 m resolution run the relative run time can vary according to different spatiotemporal forcings and configurations of the 1d 2d modelling domain in fig 5 the run time for input and output i o operations was not included in order to examine the time requirement for each process related module only it was found that the solver for the momentum equations for the 2d domain flux module accounted for most run time 47 4 followed by the module that calculates values of each flux and water depth variables in eqs 1 3 for the next time step mdvel 30 9 and the module that solves the continuity equations for the 2d domain suisin 12 4 not surprisingly the momentum and continuity equation solvers for 1d sewer flow flux sw suisin sw accounted for less than 0 1 of the run time due to the differences in the computational cost between surface and sewer domains multiple approaches are required to parallelize the code and integrate two domains which will be discussed in the next section 3 2 hybrid parallel code hybrid 1d 2d h12 the hybrid parallel code h12 was developed for the coupled 1d 2d urban flood model described above by combining the strengths of two parallelization methods of openmp and mpi openmp is an application programming interface api that supports multi platform shared memory multiprocessing since openmp is implemented through compiler directives that instruct segments of code to be run as parallel threads without major changes in serial code it is one of the least labor intensive parallelism methods neal et al 2010 despite the computational overhead to create and merge multiple threads in the beginning and end of each openmp statement communication among threads is not necessary since system memory is shared the scalability of openmp however is explicitly limited by the number of cores in the shared system for instance in this study the maximum number of cores available for openmp was sixteen because each node includes two 8 core processors mpi is a standardized and portable message passing specification to run a parallel program across distributed memory systems and therefore is scalable for large scale parallel applications if there are appropriate computing resources available such as supercomputers the hybrid approach developed in this work maximizes computational efficiency through leveraging shared memory based parallelism openmp in each mpi task in the distributed memory system fig 6 illustrates the workflow of h12 a hyper resolution surface domain is decomposed into smaller multiple domains each of which is solved by an mpi task the domain was partitioned unidirectionally to simplify the parallelization procedure for balanced and efficient parallel computation the same number of grids is assigned to each mpi task by dividing the total grids by the number of mpi tasks the surface grids in the 2 d domain are re ordered in a 1 d computation space according to the geometry of the model domain e g from the upper left corner to the lower right corner and the domain decomposition in the surface grids by mpi is also implemented in a 1 d space therefore the irregular boundary of the catchment in the 2 d domain does not affect the efficiency of parallel computation by mpi for each decomposed 2d domain openmp directives were implemented for additional parallelization using multiple openmp threads in a shared memory system i e a node in a pc or supercomputer for forward calculation of shallow water equations in a decomposed domain updated information i e water depth of connected grids which are being calculated at other mpi tasks is necessary in other words each decomposed domain has their own new boundary conditions as whole domain was divided into several segments ghost and effective grids overlap each other between two adjacent decomposed domains non blocking communication was used to exchange state variables between ghost and effective grids for structured uniform grid a complete solution can be obtained by exchanging three columns in the new virtual boundaries between the two mpi tasks for unstructured variable grid however it is necessary to exchange additional grids during the mpi communications in this study 3 of the grids are set to be exchanged based on sensitivity and integrity analyses however the number of exchanging grids may be further reduced through investigating the exact range of grids affected by virtual boundaries and also vary with respect to catchment geometry it is also worth noting that the gains from variable grid sizing far outweigh the additional cost of increasing the number of grids for communication with virtual boundaries in terms of runtime in addition since computational cost of the sewer domain is far less than that of the surface domain see fig 5 openmp parallelization was implemented for the sewer domain without domain decomposition one potential bottleneck in parallelization in coupled 1d 2d modelling is that the state variables for surface and sewer domains need synchronization at every time step to reduce the computational overhead for coupling the two domains communication among mpi tasks was minimized through synchronizing the states at the inlet manhole grids only which account for less than 0 001 of the total surface grids at 1 m resolution fig 7 shows a simplified example of hybrid parallelization applied to the momentum equation solver for the 2d domain flux module the red lines are associated with part of the mpi parallelization for each time step for a decomposed domain and communication between the ghost and effective grids the blue lines are associated with part of the openmp directives for a loop for a decomposed domain except for the lines for parallelization the other part of the code is the same as the serial code it was verified that the simulation results by h12 are entirely consistent with those by the serial code in all simulation cases 4 results and discussion 4 1 analysis of hyper resolution 1d 2d modelling in order to assess the realism of hyper resolution 1d 2d modelling simulations at two different resolutions were made with grid sizes of 1 and 8 m except for the grid size for the surface domain the same conditions were applied for all other components such as the interaction model and the 1d sewer model with variable grid sizing the resolutions of the road grids were 1 and 8 m while those of all other land cover types were twice as large at 2 and 16 m respectively the total number of grids in the surface domain at varying resolutions is summarized in table 2 each case was simulated for 2 h with uniform rainfall of 71 mm h which is equivalent to 500 year return period precipitation for 2 h duration asquith and roussel 2004 fig 8 shows the spatial distribution of water depth at the end of the 2 h simulation overall the simulated water depth at the two resolutions shows a similar pattern at the catchment scale at smaller scales however considerable differences exists compared to the 1 m resolution result fig 8 a the simulated water depth at 8 m resolution shows more dispersion and numerous artificial puddles small shallow puddles may temporally form on the surface formation of deep puddles which are frequently found in the 8 m resolution result fig 8 b appears to be affected by the coarse resolution because the 8 m resolution grid is obtained from averaging the 1 m resolution grid 64 8 rows 8 columns the terrain depicted at the coarse resolution could be prone to distortion especially when the width of flow pass e g road street and stream is smaller than the grid resolution differences in simulation results between the two resolutions are more clearly observed in fig 9 which shows enlarged views for the circled areas in fig 8 for larger contrast results valid at different simulation times were selected for comparison in fig 9 fig 9 a and b show the water depth at the southern part of the catchment at 90 min into simulation in the 1 m resolution result fig 9 a deep water shown in dark blue is consistently found along streets and streams in the 8 m resolution map fig 9 b it is found that the flow along small streams and streets is disconnected and the storm water is locally trapped in multiple locations comparison of water depth in the main stem of johnson creek is shown in fig 9 c and d the simulated water depth in the 1 m resolution is deeper than that in the 8 m resolution which renders the higher resolution simulation to convey more storm water along the creek accordingly the inundation extent in the 1 m resolution is narrower than that in the 8 m resolution along the creek whereas ground validation is necessary to verify the above it is readily seen from the visual examination that hyper resolution modelling is able to depict storm water flow at small scales more realistically and that coarse resolution modelling is likely to lose details of the terrain such as channel width and depth and ground elevation and hence distort flow dynamics and patterns particularly in urban areas the difference between 1 m and 8 m resolutions does not however directly indicate improvement in accuracy but illustrates potential benefits from high resolution urban inundation modelling as teng et al 2017 noted there is no such thing as a perfect model and the selection of model complexity including resolution must be balanced against various factors such as computation demand and the requirements of the end user a more rigorous analysis on the relation between model accuracy and grid resolution is left as a future endeavor we also note here that the study area of the johnson creek catchment is currently being developed into a hydroinformatics testbed noh et al 2016b seo et al 2015 to support validation of h12 and other research activities the study area is under the casawx network of x band radars from which real time products of high resolution quantitative precipitation estimates are also available chandrasekar et al 2012 4 2 comparison of run time this subsection assesses the computational performance of the hybrid parallel code h12 model grids in the 1d and 2d domains are identical to those in subsection 4 1 except that run time was evaluated for 1 h simulation forced by the uniform rainfall of 117 mm h or 500 year return period precipitation for 1 h duration all cases were run on the stampede supercomputer at the texas advanced computing center tacc https www tacc utexas edu the majority of the 6400 nodes in stampede include two intel xeon e5 sandy bridge processors in each node and the nodes are interconnected by mellanox fdr infiniband technology these compute nodes are configured with 32 gb of memory fig 10 compares run times for 1 h simulation at 1 m resolution using the serial code and the two parallelization methods of openmp and hybrid the run time of 48 8 h of the serial code is shown in the red bar openmp parallelization is shown in blue bars with run times ranging from 24 9 to 7 3 h as the number of openmp threads increases from 2 to 16 the gain from openmp parallelization became smaller as the number of threads increased specifically when the number of openmp threads doubled from 8 to 16 the run time was reduced only by 18 from 9 0 to 7 3 h presumably due to the configuration of the computing nodes as explained below since an intel xeon e5 processor includes eight cpu cores additional overhead may be required to implement openmp with 16 threads in two processors in a single node in hybrid parallelization shown in green bars run times were assessed with varying mpi tasks and a fixed number of openmp threads 8 threads as the number of mpi tasks increased from 8 to 16 32 and 48 run times of the hybrid cases were reduced to 1 3 0 79 0 67 and 0 61 h respectively which demonstrates the efficacy of hybrid parallelization in hyper resolution 1d 2d urban flood modelling fig 11 compares the impact of varying the number of openmp threads on run time in hybrid parallelization when the number of mpi tasks was fixed at 8 as expected a reduction in run time was observed when the number of openmp threads increased from 2 to 8 counterintuitively however the run time increased by 26 as the number of openmp threads in the hybrid parallelization increased from 8 to 16 as with the reduced performance seen in openmp parallelization in fig 10 it appears that the hybrid parallelization remains efficient when the number of openmp threads does not exceed the total number of cpu cores in a processor a similar run time pattern was also observed when the impact of the number of openmp threads was assessed with a large number of mpi tasks not shown performance of the parallel code was assessed through two metrics speedup eq 7 and efficiency eq 8 speedup indicates increased computational speed achieved by parallelization over the serial run while efficiency refers to the effectiveness of computational resources for speedup a perfect efficiency equals 1 efficiency is typically less than 1 due to the overhead associated with parallel computing such as communication and synchronization 7 s p e e d u p s e r i a l r u n t i m e p a r a l l e l r u n t i m e 8 e f f i c i e n c y s p e e d u p n u m b e r o f c p u c o r e s fig 12 a shows the speedup result by the three different parallelization methods of openmp blue line mpi grey line and hybrid green line note that mpi parallelization was implemented by assigning one openmp thread per mpi task i e omp num threads 1 in the hybrid parallel code the maximum speedup obtained by hybrid parallelization green line was 79 0 using 48 mpi tasks and 8 openmp threads 384 cores in total with hybrid parallelization green line speedup also increased from 37 2 to 61 2 as the number of total cores doubled from 64 to 128 from 8 to 16 mpi tasks with 8 openmp threads the rate of speedup in hybrid parallelization green line showed a declining pattern as the number of total cores doubled from 192 to 384 the speedup increased 18 from 67 2 to 79 0 speedup of mpi parallelization grey line showed a maximum of 29 0 using 64 cores and a decrease in spite of increased cores from 128 to 256 the decrease in speedup in mpi parallelization with a larger number of cores may be understood in terms of computational granularity of the tasks i e the amount of computational work assigned to each task the larger the number of cpu cores is the greater the potential for speedup is due to small granularity but at the expense of greater overhead for synchronization and communication among the mpi tasks with the number of mpi tasks over 64 the amount of communication per mpi task e g ghost grids in the surface domain does not change but the total communication increases linearly which negates the gain from computational granularity in hybrid parallelization on the other hand speedup is improved by balancing mpi and openmp parallelization even with a large number of cores from the perspective of the size of each mpi domain granularity of hybrid parallelization is coarser than that of mpi parallelization the overhead for openmp parallelization however does not increase linearly with increasing mpi tasks because in hybrid parallelization communication by openmp occurs only within each mpi task speedup by openmp parallelization ranged from 1 9 to 6 8 as the number of openmp threads increased from 2 to 16 in fig 12 b the parallelization methods were assessed in terms of efficiency overall the most efficient method varied according to the number of cores used when the number of cores was less than 4 efficiency of openmp parallelization blue line was higher than 0 9 between 8 and 16 mpi parallelization grey line outperformed openmp parallelization when the number of cores was larger than 64 hybrid parallelization green line outperformed mpi parallelization grey line speedup by openmp parallelization blue line increased but attenuated as the number of threads increased efficiency of mpi parallelization grey line deteriorated sharply as the number of cores exceeded 100 efficiency of mpi parallelization with 128 and 256 tasks was 0 16 and 0 07 respectively in hybrid parallelization green line on the other hand efficiency still remained higher than 0 45 until the number of total cores reached 128 in figs 10 12 the performance of parallelization methods was analyzed at 1 m resolution a question may arise whether hybrid parallelization is computationally efficient only in hyper resolution modelling fig 13 compares performance of hybrid parallelization in coarser but still high resolution cases where 4 and 8 m resolution simulations were made using different combinations of mpi tasks and openmp threads except for the grid size the simulation conditions were identical to those used in the 1 m resolution cases of 1 h simulation forced by rainfall amount of 117 mm even though the same number of cpu cores was used hybrid parallelization showed a reduction in run time over mpi parallelization in both 4 and 8 m resolution cases in addition among hybrid runs the run time was reduced as more openmp threads were assigned although the difference in run time between mpi and the best hybrid n8n8 runs decreased for coarser resolution 8 m the normalized reduction ratio of run time between the mpi and hybrid parallelization was similar as 51 and 57 for 4 and 8 m resolutions respectively the above suggests that the hybrid parallelization developed in this work is applicable to real world implementation of h12 at high to hyper resolutions 4 3 discussion in the era of hyper resolution data parallel computing plays central roles in the development of environmental modelling approaches including urban flood models however due to the rapid development of high performance computing hpc technologies significant changes have been made in recent years including heterogeneous architectures increasing core counts greater vector widths and sophisticated memory hierarchies which may challenge the development of parallel code mcintoshi smith 2016 especially the use of hundreds of cores running at lower frequency namely many core is an emerging trend in hpc for instance a considerable number of leadership class supercomputers rely on many core processors e g intel xeon phi or co processors such as gpu www top500 org it is worth noting that hybrid parallelization is flexible to speed up a wide range of high dimensional hydrologic and hydraulic modelling in various hpc environments without a significant change of the existing code the major challenge is to find optimal combinations of mpi tasks and openmp threads in different many core systems h12 which is based on hybrid parallelization is designed to be implemented in both multi core and many core computing systems we are currently pursuing improving h12 for heterogeneous parallel computing environments and the results will be reported in the near future it is worth noting that additional performance improvements could be achieved by vectorization and in line functions the key idea of vectorization is to increase the number of computations by increasing the number of operations per cycle i e applying the same operation to multiple items in parallel recently some hardware supports registration of a 512 bit width vector e g avx 512 which means an 8 fold reduction of the run time in double precision computation if the code is perfectly vectorized tacc 2017 the use of compiler flags is one of the widely selected options to vectorize the code automatically however complicated loops and frequent usage of functions and subroutines in the code are major obstacles to automatic vectorization the use of in lined functions and manual simplification of the code may further improve performance by vectorization one of the key challenges for developing hyper resolution modelling is collecting in situ reference data to validate the accuracy of the model traditional observations of street flooding from the existing flash flood survey systems are far too sparse to resolve the spatiotemporal variations that hyper resolution models such as h12 can represent toward that end non traditional observational approaches such as crowdsourcing habibi et al 2017 and unmanned aerial vehicle sensing perks et al 2016 as well as development of intensely instrumented testbeds are necessary 5 conclusions in this study a hybrid parallelization code h12 was developed for 1d 2d coupled urban flood modelling the code combines the open multi processing openmp and message passing interface mpi parallelization methods to enable street resolving hyper resolution simulation of movement of water over a large urban area h12 uses variable grid sizing for detailed geometric representation of urban land surfaces and computational efficiency to assess h12 the code developed was applied for the johnson creek catchment 40 km2 in arlington texas model topography is based on 1 m lidar data the major findings of this study are as follows hyper resolution modelling depicts storm water flow in urban areas realistically whereas coarser resolution modelling lead to locally isolated and distorted water depth distribution due to lack of topographic details hybrid parallelization for h12 achieves a 79 fold reduction in computing time compared to the serial run of the same model at 1 m resolution for a 40 km2 catchment variable grid sizing reduces the number of the surface grids by 35 45 compared to that by uniform grid sizing which greatly reduces computational cost while maintaining accuracy speedup by hybrid parallelization consistently increases up to 384 cores in usage whereas that by mpi parallelization decreases as the number of cores exceeds 128 despite the linearly increase in speedup with a smaller number of cores openmp parallelization is not effective in hyper resolution modelling due to the limited number of cores in the shared memory system improved performance of hybrid parallelization over mpi only parallelization also holds at coarser resolutions of 4 and 8 m grids acknowledgments this material is based upon work supported by the national science foundation under grant no cybersees 1442735 and by noaa under grant no na17oar4590194 dong jun seo university of texas at arlington pi s lee acknowledges the support from the apec climate center the authors acknowledge the texas advanced computing center tacc at the university of texas at austin for providing hpc resources that have contributed to the research results reported within this paper url http www tacc utexas edu the authors are grateful for constructive comments by two anonymous reviewers and dr haksu lee of noaa usa appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 02 008 
26412,there is ample evidence that short term ozone exposure is associated with increased respiratory symptoms many studies however aggregate the population activities or concentration levels of the pollutant across space and or time failing to capture critical variations in the exposure levels we couple spatiotemporal air quality estimates of ozone with a synthetic information model of the houston metropolitan area allowing us to attach exposure levels to individuals based on exact times geo locations and microenvironments of activities several scenarios of the model are run at different levels of resolution when we maintain the spatiotemporal resolution of the data the proportion of the population that experiences sharp increases in short term exposure increases substantially this can be particularly important if experienced by sensitive populations given the increased risk for adverse health effects we find that individuals in the same zip code neighborhood and even household have varying levels of exposure keywords synthetic populations air quality ozone microenvironment personal exposure 1 introduction exposure is the contact an individual has with a pollutant a function of both the concentration of the pollutant in the environment and the time an individual is exposed to that pollutant us epa 2011 localized specific exposures to ozone can dramatically increase health risks for cardiac events and asthma ensor et al 2013 2014 raun et al 2014 davis and ensor 2006 for instance each 20 parts per billion ppb increase in ambient ozone o 3 concentration in the previous one to three hours is associated with a 4 4 increased risk of having an out of hospital cardiac arrest which is particularly significant given that 90 of these cases result in death ensor et al 2013 accurate representations of the magnitude frequency and duration of localized exposure to a pollutant requires that we account for individual activity patterns across both space and time vallero 2014 many studies however average the activities of individuals in time and space into 12 or 24 h summaries klepeis et al 2001 leech et al 2002 matz et al 2014 summarizing the data in this way may miss important variations in ozone exposure across a population in this paper we couple spatiotemporal air quality estimates of ozone o 3 ppb hours across the houston metropolitan area of texas to a data informed synthetic information model the synthetic information model is a simulated population that is representative of the true population of the houston metropolitan area including the composition of households the demographics of individuals and their movement throughout the course of the day adigaa et al 2015 parikh et al 2013 we maintain the spatiotemporal resolution of the data such that individual exposure estimates may vary on a second by second basis as synthetic individuals move across the geography entering and leaving geo located microenvironments each with their own unique ozone concentration estimates it overcomes the limitations of traditional approaches as it is informed by how people move through their activities during the day allowing us to attach specific exposure levels to the synthetic individuals based on the exact geo location of the activity and time of day our research has been motivated by the many studies ensor et al 2013 2014 raun et al 2014 that have shown that the resolution of the data is an important consideration as it can impact important health effects that could be translated into lifesaving behavioral and policy changes exposure modeling has been an active research area from a variety of different research approaches duan 1982 jerrett et al 2005 ryan and lemasters 2007 zou et al 2009 community based studies rosenthal et al 2008 silverman et al 2010 wellenius et al 2012 identify a significant impact on health from air pollution levels but do not directly measure individual exposure early research using microenvironment monitoring models for instance assigned exposure concentrations based on time spent within different microenvironments e g indoor locations outdoors but did not assign these microenvironments to individual level activity patterns fugas 1975 extending this work models incorporated activity diaries allowing for some estimates of the variability and distribution of individual exposure but did not account for temporal variations in concentration levels ott and flachsbart 1982 other models attach air pollution exposure to populations at a group level based on the demographics of a subset of individuals the geographic location of homes and activities or a set of microenvironments while some of these studies model representative individuals kousa et al 2002 burke et al 2001 özkaynak et al 2008 they either stop short in their ability to trace individuals throughout the course of the day or in modeling a representative population of the geographic location in question a subset of exposure models have focused on calculating personal exposure to emissions and other pollutants while traveling accounting for factors such as transportation mode vehicle type and transportation routes hatzopoulou et al 2011 hülsmann et al 2011 while these models seek to accurately reflect exposure during travel they do not account for the individual s full daily course of activities moving away from aggregate level exposure calculations and accounting for an individual s daily activities more recent exposure models have developed synthetic populations to represent each individual in a geographic location for instance the environmental protection agency epa s air pollutants exposure apex model develops a synthetic population that characterizes the study area and utilizes the consolidated human activity database chad a repository of harmonized human activity data to assign activity patterns to synthetic individuals us epa 2012 in this model a person s exposure is obtained by mapping the activities reported in the survey into several microenvironments e g indoors residence each with an estimated exposure rate geography is measured at the level of census tracts a geographic region with a population size between 1 200 and 8 000 people activities are assigned in hourly chuncks and individuals may move from their assigned home tract only for work other synthetic population models are limited in their use for estimating environmental exposure to contaminants that vary over both space and time these methods aggregate activities into percent time e g percent of day and allocate the aggregated time to an activity location lenormand and deffuant 2013 namazi rad et al 2014 wheaton et al 2009 us epa 2014a as an example a study of sydney australia uses traditional synthetic population models with single daily exposure values and couples the percent time spent at various locations to these daily average exposure levels newth 2012 other studies focus on direct measurement of human exposure through personal monitors or home based centers but cost measurement accuracy and logistics limit the use of this approach on a scale large enough to provide continuous community wide understanding of exposure weisel et al 2005 us epa 2015 more recent studies have used personal monitoring systems as a way to estimate regional concentration levels of ozone and other air pollutants xu et al 2017 for instance nikzad et al 2012 used sensors and a mobile phone application to collect data from 16 participants in san diego california over the course of a month regional estimates derived from this data were then compared to levels measured by epa monitors across the county while results are promising for generating air quality estimates these studies do not account for the individual activity sequences of the population our contribution is the development of an in silico analytics platform that uses the synthetic information model to understand the impact of air quality over physical space and time on individual exposure levels we estimate hourly ozone levels for the geo coordinates latitude longitude associated with the activity locations in our synthetic information model and allow synthetic individuals to move across this geography based on their second by second time sequenced activities we combine individual activity patterns with the microenvironment modeling approach assigning each geo located activity location hourly exposure rates over a set of microenvironments few studies consider human activity patterns in detail at the population level due to availability of data to accurately generate representative populations and their movement and the computational burden associated with working with such vasts amounts of data coupling this information to pollutant levels that vary spatiotemporally and by microenvironment adds another level of difficulty we overcome such computational challenges by applying high performance computing and database management techniques and expand on current exposure models by integrating data sources across many publicly and commercially available datasets this allows us to trace ozone exposures of each synthetic individual as they move throughout the course of the day the spatiotemporal resolution of the data handled by this platform provides additional flexibility for comparing results across different sets of assumptions 2 methodology in this section we describe the in silico analytics platform we developed which couples a synthetic information representation of the residents in the houston metropolitan area to spatiotemporal air quality estimates we begin by describing the synthetic information model developed at virginia tech marathe et al 2014 section 2 1 it includes socio demographically relevant activity sequences and the movement of each individual in the population through their sequences second by second during the day this allows aggregation of time intervals to match the environmental quality data e g hourly intervals section 2 2 we then discuss the methodology used to determine individual level exposures to ozone section 2 3 the output of the platform is the exposure profiles for the roughly 4 9 million synthetic individuals in the houston metropolitan area fig 1 illustrates the conceptual model of the in silico analytics platform this platform provides an integrated database that can be used and reused for the analysis of various studies related to the synthetic information model and air quality moreover given the fidelity of the data the platform can process we have the flexibility to explore results under various sets of assumptions 2 1 synthetic information model the first step in creating the in silico analytics platform is to generate the synthetic information model for the houston metropolitan area the synthetic information model is a set of synthetic people each associated with demographic variables located geographically at specific points in time and place such as homes and schools each associated with specific geo locations it is created by integrating a variety of databases from commercial and public sources including statistical surveys administrative data and data on the built environment e g buildings roads and land use through a process that preserves the confidentiality of the individuals in the original data sets yet produces realistic attributes and demographics for synthetic individuals adigaa et al 2015 the synthetic information model approach has been used to study a wide range of phenomena to support policy decision making barrett et al 2006 marathe et al 2014 including transportation epidemics parikh et al 2013 and natural and man made disasters barrett et al 2005 halloran et al 2008 lewis et al 2013 marathe and vullikanti 2013 the steps to the generation of the synthetic information model include 1 population synthesis in which a synthetic representation of each individual and household in a region is created using socioeconomic characteristics from census data 2 activity and microenvironment assignments in which each synthetic individual is assigned a set of activities to perform during the day along with start and end times based on activity or time use survey data and 3 place choice in which an appropriate location is chosen for each activity for every synthetic individual based on several data sources including land use patterns and tax data the main data sources used in the creation of the houston synthetic information model are described in table 1 the american community survey acs provides tables of distributions at the census block group level a geographical region containing 600 to 3000 individuals on demographic characteristics such as age gender household income and household size which are referred to as marginal distributions joint demographic distributions are reconstructed from these marginal distributions using public use microdata sample pums data which provides a 5 representative sample of the acs data pums information are incorporated into the joint distribution through an iterative proportional fitting technique beckman et al 1996 the process ensures that the synthetic population including individual demographic characteristics and household composition match the structure of the true population each synthetic household is then located geographically using land use data and data pertaining to businesses and transportation networks realistic activity patterns and their associated activity locations are then added the activities performed by each synthetic individual on an average day is determined by analyzing the activity patterns in the national household travel survey and linking these patterns to the socio demographic composition of the households e g income and individuals within the households e g age gender this is done using a process known as the fitted values method lum et al 2016 this method preserves within household activity correlations and ensures that activity sequences of synthetic individuals are similar to actual individuals with similar socio demographic characteristics and household membership activities are further characterized by one or more microenvironments a microenvironment is defined as a three dimensional space that is relatively homogeneous with respect to pollutant concentrations for a specified time period u s epa 2007 we model 11 microenvironments 6 indoor microenvironments indoors residence indoors school indoors office indoors shopping indoors bars and restaurants indoors other 3 outdoor microenvironments outdoors near road outdoors public garage or parking outdoors other and 2 in vehicle microenvironments in vehicle cars or trucks in vehicle mass transit we adapted the epa s u s epa 2007 mapping of activity location types e g bedroom park golf course office building in chad to microenvironments moreover to link these activity location types to our synthetic information model we created a crosswalk between activities and locations in the synthetic information model to one or more microenvironments for instance the activity home can be associated with up to two microenvironments indoors residence and outdoors other using data from chad we developed empirical distributions to represent the proportion of time spent within each microenvironment while performing a specified activity we then sampled from these distributions to get time in seconds spent within each microenvironment while performing an activity the houston metropolitan area synthetic information includes any synthetic individual with either a home or activity location within harris county texas this resulted in approximately 4 9 million individuals grouped into 1 8 million households synthetic individuals can perform up to 6 different types of activities including staying home going to work going to school shopping traveling and other e g socializing at a friend s residence going to a restaurant activity types can be performed multiple times by the same individual on the same day activities occur in 1 2 million different activity locations 895 thousand housing locations 166 thousand work locations 37 thousand shopping locations 4 thousand schools and 112 thousand other locations fig 2 shows the household and activity locations across the region as well as an example set of activities for one household 2 2 air quality estimation according to the texas commission on environmental quality tceq the air quality in houston texas is monitored more closely and analyzed with more intensity than perhaps anywhere in the country if not in the world texas commission on environmental quality 2005 ozone is measured hourly at 47 sites us epa 2012 one hour ambient meteorological temperature relative humidity and wind speed data from the monitors are publicly available for this demonstration hourly recordings on august 26 2008 of ambient ozone concentration levels o 3 ppb hours across 39 epa monitors in the houston metropolitan area is used 1 1 we selected august 26 2008 because it represents a warm day in houston with time varying levels of ozone and is within the same time span of the study performed by ensor ensor et al 2013 on august 26 2008 39 of the total 47 monitors recorded ozone readings the first step to coupling the environmental pollutant data with the synthetic information model is to assign hourly ambient ozone concentration levels to each of the 1 2 million activity locations this is done using the inverse weighted distance from each location to the 39 monitors a standard method for assigning ozone concentration hwang and jaakkola 2008 lu and wong 2008 salam et al 2005 given that m j t is the concentration measured by monitor j 1 n at hour h the pollutant concentration c l at each activity location l 1 l at hour h 1 24 is calculated as follows 1 c l h j 1 n 1 d l j 2 m j h j 1 n 1 d l j 2 where d l j is the great circle distance between location l and monitor j nychka et al 2015 the next step is to compute concentration levels by microenvironment for each activity location and for each hour of the day see section 2 1 we apply one of two methods used for estimating concentration levels within microenvironments 1 mass balance and 2 factors u s epa 2007 the mass balance method assumes ozone concentration is spatially uniform at any specific time and is used to compute concentration levels for all the indoor microenvironments e g indoors residence indoors schools it is estimated using the following parameters u s epa 2007 air exchange rate r a e is the rate that air flows in and out of a microenvironment decay rate r d e is the removal rate of ozone from a microenvironment due to deposition filtration and chemical reaction the decay rate r d e l at a given activity location is a function of the ambient concentration level c l h the air exchange rate r a e m e at a given activity location is a function of the ambient concentration level c l h the microenvironment e g indoors residence indoors school the average outdoor temperature and the presence of air conditioning us epa 2014b we sample from lognormal distributions for houston provided by the epa us epa 2014b to assign r d e l and r a e l m e to each of the indoor microenvironments within each activity location the concentration level c l m e h at an activity location l at time h for microenvironment m e is calculated as follows u s epa 2007 2 c l m e h δ c l i n h r c o m b i n e d c l m e h 1 δ c l i n h r c o m b i n e d 1 e r c o m b i n e d r c o m b i n e d where δ c l i n c l h r a e l m e r c o m b i n e d r a e l m e r d e l the factors method is simpler and is used to estimate concentration levels during in vehicle travel the penetration factor f p e which represents the fraction of ozone entering the microenvironment from the outside via air exchange is the only parameter used we assign f p e l to activity locations by sampling from a normal distribution us epa 2014b we compute in vehicle concentration levels using the following equation u s epa 2007 3 c l m e h c l h f p e l concentration levels for outdoor microenvironments e g outdoors other outdoors near road are assumed to be equal to the estimated ambient concentration level c l h u s epa 2007 2 3 the personal exposure model the personal exposure model is developed by coupling the individual activity patterns from the synthetic information model to the air quality estimates specifically we have point in time movement of synthetic individuals across their geo located activity locations and microenvironments additionally we have point in time concentration levels by geo located activity location and microenvironment we map the point in time movement for each synthetic individual to the corresponding ozone concentration level at that time geo location and microenvironment note that the assignment of concentration exposure to travel is made by splitting the travel time between the origin and destination activity locations therefore travel exposure is calculated based on the concentration of ozone at the origin and destination activity locations during the time of travel synthetic individuals may come in contact with varying concentration levels depending on their movement across activity locations and between microenvironments within the hour therefore we compute average hourly concentration exposure e i h for synthetic individual i by taking the weighted average of the concentration levels over the course of hour h the output is the exposure matrix e ℝ h i where h is the number of hours in a day 24 and i is the number of synthetic individuals approximately 4 9 million 3 results in this section we present the results of our model particularly focused on investigating the heterogeity of individual household and neighborhood level exposures across the study area section 3 1 we then explore results obtained by running several scenarios of the model assuming different levels of spatiotemporal resolution section 3 2 3 1 exposure heterogeneity we explore results of the model with particular focus on the spatiotemporal variation of exposures across the study area the results shown here apply the methodology discussed in the previous section we therefore assume that individuals move across the environment using their second by second activity sequences ozone concentration levels vary hourly across a 24 h period and ambient concentration levels are adjusted for indoor in vehicle and outdoor microenvironments the 24 h average exposure concentrations by zip code across the entire synthetic population with homes in harris county are shown in fig 3 the lighter shaded areas represent zip codes with relatively low average exposure levels while the darker shaded areas represent zip codes with relatively high exposure levels we find that the population living in the northeast region of the county generally experiences lower levels of ozone exposure while the southeast and central west regions experience some of the highest exposure levels even when averaged at the zip code level exposure levels are not homogenous across the county suggesting that where you live is an important consideration in estimating individual exposure levels we can drill in further and look at 24 h average exposure levels by household fig 4 illustrates close ups of two neighborhoods each square represents a household and the colors indicate the households level of exposure darker squares represent households with higher average exposure levels lighter squares represent households with lower average exposure levels the neighborhood on the right is in the downtown area of houston and the neighborhood on the left is in the northeast region of the county as expected households in the northeast neighborhood have generally lower average exposure levels however we can more clearly see here that there is heterogeneity within the small geographic area there are households in the northeast neighborhood that experience exposure levels just as high as those in downtown houston and conversely there are households in downtown houston that experience exposure levels as low as many of those in the northeast region this demonstrates that where you go during the day should be considered when estimating individual levels of exposure the platform further allows us to trace individuals over the course of the day hourly average exposure concentrations for two synthetic families are given in fig 5 these are two demographically similar families located in different houston neighborhoods family 1 is located in the southeast and family 2 is located in the northeast this demonstrates that ozone exposure is heterogeneous even between members within a household illustrating the significance of maintaining the resolution of the data at the individual level when deriving exposure estimates finally we explore results by demographic subpopulations for illustrative purposes we look at individuals by age group across two zip codes zip code 77339 where concentration levels were relatively low median concentration levels were 8 9 ppb h and zip code 77505 where concentration levels were relatively high median concentration levels were 30 3 ppb h fig 6 shows the 1 h average exposure concentrations across different age groups for the synthetic residents of the low concentration lc zip code blue and the high concentration hc zip code pink the lines represent median hourly exposure levels the dark shaded areas represent the 25th and 75th quartiles and the light shaded areas represent the remaining individuals as expected across all age groups the majority of synthetic individuals in lc are exposed to lower ozone levels than those in hc however while median exposures are lower in zip code lc some individuals in this zip code reach exposure levels just as extreme as in zip code hc which has some the highest levels of exposure in the region within each zip code we also find some trends in lc adults between the ages of 18 and 64 have the highest exposure levels this makes sense since they are more likely to travel outside of their home zip code to go to work where concentration levels may be much higher on the other hand we see no meaningful differences between average exposures across the ages in hc 3 2 scenario evaluation in this section we run three scenarios of the model to explore the potential impact that modeling at different levels of spatiotemporal resolution has on individual level exposures across a population scenario 1 assumes individuals stay home all day and calculates exposure by using geo located hourly concentration levels scenario 2 moves the individuals through their time activity sequences for the day and uses geo located 24 h average concentrations scenario 3 as described in sections 2 and 3 1 moves the individuals through their time activity sequences for the day and uses geo located hourly concentration levels daily 24 h average mean and peak exposure concentrations across the entire synthetic population for the three scenarios are shown in fig 7 lighter colors represent zip codes with lower average exposure levels while darker colors represent zip codes with higher average exposures within each scenario and across both average and peak exposures we see similar spatial trends the northeast region shows relatively lower average exposure levels and the southeast and central west areas show relatively high average exposure levels examining average exposures shown on top we find that when individuals are assumed to stay home scenario 1 estimated average exposure concentrations are lower than when individuals are allowed to move spatially according to their activity sequences as a consequence average exposure levels may be underestimated when making this simplifying assumption on the other examining average peak exposures shown on the bottom we find that assuming daily average concentration levels scenario 2 results in comparably lower estimates of average peak exposure this makes sense since this scenario eliminates any temporal variations such as spikes and dips in ozone levels scenario 2 is likely limited in its ability to identify cases with significant short term peak exposures we can also explore temporal variations in ozone exposure across a population fig 8 gives one hour average exposure traces for the synthetic population in a houston neighborhood zip code 77026 for one day under the three scenarios the top three charts illustrate results when we use ambient ozone concentration levels as a proxy for personal exposure and the bottom three charts show results when ozone concentration is adjusted for indoor in vehicle and outdoor microenvironments hourly median ozone exposure is represented by the line in the middle of each box the box represents the inter quartile range the middle 50 of individual ozone exposure levels the vertical lines above and below the box represent the upper 75 and lower quartiles 25 of exposure levels and the whiskers represent the remaining values or extremes we find that scenario 1 does not capture the extreme values which can be particularly important if the highest levels are experienced by already vulnerable populations scenario 2 on the other hand does not capture temporal variations in ozone concentration throughout the course of the day demonstrating that time sensitive exposures may not be captured note that while median exposure levels are generally lower when we account for indoor and outdoor behaviors the variability of exposure levels across the population is much higher signifying that some individuals will experience exposure levels as high as in runs where only outdoor levels are modeled for instance exposure under scenario 1 when using outdoor levels looks quite homogeneous across the population this makes sense since individuals shown live in the same neighborhood and are assumed to stay home all day when we account for their indoor and outdoor behaviors however we find that at peak hours there is nearly a 85 ppb h difference between individuals experiencing the lowest levels of exposure to ones experiencing the highest when synthetic individuals are allowed to move scenario 3 this range increases to 100 ppb h as discussed in section 1 previous studies have found that each 20 ppb increase in ozone over a period of one to three hours is associated with a 4 4 increased risk of having an out of hospital cardiac arrest ensor et al 2013 we examined whether modeling at finer spatiotemoral resolutions could better capture these potential risks we computed the one hour lagged change in ozone exposure for all individuals in zip code 77026 the same population as shown in fig 8 fig 9 shows the highest change in ozone exposure experienced by each individual across the three scenarios when we use outdoor levels only we find that nearly all individuals under scenarios 1 and 3 experienced at least once an increase of 20 ppb h or higher in ozone exposure over the course of the day on the other hand few synthetic individuals experienced any short term peak exposures and only one was over 20 ppb h under scenario 2 when we adjust for indoor in vehicle and outdoor microenvironments we find that almost half of the population experiences one hour exposure changes of less than 10 ppb under scenarios 1 and 2 the proportion of the population that experienced a 20 ppb h or higher increase in ozone exposure is 15 in scenario 1 and 5 in scenario 2 the distribution is much flatter and has a longer tail under scenario 3 signifying that more individuals are experiencing more extreme short term peak exposures nearly 30 of the population in scenario 3 experienced a 20 ppb h or higher ozone change note that for purposes of readability we cut the x axis at 45 ppb h values can reach up to 82 ppb h under scenario 3 4 discussion and conclusions in this paper we coupled air quality spatiotemporal estimates to the synthetic information model of the houston metropolitan area that captures the detailed individual level activities throughout the day one of the major challenges associated with creating high fidelity models is the limitation in computational resources and performance developing the synthetic information model creating air quality estimates and then coupling them is computationally intensive requiring a system that can handle processing and joining tables each with millions of rows of data while this type of computational challenge may have limited our ability to model at this level of resolution in the past the availability of high performance computing and database management techniques e g parallel processing b tree clustered indexing allowed for the effective processing summarization and exploration of these data this allowed us to attach specific exposure levels to the synthetic individuals based on the exact time of day the geo location of the activity and the condition of the physical environment this is crucial to better understand exposure pattern heterogeneity moreover it provided the flexibility to run various scenarios across different levels of spatiotemporal resolution allowing us to compare results when certain assumptions are relaxed models that do not account for finer levels of resolution in space and time may be missing important differences in the distribution of exposure in the population several scenarios of the model were run at different levels of resolution scenario 1 and scenario 2 relax assumptions around the geospatial movement of synthetic individuals and the temporal variations in ambient ozone concentrations respectively the first scenario is similar to studies that assumed individuals stayed home or close to home e g within the same census tract throughout the course of the day burke et al 2001 özkaynak et al 2008 us epa 2014a but modeled the temporal variations of a pollutant the second scenario most closely resembles studies that used similar 12 or 24 h averages of pollutant levels newth 2012 scenario 3 on the other hand maintains the spatiotemporal resolution of the data to compute estimates that account for spatiotemporal variations in ozone concentration levels and the movement of individuals across the geography and different microenvironments when we assume individuals remain within small geographic areas as in scenario 1 we may be underestimating exposure of those living in the relatively low concentration areas of houston particularly for populations over 18 that are more likely to travel outside of their home neighborhood for work and other activities such as to attend postsecondary schools or go shopping we saw evidence of this both spatially lighter regions in scenario 1 became darker under scenarios 2 and 3 and temporally populations aged over 18 were more likely to experience higher exposure to ozone during the day by not accounting for the geospatial movement of individuals we could potentially neglect vulnerable subpopulations that live in these types of neighborhoods scenario 2 is inadequate for detecting short term peak exposures which is important for capturing populations sensitive to health effects such as cardiac arrest and asthma attacks on the other hand one may need to consider the trade off between computational cost and data fidelity when studying the effects of certain pollutants to long term health outcomes e g lung cancer where understanding long term exposures to pollutants is important vallero 2014 studies have also used ambient concentration levels as a proxy for personal exposure newth 2012 we find that using this assumption likely overestimates individual ozone exposure levels and may fail to capture populations most at risk for adverse health effects associated with short term peak exposures as has been shown in previous studies increases in ozone levels over the course of a few hours can have major health implications supporting the need for exposure calculations that account for the detailed spatiotemporal resolution of pollutant levels and population movement ensor et al 2013 our results found at risk populations followed a bi modal distribution when we assume outdoor levels only nearly all individuals in scenarios 1 and 3 experienced at least once a change in hourly ozone levels above 20 ppb while in scenario 2 nearly no individuals experienced this on the other hand adjusting for indoor in vehicle and outdoor microenvironments resulted in a smoother distribution with greater variability across the population when considering at risk populations scenario 3 in this case is particularly concerning nearly 30 of the population experienced hourly peak exposures above 20 ppb with some as high as 80 ppb an increase that quadruples the risk of an out of hospital cardiac arrest keeping in mind the limitations and advantageous across each of the three scenarios one could cross reference the at risk populations under each scenario potentially allowing policy makers and healthcare professionals to be better informed of the subpopulations at risk data collected from personal monitoring systems allows one to develop individual exposure traces nikzad et al 2012 extending this to estimate personal exposures across a population however remains a challenge in this study we traced individual exposures capturing the variability of personal exposure levels across the population in our study area because of this we were able to detect the heterogeneity of exposures finding that an individual s exposure level throughout the day can be quite different across houston within neighborhoods and within households this type of analysis can potentially be used to inform resource allocation strategies that better target vulnerable neighborhoods subpopulations and individuals in further work we will couple the synthetic information model to an improved spatiotemporal ozone model developed by ensor ensor and raun 2015 we will also generate activity patterns that reflect important variations with respect to exposure levels such as type of day e g weekday vs weekend and time of year e g winter vs summer the coupling of spatiotemporal air quality estimates with the synthetic information model of the houston metropolitan area in this study resulted in an initial implementation of the in silico analytics platform to evaluate disparities in exposure to air pollution at a level of detail not possible with other models this allowed us to attach specific exposure levels to the synthetic individuals furthermore the heterogeneous exposure levels of the population across time are more accurately reflected allowing for increased sensitivity to detecting the variation of exposure across the population and within zip codes neighborhoods and even households 
26412,there is ample evidence that short term ozone exposure is associated with increased respiratory symptoms many studies however aggregate the population activities or concentration levels of the pollutant across space and or time failing to capture critical variations in the exposure levels we couple spatiotemporal air quality estimates of ozone with a synthetic information model of the houston metropolitan area allowing us to attach exposure levels to individuals based on exact times geo locations and microenvironments of activities several scenarios of the model are run at different levels of resolution when we maintain the spatiotemporal resolution of the data the proportion of the population that experiences sharp increases in short term exposure increases substantially this can be particularly important if experienced by sensitive populations given the increased risk for adverse health effects we find that individuals in the same zip code neighborhood and even household have varying levels of exposure keywords synthetic populations air quality ozone microenvironment personal exposure 1 introduction exposure is the contact an individual has with a pollutant a function of both the concentration of the pollutant in the environment and the time an individual is exposed to that pollutant us epa 2011 localized specific exposures to ozone can dramatically increase health risks for cardiac events and asthma ensor et al 2013 2014 raun et al 2014 davis and ensor 2006 for instance each 20 parts per billion ppb increase in ambient ozone o 3 concentration in the previous one to three hours is associated with a 4 4 increased risk of having an out of hospital cardiac arrest which is particularly significant given that 90 of these cases result in death ensor et al 2013 accurate representations of the magnitude frequency and duration of localized exposure to a pollutant requires that we account for individual activity patterns across both space and time vallero 2014 many studies however average the activities of individuals in time and space into 12 or 24 h summaries klepeis et al 2001 leech et al 2002 matz et al 2014 summarizing the data in this way may miss important variations in ozone exposure across a population in this paper we couple spatiotemporal air quality estimates of ozone o 3 ppb hours across the houston metropolitan area of texas to a data informed synthetic information model the synthetic information model is a simulated population that is representative of the true population of the houston metropolitan area including the composition of households the demographics of individuals and their movement throughout the course of the day adigaa et al 2015 parikh et al 2013 we maintain the spatiotemporal resolution of the data such that individual exposure estimates may vary on a second by second basis as synthetic individuals move across the geography entering and leaving geo located microenvironments each with their own unique ozone concentration estimates it overcomes the limitations of traditional approaches as it is informed by how people move through their activities during the day allowing us to attach specific exposure levels to the synthetic individuals based on the exact geo location of the activity and time of day our research has been motivated by the many studies ensor et al 2013 2014 raun et al 2014 that have shown that the resolution of the data is an important consideration as it can impact important health effects that could be translated into lifesaving behavioral and policy changes exposure modeling has been an active research area from a variety of different research approaches duan 1982 jerrett et al 2005 ryan and lemasters 2007 zou et al 2009 community based studies rosenthal et al 2008 silverman et al 2010 wellenius et al 2012 identify a significant impact on health from air pollution levels but do not directly measure individual exposure early research using microenvironment monitoring models for instance assigned exposure concentrations based on time spent within different microenvironments e g indoor locations outdoors but did not assign these microenvironments to individual level activity patterns fugas 1975 extending this work models incorporated activity diaries allowing for some estimates of the variability and distribution of individual exposure but did not account for temporal variations in concentration levels ott and flachsbart 1982 other models attach air pollution exposure to populations at a group level based on the demographics of a subset of individuals the geographic location of homes and activities or a set of microenvironments while some of these studies model representative individuals kousa et al 2002 burke et al 2001 özkaynak et al 2008 they either stop short in their ability to trace individuals throughout the course of the day or in modeling a representative population of the geographic location in question a subset of exposure models have focused on calculating personal exposure to emissions and other pollutants while traveling accounting for factors such as transportation mode vehicle type and transportation routes hatzopoulou et al 2011 hülsmann et al 2011 while these models seek to accurately reflect exposure during travel they do not account for the individual s full daily course of activities moving away from aggregate level exposure calculations and accounting for an individual s daily activities more recent exposure models have developed synthetic populations to represent each individual in a geographic location for instance the environmental protection agency epa s air pollutants exposure apex model develops a synthetic population that characterizes the study area and utilizes the consolidated human activity database chad a repository of harmonized human activity data to assign activity patterns to synthetic individuals us epa 2012 in this model a person s exposure is obtained by mapping the activities reported in the survey into several microenvironments e g indoors residence each with an estimated exposure rate geography is measured at the level of census tracts a geographic region with a population size between 1 200 and 8 000 people activities are assigned in hourly chuncks and individuals may move from their assigned home tract only for work other synthetic population models are limited in their use for estimating environmental exposure to contaminants that vary over both space and time these methods aggregate activities into percent time e g percent of day and allocate the aggregated time to an activity location lenormand and deffuant 2013 namazi rad et al 2014 wheaton et al 2009 us epa 2014a as an example a study of sydney australia uses traditional synthetic population models with single daily exposure values and couples the percent time spent at various locations to these daily average exposure levels newth 2012 other studies focus on direct measurement of human exposure through personal monitors or home based centers but cost measurement accuracy and logistics limit the use of this approach on a scale large enough to provide continuous community wide understanding of exposure weisel et al 2005 us epa 2015 more recent studies have used personal monitoring systems as a way to estimate regional concentration levels of ozone and other air pollutants xu et al 2017 for instance nikzad et al 2012 used sensors and a mobile phone application to collect data from 16 participants in san diego california over the course of a month regional estimates derived from this data were then compared to levels measured by epa monitors across the county while results are promising for generating air quality estimates these studies do not account for the individual activity sequences of the population our contribution is the development of an in silico analytics platform that uses the synthetic information model to understand the impact of air quality over physical space and time on individual exposure levels we estimate hourly ozone levels for the geo coordinates latitude longitude associated with the activity locations in our synthetic information model and allow synthetic individuals to move across this geography based on their second by second time sequenced activities we combine individual activity patterns with the microenvironment modeling approach assigning each geo located activity location hourly exposure rates over a set of microenvironments few studies consider human activity patterns in detail at the population level due to availability of data to accurately generate representative populations and their movement and the computational burden associated with working with such vasts amounts of data coupling this information to pollutant levels that vary spatiotemporally and by microenvironment adds another level of difficulty we overcome such computational challenges by applying high performance computing and database management techniques and expand on current exposure models by integrating data sources across many publicly and commercially available datasets this allows us to trace ozone exposures of each synthetic individual as they move throughout the course of the day the spatiotemporal resolution of the data handled by this platform provides additional flexibility for comparing results across different sets of assumptions 2 methodology in this section we describe the in silico analytics platform we developed which couples a synthetic information representation of the residents in the houston metropolitan area to spatiotemporal air quality estimates we begin by describing the synthetic information model developed at virginia tech marathe et al 2014 section 2 1 it includes socio demographically relevant activity sequences and the movement of each individual in the population through their sequences second by second during the day this allows aggregation of time intervals to match the environmental quality data e g hourly intervals section 2 2 we then discuss the methodology used to determine individual level exposures to ozone section 2 3 the output of the platform is the exposure profiles for the roughly 4 9 million synthetic individuals in the houston metropolitan area fig 1 illustrates the conceptual model of the in silico analytics platform this platform provides an integrated database that can be used and reused for the analysis of various studies related to the synthetic information model and air quality moreover given the fidelity of the data the platform can process we have the flexibility to explore results under various sets of assumptions 2 1 synthetic information model the first step in creating the in silico analytics platform is to generate the synthetic information model for the houston metropolitan area the synthetic information model is a set of synthetic people each associated with demographic variables located geographically at specific points in time and place such as homes and schools each associated with specific geo locations it is created by integrating a variety of databases from commercial and public sources including statistical surveys administrative data and data on the built environment e g buildings roads and land use through a process that preserves the confidentiality of the individuals in the original data sets yet produces realistic attributes and demographics for synthetic individuals adigaa et al 2015 the synthetic information model approach has been used to study a wide range of phenomena to support policy decision making barrett et al 2006 marathe et al 2014 including transportation epidemics parikh et al 2013 and natural and man made disasters barrett et al 2005 halloran et al 2008 lewis et al 2013 marathe and vullikanti 2013 the steps to the generation of the synthetic information model include 1 population synthesis in which a synthetic representation of each individual and household in a region is created using socioeconomic characteristics from census data 2 activity and microenvironment assignments in which each synthetic individual is assigned a set of activities to perform during the day along with start and end times based on activity or time use survey data and 3 place choice in which an appropriate location is chosen for each activity for every synthetic individual based on several data sources including land use patterns and tax data the main data sources used in the creation of the houston synthetic information model are described in table 1 the american community survey acs provides tables of distributions at the census block group level a geographical region containing 600 to 3000 individuals on demographic characteristics such as age gender household income and household size which are referred to as marginal distributions joint demographic distributions are reconstructed from these marginal distributions using public use microdata sample pums data which provides a 5 representative sample of the acs data pums information are incorporated into the joint distribution through an iterative proportional fitting technique beckman et al 1996 the process ensures that the synthetic population including individual demographic characteristics and household composition match the structure of the true population each synthetic household is then located geographically using land use data and data pertaining to businesses and transportation networks realistic activity patterns and their associated activity locations are then added the activities performed by each synthetic individual on an average day is determined by analyzing the activity patterns in the national household travel survey and linking these patterns to the socio demographic composition of the households e g income and individuals within the households e g age gender this is done using a process known as the fitted values method lum et al 2016 this method preserves within household activity correlations and ensures that activity sequences of synthetic individuals are similar to actual individuals with similar socio demographic characteristics and household membership activities are further characterized by one or more microenvironments a microenvironment is defined as a three dimensional space that is relatively homogeneous with respect to pollutant concentrations for a specified time period u s epa 2007 we model 11 microenvironments 6 indoor microenvironments indoors residence indoors school indoors office indoors shopping indoors bars and restaurants indoors other 3 outdoor microenvironments outdoors near road outdoors public garage or parking outdoors other and 2 in vehicle microenvironments in vehicle cars or trucks in vehicle mass transit we adapted the epa s u s epa 2007 mapping of activity location types e g bedroom park golf course office building in chad to microenvironments moreover to link these activity location types to our synthetic information model we created a crosswalk between activities and locations in the synthetic information model to one or more microenvironments for instance the activity home can be associated with up to two microenvironments indoors residence and outdoors other using data from chad we developed empirical distributions to represent the proportion of time spent within each microenvironment while performing a specified activity we then sampled from these distributions to get time in seconds spent within each microenvironment while performing an activity the houston metropolitan area synthetic information includes any synthetic individual with either a home or activity location within harris county texas this resulted in approximately 4 9 million individuals grouped into 1 8 million households synthetic individuals can perform up to 6 different types of activities including staying home going to work going to school shopping traveling and other e g socializing at a friend s residence going to a restaurant activity types can be performed multiple times by the same individual on the same day activities occur in 1 2 million different activity locations 895 thousand housing locations 166 thousand work locations 37 thousand shopping locations 4 thousand schools and 112 thousand other locations fig 2 shows the household and activity locations across the region as well as an example set of activities for one household 2 2 air quality estimation according to the texas commission on environmental quality tceq the air quality in houston texas is monitored more closely and analyzed with more intensity than perhaps anywhere in the country if not in the world texas commission on environmental quality 2005 ozone is measured hourly at 47 sites us epa 2012 one hour ambient meteorological temperature relative humidity and wind speed data from the monitors are publicly available for this demonstration hourly recordings on august 26 2008 of ambient ozone concentration levels o 3 ppb hours across 39 epa monitors in the houston metropolitan area is used 1 1 we selected august 26 2008 because it represents a warm day in houston with time varying levels of ozone and is within the same time span of the study performed by ensor ensor et al 2013 on august 26 2008 39 of the total 47 monitors recorded ozone readings the first step to coupling the environmental pollutant data with the synthetic information model is to assign hourly ambient ozone concentration levels to each of the 1 2 million activity locations this is done using the inverse weighted distance from each location to the 39 monitors a standard method for assigning ozone concentration hwang and jaakkola 2008 lu and wong 2008 salam et al 2005 given that m j t is the concentration measured by monitor j 1 n at hour h the pollutant concentration c l at each activity location l 1 l at hour h 1 24 is calculated as follows 1 c l h j 1 n 1 d l j 2 m j h j 1 n 1 d l j 2 where d l j is the great circle distance between location l and monitor j nychka et al 2015 the next step is to compute concentration levels by microenvironment for each activity location and for each hour of the day see section 2 1 we apply one of two methods used for estimating concentration levels within microenvironments 1 mass balance and 2 factors u s epa 2007 the mass balance method assumes ozone concentration is spatially uniform at any specific time and is used to compute concentration levels for all the indoor microenvironments e g indoors residence indoors schools it is estimated using the following parameters u s epa 2007 air exchange rate r a e is the rate that air flows in and out of a microenvironment decay rate r d e is the removal rate of ozone from a microenvironment due to deposition filtration and chemical reaction the decay rate r d e l at a given activity location is a function of the ambient concentration level c l h the air exchange rate r a e m e at a given activity location is a function of the ambient concentration level c l h the microenvironment e g indoors residence indoors school the average outdoor temperature and the presence of air conditioning us epa 2014b we sample from lognormal distributions for houston provided by the epa us epa 2014b to assign r d e l and r a e l m e to each of the indoor microenvironments within each activity location the concentration level c l m e h at an activity location l at time h for microenvironment m e is calculated as follows u s epa 2007 2 c l m e h δ c l i n h r c o m b i n e d c l m e h 1 δ c l i n h r c o m b i n e d 1 e r c o m b i n e d r c o m b i n e d where δ c l i n c l h r a e l m e r c o m b i n e d r a e l m e r d e l the factors method is simpler and is used to estimate concentration levels during in vehicle travel the penetration factor f p e which represents the fraction of ozone entering the microenvironment from the outside via air exchange is the only parameter used we assign f p e l to activity locations by sampling from a normal distribution us epa 2014b we compute in vehicle concentration levels using the following equation u s epa 2007 3 c l m e h c l h f p e l concentration levels for outdoor microenvironments e g outdoors other outdoors near road are assumed to be equal to the estimated ambient concentration level c l h u s epa 2007 2 3 the personal exposure model the personal exposure model is developed by coupling the individual activity patterns from the synthetic information model to the air quality estimates specifically we have point in time movement of synthetic individuals across their geo located activity locations and microenvironments additionally we have point in time concentration levels by geo located activity location and microenvironment we map the point in time movement for each synthetic individual to the corresponding ozone concentration level at that time geo location and microenvironment note that the assignment of concentration exposure to travel is made by splitting the travel time between the origin and destination activity locations therefore travel exposure is calculated based on the concentration of ozone at the origin and destination activity locations during the time of travel synthetic individuals may come in contact with varying concentration levels depending on their movement across activity locations and between microenvironments within the hour therefore we compute average hourly concentration exposure e i h for synthetic individual i by taking the weighted average of the concentration levels over the course of hour h the output is the exposure matrix e ℝ h i where h is the number of hours in a day 24 and i is the number of synthetic individuals approximately 4 9 million 3 results in this section we present the results of our model particularly focused on investigating the heterogeity of individual household and neighborhood level exposures across the study area section 3 1 we then explore results obtained by running several scenarios of the model assuming different levels of spatiotemporal resolution section 3 2 3 1 exposure heterogeneity we explore results of the model with particular focus on the spatiotemporal variation of exposures across the study area the results shown here apply the methodology discussed in the previous section we therefore assume that individuals move across the environment using their second by second activity sequences ozone concentration levels vary hourly across a 24 h period and ambient concentration levels are adjusted for indoor in vehicle and outdoor microenvironments the 24 h average exposure concentrations by zip code across the entire synthetic population with homes in harris county are shown in fig 3 the lighter shaded areas represent zip codes with relatively low average exposure levels while the darker shaded areas represent zip codes with relatively high exposure levels we find that the population living in the northeast region of the county generally experiences lower levels of ozone exposure while the southeast and central west regions experience some of the highest exposure levels even when averaged at the zip code level exposure levels are not homogenous across the county suggesting that where you live is an important consideration in estimating individual exposure levels we can drill in further and look at 24 h average exposure levels by household fig 4 illustrates close ups of two neighborhoods each square represents a household and the colors indicate the households level of exposure darker squares represent households with higher average exposure levels lighter squares represent households with lower average exposure levels the neighborhood on the right is in the downtown area of houston and the neighborhood on the left is in the northeast region of the county as expected households in the northeast neighborhood have generally lower average exposure levels however we can more clearly see here that there is heterogeneity within the small geographic area there are households in the northeast neighborhood that experience exposure levels just as high as those in downtown houston and conversely there are households in downtown houston that experience exposure levels as low as many of those in the northeast region this demonstrates that where you go during the day should be considered when estimating individual levels of exposure the platform further allows us to trace individuals over the course of the day hourly average exposure concentrations for two synthetic families are given in fig 5 these are two demographically similar families located in different houston neighborhoods family 1 is located in the southeast and family 2 is located in the northeast this demonstrates that ozone exposure is heterogeneous even between members within a household illustrating the significance of maintaining the resolution of the data at the individual level when deriving exposure estimates finally we explore results by demographic subpopulations for illustrative purposes we look at individuals by age group across two zip codes zip code 77339 where concentration levels were relatively low median concentration levels were 8 9 ppb h and zip code 77505 where concentration levels were relatively high median concentration levels were 30 3 ppb h fig 6 shows the 1 h average exposure concentrations across different age groups for the synthetic residents of the low concentration lc zip code blue and the high concentration hc zip code pink the lines represent median hourly exposure levels the dark shaded areas represent the 25th and 75th quartiles and the light shaded areas represent the remaining individuals as expected across all age groups the majority of synthetic individuals in lc are exposed to lower ozone levels than those in hc however while median exposures are lower in zip code lc some individuals in this zip code reach exposure levels just as extreme as in zip code hc which has some the highest levels of exposure in the region within each zip code we also find some trends in lc adults between the ages of 18 and 64 have the highest exposure levels this makes sense since they are more likely to travel outside of their home zip code to go to work where concentration levels may be much higher on the other hand we see no meaningful differences between average exposures across the ages in hc 3 2 scenario evaluation in this section we run three scenarios of the model to explore the potential impact that modeling at different levels of spatiotemporal resolution has on individual level exposures across a population scenario 1 assumes individuals stay home all day and calculates exposure by using geo located hourly concentration levels scenario 2 moves the individuals through their time activity sequences for the day and uses geo located 24 h average concentrations scenario 3 as described in sections 2 and 3 1 moves the individuals through their time activity sequences for the day and uses geo located hourly concentration levels daily 24 h average mean and peak exposure concentrations across the entire synthetic population for the three scenarios are shown in fig 7 lighter colors represent zip codes with lower average exposure levels while darker colors represent zip codes with higher average exposures within each scenario and across both average and peak exposures we see similar spatial trends the northeast region shows relatively lower average exposure levels and the southeast and central west areas show relatively high average exposure levels examining average exposures shown on top we find that when individuals are assumed to stay home scenario 1 estimated average exposure concentrations are lower than when individuals are allowed to move spatially according to their activity sequences as a consequence average exposure levels may be underestimated when making this simplifying assumption on the other examining average peak exposures shown on the bottom we find that assuming daily average concentration levels scenario 2 results in comparably lower estimates of average peak exposure this makes sense since this scenario eliminates any temporal variations such as spikes and dips in ozone levels scenario 2 is likely limited in its ability to identify cases with significant short term peak exposures we can also explore temporal variations in ozone exposure across a population fig 8 gives one hour average exposure traces for the synthetic population in a houston neighborhood zip code 77026 for one day under the three scenarios the top three charts illustrate results when we use ambient ozone concentration levels as a proxy for personal exposure and the bottom three charts show results when ozone concentration is adjusted for indoor in vehicle and outdoor microenvironments hourly median ozone exposure is represented by the line in the middle of each box the box represents the inter quartile range the middle 50 of individual ozone exposure levels the vertical lines above and below the box represent the upper 75 and lower quartiles 25 of exposure levels and the whiskers represent the remaining values or extremes we find that scenario 1 does not capture the extreme values which can be particularly important if the highest levels are experienced by already vulnerable populations scenario 2 on the other hand does not capture temporal variations in ozone concentration throughout the course of the day demonstrating that time sensitive exposures may not be captured note that while median exposure levels are generally lower when we account for indoor and outdoor behaviors the variability of exposure levels across the population is much higher signifying that some individuals will experience exposure levels as high as in runs where only outdoor levels are modeled for instance exposure under scenario 1 when using outdoor levels looks quite homogeneous across the population this makes sense since individuals shown live in the same neighborhood and are assumed to stay home all day when we account for their indoor and outdoor behaviors however we find that at peak hours there is nearly a 85 ppb h difference between individuals experiencing the lowest levels of exposure to ones experiencing the highest when synthetic individuals are allowed to move scenario 3 this range increases to 100 ppb h as discussed in section 1 previous studies have found that each 20 ppb increase in ozone over a period of one to three hours is associated with a 4 4 increased risk of having an out of hospital cardiac arrest ensor et al 2013 we examined whether modeling at finer spatiotemoral resolutions could better capture these potential risks we computed the one hour lagged change in ozone exposure for all individuals in zip code 77026 the same population as shown in fig 8 fig 9 shows the highest change in ozone exposure experienced by each individual across the three scenarios when we use outdoor levels only we find that nearly all individuals under scenarios 1 and 3 experienced at least once an increase of 20 ppb h or higher in ozone exposure over the course of the day on the other hand few synthetic individuals experienced any short term peak exposures and only one was over 20 ppb h under scenario 2 when we adjust for indoor in vehicle and outdoor microenvironments we find that almost half of the population experiences one hour exposure changes of less than 10 ppb under scenarios 1 and 2 the proportion of the population that experienced a 20 ppb h or higher increase in ozone exposure is 15 in scenario 1 and 5 in scenario 2 the distribution is much flatter and has a longer tail under scenario 3 signifying that more individuals are experiencing more extreme short term peak exposures nearly 30 of the population in scenario 3 experienced a 20 ppb h or higher ozone change note that for purposes of readability we cut the x axis at 45 ppb h values can reach up to 82 ppb h under scenario 3 4 discussion and conclusions in this paper we coupled air quality spatiotemporal estimates to the synthetic information model of the houston metropolitan area that captures the detailed individual level activities throughout the day one of the major challenges associated with creating high fidelity models is the limitation in computational resources and performance developing the synthetic information model creating air quality estimates and then coupling them is computationally intensive requiring a system that can handle processing and joining tables each with millions of rows of data while this type of computational challenge may have limited our ability to model at this level of resolution in the past the availability of high performance computing and database management techniques e g parallel processing b tree clustered indexing allowed for the effective processing summarization and exploration of these data this allowed us to attach specific exposure levels to the synthetic individuals based on the exact time of day the geo location of the activity and the condition of the physical environment this is crucial to better understand exposure pattern heterogeneity moreover it provided the flexibility to run various scenarios across different levels of spatiotemporal resolution allowing us to compare results when certain assumptions are relaxed models that do not account for finer levels of resolution in space and time may be missing important differences in the distribution of exposure in the population several scenarios of the model were run at different levels of resolution scenario 1 and scenario 2 relax assumptions around the geospatial movement of synthetic individuals and the temporal variations in ambient ozone concentrations respectively the first scenario is similar to studies that assumed individuals stayed home or close to home e g within the same census tract throughout the course of the day burke et al 2001 özkaynak et al 2008 us epa 2014a but modeled the temporal variations of a pollutant the second scenario most closely resembles studies that used similar 12 or 24 h averages of pollutant levels newth 2012 scenario 3 on the other hand maintains the spatiotemporal resolution of the data to compute estimates that account for spatiotemporal variations in ozone concentration levels and the movement of individuals across the geography and different microenvironments when we assume individuals remain within small geographic areas as in scenario 1 we may be underestimating exposure of those living in the relatively low concentration areas of houston particularly for populations over 18 that are more likely to travel outside of their home neighborhood for work and other activities such as to attend postsecondary schools or go shopping we saw evidence of this both spatially lighter regions in scenario 1 became darker under scenarios 2 and 3 and temporally populations aged over 18 were more likely to experience higher exposure to ozone during the day by not accounting for the geospatial movement of individuals we could potentially neglect vulnerable subpopulations that live in these types of neighborhoods scenario 2 is inadequate for detecting short term peak exposures which is important for capturing populations sensitive to health effects such as cardiac arrest and asthma attacks on the other hand one may need to consider the trade off between computational cost and data fidelity when studying the effects of certain pollutants to long term health outcomes e g lung cancer where understanding long term exposures to pollutants is important vallero 2014 studies have also used ambient concentration levels as a proxy for personal exposure newth 2012 we find that using this assumption likely overestimates individual ozone exposure levels and may fail to capture populations most at risk for adverse health effects associated with short term peak exposures as has been shown in previous studies increases in ozone levels over the course of a few hours can have major health implications supporting the need for exposure calculations that account for the detailed spatiotemporal resolution of pollutant levels and population movement ensor et al 2013 our results found at risk populations followed a bi modal distribution when we assume outdoor levels only nearly all individuals in scenarios 1 and 3 experienced at least once a change in hourly ozone levels above 20 ppb while in scenario 2 nearly no individuals experienced this on the other hand adjusting for indoor in vehicle and outdoor microenvironments resulted in a smoother distribution with greater variability across the population when considering at risk populations scenario 3 in this case is particularly concerning nearly 30 of the population experienced hourly peak exposures above 20 ppb with some as high as 80 ppb an increase that quadruples the risk of an out of hospital cardiac arrest keeping in mind the limitations and advantageous across each of the three scenarios one could cross reference the at risk populations under each scenario potentially allowing policy makers and healthcare professionals to be better informed of the subpopulations at risk data collected from personal monitoring systems allows one to develop individual exposure traces nikzad et al 2012 extending this to estimate personal exposures across a population however remains a challenge in this study we traced individual exposures capturing the variability of personal exposure levels across the population in our study area because of this we were able to detect the heterogeneity of exposures finding that an individual s exposure level throughout the day can be quite different across houston within neighborhoods and within households this type of analysis can potentially be used to inform resource allocation strategies that better target vulnerable neighborhoods subpopulations and individuals in further work we will couple the synthetic information model to an improved spatiotemporal ozone model developed by ensor ensor and raun 2015 we will also generate activity patterns that reflect important variations with respect to exposure levels such as type of day e g weekday vs weekend and time of year e g winter vs summer the coupling of spatiotemporal air quality estimates with the synthetic information model of the houston metropolitan area in this study resulted in an initial implementation of the in silico analytics platform to evaluate disparities in exposure to air pollution at a level of detail not possible with other models this allowed us to attach specific exposure levels to the synthetic individuals furthermore the heterogeneous exposure levels of the population across time are more accurately reflected allowing for increased sensitivity to detecting the variation of exposure across the population and within zip codes neighborhoods and even households 
26413,we propose a method for analyzing ocean currents using a statistical approach the proposed technique is useful for analyzing global velocity fields and producing indices to describe the probable trajectories and destinations of particles embedded in such fields short term lagrangian integration of the velocities was used to generate transition matrices that define the system locally a reshuffling algorithm based on standard markov chain theory was implemented to mix and synthesize the information involved in the global analysis iterative methods were then used to solve the resulting large and sparse linear systems the method efficiently used local information short term lagrangian integration to infer global characteristics of the system two case studies were presented to emphasize the merits of the described scheme one using modeled data from the gulf of california and another from the gulf of mexico keywords connectivity markov chain ocean current ocean models probable trajectories probable arrival time 1 introduction 1 1 models physical oceanographers are increasingly being called upon to determine the fate of freely moving matter in the ocean one goal for example is to identify areas of influence or domains of attraction consistent with a given pattern of motion these features have been extensively studied in the context of dynamic systems e g bergé et al 1984 lichtenberg and lieberman 1983 for fields associated with geophysical systems being able to statistically determine the destinations or origins of water parcels embedded in ocean currents could be even more important the applications associated with these capabilities are without a doubt significant e g in determining connectivity larvae dispersal contingency approaches for pollutant spills deployment for search and rescue and strategies for cleaning marine areas one of the ways particle movement has been addressed is by studying lagrangian trajectories e g marinone et al 2008 santiago garcía et al 2014 and extracting the appropriate statistics e g lilly 1972a b lacasce 2008 numerical hydrodynamical models are now standard tools for the analysis of ocean phenomena fig 1 for example shows the surface circulation for the gulf of mexico using an implementation of the regional ocean modeling system roms the figure shows one snapshot of the surface velocity taken from an operational system at the center for research and higher education of ensenada cicese the model runs daily and is forced at the surface using global forecast system gfs data gfs 2017 and at the boundaries using mercator ocean forecast mercator data mercator 2017 well known features of the circulation in the gulf of mexico are evident in the figure there is a strong current the loop current that flows around cuba from the caribbean to the atlantic feeding the gulf stream a well defined gyre induced by an earlier loop current detachment is also evident the reproduced gyre and its subsequent westward motion is in agreement with previously reported observations e g sturges and lugo fernandez 2005 oey et al 2005 in fig 2 a and b we present a series of trajectories calculated using this surface circulation lagrangian equations were integrated off line using a fourth order milne hamming predictor corrector scheme one thousand inert particles were released at a single point in the northern region of the gulf and followed for 120 days fig 2a shows the progression of the particles while fig 2b shows a few arbitrary trajectories for the same period although much information is inferable from these data and numerous articles have used lagrangian trajectories as the basis for the description of the integrated characteristics of numerical circulation models e g santiago garcía et al 2014 montaño cortes et al 2017 often these spaghetti diagrams are difficult to interpret and synthesize lacasce 2008 presents an excellent review of statistical analyses from oceanic lagrangian data it is evident for example that in the case of fig 2 an average direction is not clearly defined which is a reflection of the turbulent nature of the trajectories and rapid memory loss by the process a statistically more robust description of fig 2 would be very useful it is possible for example to deduce from these figures that three main regions are transited by the particles a large percentage stay near the area of release and get trapped by the large eddy in the center of the gulf while some particles go straight to the west along the northern coast then turning south at the west coast and a few others leave the gulf from the florida cuba channel of course a more precise and systematic definition of a large percentage tend to stay and a few particles would be very useful and is the primary objective of this work one strategy that has been used to synthesize such information involves the use of connectivity matrices these matrices lump together information in a rough way however and often make analysis difficult e g marinone et al 2008 montaño cortes et al 2017 the ability to track drifters as a tool for understanding dispersion in the ocean is a scientific endeavor that has been active for some time and is well represented in the environmental modeling software journal ems a light search of ems for the pasts years shows a substantial amount of articles that similar to this one present tools for finding and better understanding the dispersion of water parcels or particles in the ocean using statistics calculated from virtual drifters for most of these articles the primary goal is to develop a robust way of describing how the water circulation dictates the dispersal of things e g larvae pollutant oil droplets etc in all of these articles the drift trajectory is calculated similarly a deterministic background provided by a model analytical or numerical plus a random part to represent the dispersion the specific details are of course different with different circulation models being used i e roms hycom delf3d etc and a random part integrated using different stochastic schemes the main thrust for these studies has probably been predicting the fate of oil spills e g boufadel et al 2007 otero et al 2015 sayol et al 2014 but other motivations are also well represented the study of larvae dispersion e g kool and nichol 2015 qin et al 2017 physical and ecological connectivity e g paris et al 2013 kool and nichol 2015 analysis of pollution drifts e g suh 2006 qin et al 2017 assessment of ecological risks e g otero et al 2015 qin et al 2017 seek and rescue operation e g sayol et al 2014 or understanding the dynamics of the circulation e g suh 2006 furnans et al 2008 suh 2006 used a hybrid approach to predict the dispersion of contaminants individual particles dispersed by a combination of a deterministic component and a stochastic one associated to a monte carlo scheme suh 2006 augment the lagrangian equation by a buoyancy effect that causes additional horizontal dispersion for a test of the feasibility of the method using a simple system with an analytical solution shu founds that it gives good results near the source where the gradients are large but that it is not suitable for large distances from it for analyzing the transport of oil droplets on the water surface due to waves boufadel et al 2007 used a lagrangian equation to integrate the position of particles the velocity was composed again of two parts a random component associated to turbulence and an analytical theoretical one associated to wave motion to simulate the oil part of the droplets they also included a rising velocity due to buoyancy and found that strokes drift contributed greatly to the horizontal transport using a monte carlo approach on a simple domain they investigated the effect of the wave parameter i e steepness on their capacity of dispersion they showed that turbulence and buoyancy work in opposing direction enhancing and limiting respectively the vertical transport of the plume while stokes drift was the major mechanism for horizontal transport furnans et al 2008 investigated the influence of inertia and drag on the simulation of real drifter movement they incorporated the effect of wind and inertia in determining virtual particles movement they improved the velocity provided by the hydrodynamic model elcom incorporating winds and a component that parametrized inertia drag and slip furnans et al 2008 provide a quantitative assessment of the merits of their implementation by comparing the motion of modeled versus real drifters they found that their method improves the simulation coupling a probabilistic individual based model ibm into a series of nested hydrographic models e g hycom roms etc paris et al 2013 developed a probabilistic connectivity model to study the mechanical reasons of dispersion and migration processes in the ocean the hydrographic model provides the basic motion of individual particles while each particle is given a biological attribute their model estimates the probability of population connectivity in the form of connectivity matrix and trajectory of individual particles complex larvae migration dispersion and retention as well as the dispersion of abiotic particles such as pollutants can be assessed by their software an operational model for tracking surface objects in the ocean was presented by sayol et al 2014 the eulerian field was provided by an operational implementation of roms i e western mediterranean operational model a random walk term is added to simulate diffusivity the authors computed the probability density functions by averaging the trajectory of an ensemble of particles forecast for the final position of particles is given in the form of accumulated probability density contours this research is purportedly directed to investigate oil spill trajectories and search and rescue operations kool and nichol 2015 simulated the 3d movement of marine larvae by advecting individual particles through a velocity field and adding a random component representing diffusion each particle is analyzed at regular intervals and moved accordingly to the ambient velocity i e hycom and simulated diffusion fields and endowed with biological characteristics e g mortality is implemented by an exponential or weibull distribution probability their research provides a platform for the study of 3d connectivity in the form of connectivity matrixes and maps of probability surfaces for case study kool and nichol 2015 analyzed larval dispersion in the western australian coast otero et al 2015 calculates the drift pathway of simulated particles using advection transport by surface currents from an operational configuration of roms and winds from the wrf model plus turbulent diffusion the diffusivity velocity depends on the sea turbulent characteristics simulated as brownian motion of particles utilizing a random walk process a wind drag coefficient is also added in case of oil spill studies they evaluated the virtue of their application comparing their result with the trajectory of the prestige oil spill accident of november 2002 montero et al 2003 authors mentioned other possible applications of their work as the study of harmful algae populations qin et al 2017 used a turbulent diffusive component i e fractional brownian model combined with an advection component provided by ocean current and wind from delf3d and wrf respectively to calculate pollutant drift for each particle which represents a certain amount of simulated substance their model solves a lagrangian equation with the velocities given by the ocean and atmospheric models or constant velocities and a stochastic part added to the position of each particle to show the efficiency and applicability of their approach the authors present a case study using data from the gulf of finland they simulated the dispersion of algae bloom red tide and found that the non fickian diffusion i e fractional brownian motion better represent the shape of the trajectories their procedure gives a quick and efficient way to provide for the trajectories of surface drifting pollutant to decision makers the present article follows the on going dialog in ems summarized in the previous paragraphs on ways to represent the pathways of virtual particles embedded in a given velocity field similar to the research mentioned these pathways are calculated using a deterministic velocity field i e from roms augmented with a random component that simulates dispersion the methods presented in this article can be considered as a continuation of this topic the goals of this research are also shared with most of the research mentioned i e being able to impact in thing as the prediction of the destination of oil spills and other potential pollutants dynamic of larvae migration seek and rescue operation connectivity indexes etc different than the described research however in this article we calculate the pathway probability from a transition matrix through markov dynamics instead of statistics calculated from the trajectories themselves it is a substantial difference since for the former the probability field at every spatial point is affected by the total field of information while for the later the pathway is the result of the information encountered by each particle during its trajectory the pathway and its probability is the product of global rather and local influence as is explained in what follows in this paper we propose a strategy for the global analysis of surface currents based on the ideas of dynamical systems analysis e g arrowsmith and place 1990 hsu 1987 and chaos e g hsu 1987 following hsu 1987 compactification i e obtaining a discrete finite set of working cells can naturally be achieved by the discretized domain of the circulation model in discussing markov chains mc and random walks it is useful to talk about the system state using the positions of moving particles as a reference karlin and taylor 1975 each grid of the model becomes a cell and the problem of 2d or 3d dispersion by the circulation model can be analyzed using classical mc theory the main contribution of the present investigation was the projection of these rather abstract markov chain concepts into concrete geophysical ideas such as domain of attraction area of influence and probable time of arrival associated with a given circulation pattern one significant contribution includes the definition of an efficient algorithm for the determination of the displacement probability among locations that is consistent with a given velocity field which has direct implications for connectivity studies some of these ideas were presented at american geophysical union 2014 and european geophysical union 2015 meetings pares sierra and flores morales 2014 2015 2 methods 2 1 markov chains first we present a short review of the characteristics of mc that are most relevant to the present work in its most basic form mc is a type of stochastic process whose characteristics at a given moment depend only on its state in the previous time let η n be the cell probability vector and η i n be defined as the probability of finding a particle in cell i at time n let p i j be the transition probability from cell j at time n to cell i at time n 1 p with elements p i j prob i at t n 1 j at t n is the transition probability matrix the global properties of an mc can be found by studying lim p i j n as n these properties are intimately related to the eigenvalues of the transition matrix p e g isaacson and madsen 1976 stewart 1994 however some important results can be extracted directly from the transition matrix if it is recast into its canonical or normal form the property of communicating among cells can be used to group them into disjoint groups two cells i j communicate if p i j m 0 and p j i n 0 for some m and n all of the persistent cells can be regrouped into isolated groups so that the cells within a group communicate only with each other i e p i j 0 if i and j belong to different groups the transition matrix can then be recast using a symmetric permutation of rows and columns 1 where p i and q are square matrices and t in general is a rectangular matrix once one of the cells of a p i group is reached the particle stays inside the group that is it is only redistributed among the cells of the group and does not leave these cells are called persistent cells in its canonical form p i e 1 it is easy to see that t represents the probability of transition from one of the transient cells to one of the persistent groups in one interval and q represents the probability of going from one transient cell to another transient cell then the probability of being absorbed into a persistent cell in one step is t in two steps it is t q q being the probability of remaining in the transient cell after the previous time step in three steps it is t q 2 and so on so that the probability of being absorbed at all for a given persistent group is 2 a t t q t q 2 t q 3 t i q q 2 q 3 t i q 1 a represents the probability of being absorbed and α i j is the probability of a transient cell j being absorbed into a persistent cell i similarly the expected time of absorption is γ i j n 1 n t q n 1 and can be expressed in matrix form hsu 1987 p 223 as 3 γ t 2 t q 3 t q 2 t i 2 q 3 q 2 t i q 2 a i q 1 our goal is to determine the global behavior of the system represented by the velocity fields and to extract this information efficiently essentially we need to calculate a and γ from t and q using equations 2 and 3 although the problem is reasonably well defined for a realistic application it is also a formidable task due to the sizes of the matrices involved fortunately more often than not we only need to calculate the one column of a and γ associated with a specifically defined cell or group of cells to study i e a virtual specifically defined persistent group the first step in translating the numerical ocean circulation problem into the mc system is to equate the numerical discretization of the domain of the problem with the group of cells that compose the system and to establish the probability of a particle moving from a given cell j to another cell i i e the transition matrix p i j to transform our numerical ocean circulation problem into the domain of the mc problem five steps need to be performed we next present these steps in some detail 2 2 generation of the cell space for single cell mapping scm to begin the basic divisions need definition first we assigned the natural at sea grid locations of the ocean model as our cells of interest while the rest i e land and areas outside our focus region as the complement starting from a given address we associated each grid incrementally to a numbered cell for example with model grid 1 2 1 2 as cell 1 2 3 1 2 would then be cell 2 in fig 3 the domain representing the gulf of california was divided into regular cells inside the gulf proper cells over land and cells outside the area of interest cells that fell outside the gulf but were still in the water were marked as one big cell 1 in the figure and cells over land were marked similarly as cell 2 cells from 3 to 10300 were the regular cells and corresponded to the domain of interest cells 1 and 2 were the complementary cells and were by default absorbing cells from the integration of the lagrangian equations using the velocities from the circulation model we generated the initial map that relates cell z to its image c z to find out where a particle in cell j goes in a unit of time this initial problem is deterministic each cell has only one image this single cell mapping scm hsu 1987 will help when constructing the actual stochastic generalized cell mapping gcm problem the image map constitutes the basic information that defines the system as a simple example fig 4 shows a hypothetical system and its image map the image in the center uses red arrows to show schematically where each cell leads as obtained by the lagrangian integration explained below at left in the figure is the corresponding table with the mapping information for each cell z to its image c z 2 3 finding the periodic and transient groups of the scm a periodic cell is one where z c n z that is after n steps it returns to itself for the first time when n 1 it is called an absorbing or sink cell the latter are found first since they are easy to discover by using the image map to find cells that have themselves as images i e z c z this process is performed moving forward sequentially for all cells in the cell space marking those with themselves for an image as absorbing cells as each absorbing cell is found all of the cells that lead to it are identified by moving backward through the mapping these transient cells are marked as belonging to that particular absorbing cell and constitute its domain of attraction once all of the absorbing cells and their corresponding domains of attraction have been found and marked other periodic cells are found using the unraveling algorithm presented by hsu 1987 and sketched in fig 4 for this a sequences or chain of cells is first constructed from the mapping starting at cell z 4 z c z c 2 z c 3 z c m z the algorithm searches for repetition among the analyzed cells starting from any cell not previously marked and following the thread until a repetition is found or the chain is completed see fig 4 once this occurs the period is recorded as the length of the loop in fig 4 and the entire ring is encoded as periodic once this part of the algorithm has finished the cell space can be divided into disjoint groups associated with the respective absorbing cells or periodic groups see fig 5 a at this point the scm has been completed each cell has been related to its absorbing attractor and the absorbing groups with their domains of influence and times of arrival have been established note that virtual absorbing cells can be defined as needed see fig 5 for example where a virtual absorbing cell is defined as the place where particles exit the domain through cells 41 and 42 bottom right for our gulf of california model we defined a virtual cell for the region existing outside the gulf fig 5b shows the time it takes a particle to leave the gulf for a particular circulation field i e for the transient cells to reach virtual cell 1 in fig 3 2 4 the markov process construction of the probability transition matrix for a realistic representation of the problem we needed to advance into the stochastic realm by allowing cells to have multiple images each one carrying a fraction of the total probability for the problem to be feasible some restrictions needed to be imposed first each cell must go somewhere with probability one including the possibility of staying where it is that is no cell can disappear from one time step to the next we used repetition of lagrangian integrations with a stochastic component to create the system in the form of a transition matrix the equations were integrated for a time t such that l was a few grid points in length and l c t where c represents a typical velocity of the model e g t 1 day the basic motion could then be calculated using a zero order model for example where the randomness is included in the position of the particle 5 d x i v i d t 2 σ d w i where σ is the rms of the velocity in the i direction and d w i is a random increment with a gaussian distribution that has a zero mean and unit variance i e a weiner process numerical stochastic methods need to be used to integrate equation 5 properly e g kloeden and platen 1992 more sophisticated ways of including the stochasticity could also be used for example it could be included in the velocity or acceleration terms first or second order stochastic models respectively for the short period required for the definition of the transition matrix in this application this would make a negligible difference lacasce 2008 indicates that these changes are sensible only if the temporal spacing of the data is larger than the lagrangian time scale after m r 20 50 repetitions of the lagrangian integration with a stochastic contribution the transition matrix can be constructed we empirically took the maximum number of repetitions that where computational reasonable after about 20 repeats the structure of the transition matrix and the final probabilities seem to stabilize we did not pretend to define a formal optimal maximum fig 6 shows a very simple hypothetical example red arrows indicate the direction of particle motion from the dot to the arrowhead while the table in the middle shows the corresponding image map table the first number of each row is that for the donating cell followed by the numbers corresponding to the accepting cells the middle table shows the probability of movement to the cells indicated in the first table the last column shows the number of images associated with each cell l z 1 l z m r note that each row in the table adds up to one i e each particle goes somewhere this table constitutes the transition matrix p 6 p i j prob i at n 1 j at n i j n s using this transition matrix the evolution of the system can be described as 7 z n 1 p z n where z is the state of the system and p is a n c n c matrix that contains all of the dynamical properties for the system through proper manipulation of p the global characteristics of the system can be extracted it is important to recognize that a cell has only a small number of images l z compared with the total number of cells n c in the system s so that the transition matrix is large but sparse taking advantage of this sparsity is essential for an efficient scheme 2 5 construction of the canonical matrix persistent and transient group for transformation to the canonical form all of the persistent cells have to be found as mentioned above in a persistent group all of the cells communicate among themselves but not with other persistent groups or with transient cells to identify periodic cells the cell space is searched for persistent groups however due to the following theorem by hsu 1987 not all of the cell space has to be searched one only needs to utilize the set of periodic cells from the scm p 253 hsu 1987 theorem 1 hsu 1987 theorem 11 3 1 if a given scm and a given gcm are compatible then any existing persistent group pg in the gcm contains at least one periodic solution of the scm the actual search is done starting from a periodic group of cells in the scm and expanding the set by including the images of all members if the expanded set includes a transient cell then the group is not persistent and it is marked as transient analysis then proceeds for the next periodic group once the set does not grow and does not include a transient cell a persistent group has been found the cells are marked as a persistent group and analysis proceeds for the next periodic group once all of the persistent cells have been found the canonical form is constructed by moving all of the columns and rows associated with each of the persistent cells to the top left of the matrix this reshuffling automatically discovers the t and q matrices from 2 and 3 the reordering is of course done virtually as no actual displacement takes place it is done at the sparse description label by changing the indices in one of the special schemes used to store and manipulate a sparse matrix i e csr or csc see for examples saad 2003 once all of the n a b s absorbing cells are moved in this way the virtual cell is placed next to the final absorbing cell that is the ad hoc defined virtual cell occupies the n a b s 1 column of the canonical transition matrix 2 6 solving the very large sparse linear systems the resulting canonical stochastic transition system is still quite large for example for the relatively small system associated with our model of the gulf of california represents a system of 10 k 10 k for our the gulf of mexico roms implementation with our normal setting of 4 km resolution a system of 200 k 200 k has to be solved twice for every time step an efficient iterative solver is an essential ingredient for this algorithm we used a generalized minimum residual method gmres which is based on projecting to the krylov subspace i e k a v span v a v a 2 v see for example saad 2003 van der vorst 2003 the strategy is to define a group of virtual absorbing cells for the area of interest column nabs 1 and solve not for the entire group of absorbing and periodic cells that define the problem but only for the ad hoc defined absorbing cells of interest postmultiplication of equation 2 by i q produces 8 a i q t and upon transposition of 8 we obtain 9 i q a t to solve for the absorbing cell associated with the virtual cell of interest we solve for the n a b s 1 column that is we solve for 10 i q α n a b s 1 i t n a b s 1 i using one of the methods for solving sparse linear systems mentioned above similarly from 3 for g n a b s 1 we solve 11 i q γ n a b s 1 i α n a b s 1 i finally the conditional expected absorption time for a transient cell i into the virtual cell j n a b s 1 is 12 ν n a b s 1 i γ n a b s 1 i α n a b s 1 i recasting of ν n a b s 1 i and α n a b s 1 i into their matrix form i e to the domain of the circulation model from its long vector representation concludes the algorithm 3 results in this section we present the results for two test cases using two different configurations of roms specifically we present data from our gulf of california and gulf of mexico roms implementations seeking to demonstrate the advantages of the method described above 3 1 gulf of california biologically the gulf of california gc is one of the most productive regions of the ocean mostly due to a variety of physical processes particularly the transport of subsurface nutrient rich waters to the euphotic zone see for example lluch cota et al 2007 lluch cota et al 2010 aquaculture and fishing activities in the gc contribute more than 70 of the mexican national fishing volume and more than 50 of its value pérez enriquez 2008 in particular shrimp farming is the most important aquaculture activity in mexico a large percentage of shrimp farms in mexico are located in the eastern gulf of california along the coasts of sonora and sinaloa in 2005 millions of dollars were lost by the industry due the sudden appearance of white spot virus syndrome svmb svmb is caused by a virus that infects and kills shrimp pérez enriquez 2008 extensive work has been done to combat this disease part of the effort has included the implementation of the multidisciplinary industrial academic aeri conacyt fordecyt project to analyze and eradicate svmb motivated by the need to understand physical connectivity between the different infection sites and the manner of viral spread we implemented the method described above on the outputs of our roms model for the gulf of california in fig 7 we present the time of arrival from some of the stations displayed in fig 7 huatabampo bahía lobos and bahía kino to any part of the gulf we show results for the period march through may of 2005 fig 7 when the svmb outbreak first appeared other months and years were also analyzed but are not presented here as shown in fig 7 a d and g in march all three stations could have contributed to the contamination of any northward station as can be seen in the images an uninterrupted coastal current that flows northeasterly advected water all along the eastern coast for bahía kino water from near the station could even reach the northern gulf past the large island region fig 7a during april the destination of water parcels from each station was controlled by large eddies in the middle of the gulf it is evident for example that bahía kino and bahía lobos were influenced by an anticyclonic eddy located near 111 5w 28n the eastern side of this gyre advected water and probably the virus with it just south of bahía kino and then to the west for bahía lobos especially water would be taken directly west under the influence of this gyre and a second cyclonic gyre just to the south fig 7e for huatabampo matter was carried mostly north for a few days and then to the west again under the influence of the cyclonic eddy for the three months presented huatabambo would have impacted the waters of the small open bay where it sits it would for example undoubtedly have contaminated the water of ahome on the southern bay for may the stations communicated only with regions very close to them that is the probability of an oceanic virus infection from the other stations was small it seems that the gyres and coastal currents were not present or not near enough to the coast and the dispersion was therefore limited the water around the stations therefore remained near the station of origin it is evident that the influence of the gyral circulation is one of the most important components affecting the origin and fate of matter in the gulf models that cannot reproduce these eddies cannot reproduce the distribution of matter within the gulf of california nor its connectivity 3 2 gulf of mexico as mentioned in the introduction we used an implementation of roms in a meta study of the gulf of mexico our modeling efforts constituted a small part of a global multi disciplinary study that was initially motivated by the british pacific oil spill disaster of 2010 one of the principle motivations was to develop the ability to predict the trajectories and destinations of potential oil spills the tool developed here partly fulfills this need by allowing identification of the areas that influence an arbitrary region fig 8 shows the time it would take to arrive at an arbitrary point on the eastern side of the gulf of mexico 85w 25n under the influence of the circulation pattern for 1 nov 2013 as given by roms the point and date were both selected arbitrarily this figure shows not only the probable origins of the water but also the relevance of each region in the form of the amount of time required to influence the point of interest it shows for example that at this particular time most of the contribution came straight from the north while some came from the south through an anticyclonic loop that originated in the caribbean and flowed through the yucatan channel it also shows some influence from a large eddy to the west at about 94w 26n the absorbing cell does not have to be a point it could also be a region fig 9 for example shows the areas that impact the florida channel at four different times which were chosen to coincide with formation stages of the loop current eddy oct 2012 nov 2013 apr 2014 and may 2014 for fig 9 a b c and d respectively for this representation the florida channel fc green bar in the figure was chosen as the absorbing cell and colors indicate the probable arrival time from the surrounding area fig 9 shows that the origin of the water that will pass thru the fc comes from different places as the formation evolves in the first panel fig 9a there is a well developed loop and a separating secondary gyre at its northern end this configuration makes the water that passes through the fc come from far within the gulf and the area of influence extends in a northwesterly direction most of the water still comes through the yucatan channel but after first going around the loop there is a shadow area that requires more time to reach the channel west of florida one month later fig 9b the gyre at the extreme of the loop has separated and the main gyre is already formed but still attached to the loop current water contributing to the absorbing cell comes mostly from the northwest of the channel at this time by april of the following year fig 9c a well developed eddy was almost ready to separate the area of influence extended more westward until almost reaching the yucatan channel where it turns sharply to a northerly direction following the eastern section of the eddy the shadowy area west of florida also persisted by may the eddy had separated with water coming almost directly from the caribbean the flow moves rapidly from the yucatan channel to the fc fig 9d an important aspect of this representation is the ability to include arrival time from a source in addition to the source location 4 discussion and summary a new algorithm for the global analysis of ocean circulation has been presented global information in the form of vector fields is integrated using markovian statistics into an efficient measure of dispersion that incorporates time and space we described the technical steps to efficiently compute the connection between different areas of a region compatible with a given velocity field this measure of connectivity is given in term of the probability and most probable time it would take a particle in one place to reach any other point within the analyzed field two tests cases were presented in the first case the probability of virus contamination in shrimp farms was analyzed the appearance of the white spot virus syndrome in shrimp farms of the gulf of california motivated the study of connectivity in the gulf the algorithm presented in this article was implemented using simulated ocean circulation from roms to analyze the possibilities of autocontamination through an oceanic vector from other farms in the gulf the analysis was useful in understanding the timing and evaluation of the phenomena and in discriminating between the probability of autocontamination versus contamination from wild viruses or by land from sharing equipment among farms for example a second case was presented in which the connectivity in the gulf of mexico was analyzed seeing the influence of circulation patterns on a given point in the gulf immediately gives a sense of how the circulation is distributing matter as water parcels throughout the gulf a strong connection from afar was shown to add more than a weak connection nearby it also gave a quantitative measure of connection in the form of days to arrive at a selected location patterns of detaching gulf rings were presented from the analysis it was interesting to note that immediately following the detachment of a large anticyclonic ring water from the caribbean flowed directly to the atlantic it is important to emphasize that the method not only helped in the visualization of structures and patterns but also and maybe more importantly it helped in elucidating circulation dynamics for example in the case of the gulf of california the most important feature in the circulation was shown to be the existence or absence of gyres in the gulf this observation helped not only in identifying the presence of gyres but in establishing that they are the dominant feature this allowed for a correct description of connectivity in the gulf a feature that had been poorly analyzed in the past also of importance is that the data used for the presented examples came from circulation models i e roms but the data can equally well be observed data geostrophic current for example similarly the data used was two dimensional but nothing intrinsic other than the size of the problem precludes the used of three dimensional data the practical strength of the method is based on the fact that all information is combined by way of standard markovian methods that bypass the costly direct calculation of the transition matrix eigenvector using instead efficient reordering algorithms that allow for the use of a much faster linear solving technique for sparse matrices the algorithm has immediate geophysical applications as shown by the two examples presented an important component of the presented algorithm is the kind of random forcing used in defining the synthetic lagrangian trajectories for this work we use the simplest one adding the randomness to the position of the particle as mention by lacasce 2008 this is probably an overly simplistic assumption that would lead to somehow incorrect long term dispersion characteristics of the synthetic particles for example the zero order model implies a linear dispersion missing the quadratic grow expected to the initial times other possibilities could be used including randomness to the velocity or the acceleration data the impact of these choices though for the short time needed to define the transition matrix 1 day is probably not too severe also the structures of the algorithm and its usefulness wouldn t change however this is an important issue that needs and will be investigated further in future work acknowledgement research funded by the national council of science and technology mexico mexican ministry of energy hydrocarbon trust project 201441 this is a contribution of the gulf of mexico research consortium cigom 
26413,we propose a method for analyzing ocean currents using a statistical approach the proposed technique is useful for analyzing global velocity fields and producing indices to describe the probable trajectories and destinations of particles embedded in such fields short term lagrangian integration of the velocities was used to generate transition matrices that define the system locally a reshuffling algorithm based on standard markov chain theory was implemented to mix and synthesize the information involved in the global analysis iterative methods were then used to solve the resulting large and sparse linear systems the method efficiently used local information short term lagrangian integration to infer global characteristics of the system two case studies were presented to emphasize the merits of the described scheme one using modeled data from the gulf of california and another from the gulf of mexico keywords connectivity markov chain ocean current ocean models probable trajectories probable arrival time 1 introduction 1 1 models physical oceanographers are increasingly being called upon to determine the fate of freely moving matter in the ocean one goal for example is to identify areas of influence or domains of attraction consistent with a given pattern of motion these features have been extensively studied in the context of dynamic systems e g bergé et al 1984 lichtenberg and lieberman 1983 for fields associated with geophysical systems being able to statistically determine the destinations or origins of water parcels embedded in ocean currents could be even more important the applications associated with these capabilities are without a doubt significant e g in determining connectivity larvae dispersal contingency approaches for pollutant spills deployment for search and rescue and strategies for cleaning marine areas one of the ways particle movement has been addressed is by studying lagrangian trajectories e g marinone et al 2008 santiago garcía et al 2014 and extracting the appropriate statistics e g lilly 1972a b lacasce 2008 numerical hydrodynamical models are now standard tools for the analysis of ocean phenomena fig 1 for example shows the surface circulation for the gulf of mexico using an implementation of the regional ocean modeling system roms the figure shows one snapshot of the surface velocity taken from an operational system at the center for research and higher education of ensenada cicese the model runs daily and is forced at the surface using global forecast system gfs data gfs 2017 and at the boundaries using mercator ocean forecast mercator data mercator 2017 well known features of the circulation in the gulf of mexico are evident in the figure there is a strong current the loop current that flows around cuba from the caribbean to the atlantic feeding the gulf stream a well defined gyre induced by an earlier loop current detachment is also evident the reproduced gyre and its subsequent westward motion is in agreement with previously reported observations e g sturges and lugo fernandez 2005 oey et al 2005 in fig 2 a and b we present a series of trajectories calculated using this surface circulation lagrangian equations were integrated off line using a fourth order milne hamming predictor corrector scheme one thousand inert particles were released at a single point in the northern region of the gulf and followed for 120 days fig 2a shows the progression of the particles while fig 2b shows a few arbitrary trajectories for the same period although much information is inferable from these data and numerous articles have used lagrangian trajectories as the basis for the description of the integrated characteristics of numerical circulation models e g santiago garcía et al 2014 montaño cortes et al 2017 often these spaghetti diagrams are difficult to interpret and synthesize lacasce 2008 presents an excellent review of statistical analyses from oceanic lagrangian data it is evident for example that in the case of fig 2 an average direction is not clearly defined which is a reflection of the turbulent nature of the trajectories and rapid memory loss by the process a statistically more robust description of fig 2 would be very useful it is possible for example to deduce from these figures that three main regions are transited by the particles a large percentage stay near the area of release and get trapped by the large eddy in the center of the gulf while some particles go straight to the west along the northern coast then turning south at the west coast and a few others leave the gulf from the florida cuba channel of course a more precise and systematic definition of a large percentage tend to stay and a few particles would be very useful and is the primary objective of this work one strategy that has been used to synthesize such information involves the use of connectivity matrices these matrices lump together information in a rough way however and often make analysis difficult e g marinone et al 2008 montaño cortes et al 2017 the ability to track drifters as a tool for understanding dispersion in the ocean is a scientific endeavor that has been active for some time and is well represented in the environmental modeling software journal ems a light search of ems for the pasts years shows a substantial amount of articles that similar to this one present tools for finding and better understanding the dispersion of water parcels or particles in the ocean using statistics calculated from virtual drifters for most of these articles the primary goal is to develop a robust way of describing how the water circulation dictates the dispersal of things e g larvae pollutant oil droplets etc in all of these articles the drift trajectory is calculated similarly a deterministic background provided by a model analytical or numerical plus a random part to represent the dispersion the specific details are of course different with different circulation models being used i e roms hycom delf3d etc and a random part integrated using different stochastic schemes the main thrust for these studies has probably been predicting the fate of oil spills e g boufadel et al 2007 otero et al 2015 sayol et al 2014 but other motivations are also well represented the study of larvae dispersion e g kool and nichol 2015 qin et al 2017 physical and ecological connectivity e g paris et al 2013 kool and nichol 2015 analysis of pollution drifts e g suh 2006 qin et al 2017 assessment of ecological risks e g otero et al 2015 qin et al 2017 seek and rescue operation e g sayol et al 2014 or understanding the dynamics of the circulation e g suh 2006 furnans et al 2008 suh 2006 used a hybrid approach to predict the dispersion of contaminants individual particles dispersed by a combination of a deterministic component and a stochastic one associated to a monte carlo scheme suh 2006 augment the lagrangian equation by a buoyancy effect that causes additional horizontal dispersion for a test of the feasibility of the method using a simple system with an analytical solution shu founds that it gives good results near the source where the gradients are large but that it is not suitable for large distances from it for analyzing the transport of oil droplets on the water surface due to waves boufadel et al 2007 used a lagrangian equation to integrate the position of particles the velocity was composed again of two parts a random component associated to turbulence and an analytical theoretical one associated to wave motion to simulate the oil part of the droplets they also included a rising velocity due to buoyancy and found that strokes drift contributed greatly to the horizontal transport using a monte carlo approach on a simple domain they investigated the effect of the wave parameter i e steepness on their capacity of dispersion they showed that turbulence and buoyancy work in opposing direction enhancing and limiting respectively the vertical transport of the plume while stokes drift was the major mechanism for horizontal transport furnans et al 2008 investigated the influence of inertia and drag on the simulation of real drifter movement they incorporated the effect of wind and inertia in determining virtual particles movement they improved the velocity provided by the hydrodynamic model elcom incorporating winds and a component that parametrized inertia drag and slip furnans et al 2008 provide a quantitative assessment of the merits of their implementation by comparing the motion of modeled versus real drifters they found that their method improves the simulation coupling a probabilistic individual based model ibm into a series of nested hydrographic models e g hycom roms etc paris et al 2013 developed a probabilistic connectivity model to study the mechanical reasons of dispersion and migration processes in the ocean the hydrographic model provides the basic motion of individual particles while each particle is given a biological attribute their model estimates the probability of population connectivity in the form of connectivity matrix and trajectory of individual particles complex larvae migration dispersion and retention as well as the dispersion of abiotic particles such as pollutants can be assessed by their software an operational model for tracking surface objects in the ocean was presented by sayol et al 2014 the eulerian field was provided by an operational implementation of roms i e western mediterranean operational model a random walk term is added to simulate diffusivity the authors computed the probability density functions by averaging the trajectory of an ensemble of particles forecast for the final position of particles is given in the form of accumulated probability density contours this research is purportedly directed to investigate oil spill trajectories and search and rescue operations kool and nichol 2015 simulated the 3d movement of marine larvae by advecting individual particles through a velocity field and adding a random component representing diffusion each particle is analyzed at regular intervals and moved accordingly to the ambient velocity i e hycom and simulated diffusion fields and endowed with biological characteristics e g mortality is implemented by an exponential or weibull distribution probability their research provides a platform for the study of 3d connectivity in the form of connectivity matrixes and maps of probability surfaces for case study kool and nichol 2015 analyzed larval dispersion in the western australian coast otero et al 2015 calculates the drift pathway of simulated particles using advection transport by surface currents from an operational configuration of roms and winds from the wrf model plus turbulent diffusion the diffusivity velocity depends on the sea turbulent characteristics simulated as brownian motion of particles utilizing a random walk process a wind drag coefficient is also added in case of oil spill studies they evaluated the virtue of their application comparing their result with the trajectory of the prestige oil spill accident of november 2002 montero et al 2003 authors mentioned other possible applications of their work as the study of harmful algae populations qin et al 2017 used a turbulent diffusive component i e fractional brownian model combined with an advection component provided by ocean current and wind from delf3d and wrf respectively to calculate pollutant drift for each particle which represents a certain amount of simulated substance their model solves a lagrangian equation with the velocities given by the ocean and atmospheric models or constant velocities and a stochastic part added to the position of each particle to show the efficiency and applicability of their approach the authors present a case study using data from the gulf of finland they simulated the dispersion of algae bloom red tide and found that the non fickian diffusion i e fractional brownian motion better represent the shape of the trajectories their procedure gives a quick and efficient way to provide for the trajectories of surface drifting pollutant to decision makers the present article follows the on going dialog in ems summarized in the previous paragraphs on ways to represent the pathways of virtual particles embedded in a given velocity field similar to the research mentioned these pathways are calculated using a deterministic velocity field i e from roms augmented with a random component that simulates dispersion the methods presented in this article can be considered as a continuation of this topic the goals of this research are also shared with most of the research mentioned i e being able to impact in thing as the prediction of the destination of oil spills and other potential pollutants dynamic of larvae migration seek and rescue operation connectivity indexes etc different than the described research however in this article we calculate the pathway probability from a transition matrix through markov dynamics instead of statistics calculated from the trajectories themselves it is a substantial difference since for the former the probability field at every spatial point is affected by the total field of information while for the later the pathway is the result of the information encountered by each particle during its trajectory the pathway and its probability is the product of global rather and local influence as is explained in what follows in this paper we propose a strategy for the global analysis of surface currents based on the ideas of dynamical systems analysis e g arrowsmith and place 1990 hsu 1987 and chaos e g hsu 1987 following hsu 1987 compactification i e obtaining a discrete finite set of working cells can naturally be achieved by the discretized domain of the circulation model in discussing markov chains mc and random walks it is useful to talk about the system state using the positions of moving particles as a reference karlin and taylor 1975 each grid of the model becomes a cell and the problem of 2d or 3d dispersion by the circulation model can be analyzed using classical mc theory the main contribution of the present investigation was the projection of these rather abstract markov chain concepts into concrete geophysical ideas such as domain of attraction area of influence and probable time of arrival associated with a given circulation pattern one significant contribution includes the definition of an efficient algorithm for the determination of the displacement probability among locations that is consistent with a given velocity field which has direct implications for connectivity studies some of these ideas were presented at american geophysical union 2014 and european geophysical union 2015 meetings pares sierra and flores morales 2014 2015 2 methods 2 1 markov chains first we present a short review of the characteristics of mc that are most relevant to the present work in its most basic form mc is a type of stochastic process whose characteristics at a given moment depend only on its state in the previous time let η n be the cell probability vector and η i n be defined as the probability of finding a particle in cell i at time n let p i j be the transition probability from cell j at time n to cell i at time n 1 p with elements p i j prob i at t n 1 j at t n is the transition probability matrix the global properties of an mc can be found by studying lim p i j n as n these properties are intimately related to the eigenvalues of the transition matrix p e g isaacson and madsen 1976 stewart 1994 however some important results can be extracted directly from the transition matrix if it is recast into its canonical or normal form the property of communicating among cells can be used to group them into disjoint groups two cells i j communicate if p i j m 0 and p j i n 0 for some m and n all of the persistent cells can be regrouped into isolated groups so that the cells within a group communicate only with each other i e p i j 0 if i and j belong to different groups the transition matrix can then be recast using a symmetric permutation of rows and columns 1 where p i and q are square matrices and t in general is a rectangular matrix once one of the cells of a p i group is reached the particle stays inside the group that is it is only redistributed among the cells of the group and does not leave these cells are called persistent cells in its canonical form p i e 1 it is easy to see that t represents the probability of transition from one of the transient cells to one of the persistent groups in one interval and q represents the probability of going from one transient cell to another transient cell then the probability of being absorbed into a persistent cell in one step is t in two steps it is t q q being the probability of remaining in the transient cell after the previous time step in three steps it is t q 2 and so on so that the probability of being absorbed at all for a given persistent group is 2 a t t q t q 2 t q 3 t i q q 2 q 3 t i q 1 a represents the probability of being absorbed and α i j is the probability of a transient cell j being absorbed into a persistent cell i similarly the expected time of absorption is γ i j n 1 n t q n 1 and can be expressed in matrix form hsu 1987 p 223 as 3 γ t 2 t q 3 t q 2 t i 2 q 3 q 2 t i q 2 a i q 1 our goal is to determine the global behavior of the system represented by the velocity fields and to extract this information efficiently essentially we need to calculate a and γ from t and q using equations 2 and 3 although the problem is reasonably well defined for a realistic application it is also a formidable task due to the sizes of the matrices involved fortunately more often than not we only need to calculate the one column of a and γ associated with a specifically defined cell or group of cells to study i e a virtual specifically defined persistent group the first step in translating the numerical ocean circulation problem into the mc system is to equate the numerical discretization of the domain of the problem with the group of cells that compose the system and to establish the probability of a particle moving from a given cell j to another cell i i e the transition matrix p i j to transform our numerical ocean circulation problem into the domain of the mc problem five steps need to be performed we next present these steps in some detail 2 2 generation of the cell space for single cell mapping scm to begin the basic divisions need definition first we assigned the natural at sea grid locations of the ocean model as our cells of interest while the rest i e land and areas outside our focus region as the complement starting from a given address we associated each grid incrementally to a numbered cell for example with model grid 1 2 1 2 as cell 1 2 3 1 2 would then be cell 2 in fig 3 the domain representing the gulf of california was divided into regular cells inside the gulf proper cells over land and cells outside the area of interest cells that fell outside the gulf but were still in the water were marked as one big cell 1 in the figure and cells over land were marked similarly as cell 2 cells from 3 to 10300 were the regular cells and corresponded to the domain of interest cells 1 and 2 were the complementary cells and were by default absorbing cells from the integration of the lagrangian equations using the velocities from the circulation model we generated the initial map that relates cell z to its image c z to find out where a particle in cell j goes in a unit of time this initial problem is deterministic each cell has only one image this single cell mapping scm hsu 1987 will help when constructing the actual stochastic generalized cell mapping gcm problem the image map constitutes the basic information that defines the system as a simple example fig 4 shows a hypothetical system and its image map the image in the center uses red arrows to show schematically where each cell leads as obtained by the lagrangian integration explained below at left in the figure is the corresponding table with the mapping information for each cell z to its image c z 2 3 finding the periodic and transient groups of the scm a periodic cell is one where z c n z that is after n steps it returns to itself for the first time when n 1 it is called an absorbing or sink cell the latter are found first since they are easy to discover by using the image map to find cells that have themselves as images i e z c z this process is performed moving forward sequentially for all cells in the cell space marking those with themselves for an image as absorbing cells as each absorbing cell is found all of the cells that lead to it are identified by moving backward through the mapping these transient cells are marked as belonging to that particular absorbing cell and constitute its domain of attraction once all of the absorbing cells and their corresponding domains of attraction have been found and marked other periodic cells are found using the unraveling algorithm presented by hsu 1987 and sketched in fig 4 for this a sequences or chain of cells is first constructed from the mapping starting at cell z 4 z c z c 2 z c 3 z c m z the algorithm searches for repetition among the analyzed cells starting from any cell not previously marked and following the thread until a repetition is found or the chain is completed see fig 4 once this occurs the period is recorded as the length of the loop in fig 4 and the entire ring is encoded as periodic once this part of the algorithm has finished the cell space can be divided into disjoint groups associated with the respective absorbing cells or periodic groups see fig 5 a at this point the scm has been completed each cell has been related to its absorbing attractor and the absorbing groups with their domains of influence and times of arrival have been established note that virtual absorbing cells can be defined as needed see fig 5 for example where a virtual absorbing cell is defined as the place where particles exit the domain through cells 41 and 42 bottom right for our gulf of california model we defined a virtual cell for the region existing outside the gulf fig 5b shows the time it takes a particle to leave the gulf for a particular circulation field i e for the transient cells to reach virtual cell 1 in fig 3 2 4 the markov process construction of the probability transition matrix for a realistic representation of the problem we needed to advance into the stochastic realm by allowing cells to have multiple images each one carrying a fraction of the total probability for the problem to be feasible some restrictions needed to be imposed first each cell must go somewhere with probability one including the possibility of staying where it is that is no cell can disappear from one time step to the next we used repetition of lagrangian integrations with a stochastic component to create the system in the form of a transition matrix the equations were integrated for a time t such that l was a few grid points in length and l c t where c represents a typical velocity of the model e g t 1 day the basic motion could then be calculated using a zero order model for example where the randomness is included in the position of the particle 5 d x i v i d t 2 σ d w i where σ is the rms of the velocity in the i direction and d w i is a random increment with a gaussian distribution that has a zero mean and unit variance i e a weiner process numerical stochastic methods need to be used to integrate equation 5 properly e g kloeden and platen 1992 more sophisticated ways of including the stochasticity could also be used for example it could be included in the velocity or acceleration terms first or second order stochastic models respectively for the short period required for the definition of the transition matrix in this application this would make a negligible difference lacasce 2008 indicates that these changes are sensible only if the temporal spacing of the data is larger than the lagrangian time scale after m r 20 50 repetitions of the lagrangian integration with a stochastic contribution the transition matrix can be constructed we empirically took the maximum number of repetitions that where computational reasonable after about 20 repeats the structure of the transition matrix and the final probabilities seem to stabilize we did not pretend to define a formal optimal maximum fig 6 shows a very simple hypothetical example red arrows indicate the direction of particle motion from the dot to the arrowhead while the table in the middle shows the corresponding image map table the first number of each row is that for the donating cell followed by the numbers corresponding to the accepting cells the middle table shows the probability of movement to the cells indicated in the first table the last column shows the number of images associated with each cell l z 1 l z m r note that each row in the table adds up to one i e each particle goes somewhere this table constitutes the transition matrix p 6 p i j prob i at n 1 j at n i j n s using this transition matrix the evolution of the system can be described as 7 z n 1 p z n where z is the state of the system and p is a n c n c matrix that contains all of the dynamical properties for the system through proper manipulation of p the global characteristics of the system can be extracted it is important to recognize that a cell has only a small number of images l z compared with the total number of cells n c in the system s so that the transition matrix is large but sparse taking advantage of this sparsity is essential for an efficient scheme 2 5 construction of the canonical matrix persistent and transient group for transformation to the canonical form all of the persistent cells have to be found as mentioned above in a persistent group all of the cells communicate among themselves but not with other persistent groups or with transient cells to identify periodic cells the cell space is searched for persistent groups however due to the following theorem by hsu 1987 not all of the cell space has to be searched one only needs to utilize the set of periodic cells from the scm p 253 hsu 1987 theorem 1 hsu 1987 theorem 11 3 1 if a given scm and a given gcm are compatible then any existing persistent group pg in the gcm contains at least one periodic solution of the scm the actual search is done starting from a periodic group of cells in the scm and expanding the set by including the images of all members if the expanded set includes a transient cell then the group is not persistent and it is marked as transient analysis then proceeds for the next periodic group once the set does not grow and does not include a transient cell a persistent group has been found the cells are marked as a persistent group and analysis proceeds for the next periodic group once all of the persistent cells have been found the canonical form is constructed by moving all of the columns and rows associated with each of the persistent cells to the top left of the matrix this reshuffling automatically discovers the t and q matrices from 2 and 3 the reordering is of course done virtually as no actual displacement takes place it is done at the sparse description label by changing the indices in one of the special schemes used to store and manipulate a sparse matrix i e csr or csc see for examples saad 2003 once all of the n a b s absorbing cells are moved in this way the virtual cell is placed next to the final absorbing cell that is the ad hoc defined virtual cell occupies the n a b s 1 column of the canonical transition matrix 2 6 solving the very large sparse linear systems the resulting canonical stochastic transition system is still quite large for example for the relatively small system associated with our model of the gulf of california represents a system of 10 k 10 k for our the gulf of mexico roms implementation with our normal setting of 4 km resolution a system of 200 k 200 k has to be solved twice for every time step an efficient iterative solver is an essential ingredient for this algorithm we used a generalized minimum residual method gmres which is based on projecting to the krylov subspace i e k a v span v a v a 2 v see for example saad 2003 van der vorst 2003 the strategy is to define a group of virtual absorbing cells for the area of interest column nabs 1 and solve not for the entire group of absorbing and periodic cells that define the problem but only for the ad hoc defined absorbing cells of interest postmultiplication of equation 2 by i q produces 8 a i q t and upon transposition of 8 we obtain 9 i q a t to solve for the absorbing cell associated with the virtual cell of interest we solve for the n a b s 1 column that is we solve for 10 i q α n a b s 1 i t n a b s 1 i using one of the methods for solving sparse linear systems mentioned above similarly from 3 for g n a b s 1 we solve 11 i q γ n a b s 1 i α n a b s 1 i finally the conditional expected absorption time for a transient cell i into the virtual cell j n a b s 1 is 12 ν n a b s 1 i γ n a b s 1 i α n a b s 1 i recasting of ν n a b s 1 i and α n a b s 1 i into their matrix form i e to the domain of the circulation model from its long vector representation concludes the algorithm 3 results in this section we present the results for two test cases using two different configurations of roms specifically we present data from our gulf of california and gulf of mexico roms implementations seeking to demonstrate the advantages of the method described above 3 1 gulf of california biologically the gulf of california gc is one of the most productive regions of the ocean mostly due to a variety of physical processes particularly the transport of subsurface nutrient rich waters to the euphotic zone see for example lluch cota et al 2007 lluch cota et al 2010 aquaculture and fishing activities in the gc contribute more than 70 of the mexican national fishing volume and more than 50 of its value pérez enriquez 2008 in particular shrimp farming is the most important aquaculture activity in mexico a large percentage of shrimp farms in mexico are located in the eastern gulf of california along the coasts of sonora and sinaloa in 2005 millions of dollars were lost by the industry due the sudden appearance of white spot virus syndrome svmb svmb is caused by a virus that infects and kills shrimp pérez enriquez 2008 extensive work has been done to combat this disease part of the effort has included the implementation of the multidisciplinary industrial academic aeri conacyt fordecyt project to analyze and eradicate svmb motivated by the need to understand physical connectivity between the different infection sites and the manner of viral spread we implemented the method described above on the outputs of our roms model for the gulf of california in fig 7 we present the time of arrival from some of the stations displayed in fig 7 huatabampo bahía lobos and bahía kino to any part of the gulf we show results for the period march through may of 2005 fig 7 when the svmb outbreak first appeared other months and years were also analyzed but are not presented here as shown in fig 7 a d and g in march all three stations could have contributed to the contamination of any northward station as can be seen in the images an uninterrupted coastal current that flows northeasterly advected water all along the eastern coast for bahía kino water from near the station could even reach the northern gulf past the large island region fig 7a during april the destination of water parcels from each station was controlled by large eddies in the middle of the gulf it is evident for example that bahía kino and bahía lobos were influenced by an anticyclonic eddy located near 111 5w 28n the eastern side of this gyre advected water and probably the virus with it just south of bahía kino and then to the west for bahía lobos especially water would be taken directly west under the influence of this gyre and a second cyclonic gyre just to the south fig 7e for huatabampo matter was carried mostly north for a few days and then to the west again under the influence of the cyclonic eddy for the three months presented huatabambo would have impacted the waters of the small open bay where it sits it would for example undoubtedly have contaminated the water of ahome on the southern bay for may the stations communicated only with regions very close to them that is the probability of an oceanic virus infection from the other stations was small it seems that the gyres and coastal currents were not present or not near enough to the coast and the dispersion was therefore limited the water around the stations therefore remained near the station of origin it is evident that the influence of the gyral circulation is one of the most important components affecting the origin and fate of matter in the gulf models that cannot reproduce these eddies cannot reproduce the distribution of matter within the gulf of california nor its connectivity 3 2 gulf of mexico as mentioned in the introduction we used an implementation of roms in a meta study of the gulf of mexico our modeling efforts constituted a small part of a global multi disciplinary study that was initially motivated by the british pacific oil spill disaster of 2010 one of the principle motivations was to develop the ability to predict the trajectories and destinations of potential oil spills the tool developed here partly fulfills this need by allowing identification of the areas that influence an arbitrary region fig 8 shows the time it would take to arrive at an arbitrary point on the eastern side of the gulf of mexico 85w 25n under the influence of the circulation pattern for 1 nov 2013 as given by roms the point and date were both selected arbitrarily this figure shows not only the probable origins of the water but also the relevance of each region in the form of the amount of time required to influence the point of interest it shows for example that at this particular time most of the contribution came straight from the north while some came from the south through an anticyclonic loop that originated in the caribbean and flowed through the yucatan channel it also shows some influence from a large eddy to the west at about 94w 26n the absorbing cell does not have to be a point it could also be a region fig 9 for example shows the areas that impact the florida channel at four different times which were chosen to coincide with formation stages of the loop current eddy oct 2012 nov 2013 apr 2014 and may 2014 for fig 9 a b c and d respectively for this representation the florida channel fc green bar in the figure was chosen as the absorbing cell and colors indicate the probable arrival time from the surrounding area fig 9 shows that the origin of the water that will pass thru the fc comes from different places as the formation evolves in the first panel fig 9a there is a well developed loop and a separating secondary gyre at its northern end this configuration makes the water that passes through the fc come from far within the gulf and the area of influence extends in a northwesterly direction most of the water still comes through the yucatan channel but after first going around the loop there is a shadow area that requires more time to reach the channel west of florida one month later fig 9b the gyre at the extreme of the loop has separated and the main gyre is already formed but still attached to the loop current water contributing to the absorbing cell comes mostly from the northwest of the channel at this time by april of the following year fig 9c a well developed eddy was almost ready to separate the area of influence extended more westward until almost reaching the yucatan channel where it turns sharply to a northerly direction following the eastern section of the eddy the shadowy area west of florida also persisted by may the eddy had separated with water coming almost directly from the caribbean the flow moves rapidly from the yucatan channel to the fc fig 9d an important aspect of this representation is the ability to include arrival time from a source in addition to the source location 4 discussion and summary a new algorithm for the global analysis of ocean circulation has been presented global information in the form of vector fields is integrated using markovian statistics into an efficient measure of dispersion that incorporates time and space we described the technical steps to efficiently compute the connection between different areas of a region compatible with a given velocity field this measure of connectivity is given in term of the probability and most probable time it would take a particle in one place to reach any other point within the analyzed field two tests cases were presented in the first case the probability of virus contamination in shrimp farms was analyzed the appearance of the white spot virus syndrome in shrimp farms of the gulf of california motivated the study of connectivity in the gulf the algorithm presented in this article was implemented using simulated ocean circulation from roms to analyze the possibilities of autocontamination through an oceanic vector from other farms in the gulf the analysis was useful in understanding the timing and evaluation of the phenomena and in discriminating between the probability of autocontamination versus contamination from wild viruses or by land from sharing equipment among farms for example a second case was presented in which the connectivity in the gulf of mexico was analyzed seeing the influence of circulation patterns on a given point in the gulf immediately gives a sense of how the circulation is distributing matter as water parcels throughout the gulf a strong connection from afar was shown to add more than a weak connection nearby it also gave a quantitative measure of connection in the form of days to arrive at a selected location patterns of detaching gulf rings were presented from the analysis it was interesting to note that immediately following the detachment of a large anticyclonic ring water from the caribbean flowed directly to the atlantic it is important to emphasize that the method not only helped in the visualization of structures and patterns but also and maybe more importantly it helped in elucidating circulation dynamics for example in the case of the gulf of california the most important feature in the circulation was shown to be the existence or absence of gyres in the gulf this observation helped not only in identifying the presence of gyres but in establishing that they are the dominant feature this allowed for a correct description of connectivity in the gulf a feature that had been poorly analyzed in the past also of importance is that the data used for the presented examples came from circulation models i e roms but the data can equally well be observed data geostrophic current for example similarly the data used was two dimensional but nothing intrinsic other than the size of the problem precludes the used of three dimensional data the practical strength of the method is based on the fact that all information is combined by way of standard markovian methods that bypass the costly direct calculation of the transition matrix eigenvector using instead efficient reordering algorithms that allow for the use of a much faster linear solving technique for sparse matrices the algorithm has immediate geophysical applications as shown by the two examples presented an important component of the presented algorithm is the kind of random forcing used in defining the synthetic lagrangian trajectories for this work we use the simplest one adding the randomness to the position of the particle as mention by lacasce 2008 this is probably an overly simplistic assumption that would lead to somehow incorrect long term dispersion characteristics of the synthetic particles for example the zero order model implies a linear dispersion missing the quadratic grow expected to the initial times other possibilities could be used including randomness to the velocity or the acceleration data the impact of these choices though for the short time needed to define the transition matrix 1 day is probably not too severe also the structures of the algorithm and its usefulness wouldn t change however this is an important issue that needs and will be investigated further in future work acknowledgement research funded by the national council of science and technology mexico mexican ministry of energy hydrocarbon trust project 201441 this is a contribution of the gulf of mexico research consortium cigom 
26414,east african wetlands are hotspots of ecosystem services particularly for climate regulation water provision and food production we review the ability of current approaches to ecosystem service assessments to capture important social ecological dynamics to provide insight for wetland management and human wellbeing we synthesise evidence of human influences on wetlands and gauge the suitability of models and tools for simulating spatial and temporal dynamics and land management on multiple ecosystem functions and services current approaches are largely unsuitable for advancing knowledge of social ecological system dynamics and could be greatly improved with inter disciplinary model integration to focus upon interactions between multiple ecosystem functions and services modelling can alleviate challenges that tropical wetland ecosystem services management faces and support decision making of land managers and policymakers better understanding of social ecological systems dynamics is crucial in east africa where societies are vulnerable to poverty and climate variability whilst dependent upon agrarian ecological based economies keywords tropical wetlands east africa modelling ecosystem services 1 introduction wetlands are the interface of aquatic and terrestrial ecosystems forming specialised ecosystems where complex ecological processes occur due to the interactions between water vegetation and soils naiman and henri 1997 globally wetlands cover 5 7 0 3 m km2 mean annual maximum extent 1993 2007 standard deviation prigent et al 2012 approximately 6 of the earth s surface lehner and döll 2004 tropical wetlands located between 30 s and 30 n cover 2 6 m 0 2 m km2 and account for approximately 46 of global wetland coverage prigent et al 2012 estimates suggest that upwards of 50 of wetlands globally have been lost but data on wetland land use change in sub saharan africa is sparse davidson 2014 leemhuis et al 2016 russi et al 2013 unwwap 2003 the provision of a range of goods and benefits to local populations from wetlands is well documented particularly in areas of high rural poverty such as east africa iwmi 2014 maltby and acreman 2011 mccartney et al 2010 naiman and henri 1997 rebelo et al 2009 russi et al 2013 sellamuttu et al 2008 population pressures demands for food and energy and limited natural resource management have driven an expansion of cultivation in wetlands across the tropics mccartney et al 2010 2005 the extent of agricultural impacts on wetland ecosystem health is not well understood particularly in africa davidson 2014 joosten 2010 owino and ryan 2007 between 1999 and 2008 agricultural expansion in wetland areas in uganda averaged 410 ha 1 y 1 nema 2010 yet little is known about the effects of anthropogenic influences on wetland functions and more importantly ecosystem services es and impacts on wellbeing in local communities there is growing recognition of the value of wetland es particularly with regard to climate mitigation and adaptation duraiappah et al 2013 kolka et al 2016 russi et al 2013 increasing demand for climate led wetland protection and restoration by public and private sectors may lead to increased investment in wetland management through government policy and market based mechanisms such as carbon markets and water funds bonn et al 2014 kumar et al 2011 worrall et al 2009 as a result climate change action may become an important future driver of wetland management with impacts effecting multiple es beyond just carbon and wetland dependent communities to understand the future impact of wetland management on wellbeing disaggregated assessments of es are vital for informed decision making in respect to poverty alleviation daw et al 2011 maltby and acreman 2011 rodríguez et al 2006 awareness of the value of es has increased with progress largely due to the re structuring of existing knowledge into new frameworks e g maltby and acreman 2011 this has generated a better understanding of the important components of social ecological systems particularly the need to identify trade offs the role of management and policy and plurality of societal values bennett et al 2009 daw et al 2011 reyers et al 2013 further examination of these systems is required to confirm suppositions and advance understanding of es dynamics modelling and field experiments as the primary tools for informing and developing understanding about systems are essential for creating useful predictions and insights to expand understanding of wetland social ecological systems dynamics and evaluate the impact of wetland management on wellbeing setting the focus of this review in the context of arguably two of east africa s most significant anthropogenic and environmental challenges rural poverty and climate change we focus upon wetland es that are central to these challenges food production and climate and water regulation against this background this paper sets out the critical social ecological dynamics that wetland es assessments need to capture the paper then provides a synthesis of knowledge of the current interactions between wetland functions and anthropogenic influences on es based on a review of existing literature a selection of models and tools commonly used for assessing es and simulating wetland processes are reviewed to assess their ability to capture these important dynamics for wetland es assessments the review is set in an east african context drawing upon perspectives from uganda this allows us to highlight the regional importance of wetlands and current and potential future management options to reduce greenhouse gas ghg emissions and ensure food security against a background of poverty and an increasingly variable climate 2 requirements for wetland ecosystem services modelling ecosystem services are the manifestation of complex interactions between biophysical properties ecological functions and human interventions bennett et al 2015 reyers et al 2013 wetland systems are amongst the most anthropogenically influenced ecosystems in the world with modifications to hydrology vegetation and soil carpenter et al 2011 prigent et al 2012 wetland land use change for agriculture in uganda has typically involved the installation of crude drainage infrastructure to lower water tables tilling wetland soils replacing natural vegetation with crops and grazing pastures and adding agrochemicals for soil management and pest and disease control andriesse 1988 holden 2004 mwita et al 2013 owino and ryan 2007 schuyt 2005 understanding the influence of different land management practices such as precision agriculture or improved water management on ecological function and es is required to evaluate and design alternative land use systems gordon et al 2008 mccartney et al 2010 2005 rebelo et al 2009 this knowledge can further establish best practice techniques such as setting sustainable rates for biomass fibre extraction and processing jones et al 2016 spatial interactions play an important role in the provision of wetland es particularly hydrological pathways between upstream activities which can influence wetland properties and have multiple downstream impacts on a range of es iwmi 2014 wetlands are highly sensitive to temporal dynamics such as seasonal variations in hydrology and vegetation over longer temporal scales there are important system dynamics such as compound changes i e year on year declines in soil fertility inflection points in ecological functions due to climatic thresholds and the role of legacy impacts such as the long term irreversible effects of peatland drainage bennett et al 2015 crépin et al 2012 drexler et al 2009 gordon et al 2008 rodríguez et al 2006 wetlands will be heavily influenced by future climate change and variability hence it is important to understand the impacts of these on es for future management erwin 2009 millennium ecosystem asssessment 2005 assessments of wetland es require simulation of multiple processes that act over a range of spatial and temporal scales in a useful and meaningful way to inform ecosystem management whilst being sensitive to the role of climate and anthropogenic influences on wetland functions and es bennett et al 2015 burkhard et al 2014 van oudenhoven and de groot 2013 due to limited empirical studies and data on tropical african wetlands the data requirements for wetland es assessments is critical to their application sjögersten et al 2014 joosten 2010 this review evaluates es assessment approaches against these important dimensions anthropogenic influences on ecosystem functions spatial and temporal dynamics and data requirements 3 ecosystem services and interactions with anthropogenic influences a summary of anthropogenic impacts on wetland hydrology soil and vegetation under typical land uses and management in uganda is presented in table 1 based on a literature review including of policy and authors experiences interviews with national and local wetland stakeholders in uganda were used to identify the most important es for poverty alleviation and climate change action water regulation climate regulation and provisioning es and their underlying ecological functions in tropical freshwater wetlands table 2 alter 2015 whilst east africa wetlands have already been studied to a certain extent jones and muthuri 1997 kansiime et al 2007 muthuri et al 1989 saunders et al 2013 evidence of climatic and anthropogenic influences on these wetland functions and es is limited thus we draw on evidence from across the field of wetland research including temperate and boreal wetlands that have been more widely studied dommain et al 2011 sjögersten et al 2014 to review evidence of human influences on these wetland ecosystem functions and services 3 1 water regulation wetlands play an important yet complex role within the hydrological cycle supporting direct human uses and indirect benefits like floodwater storage and water purification wetlands play a significant role in landscape water storage providing a source and sink for atmospheric moisture surface water and ground water water storage type has implications for accessibility and availability of water over a range of spatial and temporal scales surface water is directly usable for domestic and agricultural purposes while atmospheric moisture contributes to microclimate regulation supporting catchment precipitation benefiting rain fed agriculture wetland management practices e g the draining of surface waters and abstraction of ground water directly changes the location of water storage within landscapes wetlands with fluvial flows can influence catchment hydrology through long and short term surface water storage which can aid or undermine attenuation of flood waters bullock and acreman 2003 the capacity of wetlands to absorb floodwaters is complex and highly sensitive to the timing of rainfall and the soil and hydrological properties acreman et al 2007 bullock and acreman 2003 moore 1989 usda 1993 water availability also requires the replenishment of water stores regulated by the balance of water cycling processes evapotranspiration surface runoff sub surface water flows and upwelling and infiltration productive tropical vegetation species and warm air temperatures result in high losses of water to the atmosphere through evapotranspiration that are greater than those from open water increasing micro climate humidity and reducing river flows in dry periods abtew and melesse 2013 bullock and acreman 2003 carrington et al 2001 saunders et al 2007 the transfer of water between surface and ground water through upwelling and infiltration is specific to localised underlying wetland geology permeability of subsoil layers and surface soil conditions bullock and acreman 2003 soil properties influence subsurface water flows as the degree of peat humification can have significant impacts on soil hydrological conductivity wösten et al 2008 anthropogenically induced changes are likely to result in a complex response in water availability and cycling in tropical wetlands due to modification of evapotranspiration rates from changes in vegetation increased runoff owing to drained organic soils and significant alteration of water movement within wetland soils as a result of water table manipulation upstream water abstractions may also significantly alter wetland water cycling dynamics allen et al 1998 the deposition of sediment results in the visual purification of water and is an important component in wetland nutrient cycling kansiime et al 2007 naiman and henri 1997 venterink et al 2006 fluvial fed wetlands are generally richer in nutrients from inflowing sediment and minerals associated with flooding and upstream land management compared to rain fed ombrotrophic wetlands moore 1995 1989 wetland vegetation plays an important role in the chemical cycling functions through nutrient uptake and trapping of sediment particularly due to rooted rather than floating vegetation howard williams and gaudet 1985 naiman and henri 1997 venterink et al 2006 under submerged and vegetated conditions wetland soils are protected from the erosive power of rainfall by surface water vegetation canopy and root structures while structured soils containing mineral and clay deposits increase organic soil stability reducing soil and sediment erosion losses lilly et al 2002 hulme and blyth 1985 wischmeier and smith 1978 agricultural plant species and the draining of standing water may significantly alter nutrient cycling dynamics drying and tilling of wetland soils increases their vulnerability to leaching and erosion while crop canopy cover provides less protection against rainfall increasing risks of organic soil erosion holden et al 2004 hulme and blyth 1985 3 2 climate regulation tropical wetlands are important contributors to the global carbon cycle mitsch et al 2013 saunders et al 2013 sjögersten et al 2014 atmospheric carbon fixation during plant growth follows four main pathways largely due wetland hydrological conditions exported as detritus in fluvial flows as dissolved organic carbon doc or particulate organic carbon poc emission as ghg as a result of soil organic matter som turnover due to microbial decomposition accumulation and storage of carbon through the formation of peat soils and removal due to management practices such as burning grazing or harvesting howard williams and gaudet 1985 the interaction between these processes directly influences the climate regulating services that wetlands provide research in indonesia shows losses from long term storage carbon pools and the doubling of poc and doc losses when tropical wetland soils are disturbed due to erosion and leaching which accounts for a fifth of soil carbon balances in this landscape moore et al 2013 murdiyarso and kauffman 2012 worrall et al 2003 hydrological conditions modulate aerobic and anaerobic turnover and rates of som decomposition resulting in carbon dioxide and methane losses adhikari et al 2009 bridgham et al 2012 couwenberg et al 2010 lai 2009 mitsch et al 2013 segers 1998 whalen 2005 the drainage and tilling of wetland soils increases oxygen availability and enables rapid rates aerobic decomposition of peat soils hooijer et al 2012 joosten 2010 microbial decomposition of peat soil is highly sensitive to temperatures changes thus the cooling effects of waterlogging and the shading effect of vegetation can significantly influence rates of som turnover davidson et al 2006 jauhiainen et al 2014 kirschbaum 1995 soil properties particularly the degree of humification and nutrient availability influence the resistance of wetland soils to decomposition smith et al 2010 management practices such as controlling ph plant inputs and agrochemical application can substantially alter whether soil is recalcitrant reactive or labile to decomposition andersson and nilsson 2001 hall et al 1998 lai 2009 segers 1998 whalen 2005 formation of peat soils in wetland areas occurs through an accumulation of som due to excess of net primary production npp over som turnover linked to waterlogging retarding microbial decomposition forming large sinks of carbon moore 1995 1989 page et al 2011 accumulation occurs over many thousands of years making empirical studies of the processes difficult particularly as key properties water table temperature and plant functional types also evolve over these time scales dommain et al 2011 heinemeyer et al 2010 agriculture substantially modifies organic material inputs due to vegetation management practices such as residues management vegetation burning and fibre extraction kurnianto et al 2015 3 3 provisioning services tropical wetland vegetation including forest papyrus and other sedges is characterised by high rates of plant growth often used as a source of fuel fibre and fodder howard williams and gaudet 1985 jones et al 2016 jones and humphries 2002 saunders et al 2007 thompson et al 1979 temperature water availability nutrient uptake and irradiance and energy capture all modulate the rate of npp of wetland vegetation muthuri et al 1989 saunders et al 2007 plant species with greater tolerance of temperature variations i e plants with a c4 photosynthesis pathway e g papyrus sedges cabbages maize may better adapt to climate change while other plants species sensitive to temperature may fail i e potatoes howard williams and gaudet 1985 kansiime et al 2007 saunders et al 2007 plant water availability due to seasonal or anthropogenic influences on soil moisture may limit npp particularly for non wetland specialist vegetation not adapted to anoxic soil conditions pezeshki and delaune 2012 vartapetian and jackson 1995 wegner 2010 conversely lowering of water levels for non wetland vegetation may result in plant moisture stress due to soil moisture retention by organic wetland soils particularly under high rates of evapotranspiration andriesse 1988 boelter 1968 it is unclear how the performance of wetland specialist vegetation is affected by lower water availability boar 2006 plant nutrient availability linked to soil mineralisation nutrient and sediment flows and the application of synthetic fertilizers can have a significant effect on plant and crop growth due to plant species nutrient efficacy kanyiginya et al 2010 mugisha et al 2007 changes to soil vegetation and hydrological management will have multiple effects on nutrient cycling dynamics and plant growth the performance of vegetation is heavily influenced by rates of grazing harvesting and fire occurrences due to impact on canopy interception of irradiance boar 2006 kansiime et al 2007 saunders et al 2012 terer et al 2011 the timing of water and temperature stresses can have significant impact on crop yield as the apportionment of plant growth to harvested product varies over the growing season the influence of diseases on crop yield is widely recognised although patterns of fungal diseases such as late blight are under researched in tropical regions the presence of pest biodiversity for example nematodes in wetland soils can be highly destructive to crops without use of pesticides or prolonged inundation drained wetland soils create a highly suitable habitat for nematode populations andriesse 1988 wetland es assessment requires a greater understanding of the impact these influences will have on multiple ecosystem functions and services 4 review of approaches to wetland ecosystem assessments the evidence base for the interactions between social ecological systems has been built upon a wide range of individual studies mostly focused on a small sub component of the system with the supposition of non uniqueness of place or people modelling allows us to analytically examine these assumptions by replicating and testing our understanding of whether the expected system dynamics hold we identify five broad types of models and tools that have been used to study wetland functions and es spatial es models wetland inventories process models specifically for ghg cycling catchment hydrological models and spatial and remote sensing models we review a selection of models within each of these types used for prediction of the above ecological functions and review how these models could be used to model water carbon and provisioning es from tropical wetlands the review is not a comprehensive list due to the large number of environmental models available but includes some of those most widely used and relevant to modelling es or applied to wetlands across a number of disciplines table 3 presents a list of references for each model for further reading a review of the models ability to simulate key dimensions in the production of es spatial and temporal dynamics land management and data scarcity a summary of the processes captured by each model is presented in table 4 4 1 spatial ecosystem services modelling approaches a number of es modelling and mapping platforms are being developed to spatially apply simplified process based models with user friendly interfaces and minimising model data and parameterisation requirements e g bagstad et al 2011 jackson et al 2013 tallis et al 2013 however many of these models remain proprietary bagstad et al 2013 many es models e g invest use approaches where predictions are based on one dimensional proxies often land use or land cover rather than simulating the dynamic processes underlying the many ecosystem functions particularly for modelling carbon these simplified static models do not quantify important carbon fluxes for tropical wetlands particularly som turnover poc and doc e g barquín et al 2015 while es mapping models do capture important functions in the provision of wetland es using simplified well understood hydrological relationships especially water balance and sediment dynamics their applicability to organic soils remain limited due to a lack of data for parameterisation lilly et al 2002 the strength of es mapping models is their ability to examine the impact of land use change on es through scenario exploration sharp et al 2016 vigerstol and aukema 2011 however model specifications often mean that important dynamics of ecosystem functions are not captured e g the use of annual reporting periods misses important seasonal variability in ecological functions important for understanding climate change implications on wetland es 4 2 inventory approaches to es inventory and matrix type approaches are based on identification of ecological and environmental characteristics to predict es using one dimensional relationships e g burkhard et al 2012 müller and burkhard 2012 tropical wetland assessments are based on detailed fieldwork for a range of applications including wetland es assessment wet ecoservice developed and applied in south africa kotze et al 2008 and wetland ecosystem service protocol wespus adamus 2011 and wetland uses the national wetland information system nwis for uganda henninger and landsberg 2009 inventories develop detailed descriptions of localised wetland functions due to unique environmental characteristics requiring extensive field observation for small wetland areas often limited to one point in time e g namaalwa et al 2013 this approach provides limited predictive power for scaling up the assessment findings or to provide insight into the role of resource management on es through which to understand alternative management practices and climate change impacts inventories are often conducted at the wetland level or hydro geomorphic unit areas of similar hydrology and geomorphology and as such do not provide relevant information to evaluate environmental effects at the field and plot level where changes in land management practices can be realised there has been an international movement towards developing country specific national wetland inventories however many have been developed in isolation compounding problems for modelling rather than more coordinated efforts that have typified global campaigns for other natural resource mapping e g forests and soil leemhuis et al 2016 vågen et al 2016 inventories have also failed to generate data useful for modelling due to a lack of standardisation of metrics and comparability across different wetlands sites kotze et al 2008 4 3 greenhouse gas process models process based models of ghg emissions have been widely used to simulate water and carbon and other nutrient cycling from global to field scale gerten et al 2004 sitch et al 2003 smith et al 1997 water cycling models often use a mechanistic approach to simulate water balance based on climate and land cover data and some have incorporated routines of hydrological dynamics for vertical and horizontal water movement in soils carbon models generally use a pool based approach for vegetation and soil components such as biomass plant litter labile and recalcitrant som and models simulate the role of external environmental variables in modifying transition rates between different carbon pools smith et al 2010 a wide range of models combine different water and carbon cycling processes with important differences in the design and structure including the number of soil layers and carbon pools representations of hydrology and calculation of photosynthesis and productivity many widely used ghg models have been modified specifically for use on organic or waterlogged soils to capture important wetland sub processes by including routines for dynamic water table levels and ch4 gaseous transport pathways e g lpj why me wania et al 2009 wetland dndc li et al 2004 ecosse smith et al 2010 see farmer et al 2011 for a comprehensive review of the applicability of ghg models to tropical peatlands greenhouse gas process models have been developed for application over a wide range of spatial scales and model complexity models such as lpj developed to examine global level water and carbon cycling dynamics and the response of vegetation to climate effects are unsuitable for downscaling to the field scale required to inform management due to approximations within parameterisation e g the use of broad biome plant function types classifications sitch et al 2003 models requiring fewer input variables to parameterise such as the rothc model use easily obtainable field data at a range of data resolutions for parameterisation during initialisation runs outputs of these models are based on the same assumptions when modelling at a range of scales and predictions and are only limited by the quality of the input data rather than model parameterisations and thus may be more easily applied in a tropical wetland context where data is limited the majority of models applied to wetlands have only been developed and used in boreal and temperate regions dommain et al 2011 wania et al 2010 2009 although recently some of these models have been successfully applied to tropical wetlands e g hpmtrop kurnianto et al 2015 the complexity and sophistication of the process model varies considerably with direct implications of data needs for model parameterisation and initialisation some models use a spin up period where the model is parameterised using historical climate data to establish a steady state of carbon cycling in the soil however this approach poses problems for peatlands as they are characterised by natural long term carbon accumulation and so may underestimate co2 uptake sulman et al 2012 model input variables and fixed parameters may not be easy to measure and data needed to evaluate models particularly long term data may not be available in tropical wetlands vereecken et al 2016 4 4 catchment and hydrological transport models catchment models are widely used to investigate hydrology and soil interactions through simulating infiltration runoff base flow and pollutant loading hydrological models have been developed for a spectrum of spatial and temporal scales lumped models aggregate processes into specific hydrological units i e sub catchment or wetland complex while distributed models are spatially explicit similarly event based models are used to identify storm and flood peaks compared to continuous models that capture base flows singh and woolhiser 2002 model design and structure has direct implications for understanding spatial and temporal resolution of ecological functions and subsequent es distributed and semi distributed catchment models such as swat and vic are widely used to study water soil and vegetation interactions on water cycling gao et al 2009 gassman et al 2007 liang et al 1994 despite wide recognition that hydraulic conductivity and storage properties of wetland soils differ largely from mineral soils these models often struggle to capture the role of organic soils in catchment hydrology particularly the variability due to water table depth and its impact on water storage and movement bengtson and padmanabhan 1999 whitfield et al 2009 catchment modelling is often performed at large scales regional to global level to examine policy and climate change impact to es rather than field specific drivers van roosmalen et al 2009 the development of modular structures means models are able to simulate additional ecological functions including wetland hydrology plant and crop growth and carbon cycling although they require increasing model complexity data and modelling expertise vigerstol and aukema 2011 these models are able to capture spatial relationships in a landscape e g using routine models for determining water flows and are increasingly drawing upon remote sensing data for model inputs e g wateraccounting van eekelen et al 2015 applications in tropical wetlands include hydro pedological modelling of surface and ground water interactions to predict monthly water table depths e g simgro wösten et al 2008 lumped models often based on bucket models or reservoirs are used to estimate water storage of water bodies and discharge rates based on detailed geomorphological data on reservoir size and climate data these models have been developed to include a range of processes linked to water cycling and storage functions including evapotranspiration infiltration surface runoff and subsurface flow model applications include the study of the effects of agricultural drainage e g drainmod skaggs et al 2012 climate change impacts on temperate peats e g wetscape johnson et al 2010 werner et al 2013 and water quality e g wetmod cetin et al 2001 the site specific nature of lumped models means they are generally applied at a small scale and calibrated models are not spatially transferable without renewed parameterisation limiting their application parameterisation of catchment and hydrological models generally requires long term data records of water flows and water quality from gauging stations although newer model approaches are improving predictions for un gauged catchments van emmerik et al 2015 wagener et al 2004 4 5 remote sensing and spatial models increasing availability of satellite imagery and spatial data sets has seen a rapid expansion of new techniques for interpreting data for ecological assessment relevant for examining wetland es by mapping biophysical parameters e g soil moisture vegetation type leaf area index petter et al 2013 remote sensing applications for wetlands have attempted to map global extents of wetlands distribution and examine land use at local scale fluet chouinard et al 2015 lehner and döll 2004 mwita et al 2013 sakané et al 2011 remote sensing of wetland functions has been limited particularly in african wetlands where the effects of soil moisture canopy and ecosystem structure on vegetation spectral reflectance variability remain poorly understood adam et al 2010 ozesmi and bauer 2002 a number of studies have been successful in classifying african wetland vegetation when combining field work with remote sensing data adam and mutanga 2009 fuller et al 1998 rudiyanto et al 2015 the combination of spatial datasets and standardised field measures has demonstrated a useful tool of digitally mapping natural resource extent and condition to provide input for modelling terrestrial ecosystem functions and services such as digital soil mapping for hydrological carbon and crop yield functions mendonça santos et al 2010 minasny and mcbratney 2016 tavares wahren et al 2016 vågen et al 2016 winowiecki et al 2015 due to the high heterogeneity of wetlands using proximal measures to identify key properties and functions such as using plant physiological status to estimate water table may provide insight on the complex relationships between ecosystem properties functions and es such approaches are being explored in temperate and boreal wetlands couwenberg et al 2011 5 discussion a significant challenge for assessing es is how to appropriately and transparently incorporate a number of ecological functions acting at different spatial and temporal scales while retaining the operational simplicity desired by decision makers bagstad et al 2013 vigerstol and aukema 2011 we analyse a range of environmental modelling approaches to determine the suitability of existing tools for use in wetland es assessments and support wetland management for poverty alleviation and climate action wetland es assessments require understanding of the multiple impacts of changes to vegetation soil and hydrology under land use and climate change scenarios table 1 we identify 21 ecological processes contributing to eight functions easily modified by anthropogenic influences in the production of just three wetland es key to rural poverty alleviation and climate change dynamics table 2 large complexities exist for modelling wetland es at the field and plot level due to specific wetland properties and high heterogeneity within and between wetlands the reviewed models and tools capture a mixed selection of wetland properties processes and functions relevant to the creation of wetland es and show a spectrum of sensitivity for capturing spatial and temporal dynamics and anthropogenic influences tables 3 and 4 commonly used models for es assessments are often not appropriate for wetlands due to a lack of data for parameterisation owing to unique wetland properties and oversimplification of specialised wetland ecological functions this is particularly evident for modelling carbon cycling where complex interactions exist between wetland hydrology soils and vegetation these models are often driven by land use change and have limited ability to examine changes within land use class management practices such as the impact of increasing fertiliser use these models often simulate a number of ecological functions in isolation and fail to provide insight into interactions between ecological functions and their effects on es inventory type approaches although widely used have limited ability to make predictive estimates of es under management and climate change scenarios further many inventory systems have not been designed with modelling objectives in mind limiting the use of data for modelling purposes a number of process based models have been modified explicitly to capture important wetland functions however most are confined to a very limited number of ecological functions lack dynamic simulation of important features particularly the hydrological interaction between surface and ground water and upstream downstream spatial interactions and face severe data limitations to parameterise and evaluate model outputs data limitations increase rapidly if a number of models are required to capture multiple es each with different input data specifications while catchment models capture spatial and temporal dynamics they are widely run at large scales where the importance of wetland functions due to the small areal extent of wetlands compared to catchment size is lost and poorly capture the role of field scale land management practices by contrast non distributed hydrological models are developed as site specific and subsequently not widely transferable the increasing availability of remote sensing data provides valuable insight into current environmental properties and ecological processes however attention is needed to develop models and tools that allow the use of remote sensing data to simulate ecological functions and the creation of es we need to ask how remote sensing products for example the modis global terrestrial evapotranspiration product mu et al 2013 can be used to inform understanding of future climate or land use changes while there has been much development in the recognition and discussion of wetland es from general es assessments millennium ecosystem asssessment 2005 to wetland specific es maltby and acreman 2011 and then on to economic valuation of wetland es russi et al 2013 this evolution has been mostly directed through the advancement of frameworks and has successfully engaged and been taken up by a range of stakeholders in the policy research and practitioner environments our review suggests there is a wealth of knowledge in the literature about many elements of wetland social ecological systems particularly in terms of biophysical processes however there is less understanding about multiple interactions between ecosystem processes functions and es particularly social ecological dynamics that drive these complex systems we suggest that this is reflected in current models and tools available for wetland es assessments no one model that we reviewed was able to capture the range of ecological functions and anthropogenic influences to predict the three wetland es table 4 shows that many models have been developed that capture a small number of closely related ecological processes in varying detail rather than a greater range of functions in the past model development has focused upon a single function e g soil erosion and while a number of models have made good progress in shifting focus towards modelling functions to services environmental modelling has been dominated by a focus on regulating services particularly carbon cycling and hydrology the range of functions for which there are models is a limited narrow field and there is a tendency for models to focus on a few of these better known ecological functions in regard to one es the universal soil loss equation is a good example of modelling a single function capturing multiple simplified processes coded into the model terms soil erosion is not a es on its own and understanding is needed as to how soil erosion links to crop production water flow and water purification services model specialisation presents a challenge for understanding the interactions between different es and increases data requirements for es assessments model specialisation could be tackled through a concerted effort to build integrated modelling approaches across biological disciplines such as ghg hydrology and nutrient cycling modelling communities a solution may be a modular approach for combining different model types to create function based models however the complexity underlying ecosystem functions and processes has tended to result in the development of increasingly computational models creating a variety of issues including difficulty in obtaining an intuitive understanding of model dynamics non identifiable or un measurable model parameters and a diversity of model outputs that are not easily comparable due to differing model spatial and temporal scales e g the use of daily or annual reporting periods care will be required to avoid overly complex models streamline data requirements and provide easily comparable and interpretable outputs that inform tropical and non tropical wetland management reducing model complexity and data needs will be challenging particularly given the need to maintain important model behaviour and predictive capacity however there are successful examples of combining climate change hydrological processes and ecological responses of wetlands acreman et al 2009 model development is lagging behind our understanding of the conceptual framework of es which will hinder progress in understanding these complex systems in order to improve understanding of socio ecological systems around wetland es there is a need for models to examine a much wider breadth of ecological functions their interactions and impacts on es rather than increasing complexity and specialisation of modelling focused on understanding ecosystem processes increasingly es tools and models are being applied at the landscape scale to assess a broader range of es egoh et al 2008 nelson et al 2009 due to the weak representation of wetland functions this is likely to result in the poor prediction of wetland es and subsequently may lead to under valuation of wetland es although temperate and boreal wetlands have been more widely studied modelling of wetland es across the globe faces large data limitations for parameterisation and evaluation particularly for modelling multiple es especially in tropical wetlands many process models have not yet been fully applied in wetland areas due to data limitations and these models usually only examine a very narrow range of functions rather than the full suite of wetland es which is ultimately required farmer et al 2011 because of this interdisciplinary understanding of how wetland processes work is required across a range of different conditions but there is very little in the literature about the differences in wetland processes and functions in different geographies wetland management requires scarce data compatible models to enhance the understanding of complex interactions and feedbacks between wetland ecological functions anthropogenic influences and es to inform management data on tropical wetlands is patchy and where available is not suitable for modelling purposes as it is often based on localised ranking and scoring systems from inventories such as in henninger and landsberg 2009 namaalwa et al 2013 little attention has been given to the design of data generation integration with existing and increasing data sources i e remote sensing and flow into further application i e modelling while many models remain flexible to a range of spatial and temporal scales efforts are limited by available data for parameterisation particularly lack of empirical studies to draw data from or limited application and evaluation of theoretically derived relationships between environmental factors the evaluation of models often requires long term data or datasets such as carbon exchange or water discharge and loadings which are not readily available in tropical regions particularly east africa consideration must also be given to uses and users of model outputs particularly the aim of the model as this has a number of implications for the type of model selected for example the scale of the model is dependent on the intended beneficiaries land managers i e local community groups farmers or local government require understanding of management options at the field to landscape scale particularly in terms of spatial interactions and trade offs due to different land management techniques while policy makers require subnational application of models to create understanding of future alternative states of wetland systems to inform policy development and planning uncertainty and accuracy concerns of model forecasts that have not been extensively evaluated will hinder the use of models in both these management applications ensemble modelling techniques running different models and comparing the outputs developed in climate and weather forecasting are increasingly being used in environmental modelling to understand precision and uncertainty of models through inter model comparison and are increasingly being used in es assessments araujo and new 2007 marmion et al 2009 ensemble modelling will require standardised datasets with common protocols data and model outputs for comparison which is currently challenged by the diversity of inputs and outputs used by individual models in the absence of traditional datasets and recognising the role of anthropogenic influences in shaping es there is a need to integrate a range of knowledge sources in modelling efforts by combining data with other information sources such as expert and stakeholder opinion to capture knowledge and human interpretation of es 6 conclusions while wetlands are hugely important due to the es they provide to the global climate and rural poor in east africa there remains limited understanding and integrated knowledge of important social ecological system dynamics there is an urgent need to understand spatial and temporal trade offs and the influence of wetland land management practices on es particularly as the value of climate regulating services wetlands provide is increasingly realised we identify 21 ecological processes contributing to eight ecosystem functions heavily modified by anthropogenic influences in the production of only three es water climate and food a number of approaches have been used to study wetland es but these face limitations es mapping models are overly simplistic for interactions between wetland ecological functions inventories have limited predictive power process based modelling of wetland functions is limited by data availability and narrow range of modelled functions and catchment models are inappropriate for informing field scale management decisions while there has been a large shift in the environmental research discourse towards es much of the progress in understanding social ecological systems dynamics has been to forward existing knowledge through frameworks modelling which is a hugely important tool for furthering knowledge about systems dynamics currently lags behind this evolution with a focus upon a narrow range of one or two functions largely linked to regulating es our understanding of interactions between multiple ecological functions is still limited particularly in regard to provisioning services and supporting functions like biodiversity and soil fertility there is a need for integration of modelling disciplines to focus upon the interactions of ecological functions in the production of multiple es to evolve our understanding of complex social ecological systems while tropical wetlands remain poorly studied compared to temperate and boreal wetlands there is an urgent need for assessments of multiple wetland es to inform decision making at local and larger scales for land managers and policymakers this is nowhere more important than in regions such as east africa where societies are vulnerable to poverty and climate variability and underpinned by agrarian ecological based economies acknowledgments this work alternative carbon investments in ecosystems for poverty alleviation alter grant no ne k010441 1 was funded with support from the ecosystem services for poverty alleviation espa programme the espa programme is funded by the department for international development dfid the economic and social research council esrc and the natural environment research council nerc appendix a supplementary data the following is the supplementary data related to this article supplementary data supplementary data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 022 
26414,east african wetlands are hotspots of ecosystem services particularly for climate regulation water provision and food production we review the ability of current approaches to ecosystem service assessments to capture important social ecological dynamics to provide insight for wetland management and human wellbeing we synthesise evidence of human influences on wetlands and gauge the suitability of models and tools for simulating spatial and temporal dynamics and land management on multiple ecosystem functions and services current approaches are largely unsuitable for advancing knowledge of social ecological system dynamics and could be greatly improved with inter disciplinary model integration to focus upon interactions between multiple ecosystem functions and services modelling can alleviate challenges that tropical wetland ecosystem services management faces and support decision making of land managers and policymakers better understanding of social ecological systems dynamics is crucial in east africa where societies are vulnerable to poverty and climate variability whilst dependent upon agrarian ecological based economies keywords tropical wetlands east africa modelling ecosystem services 1 introduction wetlands are the interface of aquatic and terrestrial ecosystems forming specialised ecosystems where complex ecological processes occur due to the interactions between water vegetation and soils naiman and henri 1997 globally wetlands cover 5 7 0 3 m km2 mean annual maximum extent 1993 2007 standard deviation prigent et al 2012 approximately 6 of the earth s surface lehner and döll 2004 tropical wetlands located between 30 s and 30 n cover 2 6 m 0 2 m km2 and account for approximately 46 of global wetland coverage prigent et al 2012 estimates suggest that upwards of 50 of wetlands globally have been lost but data on wetland land use change in sub saharan africa is sparse davidson 2014 leemhuis et al 2016 russi et al 2013 unwwap 2003 the provision of a range of goods and benefits to local populations from wetlands is well documented particularly in areas of high rural poverty such as east africa iwmi 2014 maltby and acreman 2011 mccartney et al 2010 naiman and henri 1997 rebelo et al 2009 russi et al 2013 sellamuttu et al 2008 population pressures demands for food and energy and limited natural resource management have driven an expansion of cultivation in wetlands across the tropics mccartney et al 2010 2005 the extent of agricultural impacts on wetland ecosystem health is not well understood particularly in africa davidson 2014 joosten 2010 owino and ryan 2007 between 1999 and 2008 agricultural expansion in wetland areas in uganda averaged 410 ha 1 y 1 nema 2010 yet little is known about the effects of anthropogenic influences on wetland functions and more importantly ecosystem services es and impacts on wellbeing in local communities there is growing recognition of the value of wetland es particularly with regard to climate mitigation and adaptation duraiappah et al 2013 kolka et al 2016 russi et al 2013 increasing demand for climate led wetland protection and restoration by public and private sectors may lead to increased investment in wetland management through government policy and market based mechanisms such as carbon markets and water funds bonn et al 2014 kumar et al 2011 worrall et al 2009 as a result climate change action may become an important future driver of wetland management with impacts effecting multiple es beyond just carbon and wetland dependent communities to understand the future impact of wetland management on wellbeing disaggregated assessments of es are vital for informed decision making in respect to poverty alleviation daw et al 2011 maltby and acreman 2011 rodríguez et al 2006 awareness of the value of es has increased with progress largely due to the re structuring of existing knowledge into new frameworks e g maltby and acreman 2011 this has generated a better understanding of the important components of social ecological systems particularly the need to identify trade offs the role of management and policy and plurality of societal values bennett et al 2009 daw et al 2011 reyers et al 2013 further examination of these systems is required to confirm suppositions and advance understanding of es dynamics modelling and field experiments as the primary tools for informing and developing understanding about systems are essential for creating useful predictions and insights to expand understanding of wetland social ecological systems dynamics and evaluate the impact of wetland management on wellbeing setting the focus of this review in the context of arguably two of east africa s most significant anthropogenic and environmental challenges rural poverty and climate change we focus upon wetland es that are central to these challenges food production and climate and water regulation against this background this paper sets out the critical social ecological dynamics that wetland es assessments need to capture the paper then provides a synthesis of knowledge of the current interactions between wetland functions and anthropogenic influences on es based on a review of existing literature a selection of models and tools commonly used for assessing es and simulating wetland processes are reviewed to assess their ability to capture these important dynamics for wetland es assessments the review is set in an east african context drawing upon perspectives from uganda this allows us to highlight the regional importance of wetlands and current and potential future management options to reduce greenhouse gas ghg emissions and ensure food security against a background of poverty and an increasingly variable climate 2 requirements for wetland ecosystem services modelling ecosystem services are the manifestation of complex interactions between biophysical properties ecological functions and human interventions bennett et al 2015 reyers et al 2013 wetland systems are amongst the most anthropogenically influenced ecosystems in the world with modifications to hydrology vegetation and soil carpenter et al 2011 prigent et al 2012 wetland land use change for agriculture in uganda has typically involved the installation of crude drainage infrastructure to lower water tables tilling wetland soils replacing natural vegetation with crops and grazing pastures and adding agrochemicals for soil management and pest and disease control andriesse 1988 holden 2004 mwita et al 2013 owino and ryan 2007 schuyt 2005 understanding the influence of different land management practices such as precision agriculture or improved water management on ecological function and es is required to evaluate and design alternative land use systems gordon et al 2008 mccartney et al 2010 2005 rebelo et al 2009 this knowledge can further establish best practice techniques such as setting sustainable rates for biomass fibre extraction and processing jones et al 2016 spatial interactions play an important role in the provision of wetland es particularly hydrological pathways between upstream activities which can influence wetland properties and have multiple downstream impacts on a range of es iwmi 2014 wetlands are highly sensitive to temporal dynamics such as seasonal variations in hydrology and vegetation over longer temporal scales there are important system dynamics such as compound changes i e year on year declines in soil fertility inflection points in ecological functions due to climatic thresholds and the role of legacy impacts such as the long term irreversible effects of peatland drainage bennett et al 2015 crépin et al 2012 drexler et al 2009 gordon et al 2008 rodríguez et al 2006 wetlands will be heavily influenced by future climate change and variability hence it is important to understand the impacts of these on es for future management erwin 2009 millennium ecosystem asssessment 2005 assessments of wetland es require simulation of multiple processes that act over a range of spatial and temporal scales in a useful and meaningful way to inform ecosystem management whilst being sensitive to the role of climate and anthropogenic influences on wetland functions and es bennett et al 2015 burkhard et al 2014 van oudenhoven and de groot 2013 due to limited empirical studies and data on tropical african wetlands the data requirements for wetland es assessments is critical to their application sjögersten et al 2014 joosten 2010 this review evaluates es assessment approaches against these important dimensions anthropogenic influences on ecosystem functions spatial and temporal dynamics and data requirements 3 ecosystem services and interactions with anthropogenic influences a summary of anthropogenic impacts on wetland hydrology soil and vegetation under typical land uses and management in uganda is presented in table 1 based on a literature review including of policy and authors experiences interviews with national and local wetland stakeholders in uganda were used to identify the most important es for poverty alleviation and climate change action water regulation climate regulation and provisioning es and their underlying ecological functions in tropical freshwater wetlands table 2 alter 2015 whilst east africa wetlands have already been studied to a certain extent jones and muthuri 1997 kansiime et al 2007 muthuri et al 1989 saunders et al 2013 evidence of climatic and anthropogenic influences on these wetland functions and es is limited thus we draw on evidence from across the field of wetland research including temperate and boreal wetlands that have been more widely studied dommain et al 2011 sjögersten et al 2014 to review evidence of human influences on these wetland ecosystem functions and services 3 1 water regulation wetlands play an important yet complex role within the hydrological cycle supporting direct human uses and indirect benefits like floodwater storage and water purification wetlands play a significant role in landscape water storage providing a source and sink for atmospheric moisture surface water and ground water water storage type has implications for accessibility and availability of water over a range of spatial and temporal scales surface water is directly usable for domestic and agricultural purposes while atmospheric moisture contributes to microclimate regulation supporting catchment precipitation benefiting rain fed agriculture wetland management practices e g the draining of surface waters and abstraction of ground water directly changes the location of water storage within landscapes wetlands with fluvial flows can influence catchment hydrology through long and short term surface water storage which can aid or undermine attenuation of flood waters bullock and acreman 2003 the capacity of wetlands to absorb floodwaters is complex and highly sensitive to the timing of rainfall and the soil and hydrological properties acreman et al 2007 bullock and acreman 2003 moore 1989 usda 1993 water availability also requires the replenishment of water stores regulated by the balance of water cycling processes evapotranspiration surface runoff sub surface water flows and upwelling and infiltration productive tropical vegetation species and warm air temperatures result in high losses of water to the atmosphere through evapotranspiration that are greater than those from open water increasing micro climate humidity and reducing river flows in dry periods abtew and melesse 2013 bullock and acreman 2003 carrington et al 2001 saunders et al 2007 the transfer of water between surface and ground water through upwelling and infiltration is specific to localised underlying wetland geology permeability of subsoil layers and surface soil conditions bullock and acreman 2003 soil properties influence subsurface water flows as the degree of peat humification can have significant impacts on soil hydrological conductivity wösten et al 2008 anthropogenically induced changes are likely to result in a complex response in water availability and cycling in tropical wetlands due to modification of evapotranspiration rates from changes in vegetation increased runoff owing to drained organic soils and significant alteration of water movement within wetland soils as a result of water table manipulation upstream water abstractions may also significantly alter wetland water cycling dynamics allen et al 1998 the deposition of sediment results in the visual purification of water and is an important component in wetland nutrient cycling kansiime et al 2007 naiman and henri 1997 venterink et al 2006 fluvial fed wetlands are generally richer in nutrients from inflowing sediment and minerals associated with flooding and upstream land management compared to rain fed ombrotrophic wetlands moore 1995 1989 wetland vegetation plays an important role in the chemical cycling functions through nutrient uptake and trapping of sediment particularly due to rooted rather than floating vegetation howard williams and gaudet 1985 naiman and henri 1997 venterink et al 2006 under submerged and vegetated conditions wetland soils are protected from the erosive power of rainfall by surface water vegetation canopy and root structures while structured soils containing mineral and clay deposits increase organic soil stability reducing soil and sediment erosion losses lilly et al 2002 hulme and blyth 1985 wischmeier and smith 1978 agricultural plant species and the draining of standing water may significantly alter nutrient cycling dynamics drying and tilling of wetland soils increases their vulnerability to leaching and erosion while crop canopy cover provides less protection against rainfall increasing risks of organic soil erosion holden et al 2004 hulme and blyth 1985 3 2 climate regulation tropical wetlands are important contributors to the global carbon cycle mitsch et al 2013 saunders et al 2013 sjögersten et al 2014 atmospheric carbon fixation during plant growth follows four main pathways largely due wetland hydrological conditions exported as detritus in fluvial flows as dissolved organic carbon doc or particulate organic carbon poc emission as ghg as a result of soil organic matter som turnover due to microbial decomposition accumulation and storage of carbon through the formation of peat soils and removal due to management practices such as burning grazing or harvesting howard williams and gaudet 1985 the interaction between these processes directly influences the climate regulating services that wetlands provide research in indonesia shows losses from long term storage carbon pools and the doubling of poc and doc losses when tropical wetland soils are disturbed due to erosion and leaching which accounts for a fifth of soil carbon balances in this landscape moore et al 2013 murdiyarso and kauffman 2012 worrall et al 2003 hydrological conditions modulate aerobic and anaerobic turnover and rates of som decomposition resulting in carbon dioxide and methane losses adhikari et al 2009 bridgham et al 2012 couwenberg et al 2010 lai 2009 mitsch et al 2013 segers 1998 whalen 2005 the drainage and tilling of wetland soils increases oxygen availability and enables rapid rates aerobic decomposition of peat soils hooijer et al 2012 joosten 2010 microbial decomposition of peat soil is highly sensitive to temperatures changes thus the cooling effects of waterlogging and the shading effect of vegetation can significantly influence rates of som turnover davidson et al 2006 jauhiainen et al 2014 kirschbaum 1995 soil properties particularly the degree of humification and nutrient availability influence the resistance of wetland soils to decomposition smith et al 2010 management practices such as controlling ph plant inputs and agrochemical application can substantially alter whether soil is recalcitrant reactive or labile to decomposition andersson and nilsson 2001 hall et al 1998 lai 2009 segers 1998 whalen 2005 formation of peat soils in wetland areas occurs through an accumulation of som due to excess of net primary production npp over som turnover linked to waterlogging retarding microbial decomposition forming large sinks of carbon moore 1995 1989 page et al 2011 accumulation occurs over many thousands of years making empirical studies of the processes difficult particularly as key properties water table temperature and plant functional types also evolve over these time scales dommain et al 2011 heinemeyer et al 2010 agriculture substantially modifies organic material inputs due to vegetation management practices such as residues management vegetation burning and fibre extraction kurnianto et al 2015 3 3 provisioning services tropical wetland vegetation including forest papyrus and other sedges is characterised by high rates of plant growth often used as a source of fuel fibre and fodder howard williams and gaudet 1985 jones et al 2016 jones and humphries 2002 saunders et al 2007 thompson et al 1979 temperature water availability nutrient uptake and irradiance and energy capture all modulate the rate of npp of wetland vegetation muthuri et al 1989 saunders et al 2007 plant species with greater tolerance of temperature variations i e plants with a c4 photosynthesis pathway e g papyrus sedges cabbages maize may better adapt to climate change while other plants species sensitive to temperature may fail i e potatoes howard williams and gaudet 1985 kansiime et al 2007 saunders et al 2007 plant water availability due to seasonal or anthropogenic influences on soil moisture may limit npp particularly for non wetland specialist vegetation not adapted to anoxic soil conditions pezeshki and delaune 2012 vartapetian and jackson 1995 wegner 2010 conversely lowering of water levels for non wetland vegetation may result in plant moisture stress due to soil moisture retention by organic wetland soils particularly under high rates of evapotranspiration andriesse 1988 boelter 1968 it is unclear how the performance of wetland specialist vegetation is affected by lower water availability boar 2006 plant nutrient availability linked to soil mineralisation nutrient and sediment flows and the application of synthetic fertilizers can have a significant effect on plant and crop growth due to plant species nutrient efficacy kanyiginya et al 2010 mugisha et al 2007 changes to soil vegetation and hydrological management will have multiple effects on nutrient cycling dynamics and plant growth the performance of vegetation is heavily influenced by rates of grazing harvesting and fire occurrences due to impact on canopy interception of irradiance boar 2006 kansiime et al 2007 saunders et al 2012 terer et al 2011 the timing of water and temperature stresses can have significant impact on crop yield as the apportionment of plant growth to harvested product varies over the growing season the influence of diseases on crop yield is widely recognised although patterns of fungal diseases such as late blight are under researched in tropical regions the presence of pest biodiversity for example nematodes in wetland soils can be highly destructive to crops without use of pesticides or prolonged inundation drained wetland soils create a highly suitable habitat for nematode populations andriesse 1988 wetland es assessment requires a greater understanding of the impact these influences will have on multiple ecosystem functions and services 4 review of approaches to wetland ecosystem assessments the evidence base for the interactions between social ecological systems has been built upon a wide range of individual studies mostly focused on a small sub component of the system with the supposition of non uniqueness of place or people modelling allows us to analytically examine these assumptions by replicating and testing our understanding of whether the expected system dynamics hold we identify five broad types of models and tools that have been used to study wetland functions and es spatial es models wetland inventories process models specifically for ghg cycling catchment hydrological models and spatial and remote sensing models we review a selection of models within each of these types used for prediction of the above ecological functions and review how these models could be used to model water carbon and provisioning es from tropical wetlands the review is not a comprehensive list due to the large number of environmental models available but includes some of those most widely used and relevant to modelling es or applied to wetlands across a number of disciplines table 3 presents a list of references for each model for further reading a review of the models ability to simulate key dimensions in the production of es spatial and temporal dynamics land management and data scarcity a summary of the processes captured by each model is presented in table 4 4 1 spatial ecosystem services modelling approaches a number of es modelling and mapping platforms are being developed to spatially apply simplified process based models with user friendly interfaces and minimising model data and parameterisation requirements e g bagstad et al 2011 jackson et al 2013 tallis et al 2013 however many of these models remain proprietary bagstad et al 2013 many es models e g invest use approaches where predictions are based on one dimensional proxies often land use or land cover rather than simulating the dynamic processes underlying the many ecosystem functions particularly for modelling carbon these simplified static models do not quantify important carbon fluxes for tropical wetlands particularly som turnover poc and doc e g barquín et al 2015 while es mapping models do capture important functions in the provision of wetland es using simplified well understood hydrological relationships especially water balance and sediment dynamics their applicability to organic soils remain limited due to a lack of data for parameterisation lilly et al 2002 the strength of es mapping models is their ability to examine the impact of land use change on es through scenario exploration sharp et al 2016 vigerstol and aukema 2011 however model specifications often mean that important dynamics of ecosystem functions are not captured e g the use of annual reporting periods misses important seasonal variability in ecological functions important for understanding climate change implications on wetland es 4 2 inventory approaches to es inventory and matrix type approaches are based on identification of ecological and environmental characteristics to predict es using one dimensional relationships e g burkhard et al 2012 müller and burkhard 2012 tropical wetland assessments are based on detailed fieldwork for a range of applications including wetland es assessment wet ecoservice developed and applied in south africa kotze et al 2008 and wetland ecosystem service protocol wespus adamus 2011 and wetland uses the national wetland information system nwis for uganda henninger and landsberg 2009 inventories develop detailed descriptions of localised wetland functions due to unique environmental characteristics requiring extensive field observation for small wetland areas often limited to one point in time e g namaalwa et al 2013 this approach provides limited predictive power for scaling up the assessment findings or to provide insight into the role of resource management on es through which to understand alternative management practices and climate change impacts inventories are often conducted at the wetland level or hydro geomorphic unit areas of similar hydrology and geomorphology and as such do not provide relevant information to evaluate environmental effects at the field and plot level where changes in land management practices can be realised there has been an international movement towards developing country specific national wetland inventories however many have been developed in isolation compounding problems for modelling rather than more coordinated efforts that have typified global campaigns for other natural resource mapping e g forests and soil leemhuis et al 2016 vågen et al 2016 inventories have also failed to generate data useful for modelling due to a lack of standardisation of metrics and comparability across different wetlands sites kotze et al 2008 4 3 greenhouse gas process models process based models of ghg emissions have been widely used to simulate water and carbon and other nutrient cycling from global to field scale gerten et al 2004 sitch et al 2003 smith et al 1997 water cycling models often use a mechanistic approach to simulate water balance based on climate and land cover data and some have incorporated routines of hydrological dynamics for vertical and horizontal water movement in soils carbon models generally use a pool based approach for vegetation and soil components such as biomass plant litter labile and recalcitrant som and models simulate the role of external environmental variables in modifying transition rates between different carbon pools smith et al 2010 a wide range of models combine different water and carbon cycling processes with important differences in the design and structure including the number of soil layers and carbon pools representations of hydrology and calculation of photosynthesis and productivity many widely used ghg models have been modified specifically for use on organic or waterlogged soils to capture important wetland sub processes by including routines for dynamic water table levels and ch4 gaseous transport pathways e g lpj why me wania et al 2009 wetland dndc li et al 2004 ecosse smith et al 2010 see farmer et al 2011 for a comprehensive review of the applicability of ghg models to tropical peatlands greenhouse gas process models have been developed for application over a wide range of spatial scales and model complexity models such as lpj developed to examine global level water and carbon cycling dynamics and the response of vegetation to climate effects are unsuitable for downscaling to the field scale required to inform management due to approximations within parameterisation e g the use of broad biome plant function types classifications sitch et al 2003 models requiring fewer input variables to parameterise such as the rothc model use easily obtainable field data at a range of data resolutions for parameterisation during initialisation runs outputs of these models are based on the same assumptions when modelling at a range of scales and predictions and are only limited by the quality of the input data rather than model parameterisations and thus may be more easily applied in a tropical wetland context where data is limited the majority of models applied to wetlands have only been developed and used in boreal and temperate regions dommain et al 2011 wania et al 2010 2009 although recently some of these models have been successfully applied to tropical wetlands e g hpmtrop kurnianto et al 2015 the complexity and sophistication of the process model varies considerably with direct implications of data needs for model parameterisation and initialisation some models use a spin up period where the model is parameterised using historical climate data to establish a steady state of carbon cycling in the soil however this approach poses problems for peatlands as they are characterised by natural long term carbon accumulation and so may underestimate co2 uptake sulman et al 2012 model input variables and fixed parameters may not be easy to measure and data needed to evaluate models particularly long term data may not be available in tropical wetlands vereecken et al 2016 4 4 catchment and hydrological transport models catchment models are widely used to investigate hydrology and soil interactions through simulating infiltration runoff base flow and pollutant loading hydrological models have been developed for a spectrum of spatial and temporal scales lumped models aggregate processes into specific hydrological units i e sub catchment or wetland complex while distributed models are spatially explicit similarly event based models are used to identify storm and flood peaks compared to continuous models that capture base flows singh and woolhiser 2002 model design and structure has direct implications for understanding spatial and temporal resolution of ecological functions and subsequent es distributed and semi distributed catchment models such as swat and vic are widely used to study water soil and vegetation interactions on water cycling gao et al 2009 gassman et al 2007 liang et al 1994 despite wide recognition that hydraulic conductivity and storage properties of wetland soils differ largely from mineral soils these models often struggle to capture the role of organic soils in catchment hydrology particularly the variability due to water table depth and its impact on water storage and movement bengtson and padmanabhan 1999 whitfield et al 2009 catchment modelling is often performed at large scales regional to global level to examine policy and climate change impact to es rather than field specific drivers van roosmalen et al 2009 the development of modular structures means models are able to simulate additional ecological functions including wetland hydrology plant and crop growth and carbon cycling although they require increasing model complexity data and modelling expertise vigerstol and aukema 2011 these models are able to capture spatial relationships in a landscape e g using routine models for determining water flows and are increasingly drawing upon remote sensing data for model inputs e g wateraccounting van eekelen et al 2015 applications in tropical wetlands include hydro pedological modelling of surface and ground water interactions to predict monthly water table depths e g simgro wösten et al 2008 lumped models often based on bucket models or reservoirs are used to estimate water storage of water bodies and discharge rates based on detailed geomorphological data on reservoir size and climate data these models have been developed to include a range of processes linked to water cycling and storage functions including evapotranspiration infiltration surface runoff and subsurface flow model applications include the study of the effects of agricultural drainage e g drainmod skaggs et al 2012 climate change impacts on temperate peats e g wetscape johnson et al 2010 werner et al 2013 and water quality e g wetmod cetin et al 2001 the site specific nature of lumped models means they are generally applied at a small scale and calibrated models are not spatially transferable without renewed parameterisation limiting their application parameterisation of catchment and hydrological models generally requires long term data records of water flows and water quality from gauging stations although newer model approaches are improving predictions for un gauged catchments van emmerik et al 2015 wagener et al 2004 4 5 remote sensing and spatial models increasing availability of satellite imagery and spatial data sets has seen a rapid expansion of new techniques for interpreting data for ecological assessment relevant for examining wetland es by mapping biophysical parameters e g soil moisture vegetation type leaf area index petter et al 2013 remote sensing applications for wetlands have attempted to map global extents of wetlands distribution and examine land use at local scale fluet chouinard et al 2015 lehner and döll 2004 mwita et al 2013 sakané et al 2011 remote sensing of wetland functions has been limited particularly in african wetlands where the effects of soil moisture canopy and ecosystem structure on vegetation spectral reflectance variability remain poorly understood adam et al 2010 ozesmi and bauer 2002 a number of studies have been successful in classifying african wetland vegetation when combining field work with remote sensing data adam and mutanga 2009 fuller et al 1998 rudiyanto et al 2015 the combination of spatial datasets and standardised field measures has demonstrated a useful tool of digitally mapping natural resource extent and condition to provide input for modelling terrestrial ecosystem functions and services such as digital soil mapping for hydrological carbon and crop yield functions mendonça santos et al 2010 minasny and mcbratney 2016 tavares wahren et al 2016 vågen et al 2016 winowiecki et al 2015 due to the high heterogeneity of wetlands using proximal measures to identify key properties and functions such as using plant physiological status to estimate water table may provide insight on the complex relationships between ecosystem properties functions and es such approaches are being explored in temperate and boreal wetlands couwenberg et al 2011 5 discussion a significant challenge for assessing es is how to appropriately and transparently incorporate a number of ecological functions acting at different spatial and temporal scales while retaining the operational simplicity desired by decision makers bagstad et al 2013 vigerstol and aukema 2011 we analyse a range of environmental modelling approaches to determine the suitability of existing tools for use in wetland es assessments and support wetland management for poverty alleviation and climate action wetland es assessments require understanding of the multiple impacts of changes to vegetation soil and hydrology under land use and climate change scenarios table 1 we identify 21 ecological processes contributing to eight functions easily modified by anthropogenic influences in the production of just three wetland es key to rural poverty alleviation and climate change dynamics table 2 large complexities exist for modelling wetland es at the field and plot level due to specific wetland properties and high heterogeneity within and between wetlands the reviewed models and tools capture a mixed selection of wetland properties processes and functions relevant to the creation of wetland es and show a spectrum of sensitivity for capturing spatial and temporal dynamics and anthropogenic influences tables 3 and 4 commonly used models for es assessments are often not appropriate for wetlands due to a lack of data for parameterisation owing to unique wetland properties and oversimplification of specialised wetland ecological functions this is particularly evident for modelling carbon cycling where complex interactions exist between wetland hydrology soils and vegetation these models are often driven by land use change and have limited ability to examine changes within land use class management practices such as the impact of increasing fertiliser use these models often simulate a number of ecological functions in isolation and fail to provide insight into interactions between ecological functions and their effects on es inventory type approaches although widely used have limited ability to make predictive estimates of es under management and climate change scenarios further many inventory systems have not been designed with modelling objectives in mind limiting the use of data for modelling purposes a number of process based models have been modified explicitly to capture important wetland functions however most are confined to a very limited number of ecological functions lack dynamic simulation of important features particularly the hydrological interaction between surface and ground water and upstream downstream spatial interactions and face severe data limitations to parameterise and evaluate model outputs data limitations increase rapidly if a number of models are required to capture multiple es each with different input data specifications while catchment models capture spatial and temporal dynamics they are widely run at large scales where the importance of wetland functions due to the small areal extent of wetlands compared to catchment size is lost and poorly capture the role of field scale land management practices by contrast non distributed hydrological models are developed as site specific and subsequently not widely transferable the increasing availability of remote sensing data provides valuable insight into current environmental properties and ecological processes however attention is needed to develop models and tools that allow the use of remote sensing data to simulate ecological functions and the creation of es we need to ask how remote sensing products for example the modis global terrestrial evapotranspiration product mu et al 2013 can be used to inform understanding of future climate or land use changes while there has been much development in the recognition and discussion of wetland es from general es assessments millennium ecosystem asssessment 2005 to wetland specific es maltby and acreman 2011 and then on to economic valuation of wetland es russi et al 2013 this evolution has been mostly directed through the advancement of frameworks and has successfully engaged and been taken up by a range of stakeholders in the policy research and practitioner environments our review suggests there is a wealth of knowledge in the literature about many elements of wetland social ecological systems particularly in terms of biophysical processes however there is less understanding about multiple interactions between ecosystem processes functions and es particularly social ecological dynamics that drive these complex systems we suggest that this is reflected in current models and tools available for wetland es assessments no one model that we reviewed was able to capture the range of ecological functions and anthropogenic influences to predict the three wetland es table 4 shows that many models have been developed that capture a small number of closely related ecological processes in varying detail rather than a greater range of functions in the past model development has focused upon a single function e g soil erosion and while a number of models have made good progress in shifting focus towards modelling functions to services environmental modelling has been dominated by a focus on regulating services particularly carbon cycling and hydrology the range of functions for which there are models is a limited narrow field and there is a tendency for models to focus on a few of these better known ecological functions in regard to one es the universal soil loss equation is a good example of modelling a single function capturing multiple simplified processes coded into the model terms soil erosion is not a es on its own and understanding is needed as to how soil erosion links to crop production water flow and water purification services model specialisation presents a challenge for understanding the interactions between different es and increases data requirements for es assessments model specialisation could be tackled through a concerted effort to build integrated modelling approaches across biological disciplines such as ghg hydrology and nutrient cycling modelling communities a solution may be a modular approach for combining different model types to create function based models however the complexity underlying ecosystem functions and processes has tended to result in the development of increasingly computational models creating a variety of issues including difficulty in obtaining an intuitive understanding of model dynamics non identifiable or un measurable model parameters and a diversity of model outputs that are not easily comparable due to differing model spatial and temporal scales e g the use of daily or annual reporting periods care will be required to avoid overly complex models streamline data requirements and provide easily comparable and interpretable outputs that inform tropical and non tropical wetland management reducing model complexity and data needs will be challenging particularly given the need to maintain important model behaviour and predictive capacity however there are successful examples of combining climate change hydrological processes and ecological responses of wetlands acreman et al 2009 model development is lagging behind our understanding of the conceptual framework of es which will hinder progress in understanding these complex systems in order to improve understanding of socio ecological systems around wetland es there is a need for models to examine a much wider breadth of ecological functions their interactions and impacts on es rather than increasing complexity and specialisation of modelling focused on understanding ecosystem processes increasingly es tools and models are being applied at the landscape scale to assess a broader range of es egoh et al 2008 nelson et al 2009 due to the weak representation of wetland functions this is likely to result in the poor prediction of wetland es and subsequently may lead to under valuation of wetland es although temperate and boreal wetlands have been more widely studied modelling of wetland es across the globe faces large data limitations for parameterisation and evaluation particularly for modelling multiple es especially in tropical wetlands many process models have not yet been fully applied in wetland areas due to data limitations and these models usually only examine a very narrow range of functions rather than the full suite of wetland es which is ultimately required farmer et al 2011 because of this interdisciplinary understanding of how wetland processes work is required across a range of different conditions but there is very little in the literature about the differences in wetland processes and functions in different geographies wetland management requires scarce data compatible models to enhance the understanding of complex interactions and feedbacks between wetland ecological functions anthropogenic influences and es to inform management data on tropical wetlands is patchy and where available is not suitable for modelling purposes as it is often based on localised ranking and scoring systems from inventories such as in henninger and landsberg 2009 namaalwa et al 2013 little attention has been given to the design of data generation integration with existing and increasing data sources i e remote sensing and flow into further application i e modelling while many models remain flexible to a range of spatial and temporal scales efforts are limited by available data for parameterisation particularly lack of empirical studies to draw data from or limited application and evaluation of theoretically derived relationships between environmental factors the evaluation of models often requires long term data or datasets such as carbon exchange or water discharge and loadings which are not readily available in tropical regions particularly east africa consideration must also be given to uses and users of model outputs particularly the aim of the model as this has a number of implications for the type of model selected for example the scale of the model is dependent on the intended beneficiaries land managers i e local community groups farmers or local government require understanding of management options at the field to landscape scale particularly in terms of spatial interactions and trade offs due to different land management techniques while policy makers require subnational application of models to create understanding of future alternative states of wetland systems to inform policy development and planning uncertainty and accuracy concerns of model forecasts that have not been extensively evaluated will hinder the use of models in both these management applications ensemble modelling techniques running different models and comparing the outputs developed in climate and weather forecasting are increasingly being used in environmental modelling to understand precision and uncertainty of models through inter model comparison and are increasingly being used in es assessments araujo and new 2007 marmion et al 2009 ensemble modelling will require standardised datasets with common protocols data and model outputs for comparison which is currently challenged by the diversity of inputs and outputs used by individual models in the absence of traditional datasets and recognising the role of anthropogenic influences in shaping es there is a need to integrate a range of knowledge sources in modelling efforts by combining data with other information sources such as expert and stakeholder opinion to capture knowledge and human interpretation of es 6 conclusions while wetlands are hugely important due to the es they provide to the global climate and rural poor in east africa there remains limited understanding and integrated knowledge of important social ecological system dynamics there is an urgent need to understand spatial and temporal trade offs and the influence of wetland land management practices on es particularly as the value of climate regulating services wetlands provide is increasingly realised we identify 21 ecological processes contributing to eight ecosystem functions heavily modified by anthropogenic influences in the production of only three es water climate and food a number of approaches have been used to study wetland es but these face limitations es mapping models are overly simplistic for interactions between wetland ecological functions inventories have limited predictive power process based modelling of wetland functions is limited by data availability and narrow range of modelled functions and catchment models are inappropriate for informing field scale management decisions while there has been a large shift in the environmental research discourse towards es much of the progress in understanding social ecological systems dynamics has been to forward existing knowledge through frameworks modelling which is a hugely important tool for furthering knowledge about systems dynamics currently lags behind this evolution with a focus upon a narrow range of one or two functions largely linked to regulating es our understanding of interactions between multiple ecological functions is still limited particularly in regard to provisioning services and supporting functions like biodiversity and soil fertility there is a need for integration of modelling disciplines to focus upon the interactions of ecological functions in the production of multiple es to evolve our understanding of complex social ecological systems while tropical wetlands remain poorly studied compared to temperate and boreal wetlands there is an urgent need for assessments of multiple wetland es to inform decision making at local and larger scales for land managers and policymakers this is nowhere more important than in regions such as east africa where societies are vulnerable to poverty and climate variability and underpinned by agrarian ecological based economies acknowledgments this work alternative carbon investments in ecosystems for poverty alleviation alter grant no ne k010441 1 was funded with support from the ecosystem services for poverty alleviation espa programme the espa programme is funded by the department for international development dfid the economic and social research council esrc and the natural environment research council nerc appendix a supplementary data the following is the supplementary data related to this article supplementary data supplementary data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 022 
