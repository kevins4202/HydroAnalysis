index,text
23935,the sea ice component of a regional high resolution ocean model is improved with particular attention to accurate representation of the salinity budget for the coupled system the impact of this improvement is shown first using a one dimensional test and then the realistic model simulation of the eastern bering sea for the relatively high sea ice extent winter of 2009 10 improvements to the model ice ocean salt flux parameterization are demonstrated by comparison of model shelf salinity fields with observations from moorings and ctds as polynya regions can strongly influence winter salinity distributions on the bering sea shelf model ice concentrations are compared to satellite estimates for several regions areas with a tendency to exhibit higher than average winter open water area include st lawrence island and the southern coast of the chukotka peninsula a new methodology is proposed analyzing the evolution of the salinity distribution with the aid of salt flux tracers that track occurrence of brine and meltwater separately these demonstrate how cold winters on the bering sea shelf are characterized by a freshening and stratifying of the mid to outer shelf and an increase of salinity on the inner shelf use of an ice age tracer in the model reveals an anticyclonic circulation of sea ice in mid winter as the bering slope current and anadyr current recirculate ice that has been transported south southwestward by the predominant winds the characteristics of coastal polynya regions are quantitatively compared and a model transect extending from the st lawrence polynya to the shelf break is analyzed to help illustrate the temporal and spatial variability of stratification and circulation occurring as the sea ice advances and retreats over the winter season 1 background the eastern bering sea shelf is the largest shelf sea in the world south of the arctic ocean from north to south it spans nearly 1300 km from the bering strait where the shelf exchanges waters with the arctic ocean to unimak pass of the aleutian island arc where direct exchange with the pacific ocean occurs in the zonal direction from the alaskan coastline to the shelf break the shelf is nearly as wide as it is long extending over 900 km in some places as the only ocean exchange between the pacific and arctic with arctic sea ice rapidly in retreat the northern bering sea shelf is becoming part of an increasingly alluring trade route between asia and northern european countries but beyond its potential future importance to shipping this vast coastal shelf system already supports some of the largest commercial fisheries in the world while also supporting diverse ecosystems that provide critical habitat for a wide variety of marine mammals and sea birds sea ice plays an important role in the habitat structure of the bering sea in all seasons despite forming typically in late fall and melting in the spring hunt and stabeno 2002 stabeno et al 2012a b in cold years the sea ice spreads southward covering nearly the entire shelf area even in moderate years spring melt on the northern shelf contributes significantly to the stratification in the summer season ladd and stabeno 2012 stabeno et al 2012a but in some recent winters such as in 2018 and 2019 there was little sea ice at all beyond shallow portions of some of the northernmost coastlines the melting of sea ice regularly leads to the formation of a bottom cold pool over the central and southern shelf that constrained walleye pollock and pacific cod populations thorson et al 2020 years without sea ice and consequently without the cold pool have led to northward migration of those species causing arctic cod to disappear from the northern bering sea shelf where previously they were in abundance there was significantly more sea ice in the winter of 2020 than in the previous two years but over the prior decade the overall trend was negative pressing questions persist as to the rate of change and amount of variability to expect in the coming years part of the answer to these questions depend on understanding the ice melt cycle in the bering sea and the impact of variability of this on shelf stratification and circulation since the stratification in this area is largely defined by salinity this becomes a question of how salt becomes redistributed over the shelf in the winter months the salinity field on the broad shallow bering sea shelf can change dramatically in cold winters when sea ice growth is extensive in late fall the cross shore salinity gradient is directed offshore driven by freshwater coastal influx from rivers and entrainment of saltier bering sea basin water along the shelf break but over the winter ice production and brine rejection tend to be highest close to the northern bering sea coastlines due to prevailing wind patterns much of the sea ice produced in these regions in cold years is advected to the mid or outer shelf before it melts the net effect of the winter variability can be a large scale redistribution of shelf salinity over the winter season salinity gradients also develop at smaller spatial scales as the particulars of wind direction and coastline orientation determine regions of enhanced and reduced ice production both along the continental coastlines of alaska and russia and those of the shelf islands such as st lawrence st mathew nunivak to locate these and other geographic features specified in the manuscript refer to fig 1 a observational studies have led to a variety of insights related to salinity redistribution on the bering sea shelf the overall pattern of an ice conveyor belt ice formation in the northern bering sea southward transport by prevailing northerly winds and subsequent melt on the southern and outer shelf were presented by pease 1980 the role of polynyas has long been recognized as well schumacher et al 1983 stringer and groves 1991 more recent observations related to the redistribution of salinity include those of danielson et al 2006 who examined mid shelf circulation south of st lawrence island with 14 years of mooring data and oceanic drifters finding that brine rejection from the polynya competed with westward advection of fresher presumably riverine influenced water leading to high variability in the region and no clear signature of a dense water plume sullivan et al 2014 studied the evolving relationship between sea ice and water column structure using ice cores satellite data and four moorings distributed over shelf along the 70 m isobath noting latitudinal differences and emphasizing the role melt dynamics played in determining subsequent stratification not surprisingly there are many challenges to collecting comprehensive hydrographic information over an area as expansive and heterogeneous as the eastern bering sea shelf particularly in winter numerical circulation models have been a valuable tool for estimating the ocean structure and dynamics on the bering sea shelf over larger spatial and temporal scales than could not be achieved observationally most commonly for the bering sea coupled ice ocean models have been used to study interannual variability clement et al 2005 examined interannual variability in transports over the northern shelf over a 23 year period and clement kinney et al 2009 examined shelf slope exchange on a similar time scale variability in the southwesterly transport of sea ice over a 39 year period has been studied zhang et al 2010 along with the related issue of year to year variability in the cold pool extent over the same period zhang et al 2012 cheng et al 2014 examined nearly 100 years of a climate model output finding that high ice extent years led to more saline water in the northern bering sea and fresher water to the south and on the outer shelf kawai et al 2018 used a coupled atmosphere ocean ice model to explore the correlation between sea surface salinity on the northwestern portion of the bering sea shelf and arctic sea surface heights here again the focus was on interannual variability utilizing 56 years of model simulation these projects have led to a much better understanding of the large year to year changes observed in the bering sea but due to the computational costs of such lengthy calculations over a large area they have necessarily neglected a careful examination of the accuracy of their solutions at higher temporal and spatial resolutions that can be relevant to navigation and fisheries applications the approach being taken in this study which is a continuation of our earlier work durski et al 2016 mauch et al 2018 durski and kurapov 2019 is to develop and refine the model performance through careful inspection of the simulation quality for a particular year on time scales of days to weeks rather than years and on spatial scales of tens of kilometers rather than hundreds throughout this work we have found that model refinements emerge as a result of the close evaluation representing genuine physical improvement rather than tuning in part 1 of this study durski and kurapov 2019 refinements were made to the sea ice component of a coupled ice ocean circulation model in order to improve the model skill at capturing the seasonal advance and retreat of the sea ice in the bering sea comparisons were performed for the winter of 2009 2010 both because of the availability of observational datasets and because it in turn followed on a study of ice free circulation for the summer of 2009 durski et al 2016 the model succeeded in capturing the timing evolution and movement of the eastern bering sea ice cover it also reproduced coastal polynyas with reasonable timing and areal extent in this study focus is turned to the ocean structure below the ice and in particular to the evolution of the shelf salinity observational comparisons are made with moorings ctd data and satellite products spatial and temporal distributions of polynyas and their associated brine rejection are examined in much greater detail than has been presented previously novel model tracers are used to track where melting brine injection occurs and where these altered water parcels advect over the winter season the high model temporal and spatial resolution also allows examination of the evolution of both the ice thermodynamics and the underlying stratification and circulation on a transect that extends from polynya to shelf break ice circulation is examined through the use of an ice age tracer offering a perspective beyond that provided by the traditional conveyor belt model as the focus of this study is on salinity distributions a necessary requirement is that the model accurately estimates the salt fluxes associated with sea ice formation and melt as will be discussed below the formulation in the ice model used in part 1 of this study was inadequate for these purposes because it deviated significantly from conservation of salinity over the freeze melt cycle typically the principle concern in salt flux parameterizations for coupled ice ocean models is to accurately estimate ice salinity and the flushing of brine channels during spring melt because these features can play a very significant role in accurately representing the ice dynamics and thermodynamics but they may not be commensurate with accurate representation of the ocean salinity beneath here we replace the ice ocean salt flux parameterization used in part 1 with a straightforward much more conservative scheme and demonstrate the improvement for the ocean state estimate it offers descriptions of the modeling and the observations used in this study are laid out in section 2 this includes a short discussion of changes to the sea ice model surface salt flux parameterization detailed explanations of these changes can be found in appendix a and a one dimensional case study to demonstrate the effect of those changes is presented in appendix b section 3 compares the coupled model with satellite mooring and profiler data for 2009 10 the year that this study focuses on the existing data comes primarily from broadly spaced shelf moorings targeted at capturing the large scale temporal and spatial patterns and tightly spaced bering strait moorings useful primarily for estimating exchanges with the arctic the objective here is to first demonstrate that our high resolution model reasonably reproduces the observable fields and then use it as a tool for elaborating on the ocean structure and dynamics in the many places where the observations do not reach in section 4 model results focused on the winter redistribution of salinity and the role of polynyas in that process are presented and analyzed this is followed by a summary discussion in section 5 2 methods and data much of the numerical model setup and some of the satellite data analysis in this study inherits directly from part 1 of the study durski and kurapov 2019 a very similar model setup was used for a study on summer circulation in the eastern bering sea durski et al 2016 and a study of transport through the eastern aleutian islands mauch et al 2018 a brief overview of the set up and differences from the previous model setups are described in this section for further details please refer to the earlier works 2 1 the roms numerical model setup 2 1 1 roms base configuration simulations are performed using the regional ocean modeling system roms http www myroms org in a model domain that spans the region zonally from 178 e to 157 w and meridionally from approximately 50 n to 66 4 n the model horizontal grid spacing is approximately 2 km 45 terrain following levels are used in the vertical direction the model includes tides atmospheric forcing from the north american regional reanalysis narr mesinger et al 2006 open boundary conditions from a global hycom solution climatological freshwater inflows from the yukon kuskokwim and the anadyr rivers are added distributed over depth and horizontally as point sources over the grid points along the coastlines in the vicinities of the river mouths in the previous publications the model ocean fields were initialized on june 1 2009 using outputs from the 0 08 resolution navy global model hycom glb au 0 08 chassignet et al 2007 http www hycom org for the bering sea basin melded with a bestmas regional simulation solution zhang et al 2012 2010 on the shelf however in analyzing fall shelf salinity fields for this study it was found that the salinity initialization provided for the summer fall simulation left unrealistically high salinity waters on the inner to mid shelf along much of the alaskan coast north of nunivak island in order to correct for this bering shelf mooring data to be described more in section 2 4 was used to estimate the june 2009 shelf average salinity in ice free areas inshore of the 60 m isobath the new initialization generated was identical to the original initialization in durski et al 2016 for june 1 2009 other than the adjustment of the salinity inshore of the 60 m isobath along the alaskan coast to a depth uniform value of 31 1 this was smoothly melded to the previously generated initial salinity field farther offshore with this new initialization a new summer fall ice free spin up simulation was performed to generate the november 1 initialization file for the series of winter simulations discussed here these ran through july 2010 2 1 2 roms tracer fields several tracer fields are utilized in this study for diagnostic purposes in order to track the migration of ice across the bering sea shelf an age tracer was added to the ice model similar to that in harder and lemke 2013 the age tracer obeys the prognostic equation 1 d a i h i d t h i a i m a x h i t t h 0 where a i represents the average age of the ice within a cell h i the cell averaged ice thickness and d d t the total derivative computed using two dimensional ice velocity in this equation m a x h i t t h 0 denotes the portion of the increase in ice volume due to thermodynamic processes only the second term in 1 acts to reduce the ice age in a grid cell when new ice forms there is an assumption here that ice is well mixed within a grid cell such that melting does not affect the age of the ice in a cell while in actuality the most recently formed ice is likely the earliest to melt tracers are also added to the ocean model to aid in identifying the distribution of brine injection and meltwater the accumulation transport and mixing of each are represented by 2 d p d t z k s p z 3 d p d t z k s p z 4 d p d t z k s p z where the surface boundary condition on p p and p are m a x f s 0 m i n f s 0 and f s respectively with f s being the surface salinity flux each of these tracer fields is initialized with a uniform value of zero 2 1 3 roms ice model configurations the ice model used in this study originated in a branch version of roms as an implementation by budgell 2005 it is a single category ice model based on thermodynamics by mellor and kantha 1989 and elastic viscous plastic evp rheology hunke and dukowicz 1997 hunke 2001 a series of modifications of that model were presented and discussed in part 1 of this study leading to a drastic improvement in the prediction of the ice fraction concentration as compared to the satellite data here we focus on simulations that use the full set of modifications presented there both thermodynamic and dynamic the experiment labeled s dyn in part 1 forms the basis here but is modified further in this study the previous s dyn sea ice model configuration is referred to as m p1 dyn additional changes were made to the sea ice model in order to more accurately simulate changes in salinity on the bering sea shelf the first alteration was to correct a coding error inherited from the version used as a starting point for part 1 this error caused an overestimate of the salinity flux associated with ice formation in open water portions of grid cells by double counting a negative surface heat flux as contributing to both surface ice production w ao and frazil ice production w fr correcting this error led to more reasonable salinity fluxes in polynya regions where ice concentration was low but sea ice production was high another modification involved the formulation of the molecular sublayer salinity parameter s ms which can have a large influence in the model on ice production and melt rates and on the relationship between these rates and the surface salinity flux the formulation for s ms in part 1 of this study which was inherited from earlier roms implementations was found to be inconsistent with that presented in the mellor and kantha 1989 paper used as the reference for the ice thermodynamics but contributed to good predictions of shelf sea ice concentration details of the two estimates of s ms the one in the inherited roms code and the one by mellor and kantha are described in appendix a in sensitivity studies as described below we included simulations that utilize the original mellor and kantha formulation referred to as m mkorig the most essential change to the sea ice model in this part of the study was replacing the salt flux parameterization with a more conservative form the original scheme which is explained in more detail in appendix a produced significant net changes in shelf averaged salinity over a seasonal freeze melt cycle that could not be accounted for by lateral exchanges with the basin or arctic rather the salt flux during a freezing process could differ significantly from the freshwater flux during a comparable melting process a very simple way to ensure that the brine rejection during freezing equals the freshwater equivalent negative salt flux during melting is to make the salt flux due to ice growth and melt a linear function of the thermodynamic rate of change of ice mass in a grid cell to this end we consider a salt flux parameterization in which the difference between surface ocean salinity and sea ice salinity is constant and uniform as a check on conservation 5 f s i d h i dt δ s where δ s is set to a value close to the difference between the specified sea ice salinity s i 3 2 and the shelf averaged ocean surface salinity δ s 31 5 3 2 28 3 the rate of change of sea ice cell averaged thickness in a grid cell is 6 d h i dt c i w io w ai 1 c i w ao w fr where c i is the ice concentration and the w variables indicate rates of ice production or melt positive for production at the ice ocean w io atmosphere ice w ai and atmosphere ocean w ao interfaces along with frazil ice production w fr simulations with this salt flux will be referred to as m conss while this representation is conservative in the sense described above the brine flux is not proportional to the salinity of the seawater being frozen while the sea ice salinity in this model is fixed to allow for this dependence another scheme is also considered that retains dependence on surface ocean salinity s so albeit with a possible loss of conservation 7 f s i d h i dt s so s i this will be referred to as m surfs this representation is a typical starting point for modern sea ice models that provide much more complex representations of sea ice brine and melt fluxes tartinville et al 2001 in such models that may focus on longer time scales and more sophisticated sea ice dynamics ice salinity is often allowed to vary temporally and spatially both horizontally and vertically within the ice brine rejection may happen over a period of weeks as ice cools sea ice flushing and flooding process may be parameterized and evolution of the brine channels may even be considered griewank and notz 2015 vancoppenolle et al 2009 while these processes are all relevant to the bering sea seasonal ice here we continue with the approach of limited complexity if 5 were used in a model simulation over a closed domain it would be guaranteed that the domain averaged salinity before the freezing season is equal to that after all the ice is melted in contrast 7 could lead to a net change in salinity integrated over a domain if for example ice formed in fresher water is transported to a region with relatively higher surface salinity to melt something like that typically happens on the bering sea shelf nonetheless we will find that the model using the m surfs parameterization reproduces shelf salinity observations for the winter of 2009 10 reasonably well section 3 without deviating significantly from case m conss the full surface salinity flux in a grid cell also incorporates evaporation and precipitation the portion of surface runoff that is associated with melting snow or precipitation is considered freshwater in this model giving the total salt flux in a grid cell as 8 f s f s i c i r off s so 1 c i e p where r off is the rate of surface runoff excluding surface ice melt and e and p are rates of evaporation and precipitation over the ice free portion of the grid cell respectively more details on the differences between simulations with m p1 dyn m mkorig m conss and m surfs are presented in appendix b where the parameterizations are compared in a one dimensional setting representative of the central bering sea shelf in the winter of 2009 10 this one dimensional study provides confidence that using a salt flux parameterization such as m conss or m surfs in the full eastern bering sea model domain should capture the redistribution of salinity over the winter season due to ice formation without significantly altering the total salt content of the shelf comparisons of full model domain solutions using m conss and m surfs with shelf salinity observations below will demonstrate similarly despite the differences in the salt flux parameterization between m p1 dyn and m conss or m surfs the new salinity parameterizations did not significantly alter the evolution of the sea ice concentration field in the eastern bering sea model compared to the model solutions discussed in part 1 section 2 3 because the formulation for the salinity in the molecular sublayer between ice and ocean s ms was left unaltered and effectively decoupled from the ice to ocean salt flux estimation s ms plays a driving role in the melting and freezing processes but can also lead to a loss of salt conservation depending on how it enters the salt flux parameterization see appendix a for the details 2 2 the global hycom benchmark the roms results for salinity and temperature presented here will be compared to results from the global navy hycom solution glb u 0 08 0 08 horizontal resolution that was mentioned previously with regard to the open boundary conditions the purpose of this comparison is to illustrate regional shortcomings that this global ocean model product widely used for oceanographic analyses on multiyear time scales may present although hycom includes the option to couple to the multicomponent cice sea ice model hunke et al 2019 for these publicly available simulations the sea ice is represented through a simpler energy loan model that follows from semtner 1976 hycom uses data assimilation to correct sequentially discrepancies between the model state and observations the data assimilated includes satellite observations of surface ocean temperature and sea surface height in situ temperature and salinity profiles and satellite estimates of sea ice concentration 2 3 satellite data the simulation results are compared with two satellite products primarily the 5 km resolution product based on the arctic radiation and turbulence interaction study sea ice algorithm asi spreen et al 2008 is used this algorithm uses the higher frequency 89 mhz channel of the advanced microwave scanning radiometer to improve spatial resolution at the cost of needing to use weather filters to correct for greater cloud interference in this waveband while it captures features well it can exhibit rapid fluctuations in ice concentration due to signal interference from clouds the lower resolution operational sea surface temperature and sea ice analysis ostia donlon et al 2012 product with a nominal resolution of 12 5 km is also used for shelf averaged comparisons the daily composite ice concentration estimates it provides are based on using data from several special sensor microwave imager ssm i satellites processed for eumetsat andersen et al 2007 because ostia often over smooths the concentration fields it fails to resolve polynyas and so is not considered for model comparison in that portion of this study in order to demonstrate the consistency of the ice concentration results in part 1 with those in this part of the study following the changes to the ice model thermodynamics discussed in section 2 1 3 fig 5 a from part 1 is replicated here with comparison to the new model solutions fig 2 the fraction of the shelf covered in sea ice evolved quite similarly to the earlier results and agreed well with the asi and ostia satellite estimates this occurs even though the shelf averaged ice thickness for m surfs for example is approximately 0 18 m less than for m p1 dyn at the winter peak ice extent in mid march 2010 the change in ice thickness was largely due to fixing the coding error associated with over estimating the magnitude of the atmospheric heat flux mentioned above 2 4 field measurements as part of the nsf best bsierp program a configuration of 9 subsurface moorings was deployed on the central bering sea shelf from 2008 through 2010 danielson et al 2012a locations are depicted in fig 1 a the moorings were arranged in approximately 3 radial lines extending from the alaskan coast near 61 n such that there were northern n central c and southern s mooring positions close to the 25 m 40 m and 55 m isobaths fig 1 a the s25 mooring data was not operational during this study period but we include this location for model model comparison near bottom salinity data from these moorings were used in this study both for model validation and for the june 2009 initialization of the summer spin up simulation multi decadal time series of temperature and salinity are available from moorings that span bering strait woodgate 2018 woodgate et al 2015 of the four mooring locations that are examined here two a1e and a1w are in the channel west of the diomedes one is in the eastern channel a2 and one is near the alaskan coast a4 fig 1 b near bottom salinity measurements from these four moorings are used in this study for model validation and analysis in each case water depth was approximately 50 m with instrumentation set 10 20 m above bottom as mentioned in part 1 of this study current measurements at these same locations were used to adjust the model northern boundary condition to more closely match observed transport out of the bering sea velocity data will also be used here to help analyze the observed changes in salinity at the mooring locations the current temperature and salinity datasets for these moorings are publicly available woodgate and weingartner 2015 woodgate 2011 from march 7th through april 1 2010 85 ctd profiles were collected from the coast guard cutter polar sea covering the region immediately south of st lawrence island and extending out approximately along the 100 m isobath past both st matthew island and the pribilofs fig 1 c abbreviated psea this data was also collected as part of the nsf best bsierp program depth averaged temperature and salinity data from this cruise were used here for model validation this dataset is also publicly available stabeno et al 2011 3 model observation comparison in this section results primarily from the m surfs eastern bering sea simulation are compared to both observations and the global hycom output the global hycom solution is included in some of these comparisons not because there is an expectation that it should give comparable results but rather because it is an often referenced publicly available model output 3 1 salinity eight of the nine nsf best shelf moorings all but s25 fig 1 a recorded near bottom salinity time series through the winter of 2010 and have proven useful in describing the spatial and temporal shelf variability danielson et al 2012b here in order to focus on the change in salinity caused by ice formation comparisons are made at the near bottom mooring measurement positions fig 3 in order to exclude systematic differences in the initial fields the observations and model outputs are displayed as the salinity difference in each case relative to their time averaged value for the first week of november 2009 at each mooring position the water column salinization due to ice formation is most apparent at the northern mooring locations n55 n40 and n25 salinity increases earliest and by the greatest amount at the most shoreward mooring locations the roms model captures this pattern well hycom which drives ice variability in part by assimilation of the ice concentration data exhibits significantly smaller salinity changes there is no documentation available to us to suggest that the energy loan ice model used for these simulations includes a salinity flux parameterization intraseasonal variability in the observations is highest along the central mooring line c55 c40 and c25 danielson et al 2012b note that salinity on the shelf reaches the annual minimum in late fall and that the position of the salinity minimum tends to gradually relocate offshore over the course of the winter season but that pattern does not hold for the c40 mooring in the winter of 2010 the observations suggest that low salinity water arrives at c40 in mid november of 2009 but that afterwards the salinity intermittently increases by as much as 2 between february and the first week of april 2010 roms displays a similar salinization but without the late fall freshening that appears in the observations the abrupt changes in salinity at the mooring locations in the model and presumably in the observations are due to frontal movements at times these fronts are associated with freshwater discharge from the yukon river strong salinization from nearshore ice formation or a combination of both in the model these fronts meander and shift in response to wind events this likely accounts for the model tendency to produce abrupt changes in salinity at the mooring location that correlate in timing with the observation but not necessarily in magnitude along the southern line of moorings the m surfs solution overestimates the winter salinity changes at s40 the central mooring the overestimate may be due to an underestimate of the amount and timing of freshwater input to the shelf over the months preceding the winter that results from using climatological monthly freshwater inflow this would also explain the model missing lower salinity signals on the northern two mooring lines between november 2009 and february 2010 closer to shore at s25 the roms solution shows a winter increase in salinity that is likely reasonable given the proximity of this station to c25 although no observations are available at s25 for comparison the global hycom solution shows little indication that brine rejection from sea ice formation causes an increase in salinity at any of the mooring locations salinity observations are also available for the polynya region south of st lawrence island and for midshelf locations roughly along the 100 m isobath from the psea ship survey that took place in april 2010 eighty five profiles of temperature and salinity were collected at locations as depicted in fig 1 c fig 4 a b shows the depth averaged salinity and temperature as functions of the distance from st lawrence island measured from the midpoint of the island southern shore the figure exhibits the expected pattern of cold salty water under the polynya and warmer fresher water farther offshore where sea ice has begun melting there is significant variability in both the observations and in the roms results in the 200 km region closest to the island indicative of the complex flow structure that develops as a result of intermittent brine rejection events but the change in salinity and temperature as a function of distance from the island in the model is consistent with the observations as was the case with the mooring data there is little evidence of salinization of the water column in the hycom fields it is unclear whether this is associated with the rate of ice formation the salt flux parameterization or a failure of the model to resolve the polynya dynamics sea ice fields from hycom were not available for analysis salinity and velocity data from the bering strait moorings woodgate 2018 woodgate et al 2015 fig 1 b provide information on the exchange with the arctic when flow is northward out of the bering sea the water passing the western mooring locations a1e and a1w typically has characteristics of anadyr current water and the shelf waters along the chukotka coast fig 5 the water characteristics at the eastern moorings a2 and a4 during northward transport resemble those of the eastern bering sea inner shelf and the alaska coastal current woodgate et al 2015 variability in the salinity at the two western moorings a1w and a1e is less than 1 between january and july 2010 when the current through the strait is predominantly northward fig 5 a and b this low variability is in part due to the outflow of anadyr current waters that have origins at the shelf break where ice formation is limited and that transit the shelf relatively quickly the roms and hycom model results are consistent with this in general when the transport is southward through the straight november through early december 2009 the data from a1w and a1e moorings show significant freshening of the water column this feature depends on transport of fresher water from the arctic it is not captured in the roms solution in part due to an underestimate of the southward transport based on velocity comparison at the mooring locations during this period and in part due to a lack of a fresher water source from the north it may be that early season ice is advecting out of the arctic during this period and melting in the vicinity of the strait variability in salinity between january and june 2010 is relatively larger east of the diomedes at the eastern mooring locations a2 and a4 fig 5 c d positive fluctuations during periods of strong northward transport such as in late february 2010 and mid april 2010 are associated with northward transport of saltier water from the northern bering sea shelf primarily east of st lawrence island these peaks in salinity are likely associated with northward flow of water from regions of high sea ice production as mentioned above in discussing salinity comparison at other sites the roms simulation captures salinization of the water column by sea ice formation more accurately than the global hycom solution on the northern bering sea shelf this may explain the better match with observations of the roms solution at a2 and a4 3 2 ice concentration in part 1 of this study comparisons were made between the model ice concentration and satellite estimates results were presented that demonstrated the model skill in capturing the overall advance and retreat of sea ice as well as the appearance of the st lawrence polynya in order to further analyze the model skill in capturing polynyas the full season of asi daily satellite based estimates of ice concentration spreen et al 2008 is examined and additional coastal regions with intermittent reductions in c i are identified during the winter of 2010 these include fig 6 a 1 the southern side of st lawrence island 2 the southern end of the chukotka peninsula 3 the southwestern facing portion of the seward peninsula 4 the eastern facing alaskan coast in the vicinity of unalakleet norton sound and 5 the northwestward facing alaskan coast region near the yukon river outflow the open water fraction is defined as in part 1 9 o w 1 a reg j 1 n reg 1 c j t δ x δ y where a reg is the total area of the region the model estimate of open water fraction on average exceeds the satellite estimate for the northeastern bering sea shelf overall fig 6 g and for most of the five small regions fig 6 b e but the discrepancy is generally small early in the season the model tends to underestimate ice coverage likely due to a warm bias in sst but throughout much of the winter changes in ice concentration in the model in the polynya regions are well correlated with the satellite observations in some cases rapid change in satellite estimates of o w may reflect limitations of the processing algorithm as consecutive daily composites exhibit differences that appear to occur more rapidly than can be accounted for by sea ice advection or surface heat loss the model tends to show more persistent periods of high o w than the satellite in most regions fig 6 b c d e it is conceivable that after polynyas form cloud formation over the open water may be misclassified as ice reducing the observation based o w prematurely landfast or grounded sea ice plays a role in some of the discrepancies in o w between the model and observation along the alaskan coast satellite imagery indicates that grounded sea ice persists along portions of the alaskan coast for up to 2 weeks at a time esp in march and april of 2010 fig 7 a a supplemental animation showing the distribution of ice concentration from the model and from the satellite estimate is included to demonstrate the persistence of this feature sea ice concentration model satellite comparison polynyas only develop at the offshore edges of these features consequently reducing o w in the alaskan nearshore region the model however does not include a mechanism to account for grounding or landfast ice as a result the modeled polynya opens up right at the coast fig 7 b consistently the model estimates of o w exceed the satellite estimates in these regions fig 6 e f when landfast ice is present the model underestimates o w in january and february in the yukon delta region region 5 fig 6 f for a related reason close examination of the satellite derived ice concentration fields in this area shows that a promontory of grounded or landfast ice at times extends offshore in the vicinity of the river delta fig 7 c blocking the transport of ice from the northeast into this region several times during these months when winds shift to north northeasterly the promontory of landfast ice blocks southward transport of sea ice leaving a band of ice free water along the coast in the shadow of the promontory lacking the ice promontory sea ice quickly covers this polynya region in the model fig 7 d leading to underestimates of o w fig 6 f the appearance of polynyas in the bering sea coastal regions is typically correlated with winds in order to examine the relationship between wind direction and enhanced open water area in each of the regions delineated in fig 6 a polar histograms are generated fig 8 each histogram shows counts of 3 hour averaged model forcing winds greater than 3 m s between december 1 2009 and april 15th 2010 sorted into directional bins gray bars blue bars count how often o w based on the satellite estimate is greater than the december april average for the wind events in each bar segment note for this analysis the o w estimates are sampled with a 24 h time lag likewise red bars count occurrences of greater than average o w based on the model for those wind events over the winter winds are predominantly to the south southwestover much of the bering sea shelf fig 8 consequently coastlines perpendicular to this orientation such as the south side of st lawrence island and the chukotka peninsula have the highest count of larger than average open water area fig 6 b c for both these regions the observations and the model consistently produce higher than average o w with greater than 3 m s winds in these directions this is indicated in fig 8 a and b by the nearly identical size of the blue and red bars that are no shorter than the gray bars for each wind orientation along the seward peninsula area 3 in fig 6 a there are numerous occurrences of above average o w associated with southward winds fig 8 c despite this coastline being aligned primarily west southwest this appears in both the observations and the model presumably the enhanced open water results due to blocking of sea ice transport by the bering strait promontory just to the north of area 3 see fig 6 a there are notable differences between the modeled and observed response to winds along the regions of the alaskan coast near unalakleet and the yukon river areas 4 an 5 in figs 6 a 8 d e the reason for the discrepancies near the yukon river mouth relate to the formation of a landfast ice promontory at the northern end of this region as discussed above and depicted in fig 7 c the differences in the unalakleet region also result from inaccurate representation of the sea ice physics nearshore the occasions when the model exhibits greater than average o w but the observations do not are mostly associated with south southwestward winds during these periods the model allows the sea ice to be flushed out to the west but the satellite imagery suggests it accumulates becoming landfast along the northward facing coast between areas 4 and 5 4 oceanic variability over the bering sea shelf in winter 4 1 redistribution of shelf salinity the efforts to arrive at a reasonably accurate representation of the ice ocean salt flux were made here in order to make an accurate assessment of the overall salinity changes on the bering sea shelf in winter numerical models with some demonstrated fidelity for matching observations are uniquely qualified for such analysis due to the spatial and temporal extent of the information they can provide for the bering sea shelf several competing fluxes determine the overall salinity changes these include the brine melt cycle associated with sea ice formation the exchange of water and ice with the arctic and bering sea basin freshwater riverine inflows particularly in late spring and precipitation and evaporation in relatively cold winters including the winter of 2009 10 the factor that likely leads to the largest changes in shelf averaged salinity is ice formation and melt so accurate representation of this component is essential for a meaningful analysis of how salinity is redistributed over the winter in the simulations conducted for this study the estimates of the seasonal change in shelf averaged salinity shoreward from the 200 m isobath between november 2009 and june 2010 differ notably depending on the salt flux parameterization used fig 9 a with both m conss and m surfs the shelf average salinity returns by the end of winter nearly to the same value it had at the start of the season of course when open boundaries are present even with a conservative scheme such as m conss the overall dilution due to melt will not match the brine added during ice formation if ice enters or exits the domain in unequal amounts so this result suggests that if there were net import or export of ice in the winter of 2009 10 the effect on salinity was compensated for by a comparable lateral exchange of salt the similarity in the curves for the m conss and m surfs cases further suggests that the net effect of sea surface salinity variations on the overall salt flux was not large other simulations m p1 dyn m mkorig and in the hycom solution showed notable changes in shelf averaged salinity over the winter season fig 9 a for m p1 dyn and m mkorig these changes cannot be attributed to boundary flux differences because the circulation and ice volumes were similar across all the roms simulations so they are necessarily related to the surface salinity flux differences there is a net positive change over the season with m mkorig similar to the one dimensional experiment in appendix b m p1 dyn also exhibits a net positive salinity change here as mentioned earlier it was this overestimate in shelf salinities compared with observations that led to considering the m conss and m surfs formulations the global hycom solution exhibits a shelf averaged decrease in salinity over the winter fig 9 a suggesting that the model may underestimate brine rejection and or over dilute the shelf during ice melt it may also have notably different lateral boundary salinity fluxes compared to roms e g across the 200 m isobath in the hycom solution there is no distinction between the pattern in average salinity on the outer shelf between the 75 m and 200 m depth contours and the inner shelf inshore of the 75 m isobath fig 9 b and c in both subdomains the hycom average salinity increases during freeze up between november and april and decreases during melt afterwards this is not the case for any of the roms simulations for which much of the ice production occurs on the inner shelf leading to a strong local increase in salinity during freeze up fig 9 c in contrast on the outer shelf in the roms solutions where there is less local ice production but significant influx of sea ice that subsequently melts there is net freshening throughout much of the winter fig 9 b in all roms cases the inner shelf finishes the winter season saltier than it began and the outer shelf becomes fresher while lateral fluxes contribute in both cases the primary driver is the net amount of sea ice formation and melt occurring locally the effect of including surface salinity in the salt flux parameterization can be compared by looking at the small differences between the m conss and m surfs solutions in the m conss case the rate of salinization or freshening is solely a function of the rate of ice production or melt while this allows the scheme to be conservative over a closed volume the tendency given the choice of the coefficient in eq 5 is to withdraw more salt than m surfs during melting where the surface salinity is greater than 31 5 and to add more salt than m surfs during freezing where the salinity is less than 31 5 since the surface salinity is generally greater than 31 5 offshore of the 75 m isobath m conss freshens offshore waters more than m surfs as depicted in fig 9 b though the effect is small as the m surfs solution most accurately balances the objective of salt conservation with accurate representation of local changes in salinity it will be used exclusively in the model analysis that follows fig 10 displays a map of the depth averaged salinity distribution on the shelf at the onset of the winter season november 1 2009 panel a at the time of maximum sea ice extent april 10th 2010 panel b and once all the sea ice on the shelf has melted july 1 2010 panel c a link is provided to an animation of bottom salinity to elaborate on what is displayed in this figure bottom salinity animation before the winter season the salinity pattern is dominated by the effect of freshwater input primarily from the yukon river in the east and saline water transport between the bering sea slope and the bering strait via the anadyr current in the west by the time of maximum ice extent in early april the shallow coastal regions exhibit the highest salinity on the shelf while the mid shelf persists with the lowest depth averaged salinity the dividing line between regions of net salinity increase and reduction lies roughly along the 75 m isobath fig 10 e and f and remains at this position throughout the season of ice retreat the outer shelf ends the season with an average salinity decrease of approximately 0 1 while the inner shelf ends up approximately 0 3 saltier fig 9 b c the high salinity of the waters along the eastern bering sea coastlines result in part from local ice formation but also reflect the typically north to south circulation near the shorelines in particular saltier water from strong brine rejection along the coast of the seward peninsula circulates southward along the alaskan coast as far south as the kuskokwim river in the model see fig 10 e similarly saltier water produced in the model in kresta bay on the chukotka peninsula near 179w 66n advects southward past anadyr bay averaged over the winter currents close to the surface fig 11 a are strongly influenced by the direction of winds and sea ice movement but at mid depth and lower fig 11 b c and d an overall anticyclonic circulation can be recognized through much of the winter driven by the bering slope current on the outer shelf and the anadyr current on the northwestern shelf in the model this pattern eventually leads to some of the mid shelf meltwater advecting back north in anadyr basin towards the high salinity coastal regions of the northern inner shelf the simulations show southward flow from the surface through mid depth in shpanberg strait between the east side of st lawrence island and the alaskan coastline clement et al 2005 reported in a modeling study of interannual variability in transport on the bering sea shelf that typically this flow is northward but tended to shift southward in years in which winds were predominantly from the east rather than the north the winter of 2009 10 exhibited mostly northeasterly winds perhaps supporting the correlation they observed and the possibility that this winter differed in a notable way from the historical average danielson et al 2012a noted from current meter observations between july 2008 and july 2010 that depth averaged transports in shpanberg strait were southward when winds were northwesterly in agreement with our modeling results they also examined model simulations for earlier years identifying december of 1999 as exhibiting similar circulation clement et al noted reversal in their model as well during the same period the complex spatial distributions of brine and melt water that develop during winter due to the variable weather and the intermittency of polynyas can be analyzed using the passive tracers 2 4 these represent the contributions of the positive and negative components of the surface salinity flux and the net surface salinity flux depth integrals of these fields are displayed in fig 12 a supplemental animation of these fields overlaid with atmospheric wind forcing and sea ice concentration has been included as a supplement depth integrated brine and melt tracer animation these fields allow us to easily identify regions where the highest brine concentration waters or the most meltwater accumulate and where they form for example there is very high brine concentration water south of nunivak island 60n inshore of the 50 m isobath that originates farther north from along the alaskan coastal region near the yukon river outflow fig 12 d as the water advects southward it increases in salinity until it reaches warmer and potentially saltier surface waters where melt exceeds ice formation fig 12 f i this occurs in the model approximately 100 km shoreward of the ice edge by late march the brine water produced in the st lawrence polynya region 63n similarly at times advects south and at other times westward but accumulates primarily in the region between st lawrence and st matthew islands kresta bay is a region of high brine production as well waters from kresta bay flow south along the russian coastline through most of the winter occasionally being entrained into the anadyr current late in the season however strong inflow into the domain along the russian coast flushes the coastal region south of kresta bay causing the accumulated brine water displacement to the east of the bay along the southward facing portion of the chukotka peninsula before entraining into the anadyr current fig 12 g j one region where brine accumulates close to where it initially fluxed into the water column is in the vicinity of nome along the southern coastline of the seward peninsula in norton sound this portion of the alaskan coast does not appear to flush brine water as effectively as portions of the coast fig 11 to the south perhaps due to its geography meltwater accumulated primarily on the mid to outer shelf between the 50 and 200 m isobaths through april of 2010 fig 12 middle column some of the outer shelf meltwater recirculates north of st lawrence island carried by the anadyr current while some is entrained off the shelf into eddies of the bering slope current fig 12 k by late april there is melting inshore of the 50 m isobath as well while the surface salt flux tracer analysis identifies how brine and meltwater are distributed on the shelf it can also provide insight into how the preexisting fall shelf salinity distribution was rearranged over the winter by subtracting the salt flux tracer p from the model salinity field s s p it is possible to explore how circulation and boundary fluxes alone redistribute salinity over the winter season in particular we can examine how the low salinity nearshore water that results from the yukon and kuskokwim river outflows is advected and stirred laterally fig 13 displays s at four different times over the season a supplemental animation has also been included to aid in the visualization s animation for bottom model layer in mid december fig 12 a the distribution appears quite similar to the initial state fig 10 a with fresher water hugging the coast from norton sound to the kuskokwim outflow but a strong inflow of saltier arctic water tends to flush much of this fresher water southward to the vicinity of nunivak island by the beginning of february 2010 fig 13 b and farther south by mid march fig 13 c thus the increased salinity apparent onshore of the 50 m isobath in the eastern bering sea in fig 10 f results from the combined effects of ice formation and the arctic saline water influx the low s water that is pushed southward and some also eastward into norton sound which was the freshest on the shelf at the start of the season has experienced some of the strongest brine injection of any water on the shelf fig 12 d as this water advects and stirs laterally in portions it increases in density due to ice formation while other portions may decrease due to meltwater freshening the resultant filamentous patch of low s water visible to the south of nunivak island in panels c and d of fig 13 results from a complex evolving circulation as horizontal density gradients are altered by the lateral and surface processes apparent in the animation 4 2 circulation of the sea ice transport of the seasonal sea ice that lies atop the coastal waters of the bering sea shelf is driven primarily by winds but also by the ocean currents the ice age tracer 1 is a convenient tool for tracking how sea ice moves over the course of the winter as much of the earliest ice is produced at the northernmost latitudes of the domain along coastlines or is advected into the domain through the bering strait in our implementation the no gradient boundary condition is used for the ice at the northern boundary in chukchi sea since no information about the ice age is obtained for this ice some of which will later be advected in the bering sea through the bering strait all ice advecting into the domain through the bering strait is assumed to have formed on november 1 2009 recall this was the initial date for the series of the coupled tests presented here fig 14 displays maps of ice age at six times over the winter season a supplemental animation has been included ice age animation throughout the winter the average age of ice tends to be lowest in regions of active ice formation and export such as polynyas ice ages as it is transported generally southward to the outer shelf while new ice formation can only reduce the average age of a parcel of ice ice transported to the warmer waters of the outer shelf ages steadily as a result some of the oldest ice accumulates at the ice edge this is consistent with the established idea of a sea ice conveyor belt from north to south in the bering sea pease 1980 the pattern in fig 14 c d and e indicates northwestward advection of this surviving ice along the shelf break with the slope current the anticyclonic circulation is nearly closed when some of this old ice is transported northward with the anadyr current late in the season as the ice edge retreats from the shelf break the alongslope transport of the ice is terminated the oldest remaining ice appears to be that advected in through the bering strait much earlier in the season and accumulated in the vicinity of nunivak island fig 14 f the apparent influx of arctic sea ice indicated by the oldest seasonal ice is one of the most prominent features in the ice age distribution in general winter influx of sea ice from the arctic has been thought to be low except for in anomalous years babb et al 2013 but based on their analysis of satellite derived ice drift velocity in the chuchki sea in the vicinity of bering strait babb et al 2013 quantify winter 2009 10 as the season with the second strongest seasonal average southward transport in that region over the 33 years of data analyzed so the presence of an influx in the model is not surprising the pattern can also be interpreted as demonstrating the distribution of the longest surviving ice which is likely to originate in the northernmost extreme of the model domain 4 3 comparison of polynya regions as demonstrated in fig 8 polynyas on the bering sea shelf tend to be associated with offshore winds they are regions of enhanced sea ice production and brine injection so they play an important role in determining how shelf salinity changes over the winter season satellite based measurements now provide reasonably accurate estimates of the changes in open water area associated with these features but cannot yet provide accurate information about their ice production rates nor the potentially differing dynamics and thermodynamics that lead to their formation in section 3 2 the modeled ice concentration was compared to satellite estimates for the 5 polynya regions depicted in fig 6 a fig 14 provides additional time series information on these 5 regions quantifying their relative contributions to the sea ice ocean salt flux the cumulative salt flux per unit area s f and the cumulative ice production per unit area m i are shown in fig 15 a each of these regions produced ice on a per unit area basis at a higher rate than the northern shelf average also depicted in fig 15 a the rate of productivity in the st lawrence polynya region was the highest followed by the seward peninsula region the chukotka region and the two alaskan coastal regions in all the regions displayed in the figure more ice was produced than eventually melted indicating that there was a net export of ice from each s f for each of these regions followed the same pattern as m i as is expected with this salt flux parameterization ice formation in fresher water leads to less salt flux consequently regions such as the alaskan coast near the yukon outflow exhibit lower rates of salt flux relative to ice production early in the winter an interesting difference that emerges from this analysis is that the chukotka peninsula box exhibits stronger negative heat flux than either the st lawrence or seward peninsula boxes yet produces less ice and fluxes less salt per unit area polynyas can be classified as predominantly latent or sensible heat polynyas latent heat polynyas arise when the mechanical action of the wind sweeps sea ice out of a region exposing freezing temperature water that rapidly begins to produce new ice the phase change being a latent heat process sensible heat polynyas on the other hand exist where surface waters exceed the freezing temperature allowing for the persistence of open water despite a strong heat flux to the atmosphere the exchange with the atmosphere primarily alters the temperature but not the phase of the surface water a sensible heat process the chukotka coastal region has a larger negative net surface heat flux out of the ocean per unit area than any of the other regions fig 15 b in part because this region stays more open has smaller ice concentration throughout the season than the other regions fig 6 but the associated salt flux falls below that of the st lawrence or seward regions because this portion of the chukotka coast has some characteristics of a sensible heat polynya intermittently throughout the winter the model predicts the chukotka region to exhibit above freezing surface temperatures fig 15 d which reduce brine rejection and ice formation this does not indicate that the chukotka coast polynya should be classified as an entirely sensible heat polynya however as it presents characteristics of latent heat polynyas as well with a coastline roughly parallel to the south coast of st lawrence island it experiences comparable winds and events of open water coincide with winds to the southwest see fig 8 so the presence of the warm water alone did not produce polynyas off the chukotka coast in the winter of 2009 10 but rather inhibited ice formation when the polynya formed due to the wind action the source of the warm water can be traced to the anadyr current fig 16 displays several fields from a model transect between the alaskan and russian coasts green dashed line in fig 1 a the winter averaged velocity normal to the transect shows north northeastward flow over the gulf of anadyr along with southward flow close to the russian coast and on the eastern shelf fig 16 b although the warm water transport in this bottom layer fig 16 d is intermittent due to variation in winds and the bering sea arctic pressure gradient that sometimes reduce the anadyr current strength the presence of above freezing temperature bottom water on the transect throughout the winter fig 16 e suggests that vertical mixing fig 16 b remains limited enough to isolate this bottom layer from the surface boundary layer this is supported by the persistence of stratification in winter in the deepest portion of this transect panel c the core of warmer bottom water aligns with the northward current panel d and is centered approximately half a degree to the west of the minima in surface salinity panel c it is likely that this low surface salinity water that is a product of ice melt contributes to maintaining the stratification that allows the warm bottom waters to propagate northward the wintertime northward flow of the anadyr current for earlier winters is documented in the literature muench et al 1988 overland et al 1996 muench et al have noted the transport of above freezing temperature water in the current from the bering sea slope to anadyr strait in the winter of 1985 although we have not found recent observations to corroborate the sensible heat characteristics of the modeled chukotka region polynya the persistence of a stratified water column in the anadyr basin through the winter has been mentioned coachman and shigaev 1992 it is possible that the model overestimates bering slope bottom water temperatures and or underestimates heat dissipating mixing processes as the current progresses northward but given that the model resolves the flow well produces an anadyr current of reasonable magnitude and shows good agreement with the bering sea shelf observations in other regions it is plausible that the model representation in this region is at least qualitatively correct 4 4 changing characteristics of the mid to outer shelf over the winter season typical cold winters on the bering sea shelf are characterized by high sea ice production in the northern coastal regions advection of ice southward over the broad shelf where ice production rates are lower and an outer shelf region where warm waters from the bering sea basin melt a large portion of the sea ice produced over the shelf in this section ice and water properties on a model transect from the southern side of st lawrence island to the shelf break north of zhemchug canyon model transect 1 in fig 1 a are used to elucidate some of the winter characteristics of the shelf several of the variables are presented in fig 18 as functions of the coordinate along the section and time these analyses are supported by the plots of the individual ice production terms in fig 17 ice is present on this transect from late november 2009 through mid may 2010 indicated by the overlaid contours in figs 17 and 18 the first appearance of ice in the fall occurs as a result of advection visible in fig 17 as the presence of ice without any contributions from thermodynamic production terms at the start of the season subsequently thermodynamic production begins particularly strongly in the polynya region by early december based on the model the st lawrence polynya delineated in the figure by lower ice concentrations near the zero of the vertical axis occupies a region of approximately 0 to 50 km from the island coast between the months of december 2009 and april 2010 intermittent disappearances of the feature over the course of the winter coincide with wind reversals that drive sea ice shoreward northward the principle mechanism for ice generation in the polynya region is heat loss from the ocean to the atmosphere indicated by large magnitudes in the w ao and w fr terms fig 18 a and d where ice concentration increases offshore downwind of the polynya region sea ice growth occurs mostly through w io though only at a rate approximately one quarter as large as in the more open water of the polynya as the ice edge advances offshore largely due to wind driven advection it eventually encounters surface waters containing enough heat to cause ice melt through the negative w io term see fig 17 b in late december through mid january this occurs approximately 200 km south of st lawrence island as the shelf continues to cool the melt transition boundary advances farther south behind the ice edge such that for over much of the winter an approximately 200 km wide band of melting ice is present as incoming solar radiation increases and surface air temperatures rise melt commences over the entire transect in mid april 2010 the melt is driven both by the atmosphere ocean heat exchange in the open water portions of each grid cell through the w ao term fig 17 a and at the ice surface through atmosphere ice exchange indicated by w ai figs 18 17 c however from mid april through the disappearance of all sea ice on this transect the model predicts that ice formation continues along the underside of the ice at the ice ocean interface as the sub freezing temperature ice continues to extract heat from the ocean based on the net thermodynamic ice production in fig 17 e the transect can be separated into 4 regions in mid winter these include a region of net ice production brine rejection a region of approximately balanced ice production and melt a melt region and open water south of the ice edge beneath the ice in the net ice production region the seasonal impact of the brine rejection is visible in the sea surface salinity distribution and stratification on the transect fig 18 a and b a salinity gradient develops south of st lawrence island over the first part of the winter season due to ice formation both in the polynya and farther offshore such that by mid january the high salinity extends 150 km offshore from the island along the transect through february march and april this gradient tends to intensify and advance southward but remains within the net ice production region delineated in fig 17 d in this same region high stratification intermittently develops as the brine water sinks and is transported laterally away from the island the variability in the stratification reflects the influence of varying winds and ice production rates strong winds tend to mix away the vertical stratification but because they are often oriented to expand the area of the polynya they can intensify ice formation brine rejection and consequently horizontal density gradients as the winds relax patches of higher stratification often appear as depicted in fig 18 b as the horizontal density gradients relax but other mechanisms restratify the water column in this region as well the reappearance of lower salinity surface water within 100 km of the st lawrence coast in mid march 2010 coincident with high stratification in the polynya region corresponds to a period when a branch of the relatively fresh surface waters of the anadyr current deflect to the south of the island rather than pass through anadyr strait in late december and early january melting occurs on this transect between 150 and 400 km from the coast fig 17 d contributing to a decrease in mid shelf surface salinity that mostly persists throughout the ice covered season fig 18 a from february to april an approximately 60 km wide band of unstratified water persists in the midshelf region where neither the presence of ice nor melt is large enough to suppress vertical mixing as the ice advances farther from shore the melt region moves farther south generating stratification over a 200 km wide swath that extends approximately 200 km into the ice cover from the ice edge by mid march fig 18 b offshore of the ice edge the maximum stratification in the water column is significantly lower than under the ice as the influence of wind mixing is much greater by mid february the ice edge on this transect is approaching the shelf break where the warmer waters of the bering sea basin and the strong northwestward flow of the slope current prohibit any farther advance of the ice for the mid winter season months of february and march the shelf temperature and salinity structure remain relatively persistent vertical sections of potential temperature salinity potential density vertical mixing coefficient and velocity normal to the transect reinforce the description of the dynamics above fig 19 the density gradients are very largely controlled by salinity across the entire transect warmer and saltier water underlies colder and fresher water in the broad melt region over the outer shelf the polynya region is increasingly saltier towards the coast but uniformly cold mixing is suppressed most notably over a band approximately 150 km wide in the melt region but is bounded by well mixed water columns both offshore of the ice edge and in the midshelf region the density gradients established in the melt region coincide with a low frequency circulation that transports water in the melt region northwestward consistent with geostrophy in the ice production region closer to the st lawrence coast where the brine induced salinity gradient forms a weaker southeasterly flow develops also consistent with geostrophy fig 18 c shows the mid depth relative vorticity normalized by the local coriolis parameter to demonstrate that mesoscale eddy variability beneath the ice coverage is largely suppressed except for locations close to the shoreward polynya and seaward ice edges regardless of stratification however once the ice cover melts and stratification intensifies vigorous eddying motions develop across much of this transect 5 summary in this study we sought to better understand the role of sea ice in the alteration of the eastern bering sea salinity distribution over the course of a winter season for this objective we needed a model that estimated reasonably well the ice coverage the ice movement and the salinity fluxes between the two water phases in part 1 of this study we made thermodynamic and dynamic alterations to a relatively unsophisticated sea ice model and demonstrated the model capability for capturing both the seasonal trend in sea ice coverage over the winter of 2009 10 and the overall movement of the sea ice on shorter time scales here in part 2 the model salt flux parameterization was improved and reasonable representation of polynya regions was demonstrated the model salinity comparisons with mooring and profiler observations were improved significantly by the use of a straightforward salt flux parameterization that depended only on the rate of change of ice mass and the salinity difference between the surface ocean and a fixed sea ice salinity the model reproduced the increase in salinity on the inner shelf particularly in the northern bering sea due to excess ice production relative to melt while the global hycom model failed to reproduce this pattern south of st lawrence island the model captured the scale of change and variability in salinity as a function of distance from the island that was observed in ctd profiles from april 2010 the winter of 2009 10 was a relatively cold year for the bering sea with large sea ice extent under these conditions the model exhibited freshening of the outer shelf approximately offshore the 75 m isobath and salinization of the inner shelf to the extent that the direction of the positive salinity gradient changes from the offshore direction in the fall to shoreward by may before riverine input helps to gradually restore the original pattern over the following summer and fall this evolution results from both a general south westward movement of the sea ice over the season away from the areas where ice formation was strongest and from transport of the fresher nearshore waters offshore and to the south overall the volume averaged salinity on the shelf at the end of the melt season is nearly the same as it was before the onset of ice coverage suggesting that salt and ice exchanges with the arctic and the bering shelf slope were negligible or counterbalancing future studies of years with more limited ice extent will be beneficial for determining if no net change in shelf salinity is typical or if interannual variability might be expected the model produces seasonally averaged anti cyclonic circulation of both sea ice and ocean waters on the bering sea shelf this leads to recirculation of meltwater northward in the anadyr basin and also transport of warm salty bottom water of slope origin in the anadyr current causing the deeper waters of the anadyr basin to remain stratified over much of the winter unfortunately there are few observations available to validate the vertical structure produced by the model in this region coastal polynyas contribute significantly to the redistribution of salinity over the winter season with the polynya on the south side of st lawrence island being the largest contributor but other coastal regions similarly aligned relative to the predominant wind forcing for example along the south side of the chukotka and seward peninsulas also frequently exhibited increased open water interestingly ice production and brine rejection were comparatively low in the chukotka polynya region in the model despite frequent high open water area because of its exposure to the relatively warmer water being transported northward in the anadyr current giving it some characteristics of a sensible heat polynya some discrepancies in nearshore ice coverage between the model and observations were found in the vicinity of the yukon river outflow where the model at times failed to capture the polynya geometry these occurrences often appeared to be associated with times in the observations when ice advection was blocked by promontories of landfast ice future work can focus on including parameterizations of sea ice grounding lemieux et al 2015 to improve model performance along these portions the alaskan coast examination of the water column on a transect from the south side of st lawrence island to the shelf break illustrate the variability in stratification beneath the ice coverage on the outer shelf where ice was melting above warmer and saltier slope influenced water the water column was persistently stratified beneath the ice in a band at times over 200 km wide intermittent stratification also developed due to strong brine rejection in the polynya resulting in patches of stratified water within 200 km of the south side of the island the mid shelf where neither freezing nor melting processes dominated was most frequently unstratified despite the encouraging performance of this coupled model further refinements are warranted one of these is to more accurately represent the influx of sea ice from the arctic ocean in general an ice bridge forms north of bering strait that acts to inhibit the inflow of arctic ice but the model has no mechanism presently to emulate this effect consequently the model may be significantly overestimating this effectively freshwater input to the bering sea shelf with arctic ice a second improvement involves allowing the salinity of the sea ice to vary spatially and temporally sea ice has been found to only gradually reject brine over the first couple weeks after its formation in the bering sea that ice may be advected a significant distance away from where it was formed over that time consequently the model may be somewhat overestimating the brine rejection in polynya regions and underestimating the rejection farther offshore nonetheless even a limited coupled sea ice ocean model such as the one presented here can help to elucidate the rich and changing dynamics of the bering sea shelf credit authorship contribution statement scott m durski conceptualization methodology software writing original draft visualization investigation validation formal analysis data curation alexander l kurapov conceptualization resources writing review editing program administration supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partially supported by noaa united states of america total water initiative and nasa united states of america grant nnx13ad89g satellite data was obtained by the asi data repository https seaice uni bremen de start data archive appendix a adjustment of the salt flux parameterization in the original mellor and kantha paper 1989 referred to here as mk the salt flux f s into the ocean is parameterized to depend on the salinity difference between the near surface ocean s so and the molecular sublayer beneath the ice s ms along with a coefficient that accounts for the surface boundary layer properties a 1 f s k s s z c s z s m s s s o z 0 where c sz is a 2 c s z u τ p rt k 1 l n z z o b s and a 3 b s b z o u τ ν 1 2 s c 2 3 in the equations above p rt is the turbulent prandtl number k is von karman s constant z is depth z o is roughness length and u τ is a friction velocity in mk a 1 must match the flux between the molecular sublayer and the sea ice a 4 f s w o c i r m w s m s s i 1 c i p e s ms where r m w is the rate of runoff from surface melt ponds due to contributions from ice melt snow melt and precipitation here a 5 w o c i w i o 1 c i w a o as noted in appendix b of mk with some approximation a system of equations can be solved to obtain the salt and heat fluxes ice production rates and molecular sublayer temperature and salinity but they conclude that it is numerically simpler to estimate the ice production and loss terms from the heat fluxes determine t m s from s m s f t from the temperature analog to eq 1 and combine eqs a 1 and a 4 to give the molecular sublayer salinity as a 6 s m s c s z s s o w o s i c i r m w s i c s z w o c i r m w 1 c i p e this expression for s m s is applied in the m mkorig model setup discussed above in the roms implementation of this parameterization inherited for use in part 1 of this study a coding error or perhaps some undocumented modifications to eq a 6 had been made such that the molecular sublayer salinity was coded as a 7 s m s c s z s s o w i o s i w a i 0 s i c s z w i o w a i 0 r m w where w a i 0 represents min w a i 0 the total salt flux in a grid cell was estimated as a 8 f s w o c i w ai 0 s ms s i c i r mw s i 1 c i e p s so the model using eqs a 7 and a 8 with the aforementioned coding error uncorrected is referred to as m p1 dyn in the manuscript this formulation is identical to s dyn discussed in part 1 it is not obvious and not documented how this parameterization was determined yet it produces quite reasonable evolution of surface sea ice concentration for the winter of 2009 10 as discussed in part 1 variables w ao and c i are absent so presumably eq a 7 is based on the assumption that the salinity of the molecular sublayer should be a function of the rates of ice growth loss and meltwater runoff only in the ice covered portion of a grid cell also as r m w is often equal to and redundant with w a i when surface melting of ice is occurring this equation implies not unreasonably that their contribution to the molecular sublayer characteristics should be weighted differently than changes due to melting at the ice ocean interface whether m mkorig or m p1 dyn is utilized questions arise regarding conservation of salt over a freeze melt cycle for example due to the dependence on wind stress in c s z the same thermodynamic production of ice will lead to a different salt flux under different wind conditions additionally with m p1 dyn the salinity flux associated with the same production melt of ice will differ depending on which mechanism w ao w io w ai etc it is associated with over a freeze melt season in a closed domain neither formulation assures that the net freshwater flux during the melt season will balance the total brine rejection during freeze up the primary motivation of this part of the study is to evaluate the evolution of the eastern bering sea shelf salinity structure over the winter but salinity fields obtained using m p1 dyn s dyn in part 1 exhibited poor agreement with salinity observations from shelf moorings and ctd profiles for the winter of 2009 10 the alternative of using m mkorig did not prove beneficial either even small changes in s ms significantly altered the evolution of ice concentration in the bering sea so in order to preserve the quality of the solution in terms of ice concentration found in part 1 simple alternative parameterizations for f s are considered here while the coded formulation for s ms a 7 is retained despite the ambiguity of its origins we retain this parameterization in calculating the molecular sublayer temperature and ice temperature evolution but decouple it from the calculation of salt flux in principle formulating a set of simultaneous equations that capture the interdependence of the interfacial salinities temperatures heat fluxes and ice production in a consistent manner may be possible but it is numerically challenging and beyond the scope of this work for the purposes here we opt for simple alternatives to the flux formulation outlined above that reproduce the ice formation melt rates obtained in previous simulations while improving the salt flux estimates and overall salt conservation as presented in 2 1 3 above the total surface salt flux in the principle simulations discussed here is 8 with the ice ocean salt flux being given by 5 for m cons and 7 for m surfs appendix b 1 dimensional test case an idealized test of these salt flux parameterizations is presented here to summarize the impact of the model modifications mentioned above we simulate a seasonal cycle in a spatially uniform doubly periodic domain effectively a 1 dimensional water column simulation representative of the central bering sea shelf the initial conditions and forcing are identical to those used in the full 3 dimensional model at approximately the position of the n55 bering sea shelf mooring on the 55 m isobath 62 n 172 6 w the maximum allowable value of c i is set to 0 97 in these experiments to ensure that lateral ice melt w ao can contribute without this limit the sea ice uniformly thins over the entire grid cell during the melt season and the role of w ao in the salt flux cannot be analyzed simulations are performed using 5 different salt flux parameterizations these include m p1 dyn m mkorig m conss m surfs and a case in which ice formation and melt do not contribute to the surface salt flux m precip f s e p s s o in this case ice still forms and melts comparably to the other four cases but ocean surface salinities change much less in these 1 dimensional simulations c i changes rapidly reaching a maximum by early january while the mass of ice or h i the cell averaged thickness gradually increases into may fig 20 a the ice cover persists longer than in the bering sea simulations for several reasons first by not allowing c i to reach a value of 1 the direct atmosphere ocean heat exchange continues through the winter enhancing sea ice production second the doubly periodic boundary conditions provide no lateral exchange with warmer water as on the actual bering sea shelf and no possibility for the sea ice field to diverge and allow increased heat absorption by the ocean the m p1 dyn idealized simulation produces significantly thicker sea ice and consequently a longer melt season than any of the other cases fig 20 largely due to the code error mentioned above the over estimated cooling of the water column compounded by the maximum set on c i leads to excessive frazil ice accretion the m mkorig case produces more ice than either m conss or m surfs due primarily to higher ice production between mid december 2009 and mid january 2010 during this period the m mkorig estimate of s ms a 6 is significantly closer to the sea surface salinity than that for the cases using eq a 7 this leads to a fresher water column and increased frazil ice production which largely accounts for the difference m precip demonstrates that the net effect of evaporation and precipitation over the season is to slightly freshen the water column fig 20 b the abrupt drop in salinity with the final melting of the ice in late june is due to the flushing of fresh snowmelt from the ice surface the changes in water column salinity due to ice formation and melt are much larger in m p1 dyn the excess ice production associated with the w fr term is not accompanied by a commensurate increase in salt flux so when melting occurs through the w ao w ai and w io terms the freshwater flux exceeds the brine injection causing a net loss of salt in the water column over the winter fig 20 b the m mkorig experiment produces a net increase in salinity over the season this is presumably due in part to an underestimate of s ms during the melt season but it is also likely due to the assumption in eq a 4 that all surface runoff has a salinity of s i an overestimate for the melted snow the net change in salinity in both m conss and m surfs is very similar to the m precip case indicating that these schemes are conserving salinity well over the ice formation melt cycle though indiscernible in fig 20 b s avg for m conss is approximately 0 01 higher than for m surfs at the end of the winter season due to the role of s so in the salt flux parameterization 7 comparison of fig 20 b with the top panel of fig 9 a shows that for the one dimensional study m p1 dyn leads to a net decrease in salinity while in the full eastern bering sea simulation it leads to a net increase on the shelf this results from both the code error and the inconsistencies of that formulation s ms in this formulation which plays a role in determining the magnitude of the salt flux is a function of w io and w ai but not w ao so w ao affects the salt flux associated with freezing or melting differently than the other mechanisms as the contributing mechanisms differ between the full shelf and the idealized 1 dimensional case so too does the error in the net salt flux over the winter season 
23935,the sea ice component of a regional high resolution ocean model is improved with particular attention to accurate representation of the salinity budget for the coupled system the impact of this improvement is shown first using a one dimensional test and then the realistic model simulation of the eastern bering sea for the relatively high sea ice extent winter of 2009 10 improvements to the model ice ocean salt flux parameterization are demonstrated by comparison of model shelf salinity fields with observations from moorings and ctds as polynya regions can strongly influence winter salinity distributions on the bering sea shelf model ice concentrations are compared to satellite estimates for several regions areas with a tendency to exhibit higher than average winter open water area include st lawrence island and the southern coast of the chukotka peninsula a new methodology is proposed analyzing the evolution of the salinity distribution with the aid of salt flux tracers that track occurrence of brine and meltwater separately these demonstrate how cold winters on the bering sea shelf are characterized by a freshening and stratifying of the mid to outer shelf and an increase of salinity on the inner shelf use of an ice age tracer in the model reveals an anticyclonic circulation of sea ice in mid winter as the bering slope current and anadyr current recirculate ice that has been transported south southwestward by the predominant winds the characteristics of coastal polynya regions are quantitatively compared and a model transect extending from the st lawrence polynya to the shelf break is analyzed to help illustrate the temporal and spatial variability of stratification and circulation occurring as the sea ice advances and retreats over the winter season 1 background the eastern bering sea shelf is the largest shelf sea in the world south of the arctic ocean from north to south it spans nearly 1300 km from the bering strait where the shelf exchanges waters with the arctic ocean to unimak pass of the aleutian island arc where direct exchange with the pacific ocean occurs in the zonal direction from the alaskan coastline to the shelf break the shelf is nearly as wide as it is long extending over 900 km in some places as the only ocean exchange between the pacific and arctic with arctic sea ice rapidly in retreat the northern bering sea shelf is becoming part of an increasingly alluring trade route between asia and northern european countries but beyond its potential future importance to shipping this vast coastal shelf system already supports some of the largest commercial fisheries in the world while also supporting diverse ecosystems that provide critical habitat for a wide variety of marine mammals and sea birds sea ice plays an important role in the habitat structure of the bering sea in all seasons despite forming typically in late fall and melting in the spring hunt and stabeno 2002 stabeno et al 2012a b in cold years the sea ice spreads southward covering nearly the entire shelf area even in moderate years spring melt on the northern shelf contributes significantly to the stratification in the summer season ladd and stabeno 2012 stabeno et al 2012a but in some recent winters such as in 2018 and 2019 there was little sea ice at all beyond shallow portions of some of the northernmost coastlines the melting of sea ice regularly leads to the formation of a bottom cold pool over the central and southern shelf that constrained walleye pollock and pacific cod populations thorson et al 2020 years without sea ice and consequently without the cold pool have led to northward migration of those species causing arctic cod to disappear from the northern bering sea shelf where previously they were in abundance there was significantly more sea ice in the winter of 2020 than in the previous two years but over the prior decade the overall trend was negative pressing questions persist as to the rate of change and amount of variability to expect in the coming years part of the answer to these questions depend on understanding the ice melt cycle in the bering sea and the impact of variability of this on shelf stratification and circulation since the stratification in this area is largely defined by salinity this becomes a question of how salt becomes redistributed over the shelf in the winter months the salinity field on the broad shallow bering sea shelf can change dramatically in cold winters when sea ice growth is extensive in late fall the cross shore salinity gradient is directed offshore driven by freshwater coastal influx from rivers and entrainment of saltier bering sea basin water along the shelf break but over the winter ice production and brine rejection tend to be highest close to the northern bering sea coastlines due to prevailing wind patterns much of the sea ice produced in these regions in cold years is advected to the mid or outer shelf before it melts the net effect of the winter variability can be a large scale redistribution of shelf salinity over the winter season salinity gradients also develop at smaller spatial scales as the particulars of wind direction and coastline orientation determine regions of enhanced and reduced ice production both along the continental coastlines of alaska and russia and those of the shelf islands such as st lawrence st mathew nunivak to locate these and other geographic features specified in the manuscript refer to fig 1 a observational studies have led to a variety of insights related to salinity redistribution on the bering sea shelf the overall pattern of an ice conveyor belt ice formation in the northern bering sea southward transport by prevailing northerly winds and subsequent melt on the southern and outer shelf were presented by pease 1980 the role of polynyas has long been recognized as well schumacher et al 1983 stringer and groves 1991 more recent observations related to the redistribution of salinity include those of danielson et al 2006 who examined mid shelf circulation south of st lawrence island with 14 years of mooring data and oceanic drifters finding that brine rejection from the polynya competed with westward advection of fresher presumably riverine influenced water leading to high variability in the region and no clear signature of a dense water plume sullivan et al 2014 studied the evolving relationship between sea ice and water column structure using ice cores satellite data and four moorings distributed over shelf along the 70 m isobath noting latitudinal differences and emphasizing the role melt dynamics played in determining subsequent stratification not surprisingly there are many challenges to collecting comprehensive hydrographic information over an area as expansive and heterogeneous as the eastern bering sea shelf particularly in winter numerical circulation models have been a valuable tool for estimating the ocean structure and dynamics on the bering sea shelf over larger spatial and temporal scales than could not be achieved observationally most commonly for the bering sea coupled ice ocean models have been used to study interannual variability clement et al 2005 examined interannual variability in transports over the northern shelf over a 23 year period and clement kinney et al 2009 examined shelf slope exchange on a similar time scale variability in the southwesterly transport of sea ice over a 39 year period has been studied zhang et al 2010 along with the related issue of year to year variability in the cold pool extent over the same period zhang et al 2012 cheng et al 2014 examined nearly 100 years of a climate model output finding that high ice extent years led to more saline water in the northern bering sea and fresher water to the south and on the outer shelf kawai et al 2018 used a coupled atmosphere ocean ice model to explore the correlation between sea surface salinity on the northwestern portion of the bering sea shelf and arctic sea surface heights here again the focus was on interannual variability utilizing 56 years of model simulation these projects have led to a much better understanding of the large year to year changes observed in the bering sea but due to the computational costs of such lengthy calculations over a large area they have necessarily neglected a careful examination of the accuracy of their solutions at higher temporal and spatial resolutions that can be relevant to navigation and fisheries applications the approach being taken in this study which is a continuation of our earlier work durski et al 2016 mauch et al 2018 durski and kurapov 2019 is to develop and refine the model performance through careful inspection of the simulation quality for a particular year on time scales of days to weeks rather than years and on spatial scales of tens of kilometers rather than hundreds throughout this work we have found that model refinements emerge as a result of the close evaluation representing genuine physical improvement rather than tuning in part 1 of this study durski and kurapov 2019 refinements were made to the sea ice component of a coupled ice ocean circulation model in order to improve the model skill at capturing the seasonal advance and retreat of the sea ice in the bering sea comparisons were performed for the winter of 2009 2010 both because of the availability of observational datasets and because it in turn followed on a study of ice free circulation for the summer of 2009 durski et al 2016 the model succeeded in capturing the timing evolution and movement of the eastern bering sea ice cover it also reproduced coastal polynyas with reasonable timing and areal extent in this study focus is turned to the ocean structure below the ice and in particular to the evolution of the shelf salinity observational comparisons are made with moorings ctd data and satellite products spatial and temporal distributions of polynyas and their associated brine rejection are examined in much greater detail than has been presented previously novel model tracers are used to track where melting brine injection occurs and where these altered water parcels advect over the winter season the high model temporal and spatial resolution also allows examination of the evolution of both the ice thermodynamics and the underlying stratification and circulation on a transect that extends from polynya to shelf break ice circulation is examined through the use of an ice age tracer offering a perspective beyond that provided by the traditional conveyor belt model as the focus of this study is on salinity distributions a necessary requirement is that the model accurately estimates the salt fluxes associated with sea ice formation and melt as will be discussed below the formulation in the ice model used in part 1 of this study was inadequate for these purposes because it deviated significantly from conservation of salinity over the freeze melt cycle typically the principle concern in salt flux parameterizations for coupled ice ocean models is to accurately estimate ice salinity and the flushing of brine channels during spring melt because these features can play a very significant role in accurately representing the ice dynamics and thermodynamics but they may not be commensurate with accurate representation of the ocean salinity beneath here we replace the ice ocean salt flux parameterization used in part 1 with a straightforward much more conservative scheme and demonstrate the improvement for the ocean state estimate it offers descriptions of the modeling and the observations used in this study are laid out in section 2 this includes a short discussion of changes to the sea ice model surface salt flux parameterization detailed explanations of these changes can be found in appendix a and a one dimensional case study to demonstrate the effect of those changes is presented in appendix b section 3 compares the coupled model with satellite mooring and profiler data for 2009 10 the year that this study focuses on the existing data comes primarily from broadly spaced shelf moorings targeted at capturing the large scale temporal and spatial patterns and tightly spaced bering strait moorings useful primarily for estimating exchanges with the arctic the objective here is to first demonstrate that our high resolution model reasonably reproduces the observable fields and then use it as a tool for elaborating on the ocean structure and dynamics in the many places where the observations do not reach in section 4 model results focused on the winter redistribution of salinity and the role of polynyas in that process are presented and analyzed this is followed by a summary discussion in section 5 2 methods and data much of the numerical model setup and some of the satellite data analysis in this study inherits directly from part 1 of the study durski and kurapov 2019 a very similar model setup was used for a study on summer circulation in the eastern bering sea durski et al 2016 and a study of transport through the eastern aleutian islands mauch et al 2018 a brief overview of the set up and differences from the previous model setups are described in this section for further details please refer to the earlier works 2 1 the roms numerical model setup 2 1 1 roms base configuration simulations are performed using the regional ocean modeling system roms http www myroms org in a model domain that spans the region zonally from 178 e to 157 w and meridionally from approximately 50 n to 66 4 n the model horizontal grid spacing is approximately 2 km 45 terrain following levels are used in the vertical direction the model includes tides atmospheric forcing from the north american regional reanalysis narr mesinger et al 2006 open boundary conditions from a global hycom solution climatological freshwater inflows from the yukon kuskokwim and the anadyr rivers are added distributed over depth and horizontally as point sources over the grid points along the coastlines in the vicinities of the river mouths in the previous publications the model ocean fields were initialized on june 1 2009 using outputs from the 0 08 resolution navy global model hycom glb au 0 08 chassignet et al 2007 http www hycom org for the bering sea basin melded with a bestmas regional simulation solution zhang et al 2012 2010 on the shelf however in analyzing fall shelf salinity fields for this study it was found that the salinity initialization provided for the summer fall simulation left unrealistically high salinity waters on the inner to mid shelf along much of the alaskan coast north of nunivak island in order to correct for this bering shelf mooring data to be described more in section 2 4 was used to estimate the june 2009 shelf average salinity in ice free areas inshore of the 60 m isobath the new initialization generated was identical to the original initialization in durski et al 2016 for june 1 2009 other than the adjustment of the salinity inshore of the 60 m isobath along the alaskan coast to a depth uniform value of 31 1 this was smoothly melded to the previously generated initial salinity field farther offshore with this new initialization a new summer fall ice free spin up simulation was performed to generate the november 1 initialization file for the series of winter simulations discussed here these ran through july 2010 2 1 2 roms tracer fields several tracer fields are utilized in this study for diagnostic purposes in order to track the migration of ice across the bering sea shelf an age tracer was added to the ice model similar to that in harder and lemke 2013 the age tracer obeys the prognostic equation 1 d a i h i d t h i a i m a x h i t t h 0 where a i represents the average age of the ice within a cell h i the cell averaged ice thickness and d d t the total derivative computed using two dimensional ice velocity in this equation m a x h i t t h 0 denotes the portion of the increase in ice volume due to thermodynamic processes only the second term in 1 acts to reduce the ice age in a grid cell when new ice forms there is an assumption here that ice is well mixed within a grid cell such that melting does not affect the age of the ice in a cell while in actuality the most recently formed ice is likely the earliest to melt tracers are also added to the ocean model to aid in identifying the distribution of brine injection and meltwater the accumulation transport and mixing of each are represented by 2 d p d t z k s p z 3 d p d t z k s p z 4 d p d t z k s p z where the surface boundary condition on p p and p are m a x f s 0 m i n f s 0 and f s respectively with f s being the surface salinity flux each of these tracer fields is initialized with a uniform value of zero 2 1 3 roms ice model configurations the ice model used in this study originated in a branch version of roms as an implementation by budgell 2005 it is a single category ice model based on thermodynamics by mellor and kantha 1989 and elastic viscous plastic evp rheology hunke and dukowicz 1997 hunke 2001 a series of modifications of that model were presented and discussed in part 1 of this study leading to a drastic improvement in the prediction of the ice fraction concentration as compared to the satellite data here we focus on simulations that use the full set of modifications presented there both thermodynamic and dynamic the experiment labeled s dyn in part 1 forms the basis here but is modified further in this study the previous s dyn sea ice model configuration is referred to as m p1 dyn additional changes were made to the sea ice model in order to more accurately simulate changes in salinity on the bering sea shelf the first alteration was to correct a coding error inherited from the version used as a starting point for part 1 this error caused an overestimate of the salinity flux associated with ice formation in open water portions of grid cells by double counting a negative surface heat flux as contributing to both surface ice production w ao and frazil ice production w fr correcting this error led to more reasonable salinity fluxes in polynya regions where ice concentration was low but sea ice production was high another modification involved the formulation of the molecular sublayer salinity parameter s ms which can have a large influence in the model on ice production and melt rates and on the relationship between these rates and the surface salinity flux the formulation for s ms in part 1 of this study which was inherited from earlier roms implementations was found to be inconsistent with that presented in the mellor and kantha 1989 paper used as the reference for the ice thermodynamics but contributed to good predictions of shelf sea ice concentration details of the two estimates of s ms the one in the inherited roms code and the one by mellor and kantha are described in appendix a in sensitivity studies as described below we included simulations that utilize the original mellor and kantha formulation referred to as m mkorig the most essential change to the sea ice model in this part of the study was replacing the salt flux parameterization with a more conservative form the original scheme which is explained in more detail in appendix a produced significant net changes in shelf averaged salinity over a seasonal freeze melt cycle that could not be accounted for by lateral exchanges with the basin or arctic rather the salt flux during a freezing process could differ significantly from the freshwater flux during a comparable melting process a very simple way to ensure that the brine rejection during freezing equals the freshwater equivalent negative salt flux during melting is to make the salt flux due to ice growth and melt a linear function of the thermodynamic rate of change of ice mass in a grid cell to this end we consider a salt flux parameterization in which the difference between surface ocean salinity and sea ice salinity is constant and uniform as a check on conservation 5 f s i d h i dt δ s where δ s is set to a value close to the difference between the specified sea ice salinity s i 3 2 and the shelf averaged ocean surface salinity δ s 31 5 3 2 28 3 the rate of change of sea ice cell averaged thickness in a grid cell is 6 d h i dt c i w io w ai 1 c i w ao w fr where c i is the ice concentration and the w variables indicate rates of ice production or melt positive for production at the ice ocean w io atmosphere ice w ai and atmosphere ocean w ao interfaces along with frazil ice production w fr simulations with this salt flux will be referred to as m conss while this representation is conservative in the sense described above the brine flux is not proportional to the salinity of the seawater being frozen while the sea ice salinity in this model is fixed to allow for this dependence another scheme is also considered that retains dependence on surface ocean salinity s so albeit with a possible loss of conservation 7 f s i d h i dt s so s i this will be referred to as m surfs this representation is a typical starting point for modern sea ice models that provide much more complex representations of sea ice brine and melt fluxes tartinville et al 2001 in such models that may focus on longer time scales and more sophisticated sea ice dynamics ice salinity is often allowed to vary temporally and spatially both horizontally and vertically within the ice brine rejection may happen over a period of weeks as ice cools sea ice flushing and flooding process may be parameterized and evolution of the brine channels may even be considered griewank and notz 2015 vancoppenolle et al 2009 while these processes are all relevant to the bering sea seasonal ice here we continue with the approach of limited complexity if 5 were used in a model simulation over a closed domain it would be guaranteed that the domain averaged salinity before the freezing season is equal to that after all the ice is melted in contrast 7 could lead to a net change in salinity integrated over a domain if for example ice formed in fresher water is transported to a region with relatively higher surface salinity to melt something like that typically happens on the bering sea shelf nonetheless we will find that the model using the m surfs parameterization reproduces shelf salinity observations for the winter of 2009 10 reasonably well section 3 without deviating significantly from case m conss the full surface salinity flux in a grid cell also incorporates evaporation and precipitation the portion of surface runoff that is associated with melting snow or precipitation is considered freshwater in this model giving the total salt flux in a grid cell as 8 f s f s i c i r off s so 1 c i e p where r off is the rate of surface runoff excluding surface ice melt and e and p are rates of evaporation and precipitation over the ice free portion of the grid cell respectively more details on the differences between simulations with m p1 dyn m mkorig m conss and m surfs are presented in appendix b where the parameterizations are compared in a one dimensional setting representative of the central bering sea shelf in the winter of 2009 10 this one dimensional study provides confidence that using a salt flux parameterization such as m conss or m surfs in the full eastern bering sea model domain should capture the redistribution of salinity over the winter season due to ice formation without significantly altering the total salt content of the shelf comparisons of full model domain solutions using m conss and m surfs with shelf salinity observations below will demonstrate similarly despite the differences in the salt flux parameterization between m p1 dyn and m conss or m surfs the new salinity parameterizations did not significantly alter the evolution of the sea ice concentration field in the eastern bering sea model compared to the model solutions discussed in part 1 section 2 3 because the formulation for the salinity in the molecular sublayer between ice and ocean s ms was left unaltered and effectively decoupled from the ice to ocean salt flux estimation s ms plays a driving role in the melting and freezing processes but can also lead to a loss of salt conservation depending on how it enters the salt flux parameterization see appendix a for the details 2 2 the global hycom benchmark the roms results for salinity and temperature presented here will be compared to results from the global navy hycom solution glb u 0 08 0 08 horizontal resolution that was mentioned previously with regard to the open boundary conditions the purpose of this comparison is to illustrate regional shortcomings that this global ocean model product widely used for oceanographic analyses on multiyear time scales may present although hycom includes the option to couple to the multicomponent cice sea ice model hunke et al 2019 for these publicly available simulations the sea ice is represented through a simpler energy loan model that follows from semtner 1976 hycom uses data assimilation to correct sequentially discrepancies between the model state and observations the data assimilated includes satellite observations of surface ocean temperature and sea surface height in situ temperature and salinity profiles and satellite estimates of sea ice concentration 2 3 satellite data the simulation results are compared with two satellite products primarily the 5 km resolution product based on the arctic radiation and turbulence interaction study sea ice algorithm asi spreen et al 2008 is used this algorithm uses the higher frequency 89 mhz channel of the advanced microwave scanning radiometer to improve spatial resolution at the cost of needing to use weather filters to correct for greater cloud interference in this waveband while it captures features well it can exhibit rapid fluctuations in ice concentration due to signal interference from clouds the lower resolution operational sea surface temperature and sea ice analysis ostia donlon et al 2012 product with a nominal resolution of 12 5 km is also used for shelf averaged comparisons the daily composite ice concentration estimates it provides are based on using data from several special sensor microwave imager ssm i satellites processed for eumetsat andersen et al 2007 because ostia often over smooths the concentration fields it fails to resolve polynyas and so is not considered for model comparison in that portion of this study in order to demonstrate the consistency of the ice concentration results in part 1 with those in this part of the study following the changes to the ice model thermodynamics discussed in section 2 1 3 fig 5 a from part 1 is replicated here with comparison to the new model solutions fig 2 the fraction of the shelf covered in sea ice evolved quite similarly to the earlier results and agreed well with the asi and ostia satellite estimates this occurs even though the shelf averaged ice thickness for m surfs for example is approximately 0 18 m less than for m p1 dyn at the winter peak ice extent in mid march 2010 the change in ice thickness was largely due to fixing the coding error associated with over estimating the magnitude of the atmospheric heat flux mentioned above 2 4 field measurements as part of the nsf best bsierp program a configuration of 9 subsurface moorings was deployed on the central bering sea shelf from 2008 through 2010 danielson et al 2012a locations are depicted in fig 1 a the moorings were arranged in approximately 3 radial lines extending from the alaskan coast near 61 n such that there were northern n central c and southern s mooring positions close to the 25 m 40 m and 55 m isobaths fig 1 a the s25 mooring data was not operational during this study period but we include this location for model model comparison near bottom salinity data from these moorings were used in this study both for model validation and for the june 2009 initialization of the summer spin up simulation multi decadal time series of temperature and salinity are available from moorings that span bering strait woodgate 2018 woodgate et al 2015 of the four mooring locations that are examined here two a1e and a1w are in the channel west of the diomedes one is in the eastern channel a2 and one is near the alaskan coast a4 fig 1 b near bottom salinity measurements from these four moorings are used in this study for model validation and analysis in each case water depth was approximately 50 m with instrumentation set 10 20 m above bottom as mentioned in part 1 of this study current measurements at these same locations were used to adjust the model northern boundary condition to more closely match observed transport out of the bering sea velocity data will also be used here to help analyze the observed changes in salinity at the mooring locations the current temperature and salinity datasets for these moorings are publicly available woodgate and weingartner 2015 woodgate 2011 from march 7th through april 1 2010 85 ctd profiles were collected from the coast guard cutter polar sea covering the region immediately south of st lawrence island and extending out approximately along the 100 m isobath past both st matthew island and the pribilofs fig 1 c abbreviated psea this data was also collected as part of the nsf best bsierp program depth averaged temperature and salinity data from this cruise were used here for model validation this dataset is also publicly available stabeno et al 2011 3 model observation comparison in this section results primarily from the m surfs eastern bering sea simulation are compared to both observations and the global hycom output the global hycom solution is included in some of these comparisons not because there is an expectation that it should give comparable results but rather because it is an often referenced publicly available model output 3 1 salinity eight of the nine nsf best shelf moorings all but s25 fig 1 a recorded near bottom salinity time series through the winter of 2010 and have proven useful in describing the spatial and temporal shelf variability danielson et al 2012b here in order to focus on the change in salinity caused by ice formation comparisons are made at the near bottom mooring measurement positions fig 3 in order to exclude systematic differences in the initial fields the observations and model outputs are displayed as the salinity difference in each case relative to their time averaged value for the first week of november 2009 at each mooring position the water column salinization due to ice formation is most apparent at the northern mooring locations n55 n40 and n25 salinity increases earliest and by the greatest amount at the most shoreward mooring locations the roms model captures this pattern well hycom which drives ice variability in part by assimilation of the ice concentration data exhibits significantly smaller salinity changes there is no documentation available to us to suggest that the energy loan ice model used for these simulations includes a salinity flux parameterization intraseasonal variability in the observations is highest along the central mooring line c55 c40 and c25 danielson et al 2012b note that salinity on the shelf reaches the annual minimum in late fall and that the position of the salinity minimum tends to gradually relocate offshore over the course of the winter season but that pattern does not hold for the c40 mooring in the winter of 2010 the observations suggest that low salinity water arrives at c40 in mid november of 2009 but that afterwards the salinity intermittently increases by as much as 2 between february and the first week of april 2010 roms displays a similar salinization but without the late fall freshening that appears in the observations the abrupt changes in salinity at the mooring locations in the model and presumably in the observations are due to frontal movements at times these fronts are associated with freshwater discharge from the yukon river strong salinization from nearshore ice formation or a combination of both in the model these fronts meander and shift in response to wind events this likely accounts for the model tendency to produce abrupt changes in salinity at the mooring location that correlate in timing with the observation but not necessarily in magnitude along the southern line of moorings the m surfs solution overestimates the winter salinity changes at s40 the central mooring the overestimate may be due to an underestimate of the amount and timing of freshwater input to the shelf over the months preceding the winter that results from using climatological monthly freshwater inflow this would also explain the model missing lower salinity signals on the northern two mooring lines between november 2009 and february 2010 closer to shore at s25 the roms solution shows a winter increase in salinity that is likely reasonable given the proximity of this station to c25 although no observations are available at s25 for comparison the global hycom solution shows little indication that brine rejection from sea ice formation causes an increase in salinity at any of the mooring locations salinity observations are also available for the polynya region south of st lawrence island and for midshelf locations roughly along the 100 m isobath from the psea ship survey that took place in april 2010 eighty five profiles of temperature and salinity were collected at locations as depicted in fig 1 c fig 4 a b shows the depth averaged salinity and temperature as functions of the distance from st lawrence island measured from the midpoint of the island southern shore the figure exhibits the expected pattern of cold salty water under the polynya and warmer fresher water farther offshore where sea ice has begun melting there is significant variability in both the observations and in the roms results in the 200 km region closest to the island indicative of the complex flow structure that develops as a result of intermittent brine rejection events but the change in salinity and temperature as a function of distance from the island in the model is consistent with the observations as was the case with the mooring data there is little evidence of salinization of the water column in the hycom fields it is unclear whether this is associated with the rate of ice formation the salt flux parameterization or a failure of the model to resolve the polynya dynamics sea ice fields from hycom were not available for analysis salinity and velocity data from the bering strait moorings woodgate 2018 woodgate et al 2015 fig 1 b provide information on the exchange with the arctic when flow is northward out of the bering sea the water passing the western mooring locations a1e and a1w typically has characteristics of anadyr current water and the shelf waters along the chukotka coast fig 5 the water characteristics at the eastern moorings a2 and a4 during northward transport resemble those of the eastern bering sea inner shelf and the alaska coastal current woodgate et al 2015 variability in the salinity at the two western moorings a1w and a1e is less than 1 between january and july 2010 when the current through the strait is predominantly northward fig 5 a and b this low variability is in part due to the outflow of anadyr current waters that have origins at the shelf break where ice formation is limited and that transit the shelf relatively quickly the roms and hycom model results are consistent with this in general when the transport is southward through the straight november through early december 2009 the data from a1w and a1e moorings show significant freshening of the water column this feature depends on transport of fresher water from the arctic it is not captured in the roms solution in part due to an underestimate of the southward transport based on velocity comparison at the mooring locations during this period and in part due to a lack of a fresher water source from the north it may be that early season ice is advecting out of the arctic during this period and melting in the vicinity of the strait variability in salinity between january and june 2010 is relatively larger east of the diomedes at the eastern mooring locations a2 and a4 fig 5 c d positive fluctuations during periods of strong northward transport such as in late february 2010 and mid april 2010 are associated with northward transport of saltier water from the northern bering sea shelf primarily east of st lawrence island these peaks in salinity are likely associated with northward flow of water from regions of high sea ice production as mentioned above in discussing salinity comparison at other sites the roms simulation captures salinization of the water column by sea ice formation more accurately than the global hycom solution on the northern bering sea shelf this may explain the better match with observations of the roms solution at a2 and a4 3 2 ice concentration in part 1 of this study comparisons were made between the model ice concentration and satellite estimates results were presented that demonstrated the model skill in capturing the overall advance and retreat of sea ice as well as the appearance of the st lawrence polynya in order to further analyze the model skill in capturing polynyas the full season of asi daily satellite based estimates of ice concentration spreen et al 2008 is examined and additional coastal regions with intermittent reductions in c i are identified during the winter of 2010 these include fig 6 a 1 the southern side of st lawrence island 2 the southern end of the chukotka peninsula 3 the southwestern facing portion of the seward peninsula 4 the eastern facing alaskan coast in the vicinity of unalakleet norton sound and 5 the northwestward facing alaskan coast region near the yukon river outflow the open water fraction is defined as in part 1 9 o w 1 a reg j 1 n reg 1 c j t δ x δ y where a reg is the total area of the region the model estimate of open water fraction on average exceeds the satellite estimate for the northeastern bering sea shelf overall fig 6 g and for most of the five small regions fig 6 b e but the discrepancy is generally small early in the season the model tends to underestimate ice coverage likely due to a warm bias in sst but throughout much of the winter changes in ice concentration in the model in the polynya regions are well correlated with the satellite observations in some cases rapid change in satellite estimates of o w may reflect limitations of the processing algorithm as consecutive daily composites exhibit differences that appear to occur more rapidly than can be accounted for by sea ice advection or surface heat loss the model tends to show more persistent periods of high o w than the satellite in most regions fig 6 b c d e it is conceivable that after polynyas form cloud formation over the open water may be misclassified as ice reducing the observation based o w prematurely landfast or grounded sea ice plays a role in some of the discrepancies in o w between the model and observation along the alaskan coast satellite imagery indicates that grounded sea ice persists along portions of the alaskan coast for up to 2 weeks at a time esp in march and april of 2010 fig 7 a a supplemental animation showing the distribution of ice concentration from the model and from the satellite estimate is included to demonstrate the persistence of this feature sea ice concentration model satellite comparison polynyas only develop at the offshore edges of these features consequently reducing o w in the alaskan nearshore region the model however does not include a mechanism to account for grounding or landfast ice as a result the modeled polynya opens up right at the coast fig 7 b consistently the model estimates of o w exceed the satellite estimates in these regions fig 6 e f when landfast ice is present the model underestimates o w in january and february in the yukon delta region region 5 fig 6 f for a related reason close examination of the satellite derived ice concentration fields in this area shows that a promontory of grounded or landfast ice at times extends offshore in the vicinity of the river delta fig 7 c blocking the transport of ice from the northeast into this region several times during these months when winds shift to north northeasterly the promontory of landfast ice blocks southward transport of sea ice leaving a band of ice free water along the coast in the shadow of the promontory lacking the ice promontory sea ice quickly covers this polynya region in the model fig 7 d leading to underestimates of o w fig 6 f the appearance of polynyas in the bering sea coastal regions is typically correlated with winds in order to examine the relationship between wind direction and enhanced open water area in each of the regions delineated in fig 6 a polar histograms are generated fig 8 each histogram shows counts of 3 hour averaged model forcing winds greater than 3 m s between december 1 2009 and april 15th 2010 sorted into directional bins gray bars blue bars count how often o w based on the satellite estimate is greater than the december april average for the wind events in each bar segment note for this analysis the o w estimates are sampled with a 24 h time lag likewise red bars count occurrences of greater than average o w based on the model for those wind events over the winter winds are predominantly to the south southwestover much of the bering sea shelf fig 8 consequently coastlines perpendicular to this orientation such as the south side of st lawrence island and the chukotka peninsula have the highest count of larger than average open water area fig 6 b c for both these regions the observations and the model consistently produce higher than average o w with greater than 3 m s winds in these directions this is indicated in fig 8 a and b by the nearly identical size of the blue and red bars that are no shorter than the gray bars for each wind orientation along the seward peninsula area 3 in fig 6 a there are numerous occurrences of above average o w associated with southward winds fig 8 c despite this coastline being aligned primarily west southwest this appears in both the observations and the model presumably the enhanced open water results due to blocking of sea ice transport by the bering strait promontory just to the north of area 3 see fig 6 a there are notable differences between the modeled and observed response to winds along the regions of the alaskan coast near unalakleet and the yukon river areas 4 an 5 in figs 6 a 8 d e the reason for the discrepancies near the yukon river mouth relate to the formation of a landfast ice promontory at the northern end of this region as discussed above and depicted in fig 7 c the differences in the unalakleet region also result from inaccurate representation of the sea ice physics nearshore the occasions when the model exhibits greater than average o w but the observations do not are mostly associated with south southwestward winds during these periods the model allows the sea ice to be flushed out to the west but the satellite imagery suggests it accumulates becoming landfast along the northward facing coast between areas 4 and 5 4 oceanic variability over the bering sea shelf in winter 4 1 redistribution of shelf salinity the efforts to arrive at a reasonably accurate representation of the ice ocean salt flux were made here in order to make an accurate assessment of the overall salinity changes on the bering sea shelf in winter numerical models with some demonstrated fidelity for matching observations are uniquely qualified for such analysis due to the spatial and temporal extent of the information they can provide for the bering sea shelf several competing fluxes determine the overall salinity changes these include the brine melt cycle associated with sea ice formation the exchange of water and ice with the arctic and bering sea basin freshwater riverine inflows particularly in late spring and precipitation and evaporation in relatively cold winters including the winter of 2009 10 the factor that likely leads to the largest changes in shelf averaged salinity is ice formation and melt so accurate representation of this component is essential for a meaningful analysis of how salinity is redistributed over the winter in the simulations conducted for this study the estimates of the seasonal change in shelf averaged salinity shoreward from the 200 m isobath between november 2009 and june 2010 differ notably depending on the salt flux parameterization used fig 9 a with both m conss and m surfs the shelf average salinity returns by the end of winter nearly to the same value it had at the start of the season of course when open boundaries are present even with a conservative scheme such as m conss the overall dilution due to melt will not match the brine added during ice formation if ice enters or exits the domain in unequal amounts so this result suggests that if there were net import or export of ice in the winter of 2009 10 the effect on salinity was compensated for by a comparable lateral exchange of salt the similarity in the curves for the m conss and m surfs cases further suggests that the net effect of sea surface salinity variations on the overall salt flux was not large other simulations m p1 dyn m mkorig and in the hycom solution showed notable changes in shelf averaged salinity over the winter season fig 9 a for m p1 dyn and m mkorig these changes cannot be attributed to boundary flux differences because the circulation and ice volumes were similar across all the roms simulations so they are necessarily related to the surface salinity flux differences there is a net positive change over the season with m mkorig similar to the one dimensional experiment in appendix b m p1 dyn also exhibits a net positive salinity change here as mentioned earlier it was this overestimate in shelf salinities compared with observations that led to considering the m conss and m surfs formulations the global hycom solution exhibits a shelf averaged decrease in salinity over the winter fig 9 a suggesting that the model may underestimate brine rejection and or over dilute the shelf during ice melt it may also have notably different lateral boundary salinity fluxes compared to roms e g across the 200 m isobath in the hycom solution there is no distinction between the pattern in average salinity on the outer shelf between the 75 m and 200 m depth contours and the inner shelf inshore of the 75 m isobath fig 9 b and c in both subdomains the hycom average salinity increases during freeze up between november and april and decreases during melt afterwards this is not the case for any of the roms simulations for which much of the ice production occurs on the inner shelf leading to a strong local increase in salinity during freeze up fig 9 c in contrast on the outer shelf in the roms solutions where there is less local ice production but significant influx of sea ice that subsequently melts there is net freshening throughout much of the winter fig 9 b in all roms cases the inner shelf finishes the winter season saltier than it began and the outer shelf becomes fresher while lateral fluxes contribute in both cases the primary driver is the net amount of sea ice formation and melt occurring locally the effect of including surface salinity in the salt flux parameterization can be compared by looking at the small differences between the m conss and m surfs solutions in the m conss case the rate of salinization or freshening is solely a function of the rate of ice production or melt while this allows the scheme to be conservative over a closed volume the tendency given the choice of the coefficient in eq 5 is to withdraw more salt than m surfs during melting where the surface salinity is greater than 31 5 and to add more salt than m surfs during freezing where the salinity is less than 31 5 since the surface salinity is generally greater than 31 5 offshore of the 75 m isobath m conss freshens offshore waters more than m surfs as depicted in fig 9 b though the effect is small as the m surfs solution most accurately balances the objective of salt conservation with accurate representation of local changes in salinity it will be used exclusively in the model analysis that follows fig 10 displays a map of the depth averaged salinity distribution on the shelf at the onset of the winter season november 1 2009 panel a at the time of maximum sea ice extent april 10th 2010 panel b and once all the sea ice on the shelf has melted july 1 2010 panel c a link is provided to an animation of bottom salinity to elaborate on what is displayed in this figure bottom salinity animation before the winter season the salinity pattern is dominated by the effect of freshwater input primarily from the yukon river in the east and saline water transport between the bering sea slope and the bering strait via the anadyr current in the west by the time of maximum ice extent in early april the shallow coastal regions exhibit the highest salinity on the shelf while the mid shelf persists with the lowest depth averaged salinity the dividing line between regions of net salinity increase and reduction lies roughly along the 75 m isobath fig 10 e and f and remains at this position throughout the season of ice retreat the outer shelf ends the season with an average salinity decrease of approximately 0 1 while the inner shelf ends up approximately 0 3 saltier fig 9 b c the high salinity of the waters along the eastern bering sea coastlines result in part from local ice formation but also reflect the typically north to south circulation near the shorelines in particular saltier water from strong brine rejection along the coast of the seward peninsula circulates southward along the alaskan coast as far south as the kuskokwim river in the model see fig 10 e similarly saltier water produced in the model in kresta bay on the chukotka peninsula near 179w 66n advects southward past anadyr bay averaged over the winter currents close to the surface fig 11 a are strongly influenced by the direction of winds and sea ice movement but at mid depth and lower fig 11 b c and d an overall anticyclonic circulation can be recognized through much of the winter driven by the bering slope current on the outer shelf and the anadyr current on the northwestern shelf in the model this pattern eventually leads to some of the mid shelf meltwater advecting back north in anadyr basin towards the high salinity coastal regions of the northern inner shelf the simulations show southward flow from the surface through mid depth in shpanberg strait between the east side of st lawrence island and the alaskan coastline clement et al 2005 reported in a modeling study of interannual variability in transport on the bering sea shelf that typically this flow is northward but tended to shift southward in years in which winds were predominantly from the east rather than the north the winter of 2009 10 exhibited mostly northeasterly winds perhaps supporting the correlation they observed and the possibility that this winter differed in a notable way from the historical average danielson et al 2012a noted from current meter observations between july 2008 and july 2010 that depth averaged transports in shpanberg strait were southward when winds were northwesterly in agreement with our modeling results they also examined model simulations for earlier years identifying december of 1999 as exhibiting similar circulation clement et al noted reversal in their model as well during the same period the complex spatial distributions of brine and melt water that develop during winter due to the variable weather and the intermittency of polynyas can be analyzed using the passive tracers 2 4 these represent the contributions of the positive and negative components of the surface salinity flux and the net surface salinity flux depth integrals of these fields are displayed in fig 12 a supplemental animation of these fields overlaid with atmospheric wind forcing and sea ice concentration has been included as a supplement depth integrated brine and melt tracer animation these fields allow us to easily identify regions where the highest brine concentration waters or the most meltwater accumulate and where they form for example there is very high brine concentration water south of nunivak island 60n inshore of the 50 m isobath that originates farther north from along the alaskan coastal region near the yukon river outflow fig 12 d as the water advects southward it increases in salinity until it reaches warmer and potentially saltier surface waters where melt exceeds ice formation fig 12 f i this occurs in the model approximately 100 km shoreward of the ice edge by late march the brine water produced in the st lawrence polynya region 63n similarly at times advects south and at other times westward but accumulates primarily in the region between st lawrence and st matthew islands kresta bay is a region of high brine production as well waters from kresta bay flow south along the russian coastline through most of the winter occasionally being entrained into the anadyr current late in the season however strong inflow into the domain along the russian coast flushes the coastal region south of kresta bay causing the accumulated brine water displacement to the east of the bay along the southward facing portion of the chukotka peninsula before entraining into the anadyr current fig 12 g j one region where brine accumulates close to where it initially fluxed into the water column is in the vicinity of nome along the southern coastline of the seward peninsula in norton sound this portion of the alaskan coast does not appear to flush brine water as effectively as portions of the coast fig 11 to the south perhaps due to its geography meltwater accumulated primarily on the mid to outer shelf between the 50 and 200 m isobaths through april of 2010 fig 12 middle column some of the outer shelf meltwater recirculates north of st lawrence island carried by the anadyr current while some is entrained off the shelf into eddies of the bering slope current fig 12 k by late april there is melting inshore of the 50 m isobath as well while the surface salt flux tracer analysis identifies how brine and meltwater are distributed on the shelf it can also provide insight into how the preexisting fall shelf salinity distribution was rearranged over the winter by subtracting the salt flux tracer p from the model salinity field s s p it is possible to explore how circulation and boundary fluxes alone redistribute salinity over the winter season in particular we can examine how the low salinity nearshore water that results from the yukon and kuskokwim river outflows is advected and stirred laterally fig 13 displays s at four different times over the season a supplemental animation has also been included to aid in the visualization s animation for bottom model layer in mid december fig 12 a the distribution appears quite similar to the initial state fig 10 a with fresher water hugging the coast from norton sound to the kuskokwim outflow but a strong inflow of saltier arctic water tends to flush much of this fresher water southward to the vicinity of nunivak island by the beginning of february 2010 fig 13 b and farther south by mid march fig 13 c thus the increased salinity apparent onshore of the 50 m isobath in the eastern bering sea in fig 10 f results from the combined effects of ice formation and the arctic saline water influx the low s water that is pushed southward and some also eastward into norton sound which was the freshest on the shelf at the start of the season has experienced some of the strongest brine injection of any water on the shelf fig 12 d as this water advects and stirs laterally in portions it increases in density due to ice formation while other portions may decrease due to meltwater freshening the resultant filamentous patch of low s water visible to the south of nunivak island in panels c and d of fig 13 results from a complex evolving circulation as horizontal density gradients are altered by the lateral and surface processes apparent in the animation 4 2 circulation of the sea ice transport of the seasonal sea ice that lies atop the coastal waters of the bering sea shelf is driven primarily by winds but also by the ocean currents the ice age tracer 1 is a convenient tool for tracking how sea ice moves over the course of the winter as much of the earliest ice is produced at the northernmost latitudes of the domain along coastlines or is advected into the domain through the bering strait in our implementation the no gradient boundary condition is used for the ice at the northern boundary in chukchi sea since no information about the ice age is obtained for this ice some of which will later be advected in the bering sea through the bering strait all ice advecting into the domain through the bering strait is assumed to have formed on november 1 2009 recall this was the initial date for the series of the coupled tests presented here fig 14 displays maps of ice age at six times over the winter season a supplemental animation has been included ice age animation throughout the winter the average age of ice tends to be lowest in regions of active ice formation and export such as polynyas ice ages as it is transported generally southward to the outer shelf while new ice formation can only reduce the average age of a parcel of ice ice transported to the warmer waters of the outer shelf ages steadily as a result some of the oldest ice accumulates at the ice edge this is consistent with the established idea of a sea ice conveyor belt from north to south in the bering sea pease 1980 the pattern in fig 14 c d and e indicates northwestward advection of this surviving ice along the shelf break with the slope current the anticyclonic circulation is nearly closed when some of this old ice is transported northward with the anadyr current late in the season as the ice edge retreats from the shelf break the alongslope transport of the ice is terminated the oldest remaining ice appears to be that advected in through the bering strait much earlier in the season and accumulated in the vicinity of nunivak island fig 14 f the apparent influx of arctic sea ice indicated by the oldest seasonal ice is one of the most prominent features in the ice age distribution in general winter influx of sea ice from the arctic has been thought to be low except for in anomalous years babb et al 2013 but based on their analysis of satellite derived ice drift velocity in the chuchki sea in the vicinity of bering strait babb et al 2013 quantify winter 2009 10 as the season with the second strongest seasonal average southward transport in that region over the 33 years of data analyzed so the presence of an influx in the model is not surprising the pattern can also be interpreted as demonstrating the distribution of the longest surviving ice which is likely to originate in the northernmost extreme of the model domain 4 3 comparison of polynya regions as demonstrated in fig 8 polynyas on the bering sea shelf tend to be associated with offshore winds they are regions of enhanced sea ice production and brine injection so they play an important role in determining how shelf salinity changes over the winter season satellite based measurements now provide reasonably accurate estimates of the changes in open water area associated with these features but cannot yet provide accurate information about their ice production rates nor the potentially differing dynamics and thermodynamics that lead to their formation in section 3 2 the modeled ice concentration was compared to satellite estimates for the 5 polynya regions depicted in fig 6 a fig 14 provides additional time series information on these 5 regions quantifying their relative contributions to the sea ice ocean salt flux the cumulative salt flux per unit area s f and the cumulative ice production per unit area m i are shown in fig 15 a each of these regions produced ice on a per unit area basis at a higher rate than the northern shelf average also depicted in fig 15 a the rate of productivity in the st lawrence polynya region was the highest followed by the seward peninsula region the chukotka region and the two alaskan coastal regions in all the regions displayed in the figure more ice was produced than eventually melted indicating that there was a net export of ice from each s f for each of these regions followed the same pattern as m i as is expected with this salt flux parameterization ice formation in fresher water leads to less salt flux consequently regions such as the alaskan coast near the yukon outflow exhibit lower rates of salt flux relative to ice production early in the winter an interesting difference that emerges from this analysis is that the chukotka peninsula box exhibits stronger negative heat flux than either the st lawrence or seward peninsula boxes yet produces less ice and fluxes less salt per unit area polynyas can be classified as predominantly latent or sensible heat polynyas latent heat polynyas arise when the mechanical action of the wind sweeps sea ice out of a region exposing freezing temperature water that rapidly begins to produce new ice the phase change being a latent heat process sensible heat polynyas on the other hand exist where surface waters exceed the freezing temperature allowing for the persistence of open water despite a strong heat flux to the atmosphere the exchange with the atmosphere primarily alters the temperature but not the phase of the surface water a sensible heat process the chukotka coastal region has a larger negative net surface heat flux out of the ocean per unit area than any of the other regions fig 15 b in part because this region stays more open has smaller ice concentration throughout the season than the other regions fig 6 but the associated salt flux falls below that of the st lawrence or seward regions because this portion of the chukotka coast has some characteristics of a sensible heat polynya intermittently throughout the winter the model predicts the chukotka region to exhibit above freezing surface temperatures fig 15 d which reduce brine rejection and ice formation this does not indicate that the chukotka coast polynya should be classified as an entirely sensible heat polynya however as it presents characteristics of latent heat polynyas as well with a coastline roughly parallel to the south coast of st lawrence island it experiences comparable winds and events of open water coincide with winds to the southwest see fig 8 so the presence of the warm water alone did not produce polynyas off the chukotka coast in the winter of 2009 10 but rather inhibited ice formation when the polynya formed due to the wind action the source of the warm water can be traced to the anadyr current fig 16 displays several fields from a model transect between the alaskan and russian coasts green dashed line in fig 1 a the winter averaged velocity normal to the transect shows north northeastward flow over the gulf of anadyr along with southward flow close to the russian coast and on the eastern shelf fig 16 b although the warm water transport in this bottom layer fig 16 d is intermittent due to variation in winds and the bering sea arctic pressure gradient that sometimes reduce the anadyr current strength the presence of above freezing temperature bottom water on the transect throughout the winter fig 16 e suggests that vertical mixing fig 16 b remains limited enough to isolate this bottom layer from the surface boundary layer this is supported by the persistence of stratification in winter in the deepest portion of this transect panel c the core of warmer bottom water aligns with the northward current panel d and is centered approximately half a degree to the west of the minima in surface salinity panel c it is likely that this low surface salinity water that is a product of ice melt contributes to maintaining the stratification that allows the warm bottom waters to propagate northward the wintertime northward flow of the anadyr current for earlier winters is documented in the literature muench et al 1988 overland et al 1996 muench et al have noted the transport of above freezing temperature water in the current from the bering sea slope to anadyr strait in the winter of 1985 although we have not found recent observations to corroborate the sensible heat characteristics of the modeled chukotka region polynya the persistence of a stratified water column in the anadyr basin through the winter has been mentioned coachman and shigaev 1992 it is possible that the model overestimates bering slope bottom water temperatures and or underestimates heat dissipating mixing processes as the current progresses northward but given that the model resolves the flow well produces an anadyr current of reasonable magnitude and shows good agreement with the bering sea shelf observations in other regions it is plausible that the model representation in this region is at least qualitatively correct 4 4 changing characteristics of the mid to outer shelf over the winter season typical cold winters on the bering sea shelf are characterized by high sea ice production in the northern coastal regions advection of ice southward over the broad shelf where ice production rates are lower and an outer shelf region where warm waters from the bering sea basin melt a large portion of the sea ice produced over the shelf in this section ice and water properties on a model transect from the southern side of st lawrence island to the shelf break north of zhemchug canyon model transect 1 in fig 1 a are used to elucidate some of the winter characteristics of the shelf several of the variables are presented in fig 18 as functions of the coordinate along the section and time these analyses are supported by the plots of the individual ice production terms in fig 17 ice is present on this transect from late november 2009 through mid may 2010 indicated by the overlaid contours in figs 17 and 18 the first appearance of ice in the fall occurs as a result of advection visible in fig 17 as the presence of ice without any contributions from thermodynamic production terms at the start of the season subsequently thermodynamic production begins particularly strongly in the polynya region by early december based on the model the st lawrence polynya delineated in the figure by lower ice concentrations near the zero of the vertical axis occupies a region of approximately 0 to 50 km from the island coast between the months of december 2009 and april 2010 intermittent disappearances of the feature over the course of the winter coincide with wind reversals that drive sea ice shoreward northward the principle mechanism for ice generation in the polynya region is heat loss from the ocean to the atmosphere indicated by large magnitudes in the w ao and w fr terms fig 18 a and d where ice concentration increases offshore downwind of the polynya region sea ice growth occurs mostly through w io though only at a rate approximately one quarter as large as in the more open water of the polynya as the ice edge advances offshore largely due to wind driven advection it eventually encounters surface waters containing enough heat to cause ice melt through the negative w io term see fig 17 b in late december through mid january this occurs approximately 200 km south of st lawrence island as the shelf continues to cool the melt transition boundary advances farther south behind the ice edge such that for over much of the winter an approximately 200 km wide band of melting ice is present as incoming solar radiation increases and surface air temperatures rise melt commences over the entire transect in mid april 2010 the melt is driven both by the atmosphere ocean heat exchange in the open water portions of each grid cell through the w ao term fig 17 a and at the ice surface through atmosphere ice exchange indicated by w ai figs 18 17 c however from mid april through the disappearance of all sea ice on this transect the model predicts that ice formation continues along the underside of the ice at the ice ocean interface as the sub freezing temperature ice continues to extract heat from the ocean based on the net thermodynamic ice production in fig 17 e the transect can be separated into 4 regions in mid winter these include a region of net ice production brine rejection a region of approximately balanced ice production and melt a melt region and open water south of the ice edge beneath the ice in the net ice production region the seasonal impact of the brine rejection is visible in the sea surface salinity distribution and stratification on the transect fig 18 a and b a salinity gradient develops south of st lawrence island over the first part of the winter season due to ice formation both in the polynya and farther offshore such that by mid january the high salinity extends 150 km offshore from the island along the transect through february march and april this gradient tends to intensify and advance southward but remains within the net ice production region delineated in fig 17 d in this same region high stratification intermittently develops as the brine water sinks and is transported laterally away from the island the variability in the stratification reflects the influence of varying winds and ice production rates strong winds tend to mix away the vertical stratification but because they are often oriented to expand the area of the polynya they can intensify ice formation brine rejection and consequently horizontal density gradients as the winds relax patches of higher stratification often appear as depicted in fig 18 b as the horizontal density gradients relax but other mechanisms restratify the water column in this region as well the reappearance of lower salinity surface water within 100 km of the st lawrence coast in mid march 2010 coincident with high stratification in the polynya region corresponds to a period when a branch of the relatively fresh surface waters of the anadyr current deflect to the south of the island rather than pass through anadyr strait in late december and early january melting occurs on this transect between 150 and 400 km from the coast fig 17 d contributing to a decrease in mid shelf surface salinity that mostly persists throughout the ice covered season fig 18 a from february to april an approximately 60 km wide band of unstratified water persists in the midshelf region where neither the presence of ice nor melt is large enough to suppress vertical mixing as the ice advances farther from shore the melt region moves farther south generating stratification over a 200 km wide swath that extends approximately 200 km into the ice cover from the ice edge by mid march fig 18 b offshore of the ice edge the maximum stratification in the water column is significantly lower than under the ice as the influence of wind mixing is much greater by mid february the ice edge on this transect is approaching the shelf break where the warmer waters of the bering sea basin and the strong northwestward flow of the slope current prohibit any farther advance of the ice for the mid winter season months of february and march the shelf temperature and salinity structure remain relatively persistent vertical sections of potential temperature salinity potential density vertical mixing coefficient and velocity normal to the transect reinforce the description of the dynamics above fig 19 the density gradients are very largely controlled by salinity across the entire transect warmer and saltier water underlies colder and fresher water in the broad melt region over the outer shelf the polynya region is increasingly saltier towards the coast but uniformly cold mixing is suppressed most notably over a band approximately 150 km wide in the melt region but is bounded by well mixed water columns both offshore of the ice edge and in the midshelf region the density gradients established in the melt region coincide with a low frequency circulation that transports water in the melt region northwestward consistent with geostrophy in the ice production region closer to the st lawrence coast where the brine induced salinity gradient forms a weaker southeasterly flow develops also consistent with geostrophy fig 18 c shows the mid depth relative vorticity normalized by the local coriolis parameter to demonstrate that mesoscale eddy variability beneath the ice coverage is largely suppressed except for locations close to the shoreward polynya and seaward ice edges regardless of stratification however once the ice cover melts and stratification intensifies vigorous eddying motions develop across much of this transect 5 summary in this study we sought to better understand the role of sea ice in the alteration of the eastern bering sea salinity distribution over the course of a winter season for this objective we needed a model that estimated reasonably well the ice coverage the ice movement and the salinity fluxes between the two water phases in part 1 of this study we made thermodynamic and dynamic alterations to a relatively unsophisticated sea ice model and demonstrated the model capability for capturing both the seasonal trend in sea ice coverage over the winter of 2009 10 and the overall movement of the sea ice on shorter time scales here in part 2 the model salt flux parameterization was improved and reasonable representation of polynya regions was demonstrated the model salinity comparisons with mooring and profiler observations were improved significantly by the use of a straightforward salt flux parameterization that depended only on the rate of change of ice mass and the salinity difference between the surface ocean and a fixed sea ice salinity the model reproduced the increase in salinity on the inner shelf particularly in the northern bering sea due to excess ice production relative to melt while the global hycom model failed to reproduce this pattern south of st lawrence island the model captured the scale of change and variability in salinity as a function of distance from the island that was observed in ctd profiles from april 2010 the winter of 2009 10 was a relatively cold year for the bering sea with large sea ice extent under these conditions the model exhibited freshening of the outer shelf approximately offshore the 75 m isobath and salinization of the inner shelf to the extent that the direction of the positive salinity gradient changes from the offshore direction in the fall to shoreward by may before riverine input helps to gradually restore the original pattern over the following summer and fall this evolution results from both a general south westward movement of the sea ice over the season away from the areas where ice formation was strongest and from transport of the fresher nearshore waters offshore and to the south overall the volume averaged salinity on the shelf at the end of the melt season is nearly the same as it was before the onset of ice coverage suggesting that salt and ice exchanges with the arctic and the bering shelf slope were negligible or counterbalancing future studies of years with more limited ice extent will be beneficial for determining if no net change in shelf salinity is typical or if interannual variability might be expected the model produces seasonally averaged anti cyclonic circulation of both sea ice and ocean waters on the bering sea shelf this leads to recirculation of meltwater northward in the anadyr basin and also transport of warm salty bottom water of slope origin in the anadyr current causing the deeper waters of the anadyr basin to remain stratified over much of the winter unfortunately there are few observations available to validate the vertical structure produced by the model in this region coastal polynyas contribute significantly to the redistribution of salinity over the winter season with the polynya on the south side of st lawrence island being the largest contributor but other coastal regions similarly aligned relative to the predominant wind forcing for example along the south side of the chukotka and seward peninsulas also frequently exhibited increased open water interestingly ice production and brine rejection were comparatively low in the chukotka polynya region in the model despite frequent high open water area because of its exposure to the relatively warmer water being transported northward in the anadyr current giving it some characteristics of a sensible heat polynya some discrepancies in nearshore ice coverage between the model and observations were found in the vicinity of the yukon river outflow where the model at times failed to capture the polynya geometry these occurrences often appeared to be associated with times in the observations when ice advection was blocked by promontories of landfast ice future work can focus on including parameterizations of sea ice grounding lemieux et al 2015 to improve model performance along these portions the alaskan coast examination of the water column on a transect from the south side of st lawrence island to the shelf break illustrate the variability in stratification beneath the ice coverage on the outer shelf where ice was melting above warmer and saltier slope influenced water the water column was persistently stratified beneath the ice in a band at times over 200 km wide intermittent stratification also developed due to strong brine rejection in the polynya resulting in patches of stratified water within 200 km of the south side of the island the mid shelf where neither freezing nor melting processes dominated was most frequently unstratified despite the encouraging performance of this coupled model further refinements are warranted one of these is to more accurately represent the influx of sea ice from the arctic ocean in general an ice bridge forms north of bering strait that acts to inhibit the inflow of arctic ice but the model has no mechanism presently to emulate this effect consequently the model may be significantly overestimating this effectively freshwater input to the bering sea shelf with arctic ice a second improvement involves allowing the salinity of the sea ice to vary spatially and temporally sea ice has been found to only gradually reject brine over the first couple weeks after its formation in the bering sea that ice may be advected a significant distance away from where it was formed over that time consequently the model may be somewhat overestimating the brine rejection in polynya regions and underestimating the rejection farther offshore nonetheless even a limited coupled sea ice ocean model such as the one presented here can help to elucidate the rich and changing dynamics of the bering sea shelf credit authorship contribution statement scott m durski conceptualization methodology software writing original draft visualization investigation validation formal analysis data curation alexander l kurapov conceptualization resources writing review editing program administration supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partially supported by noaa united states of america total water initiative and nasa united states of america grant nnx13ad89g satellite data was obtained by the asi data repository https seaice uni bremen de start data archive appendix a adjustment of the salt flux parameterization in the original mellor and kantha paper 1989 referred to here as mk the salt flux f s into the ocean is parameterized to depend on the salinity difference between the near surface ocean s so and the molecular sublayer beneath the ice s ms along with a coefficient that accounts for the surface boundary layer properties a 1 f s k s s z c s z s m s s s o z 0 where c sz is a 2 c s z u τ p rt k 1 l n z z o b s and a 3 b s b z o u τ ν 1 2 s c 2 3 in the equations above p rt is the turbulent prandtl number k is von karman s constant z is depth z o is roughness length and u τ is a friction velocity in mk a 1 must match the flux between the molecular sublayer and the sea ice a 4 f s w o c i r m w s m s s i 1 c i p e s ms where r m w is the rate of runoff from surface melt ponds due to contributions from ice melt snow melt and precipitation here a 5 w o c i w i o 1 c i w a o as noted in appendix b of mk with some approximation a system of equations can be solved to obtain the salt and heat fluxes ice production rates and molecular sublayer temperature and salinity but they conclude that it is numerically simpler to estimate the ice production and loss terms from the heat fluxes determine t m s from s m s f t from the temperature analog to eq 1 and combine eqs a 1 and a 4 to give the molecular sublayer salinity as a 6 s m s c s z s s o w o s i c i r m w s i c s z w o c i r m w 1 c i p e this expression for s m s is applied in the m mkorig model setup discussed above in the roms implementation of this parameterization inherited for use in part 1 of this study a coding error or perhaps some undocumented modifications to eq a 6 had been made such that the molecular sublayer salinity was coded as a 7 s m s c s z s s o w i o s i w a i 0 s i c s z w i o w a i 0 r m w where w a i 0 represents min w a i 0 the total salt flux in a grid cell was estimated as a 8 f s w o c i w ai 0 s ms s i c i r mw s i 1 c i e p s so the model using eqs a 7 and a 8 with the aforementioned coding error uncorrected is referred to as m p1 dyn in the manuscript this formulation is identical to s dyn discussed in part 1 it is not obvious and not documented how this parameterization was determined yet it produces quite reasonable evolution of surface sea ice concentration for the winter of 2009 10 as discussed in part 1 variables w ao and c i are absent so presumably eq a 7 is based on the assumption that the salinity of the molecular sublayer should be a function of the rates of ice growth loss and meltwater runoff only in the ice covered portion of a grid cell also as r m w is often equal to and redundant with w a i when surface melting of ice is occurring this equation implies not unreasonably that their contribution to the molecular sublayer characteristics should be weighted differently than changes due to melting at the ice ocean interface whether m mkorig or m p1 dyn is utilized questions arise regarding conservation of salt over a freeze melt cycle for example due to the dependence on wind stress in c s z the same thermodynamic production of ice will lead to a different salt flux under different wind conditions additionally with m p1 dyn the salinity flux associated with the same production melt of ice will differ depending on which mechanism w ao w io w ai etc it is associated with over a freeze melt season in a closed domain neither formulation assures that the net freshwater flux during the melt season will balance the total brine rejection during freeze up the primary motivation of this part of the study is to evaluate the evolution of the eastern bering sea shelf salinity structure over the winter but salinity fields obtained using m p1 dyn s dyn in part 1 exhibited poor agreement with salinity observations from shelf moorings and ctd profiles for the winter of 2009 10 the alternative of using m mkorig did not prove beneficial either even small changes in s ms significantly altered the evolution of ice concentration in the bering sea so in order to preserve the quality of the solution in terms of ice concentration found in part 1 simple alternative parameterizations for f s are considered here while the coded formulation for s ms a 7 is retained despite the ambiguity of its origins we retain this parameterization in calculating the molecular sublayer temperature and ice temperature evolution but decouple it from the calculation of salt flux in principle formulating a set of simultaneous equations that capture the interdependence of the interfacial salinities temperatures heat fluxes and ice production in a consistent manner may be possible but it is numerically challenging and beyond the scope of this work for the purposes here we opt for simple alternatives to the flux formulation outlined above that reproduce the ice formation melt rates obtained in previous simulations while improving the salt flux estimates and overall salt conservation as presented in 2 1 3 above the total surface salt flux in the principle simulations discussed here is 8 with the ice ocean salt flux being given by 5 for m cons and 7 for m surfs appendix b 1 dimensional test case an idealized test of these salt flux parameterizations is presented here to summarize the impact of the model modifications mentioned above we simulate a seasonal cycle in a spatially uniform doubly periodic domain effectively a 1 dimensional water column simulation representative of the central bering sea shelf the initial conditions and forcing are identical to those used in the full 3 dimensional model at approximately the position of the n55 bering sea shelf mooring on the 55 m isobath 62 n 172 6 w the maximum allowable value of c i is set to 0 97 in these experiments to ensure that lateral ice melt w ao can contribute without this limit the sea ice uniformly thins over the entire grid cell during the melt season and the role of w ao in the salt flux cannot be analyzed simulations are performed using 5 different salt flux parameterizations these include m p1 dyn m mkorig m conss m surfs and a case in which ice formation and melt do not contribute to the surface salt flux m precip f s e p s s o in this case ice still forms and melts comparably to the other four cases but ocean surface salinities change much less in these 1 dimensional simulations c i changes rapidly reaching a maximum by early january while the mass of ice or h i the cell averaged thickness gradually increases into may fig 20 a the ice cover persists longer than in the bering sea simulations for several reasons first by not allowing c i to reach a value of 1 the direct atmosphere ocean heat exchange continues through the winter enhancing sea ice production second the doubly periodic boundary conditions provide no lateral exchange with warmer water as on the actual bering sea shelf and no possibility for the sea ice field to diverge and allow increased heat absorption by the ocean the m p1 dyn idealized simulation produces significantly thicker sea ice and consequently a longer melt season than any of the other cases fig 20 largely due to the code error mentioned above the over estimated cooling of the water column compounded by the maximum set on c i leads to excessive frazil ice accretion the m mkorig case produces more ice than either m conss or m surfs due primarily to higher ice production between mid december 2009 and mid january 2010 during this period the m mkorig estimate of s ms a 6 is significantly closer to the sea surface salinity than that for the cases using eq a 7 this leads to a fresher water column and increased frazil ice production which largely accounts for the difference m precip demonstrates that the net effect of evaporation and precipitation over the season is to slightly freshen the water column fig 20 b the abrupt drop in salinity with the final melting of the ice in late june is due to the flushing of fresh snowmelt from the ice surface the changes in water column salinity due to ice formation and melt are much larger in m p1 dyn the excess ice production associated with the w fr term is not accompanied by a commensurate increase in salt flux so when melting occurs through the w ao w ai and w io terms the freshwater flux exceeds the brine injection causing a net loss of salt in the water column over the winter fig 20 b the m mkorig experiment produces a net increase in salinity over the season this is presumably due in part to an underestimate of s ms during the melt season but it is also likely due to the assumption in eq a 4 that all surface runoff has a salinity of s i an overestimate for the melted snow the net change in salinity in both m conss and m surfs is very similar to the m precip case indicating that these schemes are conserving salinity well over the ice formation melt cycle though indiscernible in fig 20 b s avg for m conss is approximately 0 01 higher than for m surfs at the end of the winter season due to the role of s so in the salt flux parameterization 7 comparison of fig 20 b with the top panel of fig 9 a shows that for the one dimensional study m p1 dyn leads to a net decrease in salinity while in the full eastern bering sea simulation it leads to a net increase on the shelf this results from both the code error and the inconsistencies of that formulation s ms in this formulation which plays a role in determining the magnitude of the salt flux is a function of w io and w ai but not w ao so w ao affects the salt flux associated with freezing or melting differently than the other mechanisms as the contributing mechanisms differ between the full shelf and the idealized 1 dimensional case so too does the error in the net salt flux over the winter season 
23936,this study investigates the contribution of short wave breaking to storm surges through a high resolution hindcast of the sea state and storm surge associated with the extra tropical storm klaus this storm made landfall in january 2009 in the southern bay of biscay and produced the largest storm surges observed in this region over the last 20 years with 1 70m in the arcachon lagoon and 1 10m in the adour estuary a fully coupled 3d modelling system which uses a vortex force formalism to represent wave current interactions is applied with a spatial resolution down to 35m in the surf zones in order to properly compute the wave induced setup modelling results reveal that the wave setup contributes by up to 40 and 23 to the storm surge peak in the adour estuary and the arcachon lagoon respectively accounting for wave forces in the circulation model improves storm surge predictions by 50 to 60 this is explained by the dominant role played by wave forces in the momentum balance at the inlets under storm waves numerical experiments further reveal that the wave induced setup can be tidally modulated although this phenomenon seems to be site specific finally a sensitivity analysis highlights the importance of the model grid resolution in the surf zones to correctly resolve the wave setup along open ocean coasts inside the lagoon the storm surge and wave setup are less sensitive to the grid resolution while tidal propagation cannot be accurately represented with a resolution of 1000m which is typically used in operational storm surge forecast keywords storm surge wave setup schism klaus 1 introduction coastal flooding can be one of the most destructive natural catastrophes in recent years the combined effects of demographic growth and economic development of coastal zones with the ongoing sea level rise increased coastal flooding risk muis et al 2016 this risk can be locally aggravated by land subsidence in some regions worldwide such as the ganges brahmaputra delta in bangladesh karpytchev et al 2018 krien et al 2019 or along the mississippi delta in the gulf of mexico letetrel et al 2015 to assess future population changes in low lying coastal zones neumann et al 2015 conducted a global analysis combining socio economic and sea level rise scenarios these authors suggested that the number of people living in low lying coastal zones in 2000 625 million will increase by 50 by 2030 and will double by 2060 which stresses the need to improve coastal communities resilience in the near future on a more fundamental perspective a better knowledge of the physical processes controlling storm induced flooding is crucial to mitigate the consequences of these phenomena hurricane katrina in the gulf of mexico 2005 storm xynthia in the central part of the bay of biscay 2010 hurricane sandy in the region of new york 2012 or typhoon haiyan in the philippines 2013 are major disasters which occurred over the past 20 years and illustrate this necessity storm induced coastal flooding results from extreme sea levels which mostly occur when a high spring tide coincides with a large storm surge although the importance of this combination depends on the ratio between the storm surge and the local tidal range storm surges correspond to variations of the ocean free surface mainly caused by wind induced surface stress and atmospheric pressure gradients associated with extra tropical storms tropical hurricanes and typhoons flather 2001 since the wind effect is inversely proportional to the water depth low lying coastal zones bordered by a large continental shelf and located on storm tracks are particularly vulnerable to storm surges and coastal flooding hazards while wind induced surface stress and atmospheric pressure gradients have been identified as the main storm surge drivers since the early twentieth century doodson 1924 the contribution of wind generated surface waves hereafter short waves to storm surges has received much less attention and remains only partly understood charnock 1955 and stewart 1974 revealed that a young sea state can result in a higher surface stress and thus a higher storm surge which was corroborated by donelan et al 1993 mastenbroek et al 1993 brown and wolf 2009 nicolle et al 2009 and bertin et al 2015a among others besides this effect the breaking of short waves in coastal zones drives an increase in the mean water levels along the shoreline referred to as wave setup this phenomenon was first explained physically by the radiation stress formalism of longuet higgins and stewart 1962 1964 which corresponds to the momentum flux associated with propagation of short waves the dissipation of short wave energy in the nearshore induces spatial gradients of radiation stresses which act as a horizontal pressure force driving currents and a setup along the shoreline the absence of consensus on the representation of wave current interactions in 3d has long restricted the computation of wave setup to 2dh radiation stress formalism over the last 15 years new theories have emerged to represent wave current interactions in 3d mellor 2003 mcwilliams et al 2004 ardhuin et al 2008 also a few studies have shown that the depth varying circulation in surf zones can increase the maximum wave setup along the shoreline of sandy beaches apotsos et al 2007 guérin et al 2018 however the relevance of 3d fully coupled models to compute storm surges at regional scale with a resolution sufficiently fine to represent surf zones has yet to be evaluated while wave setup on beaches is well documented and has been studied for several decades e g see holman and sallenger jr 1985 nielsen 1988 king et al 1990 raubenheimer et al 2001 apotsos et al 2007 its correct representation in storm surge numerical models requires a good description of the surf zones through refined meshes which poses a serious challenge in terms of computational time for regional applications however thanks to the recent development of parallel computing techniques and the access to more computational resources it is nowadays possible to represent the wave setup in storm surge modelling systems at regional scales dietrich et al 2010 bertin et al 2015a krien et al 2017 and better understand its impact several authors revealed that the wave induced setup can substantially contribute to the storm surge under energetic wave conditions and even dominate the other drivers along coasts characterised by narrow to moderately wide shelves lerma et al 2017 or at volcanic islands kim et al 2010 kennedy et al 2012 pedreros et al 2018 the wave setup can range from several tens of centimetres to values of about 1m near the shoreline pedreros et al 2018 guérin et al 2018 while regional wave setup can reach tens of centimetres bertin et al 2015a fortunato et al 2017 however the contribution of wave setup in harbours where tide gauges are usually located is not fully clear in the scientific community e g thompson and hamon 1980 melet et al 2018 suggested that the wave setup is negligible in most of the sheltered areas while aucan et al 2012 reported that the midway tide gauge located in the interior lagoon of midway atoll in the northern hawaiian islands recorded high sea level anomaly sla events corresponding to the wave setup driven by breaking waves during storms the authors even suggested that the seasonal number of sla events recorded at this tide gauge can be used as an index of the storminess in the central north pacific over climatic time scales as they found a good correlation between the two recently several studies combining numerical modelling with field observations suggested that the breaking of short waves over the ebb deltas of shallow inlets malhadas et al 2009 olabarrieta et al 2011 dodet et al 2013 wargula et al 2018 or large estuaries bertin et al 2015a fortunato et al 2017 can induce a wave setup that extends at the scale of the whole lagoon or estuary thus modelling the wave setup appears to be fundamental for the prediction of flood inundation levels and floodplain management of embayments estuaries and river entrances hanslow and nielsen 1992 this study presents a high resolution hindcast of the sea state and storm surge induced by the violent extra tropical storm klaus which made landfall in the bay of biscay on the 24 th of january 2009 as klaus produced the most energetic waves ever recorded in the southern part of the bay this storm represents a unique opportunity to investigate the contribution of short wave breaking to storm surges this process is examined in two sheltered areas of the french aquitanian coast where klaus drove the largest storm surges observed over the last 20 years the arcachon lagoon and the adour estuary a fully coupled 3d modelling system with the vortex force formalism of bennis et al 2011 is applied at the scale of the bay of biscay and the english channel the relevance of the 3d model in terms of storm surge and wave setup is compared against a conventional 2dh approach additional numerical experiments are conducted in order to analyse the impact of the wave forces on the momentum balance at the inlet of the arcachon lagoon and their tidal modulation at both studied locations lastly a sensitivity analysis is carried out to analyse the impact of the grid resolution on storm surge and wave setup predictions 2 the studied area and storm 2 1 study area the bay of biscay is located in the north east atlantic ocean bordered by france to the east and spain to the south the study area is the aquitaine coast in the south eastern part of the bay of biscay which comprises two major geomorphologic settings a first unit from the northern spanish coast to the adour estuary characterised by rocky cliffs and small creeks and bordered by a continental shelf only 20km wide and a second one from the adour estuary to the gironde estuary with a sandy coast bordered by a continental shelf which width increases up to 150km in front of the gironde estuary this study focuses on two specific locations the arcachon lagoon and the adour estuary further south fig 1 which allows to investigate the influence of short wave breaking in areas sheltered from this process the arcachon lagoon fig 1 b is a semi enclosed bay which extends at high tide over an area of 160km 2 the head of the embayment is occupied by intertidal muddy and sandy flats that account for 75 of the lagoon and is divided by a large and complex network of secondary channels the lagoon is connected to the ocean by a 5km wide tidal inlet bounded to the north by the 18km long cap ferret sand spit the inlet is characterised by a well developed ebb tidal delta covering 12km 2 two deep channels called north pass and south pass and a poorly developed flood tidal delta of 2 3km 2 michel and howa 1997 the tidal regime is semi diurnal and mesotidal with a tidal range from 0 94m to 4 93m and a mean value of 2 94m dodet et al 2019 the channels are tide dominated with currents 20 30 stronger in the north pass than in the south pass salles et al 2015 because of the well developed ebb delta and the sandbar continuation of cap ferret the swells do not propagate inside the arcachon lagoon nahon 2018 and the outer inlet can be often saturated with wave breaking senechal et al 2013 according to the hydrodynamic classification proposed by hayes 1980 the arcachon lagoon corresponds to a transitional inlet under a mixed energy regime the adour estuary fig 1 c located approximately 40km north of the spanish border is defined by a narrower channel with a width varying between 150 inlet mouth and 500m over the last 6km of the river two breakwaters protect the entrance of the harbour of bayonne from longshore currents and swell waves and help stabilising the navigation channel the influence of the breakwaters on the storm surge in the adour estuary will be discussed later in this study the tidal regime of the area is semi diurnal and mesotidal with a tidal range varying from 0 78 to 4 32m and a mean value of 2 53m dodet et al 2019 tidal currents are weak in the outer part of the estuary with values lower than 0 20m s 1 while in the river mouth velocities reach values between 1 and 2m s 1 during spring tides brière 2005 the river flow discharge ranges from 30 to 2000m 3 s 1 with an annual mean of about 300m 3 s 1 bellafont et al 2018 dodet et al 2019 analysed wave regimes along the metropolitan coasts of france and provided yearly means of wave parameters along the 30m isobath line according to their study yearly averaged significant wave height in front of arcachon and bayonne is about 1 65m yearly averages of mean wave period and mean wave direction at arcachon respectively bayonne are about 6 3s resp 7 15s and about 290 resp 310 the wave climate is however characterised by important seasonal variations at arcachon resp bayonne the significant wave height has a winter average of 2 08m resp 2 06m and a summer average of 1 24m resp 1 20m and the mean period decreases by 2 5s resp 1 5s between winter and summer seaward of the arcachon lagoon storm waves can exceed 9m in water depths of 26m butel et al 2002 2 2 the storm klaus the extra tropical storm klaus hit the french coasts in the night of the 23 rd to the 24 th of january 2009 it induced the largest storm surge observed over the last 20 years in this region of the bay of biscay with 1 10m at bayonne boucau station and 1 70m at arcachon eyrac station mugica et al 2010 arnaud and bertin 2014 previous long term records of wind speeds were exceeded in some french stations like bordeaux and bayonne with wind gusts of 44 50m s 1 and 33 39m s 1 respectively it was considered as the most damaging wind storm to affect northern iberia and southern france since the destructive storm martin in late december 1999 liberato et al 2011 in 2009 klaus was the most costly weather events worldwide with over us 6 0 billion in losses reported mainly in france and spain aon benfield 2010 liberato et al 2011 described the storm from its genesis to its impact on the french and spanish coasts and the main features of its evolution are summarised here klaus was first detected on 21 january 2009 as a small atmospheric wave perturbation due to the southward displacement of the polar jet stream the winter cyclone moved eastward at an unusually low latitude between 35 n and 45 n on the southern edge of the typical north atlantic storm track climatological envelope it underwent an explosive development on 23 january around 21 w with a deepening rate of 37hpa in 24 h probably supported by an important tropical moisture export the storm rapidly reached the bay of biscay and followed a wnw to ese track towards the coasts fig 1 a the spanish oceanographic institute ieo registered two individual wave heights over 24m from a buoy 35km north of santander between 06 00 and 07 00 in the morning of january 24 th bilbao and cap ferret buoys recorded significant wave heights reaching 13m with a peak period of 15s during the storm the centre of the low pressure system passed at 5 00 am on january 24 th over la rochelle with a minimum of 965 8hpa recorded at the nearby station of chassiron fig 1 a the highest sustained wind speeds were measured further south with a maximum of 36m s 1 at cap ferret station arcachon lagoon for a lowest pressure of 976hpa at bayonne sustained wind speed reached 21m s 1 with a minimum pressure of 983 6hpa 3 methods and data 3 1 the modelling system 3 1 1 overview of the modelling system this study uses the modelling system schism semi implicit cross scale hydroscience integrated system model of zhang et al 2016 which is a 3d unstructured grid model the model uses a combination of a semi implicit scheme and an eulerian lagrangian method to treat the momentum advection which allows to relax the associated numerical stability constraints compared to the original model selfe from which it is derived zhang and baptista 2008 schism now integrates many enhancements and upgrades including new extension to large scale eddying regime and a seamless cross scale capability from creek to ocean zhang et al 2016 ye et al 2020 a detailed description of schism the governing equations and its numerical implementation can be found in zhang et al 2015 2016 the hydrostatic solver of schism can be coupled with other modules incorporated in the modelling system such as short waves sediment transport water quality oil spills and biology the generation and propagation of short waves are simulated with the wind wave model wwmii of roland et al 2012 in this study the contribution of short wave breaking to storm surges is analysed from 3d fully coupled wave current simulations the hydrodynamic and spectral wave models share the same unstructured grid and domain decomposition which reduces the exchange of information between the models and eliminates errors associated with interpolation 3 1 2 vortex force formalism in the modelling system the 3d wave current interactions are represented with the vortex force formalism proposed by ardhuin et al 2008 as described in bennis et al 2011 its detailed implementation in schism can be found in guérin et al 2018 in the vortex force framework the mass conservation and momentum equations of the hydrodynamic model read 1 u ˆ 0 2 d u ˆ d t f v ˆ 1 ρ p a x g ζ x z ν u ˆ z f w a v e x 3 d v ˆ d t f u ˆ 1 ρ p a y g ζ y z ν v ˆ z f w a v e y in eq 1 x y z and u ˆ u ˆ v ˆ w ˆ is the quasi eulerian velocity equal to the mean lagrangian velocity u u v w minus the stokes velocity u s u s v s w s in eqs 2 and 3 f is the coriolis parameter ρ is the water density p a is the sea level atmospheric pressure g is the acceleration caused by gravity ζ is the free surface elevation and ν is the vertical eddy viscosity f w a v e x and f w a v e y are the two components of the wave forces given by 4 f w a v e x v s f v ˆ x u ˆ y w s u ˆ z j x f ˆ d x 5 f w a v e y u s f v ˆ x u ˆ y w s v ˆ z j y f ˆ d y where j is the wave induced mean pressure and f ˆ d is the wave induced non conservative forces due to depth induced wave breaking a detailed description of the wave induced non conservative forces can be found in guérin et al 2018 3 1 3 model parametrisations for 2dh and 3d models there are noticeable differences between 2dh and 3d configurations in 2dh the model uses a manning coefficient and the depth integrated current velocity to evaluate the bottom stress while in 3d the model uses the bottom roughness and the velocity computed at the top of the bottom cell in the 3d model several parametrisations are available to compute the wave enhanced bottom stress but a sensitivity analysis did not result in significant improvements which corroborates the findings of bertin et al 2015a in the central part of the bay of biscay therefore wave effects on the bottom stress are not considered in the study in the 3d model the wave effects on vertical mixing are integrated in the turbulence closure scheme umlauf and burchard 2003 following the approach of moghimi et al 2013 as described in guérin et al 2018 for both 2dh and 3d models the surface stress can be computed with a bulk formula of the form ρ a c d u 10 2 where u 10 is the 10m wind speed and c d is the drag coefficient calculated with the formulation of hwang et al 2019 the surface stress can also be computed using a wave dependent parametrisation using the friction velocity u calculated in wwmii donelan et al 1993 reported that a young sea state enhances the sea surface roughness in order to correctly represent this process and predict the subsequent storm surge mastenbroek et al 1993 and bertin et al 2015a showed that a wave dependent surface stress is required the influence of the surface stress formulation will be discussed later in this study 3 1 4 the spectral wave model wwmii solves the equation for the conservation of the wave action e g see komen et al 1994 to simulate the generation and propagation of wind generated waves the model accounts for wind input and energy dissipation by whitecapping computed according to ardhuin et al 2010 energy dissipation due to bottom friction which is modelled based on the results obtained during the jonswap project hasselmann et al 1973 and depth induced breaking computed according to the model of battjes and janssen 1978 which is parametrised with the breaker index γ and the dissipation coefficient b as wave measurements in the surf zone during the storm were not available γ and b are set to the default values of 0 73 and 1 respectively which will be discussed later finally the non linear wave wave interactions are calculated following the discrete interaction approximation of hasselmann et al 1985 and the lumped triad approximation of eldeberky 1996 in deep water and shallow water respectively a detailed description of the coupling between schism and wwmii can be found in roland et al 2012 and schloen et al 2017 at the coupling time step schism provides wwmii with fields of 2dh currents and water levels while schism receives wave forces from wwmii 3 1 5 model implementation the unstructured computational grid used to perform the hindcast of the storm covers the whole bay of biscay from 10 w to the french coasts the english channel and a part of the north sea up to 55 n fig 1 a the grid has 281000 nodes in the horizontal with a spatial resolution ranging from 5000m along the open boundary to 35m along the shoreline of the studied areas i e the arcachon lagoon and the adour estuary in the vertical the grid is discretised in 35 s levels for the 3d simulations the circulation model is forced at its open boundaries by the 16 main astronomical constituents linearly interpolated from the regional model of bertin et al 2012 the tidal potential is switched off since a sensitivity analysis revealed a negligible effect on tidal predictions after calibration of the tidal model the bed roughness in the 3d model is set to 0 0001m in the open ocean and 0 002m in the arcachon lagoon and the adour estuary in the 2dh model a manning coefficient of 0 02 is employed for the open ocean while a value of 0 029 is considered for the adour estuary and the arcachon lagoon the manning coefficient used for the arcachon lagoon is between the values used by cayocca 1996 0 028 and nahon 2018 0 032 the simulations are started on the 22 th of january 2009 two days before the peak of the storm and last 4 days the time step is set to 60s for both the hydrodynamic and the wave models in the 2dh and 3d simulations over the whole domain the circulation model is forced by hourly 10m wind speed and sea level pressure fields from the climate forecast system reanalysis cfsr saha et al 2010 the datasets are provided on a regular grid with a spatial resolution of 0 312 and 0 5 for the wind and the atmospheric pressure respectively wwmii is forced with the cfsr wind fields over the whole domain wwmii is also forced along the open boundaries by time series of directional wave spectra previously computed from a regional application of the wavewatchiii wwiii spectral wave model described in bertin et al 2015a wind fields from cfsr are also used to run the wwiii model over the north atlantic ocean 3 2 wave and water level observations the accuracy of the wave predictions is evaluated with the measurements recorded by three buoys in the bay of biscay see fig 1 a for their location the biscay buoy is a non directional buoy located by 4500m depth operated by météo france and uk met office the cap ferret and bilbao buoys are located in more intermediate water depths of the southern part of the bay of biscay depths of 50m and 600m respectively and are operated by cerema and puerto del estado respectively the three buoys provide time series of significant wave height h s while the mean wave period t m 02 is available at cap ferret and biscay buoys and the peak wave period t p at bilbao buoy wave bulk parameters are estimated every 60 min at biscay and bilbao buoys and every 30 min at cap ferret buoy since the atmospheric data used to force the model has a hourly time resolution the wave predictions cannot represent the sub hourly variability and the measurements at cap ferret buoy are therefore averaged over one hour to yield a consistent comparison with the model predictions simulated water levels are validated through a comparison against observations recorded with a 10 min sampling interval during the storm period at the two tide gauges of arcachon eyrac and bayonne boucau see fig 1 b and c for their respective location a tidal prediction is obtained based on a 5 year long time series 2008 2012 with a harmonic analysis using the utide code codiga 2011 tides are reconstructed with the 67 main astronomical constituents previously computed note that in the north east atlantic ocean the constituent sa results from a combination of thermo steric and atmospheric effects bertin et al 2015b payo payo and bertin 2020 therefore it is not included in the tidal prediction since storm surges are computed as the difference between the observed water level and the astronomic tidal prediction 4 modelling results 4 1 atmospheric forcing in order to validate the atmospheric forcing originating from cfsr a comparison is performed against field observations available during the storm and collected at the meteorological stations of cap ferret and bayonne see fig 1 a for their location the comparison fig 2 of modelled against observed 10m wind speeds hereafter u10 and sea level pressure hereafter slp reveals that slp is well reproduced with a root mean square error hereafter rmse lower than 1 5 hpa at both locations at cap ferret u10 is accurately predicted with a rmse of 2 3m s 1 although with a slight underestimation of approximately 4m s 1 two hours before the peak of the storm since the meteorological station at bayonne is located at 75m above sea level and 3km inland the model providing 10 m wind speed with a 0 3 resolution does not accurately reproduce the observations which probably explains the positive bias of 1 6 m s 1 overall it should be noted that the intensity of the storm is correctly represented peak values of u10 are reasonably predicted with stronger values at cap ferret 34m s 1 than at bayonne 22m s 1 4 2 wave predictions modelled wave bulk parameters are compared against the measurements available during klaus at cap ferret bilbao and biscay buoys fig 3 the comparison reveals a good agreement between modelled and measured data h s is well reproduced with a rmse ranging from 0 51 to 1m which corresponds to a 10 17 error once normalised by the mean of the observations hereafter nrmse however for the three stations the model displays a positive bias of 0 35 0 50m at cap ferret and biscay buoys and 0 70m at bilbao buoy it should be noted that the larger error at bilbao buoy is partly due to a one hour time lag representing 35 of the bias and the nrmse which we are unable to explain the model correctly captures the peak storm wave height with less than 10 error at the three buoys t m 02 available at cap ferret and biscay buoys is well predicted with a nrmse less than 6 while at bilbao buoy t p is adequately reproduced with a 10 nrmse 4 3 storm surge and water level predictions a tide only simulation is first performed and the modelled water levels are compared against the tidal predictions based on the observations at each station the tidal forcing together with the distribution of the manning coefficient yields good results with a rmse on tides of 0 11m at bayonne and 0 08m at arcachon not shown the effect of the parametrisation of the surface stress on the storm surge is investigated by comparing simulations using the bulk formula of hwang et al 2019 and the wave dependent approach see section 3 1 3 this comparison reveals moderate differences between both parametrisations lower than 0 05m with the predictions of the model using the bulk formula slightly better matching the observations to explain the negligible effects of the wave dependent approach on the storm surge predictions the sea state is characterised by the wave age defined as c p u 10 where c p is the peak wave phase speed considering a 20 hour window centred on the storm peak the wave age varies from 0 7 to 2 3 with an average value of 1 32 with a standard deviation of 0 5 which is characteristic of a mature sea state and explains the very slight impact of the wave dependent approach on the results this behaviour corroborates the study of bertin et al 2015a who showed that the surface stress was little dependent on the sea state for the storm joachim characterised by comparable wave height and peak period as during klaus according to these results the bulk formula of the surface stress is adopted in the rest of the study the contribution of short wave breaking to the storm surge is analysed by comparing a first simulation without wave forces and a fully coupled simulation i e including wave forces hereafter referred to as the baseline model the modelled storm surges are obtained by subtracting the tide only simulation to each case of simulation the results are presented in fig 4 where each simulation is compared against storm surges and water levels observed at bayonne and arcachon during the storm the baseline model accurately reproduces the water levels with a rmse of 0 09 and 0 15m at arcachon and bayonne respectively the storm surges are well predicted by the model with a rmse of 0 12m at bayonne and 0 10m at arcachon although a 0 25m underestimation is noticed at this station approximately two hours before the storm peak without wave forces storm surge and water level predictions considerably deteriorate compared to the baseline model with a rmse two to three times larger at both locations the modelled water levels display a negative bias ranging from 0 18 to 0 24m the surge peak is underestimated by 0 40 0 45m at arcachon and bayonne which results in a negative bias of 0 23m over the duration of the storm the comparison of both simulations reveals that the wave setup driven by the wave forces in the baseline model accounts for 40 and 23 of the surge peak in the adour estuary and the arcachon lagoon respectively which explains that the baseline model much better matches the observed peak values in order to get a spatial overview of this process modelled storm surges with and without wave forces as well as their difference are computed at the scale of the arcachon lagoon and the adour estuary fig 5 in the adour estuary the storm surge in the fully coupled simulation increases by more than 0 5m at adjacent beaches while being almost constant inside the estuary fig 5 a the comparison between fig 5 c atmospheric surge only and fig 5 e wave setup only reveals that this behaviour is due to the development of a wave setup along adjacent shorelines reaching up to 0 75m and extending at the scale of the whole estuary where it raises the water level by 0 45m a different pattern can be observed in the arcachon lagoon where the storm surge in the fully coupled simulation increases from the inlet to the lagoon head fig 5 b the comparison between fig 5 d and f suggests that this behaviour results from the increase in atmospheric surge towards the lagoon head combined with the development of a wave setup reaching 0 40m at the scale of the lagoon as in the adour estuary the wave setup develops at the inlet and then exhibits a plateau inside the lagoon along the adjacent shorelines of the lagoon the maximum wave setup reaches 0 80m the maximum wave setup along the adjacent shorelines are not shown in fig 5 e and f as computational nodes dry in the tide only simulation are not represented 5 contribution of wave breaking to storm surges 5 1 model predictive skills wave parameters are accurately reproduced by the model and correspond to the state of the art considering previously published studies led under storm wave conditions e g kerr et al 2013 bertin et al 2012 staneva et al 2016 storm surges are also well predicted with errors similar or even lower compared to previously published studies e g kerr et al 2013 brown et al 2013 bertin et al 2015a in details the storm surge is underestimated by up to 0 25m during the first part of the storm peak at arcachon this can be explained by an underestimation of the cfsr sustained wind speeds by up to 4 m s 1 during this period fig 2 which leads to a wind induced surge lower than expected this hypothesis was tested by correcting wind speeds empirically on the time steps corresponding to this period cf appendix the results reveal that this correction almost cancels out the local underestimation in the surge thus supporting this hypothesis in the adour estuary model results at an earlier stage of this study showed a 0 05to 0 1m negative bias in the storm surge before the storm peak when the breakwaters bounding the estuary mouth were considered as impermeable walls in fact these breakwaters are made of large blocks 4 to 40 tons that allow large amounts of water to flow through the structures when a gradient in water levels exists on both sides of the structure prof abadie pers com such flows can take place when a wave setup develops at adjacent beaches a process already reported at other engineered estuaries hanslow and nielsen 1992 hanslow et al 1996 in order to account for these possible flows we took advantage of hydraulic structure options implemented in the code although this parametrisation improves storm surge prediction by 0 04m verifying the adequate representation of these flows is outside the scope of the study and would deserve a specific analysis the comparison of the results between the baseline model and the model without wave forces fig 4 reveals that including wave forces in the circulation model substantially improves its predictive skills the analysis of the different terms included in wave forces eqs 4 and 5 shows that the wave dissipation term by depth induced breaking is clearly dominant over the vortex force and the wave mean pressure terms in accordance with previous studies staneva et al 2016 this analysis highlights the importance of accounting for short waves in storm surge modelling systems provided that wave energy dissipation due to wave breaking is correctly represented guérin et al 2018 investigated wave induced circulation in a surf zone with varying bed slope the authors computed the wave breaking process according to the model of thornton and guza 1983 in which they calculated the breaking index γ and the dissipation coefficient b as a linear function of the beach slope the authors showed that this adaptive approach improved the predictions of short wave bulk parameters and wave setup by 30 following this study pezerat et al 2020 showed that a dissipation coefficient b taken at 40 times the local bed slope strongly improves wave predictions at gently sloping shorefaces 1 1000 at both study sites bottom slopes are much steeper 1 50 to 1 100 so that this adaptive parametrisation results in values for b close to the default value of 1 indeed a sensitivity analysis shows that the adaptive parametrisation of pezerat et al 2020 yields very similar short wave and setup predictions compared to the default values for γ and b in the model of battjes and janssen 1978 new field experiments are required to investigate further wave dissipation mechanisms in coastal zones and validate the numerical model under very high energetic conditions although such field deployments remain very challenging finally the comparison between 2dh and 3d simulations reveals only modest differences with water level and surge predictions slightly improved in 3d in arcachon and slightly deteriorated in bayonne fig 6 in the arcachon lagoon improved storm surge predictions are obtained before and during the storm peak when winds blow from sw to w and drive an ekman transport towards the coast a process better represented with a 3d model roland et al 2012 in the case of the adour estuary maximum wave setup at adjacent beaches is slightly lower in 2dh compared to 3d but extends further offshore thereby more impacting water levels in the estuary guérin et al 2018 showed that the depth varying circulation driven by short waves in surf zones can increase the wave setup along the coast but this process is only substantial at steep beaches i e mean slope of 1 30 and over also these authors reported that 3d runs yield larger wave setup compared to 2dh runs very close to the shoreline so that reproducing these differences requires a spatial resolution of a few metres which is one order of magnitude finer than in this study 5 2 momentum balance previous studies already reported the development of a wave setup in inlets river entrances and shallow lagoons hanslow and nielsen 1992 hanslow et al 1996 dunn et al 2001 oshiyama et al 2001 tanaka et al 2001 2003 nguyen et al 2007 bertin et al 2009 malhadas et al 2009 olabarrieta et al 2011 dodet et al 2013 wargula et al 2018 which can be further investigated by analysing the momentum balance at the inlet hench and luettich jr 2003 analysed the momentum balance without waves in the beaufort inlet in north carolina and in an idealised inlet and reported that near maximum flood and ebb the along stream momentum balance in both cases is dominated by advection barotropic pressure gradient and bottom friction olabarrieta et al 2011 corroborated these results in a study conducted in willapa bay usa during a storm event by activating the wave forces in their fully coupled modelling system they also revealed that they can substantially change the barotropic pressure gradient and the bottom friction while being one of the dominant terms in the momentum balance in the inlet area these findings were then corroborated by dodet et al 2013 and wargula et al 2014 in particular dodet et al 2013 combined both modelling and observations to study wave current interactions on the albufeira lagoon a shallow wave dominated tidal inlet in portugal during energetic oceanic swell conditions the authors showed that the wave forces term oriented towards the lagoon was of the same order of magnitude as the other terms in the momentum balance in the inlet which therefore had a significant impact on the hydrodynamics including a setup that developed within the lagoon recently fortunato et al 2017 conducted a high resolution hindcast of the storm surge associated with the 1941 storm that made landfall in the north of portugal and has driven the development of a large surge in the tagus estuary their model results suggested that the breaking of storm waves generated a wave setup of up to 0 50m in the tagus estuary showing that a substantial wave setup can also impact water levels at the scale of a large estuary this phenomenon is explained by the authors as the result of large onshore directed wave forces owing to storm waves breaking over the ebb delta generating a wave setup that extended beyond the surf zone and in the inlet the previous analysis of figs 4 and 5 suggests that such a phenomenon occurred at the arcachon lagoon during klaus large wave breaking on the ebb delta generated a wave setup that affected the whole lagoon to understand the underlying mechanisms the magnitude of the leading terms of the momentum equations i e the barotropic pressure gradient term third term of the right hand side of eqs 2 3 the wave forces last term of the right hand side of eqs 2 3 the bottom stress and surface stress terms are computed at the inlet of the arcachon lagoon fig 7 in order to analyse the momentum balance at mid flood and mid ebb under similar forcing corresponding to the peak of the storm two additional simulations are performed where tides are shifted the analysis of fig 7 shows that the outer part of the inlet behaves like a sandy beach with a balance between the wave forces hereafter wf and the barotropic pressure gradient hereafter bpg term battjes and stive 1985 lentz and raubenheimer 1999 in this area the wf reach values one order of magnitude larger than the bottom stress hereafter bs and the surface stress hereafter ss terms the dominant role of wf in the momentum balance at the inlet corroborates the findings of olabarrieta et al 2011 and dodet et al 2013 in the inlet channel the wf become much weaker and the alongstream dynamics is controlled by a balance between the bpgr and the bs terms which is typical of tidal channels hench and luettich jr 2003 between the flood and the ebb the signs of the bpgr and the bs terms are inverted except in the outer part of the inlet where the bpgr term compensates the wf during all tidal phases the major contribution of the wave forces to the momentum balance in the inlet directly explains the strong effect of short wave breaking on the hydrodynamics the main impact being a wave setup that reaches several tens of centimetres within the lagoon fig 4 in more details the rapid decrease in wf inside the lagoon explains why the wave setup displays a plateau inside the lagoon fig 5 f over the ebb delta the wind driven surge reaches approximately 0 40m fig 5 d and assuming 0 35m of inverse barometer effect which is comparable to the wave setup while ss are one order of magnitude lower than wf this behaviour is explained by the fact that strong wf only extend over the 3 km wide ebb delta while the wind effect is integrated across the 60 km wide shelf inside the lagoon the atmospheric surge further grows as the water depth decreases fig 5 d in the adour estuary the weaker atmospheric surge fig 5 c is explained not only by weaker winds fig 2 but also by the narrower continental shelf indeed many studies already demonstrated that for a given wind speed the wind driven surge is also controlled by the shelf width such as in the bay of biscay bertin et al 2012 in north sea wolf and flather 2005 or in the gulf of mexico kennedy et al 2012 5 3 tidal modulation of the wave setup some of the studies that highlighted the development of a wave setup in tidal inlets also suggested that the wave setup can be tidally modulated olabarrieta et al 2011 dodet et al 2013 fortunato et al 2017 fortunato et al 2017 showed that the wave setup that developed in the tagus estuary mouth during the 1941 storm was strongly tidally modulated with values of 0 10 0 15m at high tide while being three times larger at low tide with values of 0 30 0 35m the authors attributed this phenomenon to more intense wave breaking on the ebb delta at low tide when waves do not break over the ebb delta they propagate into the inlet or to the coast in the vicinity of the estuary mouth and thus their contribution to the setup inside the estuary is lower in this section the tidal modulation of the wave setup is investigated at the arcachon lagoon and the adour estuary with additional numerical experiments the arcachon lagoon exhibits large intertidal flats which makes the tidal propagation and asymmetry very sensitive to the mean water depth therefore tidal propagation is different when the wave setup raises the mean water level of the lagoon computing the wave setup as the difference between a simulation including tides and waves and a simulation with tides only results in difference not only including the wave setup but also the differences in tidal levels due to the higher mean water level in the coupled simulation a process also referred to as tide surge interactions to overcome this problem a series of stationary runs is performed with constant water levels and wave forcing fig 8 a two sub grids of smaller extent covering each studied area are forced at the ocean boundary by constant water elevations ranging from 1 5m to 1 5m and a jonswap spectrum to simulate short waves the spectrum is characterised by a significant wave height of 14m and a peak period of 15s which corresponds to the peak values reached during klaus in the region in the case of the arcachon lagoon the results reveal a small tidal modulation of the order of 0 07m fig 8 b the wave setup being larger at low tide at the adour estuary the tidal modulation is stronger with a wave setup reaching 0 60m when the mean sea level is lowered by 1 5m and decreasing to 0 45m when the mean sea level is increased by 1 5m contrary to the tagus estuary where the ebb delta is submerged with depths of the order of 5 m relative to the mean sea level the ebb delta of the arcachon lagoon extends 3km offshore and includes an elongated supratidal bank the arguin bank this setting causes the wave breaking to be almost full in front of the inlet even at high tide at lower tidal elevations wave energy mostly dissipates on the terminal lobe while at higher tidal stages waves also break over the supratidal sand bank the wave setup exhibits therefore a slight tidal modulation unlike the tagus estuary fortunato et al 2017 at the adour estuary the bathymetry is subtidal which implies that the lower the water level the larger is the wave energy dissipation and the wave setup these results indicate that tide induced water level variationschange the spatial gradients of short wave energy dissipation rates which in turn controls the wave setup depending on the morphology of the inlet the wave setup along the shoreline and in the lagoons or estuaries can experience significant tidal modulations as well tidal currents which are strong in estuaries or tidal inlets can also affect the propagation of short waves ardhuin et al 2012 rusu et al 2011 dodet et al 2013 bertin et al 2019 and subsequently the wave setup during flood waves following currents decrease while during ebb waves propagating against currents increase shifting the position of the breaking point seaward dodet et al 2013 the impact of tidal currents on short wave propagation is verified by comparing water elevations from runs including tides and waves and activating or not the feedback of currents on waves model results at the arcachon lagoon and at the adour estuary show that switching off the effects of tidal currents on short wave propagation has a small impact on wave setup lower than 0 01m this finding corroborates the study of fortunato et al 2017 which reported that the tide induced water level variations at the mouth of the tagus estuary are the main driver for the tidal modulation of the wave setup compared to tidal currents effects the comparison of the effect of tides on wave setup between both studied locations emphasises that tidal modulation is site specific in areas such as the adour estuary the higher wave setup is produced close to low tide and the tidal modulation amplitude increases with increasing tidal range such tidal modulation can therefore limit the contribution of short wave breaking to coastal flooding which mostly occurs during high tide in macro tidal regions 5 4 sensitivity of storm surge and wave setup calculation to spatial resolution recently several storm surge numerical models using unstructured grids have been developed e g kerr et al 2013 such models allow to correctly represent complex shorelines and coastal embayments using a variable grid resolution usually coarse in the deep ocean several kilometres to tens of kilometres and down to few hundreds of metres in the nearshore however such a resolution in coastal areas may not be sufficient to adequately represent small coastal morphological features such as lagoons and thus enable the model to provide accurate storm surge predictions shen et al 2006 also this study reveals that the wave setup generated by wave breaking during extreme events can greatly contribute to the storm surge even in areas sheltered from wave breaking such as lagoons and estuaries accounting for short waves in storm surge operational modelling is thus of key importance to correctly predict water levels in coastal areas during storm events and thereby improve emergency responses however a good evaluation of the wave setup requires a resolution fine enough in the surf zones which is not always possible in operational modelling systems kohno et al 2018 therefore an important question rises here how well do surf zones need to be spatially resolved in order to correctly estimate the contribution of wave setup to the storm surge the sensitivity of the storm surge wave setup to the model resolution is analysed at the arcachon lagoon region by simulating the sea state and storm surge associated to klaus with different grid resolutions the grid resolution used for the baseline model hereafter bm which goes down to 35m in the nearshore is modified to get two additional computational grids with spatial resolution from the inner shoreface to the nearshore degraded to 200 and 1000m the surge is evaluated at two locations along the coastline in the inner part of the lagoon at the eyrac tide gauge and at the shoreline exposed to the ocean computed as the average value of the storm surge in an area defined to the south of the inlet see fig 1 b this sensitivity analysis is not carried out at the adour estuary since the inlet mouth with a maximum width of 150m cannot be represented with such resolutions the results show that the modelled water levels and storm surges on the open ocean beach are lower when the grid resolution coarsens fig 9 indeed while tidal predictions show little sensitivity to the grid resolution the peak of the surge simulated with the bl resolution reaches 1 65m while being 30 and 65 higher than the surges obtained with the 200 m and 1000 m resolutions respectively a detailed analysis reveals that these differences are mostly explained by wave setup which is poorly represented with a coarse grid in the lagoon the results reveal a different behaviour of the model fig 9 surprisingly the predicted storm surge is less sensitive to the grid resolution compared to the open ocean beach the three grid resolutions well reproduce the peak of the surge with the 200m and 1000m grid resolutions resulting in a slightly lower surge than the bm resolution 0 05m however the storm surge modelled over the total duration of the storm is deteriorated with the 1000m resolution rmse of 0 17m compared to the bm and 200m resolutions rmse of 0 10m also water elevation is poorly predicted with the 1000m resolution which yields a rmse of 0 48m against 0 085m and 0 14m with the bm and 200m grid resolutions respectively when the channels of the lagoon are not correctly represented the tidal propagation in the lagoon is poorly reproduced which impacts the predictions of water level and storm surge this sensitivity analysis of model results to grid resolution reveals a contrasting situation between the inner lagoon where wave setup is reasonably represented even with a coarse resolution and adjacent sandy beaches where modelled wave setup is almost nil when using a coarse resolution this behaviour is directly explained by the cross shore extension of the surf zone which is of the order of 1000m at adjacent beaches but ranges from 3000 to 5000m in front of the lagoon as a rough guideline we estimate that accounting for wave setup in storm surge models requires at least 5 grid elements across the surf zone which implies using a finer spatial resolution when the beach slope increases and the wave height decreases this corroborates the findings of sashikant nayak et al 2012 who investigated the sensitivity of wave setup predictions to grid resolution considering idealised beaches of slope ranging from 1 80 to 1 10 6 conclusion the fully coupled modelling system schism using a vortex force formalism was used to hindcast the sea state and storm surge associated to the strongest storm that occurred in the southern part of the bay of biscay for the last 20 years after the verification of the model with wave and water level observations available during the storm the analyses of the simulations revealed that the predictions of the storm surges at the arcachon lagoon and the adour estuary were improved by 50 to 60 when the wave forces were accounted for the wave setup induced by the storm waves breaking in the vicinity of these two inlets extended outside the surf zones and significantly increased the water level at the scale of the whole lagoon and estuary to understand the impact of storm wave breaking on the hydrodynamics of the tidal inlets the local momentum balance was analysed at the inlet of the arcachon lagoon by reaching values one order of magnitude larger than the bottom stress and the surface stress terms the wave forces were one of the leading terms of the momentum balance and thereby greatly affected hydrodynamics in the inlet the main impact being the development of a wave setup at the scale of the whole lagoon further analysis showed that the wave setup in tidal inlets can be tidally modulated while this phenomenon is site specific and depends on the morphology of the inlet at arcachon as the ebb delta is characterised by supra tidal sand banks wave breaking is total at all tidal phases the wave setup exhibits therefore a slight tidal modulation at bayonne waves are subjected to more intense breaking at low tide than at high tide the tidal modulation of the wave setup is thus more pronounced finally a sensitivity analysis of the storm surge and wave setup to the spatial resolution of the computational grid was carried out this work revealed that the calculated wave setup at the shoreline is highly sensitive to the grid resolution in the lagoon the modelled storm surge and wave setup were found to be comparable between different grid resolutions while tidal propagation cannot be accurately represented with a resolution of 1000m this study highlighted the need to account for wave breaking in operational storm surge models although resolving the wave setup requires a spatial resolution that depends on the width of the surf zone which is in turn controlled by the bottom slope and the wave height in a context of upcoming altimetry satellite missions with spatial footprints below 1000m swot durand et al 2010 the results presented in this study are of key importance as they show that the wave setup can impact the water level in sheltered areas such as harbours large lagoons and estuaries as these coastal areas are usually instrumented with tide gauges that are used to calibrate altimeter measurement systems it is crucial to determine the physical drivers of the water level variations recorded at these stations credit authorship contribution statement laura lavaud writing original draft software investigation formal analysis xavier bertin supervision conceptualization methodology software investigation formal analysis writing original draft kévin martins software investigation formal analysis writing original draft gael arnaud investigation formal analysis marie noëlle bouin investigation formal analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the editor as well as the three anonymous reviewers are greatly acknowledged for their constructive comments which resulted in a substantial improvement of this paper laura lavaud is supported by a phd fellowship from the region nouvelle aquitaine and the unima engineering consulting company xavier bertin thanks the support from the regional chaire program evex funded by region poitou charentes kévin martins acknowledges the financial support from the university of bordeaux through an international postdoctoral grant idex nb 1024r 5030 the authors greatly acknowledge schism developers community wave data in the bay of biscay were provided by météo france uk met office cerema http candhis cetmef developpement durable gouv fr and puerto del estado http www puertos es water level data and atmospheric forcings were provided by the french oceanographic and hydrographic institute shom through the refmar portal http data shom fr and ncep cfsr respectively appendix the underestimation of the storm surge before the peak of the surge can be attributed to a negative bias in the 10m wind speed from cfsr in order to verify this hypothesis the modelled wind speed from cfsr is increased by 12 15 over three time steps before the storm peak fig a 10 a the comparison between the original modelled storm surge and the storm surge computed with the tuned wind speed fig a 10 b shows a significant difference at the considered period the rmse is improved by 20 and the localised error is cancelled out these results confirm that the underestimation of the storm surge at this stage of the storm is due to a local negative bias in the modelled wind speed of the order of 4m s 1 
23936,this study investigates the contribution of short wave breaking to storm surges through a high resolution hindcast of the sea state and storm surge associated with the extra tropical storm klaus this storm made landfall in january 2009 in the southern bay of biscay and produced the largest storm surges observed in this region over the last 20 years with 1 70m in the arcachon lagoon and 1 10m in the adour estuary a fully coupled 3d modelling system which uses a vortex force formalism to represent wave current interactions is applied with a spatial resolution down to 35m in the surf zones in order to properly compute the wave induced setup modelling results reveal that the wave setup contributes by up to 40 and 23 to the storm surge peak in the adour estuary and the arcachon lagoon respectively accounting for wave forces in the circulation model improves storm surge predictions by 50 to 60 this is explained by the dominant role played by wave forces in the momentum balance at the inlets under storm waves numerical experiments further reveal that the wave induced setup can be tidally modulated although this phenomenon seems to be site specific finally a sensitivity analysis highlights the importance of the model grid resolution in the surf zones to correctly resolve the wave setup along open ocean coasts inside the lagoon the storm surge and wave setup are less sensitive to the grid resolution while tidal propagation cannot be accurately represented with a resolution of 1000m which is typically used in operational storm surge forecast keywords storm surge wave setup schism klaus 1 introduction coastal flooding can be one of the most destructive natural catastrophes in recent years the combined effects of demographic growth and economic development of coastal zones with the ongoing sea level rise increased coastal flooding risk muis et al 2016 this risk can be locally aggravated by land subsidence in some regions worldwide such as the ganges brahmaputra delta in bangladesh karpytchev et al 2018 krien et al 2019 or along the mississippi delta in the gulf of mexico letetrel et al 2015 to assess future population changes in low lying coastal zones neumann et al 2015 conducted a global analysis combining socio economic and sea level rise scenarios these authors suggested that the number of people living in low lying coastal zones in 2000 625 million will increase by 50 by 2030 and will double by 2060 which stresses the need to improve coastal communities resilience in the near future on a more fundamental perspective a better knowledge of the physical processes controlling storm induced flooding is crucial to mitigate the consequences of these phenomena hurricane katrina in the gulf of mexico 2005 storm xynthia in the central part of the bay of biscay 2010 hurricane sandy in the region of new york 2012 or typhoon haiyan in the philippines 2013 are major disasters which occurred over the past 20 years and illustrate this necessity storm induced coastal flooding results from extreme sea levels which mostly occur when a high spring tide coincides with a large storm surge although the importance of this combination depends on the ratio between the storm surge and the local tidal range storm surges correspond to variations of the ocean free surface mainly caused by wind induced surface stress and atmospheric pressure gradients associated with extra tropical storms tropical hurricanes and typhoons flather 2001 since the wind effect is inversely proportional to the water depth low lying coastal zones bordered by a large continental shelf and located on storm tracks are particularly vulnerable to storm surges and coastal flooding hazards while wind induced surface stress and atmospheric pressure gradients have been identified as the main storm surge drivers since the early twentieth century doodson 1924 the contribution of wind generated surface waves hereafter short waves to storm surges has received much less attention and remains only partly understood charnock 1955 and stewart 1974 revealed that a young sea state can result in a higher surface stress and thus a higher storm surge which was corroborated by donelan et al 1993 mastenbroek et al 1993 brown and wolf 2009 nicolle et al 2009 and bertin et al 2015a among others besides this effect the breaking of short waves in coastal zones drives an increase in the mean water levels along the shoreline referred to as wave setup this phenomenon was first explained physically by the radiation stress formalism of longuet higgins and stewart 1962 1964 which corresponds to the momentum flux associated with propagation of short waves the dissipation of short wave energy in the nearshore induces spatial gradients of radiation stresses which act as a horizontal pressure force driving currents and a setup along the shoreline the absence of consensus on the representation of wave current interactions in 3d has long restricted the computation of wave setup to 2dh radiation stress formalism over the last 15 years new theories have emerged to represent wave current interactions in 3d mellor 2003 mcwilliams et al 2004 ardhuin et al 2008 also a few studies have shown that the depth varying circulation in surf zones can increase the maximum wave setup along the shoreline of sandy beaches apotsos et al 2007 guérin et al 2018 however the relevance of 3d fully coupled models to compute storm surges at regional scale with a resolution sufficiently fine to represent surf zones has yet to be evaluated while wave setup on beaches is well documented and has been studied for several decades e g see holman and sallenger jr 1985 nielsen 1988 king et al 1990 raubenheimer et al 2001 apotsos et al 2007 its correct representation in storm surge numerical models requires a good description of the surf zones through refined meshes which poses a serious challenge in terms of computational time for regional applications however thanks to the recent development of parallel computing techniques and the access to more computational resources it is nowadays possible to represent the wave setup in storm surge modelling systems at regional scales dietrich et al 2010 bertin et al 2015a krien et al 2017 and better understand its impact several authors revealed that the wave induced setup can substantially contribute to the storm surge under energetic wave conditions and even dominate the other drivers along coasts characterised by narrow to moderately wide shelves lerma et al 2017 or at volcanic islands kim et al 2010 kennedy et al 2012 pedreros et al 2018 the wave setup can range from several tens of centimetres to values of about 1m near the shoreline pedreros et al 2018 guérin et al 2018 while regional wave setup can reach tens of centimetres bertin et al 2015a fortunato et al 2017 however the contribution of wave setup in harbours where tide gauges are usually located is not fully clear in the scientific community e g thompson and hamon 1980 melet et al 2018 suggested that the wave setup is negligible in most of the sheltered areas while aucan et al 2012 reported that the midway tide gauge located in the interior lagoon of midway atoll in the northern hawaiian islands recorded high sea level anomaly sla events corresponding to the wave setup driven by breaking waves during storms the authors even suggested that the seasonal number of sla events recorded at this tide gauge can be used as an index of the storminess in the central north pacific over climatic time scales as they found a good correlation between the two recently several studies combining numerical modelling with field observations suggested that the breaking of short waves over the ebb deltas of shallow inlets malhadas et al 2009 olabarrieta et al 2011 dodet et al 2013 wargula et al 2018 or large estuaries bertin et al 2015a fortunato et al 2017 can induce a wave setup that extends at the scale of the whole lagoon or estuary thus modelling the wave setup appears to be fundamental for the prediction of flood inundation levels and floodplain management of embayments estuaries and river entrances hanslow and nielsen 1992 this study presents a high resolution hindcast of the sea state and storm surge induced by the violent extra tropical storm klaus which made landfall in the bay of biscay on the 24 th of january 2009 as klaus produced the most energetic waves ever recorded in the southern part of the bay this storm represents a unique opportunity to investigate the contribution of short wave breaking to storm surges this process is examined in two sheltered areas of the french aquitanian coast where klaus drove the largest storm surges observed over the last 20 years the arcachon lagoon and the adour estuary a fully coupled 3d modelling system with the vortex force formalism of bennis et al 2011 is applied at the scale of the bay of biscay and the english channel the relevance of the 3d model in terms of storm surge and wave setup is compared against a conventional 2dh approach additional numerical experiments are conducted in order to analyse the impact of the wave forces on the momentum balance at the inlet of the arcachon lagoon and their tidal modulation at both studied locations lastly a sensitivity analysis is carried out to analyse the impact of the grid resolution on storm surge and wave setup predictions 2 the studied area and storm 2 1 study area the bay of biscay is located in the north east atlantic ocean bordered by france to the east and spain to the south the study area is the aquitaine coast in the south eastern part of the bay of biscay which comprises two major geomorphologic settings a first unit from the northern spanish coast to the adour estuary characterised by rocky cliffs and small creeks and bordered by a continental shelf only 20km wide and a second one from the adour estuary to the gironde estuary with a sandy coast bordered by a continental shelf which width increases up to 150km in front of the gironde estuary this study focuses on two specific locations the arcachon lagoon and the adour estuary further south fig 1 which allows to investigate the influence of short wave breaking in areas sheltered from this process the arcachon lagoon fig 1 b is a semi enclosed bay which extends at high tide over an area of 160km 2 the head of the embayment is occupied by intertidal muddy and sandy flats that account for 75 of the lagoon and is divided by a large and complex network of secondary channels the lagoon is connected to the ocean by a 5km wide tidal inlet bounded to the north by the 18km long cap ferret sand spit the inlet is characterised by a well developed ebb tidal delta covering 12km 2 two deep channels called north pass and south pass and a poorly developed flood tidal delta of 2 3km 2 michel and howa 1997 the tidal regime is semi diurnal and mesotidal with a tidal range from 0 94m to 4 93m and a mean value of 2 94m dodet et al 2019 the channels are tide dominated with currents 20 30 stronger in the north pass than in the south pass salles et al 2015 because of the well developed ebb delta and the sandbar continuation of cap ferret the swells do not propagate inside the arcachon lagoon nahon 2018 and the outer inlet can be often saturated with wave breaking senechal et al 2013 according to the hydrodynamic classification proposed by hayes 1980 the arcachon lagoon corresponds to a transitional inlet under a mixed energy regime the adour estuary fig 1 c located approximately 40km north of the spanish border is defined by a narrower channel with a width varying between 150 inlet mouth and 500m over the last 6km of the river two breakwaters protect the entrance of the harbour of bayonne from longshore currents and swell waves and help stabilising the navigation channel the influence of the breakwaters on the storm surge in the adour estuary will be discussed later in this study the tidal regime of the area is semi diurnal and mesotidal with a tidal range varying from 0 78 to 4 32m and a mean value of 2 53m dodet et al 2019 tidal currents are weak in the outer part of the estuary with values lower than 0 20m s 1 while in the river mouth velocities reach values between 1 and 2m s 1 during spring tides brière 2005 the river flow discharge ranges from 30 to 2000m 3 s 1 with an annual mean of about 300m 3 s 1 bellafont et al 2018 dodet et al 2019 analysed wave regimes along the metropolitan coasts of france and provided yearly means of wave parameters along the 30m isobath line according to their study yearly averaged significant wave height in front of arcachon and bayonne is about 1 65m yearly averages of mean wave period and mean wave direction at arcachon respectively bayonne are about 6 3s resp 7 15s and about 290 resp 310 the wave climate is however characterised by important seasonal variations at arcachon resp bayonne the significant wave height has a winter average of 2 08m resp 2 06m and a summer average of 1 24m resp 1 20m and the mean period decreases by 2 5s resp 1 5s between winter and summer seaward of the arcachon lagoon storm waves can exceed 9m in water depths of 26m butel et al 2002 2 2 the storm klaus the extra tropical storm klaus hit the french coasts in the night of the 23 rd to the 24 th of january 2009 it induced the largest storm surge observed over the last 20 years in this region of the bay of biscay with 1 10m at bayonne boucau station and 1 70m at arcachon eyrac station mugica et al 2010 arnaud and bertin 2014 previous long term records of wind speeds were exceeded in some french stations like bordeaux and bayonne with wind gusts of 44 50m s 1 and 33 39m s 1 respectively it was considered as the most damaging wind storm to affect northern iberia and southern france since the destructive storm martin in late december 1999 liberato et al 2011 in 2009 klaus was the most costly weather events worldwide with over us 6 0 billion in losses reported mainly in france and spain aon benfield 2010 liberato et al 2011 described the storm from its genesis to its impact on the french and spanish coasts and the main features of its evolution are summarised here klaus was first detected on 21 january 2009 as a small atmospheric wave perturbation due to the southward displacement of the polar jet stream the winter cyclone moved eastward at an unusually low latitude between 35 n and 45 n on the southern edge of the typical north atlantic storm track climatological envelope it underwent an explosive development on 23 january around 21 w with a deepening rate of 37hpa in 24 h probably supported by an important tropical moisture export the storm rapidly reached the bay of biscay and followed a wnw to ese track towards the coasts fig 1 a the spanish oceanographic institute ieo registered two individual wave heights over 24m from a buoy 35km north of santander between 06 00 and 07 00 in the morning of january 24 th bilbao and cap ferret buoys recorded significant wave heights reaching 13m with a peak period of 15s during the storm the centre of the low pressure system passed at 5 00 am on january 24 th over la rochelle with a minimum of 965 8hpa recorded at the nearby station of chassiron fig 1 a the highest sustained wind speeds were measured further south with a maximum of 36m s 1 at cap ferret station arcachon lagoon for a lowest pressure of 976hpa at bayonne sustained wind speed reached 21m s 1 with a minimum pressure of 983 6hpa 3 methods and data 3 1 the modelling system 3 1 1 overview of the modelling system this study uses the modelling system schism semi implicit cross scale hydroscience integrated system model of zhang et al 2016 which is a 3d unstructured grid model the model uses a combination of a semi implicit scheme and an eulerian lagrangian method to treat the momentum advection which allows to relax the associated numerical stability constraints compared to the original model selfe from which it is derived zhang and baptista 2008 schism now integrates many enhancements and upgrades including new extension to large scale eddying regime and a seamless cross scale capability from creek to ocean zhang et al 2016 ye et al 2020 a detailed description of schism the governing equations and its numerical implementation can be found in zhang et al 2015 2016 the hydrostatic solver of schism can be coupled with other modules incorporated in the modelling system such as short waves sediment transport water quality oil spills and biology the generation and propagation of short waves are simulated with the wind wave model wwmii of roland et al 2012 in this study the contribution of short wave breaking to storm surges is analysed from 3d fully coupled wave current simulations the hydrodynamic and spectral wave models share the same unstructured grid and domain decomposition which reduces the exchange of information between the models and eliminates errors associated with interpolation 3 1 2 vortex force formalism in the modelling system the 3d wave current interactions are represented with the vortex force formalism proposed by ardhuin et al 2008 as described in bennis et al 2011 its detailed implementation in schism can be found in guérin et al 2018 in the vortex force framework the mass conservation and momentum equations of the hydrodynamic model read 1 u ˆ 0 2 d u ˆ d t f v ˆ 1 ρ p a x g ζ x z ν u ˆ z f w a v e x 3 d v ˆ d t f u ˆ 1 ρ p a y g ζ y z ν v ˆ z f w a v e y in eq 1 x y z and u ˆ u ˆ v ˆ w ˆ is the quasi eulerian velocity equal to the mean lagrangian velocity u u v w minus the stokes velocity u s u s v s w s in eqs 2 and 3 f is the coriolis parameter ρ is the water density p a is the sea level atmospheric pressure g is the acceleration caused by gravity ζ is the free surface elevation and ν is the vertical eddy viscosity f w a v e x and f w a v e y are the two components of the wave forces given by 4 f w a v e x v s f v ˆ x u ˆ y w s u ˆ z j x f ˆ d x 5 f w a v e y u s f v ˆ x u ˆ y w s v ˆ z j y f ˆ d y where j is the wave induced mean pressure and f ˆ d is the wave induced non conservative forces due to depth induced wave breaking a detailed description of the wave induced non conservative forces can be found in guérin et al 2018 3 1 3 model parametrisations for 2dh and 3d models there are noticeable differences between 2dh and 3d configurations in 2dh the model uses a manning coefficient and the depth integrated current velocity to evaluate the bottom stress while in 3d the model uses the bottom roughness and the velocity computed at the top of the bottom cell in the 3d model several parametrisations are available to compute the wave enhanced bottom stress but a sensitivity analysis did not result in significant improvements which corroborates the findings of bertin et al 2015a in the central part of the bay of biscay therefore wave effects on the bottom stress are not considered in the study in the 3d model the wave effects on vertical mixing are integrated in the turbulence closure scheme umlauf and burchard 2003 following the approach of moghimi et al 2013 as described in guérin et al 2018 for both 2dh and 3d models the surface stress can be computed with a bulk formula of the form ρ a c d u 10 2 where u 10 is the 10m wind speed and c d is the drag coefficient calculated with the formulation of hwang et al 2019 the surface stress can also be computed using a wave dependent parametrisation using the friction velocity u calculated in wwmii donelan et al 1993 reported that a young sea state enhances the sea surface roughness in order to correctly represent this process and predict the subsequent storm surge mastenbroek et al 1993 and bertin et al 2015a showed that a wave dependent surface stress is required the influence of the surface stress formulation will be discussed later in this study 3 1 4 the spectral wave model wwmii solves the equation for the conservation of the wave action e g see komen et al 1994 to simulate the generation and propagation of wind generated waves the model accounts for wind input and energy dissipation by whitecapping computed according to ardhuin et al 2010 energy dissipation due to bottom friction which is modelled based on the results obtained during the jonswap project hasselmann et al 1973 and depth induced breaking computed according to the model of battjes and janssen 1978 which is parametrised with the breaker index γ and the dissipation coefficient b as wave measurements in the surf zone during the storm were not available γ and b are set to the default values of 0 73 and 1 respectively which will be discussed later finally the non linear wave wave interactions are calculated following the discrete interaction approximation of hasselmann et al 1985 and the lumped triad approximation of eldeberky 1996 in deep water and shallow water respectively a detailed description of the coupling between schism and wwmii can be found in roland et al 2012 and schloen et al 2017 at the coupling time step schism provides wwmii with fields of 2dh currents and water levels while schism receives wave forces from wwmii 3 1 5 model implementation the unstructured computational grid used to perform the hindcast of the storm covers the whole bay of biscay from 10 w to the french coasts the english channel and a part of the north sea up to 55 n fig 1 a the grid has 281000 nodes in the horizontal with a spatial resolution ranging from 5000m along the open boundary to 35m along the shoreline of the studied areas i e the arcachon lagoon and the adour estuary in the vertical the grid is discretised in 35 s levels for the 3d simulations the circulation model is forced at its open boundaries by the 16 main astronomical constituents linearly interpolated from the regional model of bertin et al 2012 the tidal potential is switched off since a sensitivity analysis revealed a negligible effect on tidal predictions after calibration of the tidal model the bed roughness in the 3d model is set to 0 0001m in the open ocean and 0 002m in the arcachon lagoon and the adour estuary in the 2dh model a manning coefficient of 0 02 is employed for the open ocean while a value of 0 029 is considered for the adour estuary and the arcachon lagoon the manning coefficient used for the arcachon lagoon is between the values used by cayocca 1996 0 028 and nahon 2018 0 032 the simulations are started on the 22 th of january 2009 two days before the peak of the storm and last 4 days the time step is set to 60s for both the hydrodynamic and the wave models in the 2dh and 3d simulations over the whole domain the circulation model is forced by hourly 10m wind speed and sea level pressure fields from the climate forecast system reanalysis cfsr saha et al 2010 the datasets are provided on a regular grid with a spatial resolution of 0 312 and 0 5 for the wind and the atmospheric pressure respectively wwmii is forced with the cfsr wind fields over the whole domain wwmii is also forced along the open boundaries by time series of directional wave spectra previously computed from a regional application of the wavewatchiii wwiii spectral wave model described in bertin et al 2015a wind fields from cfsr are also used to run the wwiii model over the north atlantic ocean 3 2 wave and water level observations the accuracy of the wave predictions is evaluated with the measurements recorded by three buoys in the bay of biscay see fig 1 a for their location the biscay buoy is a non directional buoy located by 4500m depth operated by météo france and uk met office the cap ferret and bilbao buoys are located in more intermediate water depths of the southern part of the bay of biscay depths of 50m and 600m respectively and are operated by cerema and puerto del estado respectively the three buoys provide time series of significant wave height h s while the mean wave period t m 02 is available at cap ferret and biscay buoys and the peak wave period t p at bilbao buoy wave bulk parameters are estimated every 60 min at biscay and bilbao buoys and every 30 min at cap ferret buoy since the atmospheric data used to force the model has a hourly time resolution the wave predictions cannot represent the sub hourly variability and the measurements at cap ferret buoy are therefore averaged over one hour to yield a consistent comparison with the model predictions simulated water levels are validated through a comparison against observations recorded with a 10 min sampling interval during the storm period at the two tide gauges of arcachon eyrac and bayonne boucau see fig 1 b and c for their respective location a tidal prediction is obtained based on a 5 year long time series 2008 2012 with a harmonic analysis using the utide code codiga 2011 tides are reconstructed with the 67 main astronomical constituents previously computed note that in the north east atlantic ocean the constituent sa results from a combination of thermo steric and atmospheric effects bertin et al 2015b payo payo and bertin 2020 therefore it is not included in the tidal prediction since storm surges are computed as the difference between the observed water level and the astronomic tidal prediction 4 modelling results 4 1 atmospheric forcing in order to validate the atmospheric forcing originating from cfsr a comparison is performed against field observations available during the storm and collected at the meteorological stations of cap ferret and bayonne see fig 1 a for their location the comparison fig 2 of modelled against observed 10m wind speeds hereafter u10 and sea level pressure hereafter slp reveals that slp is well reproduced with a root mean square error hereafter rmse lower than 1 5 hpa at both locations at cap ferret u10 is accurately predicted with a rmse of 2 3m s 1 although with a slight underestimation of approximately 4m s 1 two hours before the peak of the storm since the meteorological station at bayonne is located at 75m above sea level and 3km inland the model providing 10 m wind speed with a 0 3 resolution does not accurately reproduce the observations which probably explains the positive bias of 1 6 m s 1 overall it should be noted that the intensity of the storm is correctly represented peak values of u10 are reasonably predicted with stronger values at cap ferret 34m s 1 than at bayonne 22m s 1 4 2 wave predictions modelled wave bulk parameters are compared against the measurements available during klaus at cap ferret bilbao and biscay buoys fig 3 the comparison reveals a good agreement between modelled and measured data h s is well reproduced with a rmse ranging from 0 51 to 1m which corresponds to a 10 17 error once normalised by the mean of the observations hereafter nrmse however for the three stations the model displays a positive bias of 0 35 0 50m at cap ferret and biscay buoys and 0 70m at bilbao buoy it should be noted that the larger error at bilbao buoy is partly due to a one hour time lag representing 35 of the bias and the nrmse which we are unable to explain the model correctly captures the peak storm wave height with less than 10 error at the three buoys t m 02 available at cap ferret and biscay buoys is well predicted with a nrmse less than 6 while at bilbao buoy t p is adequately reproduced with a 10 nrmse 4 3 storm surge and water level predictions a tide only simulation is first performed and the modelled water levels are compared against the tidal predictions based on the observations at each station the tidal forcing together with the distribution of the manning coefficient yields good results with a rmse on tides of 0 11m at bayonne and 0 08m at arcachon not shown the effect of the parametrisation of the surface stress on the storm surge is investigated by comparing simulations using the bulk formula of hwang et al 2019 and the wave dependent approach see section 3 1 3 this comparison reveals moderate differences between both parametrisations lower than 0 05m with the predictions of the model using the bulk formula slightly better matching the observations to explain the negligible effects of the wave dependent approach on the storm surge predictions the sea state is characterised by the wave age defined as c p u 10 where c p is the peak wave phase speed considering a 20 hour window centred on the storm peak the wave age varies from 0 7 to 2 3 with an average value of 1 32 with a standard deviation of 0 5 which is characteristic of a mature sea state and explains the very slight impact of the wave dependent approach on the results this behaviour corroborates the study of bertin et al 2015a who showed that the surface stress was little dependent on the sea state for the storm joachim characterised by comparable wave height and peak period as during klaus according to these results the bulk formula of the surface stress is adopted in the rest of the study the contribution of short wave breaking to the storm surge is analysed by comparing a first simulation without wave forces and a fully coupled simulation i e including wave forces hereafter referred to as the baseline model the modelled storm surges are obtained by subtracting the tide only simulation to each case of simulation the results are presented in fig 4 where each simulation is compared against storm surges and water levels observed at bayonne and arcachon during the storm the baseline model accurately reproduces the water levels with a rmse of 0 09 and 0 15m at arcachon and bayonne respectively the storm surges are well predicted by the model with a rmse of 0 12m at bayonne and 0 10m at arcachon although a 0 25m underestimation is noticed at this station approximately two hours before the storm peak without wave forces storm surge and water level predictions considerably deteriorate compared to the baseline model with a rmse two to three times larger at both locations the modelled water levels display a negative bias ranging from 0 18 to 0 24m the surge peak is underestimated by 0 40 0 45m at arcachon and bayonne which results in a negative bias of 0 23m over the duration of the storm the comparison of both simulations reveals that the wave setup driven by the wave forces in the baseline model accounts for 40 and 23 of the surge peak in the adour estuary and the arcachon lagoon respectively which explains that the baseline model much better matches the observed peak values in order to get a spatial overview of this process modelled storm surges with and without wave forces as well as their difference are computed at the scale of the arcachon lagoon and the adour estuary fig 5 in the adour estuary the storm surge in the fully coupled simulation increases by more than 0 5m at adjacent beaches while being almost constant inside the estuary fig 5 a the comparison between fig 5 c atmospheric surge only and fig 5 e wave setup only reveals that this behaviour is due to the development of a wave setup along adjacent shorelines reaching up to 0 75m and extending at the scale of the whole estuary where it raises the water level by 0 45m a different pattern can be observed in the arcachon lagoon where the storm surge in the fully coupled simulation increases from the inlet to the lagoon head fig 5 b the comparison between fig 5 d and f suggests that this behaviour results from the increase in atmospheric surge towards the lagoon head combined with the development of a wave setup reaching 0 40m at the scale of the lagoon as in the adour estuary the wave setup develops at the inlet and then exhibits a plateau inside the lagoon along the adjacent shorelines of the lagoon the maximum wave setup reaches 0 80m the maximum wave setup along the adjacent shorelines are not shown in fig 5 e and f as computational nodes dry in the tide only simulation are not represented 5 contribution of wave breaking to storm surges 5 1 model predictive skills wave parameters are accurately reproduced by the model and correspond to the state of the art considering previously published studies led under storm wave conditions e g kerr et al 2013 bertin et al 2012 staneva et al 2016 storm surges are also well predicted with errors similar or even lower compared to previously published studies e g kerr et al 2013 brown et al 2013 bertin et al 2015a in details the storm surge is underestimated by up to 0 25m during the first part of the storm peak at arcachon this can be explained by an underestimation of the cfsr sustained wind speeds by up to 4 m s 1 during this period fig 2 which leads to a wind induced surge lower than expected this hypothesis was tested by correcting wind speeds empirically on the time steps corresponding to this period cf appendix the results reveal that this correction almost cancels out the local underestimation in the surge thus supporting this hypothesis in the adour estuary model results at an earlier stage of this study showed a 0 05to 0 1m negative bias in the storm surge before the storm peak when the breakwaters bounding the estuary mouth were considered as impermeable walls in fact these breakwaters are made of large blocks 4 to 40 tons that allow large amounts of water to flow through the structures when a gradient in water levels exists on both sides of the structure prof abadie pers com such flows can take place when a wave setup develops at adjacent beaches a process already reported at other engineered estuaries hanslow and nielsen 1992 hanslow et al 1996 in order to account for these possible flows we took advantage of hydraulic structure options implemented in the code although this parametrisation improves storm surge prediction by 0 04m verifying the adequate representation of these flows is outside the scope of the study and would deserve a specific analysis the comparison of the results between the baseline model and the model without wave forces fig 4 reveals that including wave forces in the circulation model substantially improves its predictive skills the analysis of the different terms included in wave forces eqs 4 and 5 shows that the wave dissipation term by depth induced breaking is clearly dominant over the vortex force and the wave mean pressure terms in accordance with previous studies staneva et al 2016 this analysis highlights the importance of accounting for short waves in storm surge modelling systems provided that wave energy dissipation due to wave breaking is correctly represented guérin et al 2018 investigated wave induced circulation in a surf zone with varying bed slope the authors computed the wave breaking process according to the model of thornton and guza 1983 in which they calculated the breaking index γ and the dissipation coefficient b as a linear function of the beach slope the authors showed that this adaptive approach improved the predictions of short wave bulk parameters and wave setup by 30 following this study pezerat et al 2020 showed that a dissipation coefficient b taken at 40 times the local bed slope strongly improves wave predictions at gently sloping shorefaces 1 1000 at both study sites bottom slopes are much steeper 1 50 to 1 100 so that this adaptive parametrisation results in values for b close to the default value of 1 indeed a sensitivity analysis shows that the adaptive parametrisation of pezerat et al 2020 yields very similar short wave and setup predictions compared to the default values for γ and b in the model of battjes and janssen 1978 new field experiments are required to investigate further wave dissipation mechanisms in coastal zones and validate the numerical model under very high energetic conditions although such field deployments remain very challenging finally the comparison between 2dh and 3d simulations reveals only modest differences with water level and surge predictions slightly improved in 3d in arcachon and slightly deteriorated in bayonne fig 6 in the arcachon lagoon improved storm surge predictions are obtained before and during the storm peak when winds blow from sw to w and drive an ekman transport towards the coast a process better represented with a 3d model roland et al 2012 in the case of the adour estuary maximum wave setup at adjacent beaches is slightly lower in 2dh compared to 3d but extends further offshore thereby more impacting water levels in the estuary guérin et al 2018 showed that the depth varying circulation driven by short waves in surf zones can increase the wave setup along the coast but this process is only substantial at steep beaches i e mean slope of 1 30 and over also these authors reported that 3d runs yield larger wave setup compared to 2dh runs very close to the shoreline so that reproducing these differences requires a spatial resolution of a few metres which is one order of magnitude finer than in this study 5 2 momentum balance previous studies already reported the development of a wave setup in inlets river entrances and shallow lagoons hanslow and nielsen 1992 hanslow et al 1996 dunn et al 2001 oshiyama et al 2001 tanaka et al 2001 2003 nguyen et al 2007 bertin et al 2009 malhadas et al 2009 olabarrieta et al 2011 dodet et al 2013 wargula et al 2018 which can be further investigated by analysing the momentum balance at the inlet hench and luettich jr 2003 analysed the momentum balance without waves in the beaufort inlet in north carolina and in an idealised inlet and reported that near maximum flood and ebb the along stream momentum balance in both cases is dominated by advection barotropic pressure gradient and bottom friction olabarrieta et al 2011 corroborated these results in a study conducted in willapa bay usa during a storm event by activating the wave forces in their fully coupled modelling system they also revealed that they can substantially change the barotropic pressure gradient and the bottom friction while being one of the dominant terms in the momentum balance in the inlet area these findings were then corroborated by dodet et al 2013 and wargula et al 2014 in particular dodet et al 2013 combined both modelling and observations to study wave current interactions on the albufeira lagoon a shallow wave dominated tidal inlet in portugal during energetic oceanic swell conditions the authors showed that the wave forces term oriented towards the lagoon was of the same order of magnitude as the other terms in the momentum balance in the inlet which therefore had a significant impact on the hydrodynamics including a setup that developed within the lagoon recently fortunato et al 2017 conducted a high resolution hindcast of the storm surge associated with the 1941 storm that made landfall in the north of portugal and has driven the development of a large surge in the tagus estuary their model results suggested that the breaking of storm waves generated a wave setup of up to 0 50m in the tagus estuary showing that a substantial wave setup can also impact water levels at the scale of a large estuary this phenomenon is explained by the authors as the result of large onshore directed wave forces owing to storm waves breaking over the ebb delta generating a wave setup that extended beyond the surf zone and in the inlet the previous analysis of figs 4 and 5 suggests that such a phenomenon occurred at the arcachon lagoon during klaus large wave breaking on the ebb delta generated a wave setup that affected the whole lagoon to understand the underlying mechanisms the magnitude of the leading terms of the momentum equations i e the barotropic pressure gradient term third term of the right hand side of eqs 2 3 the wave forces last term of the right hand side of eqs 2 3 the bottom stress and surface stress terms are computed at the inlet of the arcachon lagoon fig 7 in order to analyse the momentum balance at mid flood and mid ebb under similar forcing corresponding to the peak of the storm two additional simulations are performed where tides are shifted the analysis of fig 7 shows that the outer part of the inlet behaves like a sandy beach with a balance between the wave forces hereafter wf and the barotropic pressure gradient hereafter bpg term battjes and stive 1985 lentz and raubenheimer 1999 in this area the wf reach values one order of magnitude larger than the bottom stress hereafter bs and the surface stress hereafter ss terms the dominant role of wf in the momentum balance at the inlet corroborates the findings of olabarrieta et al 2011 and dodet et al 2013 in the inlet channel the wf become much weaker and the alongstream dynamics is controlled by a balance between the bpgr and the bs terms which is typical of tidal channels hench and luettich jr 2003 between the flood and the ebb the signs of the bpgr and the bs terms are inverted except in the outer part of the inlet where the bpgr term compensates the wf during all tidal phases the major contribution of the wave forces to the momentum balance in the inlet directly explains the strong effect of short wave breaking on the hydrodynamics the main impact being a wave setup that reaches several tens of centimetres within the lagoon fig 4 in more details the rapid decrease in wf inside the lagoon explains why the wave setup displays a plateau inside the lagoon fig 5 f over the ebb delta the wind driven surge reaches approximately 0 40m fig 5 d and assuming 0 35m of inverse barometer effect which is comparable to the wave setup while ss are one order of magnitude lower than wf this behaviour is explained by the fact that strong wf only extend over the 3 km wide ebb delta while the wind effect is integrated across the 60 km wide shelf inside the lagoon the atmospheric surge further grows as the water depth decreases fig 5 d in the adour estuary the weaker atmospheric surge fig 5 c is explained not only by weaker winds fig 2 but also by the narrower continental shelf indeed many studies already demonstrated that for a given wind speed the wind driven surge is also controlled by the shelf width such as in the bay of biscay bertin et al 2012 in north sea wolf and flather 2005 or in the gulf of mexico kennedy et al 2012 5 3 tidal modulation of the wave setup some of the studies that highlighted the development of a wave setup in tidal inlets also suggested that the wave setup can be tidally modulated olabarrieta et al 2011 dodet et al 2013 fortunato et al 2017 fortunato et al 2017 showed that the wave setup that developed in the tagus estuary mouth during the 1941 storm was strongly tidally modulated with values of 0 10 0 15m at high tide while being three times larger at low tide with values of 0 30 0 35m the authors attributed this phenomenon to more intense wave breaking on the ebb delta at low tide when waves do not break over the ebb delta they propagate into the inlet or to the coast in the vicinity of the estuary mouth and thus their contribution to the setup inside the estuary is lower in this section the tidal modulation of the wave setup is investigated at the arcachon lagoon and the adour estuary with additional numerical experiments the arcachon lagoon exhibits large intertidal flats which makes the tidal propagation and asymmetry very sensitive to the mean water depth therefore tidal propagation is different when the wave setup raises the mean water level of the lagoon computing the wave setup as the difference between a simulation including tides and waves and a simulation with tides only results in difference not only including the wave setup but also the differences in tidal levels due to the higher mean water level in the coupled simulation a process also referred to as tide surge interactions to overcome this problem a series of stationary runs is performed with constant water levels and wave forcing fig 8 a two sub grids of smaller extent covering each studied area are forced at the ocean boundary by constant water elevations ranging from 1 5m to 1 5m and a jonswap spectrum to simulate short waves the spectrum is characterised by a significant wave height of 14m and a peak period of 15s which corresponds to the peak values reached during klaus in the region in the case of the arcachon lagoon the results reveal a small tidal modulation of the order of 0 07m fig 8 b the wave setup being larger at low tide at the adour estuary the tidal modulation is stronger with a wave setup reaching 0 60m when the mean sea level is lowered by 1 5m and decreasing to 0 45m when the mean sea level is increased by 1 5m contrary to the tagus estuary where the ebb delta is submerged with depths of the order of 5 m relative to the mean sea level the ebb delta of the arcachon lagoon extends 3km offshore and includes an elongated supratidal bank the arguin bank this setting causes the wave breaking to be almost full in front of the inlet even at high tide at lower tidal elevations wave energy mostly dissipates on the terminal lobe while at higher tidal stages waves also break over the supratidal sand bank the wave setup exhibits therefore a slight tidal modulation unlike the tagus estuary fortunato et al 2017 at the adour estuary the bathymetry is subtidal which implies that the lower the water level the larger is the wave energy dissipation and the wave setup these results indicate that tide induced water level variationschange the spatial gradients of short wave energy dissipation rates which in turn controls the wave setup depending on the morphology of the inlet the wave setup along the shoreline and in the lagoons or estuaries can experience significant tidal modulations as well tidal currents which are strong in estuaries or tidal inlets can also affect the propagation of short waves ardhuin et al 2012 rusu et al 2011 dodet et al 2013 bertin et al 2019 and subsequently the wave setup during flood waves following currents decrease while during ebb waves propagating against currents increase shifting the position of the breaking point seaward dodet et al 2013 the impact of tidal currents on short wave propagation is verified by comparing water elevations from runs including tides and waves and activating or not the feedback of currents on waves model results at the arcachon lagoon and at the adour estuary show that switching off the effects of tidal currents on short wave propagation has a small impact on wave setup lower than 0 01m this finding corroborates the study of fortunato et al 2017 which reported that the tide induced water level variations at the mouth of the tagus estuary are the main driver for the tidal modulation of the wave setup compared to tidal currents effects the comparison of the effect of tides on wave setup between both studied locations emphasises that tidal modulation is site specific in areas such as the adour estuary the higher wave setup is produced close to low tide and the tidal modulation amplitude increases with increasing tidal range such tidal modulation can therefore limit the contribution of short wave breaking to coastal flooding which mostly occurs during high tide in macro tidal regions 5 4 sensitivity of storm surge and wave setup calculation to spatial resolution recently several storm surge numerical models using unstructured grids have been developed e g kerr et al 2013 such models allow to correctly represent complex shorelines and coastal embayments using a variable grid resolution usually coarse in the deep ocean several kilometres to tens of kilometres and down to few hundreds of metres in the nearshore however such a resolution in coastal areas may not be sufficient to adequately represent small coastal morphological features such as lagoons and thus enable the model to provide accurate storm surge predictions shen et al 2006 also this study reveals that the wave setup generated by wave breaking during extreme events can greatly contribute to the storm surge even in areas sheltered from wave breaking such as lagoons and estuaries accounting for short waves in storm surge operational modelling is thus of key importance to correctly predict water levels in coastal areas during storm events and thereby improve emergency responses however a good evaluation of the wave setup requires a resolution fine enough in the surf zones which is not always possible in operational modelling systems kohno et al 2018 therefore an important question rises here how well do surf zones need to be spatially resolved in order to correctly estimate the contribution of wave setup to the storm surge the sensitivity of the storm surge wave setup to the model resolution is analysed at the arcachon lagoon region by simulating the sea state and storm surge associated to klaus with different grid resolutions the grid resolution used for the baseline model hereafter bm which goes down to 35m in the nearshore is modified to get two additional computational grids with spatial resolution from the inner shoreface to the nearshore degraded to 200 and 1000m the surge is evaluated at two locations along the coastline in the inner part of the lagoon at the eyrac tide gauge and at the shoreline exposed to the ocean computed as the average value of the storm surge in an area defined to the south of the inlet see fig 1 b this sensitivity analysis is not carried out at the adour estuary since the inlet mouth with a maximum width of 150m cannot be represented with such resolutions the results show that the modelled water levels and storm surges on the open ocean beach are lower when the grid resolution coarsens fig 9 indeed while tidal predictions show little sensitivity to the grid resolution the peak of the surge simulated with the bl resolution reaches 1 65m while being 30 and 65 higher than the surges obtained with the 200 m and 1000 m resolutions respectively a detailed analysis reveals that these differences are mostly explained by wave setup which is poorly represented with a coarse grid in the lagoon the results reveal a different behaviour of the model fig 9 surprisingly the predicted storm surge is less sensitive to the grid resolution compared to the open ocean beach the three grid resolutions well reproduce the peak of the surge with the 200m and 1000m grid resolutions resulting in a slightly lower surge than the bm resolution 0 05m however the storm surge modelled over the total duration of the storm is deteriorated with the 1000m resolution rmse of 0 17m compared to the bm and 200m resolutions rmse of 0 10m also water elevation is poorly predicted with the 1000m resolution which yields a rmse of 0 48m against 0 085m and 0 14m with the bm and 200m grid resolutions respectively when the channels of the lagoon are not correctly represented the tidal propagation in the lagoon is poorly reproduced which impacts the predictions of water level and storm surge this sensitivity analysis of model results to grid resolution reveals a contrasting situation between the inner lagoon where wave setup is reasonably represented even with a coarse resolution and adjacent sandy beaches where modelled wave setup is almost nil when using a coarse resolution this behaviour is directly explained by the cross shore extension of the surf zone which is of the order of 1000m at adjacent beaches but ranges from 3000 to 5000m in front of the lagoon as a rough guideline we estimate that accounting for wave setup in storm surge models requires at least 5 grid elements across the surf zone which implies using a finer spatial resolution when the beach slope increases and the wave height decreases this corroborates the findings of sashikant nayak et al 2012 who investigated the sensitivity of wave setup predictions to grid resolution considering idealised beaches of slope ranging from 1 80 to 1 10 6 conclusion the fully coupled modelling system schism using a vortex force formalism was used to hindcast the sea state and storm surge associated to the strongest storm that occurred in the southern part of the bay of biscay for the last 20 years after the verification of the model with wave and water level observations available during the storm the analyses of the simulations revealed that the predictions of the storm surges at the arcachon lagoon and the adour estuary were improved by 50 to 60 when the wave forces were accounted for the wave setup induced by the storm waves breaking in the vicinity of these two inlets extended outside the surf zones and significantly increased the water level at the scale of the whole lagoon and estuary to understand the impact of storm wave breaking on the hydrodynamics of the tidal inlets the local momentum balance was analysed at the inlet of the arcachon lagoon by reaching values one order of magnitude larger than the bottom stress and the surface stress terms the wave forces were one of the leading terms of the momentum balance and thereby greatly affected hydrodynamics in the inlet the main impact being the development of a wave setup at the scale of the whole lagoon further analysis showed that the wave setup in tidal inlets can be tidally modulated while this phenomenon is site specific and depends on the morphology of the inlet at arcachon as the ebb delta is characterised by supra tidal sand banks wave breaking is total at all tidal phases the wave setup exhibits therefore a slight tidal modulation at bayonne waves are subjected to more intense breaking at low tide than at high tide the tidal modulation of the wave setup is thus more pronounced finally a sensitivity analysis of the storm surge and wave setup to the spatial resolution of the computational grid was carried out this work revealed that the calculated wave setup at the shoreline is highly sensitive to the grid resolution in the lagoon the modelled storm surge and wave setup were found to be comparable between different grid resolutions while tidal propagation cannot be accurately represented with a resolution of 1000m this study highlighted the need to account for wave breaking in operational storm surge models although resolving the wave setup requires a spatial resolution that depends on the width of the surf zone which is in turn controlled by the bottom slope and the wave height in a context of upcoming altimetry satellite missions with spatial footprints below 1000m swot durand et al 2010 the results presented in this study are of key importance as they show that the wave setup can impact the water level in sheltered areas such as harbours large lagoons and estuaries as these coastal areas are usually instrumented with tide gauges that are used to calibrate altimeter measurement systems it is crucial to determine the physical drivers of the water level variations recorded at these stations credit authorship contribution statement laura lavaud writing original draft software investigation formal analysis xavier bertin supervision conceptualization methodology software investigation formal analysis writing original draft kévin martins software investigation formal analysis writing original draft gael arnaud investigation formal analysis marie noëlle bouin investigation formal analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the editor as well as the three anonymous reviewers are greatly acknowledged for their constructive comments which resulted in a substantial improvement of this paper laura lavaud is supported by a phd fellowship from the region nouvelle aquitaine and the unima engineering consulting company xavier bertin thanks the support from the regional chaire program evex funded by region poitou charentes kévin martins acknowledges the financial support from the university of bordeaux through an international postdoctoral grant idex nb 1024r 5030 the authors greatly acknowledge schism developers community wave data in the bay of biscay were provided by météo france uk met office cerema http candhis cetmef developpement durable gouv fr and puerto del estado http www puertos es water level data and atmospheric forcings were provided by the french oceanographic and hydrographic institute shom through the refmar portal http data shom fr and ncep cfsr respectively appendix the underestimation of the storm surge before the peak of the surge can be attributed to a negative bias in the 10m wind speed from cfsr in order to verify this hypothesis the modelled wind speed from cfsr is increased by 12 15 over three time steps before the storm peak fig a 10 a the comparison between the original modelled storm surge and the storm surge computed with the tuned wind speed fig a 10 b shows a significant difference at the considered period the rmse is improved by 20 and the localised error is cancelled out these results confirm that the underestimation of the storm surge at this stage of the storm is due to a local negative bias in the modelled wind speed of the order of 4m s 1 
23937,while lateral boundary conditions are crucial for the physical modeling of ocean dynamics their estimation may lack accuracy in coastal regions data assimilation has long been used to improve accuracy but most of the widely used methods are difficult to implement we tried a new and an easy to implement method to estimate boundary conditions this method uses data assimilation with a stochastic gradient descent and successive approximations of the boundary conditions we tested it with twin experiments and a more realistic setting on a tidal model in the lagoon of ouano in new caledonia the method proved successful and provided good estimation of the boundary conditions with various settings of subsampling and noise for the pseudo data in the twin experiments but there were important oscillations in the experiments with more realistic settings here we present those results and discuss the use of our new and easy to implement method keywords parameters identification tidal modeling stochastic algorithms data assimilation 1 introduction lateral boundary conditions for physical models are of major importance and very difficult to tune with accuracy in coastal regions james 2002 they strongly constrain the water circulation inside the domain by controlling the exchanges at the boundaries of the domain of interest in many applications they are derived from larger scale models either with nesting techniques or with interpolations unfortunately those boundary conditions are not always directly interpolable especially for coastal regions where the bathymetry frictional effects and submesoscale variability constrain the small scale dynamics a simple solution can be to implement data assimilation to adjust the boundary conditions taillandier et al 2004 however some of those data assimilation techniques require the development of an inverse model to compute the corrections to apply to the boundary conditions devenon 1990 this process is time consuming and requires a good knowledge both of the direct model and of inverse method discretization theory lellouche et al 1998 finally the inverse model can simply be impossible to develop if the code of the direct model is not available some other methods do not require the coding of an inverse model some methods use repeated runs of the direct model with perturbations in the parameters to generate approximations of the direct model that can then be inverted such as the reduced model method of vermeulen and heemink 2006 and altaf et al 2012 or hoteit and köhl 2006 other methods do not use approximations of the direct model and directly use the perturbations of the parameters to determine the best set of parameters such as the simultaneous perturbations stochastic approximation spsa algorithm spall 1998 messié et al 2020 those two kinds of methods both require limited coding work since they only need access to the parameters to be estimated in the model and to outputs of the model and can therefore be implemented on any model their main limitation is that they require large number of iterations of the direct model and unlike methods relying on an inverse model this number of iterations grows exponentially with the number of parameters therefore parameters such as lateral boundary conditions which are clusters of at least hundreds of grid points each with independent values were out of the reach of such methods but those methods could be used to estimate approximations of those kinds of parameters this is done naturally for the reduced model methods by the decomposition in empirical orthogonal functions but for the spsa algorithms finding good approximations is still an open problem for example boutet et al 2015 used the spsa algorithm for determining friction coefficients in the gascogne gulf by covering the gulf with a domain of constant friction according to the type of sediments found and outside the domain of oceanography tympakianaki et al 2015 used clustering techniques to generate approximations the contribution of altaf et al 2011 was important in that regard because they compared the results of the spsa algorithm with piecewise approximations with the results a reduced model for the calibration of the depth in a north sea model they concluded that the spsa method had the advantage of being able to identify many parameters at once with these approximations however the use of piecewise constant functions quickly becomes difficult when the parameters vary continuously or when the precise location of those corrections is not known in advance examples of cases where those corrections are hard to know a priori are coral lagoons where the steep bathymetry the presence of a coral reef and the optional connexion with other lagoons can bring unexpected variations in currents and sea level most approximation methods try to identify the coefficients of a given set of basis functions for approaching boundary conditions this approach is limited because the only way to improve the approximations is to redo the entire approximations with new basis functions instead we chose to use several sets of basis functions whose coefficients could be easily linked from one set to another so that approximations could be improved without having to redo the whole procedure this methodology had already been tested jahns 1966 and thevenaz et al 1998 but never for boundary conditions in oceanography to the authors knowledge we performed twin experiments on a tidal model of the ouano lagoon in new caledonia to test our method it is of interest as the boundary conditions are complex to determine due to the influence of the wave breaking patterns the tides and the high bathymetric and friction variations due to the coral barrier sous et al 2017 it enabled us to see how the approximations worked in a complex situation where boundary conditions sometimes vary very sharply and sometimes are nearly constant in addition by adding noise and subsampling the data we tested the robustness of the method in more realistic settings we finally tested the method in a more realistic setting with no a priori knowledge of the boundary conditions and data collected during survey of the lagoon 2 material and methods 2 1 study zone and parameters we studied the circulation patterns of the ouano lagoon this lagoon is about 30 km long and 10 km wide and is connected to a wider lagoon system located on the south west coast of new caledonia the coral reef separates the pacific ocean and the lagoon and forms a porous wall with only 2 passes directly linking the ocean and the lagoon the currents inside this lagoon are dominated by tides with a spring amplitude of about 0 8 m chevalier et al 2015 sous et al 2017 the wave breaking occurring on the barrier reef also drives the currents chevalier et al 2015 as it creates a horizontal gradient of radiation stress across the coral barrier that leads to a water flux inside the lagoon sous et al 2017 the wave breaking and the currents it generates are in addition influenced by the tidal level of the lagoon therefore the determination of accurate tidal boundary conditions in the lagoon is necessary for any proper physical modeling tidal boundary conditions were provided by c chevalier so as to be able to test the method under realistic conditions these consisted in phase and amplitude for the m2 component of the sea level are shown in fig 1 both variables exhibited zones with strong variations due to the fact that the southern boundary and the northern boundary passed through the barrier and therefore encompassed both the ocean and the lagoon furthermore boundary conditions in shallow zones may have been influenced by the current coming from the rest of the new caledonia lagoon complex ouillon et al 2010 jouon et al 2006 the model used here is a reduced tidal model it is based on a decomposition of the velocities and elevation in tidal modes in the shallow water equations it was used because the equations can then be reduced to a purely boundary conditions problem the derivation of the model is given in the appendix 2 2 gradient descent algorithm to identify boundary conditions for each of our successive problems we used a gradient descent algorithm the one we used is strongly inspired by the simultaneous perturbation stochastic approximation spsa algorithm spall 1998 that we adapted in order to speed up convergence as for any gradient descent algorithm the objective was to determine the gradient of a cost function according to certain parameters and to follow the gradient to determine an extremal value in our case we wanted to minimize the discrepancies between the output of a model and pseudo data previously generated by adjusting the boundary conditions of the model the metrics used are shown in eq 1 1 j θ 1 2 k f θ k y ˆ k 2 here j is the cost function θ represents the set of control parameters being here the boundary conditions y ˆ represents the data or pseudo data for twin experiments and f is an operator that represents the action of the model those data in our case are the complex values of the sea level the index k serves to localize the data point and the corresponding model output we chose this cost function because it is standard in the field and corresponds to an addition of gaussian uncertainties some terms can be added in order to smooth the output sasaki 1970 or to reduce the size of the search space blum et al 2009 but those were not used here now we have to determine new parameters incrementally if we index the iteration of the update of parameters with i then we can write g i as the gradient of the cost function according to the parameters and a i a parameter used for controlling the size of updates at the iteration i the gradient descent then update the parameters is written as 2 θ i 1 θ i a i g i the modification of parameters is done until a stopping criterion is reached this criterion can be a measure of the magnitude of the gradient g i or a number of iterations of the gradient descent for example in this procedure the value of the parameters a i is crucial as well as the value of the gradient g i as it indicates the direction to reach the minimum here we did not compute the gradient directly but rather an approximation to the gradient the approximation was the one used in the spsa spall 1998 algorithm and corresponded to a directional derivative as shown in system 3 3 θ i θ i c i δ i j θ i δ i j θ i j θ i δ k c i 1 g ˆ i j θ ˆ i a i 1 g i 1 δ ˆ i g i g ˆ i β g i 1 in this algorithm the direction was determined by a random perturbation of all the parameters and the perturbation c i δ i was determined by a predetermined term c i and a sign term δ i given by a bernoulli law the perturbation and its symmetric counterpart are used to determine a centered estimate of the derivative of the cost function in this direction finally to provide an estimate closer to the real value of the gradient we used a momentum technique as described in sutskever et al 2013 this technique allows a smoother descent by averaging the current estimate of the gradient with the previous estimates of the gradient hence the determination of the gradient is given in system 4 4 g i g i β g i 1 where β is a parameter between 0 and 1 according to the importance we want to give to previous estimates in many applications this sum is normalized but here we decided to take another option this is due to the adjustment we made to our algorithm compared to the standard spsa normally the gain coefficient a i and the perturbation coefficient c i are supposed to be decreasing as in the system 5 5 a i a 0 a i γ c i c 0 i δ where a 0 and c 0 are initial estimates of the perturbations and gain coefficients a is a regularization term for the decrease and γ and δ are exponents under one spall 1998 however we used a different approach we kept the perturbation amplitude c i constant but started with a small value for the gain coefficient a i and increased it in geometric fashion during the gradient descent this avoided the necessity of choosing the right parameters for the gain of every subproblem the gain coefficient was updated to take into account the fact that we were going to solve of succession of problems and that we would not be able to tune the gain coefficient a i for each problem this is shown in the system 6 6 a i a 0 1 q i c i c 0 where q was a small parameter for the increase this growth value was chosen so that the algorithm would fit every subproblem we implemented a line search procedure we used the procedure described in armijo 1966 that checked whether the parameter modifications effectively reduced the cost functions and if not reduced the gain coefficient a i a few times by a factor depending on the ratio between the estimated cost function and the lowest one if after some reductions of the gain coefficient a i the cost function still did not decrease the gain coefficient a i was reset to its initial value and momentum terms g i 1 were cancelled for the gradient estimation in the spsa algorithm the gain coefficients and the perturbation coefficients are often decreasing with the goal of being as small as possible in the last stage of the parameters estimation in order to make only small adjustments around the minimum the growth procedure used in our case may seem to do the opposite however the line search algorithm tends to reduce the gain coefficient if we are close enough to a minimum if we combine this with a very small initial gain coefficient we could expect that close to a minimum our algorithm and the spsa would show close behavior by only performing small adjustments in contrast at the beginning of each subproblem we expect an important growth of the gain coefficient so that it adjusts to the sub problem since the gradient descent algorithm relied on few parameters we report them in table 1 2 3 successive approximations as mentioned in the introduction here we suggested determining approximations of the boundary conditions the boundary conditions were determined by amplitude and phase at each boundary point to give a complex number as shown in eq 7 7 η k η k e i ϕ k where η k is the local amplitude and ϕ k the local phase one can set the boundary conditions on a 1d vector as shown in fig 1 it is then possible to approximate boundary conditions with one dimensional functions in this case the phase and amplitude can be determined by eq 8 8 η k or ϕ k j 0 m f n b j x j x k where f n were the functions used at the level of approximation n b j were their coefficients and m the number of such functions used due to the fact that we used nodal functions for the approximations we also needed a parameter x k that specifies the position where the value of the function is fixed those positions are called the nodes the nodal functions we used are cubic splines with natural boundary conditions those functions are a collection of piecewise cubic polynomial linked by continuity and continuity of the first derivative their junctions are situated at nodes x k and their values at those nodes at the b j values 9 f n b j x j x k j 0 m a j x 3 b j x 2 c j x d j π x k x j x j f n b j x j x j b j d f n b j x j 0 d x c 0 d f n b j x j t d x c m where here π x k x j x j is the door function that is worth 1 between x j and x j 1 and 0 elsewhere the computation of the coefficients a j b j c j and d j is done following the python implementation of the algorithm of the scipy package of python the scipy community 2017 de boor 1978 overall our aim was to determine the parameters b j that minimized the cost function for a given level of approximation n once a minimum is reached or approached with a given number of nodes we used this approximation to generate a new set of nodes and node values then we performed another gradient descent to identify the minimum of the cost function relative to the new set of parameters we thought that reaching the new minimum would require few iterations of the gradient descent since the last step of minimization had brought us to a good starting point an example of this approach by successive approximations is shown in fig 2 the new nodes were created midway between existing nodes also the values of the new nodes were determined by linear interpolation between the values of the previously existing nodes 2 4 implemented tests 2 4 1 twin experiments and pseudo data we performed twin experiments to test our parameters identification algorithm we began by running our reduced model with known boundary conditions for the m2 tide those boundary conditions were boundary conditions taken from the previous work of chevalier et al 2015 we used the output of this run as references for our first assimilation we will refer to them as pseudo data in the rest of the paper as they will be used as data for our assimilation but were initially produced by the model that ensures a total compatibility of this data with the physics of the model and avoids discrepancies between the model outputs and in situ data we then used pseudo data to try to identify the boundary conditions that served to produce them since those pseudo data were outputs of the model we knew we would not have any compatibility issues between the model and the data the model was a linear one we knew that we had only one set of boundary conditions corresponding to the pseudo data if we restricted the domain of the phase from 0 to 2 π radians we also tested conditions closer to a real data assimilation by adding noise and decimating the pseudo data so that they would be closer to real in situ data it enabled us to know how the noise on data acted on the boundary conditions that were determined and how the position and number of data could affect the shape of the boundary conditions finally we performed a final experiment with in situ data and our reduced model 2 4 2 parameter identification methods we wanted to prove the efficiency of the successive approximations method was not due only to a good gradient descent algorithm but also to a well chosen set of basis functions for the approximation we then compared the results obtained with this method with results obtained by two other methods those methods did not use successive approximations and only one of them used approximations with bicubic splines but they used the same spsa like gradient descent algorithm we ran the three methods with the same number of iterations of the gradient descent algorithm we also ensured that for the methods that relied on spline approximations the approximations presented the same number of nodes at the end of the parameter identification finally we started with the same constant values of parameters on the entire boundary for each method for the successive approximations method we began with a small number of nodes and generated new ones after a certain number of iterations of the gradient descent algorithm those numbers of iterations were determined by trial and error because they generally presented a sufficient decrease of the cost function the second method used only one level of approximation and is referred to as the high order approximation method the number of nodes used in the high order approximation method is the same as the number of nodes in the final stage of the successive approximation method the third method did not use approximation and directly identified the values of phase and amplitude for each grid cell that is situated at the open boundary we refer to this method as the no approximation method we decided to split the parameter identification for amplitude and phase for the three methods in the successive approximations methods we alternated the identification of amplitude and phase also in the successive approximations method we used more gradient descent iterations for the phase than for the amplitude the parameter for the implementation of the three methods in our specific case can be found in table 2 2 4 3 noise and subsampling parameter identification was also performed in the presence of noise and subsampling for this purpose we added noise to the pseudo data by adding a uniformly distributed noise to each data point we used different levels of noise but always paired the level of noise of amplitude with that of phase we tested respectively noise levels of 0 0001 m and 0 0001 rad 0 001 m and 0 001 rad 0 01 m and 0 01 rad and 0 1 m and 0 1 rad in the results section those levels of noise are labeled with their numerical value only and not the unit we will use noise of 0 1 and not noise of 0 1 m and 0 1 rad those levels of noise were chosen to show the increase of impact of noise in the data in addition the middle values of 0 01 m and 0 01 rad s 1 are of the order of uncertainties that we have when we treat our data of the tidal elevation of the lagoon with the python implementation of codiga 2011 the values of 0 1 m and 0 1 rad s 1 correspond to extremely high noise value since we expect tidal amplitudes of the order of 0 4 m and they serve to test the robustness of the method the subsampling was performed by only keeping some pseudo data we removed them in the same way in the two directions and with no special consideration for the land we respectively kept one pseudo data on 2 4 16 and 32 in each direction for each case which ended up with only one pseudo data on 4 64 256 and 1024 4 we chose levels which respectively correspond to 16275 4143 240 55 and 12 available data points this number of data will be used as a label in the experiments finally we also performed a few tests with localized pseudo data this was done in order to see how unequally distributed data would influence the parameter determination method and how robust it was all those experiments are reported in table 3 2 5 computational cost of the algorithm the code of the reduced model took about eight seconds to run on our local computer with a core i7 processor from intel while an entire assimilation loop comprising three runs of the reduced model an estimation of the cost function and the update of the parameters took 28 s on average with our python implementation 3 results 3 1 results obtained with successive approximations we first present the boundary conditions determined with the successive approximations method in fig 3 we see different stages of the successive approximations method if those stages are all around the reference boundary conditions the 5th stage has less oscillations around the true value those oscillations are bigger between the boundary index 0 and 70 and around the boundary index 370 where variations are high between the boundary index 0 and 370 all the variations of the phase are not captured even at the 5th stage furthermore we see that the variations in this region are less steep at the 3rd stage than at the 5th in figs 4 and 5 we present the output of the model with the boundary conditions determined with the successive approximations method the output for the parameters determined with the successive approximations method is close to the original the only variations being in some oscillations close to the southern and northern boundaries apart from those variations are of the order of one millimeter for amplitude and 1 1000 of a radian for the phase 3 2 results obtained with the other two methods we present the amplitude and phase determined with the no approximation and the high order approximation method in fig 6 so that a comparison with the results of the successive approximations method can be made we see that both methods exhibit wide oscillations those oscillations are less marked and of larger scale for the high order approximation method than for the no approximation method it is interesting to note that the boundary conditions obtained with the successive approximations method are also plotted in fig 6 but are undistinguishable from reference boundary conditions 3 3 comparison of the three methods during the identification the three methods take different paths to identify parameters to show those different paths we present the changes of the cost function during the gradient descent for each the three methods in fig 7 all methods give a roughly equivalent cost function value up to the 50th iteration after that point the successive approximation method gives the lowest cost function value and the steepest slope the curves for the cost function values of high order approximation and no approximation methods only separate around the 700th iteration which is the moment of the variable shift the three methods end up with different cost function values with around 10 4 for the successive approximations method and around 1 for the high order approximation method and 10 for the high order approximation method if we use the definition of the cost function we realize that this cost function corresponds to a maximum error of 10 2 rad or m for the successive approximations method if all the error was on only one grid point the different behavior for the three methods is even more explicit if we take a closer look at the patterns of change in the cost function the high order approximation method and no approximation method have similar shapes they have a very steep descent at first and then continue to decrease slowly there is another steep descent when the parameter changes from amplitude to phase for the gradient descent of the method with successive approximations we see that we have a very steep part before the second node creation after that the gradient descent is smoother with a series of steep descents right after node creations that slowly turn into plateaus forming a stairway like overall shape we also see that after each node creation we have an increase in the cost function 3 4 noisy and subsampled pseudodata here we performed some boundary conditions identification where we used only a fraction of the available pseudo data since those pseudo data were the results of a reference run they already corresponded to the position of the model output and no interpolation was necessary to extract a fraction of those pseudo data for the subsampling we simply kept one data out of a certain number in both the y and x directions this means that if we began with 114 pseudo data in the x direction for each y level we end up with only 57 pseudo data in the x direction with a subsampling level of two and that those pseudo data are still equally spaced the different subsamplings used are illustrated in fig 8 since the decimation of the pseudo data was done in the x and y directions at the same time we refer to the different subsampling experiments by the fraction of pseudo data used in the x direction multiplied by the fraction of pseudo data considered in the y direction if we still use our example where we used one pseudo data out of two in the x and y direction we will refer to it as the experiment with a subsampling of one data point over 4 we chose levels of subsampling of one pseudo data over 4 64 256 and 1024 if we compare the number of data points available for each subsampling level of 16275 4143 240 and 12 available data points and the 33 nodes of the final splines we see that the cases with one pseudo data out of 4 and 64 still have more pseudo data than the final number of nodes of our procedure the case with one pseudo data over 256 would still have more pseudo data 55 than the final number of nodes but the land cells actually makes it close while the case with one pseudo data over 1024 has less pseudo data points than the final number of nodes at the end of the procedure we therefore expect it to behave as an underdetermined problem we show the results with different levels of subsampling in fig 9 we see that results are fairly close until the subsampling reaches one data point over 256 with so few data some oscillations begin to appear especially in the phase also the conditions determined close to the northern boundary differ strongly those oscillations are stronger with one data point over 1 1024 still those oscillations are around the reference boundary conditions we also present the results with different levels of noise added to the pseudo data in fig 10 the noise level of 0 0001 m and rad s 1 produces results that are indistinguishable from those obtained without noise the noise levels of 0 001 m or rad s 1 and 0 01 m or rad s 1 produce very comparable results where small scale oscillations start to appear finally the noise level of 0 1 exhibits very strong oscillations around the reference boundary conditions 3 5 grouped data we also performed a few experiments in which we used only 60 pseudo data located in a precise part of the domain we tried with data in the lagoon part data in the deeper ocean and data in the shallow part in the south east of the domain those locations are visible in fig 11 as we can see in fig 12 the data placed in the lagoon have oscillations of the order of a centimeter relative to the true value in the cases where the data were in the ocean we have larger oscillations in the oceanic part of the boundary and both the phase and the amplitude of the boundary conditions are underestimated in the south eastern boundary when the data are located in the south eastern shallow part of the lagoon the boundary conditions are reconstructed with an accuracy of a few centimeters but the estimates for the oceanic boundary conditions oscillate strongly with an overestimation of nearly 10 centimeters followed by an underestimation of nearly 20 centimeters 3 6 assimilation with in situ data we also tried to determine boundary conditions from in situ data gathered during the 2016 campaign in the lagoon in this situation however we had to reduce the number of nodes of the splines because we only had four data points those are shown in fig 13 it seems that the variations in the boundary conditions were large as is shown in fig 14 the mean trend of the true phase and amplitude is caught but the oscillations are very large with 10 cm for the amplitude and 0 1 rad for the phase in addition the maxima do not seem to be correlated with any variation in depth close to the boundaries those strong variations are damped in the lagoon to have outputs that are around the data with discrepancies of around 0 05 m and less than 0 01 rad as shown in fig 15 4 discussion 4 1 choice of the functions shape the results obtained with the successive approximations method were very close to the original boundary conditions compared to the other two methods not only was convergence speeded up but accuracy was also increased this is interesting for development of methods for determination of boundary conditions in a coastal model furthermore this method does not depend on a specific gradient descent method or type of parameters and could be extended to other problems such as the determination of friction coefficients or with other algorithms such as genetic algorithms we can say that the increase in accuracy and speed of convergence were indeed linked to the successive approximations as neither the no approximation or the high order approximation method could approach the reference boundary conditions the slow convergence was expected for the no approximation method as we knew that our algorithm had a speed of convergence close to that of the spsa which depends on the number of parameters even though the computing cost for gradient estimate does not for the high approximation method we expected the convergence to be faster since the number of parameters was reduced but by comparing it with the successive approximation method it seems that the starting point also plays an important role in the convergence rate and the successive approximation method effectively sped up convergence by first reducing the number of parameters and next by providing good starting points for the successive problems this method also contrasts with what had already been done with a stochastic gradient descent algorithm other examples exist such as that of altaf et al 2011 and boutet et al 2015 and hoang and baraille 2011 also used this algorithm for determining a reduced number of coefficients of approximation for a covariance field of a kalman filter in all those cases they reached an acceptable level of precision however they worked with limited number of actual parameters less than 20 each time and with parameters whose spatial and temporal variations were known before the assimilation this number of parameters is close to the number of parameters we had in our realistic application in this case we realized that our method led to strong oscillations this may be linked to the positions of the nodes of the splines or to the use of the spline approximations this could be solved by trying a dynamic placement of the nodes or by trying other basis functions such a akima interpolation or p chip that do not have such a curvature lastly the algorithmic implementation of those approximations is also to be considered for example the odd behavior that we observed at the very beginning and the very end of our determined parameters in fig 3 was due to the way a condition of zero normal gradient was imposed the algorithm we used places null derivative conditions at the extremities of the splines instead of forcing the values of the nodes at those points and extending the function with null derivative before and after those points this illustrates that not only the functions used but the way they are implemented must be carefully taken into account for good parameter identification 4 2 impact of sampling the effects of noise and of subsampling were also investigated in figs 9 and 10 we show that discrepancies stayed low for noise up to 10 of the amplitude and phase value and up to 1 16th of the pseudo data taken into account with less data we expect a less precise definition of the boundary condition therefore the problem may derive for the fact that we are looking for a precision that is beyond what is achievable by any method for this combination of model and data in that case it may be that our problem was poorly constrained even with 1 32nd pseudo data the distance and the position of the data points relative to the boundaries may play on how they are affected by them furthermore some regions are nearly isolated from the rest of the domain especially in the south eastern part of the lagoon this appeared more clearly in the experiments with the different locations of data we saw that data located in the lagoon provided the best estimates of boundary conditions showing that the lagoon zone was more sensitive to variations in the boundary conditions than the deeper oceanic zone we emphasize the need for both a good sampling strategy and a good choice of the node positions for reconstructing the function we are trying to approach perhaps the hardest part in parameter identifications with regard to determining the boundary conditions is to identify not the value but the shape of those boundary conditions indeed methods using an inverse model devenon 1990 and the methods with empirical orthogonal functions of vermeulen and heemink 2006 can provide such a shape but requires the development of a second numerical model or many iterations of the model before parameters can be determined with the successive approximations this shape is determined by the functions used for approximations and the position of the nodes hence both factors should be carefully considered with the physical knowledge of the problem to tackle in mind furthermore the successive approximation methodology presents a filtering effect on the reconstruction of the shape of the boundary conditions as is visible in figs 9 and 10 this effect is inherent to the spline interpolation method and has already been observed by unser 1999 the filtering effect could also be an advantage since depending on the number of observations that we dispose or the nature of the problem we could also not want to deal with small scale variations in addition small scale variations can be destabilizing in some applications and therefore filtering them out can be an important point as was discussed in sasaki 1970 4 3 practical implementation the implementation of the successive approximation strategy with an algorithm based on an spsa like algorithm is not necessary the pyramidal approach presented in thevenaz et al 1998 or the stepwise approach presented in jahns 1966 used other algorithms however in our case the spsa like algorithm presented many advantages as it was easy to implement and presented good convergence properties spall 1998 which are important in geophysics for models that tend to be quite complex still the reduction of the number of variables could open up the way to other algorithms based on perturbations of the parameters such as genetic algorithms for example the choice of the gradient descent algorithm would of course be highly dependent on the problem to be tackled our tests showed us some issues with the implementation of successive approximations as different gradient descent are performed one after another a robust and versatile tuning of the gradient descent algorithm is important as already pointed out there is no reason for the algorithm parameters to remain constant for two successive problems with some gradient descent algorithms the problem of the gain coefficient could be tackled by the use of algorithms that use hessian approximation as step size with the spsa algorithm there are some existing second order methods zhu and spall 2002 reddy et al 2016 some work should also be devoted to the stopping criterion as it is difficult to estimate the number of iterations necessary for different stages the number of iterations necessary depends on the adjustments needed at each stage which it would be difficult to estimate in advance some work has been done with the spsa in order to determine a stopping criterion wada and fujisaki 2013 but are so far not directly transposable to complex applications in a similar way a line search procedure was used because we knew the problem was convex and because we were not able to compute a correct step size with a second order algorithm however the use of line search algorithm is unwieldy and not suited to non convex problems thus some other methods should be used for non linear problems perhaps second order approximations could be more appropriate one of the advantages of this method is its computational cost we saw that the time required for each step of the gradient descent was largely dominated by the model runs since this method only requires estimation of the cost function and generation of vectors of parameters the ratio of the time taken by the model runs to the generation of perturbations by the algorithm is likely to increase with larger models with more data the time necessary for parameter identification is dominated by the time of the model runs this time could be reduced by parallelizing the model runs of the same iteration 4 4 perspectives here we successfully identified boundary conditions for a tidal linear model the method described in this article could be used directly for other coastal situations or other tidal components with in situ data instead of doing twin experiments in addition we could extend the work presented here to other situations in fact the problem of identifying lateral boundary conditions shares some features with other problems of interest in geophysics the high number of parameters and the continuity of the field of interest can also be found in initial conditions or friction coefficient for example the methodology of approximation with splines can also be used directly for two dimensional or even higher dimensional fields of parameters and therefore the problem addressed here can be seen as a first step towards a more general parameter identification method as a complement to these advantages our method do not require an inverse model owing to the gradient descent algorithm used also on the basis of this gradient descent algorithm we could identify parameters in non convex problems or with non linear models to perform this identification we should remove some of the adaptations that we made to the algorithm in the present work but those modifications are minor and non linear optimization has already been performed with the spsa algorithm spall 1998 outside the field of oceanography or even in the field of oceanography by boutet et al 2015 furthermore the simplicity of implementation of the method enables us to build more sophisticated tools for the study of the circulation patterns we can consider the use of the reduced tidal model as a special case of linearization of a more complex circulation model hence we could use this model in a data assimilation loop for optimizing the tidal boundary conditions for a more complex model such as croco for example 5 conclusions we performed with a linear tidal propagation model a series of twin experiments and a more realistic experiment we investigated whether a spsa like gradient descent algorithm could be used to identify boundary conditions as they represented a large number of parameters we showed it was possible with a special method involving several stages of approximations we used approximations that have coefficients whose magnitude is close to the magnitude of the function to be approximated the fact that the magnitude of the coefficients was close to the magnitude of the boundary conditions helped us in setting up the successive approximations used we were able to determine the boundary conditions of our tidal model up to a tenth of centimeter for the twin experiments the novelty here was that we succeeded with tens of effective parameters and with a methodology that would enable us to go even further however the first try outs with more realistic data locations and with in situ data showed us a more contrasted picture as for any inverse methods the precision and location of data is crucial and the successive approximations do not change that the issue that is more specific to this method is the oscillations occurring in the boundary conditions in the realistic experiments this may be improved by trying other functions for approximations with p chip or akima interpolation which reduces the oscillations or by playing with the positions of the nodes to conclude we have developed a method that was able to identify an arbitrary number of parameters and tested it with a tidal model in a lagoon setup the results were affected both by the noise in data and their sampling but also by the number of parameters of the approximation used in the method we saw an effect of filtering smaller scales of the boundary conditions there is no theoretical drawback to the use of other gradient descent algorithms or even other optimization methods with the successive approximations method but there are still some issues relative to its use with reduced numbers of data the use of approximations and the successive problems solved are fully compatible with any method however we had to modify the gain sequence of our gradient descent algorithm to perform efficiently the successive gradient descents future work will focus on the implementation of this method in realistic applications specifically we could use this method for non open source models or for models which do not have an adjoint the croco model is a good example but hardly the only one and we are sure that modelers will find many creative applications for the successive approximations method credit authorship contribution statement guillaume koenig conception and design of the study acquisition of data analysis and interpretation of the data drafting the manuscript clement aldebert revising the manuscript critically for important intellectual content cristele chevalier conception and design of the study acquisition of data revising the manuscript critically for important intellectual content jean luc devenon conception and design of the study revising the manuscript critically for important intellectual content declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research has been made possible thanks to a phd thesis funded by the french ministry of research and higher education this work was done on computing resources provided by the oplc team of the mio the authors would personally like to thank remi pages melika baklouti denis bourras and claire marc for their support during the drafting of the paper appendix reduced tidal model development of the model for the numerical modeling we can start with the shallow water equations it seems justified because previous reports of the patterns of circulation in the lagoon did not show stratification sous et al 2017 chevalier et al 2015 a 1 u t u u f u p k u h η momentum conservation η t h η u 0 mass conservation here u is the velocity in m s 1 h is the local depth in m η is the sea surface elevation in m f is the coriolis frequency at the ouano lagoon in s 1 and k is the local friction coefficient in m s 1 the linear form of the friction can be used for an average over a tidal cycle csanady 1981 devenon 1990 we can simplify those equations with a few observations in the lagoon first the depth in the lagoon is a few meters and the tidal amplitude is of the order of 0 4 m for the m2 component we expect velocities of the order of a most 0 2 m s 1 in the passes and see that the frequency of the m2 component is of the order of the coriolis frequency of around 5 1 0 4 s 1 in the lagoon we expect strong friction coefficients and can work with an estimate of k of 10 2 m s 1 if we add to that we expect typical flow length of a few kilometers we can estimate the magnitude of the different terms a 2 u t 1 0 5 m s 2 u u 1 0 6 m s 2 f u 1 0 5 m s 2 p m s 2 k u h 1 0 4 m s 2 momentum conservation η t 1 0 5 m s 1 h u 1 0 3 m s 1 0 mass conservation we see that the only remaining term is the pressure gradient that must be at equilibrium with the friction and this equilibrium gives a linear system at first order in the passes and over the reef where velocities are more important and depth reduced non linear terms can be bigger however the advection and continuity terms generate compound tides and overtides and do not affect the m2 coefficient when no constant current is present andersen et al 2006 the linear friction term that we used was fitted for the m2 circulation but does not allow for the generations of overtides and compound tides we therefore can neglect those interactions at first order therefore we can end up with the following system that is close to those used previously devenon 1990 le provost et al 1994 a 3 i ω k r h u k g η k 0 momentum conservation i ω k η k h u k 0 mass conservation here k is the index corresponding to a given tidal wave m2 in this case ω k denotes the frequency of the considered tidal wave in s 1 note that we have kept the acceleration term and the coriolis term which were of the same order of magnitude and two orders of magnitude smaller than the friction and pressure gradient terms however the acceleration term was kept because it is required for the oscillatory dynamics of the tide the usual argument for considering or not the coriolis term is the rossby number if we consider the tidal velocities of 0 10 m s 1 characteristic length of 5 km and a coriolis of the order of factor of 5 1 0 5 s 1 sous et al 2017 we have a rossby number of 0 4 but the rossby number is mainly used for comparing the advection terms and the coriolis term in open sea for tides in coastal situation the advection terms are neglected because they neglect compound and overtides and do not contribute directly to the m2 component it is more appropriate to compare the rossby radius of deformation which here would be a 4 g h f 10 m s 2 10 m 5 1 0 5 s 1 200 km if we compare this value with the 30 km of the largest dimension of the lagoon we can consider that the corrections brought by the coriolis term would be negligible at first order we can then neglect it and with some algebraic manipulations we obtain a complex helmholtz equation for the amplitude and the velocities can be deduced from the first derivative of the sea level the associated equation is eq a 5 a 5 i ω k η k β k η k 0 β k g h r h i ω k to discretize this equation we used a second order finite difference centered scheme the first order neumann condition was implemented directly in the solving matrix finally we solved it with an iterative jacobi method with a thousand iterations for ease of implementation with our programming language the number of iterations was fixed because variations were below a tenth of millimeters after a thousand iterations but the speed of convergence depended on the boundary condition the main goal here was to provide a model that could be run fast so that we could do the number of tests necessary for our method we still wanted something accurate enough to reproduce qualitatively what we observe with the complete non linear models such as croco and that was consistent with the precision of the pressure sensors that were used in the 2016 campaign in the lagoon sous et al 2017 with discrepancies of at most a few centimeters and a phase lag of a few degrees however we expected the precision to be lowest in the shallower parts of the lagoons because that would be the most sensitive to non linear effects and therefore we would expect our model to be less efficient in those parts comparison with a reference model we therefore compared the field of η obtained with this tidal model with the output of the fully non linear croco available on the site http www croco ocean org model in 2d for the m2 tidal component in a case that corresponded to the model outputs presented in chevalier et al 2015 the output held less than 10 discrepancies with the croco output the shallowest region exhibited wider discrepancies in phase as is presented in fig a 16 those discrepancies may be attributed to the friction in those regions which was set as quadratic in a previous modeling effort chevalier et al 2015 but has been linearized in the tidal model and to the challenging handling of boundary conditions and of advection in those regions 
23937,while lateral boundary conditions are crucial for the physical modeling of ocean dynamics their estimation may lack accuracy in coastal regions data assimilation has long been used to improve accuracy but most of the widely used methods are difficult to implement we tried a new and an easy to implement method to estimate boundary conditions this method uses data assimilation with a stochastic gradient descent and successive approximations of the boundary conditions we tested it with twin experiments and a more realistic setting on a tidal model in the lagoon of ouano in new caledonia the method proved successful and provided good estimation of the boundary conditions with various settings of subsampling and noise for the pseudo data in the twin experiments but there were important oscillations in the experiments with more realistic settings here we present those results and discuss the use of our new and easy to implement method keywords parameters identification tidal modeling stochastic algorithms data assimilation 1 introduction lateral boundary conditions for physical models are of major importance and very difficult to tune with accuracy in coastal regions james 2002 they strongly constrain the water circulation inside the domain by controlling the exchanges at the boundaries of the domain of interest in many applications they are derived from larger scale models either with nesting techniques or with interpolations unfortunately those boundary conditions are not always directly interpolable especially for coastal regions where the bathymetry frictional effects and submesoscale variability constrain the small scale dynamics a simple solution can be to implement data assimilation to adjust the boundary conditions taillandier et al 2004 however some of those data assimilation techniques require the development of an inverse model to compute the corrections to apply to the boundary conditions devenon 1990 this process is time consuming and requires a good knowledge both of the direct model and of inverse method discretization theory lellouche et al 1998 finally the inverse model can simply be impossible to develop if the code of the direct model is not available some other methods do not require the coding of an inverse model some methods use repeated runs of the direct model with perturbations in the parameters to generate approximations of the direct model that can then be inverted such as the reduced model method of vermeulen and heemink 2006 and altaf et al 2012 or hoteit and köhl 2006 other methods do not use approximations of the direct model and directly use the perturbations of the parameters to determine the best set of parameters such as the simultaneous perturbations stochastic approximation spsa algorithm spall 1998 messié et al 2020 those two kinds of methods both require limited coding work since they only need access to the parameters to be estimated in the model and to outputs of the model and can therefore be implemented on any model their main limitation is that they require large number of iterations of the direct model and unlike methods relying on an inverse model this number of iterations grows exponentially with the number of parameters therefore parameters such as lateral boundary conditions which are clusters of at least hundreds of grid points each with independent values were out of the reach of such methods but those methods could be used to estimate approximations of those kinds of parameters this is done naturally for the reduced model methods by the decomposition in empirical orthogonal functions but for the spsa algorithms finding good approximations is still an open problem for example boutet et al 2015 used the spsa algorithm for determining friction coefficients in the gascogne gulf by covering the gulf with a domain of constant friction according to the type of sediments found and outside the domain of oceanography tympakianaki et al 2015 used clustering techniques to generate approximations the contribution of altaf et al 2011 was important in that regard because they compared the results of the spsa algorithm with piecewise approximations with the results a reduced model for the calibration of the depth in a north sea model they concluded that the spsa method had the advantage of being able to identify many parameters at once with these approximations however the use of piecewise constant functions quickly becomes difficult when the parameters vary continuously or when the precise location of those corrections is not known in advance examples of cases where those corrections are hard to know a priori are coral lagoons where the steep bathymetry the presence of a coral reef and the optional connexion with other lagoons can bring unexpected variations in currents and sea level most approximation methods try to identify the coefficients of a given set of basis functions for approaching boundary conditions this approach is limited because the only way to improve the approximations is to redo the entire approximations with new basis functions instead we chose to use several sets of basis functions whose coefficients could be easily linked from one set to another so that approximations could be improved without having to redo the whole procedure this methodology had already been tested jahns 1966 and thevenaz et al 1998 but never for boundary conditions in oceanography to the authors knowledge we performed twin experiments on a tidal model of the ouano lagoon in new caledonia to test our method it is of interest as the boundary conditions are complex to determine due to the influence of the wave breaking patterns the tides and the high bathymetric and friction variations due to the coral barrier sous et al 2017 it enabled us to see how the approximations worked in a complex situation where boundary conditions sometimes vary very sharply and sometimes are nearly constant in addition by adding noise and subsampling the data we tested the robustness of the method in more realistic settings we finally tested the method in a more realistic setting with no a priori knowledge of the boundary conditions and data collected during survey of the lagoon 2 material and methods 2 1 study zone and parameters we studied the circulation patterns of the ouano lagoon this lagoon is about 30 km long and 10 km wide and is connected to a wider lagoon system located on the south west coast of new caledonia the coral reef separates the pacific ocean and the lagoon and forms a porous wall with only 2 passes directly linking the ocean and the lagoon the currents inside this lagoon are dominated by tides with a spring amplitude of about 0 8 m chevalier et al 2015 sous et al 2017 the wave breaking occurring on the barrier reef also drives the currents chevalier et al 2015 as it creates a horizontal gradient of radiation stress across the coral barrier that leads to a water flux inside the lagoon sous et al 2017 the wave breaking and the currents it generates are in addition influenced by the tidal level of the lagoon therefore the determination of accurate tidal boundary conditions in the lagoon is necessary for any proper physical modeling tidal boundary conditions were provided by c chevalier so as to be able to test the method under realistic conditions these consisted in phase and amplitude for the m2 component of the sea level are shown in fig 1 both variables exhibited zones with strong variations due to the fact that the southern boundary and the northern boundary passed through the barrier and therefore encompassed both the ocean and the lagoon furthermore boundary conditions in shallow zones may have been influenced by the current coming from the rest of the new caledonia lagoon complex ouillon et al 2010 jouon et al 2006 the model used here is a reduced tidal model it is based on a decomposition of the velocities and elevation in tidal modes in the shallow water equations it was used because the equations can then be reduced to a purely boundary conditions problem the derivation of the model is given in the appendix 2 2 gradient descent algorithm to identify boundary conditions for each of our successive problems we used a gradient descent algorithm the one we used is strongly inspired by the simultaneous perturbation stochastic approximation spsa algorithm spall 1998 that we adapted in order to speed up convergence as for any gradient descent algorithm the objective was to determine the gradient of a cost function according to certain parameters and to follow the gradient to determine an extremal value in our case we wanted to minimize the discrepancies between the output of a model and pseudo data previously generated by adjusting the boundary conditions of the model the metrics used are shown in eq 1 1 j θ 1 2 k f θ k y ˆ k 2 here j is the cost function θ represents the set of control parameters being here the boundary conditions y ˆ represents the data or pseudo data for twin experiments and f is an operator that represents the action of the model those data in our case are the complex values of the sea level the index k serves to localize the data point and the corresponding model output we chose this cost function because it is standard in the field and corresponds to an addition of gaussian uncertainties some terms can be added in order to smooth the output sasaki 1970 or to reduce the size of the search space blum et al 2009 but those were not used here now we have to determine new parameters incrementally if we index the iteration of the update of parameters with i then we can write g i as the gradient of the cost function according to the parameters and a i a parameter used for controlling the size of updates at the iteration i the gradient descent then update the parameters is written as 2 θ i 1 θ i a i g i the modification of parameters is done until a stopping criterion is reached this criterion can be a measure of the magnitude of the gradient g i or a number of iterations of the gradient descent for example in this procedure the value of the parameters a i is crucial as well as the value of the gradient g i as it indicates the direction to reach the minimum here we did not compute the gradient directly but rather an approximation to the gradient the approximation was the one used in the spsa spall 1998 algorithm and corresponded to a directional derivative as shown in system 3 3 θ i θ i c i δ i j θ i δ i j θ i j θ i δ k c i 1 g ˆ i j θ ˆ i a i 1 g i 1 δ ˆ i g i g ˆ i β g i 1 in this algorithm the direction was determined by a random perturbation of all the parameters and the perturbation c i δ i was determined by a predetermined term c i and a sign term δ i given by a bernoulli law the perturbation and its symmetric counterpart are used to determine a centered estimate of the derivative of the cost function in this direction finally to provide an estimate closer to the real value of the gradient we used a momentum technique as described in sutskever et al 2013 this technique allows a smoother descent by averaging the current estimate of the gradient with the previous estimates of the gradient hence the determination of the gradient is given in system 4 4 g i g i β g i 1 where β is a parameter between 0 and 1 according to the importance we want to give to previous estimates in many applications this sum is normalized but here we decided to take another option this is due to the adjustment we made to our algorithm compared to the standard spsa normally the gain coefficient a i and the perturbation coefficient c i are supposed to be decreasing as in the system 5 5 a i a 0 a i γ c i c 0 i δ where a 0 and c 0 are initial estimates of the perturbations and gain coefficients a is a regularization term for the decrease and γ and δ are exponents under one spall 1998 however we used a different approach we kept the perturbation amplitude c i constant but started with a small value for the gain coefficient a i and increased it in geometric fashion during the gradient descent this avoided the necessity of choosing the right parameters for the gain of every subproblem the gain coefficient was updated to take into account the fact that we were going to solve of succession of problems and that we would not be able to tune the gain coefficient a i for each problem this is shown in the system 6 6 a i a 0 1 q i c i c 0 where q was a small parameter for the increase this growth value was chosen so that the algorithm would fit every subproblem we implemented a line search procedure we used the procedure described in armijo 1966 that checked whether the parameter modifications effectively reduced the cost functions and if not reduced the gain coefficient a i a few times by a factor depending on the ratio between the estimated cost function and the lowest one if after some reductions of the gain coefficient a i the cost function still did not decrease the gain coefficient a i was reset to its initial value and momentum terms g i 1 were cancelled for the gradient estimation in the spsa algorithm the gain coefficients and the perturbation coefficients are often decreasing with the goal of being as small as possible in the last stage of the parameters estimation in order to make only small adjustments around the minimum the growth procedure used in our case may seem to do the opposite however the line search algorithm tends to reduce the gain coefficient if we are close enough to a minimum if we combine this with a very small initial gain coefficient we could expect that close to a minimum our algorithm and the spsa would show close behavior by only performing small adjustments in contrast at the beginning of each subproblem we expect an important growth of the gain coefficient so that it adjusts to the sub problem since the gradient descent algorithm relied on few parameters we report them in table 1 2 3 successive approximations as mentioned in the introduction here we suggested determining approximations of the boundary conditions the boundary conditions were determined by amplitude and phase at each boundary point to give a complex number as shown in eq 7 7 η k η k e i ϕ k where η k is the local amplitude and ϕ k the local phase one can set the boundary conditions on a 1d vector as shown in fig 1 it is then possible to approximate boundary conditions with one dimensional functions in this case the phase and amplitude can be determined by eq 8 8 η k or ϕ k j 0 m f n b j x j x k where f n were the functions used at the level of approximation n b j were their coefficients and m the number of such functions used due to the fact that we used nodal functions for the approximations we also needed a parameter x k that specifies the position where the value of the function is fixed those positions are called the nodes the nodal functions we used are cubic splines with natural boundary conditions those functions are a collection of piecewise cubic polynomial linked by continuity and continuity of the first derivative their junctions are situated at nodes x k and their values at those nodes at the b j values 9 f n b j x j x k j 0 m a j x 3 b j x 2 c j x d j π x k x j x j f n b j x j x j b j d f n b j x j 0 d x c 0 d f n b j x j t d x c m where here π x k x j x j is the door function that is worth 1 between x j and x j 1 and 0 elsewhere the computation of the coefficients a j b j c j and d j is done following the python implementation of the algorithm of the scipy package of python the scipy community 2017 de boor 1978 overall our aim was to determine the parameters b j that minimized the cost function for a given level of approximation n once a minimum is reached or approached with a given number of nodes we used this approximation to generate a new set of nodes and node values then we performed another gradient descent to identify the minimum of the cost function relative to the new set of parameters we thought that reaching the new minimum would require few iterations of the gradient descent since the last step of minimization had brought us to a good starting point an example of this approach by successive approximations is shown in fig 2 the new nodes were created midway between existing nodes also the values of the new nodes were determined by linear interpolation between the values of the previously existing nodes 2 4 implemented tests 2 4 1 twin experiments and pseudo data we performed twin experiments to test our parameters identification algorithm we began by running our reduced model with known boundary conditions for the m2 tide those boundary conditions were boundary conditions taken from the previous work of chevalier et al 2015 we used the output of this run as references for our first assimilation we will refer to them as pseudo data in the rest of the paper as they will be used as data for our assimilation but were initially produced by the model that ensures a total compatibility of this data with the physics of the model and avoids discrepancies between the model outputs and in situ data we then used pseudo data to try to identify the boundary conditions that served to produce them since those pseudo data were outputs of the model we knew we would not have any compatibility issues between the model and the data the model was a linear one we knew that we had only one set of boundary conditions corresponding to the pseudo data if we restricted the domain of the phase from 0 to 2 π radians we also tested conditions closer to a real data assimilation by adding noise and decimating the pseudo data so that they would be closer to real in situ data it enabled us to know how the noise on data acted on the boundary conditions that were determined and how the position and number of data could affect the shape of the boundary conditions finally we performed a final experiment with in situ data and our reduced model 2 4 2 parameter identification methods we wanted to prove the efficiency of the successive approximations method was not due only to a good gradient descent algorithm but also to a well chosen set of basis functions for the approximation we then compared the results obtained with this method with results obtained by two other methods those methods did not use successive approximations and only one of them used approximations with bicubic splines but they used the same spsa like gradient descent algorithm we ran the three methods with the same number of iterations of the gradient descent algorithm we also ensured that for the methods that relied on spline approximations the approximations presented the same number of nodes at the end of the parameter identification finally we started with the same constant values of parameters on the entire boundary for each method for the successive approximations method we began with a small number of nodes and generated new ones after a certain number of iterations of the gradient descent algorithm those numbers of iterations were determined by trial and error because they generally presented a sufficient decrease of the cost function the second method used only one level of approximation and is referred to as the high order approximation method the number of nodes used in the high order approximation method is the same as the number of nodes in the final stage of the successive approximation method the third method did not use approximation and directly identified the values of phase and amplitude for each grid cell that is situated at the open boundary we refer to this method as the no approximation method we decided to split the parameter identification for amplitude and phase for the three methods in the successive approximations methods we alternated the identification of amplitude and phase also in the successive approximations method we used more gradient descent iterations for the phase than for the amplitude the parameter for the implementation of the three methods in our specific case can be found in table 2 2 4 3 noise and subsampling parameter identification was also performed in the presence of noise and subsampling for this purpose we added noise to the pseudo data by adding a uniformly distributed noise to each data point we used different levels of noise but always paired the level of noise of amplitude with that of phase we tested respectively noise levels of 0 0001 m and 0 0001 rad 0 001 m and 0 001 rad 0 01 m and 0 01 rad and 0 1 m and 0 1 rad in the results section those levels of noise are labeled with their numerical value only and not the unit we will use noise of 0 1 and not noise of 0 1 m and 0 1 rad those levels of noise were chosen to show the increase of impact of noise in the data in addition the middle values of 0 01 m and 0 01 rad s 1 are of the order of uncertainties that we have when we treat our data of the tidal elevation of the lagoon with the python implementation of codiga 2011 the values of 0 1 m and 0 1 rad s 1 correspond to extremely high noise value since we expect tidal amplitudes of the order of 0 4 m and they serve to test the robustness of the method the subsampling was performed by only keeping some pseudo data we removed them in the same way in the two directions and with no special consideration for the land we respectively kept one pseudo data on 2 4 16 and 32 in each direction for each case which ended up with only one pseudo data on 4 64 256 and 1024 4 we chose levels which respectively correspond to 16275 4143 240 55 and 12 available data points this number of data will be used as a label in the experiments finally we also performed a few tests with localized pseudo data this was done in order to see how unequally distributed data would influence the parameter determination method and how robust it was all those experiments are reported in table 3 2 5 computational cost of the algorithm the code of the reduced model took about eight seconds to run on our local computer with a core i7 processor from intel while an entire assimilation loop comprising three runs of the reduced model an estimation of the cost function and the update of the parameters took 28 s on average with our python implementation 3 results 3 1 results obtained with successive approximations we first present the boundary conditions determined with the successive approximations method in fig 3 we see different stages of the successive approximations method if those stages are all around the reference boundary conditions the 5th stage has less oscillations around the true value those oscillations are bigger between the boundary index 0 and 70 and around the boundary index 370 where variations are high between the boundary index 0 and 370 all the variations of the phase are not captured even at the 5th stage furthermore we see that the variations in this region are less steep at the 3rd stage than at the 5th in figs 4 and 5 we present the output of the model with the boundary conditions determined with the successive approximations method the output for the parameters determined with the successive approximations method is close to the original the only variations being in some oscillations close to the southern and northern boundaries apart from those variations are of the order of one millimeter for amplitude and 1 1000 of a radian for the phase 3 2 results obtained with the other two methods we present the amplitude and phase determined with the no approximation and the high order approximation method in fig 6 so that a comparison with the results of the successive approximations method can be made we see that both methods exhibit wide oscillations those oscillations are less marked and of larger scale for the high order approximation method than for the no approximation method it is interesting to note that the boundary conditions obtained with the successive approximations method are also plotted in fig 6 but are undistinguishable from reference boundary conditions 3 3 comparison of the three methods during the identification the three methods take different paths to identify parameters to show those different paths we present the changes of the cost function during the gradient descent for each the three methods in fig 7 all methods give a roughly equivalent cost function value up to the 50th iteration after that point the successive approximation method gives the lowest cost function value and the steepest slope the curves for the cost function values of high order approximation and no approximation methods only separate around the 700th iteration which is the moment of the variable shift the three methods end up with different cost function values with around 10 4 for the successive approximations method and around 1 for the high order approximation method and 10 for the high order approximation method if we use the definition of the cost function we realize that this cost function corresponds to a maximum error of 10 2 rad or m for the successive approximations method if all the error was on only one grid point the different behavior for the three methods is even more explicit if we take a closer look at the patterns of change in the cost function the high order approximation method and no approximation method have similar shapes they have a very steep descent at first and then continue to decrease slowly there is another steep descent when the parameter changes from amplitude to phase for the gradient descent of the method with successive approximations we see that we have a very steep part before the second node creation after that the gradient descent is smoother with a series of steep descents right after node creations that slowly turn into plateaus forming a stairway like overall shape we also see that after each node creation we have an increase in the cost function 3 4 noisy and subsampled pseudodata here we performed some boundary conditions identification where we used only a fraction of the available pseudo data since those pseudo data were the results of a reference run they already corresponded to the position of the model output and no interpolation was necessary to extract a fraction of those pseudo data for the subsampling we simply kept one data out of a certain number in both the y and x directions this means that if we began with 114 pseudo data in the x direction for each y level we end up with only 57 pseudo data in the x direction with a subsampling level of two and that those pseudo data are still equally spaced the different subsamplings used are illustrated in fig 8 since the decimation of the pseudo data was done in the x and y directions at the same time we refer to the different subsampling experiments by the fraction of pseudo data used in the x direction multiplied by the fraction of pseudo data considered in the y direction if we still use our example where we used one pseudo data out of two in the x and y direction we will refer to it as the experiment with a subsampling of one data point over 4 we chose levels of subsampling of one pseudo data over 4 64 256 and 1024 if we compare the number of data points available for each subsampling level of 16275 4143 240 and 12 available data points and the 33 nodes of the final splines we see that the cases with one pseudo data out of 4 and 64 still have more pseudo data than the final number of nodes of our procedure the case with one pseudo data over 256 would still have more pseudo data 55 than the final number of nodes but the land cells actually makes it close while the case with one pseudo data over 1024 has less pseudo data points than the final number of nodes at the end of the procedure we therefore expect it to behave as an underdetermined problem we show the results with different levels of subsampling in fig 9 we see that results are fairly close until the subsampling reaches one data point over 256 with so few data some oscillations begin to appear especially in the phase also the conditions determined close to the northern boundary differ strongly those oscillations are stronger with one data point over 1 1024 still those oscillations are around the reference boundary conditions we also present the results with different levels of noise added to the pseudo data in fig 10 the noise level of 0 0001 m and rad s 1 produces results that are indistinguishable from those obtained without noise the noise levels of 0 001 m or rad s 1 and 0 01 m or rad s 1 produce very comparable results where small scale oscillations start to appear finally the noise level of 0 1 exhibits very strong oscillations around the reference boundary conditions 3 5 grouped data we also performed a few experiments in which we used only 60 pseudo data located in a precise part of the domain we tried with data in the lagoon part data in the deeper ocean and data in the shallow part in the south east of the domain those locations are visible in fig 11 as we can see in fig 12 the data placed in the lagoon have oscillations of the order of a centimeter relative to the true value in the cases where the data were in the ocean we have larger oscillations in the oceanic part of the boundary and both the phase and the amplitude of the boundary conditions are underestimated in the south eastern boundary when the data are located in the south eastern shallow part of the lagoon the boundary conditions are reconstructed with an accuracy of a few centimeters but the estimates for the oceanic boundary conditions oscillate strongly with an overestimation of nearly 10 centimeters followed by an underestimation of nearly 20 centimeters 3 6 assimilation with in situ data we also tried to determine boundary conditions from in situ data gathered during the 2016 campaign in the lagoon in this situation however we had to reduce the number of nodes of the splines because we only had four data points those are shown in fig 13 it seems that the variations in the boundary conditions were large as is shown in fig 14 the mean trend of the true phase and amplitude is caught but the oscillations are very large with 10 cm for the amplitude and 0 1 rad for the phase in addition the maxima do not seem to be correlated with any variation in depth close to the boundaries those strong variations are damped in the lagoon to have outputs that are around the data with discrepancies of around 0 05 m and less than 0 01 rad as shown in fig 15 4 discussion 4 1 choice of the functions shape the results obtained with the successive approximations method were very close to the original boundary conditions compared to the other two methods not only was convergence speeded up but accuracy was also increased this is interesting for development of methods for determination of boundary conditions in a coastal model furthermore this method does not depend on a specific gradient descent method or type of parameters and could be extended to other problems such as the determination of friction coefficients or with other algorithms such as genetic algorithms we can say that the increase in accuracy and speed of convergence were indeed linked to the successive approximations as neither the no approximation or the high order approximation method could approach the reference boundary conditions the slow convergence was expected for the no approximation method as we knew that our algorithm had a speed of convergence close to that of the spsa which depends on the number of parameters even though the computing cost for gradient estimate does not for the high approximation method we expected the convergence to be faster since the number of parameters was reduced but by comparing it with the successive approximation method it seems that the starting point also plays an important role in the convergence rate and the successive approximation method effectively sped up convergence by first reducing the number of parameters and next by providing good starting points for the successive problems this method also contrasts with what had already been done with a stochastic gradient descent algorithm other examples exist such as that of altaf et al 2011 and boutet et al 2015 and hoang and baraille 2011 also used this algorithm for determining a reduced number of coefficients of approximation for a covariance field of a kalman filter in all those cases they reached an acceptable level of precision however they worked with limited number of actual parameters less than 20 each time and with parameters whose spatial and temporal variations were known before the assimilation this number of parameters is close to the number of parameters we had in our realistic application in this case we realized that our method led to strong oscillations this may be linked to the positions of the nodes of the splines or to the use of the spline approximations this could be solved by trying a dynamic placement of the nodes or by trying other basis functions such a akima interpolation or p chip that do not have such a curvature lastly the algorithmic implementation of those approximations is also to be considered for example the odd behavior that we observed at the very beginning and the very end of our determined parameters in fig 3 was due to the way a condition of zero normal gradient was imposed the algorithm we used places null derivative conditions at the extremities of the splines instead of forcing the values of the nodes at those points and extending the function with null derivative before and after those points this illustrates that not only the functions used but the way they are implemented must be carefully taken into account for good parameter identification 4 2 impact of sampling the effects of noise and of subsampling were also investigated in figs 9 and 10 we show that discrepancies stayed low for noise up to 10 of the amplitude and phase value and up to 1 16th of the pseudo data taken into account with less data we expect a less precise definition of the boundary condition therefore the problem may derive for the fact that we are looking for a precision that is beyond what is achievable by any method for this combination of model and data in that case it may be that our problem was poorly constrained even with 1 32nd pseudo data the distance and the position of the data points relative to the boundaries may play on how they are affected by them furthermore some regions are nearly isolated from the rest of the domain especially in the south eastern part of the lagoon this appeared more clearly in the experiments with the different locations of data we saw that data located in the lagoon provided the best estimates of boundary conditions showing that the lagoon zone was more sensitive to variations in the boundary conditions than the deeper oceanic zone we emphasize the need for both a good sampling strategy and a good choice of the node positions for reconstructing the function we are trying to approach perhaps the hardest part in parameter identifications with regard to determining the boundary conditions is to identify not the value but the shape of those boundary conditions indeed methods using an inverse model devenon 1990 and the methods with empirical orthogonal functions of vermeulen and heemink 2006 can provide such a shape but requires the development of a second numerical model or many iterations of the model before parameters can be determined with the successive approximations this shape is determined by the functions used for approximations and the position of the nodes hence both factors should be carefully considered with the physical knowledge of the problem to tackle in mind furthermore the successive approximation methodology presents a filtering effect on the reconstruction of the shape of the boundary conditions as is visible in figs 9 and 10 this effect is inherent to the spline interpolation method and has already been observed by unser 1999 the filtering effect could also be an advantage since depending on the number of observations that we dispose or the nature of the problem we could also not want to deal with small scale variations in addition small scale variations can be destabilizing in some applications and therefore filtering them out can be an important point as was discussed in sasaki 1970 4 3 practical implementation the implementation of the successive approximation strategy with an algorithm based on an spsa like algorithm is not necessary the pyramidal approach presented in thevenaz et al 1998 or the stepwise approach presented in jahns 1966 used other algorithms however in our case the spsa like algorithm presented many advantages as it was easy to implement and presented good convergence properties spall 1998 which are important in geophysics for models that tend to be quite complex still the reduction of the number of variables could open up the way to other algorithms based on perturbations of the parameters such as genetic algorithms for example the choice of the gradient descent algorithm would of course be highly dependent on the problem to be tackled our tests showed us some issues with the implementation of successive approximations as different gradient descent are performed one after another a robust and versatile tuning of the gradient descent algorithm is important as already pointed out there is no reason for the algorithm parameters to remain constant for two successive problems with some gradient descent algorithms the problem of the gain coefficient could be tackled by the use of algorithms that use hessian approximation as step size with the spsa algorithm there are some existing second order methods zhu and spall 2002 reddy et al 2016 some work should also be devoted to the stopping criterion as it is difficult to estimate the number of iterations necessary for different stages the number of iterations necessary depends on the adjustments needed at each stage which it would be difficult to estimate in advance some work has been done with the spsa in order to determine a stopping criterion wada and fujisaki 2013 but are so far not directly transposable to complex applications in a similar way a line search procedure was used because we knew the problem was convex and because we were not able to compute a correct step size with a second order algorithm however the use of line search algorithm is unwieldy and not suited to non convex problems thus some other methods should be used for non linear problems perhaps second order approximations could be more appropriate one of the advantages of this method is its computational cost we saw that the time required for each step of the gradient descent was largely dominated by the model runs since this method only requires estimation of the cost function and generation of vectors of parameters the ratio of the time taken by the model runs to the generation of perturbations by the algorithm is likely to increase with larger models with more data the time necessary for parameter identification is dominated by the time of the model runs this time could be reduced by parallelizing the model runs of the same iteration 4 4 perspectives here we successfully identified boundary conditions for a tidal linear model the method described in this article could be used directly for other coastal situations or other tidal components with in situ data instead of doing twin experiments in addition we could extend the work presented here to other situations in fact the problem of identifying lateral boundary conditions shares some features with other problems of interest in geophysics the high number of parameters and the continuity of the field of interest can also be found in initial conditions or friction coefficient for example the methodology of approximation with splines can also be used directly for two dimensional or even higher dimensional fields of parameters and therefore the problem addressed here can be seen as a first step towards a more general parameter identification method as a complement to these advantages our method do not require an inverse model owing to the gradient descent algorithm used also on the basis of this gradient descent algorithm we could identify parameters in non convex problems or with non linear models to perform this identification we should remove some of the adaptations that we made to the algorithm in the present work but those modifications are minor and non linear optimization has already been performed with the spsa algorithm spall 1998 outside the field of oceanography or even in the field of oceanography by boutet et al 2015 furthermore the simplicity of implementation of the method enables us to build more sophisticated tools for the study of the circulation patterns we can consider the use of the reduced tidal model as a special case of linearization of a more complex circulation model hence we could use this model in a data assimilation loop for optimizing the tidal boundary conditions for a more complex model such as croco for example 5 conclusions we performed with a linear tidal propagation model a series of twin experiments and a more realistic experiment we investigated whether a spsa like gradient descent algorithm could be used to identify boundary conditions as they represented a large number of parameters we showed it was possible with a special method involving several stages of approximations we used approximations that have coefficients whose magnitude is close to the magnitude of the function to be approximated the fact that the magnitude of the coefficients was close to the magnitude of the boundary conditions helped us in setting up the successive approximations used we were able to determine the boundary conditions of our tidal model up to a tenth of centimeter for the twin experiments the novelty here was that we succeeded with tens of effective parameters and with a methodology that would enable us to go even further however the first try outs with more realistic data locations and with in situ data showed us a more contrasted picture as for any inverse methods the precision and location of data is crucial and the successive approximations do not change that the issue that is more specific to this method is the oscillations occurring in the boundary conditions in the realistic experiments this may be improved by trying other functions for approximations with p chip or akima interpolation which reduces the oscillations or by playing with the positions of the nodes to conclude we have developed a method that was able to identify an arbitrary number of parameters and tested it with a tidal model in a lagoon setup the results were affected both by the noise in data and their sampling but also by the number of parameters of the approximation used in the method we saw an effect of filtering smaller scales of the boundary conditions there is no theoretical drawback to the use of other gradient descent algorithms or even other optimization methods with the successive approximations method but there are still some issues relative to its use with reduced numbers of data the use of approximations and the successive problems solved are fully compatible with any method however we had to modify the gain sequence of our gradient descent algorithm to perform efficiently the successive gradient descents future work will focus on the implementation of this method in realistic applications specifically we could use this method for non open source models or for models which do not have an adjoint the croco model is a good example but hardly the only one and we are sure that modelers will find many creative applications for the successive approximations method credit authorship contribution statement guillaume koenig conception and design of the study acquisition of data analysis and interpretation of the data drafting the manuscript clement aldebert revising the manuscript critically for important intellectual content cristele chevalier conception and design of the study acquisition of data revising the manuscript critically for important intellectual content jean luc devenon conception and design of the study revising the manuscript critically for important intellectual content declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research has been made possible thanks to a phd thesis funded by the french ministry of research and higher education this work was done on computing resources provided by the oplc team of the mio the authors would personally like to thank remi pages melika baklouti denis bourras and claire marc for their support during the drafting of the paper appendix reduced tidal model development of the model for the numerical modeling we can start with the shallow water equations it seems justified because previous reports of the patterns of circulation in the lagoon did not show stratification sous et al 2017 chevalier et al 2015 a 1 u t u u f u p k u h η momentum conservation η t h η u 0 mass conservation here u is the velocity in m s 1 h is the local depth in m η is the sea surface elevation in m f is the coriolis frequency at the ouano lagoon in s 1 and k is the local friction coefficient in m s 1 the linear form of the friction can be used for an average over a tidal cycle csanady 1981 devenon 1990 we can simplify those equations with a few observations in the lagoon first the depth in the lagoon is a few meters and the tidal amplitude is of the order of 0 4 m for the m2 component we expect velocities of the order of a most 0 2 m s 1 in the passes and see that the frequency of the m2 component is of the order of the coriolis frequency of around 5 1 0 4 s 1 in the lagoon we expect strong friction coefficients and can work with an estimate of k of 10 2 m s 1 if we add to that we expect typical flow length of a few kilometers we can estimate the magnitude of the different terms a 2 u t 1 0 5 m s 2 u u 1 0 6 m s 2 f u 1 0 5 m s 2 p m s 2 k u h 1 0 4 m s 2 momentum conservation η t 1 0 5 m s 1 h u 1 0 3 m s 1 0 mass conservation we see that the only remaining term is the pressure gradient that must be at equilibrium with the friction and this equilibrium gives a linear system at first order in the passes and over the reef where velocities are more important and depth reduced non linear terms can be bigger however the advection and continuity terms generate compound tides and overtides and do not affect the m2 coefficient when no constant current is present andersen et al 2006 the linear friction term that we used was fitted for the m2 circulation but does not allow for the generations of overtides and compound tides we therefore can neglect those interactions at first order therefore we can end up with the following system that is close to those used previously devenon 1990 le provost et al 1994 a 3 i ω k r h u k g η k 0 momentum conservation i ω k η k h u k 0 mass conservation here k is the index corresponding to a given tidal wave m2 in this case ω k denotes the frequency of the considered tidal wave in s 1 note that we have kept the acceleration term and the coriolis term which were of the same order of magnitude and two orders of magnitude smaller than the friction and pressure gradient terms however the acceleration term was kept because it is required for the oscillatory dynamics of the tide the usual argument for considering or not the coriolis term is the rossby number if we consider the tidal velocities of 0 10 m s 1 characteristic length of 5 km and a coriolis of the order of factor of 5 1 0 5 s 1 sous et al 2017 we have a rossby number of 0 4 but the rossby number is mainly used for comparing the advection terms and the coriolis term in open sea for tides in coastal situation the advection terms are neglected because they neglect compound and overtides and do not contribute directly to the m2 component it is more appropriate to compare the rossby radius of deformation which here would be a 4 g h f 10 m s 2 10 m 5 1 0 5 s 1 200 km if we compare this value with the 30 km of the largest dimension of the lagoon we can consider that the corrections brought by the coriolis term would be negligible at first order we can then neglect it and with some algebraic manipulations we obtain a complex helmholtz equation for the amplitude and the velocities can be deduced from the first derivative of the sea level the associated equation is eq a 5 a 5 i ω k η k β k η k 0 β k g h r h i ω k to discretize this equation we used a second order finite difference centered scheme the first order neumann condition was implemented directly in the solving matrix finally we solved it with an iterative jacobi method with a thousand iterations for ease of implementation with our programming language the number of iterations was fixed because variations were below a tenth of millimeters after a thousand iterations but the speed of convergence depended on the boundary condition the main goal here was to provide a model that could be run fast so that we could do the number of tests necessary for our method we still wanted something accurate enough to reproduce qualitatively what we observe with the complete non linear models such as croco and that was consistent with the precision of the pressure sensors that were used in the 2016 campaign in the lagoon sous et al 2017 with discrepancies of at most a few centimeters and a phase lag of a few degrees however we expected the precision to be lowest in the shallower parts of the lagoons because that would be the most sensitive to non linear effects and therefore we would expect our model to be less efficient in those parts comparison with a reference model we therefore compared the field of η obtained with this tidal model with the output of the fully non linear croco available on the site http www croco ocean org model in 2d for the m2 tidal component in a case that corresponded to the model outputs presented in chevalier et al 2015 the output held less than 10 discrepancies with the croco output the shallowest region exhibited wider discrepancies in phase as is presented in fig a 16 those discrepancies may be attributed to the friction in those regions which was set as quadratic in a previous modeling effort chevalier et al 2015 but has been linearized in the tidal model and to the challenging handling of boundary conditions and of advection in those regions 
23938,a nested configuration of the regional ocean modeling system roms comprising three grids was used in conjunction with a 4 dimensional variational 4d var data assimilation system to compute ocean state estimates of the mid atlantic bight mab the three nested grids have a horizontal resolution ranging from 7 km to 0 8 km and capture circulation regimes that span the gulf stream western boundary current through the mesoscale eddy field and down to the rapidly evolving and energetic sub mesoscale all of these circulation regimes are challenging for any data assimilation system yet the 4d var system was found to perform well across this range of space and time scales the observational data used to constrain the ocean state estimates comes from a wide range of remote sensing in situ and mobile platforms an adjoint based procedure was used to compute the impact of each observing platform on several different indexes that describe the position of the mab front stratification and associated cross shelf exchange processes in the vicinity of the u s national science foundation s ocean observatories initiative pioneer array the impact of observations from each observing platform on the chosen indexes varies across the three grids it is a function of several factors that include the nature of the background circulation and the level of error assumed for the background ocean state and the observations the geographic distribution of the observation impacts is remarkably robust across the various indexes and the three grids in addition observations that are both local to and remote from the target regions that define each index can exert a significant influence on the circulation variations in the observation impacts through time can be used to identify observations that exert unexpectedly large influence on the 4d var analyses i e outliers and routine monitoring of observation impacts is a useful indicator of the efficacy of different components of the observing system also the observation impacts were found to be a useful performance indicator for the data assimilation system keywords data assimilation 4d var observation impacts mid atlantic bight pioneer array 1 introduction data assimilation is an integral component of any ocean analysis and forecast system it is now a mainstream activity at most operational numerical weather prediction centers and many research institutions both on regional and global scales moore et al 2019 ocean data streams are dominated by remote sensing instruments that observe temperature and sea level but developments in novel sensors and autonomous platforms are rapidly expanding the delivery of in situ observations though typically inhomogenous in space and time sampling and much less numerous subsurface in situ data are an invaluable complement to dense satellite observations when assimilated into forecast models the information that these various platforms provide can interact in complex and sometimes surprising ways and data from one platform can support measurements from another unraveling the influence of the respective observations on the ensuing ocean analyses and forecasts can be very challenging nevertheless given the considerable financial and human resources required to deploy and maintain ocean observing networks the routine quantitative assessment of the impact of observations on analysis forecast systems is an important activity indeed observation impact assessments now form a critical component of most operational numerical weather prediction systems the focus of this study is the impact of observations in an analysis forecastsystem based on the regional ocean modeling system roms that encompasses the mid atlantic bight mab and gulf of maine gom in the nw atlantic fig 1a and is run in near real time in support of the u s integrated ocean observing system ioos mid atlantic regional association coastal ocean observing system maracoos a prominent feature of the mab region is a shelf break front separating the warm saline waters of the subtropical gyre from the cooler fresher waters of the continental shelf mountain 2003 intrinsic instabilities of the front fratantoni and pickart 2003 and eddy shelf interactions tied to gulf stream induced warm core rings zhang and gawarkiewicz 2015 contribute to the complexity of mab shelf break dynamics gawarkiewicz et al 2018 and are a major focus of the u s national science foundation s nsf ocean observatories initiative ooi as part of this initiative the pioneer array comprising fixed moorings and a fleet of autonomous underwater vehicles is deployed at the continental shelf break fig 1b c with a majority of the instruments operational since april 2014 the primary aim of pioneer is to increase understanding of the processes responsible for the transport of water masses across the shelf break and their relationship to forcing on a range of time scales but its limited area high density sampling pattern provides a valuable supplement to the wider scope maracoos observing system and presents exceptional opportunities to methodically contrast the impact of such disparate observing system designs there have been several efforts in oceanography to quantify the impact of observing systems on ocean analyses using a variety of methods that include observing system experiments e g balmaseda et al 2007 oke and schiller 2007 smith and haines 2009 spectral analysis of the representer matrix le hénaff et al 2009 quantification of the degrees of freedom of the observing system moore et al 2011b assessment of observation footprints oke and sakov 2012 and ensemble methods storto et al 2013 extensive reviews of these efforts can be found in oke et al 2015a b and fujii et al 2019 the present study uses an adjoint based approach developed by langland and baker 2004 and builds on the work of moore et al 2017 and levin et al 2019 the goal of this study is to quantify the impact of the various components of the maracoos observing system on circulation estimates derived from the roms 4 dimensional variational 4d var data assimilation system in light of the goals of the nsf ooi pioneer array a specific focus is the extent to which the observing system can inform roms about shelf break exchange processes in the vicinity of the mab front a brief overview of the 4d var and observation impact methodology employed is given in section 2 although the reader is directed to levin et al 2019 for a more detailed and thorough description section 3 describes the configuration of roms and 4d var the various data sources used and documents the performance of the data assimilation system the observation impacts are quantified in terms of specific indexes that target different aspects of the shelf break circulation and these are introduced in section 4 section 5 presents a summary of the impact of the observations from the various components of the observing system on the suite of circulation indexes identified while sections 6 and 7 focus specifically on the remote sensing and in situ observations respectively a summary and conclusions follow in section 8 the companion study of levin et al 2020 hereafter referred to as part ii presents a detailed analysis of the impact of the observations from the pioneer array 2 observation impacts and 4d var the methodology used in roms to compute the impact of the observations on 4d var ocean circulation estimates is based on that employed in numerical weather prediction originally developed by langland and baker 2004 hereafter lb the procedure used in roms is described in detail by moore et al 2011a b 2017 levin et al 2019 hereafter l19 have explored in detail the impact of remote sensing observations in one component of the roms configuration considered here so for brevity only a short overview of the approach will be presented in the sequel the roms state vector will be denoted by x and comprises all of the ocean grid point values of the roms prognostic variables namely temperature t salinity s two components of horizontal velocity u v and free surface height ζ if x b denotes the background state vector and x a is the analysis then 1 x a x b k y o h x b where y o denotes the vector of observations h is the observation operator that maps from state space to observation space and k is the kalman gain matrix in the case of 4d var the observation operator h includes the nonlinear model in the roms application considered here the dual form of 4d var was used in which case k b h t hb h t r 1 where b and r are background error and observation error covariance matrices respectively and h represents the tangent linearization of the observation operator h in 4d var h includes the tangent linearization of the nonlinear model and h t includes the adjoint model the analysis x a is identified by minimizing the incremental formulation of the 4d var cost function courtier et al 1994 specifically the lanczos formulation of the restricted b preconditioned conjugate gradient rpcg method is used gratton and tshimanga 2009 as described by gürol et al 2014 following this approach the dual kalman gain matrix for each outer loop is factorized according to k b h t v m t m 1 v m t hb h t r 1 where m is the number of inner loops and each of the m columns of v m represents each cg descent direction normalized to unit amplitude the so called lanczos vectors and t m is a known tridiagonal matrix in this form k represents a reduced dimension approximation of k the impact of the observations on the analysis x a can be quantified in terms of their influence on a chosen index i x specifically δ i i x a i x b represents the change in i due to assimilating the observations y o and following lb can be expressed to 1st order as δ i y o h x b t k t i x x b the reduced dimension approximation k of k then leads to 2 δ i y o h x b t r 1 hb h t v m t m 1 v m t hb i x x b where i x x b represents the derivative of i with respect to x evaluated using the background x b from 2 it is clear that δ i is given by the dot product of the innovation vector d y o h x b and the vector g r 1 hb h t v m t m 1 v m t hb i x x b which quantifies the impact of the observations on δ i since each element of d is uniquely associated with a single observation so then are the corresponding elements of g such that the product d i g i represents the contribution aka impact of the i th observation to δ i the observation impacts for a particular data assimilation cycle can therefore be easily computed from the archived 4d var lanczos vectors 3 model configuration and data assimilation the roms configuration used here spans the mid atlantic bight and the gulf of maine as illustrated in fig 1 and three layers of nesting were employed the outer most domain g1 has a horizontal resolution 7 km and 40 terrain following levels stretched so that the thickness of the surface most layers is in the range 0 1 1 8 m and 0 1 3 4 m near the bottom over the continental shelf the choice of number of vertical levels was based on previous experience with roms in the ne atlantic e g fennel et al 2006 zhang et al 2010 wilkin and hunter 2013 the middle refined grid g2 is centered on the nsf ooi pioneer array with a horizontal resolution of 2 4 km also with 40 terrain following levels in the vertical the innermost refined grid g3 is likewise centered on the pioneer array with 40 levels in the vertical and 0 8 km horizontal resolution g1 was constrained at the open boundaries using data from the mercator océan global analysis drévillon et al 2008 with temperature and salinity adjusted to remove seasonal bias compared to a local regional climatology of fleming 2016 in typical forward simulations all three grids can be run using one or two way nesting the open boundary mean dynamic topography mdt and seasonal cycle of sea surface height ssh variation were also adjusted for bias using a regional data assimilative climatological seasonal analysis computed following the procedure described by levin et al 2018 and wilkin et al 2018 the sub tidal mesoscale variability captured by mercator océan is retained harmonic tidal forcing mukai et al 2002 was added to the boundary ssh and depth averaged velocity data sea surface wind stress and heat and freshwater fluxes were derived from 3 hourly national centers for environmental prediction ncep north american mesoscale nam forecast marine boundary layer conditions and standard bulk formulae of fairall et al 2003 nam air pressure was also imposed as a surface condition to the pressure gradient force so that the model computes a dynamic inverted barometer ib response accordingly an equilibrium ib sea level term is added to the open boundary sea level data which is standard practice in altimeter data processing daily river in flows were imposed at 22 discharge sites based on u s geological survey and water survey of canada observations and a statistical model that adjusts for ungauged portions of the watershed lopez et al 2020 wilkin et al 2018 a full description of the 4d var system applied to g1 can be found in levin et al 2018 wilkin et al 2018 and l19 so only a summary of the crucial points will be presented here the data assimilation system used is the dual formulation of the roms 4 dimensional variational 4d var system moore et al 2011a gürol et al 2014 roms 4d var was run using two outer loops and seven inner loops a list of the data assimilated and the source of each data type is given in table 1 and span the period jan 2014 dec 2017 the roms 4d var systems do not yet function across one or two way nested configurations although this capability is currently under development therefore the following strategy was employed to assimilate the available observations into the three grids 1 observations were first assimilated into g1 for the full 2014 2017 period using a 3 day assimilation window and treating the model initial conditions surface forcing all components and open boundary conditions as control variables the analysis state x a at the end of the previous 3 day 4d var cycle was used as the background state x b at the beginning of the current analysis cycle 2 step 1 was then repeated for grid g2 using the 4d var analyses from each cycle of g1 as the background open boundary conditions for each 4d var cycle of g2 as in g1 the initial conditions surface forcing all components and open boundary conditions were all adjusted during a 3 day 4d var cycle 3 step 2 was then repeated for grid g3 using the 4d var analyses from each cycle of g2 as the background open boundary conditions for each 4d var cycle of g3 in this case the 4d var window was reduced to 1 day and only the initial conditions and open boundary conditions were adjusted during each 4d var cycle also because of the considerable increase in computational expense 4d var was only run on g3 for the period 2014 2015 in combination steps 1 2 and 3 lead to corrections to the initial conditions every 3 days in the case of g1 and g2 and every day in the case of g3 in the case of g1 and g2 the surface fluxes are continuously adjusted during the 3 day assimilation cycle while on g3 there are no corrections made to the surface forcing on all three grids the open boundary conditions undergo continuous adjustments clearly each child grid benefits from the 4d var estimate from the parent grid only at the child grid open boundaries therefore each grid receives information from the observations only once except for interior observation influences on the open boundary conditions the background initial conditions for the first 4d var cycle on 1 jan 2014 on g1 were taken from a previously computed 4d var reanalysis spanning the period 2007 2013 see wilkin et al 2018 the background initial conditions for the first cycle on 1 jan 2014 on g2 and g3 were linearly interpolated from g1 it is normal procedure to combine multiple observations of the same type that fall within a single grid cell and that are closely spaced in time into super observations super observations were computed where appropriate separately for each of the three grids see table 1 therefore given the difference in horizontal resolution of each grid the observations assimilated into the model in each case within the overlapping region were not the same fig 2 shows time series of the total number of observations assimilated into the model on each grid during a 4d var cycle despite the changing size of the grid and the shorter 4d var window length of g3 the total number of observations assimilated on each grid during each cycle is similar even though the grid resolution changes going from g1 to g2 to g3 because less super obing is required on the higher resolution grids compared to g1 fig 2 information can be readily converted to logarithm of the observation density by subtracting log10 of the number of grid points which is 6 01 6 25 and 6 45 for g1 though g3 respectively this indicates that there is typically one observation per 10 grid cells in a 3 day cycle fig 2 also shows time series of the number of observations assimilated from each observing platform apart from satellite altimetry the number of observations from each platform is similar across all three grids on the other hand for altimetry there is an order of magnitude reduction in the number of observations going from one grid to the next due to the spatial separation of the altimeter ground tracks indeed fig 2f shows that during some 4d var cycles no altimeter tracks crossed g3 as described in moore et al 2011a the 4d var background error covariance b matrix was modeled following the diffusion operator approach of weaver and courtier 2001 the decorrelation length scales assumed in b for errors in each control variable are listed in table 2 and these parameter choices are discussed in l19 all components of the surface fluxes were included in the control vector both components of surface wind stress the total surface heat flux and the total surface freshwater flux while formally the surface flux corrections should be computed every model time step this is not practical so the corrections were calculated every hour and linearly interpolated in time the standard deviations for the background surface errors were estimated from a multi year run of the model without data assimilation the observation error covariance matrix r was assumed to be a diagonal matrix and the observation errors are also summarized in table 1 and discussed in l19 quality control was also performed during each 4d var cycle following andersson and järvinen 1999 as described by moore et al 2013 specifically the innovation d i associated with each observation is compared to the standard error based on the assumed standard deviations of the background σ b and observation σ o errors in particular if d i 2 α 2 σ b 2 σ o 2 then the observation is rejected and not included in the analysis the threshold parameter α is dependent on the type of observation and is given in table 2 for the analyses on each grid considered here a time series of the total number of observations rejected during each 3 day 4d var cycle is shown in fig 2 and is typically o 103 104 indicating that only 1 of the total number of observations were rejected based on the chosen criteria the performance of the 4d var system on g1 is described in detail by levin et al 2018 wilkin et al 2018 and l19 fig 3 shows probability density functions pdfs for the innovations associated with observations of sea surface temperature sst ssh in situ temperature and in situ salinity for each grid in principle if b and r are correctly prescribed the innovations d should be normally distributed with a covariance given by hb h t r therefore for reference fig 3 also shows the pdfs for normal distributions with the same mean and standard deviation as the innovations computed during the 1st outer loop clearly for all observation types on all three grids the innovation pdfs depart significantly from the expected normal distributions and are more reminiscent of a laplacian distribution for the most part the mean innovations for temperature and salinity are close to zero for all three grids the mean ssh innovations however are negative on all three grids indicating that on average the mean model ssh exceeds that of the observations for salinity while the mean innovations are close to zero there is an overall tendency for the model to favor negative innovations in all three grids as evidenced by the skewed nature of the pdfs the innovation pdfs for the 2nd outer loop are qualitatively similar to those for the 1st outer loop not shown the fit of the 4d var analyses to the observations is presented in fig 4 which shows time series of ratio of the final and initial values of j o h δ x d t r 1 h δ x d the contribution of the observations to the incremental 4d var cost function for all temperature observations sst and in situ and the observations of the zonal component of velocity u fig 4 indicates that the largest reduction in j o occurs during the 1st outer loop this is the case for all data types assimilated not shown in addition fig 4 shows that while the fractional reduction in j o associated with observations of temperature is similar in all three grids the decrease in j o for u increases with increasing resolution indicating that the model is able to capture the sub mesoscale variability in ocean currents more effectively this is discussed in detail in part ii the surface forcing increments are generally small for both the g1 and g2 analyses and over most of the domain are just 1 2 of the seasonal standard deviation of the background fluxes 4 circulation indexes as noted in section 1 a dominant feature of the circulation in the mab is the front that separates the warm salty waters of the gulf stream from the cooler and fresher waters of the continental shelf significant excursions of the frontal location occur in association with numerous cross shelf exchange processes processes known to contribute significantly to cross shelf transport include frontal instabilities meandering and eddy fluxes wind forcing saline intrusions within the pycnocline vertical mixing upwelling within the bottom boundary layer and gulf stream ring interactions with the shelf gawarkiewicz et al 2018 an example of the latter is illustrated in fig 1 which shows the 4d var analyses of sea surface salinity sss on 16 may 2014 on all three grids a streamer of saline water associated with a large gulf stream ring can be seen impinging on the shelf this particular event has been studied in detail by zhang and gawarkiewicz 2015 and is captured well in the roms 4d var analyses on all three grids fig 1 shows very clearly how the 4d var circulation estimates can capture sub mesoscale secondary circulations as the grid resolution increases the observation impact indexes i considered here were chosen to target the position of the mab front and quantify the magnitude of cross shelf exchange fluxes particularly concerning the ooi pioneer array 4 1 frontal location the mab front has traditionally been associated with the position of the 34 5 isohaline and the point where this isohaline intersects the bathymetry is often used as a proxy for the foot of the front beardsley et al 1985 linder and gawarkiewicz 1998 onshore excursions of the front foot are associated with upwelling favorable conditions castelao et al 2008 where offshore ekman transport is balanced by onshore flow near the bottom lentz et al 2003 and such events are thought to be an important factor in the supply of nutrients to the continental shelf siedlecki et al 2011 following the generally accepted aforementioned definition an index was used that quantifies the change in the average front location based on the area between the position of the foot of the front in the background estimate x b and the front foot position in the analysis x a specifically 3 i f ξ 1 ξ 2 η ξ η r ξ d ξ where ξ η represent the local cartesian coordinates of the 4d var cycle average position of the front foot and η r ξ is a reference line thus i f represents the total area between the front foot location and the reference line the integral in 3 was performed along the benthic isoline that defines the front foot as it crosses the pioneer array operations domain which represents the endpoints ξ 1 and ξ 2 since the front is a dynamic feature ξ 1 and ξ 2 vary from one assimilation cycle to the next the reference line chosen for η r ξ is the seasonally varying climatological position of the front foot although the location of the reference line is unimportant since the index increment is given by 4 δ i f ξ 1 ξ 2 η a ξ η b ξ d ξ where superscripts a and b refer to the analysis and background respectively this index differs fundamentally from the general case considered in section 2 in that 4 is not an explicit function of the state vector x it is therefore necessary to linearize η a ξ about the background frontal location η b ξ in order to apply the adjoint based approach described in section 2 if r b ξ b i η b j represents the position vector of the coordinate pairs that define the position of the isohaline foot in the background then it is easy to show that to 1st order the difference between the position vector of the isohaline in the analysis and the background δ r in the direction of the background salinity gradient s b is given by δ r δ s s b s b 2 where δ s s a r b 34 5 and s a r b is the salinity of the analysis evaluated at the position of the foot of the background 34 5 isohaline 1 1 we chose to identify the displacement vector δ r in the direction of the gradient s b such that δ s s a r b 34 5 δ r s b thus we require δ r α n where n s b 1 s b is the local unit vector parallel to the background gradient and α is a scalar therefore δ s δ r s b α s b 1 s b s b α s b in which case α δ s s b 1 thus δ r α n δ s s b 2 s b thus a 1st order approximation of the position of the foot of the front in the analysis is r a ξ a i η a j r b δ r in this way the area δ i f i f r a i f r b can be expressed as a function of δ r δ s s b s b 2 which itself is a function of the background state vector x b as required by 2 to illustrate fig 5a shows a time series of front foot index i f computed from the background circulation x b on grid g1 a 30 day running mean was applied to highlight more clearly the seasonal and interannual variations in i f fig 5a indicates that the 4d var analysis tends to favor movements of the front foot onshore i f 0 in contrast offshore movements i f 0 are typically smaller furthermore while there is significant variability in i f there are no obvious interannual variations in the seasonal cycle time series of i f on g2 and g3 are qualitatively and quantitatively similar to that shown in fig 5a for g1 not shown the mean and standard deviation of i f on each grid is summarized in table 3 indicating that the front foot statistics as measured by this index are similar across all three grids time series of the foot front index increments δ i f that arise from assimilating the observations are shown in fig 5b for both the 1st and 2nd outer loops again there are no noticeable interannual variations in the seasonal cycle of the increment time series which are characterized instead by irregular movements of the front onshore and offshore in response to 4d var corrections to the circulation during the 1st outer loop the increments δ i f 0 1 i f while during the 2nd outer loop the δ i f are generally smaller the mean and standard deviation of δ i f during both outer loops and for the 4d var analyses on all three grids are presented in table 4 on g1 the mean δ i f are positive indicative of a tendency for 4d var to correct for a mean offshore bias in the front foot location of the background table 4 shows that this bias is significantly reduced on g2 and changes sign on g3 but is close to zero 4 2 frontal stratification as a measure of the level of stratification associated with the front we follow the work of simpson and bowers 1981 who studied fronts in the north sea in terms of the potential energy required to thoroughly mix the upper part of water column specifically we consider a index of stratification given by 5 i e v 1 g d ζ ρ ρ z d z d a where ρ and ρ are respectively the in situ and vertically averaged density both averaged over the assimilation window d is a chosen depth ζ is the free surface displacement and the area integral is performed over the pioneer array glider domain shown in fig 1c the depth d was chosen to be the average depth of the front foot across the pioneer array glider domain in 5 v represents the volume encompassed by the integrals with the result that i e is the energy per unit volume j m 3 that is required to completely mix the upper d meters of the water column within the glider domain fig 5c shows a time series of i e computed from the 4d var analyses of g1 the seasonal cycle is associated with low values of i e during the winter when the upper water column is fairly well mixed and high values of i e during the summer after the water column has re stratified vertical sections of salinity during a typical minimum in i e on 3 march 2016 and a typical maximum in i e on 30 august 2016 are shown in fig 6a and b respectively during march the mab front is well defined within the pioneer glider domain the depth d over which the potential energy i e in 5 is computed as the average depth of the front foot over the glider domain and the intersection of the 34 5 isohaline with the bathymetry which defines the front foot cf section 4 1 is clearly visible in fig 6a at a depth of around 75 m above this depth the water column is well mixed over much of the glider domain which accounts for the low value of i e at this time of year while the depth of the intersection of the 34 5 isohaline with the bathymetry is similar during august fig 6b the water column is strongly stratified over much of this depth within the glider domain which accounts for the high value of i e during this time thus i e can be a useful indicator of the strength of the front in terms of the mean stratification within the glider domain where low values of i e correspond to situations where the front is well defined within the pioneer target area and vice versa for high values of i e stratification is an important factor in this region since it also influences shelf slope exchange via the development of instabilities e g houghton et al 1988 onshore intrusions of saline waters from over the continental slope lentz 2003 and the efficiency of vertical mixing time series of i e from g2 and g3 are both qualitatively and quantitatively similar to that shown in fig 5c not shown the mean and standard deviation of i e on all three grids is shown in table 3 and confirm that they vary within a similar range time series of the increments δ i e arising from 4d var are shown in fig 5d for grid g1 for both outer loops during the 1st outer loop δ i e is generally negative for much of the time indicating that 4d var is reducing the stratification and potentially strengthening the mab front in the pioneer target region the increments during the 2nd outer loop are typically smaller however during some periods they partially offset those of the 1st outer loop indicating that during some cycles data assimilation reduces the stratification too much during the 1st outer loop and some re stratification is necessary during the 2nd outer loop so that the circulation is more consistent with observations the mean and standard deviation of the increments δ i e are presented in table 4 for all three grids a negative bias is apparent on g1 suggesting that in this case 4d var is largely correcting for bias in the stratification table 4 indicates that the bias is much reduced on g2 and is close to zero on g3 suggesting that in both cases the stratification is more consistent with the observations 4 3 transport indexes as noted earlier there is also considerable variability in the cross shelf exchange of water masses therefore a series of indexes were also computed to quantify the impact of the observations on the 4d var estimates of conditions at the shelf break in the vicinity of the ooi pioneer array specifically we consider the following indexes 6 i u s 1 s 2 h 0 u n u n d z d s 7 i u t ρ o c p a 1 s 1 s 2 h 0 u n u n t t d z d s 8 i u s 1 0 3 ρ o a 1 s 1 s 2 h 0 u n u n s s d z d s 9 i t a 1 s 1 s 2 h 0 t t d z d s 10 i s a 1 s 1 s 2 h 0 s s d z d s in each case s 1 s s d s represents an integral along a section of the h 200 m isobath nominally identified as the location of the continental shelf break the vertical section chosen is indicated in each panel of fig 1 and cuts through the middle of the pioneer array in 6 10 u n corresponds to the component of the velocity that is locally normal to the section s and an over bar denotes the time average over each assimilation cycle the tilde represents the mean seasonal cycle and a is the area of the cross section therefore i u i u t and i u s are measures of the departures from the mean seasonal cycle of the 4d var cycle average total volume transport heat transport and salt transport across the shelf the indexes i t and i s are a measure of the departures from the mean seasonal cycle of the 4d var cycle average temperature and salinity along the section they are used as additional diagnostics on g1 only fig 7a e show time series of each index computed from the analysis state vector x a in the case of the transport indexes i u i u t and i u s positive negative values represent onshore offshore transports relative to the mean seasonal cycle also shown in fig 7a e are time series of the same indexes computed from a one way nested run of the model without data assimilation subject to the same prior atmospheric conditions on all grids and the same mercator océan open boundary conditions on g1 fig 7a c show that there is considerable variability on a range of time scales in the cross shelf transports also the transports are significantly modified by data assimilation a closer inspection of the time series shows that periods of significant heat and salt transport are a combination of both the volume transport and changes in the mean temperature and salinity along the target section time series of i u i u t and i u s for g2 and g3 not shown are comparable to those shown in fig 7 for g1 table 3 summarizes the mean and standard deviations of the transport indexes on the three grids for i u t and i u s the mean and standard deviations are similar across all three grids although for i u there is an apparent onshore volume transport bias on g2 and g3 the volume transport increments δ i u of fig 7f are generally small compared to i u indicating that data assimilation is not making large corrections to the circulation during each 4d var analysis this is desirable behavior and suggests that the model is not subject to large adjustments and is mostly consistent with the new observations that are being assimilated on the other hand the heat and salt transport index increments δ i u t and δ i u s of fig 7g and h are a more significant fraction of i u t and i u s and are reflective of the changes in temperature and salinity increments across the target section cf fig 7i and j in all cases the increments during the 1st outer loop are typically larger than during the 2nd outer loop it is also noteworthy that the increments in the transport index time series exhibit fluctuations on time scales similar to that of the model run without data assimilation which suggests that 4d var is correcting changes in the circulation that are associated with the dynamic intrinsic variability on time scales longer than the 3 day assimilation windows the mean and standard deviation of the increments in each index are summarized in table 4 for both outer loops on all three grids on g1 the mean volume transport increments are offshore but close to zero on g2 and g3 conversely the mean heat and salt transport increments are onshore on all grids and decrease with increasing resolution 5 observation impacts since two outer loops are employed in the 4d var analyses it is necessary to compute the observation impacts separately for each outer loop if x n a denotes the 4d var analysis at the end of the n th outer loop then the observation impacts are quantified according to δ i 1 y o h x b t k 1 t i x x b and δ i 2 y o h x 1 a t k 2 t i x x 1 a where δ i 1 and δ i 2 represent the increment in the index i at the end of the 1st and 2nd outer loop respectively k 1 t and k 2 t are the reduced dimension kalman gain matrices for each outer loop and i x x b and i x x 1 a represent the derivatives of the index i evaluated using x b and x 1 a since x 1 a depends on the observation values δ i 2 cannot be unambiguously decomposed into the contributions from each observation however as discussed by trémolet 2008 since δ i 1 δ i 2 much of the impact of the observations on the final 4d var analysis can be attributed to the 1st outer loop this is also found to be the case here as confirmed in figs 5 and 7 which show time series of δ i 1 and δ i 2 for each of the target indexes on g1 similarly the standard deviations in table 4 confirm that the δ i 1 δ i 2 on g2 and g3 also therefore in the sequel we will consider only the observation impacts during the 1st outer loop it should also be noted that with additional computational effort δ i 1 can be decomposed into the contributions from the different components of the control vector however preliminary analyses of δ i u not shown revealed that 99 of the increment in volume transport is associated with the increment in the initial conditions therefore in the sequel we have considered the total impact arising collectively from all elements of the control vector the impact of the observations on each index was quantified according to 2 which represents a 1st order linearization of the index increment arising from data assimilation eq 2 shows that an important ingredient of these calculations is i x x b since 4 7 and 8 represent nonlinear indexes computation of this first derivative is an additional linear approximation in the procedure therefore before proceeding to compute the observation impacts it is essential to test the veracity of the linear assumptions in 2 to this end table 4 shows the correlation coefficient r between the time series of the increments δ i in each index computed from 2 and those calculated directly from the analysis and background estimates of i in most cases r exceeds 0 9 and in several instances is very close to 1 the lowest correlations are associated with i f on g2 and g3 0 86 and 0 75 respectively and with i e on g3 0 77 nonetheless these correlations are still respectable and confirm that the linear approximations employed will yield reliable estimates of the observation impacts in these cases also the contribution aka impact of each observing system to the increments δ i of a chosen index will vary from cycle to cycle and depends on several factors including the number and distribution of the observations the time evolution of the background circulation x b t and the hypotheses about the background and observation errors described by b and r to illustrate fig 8 shows time series from g2 of the impact of each type of observation on the 1st outer loop increments in each of the indexes considered here there are several noteworthy features of fig 8 firstly the relative impact of the various observing systems on a given δ i changes through time and it is not always the same type of observations that have the largest impact secondly there is generally a great deal of consensus between the impact of observations from different platforms in that they usually have the same sign at any given time however there are a few periods where the impacts from different platforms are in opposition thirdly the relative impact of each observation type during a particular time interval varies from index to index in the g2 example shown it is observations of temperature remotely sensed and in situ and velocity hf radar and in situ that exert the greatest control on all of the indexes the impact of salinity observations is generally small on this grid and the impacts of altimetry are generally negligible due to the limited size of the domain cf fig 1b and the low number of satellite overpasses in the following sub sections the observation impact information encapsulated in time series such as fig 8 will be examined in different ways 5 1 impact vs innovation a useful diagnostic of the performance of the 4d var system is the impact of each observation compared to the corresponding innovation i e the difference between the observation and the background sampled at the observation point to this end fig 9 shows scatter plots of the impact of each observation on the foot front index i f versus the innovation for each type of observation on all three grids each scatter plot can also be viewed as a contingency diagram and has the format of a 2 dimensional histogram showing the density of points that fall within each quadrant the percentage of the total number of points that fall within each quadrant is also indicated most of the scatter plots in fig 9 resemble butterfly wings in that observations associated with a small innovation i e instances where the model and observations are in excellent agreement also have little impact on i f as the innovation increases the range of impact that observations have on i f becomes larger as reflected by the wing structure furthermore observations that are associated with very large innovations i e instances where the model and observations are in poor agreement generally have a small impact on i f as noted in l19 this is a desirable feature of the 4d var system because very large innovations very likely represent cases of observations that have passed the quality control threshold through say the coincidence of a poor background solution at a bad observation location and we would not want these data to adversely impact the analysis however an inspection of the scatter plots also reveals some notable biases in the impacts innovations or both for example while the four quadrants of the scatter plot for ssh observations on g1 are fairly evenly populated fig 9a the corresponding scatter plots for g2 fig 9f and g3 fig 9k display a significant bias towards negative innovations in agreement with fig 3b and c another interesting feature of fig 9g and l is that the scatter plots for sst observations on g2 and g3 exhibit a banded structure in one quadrant in the case of g2 fig 9g many of the sst observations associated with positive innovations i e the model sst cooler than observed have a pronounced negative impact on the i f in that they tend to move the front further offshore conversely on g3 fig 9l sst observations associated with negative innovations i e the model sst warmer than observed impact the front foot location by moving it onshore further analysis reveals that these features are associated primarily with the avhrr and amsr and further investigation is warranted additional features of fig 9 that will be further discussed in the following sections include the general decline in the impact of the individual in situ temperature and salinity observations as grid resolution increases in contrast the impact of individual velocity measurements increases going from g1 to g3 cf fig 9e j and o scatter plots associated with the other indexes share many qualitative features in common with those of fig 9 not shown although other detailed features related to bias in the innovations bias in the impacts or bias in both are specific to different indexes other scatter plot examples for the transport index on g1 are presented and discussed in l19 5 2 rms impacts the observation impacts associated with each observation shown in the scatter plots of fig 9 can be parsed in various ways that highlight different aspects of the performance of the 4d var systems and the role played by different observing platforms in affecting circulation changes fig 10a c show the root mean square rms impact of each type of observation on i u t i f and i e for all three grids fig 10a is representative of the other transport indexes i u and i u s also not shown as discussed in l19 for g1 focusing first on remote sensing observations for g1 the sst and ssh observations collectively have a similar impact even though there are two orders of magnitude fewer observations from altimetry fig 2d for g2 the collective impact of altimetry decreases with increasing grid resolution this occurs for two reasons first as shown in l19 altimeter observations that are remote from the target section can have a significant impact on each index which accounts for some of the high impact of ssh on g1 in addition however as fig 2 shows the number of altimeter observations decreases considerably going from g1 to g3 because of the reduced geographical extent of each grid with increasing resolution for sst the impacts are higher on g2 than on g1 which is associated with a relatively large impact of these observations in the vicinity of the target section region that defines each index see also l19 in connection with g1 as fig 10a c show these local impacts carry over from g1 to g2 and g3 before discussing the impact of in situ observations we reiterate that the observation impacts depend on several factors including a the background circulation x b which of course is highly resolution dependent across the three grids b the background error covariance b and c the observation error covariance r the parameters used to compute b and r were not the same across all three grids since different error statistics are appropriate for each grid thus some of the changes in the relative impact of various components of the observing system on the three grids will depend on unavoidable variations in the error covariances it is important to remember that r is dominated by errors of representativeness which are difficult to estimate a priori for in situ temperature observations the standard deviations assumed for r are similar across all three grids and range from 0 6 c on g1 to 0 4 c on g2 and g3 however a posteriori analysis of the innovation statistics as described by desroziers et al 2005 suggests that these standard deviations should be closer to 1 c for in situ salinity observations the a priori observation errors were assumed to 0 2 on g1 while the a posteriori innovation statistics indicate that a more appropriate choice is 0 4 which is the value used for both g2 and g3 similarly for velocity measurements the standard deviation of the observation error on g1 was assumed to be 0 6 m s 1 for hf radar surface current estimates and 0 3 m s 1 for moorings these values were adjusted downwards to 0 1 m s 1 for hf radar observations and 0 04 m s 1 for moorings for both g2 and g3 and are more in line with the a posteriori innovation statistics while we would ideally like to compare cases where say only the model resolution is varying in the 4d var analyses across the three grids the high computational expense of these calculations precludes running a more detailed and controlled suite of experiments so we must draw on what we have nevertheless variations in the level of errors across the different grids provide an indication of their control on the impacts returning to fig 10 an obvious feature of fig 10a c is that in situ observations have the largest impact on g1 for all three indexes even though the number of in situ observations is an order magnitude less than the number of satellite sst observations see fig 2d in situ temperature observations maintain a relatively high impact on g2 although there is a significant decline on g3 this is partly because of the substantial reduction in the volume of observations and the loss of some remote impacts but also because of the increasing influence of velocity observations on the sub mesoscale circulation that is resolved by g3 see part ii much of the dramatic decline in the impact of in situ salinity observations on g2 and g3 when compared to g1 is most likely associated with the difference in the assumed level of observation error on g1 the level of observation error is probably too low so the 4d var analyses are drawing more heavily on these data than on g2 and g3 however there are other dynamical controls as well associated with geostrophic adjustment as discussed in part ii on g1 velocity observations have a relatively small impact on all indexes partly because of the high value assumed for the errors of representativeness but also because of dynamical controls see part ii the impact of velocity observations increases from g1 to g2 and then again from g2 to g3 mainly because current measurements from the pioneer array moorings play an increasingly greater role in shaping the sub mesoscale circulation as grid resolution increases this is discussed in more detail in part ii while fig 10a c represent the aggregate average impact of different observation types on the target indexes fig 2 shows that there is considerable disparity in the number of observations of each type that are assimilated into the model therefore it is informative to normalize the observation impacts by considering the rms impact per datum which is shown in fig 10d f note that super observations are considered as a single datum fig 10d f show that in situ observations have by far the largest impact per datum for all indexes and across all grids as discussed by l19 each ssh observation on g1 is 50 times more impactful than an individual sst observation this carries over to some extent to g2 as well although the factor decreases to 7 because the impact of ssh observations that are remote from the target sites is lost on g3 the impact per datum of ssh and sst is similar because there are so few ssh observations to impact the circulation estimates fig 10d f show that in general except for the case of velocity observations the observation impact per datum decreases with increasing resolution and decreasing domain size this is most likely a combination of two factors i as resolution increases the model can capture more faithfully the mesoscale and sub mesoscale circulation features and ii as shown in table 2 the decorrelation length assumed for the background errors decreases with increasing resolution so the radius of influence of each observation will be correspondingly smaller moving from g1 to g3 5 3 observation impact as an indicator of 4d var performance as discussed by trémolet 2008 the impact of each observation on the analysis or ensuing forecast can be tracked line by line through the data assimilation code this provides a powerful means for monitoring the performance of the 4d var system at various levels and different stages of the calculation in roms the observation impacts are evaluated during each inner loop iteration and provide a quantitative measure of how the observations are being utilized during the assimilation procedure to illustrate fig 11 shows the rms impact of each observation type on i u and i e on all grids during each inner loop of the two outer loops employed on g1 fig 11a and d confirm the dominant role played by in situ observations of t and s in controlling i u and i e for these data fig 11a reveals that for i u during the 1st outer loop the impacts asymptote to a near constant value after just three inner loops for i e fig 11d the impacts of in situ t exhibits similar behavior however for in situ s the impacts continue to trend upwards even after seven inner loops indicating that there is more useful information to be utilized from these data the aggregate impact of sst and ssh on g1 is similar for both indexes fig 11a and d consistent with fig 10a c and also show a continuing upward trend at the end of the 1st outer loop indicating that there is additional useful information that could be extracted from these data too fig 11 also confirms that the observation impacts during the 1st outer loop are larger than those during the 2nd outer loop the indexes i u t i u s and i f exhibit similar characteristics not shown but see also l19 on g2 satellite sst and velocity observations emerge to play a more dominant role as shown in fig 11b and e in this case altimetry plays a minor role and the impact in situ s has been largely relegated as noted in section 5 2 in the case of i e sst observations have the most impact for both i u and i e on g2 the impact of the dominant data types continues to exhibit an upward trend at the end of the 1st outer loop suggesting that the 4d var analyses on this grid may benefit from additional inner loops the indexes i u t i u s and i f exhibit similar characteristics not shown on g3 velocity observations emerge as generally the most impactful observations for i u fig 11c the impact of these data is also significant for i e fig 11f but velocity loses its poll position to sst observations after four inner loops for both i u and i e the continued upward march of the observation impacts at the end of the 1st outer loop indicates that additional inner loops could be beneficial on g3 also similarly for the indexes i u t i u s and i f not shown 6 remote sensing observation impacts the geographical distributions of the observation impacts associated with satellite observations are particularly revealing and display what we believe are the signature of the dynamical processes that are responsible for conveying information from the observations to the target sites that define the impact indexes i with this in mind fig 12 shows the rms impact per datum of all sst observations that fall within each model grid cell the cases shown are for i u t i f and i e on all three grids for g1 fig 12a c reveal the presence of large scale coherent patterns of impact for sst that are common to all three indexes these same patterns are present for i u and i u s also not shown and as discussed in l19 are associated with the underlying dynamics of the circulation and the structure of the inverse total error covariance matrix in observation space hb h t r 1 aka the inverse stabilized representer matrix which lies at the heart of the analysis equation 1 in particular fig 12a c show regions of elevated impact that are both local to the index target regions and remote such as the north wall of the gulf stream fig 12d f show that the geographical distributions of sst impacts on g2 for the three indexes shown also share common features the most apparent features are the high impacts extending upstream from the target areas that are associated with the equatorward flowing shelf break jet and the tongue of high impact associated with flow exiting the gulf of maine through the great south channel that defines the western edge of georges bank similar features are present on g1 also fig 12a c as in g1 it is likely that these features common to all of the indexes including i u and i u s not shown are controlled by the combined influences of the background circulation x b and prior assumptions assumed in the 4d var procedure via the inverse stabilized representer matrix hb h t r 1 in the case of g3 fig 12g i show that while there is some commonality in the geographic distribution of the sst impacts most conspicuously associated with the shelf break jet there are also some significant differences the differences are probably a reflection of the more complex nature of the flow of information through the g3 4d var analyses due to the intricacies of the sub mesoscale environment cf fig 1c this is a topic that warrants further exploration but as shown by l19 the analysis of the factors controlling the characteristic patterns of impact is rather involved the patchwork patterns apparent in fig 12g i are associated with variations in sst coverage of the different observing platforms for example wsat is a microwave instrument with a low resolution foot print that covers only part of the g3 domain the geographical distributions of the rms impacts of altimetry observations also display interesting and dynamically controlled patterns of local and remote influence as shown in detail by l19 for g1 similarly while not shown here ssh impacts on the g2 circulation indexes share some similarities with their g1 counterparts in the case of g3 the altimeter coverage during the 2014 15 period considered is fairly sparse so it not so easy to identify robust geographical distributions of impact in this case as discussed in l19 some aspects of the local and remote impacts apparent in fig 12 can be understood in terms of information horizons see also moore et al 2015 the distance over which information contained in the observations can travel via the processes of wave propagation and advection l19 estimate that during a typical 3 day assimilation cycle as employed in g1 and g2 the information horizon associated with horizontal advection is 25 km for the shelf break jet and 500 km for the gulf stream the information horizon associated with internal waves is estimated to 500 km also while information carried by barotropic waves can reach every point in the model domain 7 in situ observation impacts the impact of in situ observations on the 4d var analyses on g1 g2 and g3 is the subject of part ii with a particular focus on the nsf ooi pioneer array in this section we present a broad overview of the in situ observation impacts and the interested reader is encouraged to consult part ii for a more detailed account fig 10 shows that in situ observations of temperature and salinity have the largest impact on all indexes on g1 both on aggregate fig 10a c and in terms of the impact per datum fig 10d f the geographic distribution of the vertically integrated rms observation impact for in situ t and s combined is illustrated in fig 13 for the cross shelf heat transport index i u t as in the case of sst fig 12a the in situ observations exhibit impacts that are both local and remote from the target section in this case the influence of observations downstream along the shelf break current is a marked feature as is the upstream impact of the dense set of observations by gliders and hydrographic surveys on the scotian shelf while observations on the scotian shelf help constrain the modeled equatorward inflow from the north that is subsequently partitioned between entering the gulf of maine or following the shelf break south of georges bank towards the pioneer array site lopez et al 2020 the time scale of this transport far exceeds the 3 day analysis interval of the observation impacts this distant teleconnection is however well within scope for the influence of freely propagating coastal trapped waves ctw brunner et al 2019 calculate that the mode 1 free ctw at the pioneer site has a phase speed of some 7 m s 1 at this speed ctws originating on the scotia shelf in response to the data assimilation adjustments will traverse the 900 km to the pioneer array via the continental slope wave guide within 1 5 days as in the case of sst and ssh the geographic distribution of the in situ hydrographic observations is relatively robust across all metrics considered here not shown fig 10 also shows that while the aggregate impact of in situ hydrographic observations generally declines going from g1 to g3 the impact per datum of these data remains relatively high the geographic distribution of the observation impacts exhibits robust features on g2 and g3 across all indexes not shown and will be discussed in more detail in part ii the aggregate rms impact on i u t of in situ observations of t and s versus depth is illustrated in fig 14a c for all three grids for g1 fig 14a the impact of t and s is similar in the upper 400 m of the water column although below this depth the temperature observations dominate for g2 and g3 the impact of s is diminished compared to g1 consistent with figs 10 and 11 the vertical profiles of temperature impacts are similar across all three grids and in each case indicate the presence of elevated impact in the range 500 1000 m the rms impact per datum versus depth for t and s is shown fig 14d f which highlights the influence of subsurface temperature observations in the 500 1000 m depth range profiles of observation impacts for t and s for the other indexes display qualitatively similar features to those shown in fig 14 not shown the rms impacts of velocity observations versus depth are shown in fig 15 again for i u t although the main features are qualitatively similar for the other indexes not shown the aggregate impacts of fig 15a c indicate that velocity observations in the upper 100 m of the water column have the largest impact the impact also increases going from g1 to g3 in keeping with figs 10 and 11 the rms impact per datum on the other hand is relatively uniform below about 20 m fig 15d f while most of the surface velocity observations are from hf radar estimates the majority of subsurface measurements are from the pioneer array moorings which measure currents down to 75 m on the shelf and 400 m beyond the shelf break 8 summary and conclusions a state of the art 4 dimensional variational data assimilation system has been applied in a three level nested configuration of roms to compute estimates of the time evolving ocean circulation in the mid atlantic bight with a particular focus on the region served by the nsf ooi pioneer array the outer most model grid forms the basis of the near real time analysis forecast system that is currently run in support of the u s ioos maracoos regional association levin et al 2018 wilkin et al 2018 in which observations of the ocean from a broad range of remote sensing and in situ platforms are assimilated in the nested configuration considered here a wide range of circulation regimes are well represented spanning the western current the energetic mesoscale eddy field and the complex sub mesoscale circulation that is populated by ephemeral frontal features while these are all challenging circulation environments for any data assimilation system various diagnostic system indicators demonstrate that 4d var performs well across all three domains the primary goal of this study is to quantify the direct impact that observations from the various observing platforms that serve the maracoos region have on different aspects of the ocean circulation here the specific focus has been on the mab shelf break front and associated slope shelf exchange processes in the vicinity of the pioneer array since a goal of this component of the ooi is to explore the dynamics that control these processes with this in mind several indexes of the circulation were considered as quantitative indicators of different aspects of the dynamics in the vicinity of the shelf break front specifically we considered the location of the front the strength of the associated stratification and the cross shelf transport of mass heat and salt as one might expect significant differences exist between 4d var solutions and a one way nested free running model also 4d var leads to significant increments in the chosen circulation indexes on time scales that are similar to the intrinsic variability of the free model indicating that 4d var is not just merely making reactionary corrections to the ocean state in response to the model minus observation differences but is also informing the evolution of the circulation on a range of time scales in a dynamically consistent way in this study an adjoint approach similar to that used operationally in numerical weather prediction was used to quantify the impact of the observations on the 4d var increments in each chosen circulation index the observation impacts were found to vary considerably in space and time depending on the number type and spatial distribution of the observations the background circulation and the statistics assumed a priori for the errors in the background and observations however the geographic distribution of the observation impacts was found to be robust across all of the indexes considered and across the three domains unraveling the dynamics of the pathways by which a particular observation influences the ocean state in the near and far field is a complicated and involved process clearly there many moving parts in 2 used to compute the observation impacts l19 explored broadly the contributions and influence of model dynamics via h and h t and the error covariances b and r on the geographical distributions of the impacts while many features of fig 12 can be understood conceptually in terms of the information horizons associated with the fundamental processes of horizontal advection and wave propagation more detailed analysis is needed to identify the role of individual mechanisms it is useful to take a step back and remember what information the observation impact given by 2 provides eq 2 quantifies δ i given the observations available y o the prior hypothesis about errors in the observations via r errors in the background via b and hypotheses about the dynamics that control the ocean state via h furthermore the contribution of each observation to the dot product that defines δ i is unambiguous and is a reflection of all the assumptions and hypotheses that we have made the observation impact calculation will not however directly confirm or nullify these assumptions of hypotheses because if we change any aspect of the data assimilation system or the model then the circulation estimates will change and so too will the observations impacts however some aspects of the relative impacts of different observation types will obviously be robust since these are controlled by the dynamics and physics of the ocean nevertheless observation impact calculations like those presented here provide a quantitative measure of the relative value of observations from different observing platforms such information is of considerable value to decision makers since one could make a case for maintaining certain observing platforms based on the important or critical role they play in controlling some aspect of the state estimates and of course one could use the quantitative information that observation impact calculations provide to argue for increasing the coverage or level of redundancy of particularly impactful platforms our results show that there is generally a reasonable degree of consensus between the impacts of different observation types and observing platforms indicating that the 4d var system can make efficient use of complementary information from multiple sources on the other hand there is also considerable temporal variability in the relative impact of different observation types and as noted the impact of a particular kind of observation varies across the three domains as a result of changes in data density assumptions about error statistics and the change in the dynamical circulation regime see part ii for more analysis of the latter point the observation impacts are also a valuable tool for monitoring the efficacy of data streams and different components of an observing system for example significant changes in the impact of a particular data stream over time may be an indication of problems that are developing with the instrument or the data stream itself scatter diagrams like those in fig 9 can be used to identify outliers and anecdotally there have been instances in our own work where improvements were made to the quality control of some remotely sensed data that were identified as problematic in this way furthermore observation impact monitoring provides information about the performance of the 4d var system as in fig 11 clearly there are some observation types for which there is a continuing upward trend in the observation impact at the time that the 4d var calculations are terminated indicating that there is more useful information that can be mined from such data by further tuning of the 4d var system other calculations closely related to those presented here quantify the sensitivity of ocean state estimates to changes in the observation values or indeed the observing system by combining observation impact and observation sensitivity information the degree of synergy between different observing platforms can be quantified this idea was introduced in l19 and is developed further in part ii levin et al 2020 in which we explore in detail the role played by the pioneer array observing system in shaping the mab circulation estimates finally we note again that the observation impact methodology employed here can also be applied to the forecast problem in this case the extent to which each observation improves or degrades forecast skill as measured by a metric i can be quantified this type of analysis has been a mainstay in operational numerical weather prediction for some time and is now an important emerging activity in some near real time ocean analysis systems as well roms is at the forefront of these activities and is being used in this capacity and the results of ongoing forecast observation impact studies will be the subject of future publication credit authorship contribution statement julia levin conceptualization methodology writing original draft writing review editing software validation formal analysis investigation data curation hernan g arango methodology writing original draft writing review editing software bruce laughlin software elias hunter data curation john wilkin conceptualization methodology writing original draft writing review editing software validation formal analysis investigation resources project administration funding acquisition andrew m moore conceptualization methodology writing original draft writing review editing software validation formal analysis investigation resources project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by grants from the national science foundation united states oce 1459665 and oce 1459646 nasa united states nnx17ah58g and noaa united states na16nos0120020 pioneer array data were obtained from the nsf ocean observatories initiative data portal http ooinet oceanobservatories org 
23938,a nested configuration of the regional ocean modeling system roms comprising three grids was used in conjunction with a 4 dimensional variational 4d var data assimilation system to compute ocean state estimates of the mid atlantic bight mab the three nested grids have a horizontal resolution ranging from 7 km to 0 8 km and capture circulation regimes that span the gulf stream western boundary current through the mesoscale eddy field and down to the rapidly evolving and energetic sub mesoscale all of these circulation regimes are challenging for any data assimilation system yet the 4d var system was found to perform well across this range of space and time scales the observational data used to constrain the ocean state estimates comes from a wide range of remote sensing in situ and mobile platforms an adjoint based procedure was used to compute the impact of each observing platform on several different indexes that describe the position of the mab front stratification and associated cross shelf exchange processes in the vicinity of the u s national science foundation s ocean observatories initiative pioneer array the impact of observations from each observing platform on the chosen indexes varies across the three grids it is a function of several factors that include the nature of the background circulation and the level of error assumed for the background ocean state and the observations the geographic distribution of the observation impacts is remarkably robust across the various indexes and the three grids in addition observations that are both local to and remote from the target regions that define each index can exert a significant influence on the circulation variations in the observation impacts through time can be used to identify observations that exert unexpectedly large influence on the 4d var analyses i e outliers and routine monitoring of observation impacts is a useful indicator of the efficacy of different components of the observing system also the observation impacts were found to be a useful performance indicator for the data assimilation system keywords data assimilation 4d var observation impacts mid atlantic bight pioneer array 1 introduction data assimilation is an integral component of any ocean analysis and forecast system it is now a mainstream activity at most operational numerical weather prediction centers and many research institutions both on regional and global scales moore et al 2019 ocean data streams are dominated by remote sensing instruments that observe temperature and sea level but developments in novel sensors and autonomous platforms are rapidly expanding the delivery of in situ observations though typically inhomogenous in space and time sampling and much less numerous subsurface in situ data are an invaluable complement to dense satellite observations when assimilated into forecast models the information that these various platforms provide can interact in complex and sometimes surprising ways and data from one platform can support measurements from another unraveling the influence of the respective observations on the ensuing ocean analyses and forecasts can be very challenging nevertheless given the considerable financial and human resources required to deploy and maintain ocean observing networks the routine quantitative assessment of the impact of observations on analysis forecast systems is an important activity indeed observation impact assessments now form a critical component of most operational numerical weather prediction systems the focus of this study is the impact of observations in an analysis forecastsystem based on the regional ocean modeling system roms that encompasses the mid atlantic bight mab and gulf of maine gom in the nw atlantic fig 1a and is run in near real time in support of the u s integrated ocean observing system ioos mid atlantic regional association coastal ocean observing system maracoos a prominent feature of the mab region is a shelf break front separating the warm saline waters of the subtropical gyre from the cooler fresher waters of the continental shelf mountain 2003 intrinsic instabilities of the front fratantoni and pickart 2003 and eddy shelf interactions tied to gulf stream induced warm core rings zhang and gawarkiewicz 2015 contribute to the complexity of mab shelf break dynamics gawarkiewicz et al 2018 and are a major focus of the u s national science foundation s nsf ocean observatories initiative ooi as part of this initiative the pioneer array comprising fixed moorings and a fleet of autonomous underwater vehicles is deployed at the continental shelf break fig 1b c with a majority of the instruments operational since april 2014 the primary aim of pioneer is to increase understanding of the processes responsible for the transport of water masses across the shelf break and their relationship to forcing on a range of time scales but its limited area high density sampling pattern provides a valuable supplement to the wider scope maracoos observing system and presents exceptional opportunities to methodically contrast the impact of such disparate observing system designs there have been several efforts in oceanography to quantify the impact of observing systems on ocean analyses using a variety of methods that include observing system experiments e g balmaseda et al 2007 oke and schiller 2007 smith and haines 2009 spectral analysis of the representer matrix le hénaff et al 2009 quantification of the degrees of freedom of the observing system moore et al 2011b assessment of observation footprints oke and sakov 2012 and ensemble methods storto et al 2013 extensive reviews of these efforts can be found in oke et al 2015a b and fujii et al 2019 the present study uses an adjoint based approach developed by langland and baker 2004 and builds on the work of moore et al 2017 and levin et al 2019 the goal of this study is to quantify the impact of the various components of the maracoos observing system on circulation estimates derived from the roms 4 dimensional variational 4d var data assimilation system in light of the goals of the nsf ooi pioneer array a specific focus is the extent to which the observing system can inform roms about shelf break exchange processes in the vicinity of the mab front a brief overview of the 4d var and observation impact methodology employed is given in section 2 although the reader is directed to levin et al 2019 for a more detailed and thorough description section 3 describes the configuration of roms and 4d var the various data sources used and documents the performance of the data assimilation system the observation impacts are quantified in terms of specific indexes that target different aspects of the shelf break circulation and these are introduced in section 4 section 5 presents a summary of the impact of the observations from the various components of the observing system on the suite of circulation indexes identified while sections 6 and 7 focus specifically on the remote sensing and in situ observations respectively a summary and conclusions follow in section 8 the companion study of levin et al 2020 hereafter referred to as part ii presents a detailed analysis of the impact of the observations from the pioneer array 2 observation impacts and 4d var the methodology used in roms to compute the impact of the observations on 4d var ocean circulation estimates is based on that employed in numerical weather prediction originally developed by langland and baker 2004 hereafter lb the procedure used in roms is described in detail by moore et al 2011a b 2017 levin et al 2019 hereafter l19 have explored in detail the impact of remote sensing observations in one component of the roms configuration considered here so for brevity only a short overview of the approach will be presented in the sequel the roms state vector will be denoted by x and comprises all of the ocean grid point values of the roms prognostic variables namely temperature t salinity s two components of horizontal velocity u v and free surface height ζ if x b denotes the background state vector and x a is the analysis then 1 x a x b k y o h x b where y o denotes the vector of observations h is the observation operator that maps from state space to observation space and k is the kalman gain matrix in the case of 4d var the observation operator h includes the nonlinear model in the roms application considered here the dual form of 4d var was used in which case k b h t hb h t r 1 where b and r are background error and observation error covariance matrices respectively and h represents the tangent linearization of the observation operator h in 4d var h includes the tangent linearization of the nonlinear model and h t includes the adjoint model the analysis x a is identified by minimizing the incremental formulation of the 4d var cost function courtier et al 1994 specifically the lanczos formulation of the restricted b preconditioned conjugate gradient rpcg method is used gratton and tshimanga 2009 as described by gürol et al 2014 following this approach the dual kalman gain matrix for each outer loop is factorized according to k b h t v m t m 1 v m t hb h t r 1 where m is the number of inner loops and each of the m columns of v m represents each cg descent direction normalized to unit amplitude the so called lanczos vectors and t m is a known tridiagonal matrix in this form k represents a reduced dimension approximation of k the impact of the observations on the analysis x a can be quantified in terms of their influence on a chosen index i x specifically δ i i x a i x b represents the change in i due to assimilating the observations y o and following lb can be expressed to 1st order as δ i y o h x b t k t i x x b the reduced dimension approximation k of k then leads to 2 δ i y o h x b t r 1 hb h t v m t m 1 v m t hb i x x b where i x x b represents the derivative of i with respect to x evaluated using the background x b from 2 it is clear that δ i is given by the dot product of the innovation vector d y o h x b and the vector g r 1 hb h t v m t m 1 v m t hb i x x b which quantifies the impact of the observations on δ i since each element of d is uniquely associated with a single observation so then are the corresponding elements of g such that the product d i g i represents the contribution aka impact of the i th observation to δ i the observation impacts for a particular data assimilation cycle can therefore be easily computed from the archived 4d var lanczos vectors 3 model configuration and data assimilation the roms configuration used here spans the mid atlantic bight and the gulf of maine as illustrated in fig 1 and three layers of nesting were employed the outer most domain g1 has a horizontal resolution 7 km and 40 terrain following levels stretched so that the thickness of the surface most layers is in the range 0 1 1 8 m and 0 1 3 4 m near the bottom over the continental shelf the choice of number of vertical levels was based on previous experience with roms in the ne atlantic e g fennel et al 2006 zhang et al 2010 wilkin and hunter 2013 the middle refined grid g2 is centered on the nsf ooi pioneer array with a horizontal resolution of 2 4 km also with 40 terrain following levels in the vertical the innermost refined grid g3 is likewise centered on the pioneer array with 40 levels in the vertical and 0 8 km horizontal resolution g1 was constrained at the open boundaries using data from the mercator océan global analysis drévillon et al 2008 with temperature and salinity adjusted to remove seasonal bias compared to a local regional climatology of fleming 2016 in typical forward simulations all three grids can be run using one or two way nesting the open boundary mean dynamic topography mdt and seasonal cycle of sea surface height ssh variation were also adjusted for bias using a regional data assimilative climatological seasonal analysis computed following the procedure described by levin et al 2018 and wilkin et al 2018 the sub tidal mesoscale variability captured by mercator océan is retained harmonic tidal forcing mukai et al 2002 was added to the boundary ssh and depth averaged velocity data sea surface wind stress and heat and freshwater fluxes were derived from 3 hourly national centers for environmental prediction ncep north american mesoscale nam forecast marine boundary layer conditions and standard bulk formulae of fairall et al 2003 nam air pressure was also imposed as a surface condition to the pressure gradient force so that the model computes a dynamic inverted barometer ib response accordingly an equilibrium ib sea level term is added to the open boundary sea level data which is standard practice in altimeter data processing daily river in flows were imposed at 22 discharge sites based on u s geological survey and water survey of canada observations and a statistical model that adjusts for ungauged portions of the watershed lopez et al 2020 wilkin et al 2018 a full description of the 4d var system applied to g1 can be found in levin et al 2018 wilkin et al 2018 and l19 so only a summary of the crucial points will be presented here the data assimilation system used is the dual formulation of the roms 4 dimensional variational 4d var system moore et al 2011a gürol et al 2014 roms 4d var was run using two outer loops and seven inner loops a list of the data assimilated and the source of each data type is given in table 1 and span the period jan 2014 dec 2017 the roms 4d var systems do not yet function across one or two way nested configurations although this capability is currently under development therefore the following strategy was employed to assimilate the available observations into the three grids 1 observations were first assimilated into g1 for the full 2014 2017 period using a 3 day assimilation window and treating the model initial conditions surface forcing all components and open boundary conditions as control variables the analysis state x a at the end of the previous 3 day 4d var cycle was used as the background state x b at the beginning of the current analysis cycle 2 step 1 was then repeated for grid g2 using the 4d var analyses from each cycle of g1 as the background open boundary conditions for each 4d var cycle of g2 as in g1 the initial conditions surface forcing all components and open boundary conditions were all adjusted during a 3 day 4d var cycle 3 step 2 was then repeated for grid g3 using the 4d var analyses from each cycle of g2 as the background open boundary conditions for each 4d var cycle of g3 in this case the 4d var window was reduced to 1 day and only the initial conditions and open boundary conditions were adjusted during each 4d var cycle also because of the considerable increase in computational expense 4d var was only run on g3 for the period 2014 2015 in combination steps 1 2 and 3 lead to corrections to the initial conditions every 3 days in the case of g1 and g2 and every day in the case of g3 in the case of g1 and g2 the surface fluxes are continuously adjusted during the 3 day assimilation cycle while on g3 there are no corrections made to the surface forcing on all three grids the open boundary conditions undergo continuous adjustments clearly each child grid benefits from the 4d var estimate from the parent grid only at the child grid open boundaries therefore each grid receives information from the observations only once except for interior observation influences on the open boundary conditions the background initial conditions for the first 4d var cycle on 1 jan 2014 on g1 were taken from a previously computed 4d var reanalysis spanning the period 2007 2013 see wilkin et al 2018 the background initial conditions for the first cycle on 1 jan 2014 on g2 and g3 were linearly interpolated from g1 it is normal procedure to combine multiple observations of the same type that fall within a single grid cell and that are closely spaced in time into super observations super observations were computed where appropriate separately for each of the three grids see table 1 therefore given the difference in horizontal resolution of each grid the observations assimilated into the model in each case within the overlapping region were not the same fig 2 shows time series of the total number of observations assimilated into the model on each grid during a 4d var cycle despite the changing size of the grid and the shorter 4d var window length of g3 the total number of observations assimilated on each grid during each cycle is similar even though the grid resolution changes going from g1 to g2 to g3 because less super obing is required on the higher resolution grids compared to g1 fig 2 information can be readily converted to logarithm of the observation density by subtracting log10 of the number of grid points which is 6 01 6 25 and 6 45 for g1 though g3 respectively this indicates that there is typically one observation per 10 grid cells in a 3 day cycle fig 2 also shows time series of the number of observations assimilated from each observing platform apart from satellite altimetry the number of observations from each platform is similar across all three grids on the other hand for altimetry there is an order of magnitude reduction in the number of observations going from one grid to the next due to the spatial separation of the altimeter ground tracks indeed fig 2f shows that during some 4d var cycles no altimeter tracks crossed g3 as described in moore et al 2011a the 4d var background error covariance b matrix was modeled following the diffusion operator approach of weaver and courtier 2001 the decorrelation length scales assumed in b for errors in each control variable are listed in table 2 and these parameter choices are discussed in l19 all components of the surface fluxes were included in the control vector both components of surface wind stress the total surface heat flux and the total surface freshwater flux while formally the surface flux corrections should be computed every model time step this is not practical so the corrections were calculated every hour and linearly interpolated in time the standard deviations for the background surface errors were estimated from a multi year run of the model without data assimilation the observation error covariance matrix r was assumed to be a diagonal matrix and the observation errors are also summarized in table 1 and discussed in l19 quality control was also performed during each 4d var cycle following andersson and järvinen 1999 as described by moore et al 2013 specifically the innovation d i associated with each observation is compared to the standard error based on the assumed standard deviations of the background σ b and observation σ o errors in particular if d i 2 α 2 σ b 2 σ o 2 then the observation is rejected and not included in the analysis the threshold parameter α is dependent on the type of observation and is given in table 2 for the analyses on each grid considered here a time series of the total number of observations rejected during each 3 day 4d var cycle is shown in fig 2 and is typically o 103 104 indicating that only 1 of the total number of observations were rejected based on the chosen criteria the performance of the 4d var system on g1 is described in detail by levin et al 2018 wilkin et al 2018 and l19 fig 3 shows probability density functions pdfs for the innovations associated with observations of sea surface temperature sst ssh in situ temperature and in situ salinity for each grid in principle if b and r are correctly prescribed the innovations d should be normally distributed with a covariance given by hb h t r therefore for reference fig 3 also shows the pdfs for normal distributions with the same mean and standard deviation as the innovations computed during the 1st outer loop clearly for all observation types on all three grids the innovation pdfs depart significantly from the expected normal distributions and are more reminiscent of a laplacian distribution for the most part the mean innovations for temperature and salinity are close to zero for all three grids the mean ssh innovations however are negative on all three grids indicating that on average the mean model ssh exceeds that of the observations for salinity while the mean innovations are close to zero there is an overall tendency for the model to favor negative innovations in all three grids as evidenced by the skewed nature of the pdfs the innovation pdfs for the 2nd outer loop are qualitatively similar to those for the 1st outer loop not shown the fit of the 4d var analyses to the observations is presented in fig 4 which shows time series of ratio of the final and initial values of j o h δ x d t r 1 h δ x d the contribution of the observations to the incremental 4d var cost function for all temperature observations sst and in situ and the observations of the zonal component of velocity u fig 4 indicates that the largest reduction in j o occurs during the 1st outer loop this is the case for all data types assimilated not shown in addition fig 4 shows that while the fractional reduction in j o associated with observations of temperature is similar in all three grids the decrease in j o for u increases with increasing resolution indicating that the model is able to capture the sub mesoscale variability in ocean currents more effectively this is discussed in detail in part ii the surface forcing increments are generally small for both the g1 and g2 analyses and over most of the domain are just 1 2 of the seasonal standard deviation of the background fluxes 4 circulation indexes as noted in section 1 a dominant feature of the circulation in the mab is the front that separates the warm salty waters of the gulf stream from the cooler and fresher waters of the continental shelf significant excursions of the frontal location occur in association with numerous cross shelf exchange processes processes known to contribute significantly to cross shelf transport include frontal instabilities meandering and eddy fluxes wind forcing saline intrusions within the pycnocline vertical mixing upwelling within the bottom boundary layer and gulf stream ring interactions with the shelf gawarkiewicz et al 2018 an example of the latter is illustrated in fig 1 which shows the 4d var analyses of sea surface salinity sss on 16 may 2014 on all three grids a streamer of saline water associated with a large gulf stream ring can be seen impinging on the shelf this particular event has been studied in detail by zhang and gawarkiewicz 2015 and is captured well in the roms 4d var analyses on all three grids fig 1 shows very clearly how the 4d var circulation estimates can capture sub mesoscale secondary circulations as the grid resolution increases the observation impact indexes i considered here were chosen to target the position of the mab front and quantify the magnitude of cross shelf exchange fluxes particularly concerning the ooi pioneer array 4 1 frontal location the mab front has traditionally been associated with the position of the 34 5 isohaline and the point where this isohaline intersects the bathymetry is often used as a proxy for the foot of the front beardsley et al 1985 linder and gawarkiewicz 1998 onshore excursions of the front foot are associated with upwelling favorable conditions castelao et al 2008 where offshore ekman transport is balanced by onshore flow near the bottom lentz et al 2003 and such events are thought to be an important factor in the supply of nutrients to the continental shelf siedlecki et al 2011 following the generally accepted aforementioned definition an index was used that quantifies the change in the average front location based on the area between the position of the foot of the front in the background estimate x b and the front foot position in the analysis x a specifically 3 i f ξ 1 ξ 2 η ξ η r ξ d ξ where ξ η represent the local cartesian coordinates of the 4d var cycle average position of the front foot and η r ξ is a reference line thus i f represents the total area between the front foot location and the reference line the integral in 3 was performed along the benthic isoline that defines the front foot as it crosses the pioneer array operations domain which represents the endpoints ξ 1 and ξ 2 since the front is a dynamic feature ξ 1 and ξ 2 vary from one assimilation cycle to the next the reference line chosen for η r ξ is the seasonally varying climatological position of the front foot although the location of the reference line is unimportant since the index increment is given by 4 δ i f ξ 1 ξ 2 η a ξ η b ξ d ξ where superscripts a and b refer to the analysis and background respectively this index differs fundamentally from the general case considered in section 2 in that 4 is not an explicit function of the state vector x it is therefore necessary to linearize η a ξ about the background frontal location η b ξ in order to apply the adjoint based approach described in section 2 if r b ξ b i η b j represents the position vector of the coordinate pairs that define the position of the isohaline foot in the background then it is easy to show that to 1st order the difference between the position vector of the isohaline in the analysis and the background δ r in the direction of the background salinity gradient s b is given by δ r δ s s b s b 2 where δ s s a r b 34 5 and s a r b is the salinity of the analysis evaluated at the position of the foot of the background 34 5 isohaline 1 1 we chose to identify the displacement vector δ r in the direction of the gradient s b such that δ s s a r b 34 5 δ r s b thus we require δ r α n where n s b 1 s b is the local unit vector parallel to the background gradient and α is a scalar therefore δ s δ r s b α s b 1 s b s b α s b in which case α δ s s b 1 thus δ r α n δ s s b 2 s b thus a 1st order approximation of the position of the foot of the front in the analysis is r a ξ a i η a j r b δ r in this way the area δ i f i f r a i f r b can be expressed as a function of δ r δ s s b s b 2 which itself is a function of the background state vector x b as required by 2 to illustrate fig 5a shows a time series of front foot index i f computed from the background circulation x b on grid g1 a 30 day running mean was applied to highlight more clearly the seasonal and interannual variations in i f fig 5a indicates that the 4d var analysis tends to favor movements of the front foot onshore i f 0 in contrast offshore movements i f 0 are typically smaller furthermore while there is significant variability in i f there are no obvious interannual variations in the seasonal cycle time series of i f on g2 and g3 are qualitatively and quantitatively similar to that shown in fig 5a for g1 not shown the mean and standard deviation of i f on each grid is summarized in table 3 indicating that the front foot statistics as measured by this index are similar across all three grids time series of the foot front index increments δ i f that arise from assimilating the observations are shown in fig 5b for both the 1st and 2nd outer loops again there are no noticeable interannual variations in the seasonal cycle of the increment time series which are characterized instead by irregular movements of the front onshore and offshore in response to 4d var corrections to the circulation during the 1st outer loop the increments δ i f 0 1 i f while during the 2nd outer loop the δ i f are generally smaller the mean and standard deviation of δ i f during both outer loops and for the 4d var analyses on all three grids are presented in table 4 on g1 the mean δ i f are positive indicative of a tendency for 4d var to correct for a mean offshore bias in the front foot location of the background table 4 shows that this bias is significantly reduced on g2 and changes sign on g3 but is close to zero 4 2 frontal stratification as a measure of the level of stratification associated with the front we follow the work of simpson and bowers 1981 who studied fronts in the north sea in terms of the potential energy required to thoroughly mix the upper part of water column specifically we consider a index of stratification given by 5 i e v 1 g d ζ ρ ρ z d z d a where ρ and ρ are respectively the in situ and vertically averaged density both averaged over the assimilation window d is a chosen depth ζ is the free surface displacement and the area integral is performed over the pioneer array glider domain shown in fig 1c the depth d was chosen to be the average depth of the front foot across the pioneer array glider domain in 5 v represents the volume encompassed by the integrals with the result that i e is the energy per unit volume j m 3 that is required to completely mix the upper d meters of the water column within the glider domain fig 5c shows a time series of i e computed from the 4d var analyses of g1 the seasonal cycle is associated with low values of i e during the winter when the upper water column is fairly well mixed and high values of i e during the summer after the water column has re stratified vertical sections of salinity during a typical minimum in i e on 3 march 2016 and a typical maximum in i e on 30 august 2016 are shown in fig 6a and b respectively during march the mab front is well defined within the pioneer glider domain the depth d over which the potential energy i e in 5 is computed as the average depth of the front foot over the glider domain and the intersection of the 34 5 isohaline with the bathymetry which defines the front foot cf section 4 1 is clearly visible in fig 6a at a depth of around 75 m above this depth the water column is well mixed over much of the glider domain which accounts for the low value of i e at this time of year while the depth of the intersection of the 34 5 isohaline with the bathymetry is similar during august fig 6b the water column is strongly stratified over much of this depth within the glider domain which accounts for the high value of i e during this time thus i e can be a useful indicator of the strength of the front in terms of the mean stratification within the glider domain where low values of i e correspond to situations where the front is well defined within the pioneer target area and vice versa for high values of i e stratification is an important factor in this region since it also influences shelf slope exchange via the development of instabilities e g houghton et al 1988 onshore intrusions of saline waters from over the continental slope lentz 2003 and the efficiency of vertical mixing time series of i e from g2 and g3 are both qualitatively and quantitatively similar to that shown in fig 5c not shown the mean and standard deviation of i e on all three grids is shown in table 3 and confirm that they vary within a similar range time series of the increments δ i e arising from 4d var are shown in fig 5d for grid g1 for both outer loops during the 1st outer loop δ i e is generally negative for much of the time indicating that 4d var is reducing the stratification and potentially strengthening the mab front in the pioneer target region the increments during the 2nd outer loop are typically smaller however during some periods they partially offset those of the 1st outer loop indicating that during some cycles data assimilation reduces the stratification too much during the 1st outer loop and some re stratification is necessary during the 2nd outer loop so that the circulation is more consistent with observations the mean and standard deviation of the increments δ i e are presented in table 4 for all three grids a negative bias is apparent on g1 suggesting that in this case 4d var is largely correcting for bias in the stratification table 4 indicates that the bias is much reduced on g2 and is close to zero on g3 suggesting that in both cases the stratification is more consistent with the observations 4 3 transport indexes as noted earlier there is also considerable variability in the cross shelf exchange of water masses therefore a series of indexes were also computed to quantify the impact of the observations on the 4d var estimates of conditions at the shelf break in the vicinity of the ooi pioneer array specifically we consider the following indexes 6 i u s 1 s 2 h 0 u n u n d z d s 7 i u t ρ o c p a 1 s 1 s 2 h 0 u n u n t t d z d s 8 i u s 1 0 3 ρ o a 1 s 1 s 2 h 0 u n u n s s d z d s 9 i t a 1 s 1 s 2 h 0 t t d z d s 10 i s a 1 s 1 s 2 h 0 s s d z d s in each case s 1 s s d s represents an integral along a section of the h 200 m isobath nominally identified as the location of the continental shelf break the vertical section chosen is indicated in each panel of fig 1 and cuts through the middle of the pioneer array in 6 10 u n corresponds to the component of the velocity that is locally normal to the section s and an over bar denotes the time average over each assimilation cycle the tilde represents the mean seasonal cycle and a is the area of the cross section therefore i u i u t and i u s are measures of the departures from the mean seasonal cycle of the 4d var cycle average total volume transport heat transport and salt transport across the shelf the indexes i t and i s are a measure of the departures from the mean seasonal cycle of the 4d var cycle average temperature and salinity along the section they are used as additional diagnostics on g1 only fig 7a e show time series of each index computed from the analysis state vector x a in the case of the transport indexes i u i u t and i u s positive negative values represent onshore offshore transports relative to the mean seasonal cycle also shown in fig 7a e are time series of the same indexes computed from a one way nested run of the model without data assimilation subject to the same prior atmospheric conditions on all grids and the same mercator océan open boundary conditions on g1 fig 7a c show that there is considerable variability on a range of time scales in the cross shelf transports also the transports are significantly modified by data assimilation a closer inspection of the time series shows that periods of significant heat and salt transport are a combination of both the volume transport and changes in the mean temperature and salinity along the target section time series of i u i u t and i u s for g2 and g3 not shown are comparable to those shown in fig 7 for g1 table 3 summarizes the mean and standard deviations of the transport indexes on the three grids for i u t and i u s the mean and standard deviations are similar across all three grids although for i u there is an apparent onshore volume transport bias on g2 and g3 the volume transport increments δ i u of fig 7f are generally small compared to i u indicating that data assimilation is not making large corrections to the circulation during each 4d var analysis this is desirable behavior and suggests that the model is not subject to large adjustments and is mostly consistent with the new observations that are being assimilated on the other hand the heat and salt transport index increments δ i u t and δ i u s of fig 7g and h are a more significant fraction of i u t and i u s and are reflective of the changes in temperature and salinity increments across the target section cf fig 7i and j in all cases the increments during the 1st outer loop are typically larger than during the 2nd outer loop it is also noteworthy that the increments in the transport index time series exhibit fluctuations on time scales similar to that of the model run without data assimilation which suggests that 4d var is correcting changes in the circulation that are associated with the dynamic intrinsic variability on time scales longer than the 3 day assimilation windows the mean and standard deviation of the increments in each index are summarized in table 4 for both outer loops on all three grids on g1 the mean volume transport increments are offshore but close to zero on g2 and g3 conversely the mean heat and salt transport increments are onshore on all grids and decrease with increasing resolution 5 observation impacts since two outer loops are employed in the 4d var analyses it is necessary to compute the observation impacts separately for each outer loop if x n a denotes the 4d var analysis at the end of the n th outer loop then the observation impacts are quantified according to δ i 1 y o h x b t k 1 t i x x b and δ i 2 y o h x 1 a t k 2 t i x x 1 a where δ i 1 and δ i 2 represent the increment in the index i at the end of the 1st and 2nd outer loop respectively k 1 t and k 2 t are the reduced dimension kalman gain matrices for each outer loop and i x x b and i x x 1 a represent the derivatives of the index i evaluated using x b and x 1 a since x 1 a depends on the observation values δ i 2 cannot be unambiguously decomposed into the contributions from each observation however as discussed by trémolet 2008 since δ i 1 δ i 2 much of the impact of the observations on the final 4d var analysis can be attributed to the 1st outer loop this is also found to be the case here as confirmed in figs 5 and 7 which show time series of δ i 1 and δ i 2 for each of the target indexes on g1 similarly the standard deviations in table 4 confirm that the δ i 1 δ i 2 on g2 and g3 also therefore in the sequel we will consider only the observation impacts during the 1st outer loop it should also be noted that with additional computational effort δ i 1 can be decomposed into the contributions from the different components of the control vector however preliminary analyses of δ i u not shown revealed that 99 of the increment in volume transport is associated with the increment in the initial conditions therefore in the sequel we have considered the total impact arising collectively from all elements of the control vector the impact of the observations on each index was quantified according to 2 which represents a 1st order linearization of the index increment arising from data assimilation eq 2 shows that an important ingredient of these calculations is i x x b since 4 7 and 8 represent nonlinear indexes computation of this first derivative is an additional linear approximation in the procedure therefore before proceeding to compute the observation impacts it is essential to test the veracity of the linear assumptions in 2 to this end table 4 shows the correlation coefficient r between the time series of the increments δ i in each index computed from 2 and those calculated directly from the analysis and background estimates of i in most cases r exceeds 0 9 and in several instances is very close to 1 the lowest correlations are associated with i f on g2 and g3 0 86 and 0 75 respectively and with i e on g3 0 77 nonetheless these correlations are still respectable and confirm that the linear approximations employed will yield reliable estimates of the observation impacts in these cases also the contribution aka impact of each observing system to the increments δ i of a chosen index will vary from cycle to cycle and depends on several factors including the number and distribution of the observations the time evolution of the background circulation x b t and the hypotheses about the background and observation errors described by b and r to illustrate fig 8 shows time series from g2 of the impact of each type of observation on the 1st outer loop increments in each of the indexes considered here there are several noteworthy features of fig 8 firstly the relative impact of the various observing systems on a given δ i changes through time and it is not always the same type of observations that have the largest impact secondly there is generally a great deal of consensus between the impact of observations from different platforms in that they usually have the same sign at any given time however there are a few periods where the impacts from different platforms are in opposition thirdly the relative impact of each observation type during a particular time interval varies from index to index in the g2 example shown it is observations of temperature remotely sensed and in situ and velocity hf radar and in situ that exert the greatest control on all of the indexes the impact of salinity observations is generally small on this grid and the impacts of altimetry are generally negligible due to the limited size of the domain cf fig 1b and the low number of satellite overpasses in the following sub sections the observation impact information encapsulated in time series such as fig 8 will be examined in different ways 5 1 impact vs innovation a useful diagnostic of the performance of the 4d var system is the impact of each observation compared to the corresponding innovation i e the difference between the observation and the background sampled at the observation point to this end fig 9 shows scatter plots of the impact of each observation on the foot front index i f versus the innovation for each type of observation on all three grids each scatter plot can also be viewed as a contingency diagram and has the format of a 2 dimensional histogram showing the density of points that fall within each quadrant the percentage of the total number of points that fall within each quadrant is also indicated most of the scatter plots in fig 9 resemble butterfly wings in that observations associated with a small innovation i e instances where the model and observations are in excellent agreement also have little impact on i f as the innovation increases the range of impact that observations have on i f becomes larger as reflected by the wing structure furthermore observations that are associated with very large innovations i e instances where the model and observations are in poor agreement generally have a small impact on i f as noted in l19 this is a desirable feature of the 4d var system because very large innovations very likely represent cases of observations that have passed the quality control threshold through say the coincidence of a poor background solution at a bad observation location and we would not want these data to adversely impact the analysis however an inspection of the scatter plots also reveals some notable biases in the impacts innovations or both for example while the four quadrants of the scatter plot for ssh observations on g1 are fairly evenly populated fig 9a the corresponding scatter plots for g2 fig 9f and g3 fig 9k display a significant bias towards negative innovations in agreement with fig 3b and c another interesting feature of fig 9g and l is that the scatter plots for sst observations on g2 and g3 exhibit a banded structure in one quadrant in the case of g2 fig 9g many of the sst observations associated with positive innovations i e the model sst cooler than observed have a pronounced negative impact on the i f in that they tend to move the front further offshore conversely on g3 fig 9l sst observations associated with negative innovations i e the model sst warmer than observed impact the front foot location by moving it onshore further analysis reveals that these features are associated primarily with the avhrr and amsr and further investigation is warranted additional features of fig 9 that will be further discussed in the following sections include the general decline in the impact of the individual in situ temperature and salinity observations as grid resolution increases in contrast the impact of individual velocity measurements increases going from g1 to g3 cf fig 9e j and o scatter plots associated with the other indexes share many qualitative features in common with those of fig 9 not shown although other detailed features related to bias in the innovations bias in the impacts or bias in both are specific to different indexes other scatter plot examples for the transport index on g1 are presented and discussed in l19 5 2 rms impacts the observation impacts associated with each observation shown in the scatter plots of fig 9 can be parsed in various ways that highlight different aspects of the performance of the 4d var systems and the role played by different observing platforms in affecting circulation changes fig 10a c show the root mean square rms impact of each type of observation on i u t i f and i e for all three grids fig 10a is representative of the other transport indexes i u and i u s also not shown as discussed in l19 for g1 focusing first on remote sensing observations for g1 the sst and ssh observations collectively have a similar impact even though there are two orders of magnitude fewer observations from altimetry fig 2d for g2 the collective impact of altimetry decreases with increasing grid resolution this occurs for two reasons first as shown in l19 altimeter observations that are remote from the target section can have a significant impact on each index which accounts for some of the high impact of ssh on g1 in addition however as fig 2 shows the number of altimeter observations decreases considerably going from g1 to g3 because of the reduced geographical extent of each grid with increasing resolution for sst the impacts are higher on g2 than on g1 which is associated with a relatively large impact of these observations in the vicinity of the target section region that defines each index see also l19 in connection with g1 as fig 10a c show these local impacts carry over from g1 to g2 and g3 before discussing the impact of in situ observations we reiterate that the observation impacts depend on several factors including a the background circulation x b which of course is highly resolution dependent across the three grids b the background error covariance b and c the observation error covariance r the parameters used to compute b and r were not the same across all three grids since different error statistics are appropriate for each grid thus some of the changes in the relative impact of various components of the observing system on the three grids will depend on unavoidable variations in the error covariances it is important to remember that r is dominated by errors of representativeness which are difficult to estimate a priori for in situ temperature observations the standard deviations assumed for r are similar across all three grids and range from 0 6 c on g1 to 0 4 c on g2 and g3 however a posteriori analysis of the innovation statistics as described by desroziers et al 2005 suggests that these standard deviations should be closer to 1 c for in situ salinity observations the a priori observation errors were assumed to 0 2 on g1 while the a posteriori innovation statistics indicate that a more appropriate choice is 0 4 which is the value used for both g2 and g3 similarly for velocity measurements the standard deviation of the observation error on g1 was assumed to be 0 6 m s 1 for hf radar surface current estimates and 0 3 m s 1 for moorings these values were adjusted downwards to 0 1 m s 1 for hf radar observations and 0 04 m s 1 for moorings for both g2 and g3 and are more in line with the a posteriori innovation statistics while we would ideally like to compare cases where say only the model resolution is varying in the 4d var analyses across the three grids the high computational expense of these calculations precludes running a more detailed and controlled suite of experiments so we must draw on what we have nevertheless variations in the level of errors across the different grids provide an indication of their control on the impacts returning to fig 10 an obvious feature of fig 10a c is that in situ observations have the largest impact on g1 for all three indexes even though the number of in situ observations is an order magnitude less than the number of satellite sst observations see fig 2d in situ temperature observations maintain a relatively high impact on g2 although there is a significant decline on g3 this is partly because of the substantial reduction in the volume of observations and the loss of some remote impacts but also because of the increasing influence of velocity observations on the sub mesoscale circulation that is resolved by g3 see part ii much of the dramatic decline in the impact of in situ salinity observations on g2 and g3 when compared to g1 is most likely associated with the difference in the assumed level of observation error on g1 the level of observation error is probably too low so the 4d var analyses are drawing more heavily on these data than on g2 and g3 however there are other dynamical controls as well associated with geostrophic adjustment as discussed in part ii on g1 velocity observations have a relatively small impact on all indexes partly because of the high value assumed for the errors of representativeness but also because of dynamical controls see part ii the impact of velocity observations increases from g1 to g2 and then again from g2 to g3 mainly because current measurements from the pioneer array moorings play an increasingly greater role in shaping the sub mesoscale circulation as grid resolution increases this is discussed in more detail in part ii while fig 10a c represent the aggregate average impact of different observation types on the target indexes fig 2 shows that there is considerable disparity in the number of observations of each type that are assimilated into the model therefore it is informative to normalize the observation impacts by considering the rms impact per datum which is shown in fig 10d f note that super observations are considered as a single datum fig 10d f show that in situ observations have by far the largest impact per datum for all indexes and across all grids as discussed by l19 each ssh observation on g1 is 50 times more impactful than an individual sst observation this carries over to some extent to g2 as well although the factor decreases to 7 because the impact of ssh observations that are remote from the target sites is lost on g3 the impact per datum of ssh and sst is similar because there are so few ssh observations to impact the circulation estimates fig 10d f show that in general except for the case of velocity observations the observation impact per datum decreases with increasing resolution and decreasing domain size this is most likely a combination of two factors i as resolution increases the model can capture more faithfully the mesoscale and sub mesoscale circulation features and ii as shown in table 2 the decorrelation length assumed for the background errors decreases with increasing resolution so the radius of influence of each observation will be correspondingly smaller moving from g1 to g3 5 3 observation impact as an indicator of 4d var performance as discussed by trémolet 2008 the impact of each observation on the analysis or ensuing forecast can be tracked line by line through the data assimilation code this provides a powerful means for monitoring the performance of the 4d var system at various levels and different stages of the calculation in roms the observation impacts are evaluated during each inner loop iteration and provide a quantitative measure of how the observations are being utilized during the assimilation procedure to illustrate fig 11 shows the rms impact of each observation type on i u and i e on all grids during each inner loop of the two outer loops employed on g1 fig 11a and d confirm the dominant role played by in situ observations of t and s in controlling i u and i e for these data fig 11a reveals that for i u during the 1st outer loop the impacts asymptote to a near constant value after just three inner loops for i e fig 11d the impacts of in situ t exhibits similar behavior however for in situ s the impacts continue to trend upwards even after seven inner loops indicating that there is more useful information to be utilized from these data the aggregate impact of sst and ssh on g1 is similar for both indexes fig 11a and d consistent with fig 10a c and also show a continuing upward trend at the end of the 1st outer loop indicating that there is additional useful information that could be extracted from these data too fig 11 also confirms that the observation impacts during the 1st outer loop are larger than those during the 2nd outer loop the indexes i u t i u s and i f exhibit similar characteristics not shown but see also l19 on g2 satellite sst and velocity observations emerge to play a more dominant role as shown in fig 11b and e in this case altimetry plays a minor role and the impact in situ s has been largely relegated as noted in section 5 2 in the case of i e sst observations have the most impact for both i u and i e on g2 the impact of the dominant data types continues to exhibit an upward trend at the end of the 1st outer loop suggesting that the 4d var analyses on this grid may benefit from additional inner loops the indexes i u t i u s and i f exhibit similar characteristics not shown on g3 velocity observations emerge as generally the most impactful observations for i u fig 11c the impact of these data is also significant for i e fig 11f but velocity loses its poll position to sst observations after four inner loops for both i u and i e the continued upward march of the observation impacts at the end of the 1st outer loop indicates that additional inner loops could be beneficial on g3 also similarly for the indexes i u t i u s and i f not shown 6 remote sensing observation impacts the geographical distributions of the observation impacts associated with satellite observations are particularly revealing and display what we believe are the signature of the dynamical processes that are responsible for conveying information from the observations to the target sites that define the impact indexes i with this in mind fig 12 shows the rms impact per datum of all sst observations that fall within each model grid cell the cases shown are for i u t i f and i e on all three grids for g1 fig 12a c reveal the presence of large scale coherent patterns of impact for sst that are common to all three indexes these same patterns are present for i u and i u s also not shown and as discussed in l19 are associated with the underlying dynamics of the circulation and the structure of the inverse total error covariance matrix in observation space hb h t r 1 aka the inverse stabilized representer matrix which lies at the heart of the analysis equation 1 in particular fig 12a c show regions of elevated impact that are both local to the index target regions and remote such as the north wall of the gulf stream fig 12d f show that the geographical distributions of sst impacts on g2 for the three indexes shown also share common features the most apparent features are the high impacts extending upstream from the target areas that are associated with the equatorward flowing shelf break jet and the tongue of high impact associated with flow exiting the gulf of maine through the great south channel that defines the western edge of georges bank similar features are present on g1 also fig 12a c as in g1 it is likely that these features common to all of the indexes including i u and i u s not shown are controlled by the combined influences of the background circulation x b and prior assumptions assumed in the 4d var procedure via the inverse stabilized representer matrix hb h t r 1 in the case of g3 fig 12g i show that while there is some commonality in the geographic distribution of the sst impacts most conspicuously associated with the shelf break jet there are also some significant differences the differences are probably a reflection of the more complex nature of the flow of information through the g3 4d var analyses due to the intricacies of the sub mesoscale environment cf fig 1c this is a topic that warrants further exploration but as shown by l19 the analysis of the factors controlling the characteristic patterns of impact is rather involved the patchwork patterns apparent in fig 12g i are associated with variations in sst coverage of the different observing platforms for example wsat is a microwave instrument with a low resolution foot print that covers only part of the g3 domain the geographical distributions of the rms impacts of altimetry observations also display interesting and dynamically controlled patterns of local and remote influence as shown in detail by l19 for g1 similarly while not shown here ssh impacts on the g2 circulation indexes share some similarities with their g1 counterparts in the case of g3 the altimeter coverage during the 2014 15 period considered is fairly sparse so it not so easy to identify robust geographical distributions of impact in this case as discussed in l19 some aspects of the local and remote impacts apparent in fig 12 can be understood in terms of information horizons see also moore et al 2015 the distance over which information contained in the observations can travel via the processes of wave propagation and advection l19 estimate that during a typical 3 day assimilation cycle as employed in g1 and g2 the information horizon associated with horizontal advection is 25 km for the shelf break jet and 500 km for the gulf stream the information horizon associated with internal waves is estimated to 500 km also while information carried by barotropic waves can reach every point in the model domain 7 in situ observation impacts the impact of in situ observations on the 4d var analyses on g1 g2 and g3 is the subject of part ii with a particular focus on the nsf ooi pioneer array in this section we present a broad overview of the in situ observation impacts and the interested reader is encouraged to consult part ii for a more detailed account fig 10 shows that in situ observations of temperature and salinity have the largest impact on all indexes on g1 both on aggregate fig 10a c and in terms of the impact per datum fig 10d f the geographic distribution of the vertically integrated rms observation impact for in situ t and s combined is illustrated in fig 13 for the cross shelf heat transport index i u t as in the case of sst fig 12a the in situ observations exhibit impacts that are both local and remote from the target section in this case the influence of observations downstream along the shelf break current is a marked feature as is the upstream impact of the dense set of observations by gliders and hydrographic surveys on the scotian shelf while observations on the scotian shelf help constrain the modeled equatorward inflow from the north that is subsequently partitioned between entering the gulf of maine or following the shelf break south of georges bank towards the pioneer array site lopez et al 2020 the time scale of this transport far exceeds the 3 day analysis interval of the observation impacts this distant teleconnection is however well within scope for the influence of freely propagating coastal trapped waves ctw brunner et al 2019 calculate that the mode 1 free ctw at the pioneer site has a phase speed of some 7 m s 1 at this speed ctws originating on the scotia shelf in response to the data assimilation adjustments will traverse the 900 km to the pioneer array via the continental slope wave guide within 1 5 days as in the case of sst and ssh the geographic distribution of the in situ hydrographic observations is relatively robust across all metrics considered here not shown fig 10 also shows that while the aggregate impact of in situ hydrographic observations generally declines going from g1 to g3 the impact per datum of these data remains relatively high the geographic distribution of the observation impacts exhibits robust features on g2 and g3 across all indexes not shown and will be discussed in more detail in part ii the aggregate rms impact on i u t of in situ observations of t and s versus depth is illustrated in fig 14a c for all three grids for g1 fig 14a the impact of t and s is similar in the upper 400 m of the water column although below this depth the temperature observations dominate for g2 and g3 the impact of s is diminished compared to g1 consistent with figs 10 and 11 the vertical profiles of temperature impacts are similar across all three grids and in each case indicate the presence of elevated impact in the range 500 1000 m the rms impact per datum versus depth for t and s is shown fig 14d f which highlights the influence of subsurface temperature observations in the 500 1000 m depth range profiles of observation impacts for t and s for the other indexes display qualitatively similar features to those shown in fig 14 not shown the rms impacts of velocity observations versus depth are shown in fig 15 again for i u t although the main features are qualitatively similar for the other indexes not shown the aggregate impacts of fig 15a c indicate that velocity observations in the upper 100 m of the water column have the largest impact the impact also increases going from g1 to g3 in keeping with figs 10 and 11 the rms impact per datum on the other hand is relatively uniform below about 20 m fig 15d f while most of the surface velocity observations are from hf radar estimates the majority of subsurface measurements are from the pioneer array moorings which measure currents down to 75 m on the shelf and 400 m beyond the shelf break 8 summary and conclusions a state of the art 4 dimensional variational data assimilation system has been applied in a three level nested configuration of roms to compute estimates of the time evolving ocean circulation in the mid atlantic bight with a particular focus on the region served by the nsf ooi pioneer array the outer most model grid forms the basis of the near real time analysis forecast system that is currently run in support of the u s ioos maracoos regional association levin et al 2018 wilkin et al 2018 in which observations of the ocean from a broad range of remote sensing and in situ platforms are assimilated in the nested configuration considered here a wide range of circulation regimes are well represented spanning the western current the energetic mesoscale eddy field and the complex sub mesoscale circulation that is populated by ephemeral frontal features while these are all challenging circulation environments for any data assimilation system various diagnostic system indicators demonstrate that 4d var performs well across all three domains the primary goal of this study is to quantify the direct impact that observations from the various observing platforms that serve the maracoos region have on different aspects of the ocean circulation here the specific focus has been on the mab shelf break front and associated slope shelf exchange processes in the vicinity of the pioneer array since a goal of this component of the ooi is to explore the dynamics that control these processes with this in mind several indexes of the circulation were considered as quantitative indicators of different aspects of the dynamics in the vicinity of the shelf break front specifically we considered the location of the front the strength of the associated stratification and the cross shelf transport of mass heat and salt as one might expect significant differences exist between 4d var solutions and a one way nested free running model also 4d var leads to significant increments in the chosen circulation indexes on time scales that are similar to the intrinsic variability of the free model indicating that 4d var is not just merely making reactionary corrections to the ocean state in response to the model minus observation differences but is also informing the evolution of the circulation on a range of time scales in a dynamically consistent way in this study an adjoint approach similar to that used operationally in numerical weather prediction was used to quantify the impact of the observations on the 4d var increments in each chosen circulation index the observation impacts were found to vary considerably in space and time depending on the number type and spatial distribution of the observations the background circulation and the statistics assumed a priori for the errors in the background and observations however the geographic distribution of the observation impacts was found to be robust across all of the indexes considered and across the three domains unraveling the dynamics of the pathways by which a particular observation influences the ocean state in the near and far field is a complicated and involved process clearly there many moving parts in 2 used to compute the observation impacts l19 explored broadly the contributions and influence of model dynamics via h and h t and the error covariances b and r on the geographical distributions of the impacts while many features of fig 12 can be understood conceptually in terms of the information horizons associated with the fundamental processes of horizontal advection and wave propagation more detailed analysis is needed to identify the role of individual mechanisms it is useful to take a step back and remember what information the observation impact given by 2 provides eq 2 quantifies δ i given the observations available y o the prior hypothesis about errors in the observations via r errors in the background via b and hypotheses about the dynamics that control the ocean state via h furthermore the contribution of each observation to the dot product that defines δ i is unambiguous and is a reflection of all the assumptions and hypotheses that we have made the observation impact calculation will not however directly confirm or nullify these assumptions of hypotheses because if we change any aspect of the data assimilation system or the model then the circulation estimates will change and so too will the observations impacts however some aspects of the relative impacts of different observation types will obviously be robust since these are controlled by the dynamics and physics of the ocean nevertheless observation impact calculations like those presented here provide a quantitative measure of the relative value of observations from different observing platforms such information is of considerable value to decision makers since one could make a case for maintaining certain observing platforms based on the important or critical role they play in controlling some aspect of the state estimates and of course one could use the quantitative information that observation impact calculations provide to argue for increasing the coverage or level of redundancy of particularly impactful platforms our results show that there is generally a reasonable degree of consensus between the impacts of different observation types and observing platforms indicating that the 4d var system can make efficient use of complementary information from multiple sources on the other hand there is also considerable temporal variability in the relative impact of different observation types and as noted the impact of a particular kind of observation varies across the three domains as a result of changes in data density assumptions about error statistics and the change in the dynamical circulation regime see part ii for more analysis of the latter point the observation impacts are also a valuable tool for monitoring the efficacy of data streams and different components of an observing system for example significant changes in the impact of a particular data stream over time may be an indication of problems that are developing with the instrument or the data stream itself scatter diagrams like those in fig 9 can be used to identify outliers and anecdotally there have been instances in our own work where improvements were made to the quality control of some remotely sensed data that were identified as problematic in this way furthermore observation impact monitoring provides information about the performance of the 4d var system as in fig 11 clearly there are some observation types for which there is a continuing upward trend in the observation impact at the time that the 4d var calculations are terminated indicating that there is more useful information that can be mined from such data by further tuning of the 4d var system other calculations closely related to those presented here quantify the sensitivity of ocean state estimates to changes in the observation values or indeed the observing system by combining observation impact and observation sensitivity information the degree of synergy between different observing platforms can be quantified this idea was introduced in l19 and is developed further in part ii levin et al 2020 in which we explore in detail the role played by the pioneer array observing system in shaping the mab circulation estimates finally we note again that the observation impact methodology employed here can also be applied to the forecast problem in this case the extent to which each observation improves or degrades forecast skill as measured by a metric i can be quantified this type of analysis has been a mainstay in operational numerical weather prediction for some time and is now an important emerging activity in some near real time ocean analysis systems as well roms is at the forefront of these activities and is being used in this capacity and the results of ongoing forecast observation impact studies will be the subject of future publication credit authorship contribution statement julia levin conceptualization methodology writing original draft writing review editing software validation formal analysis investigation data curation hernan g arango methodology writing original draft writing review editing software bruce laughlin software elias hunter data curation john wilkin conceptualization methodology writing original draft writing review editing software validation formal analysis investigation resources project administration funding acquisition andrew m moore conceptualization methodology writing original draft writing review editing software validation formal analysis investigation resources project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by grants from the national science foundation united states oce 1459665 and oce 1459646 nasa united states nnx17ah58g and noaa united states na16nos0120020 pioneer array data were obtained from the nsf ocean observatories initiative data portal http ooinet oceanobservatories org 
23939,predicting ocean transport has many practical applications ranging from search and rescue operations to predicting the spread of oil debris and biogeochemical tracers yet trajectory prediction remains a challenge for existing ocean modeling techniques general circulation models require high resolution observational data in order to be properly initialized but these data do not exist for the ocean statistical models are difficult to tune with existing data and are often too simple to accurately encapsulate turbulent flows here we investigate a data driven approach to ocean transport prediction wherein the goal is to first learn from available data instead of prescribed laws of physics and then apply this information to new data more specifically we explore whether simple artificial neural networks anns are capable of learning to predict 2d particle trajectories using only previous velocity observations anns are trained in two ways first a so called one to one ann uses a particle s most recently observed velocity to predict its velocity six hours later and second a time series ann uses the past 24 hours worth of velocity observations to predict the next 24 h we present a proof of concept considering particles in a hierarchy of simulated flow regimes ranging from uniform steady flow to more complex scenarios with interacting scales of motion and then substantiate our approach on trajectories in modeled flows generated by a high resolution hybrid coordinate ocean model for a mesoscale eddy in the northern gulf of mexico we also assess ann sensitivity to the prediction window over which forecasts are made the number of training particles used and the size of the network anns successfully predict 24 h trajectories within the temporal bounds of the training data with forecast errors around half those of both rudimentary persistence and classical arima models predicting beyond the domain of the training data leads to forecast errors comparable to arima models our results suggest that anns offer promising potential as a data driven approach to forecasting material transport in the ocean keywords advection current velocity mass transport particle motion current prediction machine learning 1 introduction efforts to predict material transport in a turbulent ocean are motivated by many socioeconomic interests including oil spill response poje et al 2014 özgökmen et al 2016 search and rescue operations isaji et al 2006 and anticipating the spread of harmful algae blooms pollutants and marine debris enriquez et al 2010 olascoaga 2010 olascoaga and haller 2012 normile 2014 traditional ocean forecasting techniques merge theoretical knowledge of the underlying flow dynamics with information about the initial state from which the system evolves ground truth data are now widely available via real time ocean observing initiatives from both lagrangian and eulerian perspectives these data include direct in situ velocity measurements such as from current meters and drifters shay et al 1998 lumpkin et al 2017 novelli et al 2018 along with products derived from passive monitoring systems such as high frequency radar and satellite altimetry barrick et al 1974 1977 mcgoogan 1975 roarty et al 2010 despite decades of advances in observational and computational methods conventional theory driven ocean forecasting tools continue to be inhibited by the prevailing sparsity of ocean data özgökmen et al 2009 bolton and zanna 2019 one problem is that not enough data exist to properly initialize predictive models the highest resolution ocean general circulation models ogcms numerically compute eulerian velocity at o 1 0 9 spatial points at any given time özgökmen et al 2009 each of which requires unique initial conditions e g mariano et al 2011 wei et al 2016 still the ocean remains under sampled to such a degree that even coarse resolution ogcms are under constrained by available data especially outside of coastal regions while an ogcm may be initialized with low resolution climatology in the absence of observations this seldom provides suitable starting conditions for very localized problems such as particle prediction two examples illustrate the sensitivity of the particle prediction problem to accurate initial conditions first even in simple laminar flows two identical lagrangian drifters released simultaneously side by side eventually diverge and travel very different routes due to the unsteady time dependence of oceanic flow e g aref 1984 yang and liu 1997 özgökmen et al 2001 koshel and prants 2006 this chaotic particle behavior is notoriously difficult to predict because even slight differences in starting conditions yield vastly different forecasts a second example is the case of a drifter encountering two adjacent counter rotating eddies the systems of streamlines that encircle the centers of rotation form so called lagrangian coherent structures lcs that are known to control material transport peacock and haller 2013 olascoaga et al 2013 whether this drifter gets drawn into the inward flow of one eddy or repelled by the outward flow of the second eddy depends strongly on its launch position relative to the two eddies and to the singular streamline that divides them molcard et al 2006 small errors in the initial location of either the drifter or the critical streamline can again result in forecasts that are drastically different from actual observations a second challenge with particle prediction is the inability to adequately sample the complexity of ocean dynamics locating critical regions around lcss in nature is nearly impossible haza et al 2007 because observing them requires impractically high numbers i e millions of drifters ocean forcing mechanisms cause unique and independent dynamic features to exist across a wide range of scales stommel 1948 holland and lin 1975a b fully capturing these ocean dynamics requires that a minimum of o 1 0 23 spatial points be sampled simultaneously using a typical reynold s number r e 1 0 10 özgökmen et al 2009 even the finest ocean experiments which might sample thousands of spatial points do not come close to this requirement consequently the complexity of relevant steering dynamics on all scales of motion is neither fully understood nor accurately replicated in predictive ogcms of particular importance to material transport are localized submesoscale dynamics characterized by spatial scales 10 km and time scales on the order of hours to days these dynamics have been found to significantly impact particle trajectories in the ocean everywhere from the gulf of mexico e g poje et al 2014 berta et al 2016 to the beaufort sea mensa et al 2018 unlike mesoscale dynamics submesoscales are strongly influenced by vertical motions e g smith et al 2015 choi et al 2017 mensa et al 2018 surface convergence fronts are common at these scales and function dualistically as across front barriers to transport and as along front transport highways schroeder et al 2012 smith et al 2015 kuitenbrouwer et al 2018 taylor 2018 haza et al 2016 also identified submesoscale leakage pathways by which particles may escape larger mesoscale circulations while mesoscales are usually fully resolved by ogcms submesocale dynamics are often parameterized due to computational restrictions on model resolution even if partially resolved the modeled submesoscale dynamics are not always physically realistic because constraining them is difficult with available observations this ultimately leads to discrepancies between numerical simulations and real observations dueben and bauer 2018 stochastic or markovian models provide an alternative to calculating velocity at large numbers of grid points by considering flow fields statistically when used in conjunction with ogcms unresolved physics are added to the statistical mean flow as random increments zero th first or second order markovian models calculate successive positions velocities or accelerations respectively griffa 1996 provided the first oceanographic evaluation of stochastic models by using constant parameters corresponding to known time and length scales of the mean flow and showed that particle motion in the upper ocean can be closely simulated using first order markovian models with a finite turbulent velocity scale berloff and mcwilliams 2003 developed these further to better replicate interactive mesoscale ocean dynamics by randomizing the model parameters this allowed particles to also move between features as is commonly observed in nature berta et al 2016 haza et al 2016 haza et al 2012 later merged statistical subgrid models with mesoscale lcss computed from ogcms to better simulate particle behavior on submesoscales markovian models are among the cheapest trajectory models to run with everything parameterized trajectories can effectively be generated in a matter of seconds nevertheless it is often difficult to determine model parameters from available data and tuning the parameters once the form of the differential equations has been set can be tedious while targeted sampling of multi scale dynamics such as in the grand lagrangian deployment glad experiment in the northern gulf of mexico have provided new insights into ocean velocity statistics they have also highlighted performance shortcomings in existing statistical prediction methods beron vera and lacasce 2016 mariano et al 2016 to summarize while traditional theory driven ocean models are excellent tools for better understanding fluid dynamics and for exploring new phenomena they struggle as predictive tools because 1 the sparsity of observational data makes it impossible to accurately initialize high resolution ocean models 2 not enough data exist to observe complete ocean dynamics in nature nor to properly replicate them in ogcms and 3 important steering dynamics at local submesoscales are often poorly captured by ogcms due to computation restraints on model resolution initialization and tuning challenges also affect simpler statistical models that do not rely on solving primitive equations at millions of grid points in light of these challenges an alternative approach to ocean transport prediction may be one that starts with objective observations instead of prescribed laws of physics here we take initial steps of exploring this possibility in the field of physical oceanography by testing whether ocean particle trajectories contain inherent predictability that a data driven algorithm might discover our goal broadly speaking is to apply information learned from past data to predict future states data driven strategies such as this form the foundation of machine learning techniques that have become enormously popular for tackling complex problems in 21st century data science we now present the particle prediction problem more specifically within the framework of data driven machine learning 2 data driven machine learning for ocean prediction data driven modeling is built upon the rationale that cause and effect relationships are often discernible among relevant variables and observed outcomes solomatine et al 2008 for example one intuitively expects a relationship between wind speed and sea state data driven models are trained to identify statistical relationships between data sets and later use this learned information to make predictions about cases unseen during training although these models do not explicitly provide insight into the underlying physics of a problem they are useful in situations when knowing an outcome is more important than understanding the relevant physical mechanisms oil spill first responders for instance are more concerned about where oil is and where it is going than why it moves as it does data driven models can also be beneficial when the full physics of a problem are not known when the underlying physics are not easily modeled numerically or when large quantities of relevant data are available from which to learn solomatine et al 2008 considerable effort has been made over several decades in fluid dynamics to use data driven machine learning as alternatives to the computationally expensive task of solving the nonlinear navier stokes equations one increasingly popular method attempts to learn governing partial differential equations from large quantities of numerical model output or experimental data this approach starts with observations of one or more state variables and their time derivatives assembles a library of nonlinear features in the form of polynomial functions and finally identifies the relevant components in the feature library using weights obtained by a machine learning algorithm e g brunton et al 2016 ayed et al 2019 ouala et al 2019 the discovered equations are then integrated in time to forecast future system states other work has focused on learning flow dynamics from snapshot images e g lee and you 2017 mohan and gaitonde 2018 still others develop sophisticated black box machine or deep learning methods to either combine with or altogether bypass conventional models chen et al 2018 wikner et al 2020 these and similar studies in a rapidly growing body of literature boast promising potential for learning and predicting complex flow dynamics but most depend on having observations at resolutions and densities not yet available to the oceanographer an important aspiration of the present investigation is to replace the computationally expensive process of running circulation models with information learned entirely from available observations this requirement necessarily precludes methods that rely on high resolution model output for training perhaps the easiest instruments for the oceanographer to deploy in relation to this problem are lagrangian surface drifters that provide time series of position velocity and acceleration throughout a region of interest given the nature of the problem and the hypothetical data we chose an artificial neural network ann a biologically inspired data driven nonlinear regression model whose notable strength is its theoretical ability to fit any function neural networks have demonstrated promising potential in a variety of complex geophysical problems such as identifying climate feedback mechanisms gonzález et al 2015 weather forecasting dueben and bauer 2018 predicting climate patterns toms et al 2020 and hurricane track prediction moradi kordmahalleh et al 2016 in oceanography neural networks have been developed for locating spilled oil in synthetic aperture radar imagery kubat et al 1998 for increasing the efficiency of parameterization routines in numerical ocean models krasnopolsky et al 2002 for modeling nonlinear wind wave spectra tolman et al 2005 and for ocean eddy identification and tracking franz et al 2018 lguensat et al 2018 here we investigate whether simple anns can learn to use observed velocities of drifting lagrangian particles to predict future velocities our study differs from others in the oceanographic literature that explore machine learning tools in that we develop from scratch and test the simplest possible ann instead of using more sophisticated forms of neural networks from preexisting software packages our rationale behind using a simple feedforward ann is twofold first is the law of parsimony credited to the fourteenth century philosopher william of ockham and supported by modern probability theory which states that given two explanations for some phenomenon the simpler one is more likely to be correct in light of this and following the inspiration of influential pioneering work in neural networks e g elman 1993 we choose to start small and simple the second reason for choosing a simple feed forward network is more practical the decades old black box objection to neural networks is that their performance under the hood or within a chosen machine learning library is physically uninterpretable see e g touretzky and pomerleau 1989 open research questions within the machine learning field related to this include what exactly did the neural network learn and how is it solving this particular problem what do the weights signify what do the neuron activation behaviors mean and what are the patterns learned within the network while these questions are well beyond the scope of the present investigation answers would be of great interest to the oceanography community accustomed to extracting direct physical interpretations from theory driven models one can therefore argue that the more complex the machine learning architecture the more physically abstract its behavior is likely to be and the lower the likelihood that physical interpretability will ever be identified sometimes simpler is better the success of any data driven method fundamentally requires some degree of predictability among the desired output data e g massoud et al 2018 the predictability of ocean lagrangian trajectories has been extensively explored both numerically e g özgökmen et al 2000 rixen et al 2008 and experimentally e g özgökmen et al 2001 coelho et al 2015 özgökmen et al 2000 2001 use simple statistical models that closely resemble anns to show how trajectory prediction on the order of days to weeks can be improved by combining information from nearby lagrangian drifters with knowledge of the mean flow within a region of interest one then wonders whether lagrangian drifter data alone contain enough information from which to predict trajectories if so then an ann ought to be able to learn from lagrangian trajectories without any additional information about the underlying physics this study takes the first step toward answering this question by systematically exploring the inherent predictability of ocean trajectories using simple artificial neural networks a simple test of predictability of a time series is whether or not future states can be predicted given only information about previous states for example the sequence 2 4 6 8 10 x contains enough information for one to predict the value of x to be 12 similarly one can test whether particle trajectories contain inherent predictability by trying to predict where a particle will go next based on where it has already been this is the premise of all regression techniques and is the logical starting point for testing any new time series prediction scheme an early non oceanographic demonstration of this technique in an industrial robotic application was by payeur et al 1995 who designed neural networks to predict position velocity and acceleration of moving objects based on previous observations this work suggests that the sequential velocities of drifting ocean particles may be learned in a similar fashion we therefore pose several questions can a neural network be trained to predict a particle s future velocities using only its previous velocities as input if so how would its predictive skill compare to a more traditional autoregressive model if this predictive task proves challenging can the ann at least perform better than a rudimentary persistence model the first part of this study consists of a series of proof of concept control experiments involving particles in a hierarchy of simulated known flows deliberately engineering the flow field allows us to gradually turn up the complexity of the dynamics which inversely decreases the predictability of lagrangian trajectories in order to systematically determine where predictability collapses of the many tested flow fields three are presented here we then conduct a case study in physically based oceanic flow fields generated by the hybrid coordinate ocean model hycom for a mesoscale eddy in the gulf of mexico to assess how our approach might perform with observational data in a real ocean application two types of anns are developed one predicts a particle s velocity at some future time based only on its previous velocity and another uses as input one day s worth of observations anns are evaluated against predictions from both rudimentary persistence and autoregressive integrated moving average arima models we find that anns predict trajectories of particles not seen during training often with half the forecast error of arima we also consider ann sensitivity to the numbers of hidden layers hidden neurons and training particles and to the prediction window over which forecasts are made importantly our methodology is easily applied to field applications by replacing simulated particle velocities with those from lagrangian drifters anywhere in the ocean our results suggest that data driven anns may offer a missing supplement to traditional theory driven models by providing forecasting tools that learn entirely from objective ground truth observations 3 proof of concept 3 1 methods we first conduct a hierarchy of control experiments consisting of buoyant particles in simulated rotational coherent structures of increasing complexity 1 steady uniform and stationary eddy center of rotation is not moving within the domain 2 steady uniform and drifting eddy center of rotation moves in time and 3 drifting eddy with time dependent sinusoidal pulses of amplitude a and frequency ω added to the mean rotational flow each flow field is scaled according to a realistic mesoscale eddy in the gulf of mexico studied by haza et al 2016 as a known example of interacting mesoscale and submesoscale motions a description of the parameter space for each flow field follows 3 1 1 case 1 stationary mesoscale eddy if eulerian velocity u as a function of space x x y and time t is separated into a time independent mean circulation u and a time dependent fluctuating component u 1 u x t u x u x t then case 1 represents mean eddy flow only with no small scale turbulence that is u 0 and u x t u x velocity components for circular mean flow are given by 2a u x y 2 c o y y c r 2 exp x x c 2 y y c 2 r 2 2b v x y 2 c o x x c r 2 exp x x c 2 y y c 2 r 2 where x y are the zonal and meridional positions of the particle respectively x c y c is the center of the eddy taken as the cartesian origin for convenience r 70 103 m is the radius at which the flow velocity is a characteristic mean velocity u v 0 1 m s 1 and c o 2 6 104 m2 s 1 is a dimensional parameter that sets the mean velocity u v at radius r calculated using eq 2 with x y r and x c y c 0 and where c o 0 produces clockwise flow given the absence of temporal flow variance particles released in case 1 will travel around a perfect circle a representative particle trajectory is shown in fig 1a starting at the solid circle and ending at the 3 1 2 case 2 drifting mesoscale eddy case 2 is set up identically to case 1 except that the center of the eddy x c x c y c varies in time 3 x c m t where m 8 10 3 m s 1 to generate a steady southwesterly drift of approximately 0 7 km d 1 comparable to the observed eddy drift in haza et al 2016 over the course of 14 days particles in this second case traverse a 2d spiral pattern over time as seen in fig 1b 3 1 3 case 3 drifting mesoscale eddy with submesoscale noise case 3 introduces time dependent velocity fluctuations to the drifting eddy of case 2 in the form of periodic pulses with amplitude a 1 5 m s 1 frequency ω 2 π t and period t 4 h 4a u a sin ω t 2 a cos 1 5 ω t 4b v a cos ω t 2 a sin 1 5 ω t adding four unique pulse signals to the mean flow eq 4 simulates the combined signals of multiple coinciding forcing mechanisms that one expects in nature the resulting flow pattern fig 1c can be likened to a mesoscale eddy surrounded by smaller submesoscale circulations it is worth noting that in this simulation all particles remain trapped within the mesoscale circulation though this is seldom true in nature where scales are dynamically interconnected see haza et al 2016 a more realistic scenario of interacting scales of motion will be considered in the case study described in section 4 sensitivity studies have demonstrated that a minimum of o 200 400 drifters are necessary to measure and quantify mesoscale submesoscale interaction dynamics özgökmen et al 2011 2012 the strong dependence of local transport on submesoscales dictates that real ocean applications would ideally involve at least o 300 drifters deployed within o 1 10 km region we therefore chose to release 360 particles in each simulation this allows a neural network to be trained using 270 or 75 see section 3 1 4 of them within the threshold for capturing scale interactions we later consider the inevitable practical challenges of deploying such quantities of drifters by decreasing the number of particles to more operationally realistic quantities in the eddy case study in all three cases particles are evenly distributed around a circle of radius 70 103 m from the center of the eddy and released simultaneously they are advected for 14 days in cases 1 and 3 and for 29 days in case 2 in order to capture the spiral trajectory effect by making a complete revolution around the eddy the trajectories of these particles hereafter the real trajectories are determined by applying a fourth order runge kutta rk4 integrator dormand and prince 1980 to eq 2 with data sampling frequency δ s t we choose δ s t 3 h to be comparable to the frequency of the hycom output used in the case study described in section 4 velocity components u 1 v 1 at each time step are found by substituting new positions x 1 y 1 into eq 2 3 1 4 neural network configurations we now describe the setup of our artificial neural networks relative to the data and to the problem at hand specifics of the neural network architectures development and training process are detailed in appendix we engineer anns to predict a particle s future velocity at some time t o δ p t where subscript p indicates a prediction time window thus for every ann input v t o the target output is v t o δ p t and each v t o v t o δ p t pair constitutes a unique example the choice of δ p t is based more on engineering considerations than on physics if the time series is predictable the ann will learn whatever time interval it is trained with longer prediction time windows lead to flatter error curves over a forecast period but at decreased forecast resolution such decisions are highly problem specific but allow for many possibilities such as creating a system of anns with some making longer forecasts while others are trained to make shorter forecasts we choose δ p t 6 h for two reasons first having a different prediction window than the observational time step i e δ p t δ s t provides a better sense of how well the network learns the data instead of simply predicting the next time step the network must predict two steps into the future the second reason relates to an error propagation problem when making forecasts beyond δ p t in this configuration a 24 h forecast would be made recursively starting with the most recent velocity observations followed by the successive outputs 5 h 24 h x h 18 h h 12 h h 6 h v 0 h where h x represents the ann s hypothesis for the next velocities we seek to minimize this error propagation while still maintaining a prediction window that is small enough to be practical in applications such as oil spill forecasting we refer to anns configured in this manner as one to one anns because one observation is used to predict one future state if velocity v is decomposed into zonal and meridional components u v then each observation becomes a vector containing two attributes each attribute is assigned to an input and an output neuron as shown in fig 2 network weights lines in fig 2 are initiated randomly from a univariate gaussian distribution of mean 0 and variance 1 due to the time dependence and high variability of ocean dynamics we combine a floating window approach to training first proposed by kubat 1989 with a continuous or so called online learning paradigm as follows of the 360 particles in each experiment 75 or 270 are used for training and the remaining 25 are reserved for testing the size of the training data set is fixed to include only the most recent 24 hours worth of observations with 270 training particles and 24 3 8 observations day this amounts to 2160 training examples at any given time observations are weighed at each training step according to how recent they are by assigning to the oldest 12 hours worth of observations a weight of 1 to the next 6 h a weight of 2 and to the most recent 6 h a weight of 3 where the weight indicates how many times the example is duplicated in the training set weighing the observations introduces repetition to the training process and increases the number of training examples to 3780 the network is initially trained for 100 epochs using the observations from day 1 where one epoch indicates one round through every training example subsequent observations from each particle are then added to the training set one time step δ s t at a time to mimic for example a lagrangian drifter reporting its position every n hours old examples are discarded as new data become available and training continues for another 100 epochs each time the training set is updated this happens a total of 104 13 d 8 obs d times for cases 1 and 3 224 times for case 2 thus by the end of the simulation the anns have been trained for a total of either 105 000 or 225 000 epochs such extensive training greatly increases the risk of overfitting to combat this we recognize that each training epoch essentially produces a new version of the ann we therefore select the best model out of 100 at the end of every training session see appendix this allows for three possible scenarios after updating the training set throughout the simulation 1 if the model benefited from all additional training the final version of the model after epoch 100 is retained 2 if training the model for an additional 100 epochs causes the model to overfit such that it performed better before the additional training the original version of the model is retained 3 if the model benefited from some additional training but began to overfit during the training session the version of the model just before overfitting began is retained using an ann with the fewest possible hidden neurons also aids against overfitting since the regression flexibility of an ann is proportional to the number of hidden neurons in addition to the one to one networks we also develop time series anns that take as input a time series of data rather than a single observation this better reflects the physical reality that a particle s next location can often be anticipated from its most recent n positions or velocities under this configuration each particle constitutes an individual training example containing 16 attributes 8 observations per day 2 velocity components this results in a tenfold decrease in the number of training examples available i e from 2160 to 270 at any given time the time series is not weighted for these anns because doing so would increase the number of attributes per example rather than the number of examples thus it does not introduce the same repetition as it does with the one to one networks instead it would require an unnecessarily larger network with more input and hidden neurons while extended forecasts can be made recursively with one to one anns using eq 5 this is not so straightforward with time series anns when δ p t δ s t these anns are engineered to expect the input vector to contain observations with time step δ s t if after training a vector containing a different time step is fed through the network the output will be nonsensical thus in the case where δ p t δ s t if the ann predicts a single future time step v t o δ p t then one would need to interpolate between the predicted velocity and the most recent input velocity in order to update the input time series and feed it back to the ann even more problematically if δ p t δ s t and the ann predicts only one future time step extended forecasts are altogether impossible without extrapolation a solution to this problem is to engineer the time series ann to predict multiple time steps simultaneously where both δ p t and the number of time steps to predict are determined by the specific problem we therefore engineer our time series anns to issue a 24 hr forecast as a set of velocities at t o i δ p t where δ p t 6 h and i 1 2 3 4 as illustrated in fig 3 the learning rate η an increment that controls network weight adjustment during training see appendix is set to 10 1 for one to one and time series anns in cases 1 and 2 trial and error indicated that the complexity of case 3 requires slower learning so η 1 0 2 for both network configurations all anns contain a single hidden layer for simplicity for the one to one anns where each example is composed of two attributes we use 10 hidden neurons while for the time series anns we use 20 hidden neurons due to the larger number of attributes per example because anns are initialized randomly a network may occasionally learn faster or slower depending on where in solution space the initial weights are located we account for this by randomly initializing an ensemble of three unique anns of both configurations for each experiment thus a total of 18 different anns are trained and tested during the three proof of concept simulations all particles are assumed to be released at 00 00 on day 1 as the particles circle the eddy 24 h forecasts for all 90 test particles are issued every midnight starting on day 2 using the most recent version of the network hereafter we refer to the times at which forecasts are initiated as forecast times and the times for which velocities are predicted i e 06 00 12 00 18 00 and 24 00 as prediction times thus each trial produces a total of 13 forecasts in cases 1 and 3 and 28 forecasts in case 2 where each forecast includes four predictions for 90 particles 3 1 5 assessment metrics the most fundamental assessment metric for any prediction model is rudimentary persistence a mere continuation in both space and time of the most recent observation it is commonly used because of its simplicity but it also sets the lowest possible standard because ocean particle prediction is a notoriously difficult problem it is logical to set low expectations at least at first thus we first compare ann prediction errors to those of persistence predictions in each experiment for a far more rigorous metric we employ statistical autoregressive integrated moving average arima models that make predictions based on linear combinations of time series lags and the lagged forecast errors these models contain three parameters that must be tuned for any given time series first the autoregression parameter p indicates the number of lags to be included in the model and is determined by the correlations between an observation and each previous observation an integrated or differencing technique subtracts an observation from the previous observation in an attempt at making the time series stationary a parameter d specifies how many times this differencing is performed finally the moving average parameter q specifies the number of lags to be used in a moving window average that is compared to the forecast residual error in most time series applications the optimal combinations of p d and q are determined by brute force trial and error here however with o 1 0 4 time series to fit this is hardly feasible we instead use a parameter search function auto arima in r from forecast package r 3 4 2 using default arguments to fit a unique arima model to normalized u and v time series for each of the 360 particles like with the anns this is done every midnight and 24 h arima predictions are made for each time series the time series are allowed to grow continuously such that at the beginning of the simulation the arima models are crudely fit on only one days worth of observations while the models on the last day are fit on the full time series let the error between predicted velocity u p v p and the real target values u r v r for all i prediction times 06 12 18 24 h j test particles k forecasts issued throughout the simulation and l ann ensemble members per experiment be defined as 6 e i j k l u i j k l r u i j k l p 2 v i j k l r v i j k l p 2 1 2 averaging over j k and l quantifies the 24 h forecast errors for the entire experiment 7 e i 1 p f m j 1 p k 1 f l 1 m e j k l where p 90 particles f 13 for cases 1 and 3 and f 28 for case 2 and m 3 ensemble members average forecast errors are calculated between the ann predictions and those from both persistence and arima models 3 2 results proof of concept predicted trajectories are generated by splitting successive predicted velocities into 30 min increments using linear interpolation in order to minimize rk4 time step integration errors ann performance for cases 1 2 and 3 are summarized in figs 4 5 and 6 respectively a representative test particle trajectory during the last two days of the simulation is shown in panel a by a solid gray line with forecast times marked by circles the dotted blue and dashed green lines illustrate the trajectories predicted by the one to one and time series anns respectively arima predicted trajectories are indicated by orange dashed dotted lines and persistence predictions by solid red for comparison forecast errors from eqs 6 7 for velocity and position are shown in panels b and c respectively with error bars indicating the spread of the forecasts given by one standard deviation from the mean error the simple rotational flow of case 1 and the spiral trajectories of case 2 proved to be trivial scenarios for both the one to one and the time series anns to learn predicted trajectories from both networks are visually indistinguishable from each other and also from the real trajectory itself figs 4 and 5a this offered stark improvement over the persistence forecasts which as expected for circular trajectories are always tangent to the eddy both networks for case 1 had similar velocity errors of approximately 0 7 0 4 cm s 1 and position errors around 50 50 m at hour 6 increasing to around 450 300 m at hour 24 case 2 velocity error increased from about 0 3 0 3 cm s 1 at hour 6 to about 0 7 0 5 cm s 1 at hour 24 for the one to one ann which corresponded to position errors around 32 30 m at hour 6 increasing to around 740 550 m at hour 24 the time series ann velocity errors were fairly constant around 0 4 0 3 cm s 1 throughout the entire prediction window leading to position errors increasing from around 47 38 m at hour 6 to around 224 216 m at hour 24 the arima ensemble exhibited a smaller standard deviation than the anns especially in case 1 but the average error increased almost linearly throughout the prediction window while the one to one ann behaved similarly the time series ann mean error remained fairly constant in both cases the slight underperformance by the one to one ann relative to the time series ann beyond 12 h can be traced to the first forecast issued 00 00 on day 2 this early version of the one to one ann produced velocity prediction errors at hour 24 of nearly 7 cm s 1 for several particles in case 1 and 4 cm s 1 in case 2 these outliers aside both anns performed equally well in both of these cases case 3 is more interesting this time arima and persistence were comparable and the one to one ann performed just as poorly as both while the time series ann offered notable predictive improvement the trajectory in fig 6a illustrates a loop on day 13 that scales as a submesoscale feature a 180 turn like this might be observed in nature if a change in weather pattern such as the passage of a strong atmospheric front reverses the direction of the surface flow over the course of several hours the one to one ann predicted a curved trajectory blue dots but fell far short of anticipating the sharp curve that the particle undertook the arima model predicted a similar curved trajectory as the one to one ann but the velocities were nearly twice those of the ann this took the particle twice as far away consequently by the end of the 24 h period the particle ended up being about 55 km away from its predicted position this was only mildly better than the persistence prediction which put the particle roughly 75 km away from its actual position in comparison the time series ann green dashes successfully predicted the particle s change in direction on days 13 and 14 with the final location being about 3 km away from the prediction error analysis revealed that the mean velocity error at hour 24 for the time series ann was about 10 9 cm s 1 compared to 52 44 cm s 1 for the one to one ann which led to a 5 fold decrease in position error these results are corroborated by other particles with a variety of trajectory shapes as demonstrated in fig 7 most trajectories contain sections that are relatively easy to predict for example the last few days in fig 7d and other sections that are more challenging such as the loops in the northern and southern regions of the domain overall passing a time series of observations to the network as input significantly improves the ann s predictive ability highlighting the importance of network architecture and problem setup 4 application gulf of mexico mesoscale eddy 4 1 methods to test the anns in realistic ocean flows we set up a case study using output from a high resolution 1 12 hybrid coordinate ocean model hycom for a mesoscale eddy observed in the gulf of mexico during january 2010 see prasad and hogan 2007 and haza et al 2016 for full details of model configuration this anticyclonic mesoscale circulation surrounded by submesoscale structures provides a known example of interacting scales of motion and a realistic manifestation of our proof of concept case 3 particles are advected through the eddy in the same manner as before 360 equally spaced particles released simultaneously at 00h on 15 january 2010 in a circle of radius 70 103 m a rough estimate of the perimeter of the mesoscale circulation particles are advected for 14 days ending on 28 january hycom eulerian velocity fields were generated every 3 h thus spatial and temporal interpolations are necessary when implementing the rk4 integrator to produce trajectories from the model output acceleration fields are calculated between successive velocity output in order to interpolate velocity on time steps smaller than 3 h spatial interpolation for both velocity and acceleration is carried out using cubic 2d interpolation data setup network configurations and training methods are as described in section 3 1 the time series ann results of case 3 raise questions regarding sensitivity to the chosen prediction window if the forecast is extended beyond 24 h will the time series ann perform just as well or will its error climb to those of the other models we investigate this in the context of this realistic eddy by considering 72 h predictions at each forecast time without changing the configuration of our anns the output time series is interpolated to 3 h time steps using linear interpolation and passed back to the network twice as described in section 3 1 4 acknowledging the practical challenges of deploying hundreds of drifters at sea we also consider the sensitivity to the number of test particles by repeating this 72 h forecast simulation releasing a total of 90 and 45 particles here we utilize a 5 fold cross validation technique to assess the skill of the networks in each scenario k fold validation is easily implemented by randomly dividing all particles into k 5 groups without replacement each of these groups is then taken to be unique set of test particles and the remaining particles become the training examples for the given k fold this allows every particle to be predicted once by an ann instead of initializing three unique random anns as above we seed the random number generator so that every ann starts in the same solution space and its evolution depends only on the particles in the respective training set finally we explore different numbers of hidden layers 1 2 and 3 and hidden neurons 10 20 50 100 for one to one 20 50 100 200 for time series anns using 360 particles three uniquely initialized models are trained for each test as before to account for variations in learning due to random initialization we hereafter refer to the hycom velocity output as observations in order to differentiate them from the ann predicted velocities 4 2 results gulf of mexico mesoscale eddy application the gulf of mexico mesoscale eddy provides a far more realistic representation of what was idealized in case 3 above one important difference is that the particles in this scenario are no longer trapped within the mesoscale circulation instead some escape mostly from the northern rim of the eddy while some end up being drawn into the center of the eddy and still others stay around the perimeter of the mesoscale circulation fig 8 a second difference is that the fluctuating velocity component is no longer sinusoidal or at the very least there are enough interacting dynamics that any predictable periodicity is much harder to discern this enables us to test whether or not the time series anns in case 3 were actually learning the superimposed periodic signals if they were then they should struggle to learn these realistic trajectories fig 9 summarizes the error results for the one to one and time series anns trained on these eddy trajectories the arima errors both mean and standard deviation are indistinguishable from persistence and like with the proof of concept case 3 the one to one ann does not offer any improvement over either metric the velocity and position error plots indicate that on average the one to one ann predictions at hour 24 were worse than both of these models the representative test particle shown in fig 9a illustrates how this might happen the one to one ann predicted for day 13 that the particle would continue its slight clockwise curve that it had been tracing the day before instead the particle curved back to the left before moving due west as a result its final position ended up closer to the persistence prediction than the one to one prediction while this seems fluky a similar situation played out on day 14 the time series ann on the other hand better predicted the slightly wavy trajectory this may be because the particle s trajectory during day 12 was also somewhat s shaped the time series ann velocity errors leveled off around 7 2 4 5 cm s 1 and the position errors climbed from around 1 km at hour 6 to around 5 3 3 km at hour 24 this also illustrates the error cascade from velocity to position even if velocity errors stop growing they will still propagate through to positions and affect the entire predicted trajectory arima forecasts for days 13 and 14 were not very different from those of the time series ann interpretation of the arima results will be discussed further in the next section time series ann performance on longer range 72 h forecasts is shown in fig 10 for a total of 360 90 and 45 particles with 5 fold validation these networks were trained with 288 72 and 36 particles respectively the dashed gray lines indicate the forecast iteration times when the model output were used to generate the next 24 h prediction arima models were fit as before using the auto arima routine but full 72 h predictions were made at each forecast time all model errors were calculated using eqs 6 7 fig 10 shows mean error with one standard deviation several notable take aways follow from fig 10 first after the initial 24 h ann mean velocity error steadily increases towards though never surpasses the mean arima model error roughly doubling from almost 10cms 1 at hour 24 to approximately 23cms 1 at hour 72 secondly while ann position error increases 5 fold from approximately 5km at hour 24 to approximately 25km at hour 72 it remains well below that of arima through hour 72 such that even the upper bounds of the ann error envelope are at or only slightly greater than the arima mean position error this is promising because position is often of greater interest to the particle transport problem than velocity third the ann standard deviations remain smaller throughout the prediction window finally for the first half of the prediction window 0 36h the mean error for the anns with 360 particles is slightly lower than the ann with 90 particle and the error spread somewhat smaller as should be expected and similarly for 90 particles versus 45 after hour 36 there is little statistical difference between the three configurations though the 360 particle ann has a smaller position forecast spread throughout the entire window we discuss each of these outcomes in the next section we next investigate whether changing the size number of neurons and depth number of layers of both one to one and time series anns helps the networks learn to predict trajectories in realistic flows let the convention m n describe these networks by the number of hidden layers m and the number of hidden neurons in each layer n to assess the performance of each network configuration we calculate the relative error between the real target values x r and the predicted values x p 8 re i j k l x i j k l r x i j k l p x i j k l r where x r is either the ann output or the persistence prediction and the subscripts are as defined in section 3 2 fig 11 shows relative error ratio between positions derived from persistence predictions and those derived from ann predictions averaged over i j and k 9 ℜ l 1 g p f i 1 g j 1 p k 1 f x i j k real x i j k pers x i j k real x i j k pred where g 4 predictions per forecast p 90 particles f 13 forecasts for the eddy simulation error bars indicate standard error all ratios are 1 indicating that every ann outperformed persistence despite larger networks taking five to six times longer to train no appreciable difference in performance is observed as the anns increase in complexity one explanation for this is that the hyperparameters such as learning rate and number of training epochs were not tuned for each model but rather were kept constant in order to assess the affect of changing either m or n on model performance finally we consider whether or not the neural networks continued to learn throughout the simulations due to the continuous training method fig 12 shows position error from eq 6 averaged over i prediction times and j test particles for the one to one anns blue dots time series ann green dashes and arima model red dash dots the arima model exhibits a clear downward trend from an average position error of almost 9 km at the start of the simulation around 2 km on the last day this can be attributed to the fact that early in the simulation the time series were too short to successfully regress in these scenarios the arima forecasts were nothing more than the time mean of the few available observations as the time series grew this became less of a problem and the arima models acquired greater skill in contrast no significant trends are observed for any of the neural networks though the one to one anns seem to improve from around 9 km on 16 jan to around 5 km by 28 jan overall we conclude that while the majority of the learning by both network configurations took place during the initial training session subsequent training routines kept the networks up to date using the latest information for time varying domains this is equally as important as initial learning 5 discussion we demonstrated preliminary success at predicting particle transport in the ocean using simple artificial neural networks two questions motivated this study first inspired by the ability of human scientists to discern patterns in oceanic flows e g rupolo 2007 we asked can a data driven machine learning technique accomplish a similar task the goal was to extract enough information from ground truth observations to make respectable forecasts instead of starting with primitive equations the second question was can a data driven algorithm learn to make accurate ocean forecasts without knowing anything whatsoever about the underlying physics if so these tools could potentially supplement existing theory driven ogcms the proof of concept experiment provided a starting point for testing idealized ocean trajectory patterns such as simple inertial oscillations e g beron vera et al 2015 gough et al 2016 and interacting scales of motion e g haza et al 2016 the most promising results were the abilities of these simple anns to predict trajectories within and around a realistic gulf of mexico mesoscale eddy we attribute this success at least partially to the continuous learning training scheme whereby training continues every time the oldest observation is replaced with a new one while the more common approach is to train a neural network once using a large data set and then use it to make predictions of new data this would require having in advance an impractically large training set consisting of full trajectories for millions of particles instead we treated the domain as continuously varying in time and allowed the network to keep up with the latest available observations the contrasting performance of one to one and time series anns in realistic flows demonstrated the importance of using theory to guide model development e g faghmous et al 2014 the one to one anns have the operational advantage of only requiring one velocity observation to make a forecast but trained this way they lack any concept of time and can only remember the most recently seen example changing the input to a time series of observations significantly improved performance since the network was able to consider where the particle had been over some period of time this enhanced performance is to be expected given the fundamental nature of time series and is also in agreement with the first notable discussions on handling time and memory in learning paradigms by jordan 1986 and elman 1990 1991 it is reasonable to ask how the time series neural network managed to outperform even arima forecasts over a 24 h prediction window the primary reason for this is that arima models are inherently sensitive to time series length the wide arima forecast spread large standard deviations in figs 6 9 and 10 is due to the time series early in the simulation being too short to be properly regressed e g 16 18 jan in fig 12 in these cases auto arima returned models whose forecasts were merely the time mean of the few available observations at least 72 h of observations were necessary before skill became comparable to the one to one network where it remained for most of the simulation 18 26 jan not until the last three days of the simulation did the arima model have a sufficient number of data points to make predictions similar to those of the time series ann while the arima models could only be fit to individual time series the anns had the advantage of being able to learn from an assortment of unique time series thus both the one to one and time series anns demonstrated greater skill than arima during the first two days of the simulation fig 12 it is worth noting that the skill of both ann configurations came from seeing only 24 h worth of observations at any given time and the majority of the networks learning occurred during the initial training session that is the training leading up to the 06 jan forecast shown in fig 12 anns may therefore be favorable for generating time sensitive forecasts soon after deploying observational drifters whereas one may need to wait several days before a traditional regression model could provide any skilled insight it is well known that model accuracy inevitably breaks down when a learning machine is used to make predictions outside of the domain in which it was trained this is precisely why the predictive capability of our ann erodes beyond 24 h fig 10 this shortcoming might be overcome by modifying our ann to instead predict the full 72 h time window but doing so would require target data at times t o 6 h t o 12 h t o 72 h for each training example in addition to input observations from the previous 24 h thus such a configuration would require 96 h of training data a final caveat of our experimental setup worthy of consideration is that although the anns are evaluated on particles not seen during training predictions are made over the same time period as the training target data this is fundamentally different from time series regression models that do not require future data while both arima and anns are function approximation models the neural network represents a single mapping function developed through exposure to many representative time series in order to describe other similar time series this also explains the relatively small sensitivity to the number of training particles the unseen particles are most likely to be predicted relative to proximate training particles thus our results suggest that one can use only o 100 drifters to forecast where intermediate drifters would go as long as the drifters all sample the same dynamic features this is especially advantageous for the oceanographer since deploying vast arrays of drifters is expensive and often infeasible we conclude by commenting on overfitting an inherent risk that plagues all machine learning paradigms wherein a model learns the training data too well and then fails to extrapolate to new data the anns in proof of concept cases 1 and 2 for example may have been prone to overfitting as evidenced by velocity prediction errors greater than what might be expected for such trivial cases fig 5 and especially fig 4 aside from increasing the size of the training set which is often not possible overfitting can be minimized by decreasing the degrees of freedom of the model or minimizing the amount of training the first strategy is easily accomplished by using the fewest possible hidden neurons on the other hand minimizing the amount of training is less trivial for systems that evolve continuously in time such systems often require continuous learning despite the increased risk of overfitting we tested this by repeating the eddy simulation in the same way as described above except that the time series anns were not trained at all following the initial training on day 1 this resulted in forecast errors that were consistently on par with those of both persistence and the worst arima forecasts throughout the entire simulation not shown confirming that online learning was required ultimately the selection of the best version of the ann at each training session compromised between the need to keep the network up to date with the system dynamics while recognizing that at any given time little or even no additional training may actually be required to summarize anns may be beneficial in real ocean scenarios when i numerous time series are available that collectively contain useful information to assist with forecasting ii available time series are too short for regression models e g at the beginning of a coordinated drifter deployment or iii predicting the transport of material within a local domain is necessary using observations dispersed within the domain e g predicting the spread of an oil plume using a limited collection of drifters 6 conclusions we have demonstrated that particle trajectories in a variety of simulated and modeled oceanic flow regimes can be learned by simple artificial neural networks the fundamental idea is to move away from relying on primitive equations of motion for prediction as these are impossible to initialize and solve for all scales of motion in the ocean instead the goal is to develop a predictive tool that learns from ground truth observational data without any direct physical guidance we started with a proof of concept exploration using a hierarchy of idealized flows and then tested our approach on realistic flows generated by a high resolution hybrid coordinate ocean model for a mesoscale eddy in the northern gulf of mexico this eddy provided a particularly illustrative example of interacting mesoscale and submesoscale dynamics and we showed that our approach to ann training performed well even for these complex flow fields we tested two approaches to ann training in the first configuration the network predicted a drifting particle s velocity at time t o 6 h using only its previous velocity at time t o this approach had the operational advantage of requiring only one velocity observation to make a forecast but predictive performance suffered due to a lack of internal memory within the ann we also developed networks that predicted the drifting particle s velocity at time t o i δ t i 1 2 3 4 based on the last 24 hours worth of velocity measurements this time series ann performed between two and six times better than its one to one counterpart for both velocity and position estimates depending on the complexity of the flow the predictive capabilities of the time series anns can be attributed to several things first was allowing the ann to see and learn from a time series of data second we restricted our training data to the past 24 h of data replacing old observations with the most recent this helped the ann forget regime shifts within the data set such as a particle sharply changing direction or behavior as may result from a strong weather event we adopted a continuous training approach whereby ann training continued whenever new data became available this allowed the ann to always be up to date with the constantly changing domain since real ocean drifters continuously provide new data this training technique does not seem unreasonable in fact we argue that it presents a highly efficient and effective way of utilizing real time data for ocean prediction finally and most importantly the time series ann is able to learn from many independent trajectories and these time series can be as short as 24 h at 3 h increments this is a fundamental advantage over traditional regression models that are time series specific and in the case of ocean particles require at least 72 h of observations before any predictive skill is discernible this study addressed the question of whether a neural network could learn to predict a particle s velocity based only on its previous velocities an important distinction is in order between learning particle trajectories and learning the dynamics of the system in which the particles exist the latter is a much more difficult problem because the sparsity of observational data means that the dynamics of the ocean are seldom known a posteriori learning to interpolate point wise time series is often the best one can do when relying only on observations from an operational standpoint this should not be a concern predicting the spread of an oil spill does not necessarily require learning or even knowing the full dynamics but rather predicting how certain sections of a plume may evolve based on nearby observations the eddy application results suggest there is reason to be optimistic that anns will be able to learn observed trajectories just as well thus future work will involve testing this predictive approach on real drifter data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was made possible by a grant from the gulf of mexico research initiative united states data are publicly available through the gulf of mexico research initiative information data cooperative griidc at https data gulfresearchinitiative org doi 10 7266 n7bc3wjc the authors thank the three anonymous reviewers whose constructive suggestions and recommendations greatly improved this manuscript appendix the artificial neural network a 1 brief overview an artificial neural network ann is a biologically inspired nonlinear regression model that is trained to map input data to desired output values groups of nodes called neurons are arranged in layers such that adjacent layers are fully interconnected by weighted links but no neurons within the same layer are connected to each other see figs 2 and 3 in the text layers in between the input and output layers are commonly referred to as hidden layers because they are inside the network and the intermediate data mappings they produce are usually not known to the network engineer all neurons consist of a trainable bias or zero th weight that is added to the neuron s input signal and an activation function usually chosen from families of functions that return values either on the interval 1 1 or 0 1 e g duch and jankowski 1999 activation functions reduce the dimensionality of the data being passed through successive layers in the ann all weights and biases are initialized as small random numbers we take these from a univariate gaussian distribution of mean 0 and variance 1 training is the process by which solution space is searched for the optimum combination of these weights and biases such that the network output most accurately matches desired target values a common approach is to pass training examples a subset of all available data to the ann one at a time and compare the output to the corresponding target values for each example if the output differs from the target the error is back propagated through the network to quantify each neuron s contribution to the overall error and small adjustments are made to the weights and biases the network is always tested and evaluated on data not seen during training we now describe this training process and the details about the ann configuration used in this study the discussion closely follows kubat 2017 for a comprehensive and illustrative overview of neural networks see also nielsen 2014 a 2 network configuration and training consider a training example x defined as a vector containing n attributes describing that particular example x x 1 x 2 x n each attribute is assigned its own input neuron in the ann thus the one to one ann described in section 3 1 4 has n 2 input neurons where the attributes are horizontal velocity components for the time series ann each example contains n 16 attributes 8 observations 2 velocity components see section 3 1 4 in the text so these networks required 16 input neurons the simplest possible ann contains a single hidden layer and as few hidden neurons as possible we choose 10 hidden neurons for the one to one network and 20 hidden neurons for the time series ann determining these sizes is more art than science and we therefore conduct sensitivity tests in section 4 let the weights connecting the k th attribute to the j th hidden neuron and the j th hidden neuron to the i th output neuron be denoted w k j and w j i respectively similarly let the biases for the k output neurons and the j hidden neurons be denoted b k and b j respectively a sigmoid transfer function of the form a 1 f x 1 1 e σ is chosen for all hidden and output neurons where f x is the neuron output and σ is the weighted sum from the previous layer in the network e g a 2 σ j 1 w 11 x 1 w 21 x 2 w n 1 x n b 1 each example is passed through the network as weighted sums of the attributes to produce an output vector y y 1 y 2 y i a 3 y i f j w j i f k w k j x k b j b i in the case of the one to one anns the output vector is the same length i 2 as the input vector thus these networks also have i 2 output neurons one corresponding to zonal velocity and the other to meridional velocity while the time series anns contained i 8 output neurons 4 predictions 2 velocity components eq a 3 describes the behavior of a simple feed forward neural network note that since eq a 1 returns values on the interval 0 1 the output of the ann will also be on this interval thus all data are normalized to this interval before being presented to the network and all output must be returned to the range of the original data this requires that the minimum and maximum values of the original data set be retained with the ann and used to normalize all future data on which the trained ann might be run to train the ann we first define a target vector r r 1 r 2 r n for each velocity observation here r corresponds to a given particle s velocity at time t o δ p t where t o is the issue time of the forecast and δ p t is the forecast prediction time step each training example is passed through the network eq a 3 and the ann output is compared to the target vector prediction error is then back propagated through the network to quantify each neuron s responsibility to the overall error and the weights and biases are adjusted accordingly a 4a w j i w j i η δ i h j b i b i η δ i a 4b w k j w k j η δ j x k b j b j η δ j where η 0 1 is a small learning rate h j is the output of the j th hidden neuron and δ i δ j are quantifications of the neurons error responsibility a 5a δ i y i 1 y i r i y i a 5b δ j h j 1 h j i δ i w j i presenting the last training example to the ann indicates the completion of one training epoch the ann performance is then evaluated using a cost function whose value is to be minimized during the training process the cost function in this study is taken to be mean squared error a 6 m s e 1 n i 1 n r i y i 2 training continues for a defined number of epochs or until some performance threshold has been achieved once trained the ann is run by presenting new examples one at a time predictions are made using eq a 3 and the optimum weights and biases a 3 final remarks the strength of anns is their theoretical ability to fit any function adding neurons to an ann introduces more weights to adjust and is comparable to introducing higher order terms to a nonlinear regression equation adding additional hidden layers increases the number of times the data are transformed within the model in order to reduce dimensionality as the network attempts to map the input vector to the desired output signal like all regression models however neural networks are prone to overfitting the training data and thereby performing poorly on new data this risk can often be minimized by decreasing the size of the network i e the number of hidden neurons increasing the training data set or by shortening the training time here we adapt the smallest possible ann and treat the completion of each training epoch as producing a new version of the ann and take the best of these that is the one with smallest m s e at the end of each 100 epoch training session another disadvantage of these models is that they often lack interpretability see section 5 nevertheless they can be ideal tools for problems in which one need not know why a particular outcome occurs as discussed in the text this is often true for predicting material transport in the ocean 
23939,predicting ocean transport has many practical applications ranging from search and rescue operations to predicting the spread of oil debris and biogeochemical tracers yet trajectory prediction remains a challenge for existing ocean modeling techniques general circulation models require high resolution observational data in order to be properly initialized but these data do not exist for the ocean statistical models are difficult to tune with existing data and are often too simple to accurately encapsulate turbulent flows here we investigate a data driven approach to ocean transport prediction wherein the goal is to first learn from available data instead of prescribed laws of physics and then apply this information to new data more specifically we explore whether simple artificial neural networks anns are capable of learning to predict 2d particle trajectories using only previous velocity observations anns are trained in two ways first a so called one to one ann uses a particle s most recently observed velocity to predict its velocity six hours later and second a time series ann uses the past 24 hours worth of velocity observations to predict the next 24 h we present a proof of concept considering particles in a hierarchy of simulated flow regimes ranging from uniform steady flow to more complex scenarios with interacting scales of motion and then substantiate our approach on trajectories in modeled flows generated by a high resolution hybrid coordinate ocean model for a mesoscale eddy in the northern gulf of mexico we also assess ann sensitivity to the prediction window over which forecasts are made the number of training particles used and the size of the network anns successfully predict 24 h trajectories within the temporal bounds of the training data with forecast errors around half those of both rudimentary persistence and classical arima models predicting beyond the domain of the training data leads to forecast errors comparable to arima models our results suggest that anns offer promising potential as a data driven approach to forecasting material transport in the ocean keywords advection current velocity mass transport particle motion current prediction machine learning 1 introduction efforts to predict material transport in a turbulent ocean are motivated by many socioeconomic interests including oil spill response poje et al 2014 özgökmen et al 2016 search and rescue operations isaji et al 2006 and anticipating the spread of harmful algae blooms pollutants and marine debris enriquez et al 2010 olascoaga 2010 olascoaga and haller 2012 normile 2014 traditional ocean forecasting techniques merge theoretical knowledge of the underlying flow dynamics with information about the initial state from which the system evolves ground truth data are now widely available via real time ocean observing initiatives from both lagrangian and eulerian perspectives these data include direct in situ velocity measurements such as from current meters and drifters shay et al 1998 lumpkin et al 2017 novelli et al 2018 along with products derived from passive monitoring systems such as high frequency radar and satellite altimetry barrick et al 1974 1977 mcgoogan 1975 roarty et al 2010 despite decades of advances in observational and computational methods conventional theory driven ocean forecasting tools continue to be inhibited by the prevailing sparsity of ocean data özgökmen et al 2009 bolton and zanna 2019 one problem is that not enough data exist to properly initialize predictive models the highest resolution ocean general circulation models ogcms numerically compute eulerian velocity at o 1 0 9 spatial points at any given time özgökmen et al 2009 each of which requires unique initial conditions e g mariano et al 2011 wei et al 2016 still the ocean remains under sampled to such a degree that even coarse resolution ogcms are under constrained by available data especially outside of coastal regions while an ogcm may be initialized with low resolution climatology in the absence of observations this seldom provides suitable starting conditions for very localized problems such as particle prediction two examples illustrate the sensitivity of the particle prediction problem to accurate initial conditions first even in simple laminar flows two identical lagrangian drifters released simultaneously side by side eventually diverge and travel very different routes due to the unsteady time dependence of oceanic flow e g aref 1984 yang and liu 1997 özgökmen et al 2001 koshel and prants 2006 this chaotic particle behavior is notoriously difficult to predict because even slight differences in starting conditions yield vastly different forecasts a second example is the case of a drifter encountering two adjacent counter rotating eddies the systems of streamlines that encircle the centers of rotation form so called lagrangian coherent structures lcs that are known to control material transport peacock and haller 2013 olascoaga et al 2013 whether this drifter gets drawn into the inward flow of one eddy or repelled by the outward flow of the second eddy depends strongly on its launch position relative to the two eddies and to the singular streamline that divides them molcard et al 2006 small errors in the initial location of either the drifter or the critical streamline can again result in forecasts that are drastically different from actual observations a second challenge with particle prediction is the inability to adequately sample the complexity of ocean dynamics locating critical regions around lcss in nature is nearly impossible haza et al 2007 because observing them requires impractically high numbers i e millions of drifters ocean forcing mechanisms cause unique and independent dynamic features to exist across a wide range of scales stommel 1948 holland and lin 1975a b fully capturing these ocean dynamics requires that a minimum of o 1 0 23 spatial points be sampled simultaneously using a typical reynold s number r e 1 0 10 özgökmen et al 2009 even the finest ocean experiments which might sample thousands of spatial points do not come close to this requirement consequently the complexity of relevant steering dynamics on all scales of motion is neither fully understood nor accurately replicated in predictive ogcms of particular importance to material transport are localized submesoscale dynamics characterized by spatial scales 10 km and time scales on the order of hours to days these dynamics have been found to significantly impact particle trajectories in the ocean everywhere from the gulf of mexico e g poje et al 2014 berta et al 2016 to the beaufort sea mensa et al 2018 unlike mesoscale dynamics submesoscales are strongly influenced by vertical motions e g smith et al 2015 choi et al 2017 mensa et al 2018 surface convergence fronts are common at these scales and function dualistically as across front barriers to transport and as along front transport highways schroeder et al 2012 smith et al 2015 kuitenbrouwer et al 2018 taylor 2018 haza et al 2016 also identified submesoscale leakage pathways by which particles may escape larger mesoscale circulations while mesoscales are usually fully resolved by ogcms submesocale dynamics are often parameterized due to computational restrictions on model resolution even if partially resolved the modeled submesoscale dynamics are not always physically realistic because constraining them is difficult with available observations this ultimately leads to discrepancies between numerical simulations and real observations dueben and bauer 2018 stochastic or markovian models provide an alternative to calculating velocity at large numbers of grid points by considering flow fields statistically when used in conjunction with ogcms unresolved physics are added to the statistical mean flow as random increments zero th first or second order markovian models calculate successive positions velocities or accelerations respectively griffa 1996 provided the first oceanographic evaluation of stochastic models by using constant parameters corresponding to known time and length scales of the mean flow and showed that particle motion in the upper ocean can be closely simulated using first order markovian models with a finite turbulent velocity scale berloff and mcwilliams 2003 developed these further to better replicate interactive mesoscale ocean dynamics by randomizing the model parameters this allowed particles to also move between features as is commonly observed in nature berta et al 2016 haza et al 2016 haza et al 2012 later merged statistical subgrid models with mesoscale lcss computed from ogcms to better simulate particle behavior on submesoscales markovian models are among the cheapest trajectory models to run with everything parameterized trajectories can effectively be generated in a matter of seconds nevertheless it is often difficult to determine model parameters from available data and tuning the parameters once the form of the differential equations has been set can be tedious while targeted sampling of multi scale dynamics such as in the grand lagrangian deployment glad experiment in the northern gulf of mexico have provided new insights into ocean velocity statistics they have also highlighted performance shortcomings in existing statistical prediction methods beron vera and lacasce 2016 mariano et al 2016 to summarize while traditional theory driven ocean models are excellent tools for better understanding fluid dynamics and for exploring new phenomena they struggle as predictive tools because 1 the sparsity of observational data makes it impossible to accurately initialize high resolution ocean models 2 not enough data exist to observe complete ocean dynamics in nature nor to properly replicate them in ogcms and 3 important steering dynamics at local submesoscales are often poorly captured by ogcms due to computation restraints on model resolution initialization and tuning challenges also affect simpler statistical models that do not rely on solving primitive equations at millions of grid points in light of these challenges an alternative approach to ocean transport prediction may be one that starts with objective observations instead of prescribed laws of physics here we take initial steps of exploring this possibility in the field of physical oceanography by testing whether ocean particle trajectories contain inherent predictability that a data driven algorithm might discover our goal broadly speaking is to apply information learned from past data to predict future states data driven strategies such as this form the foundation of machine learning techniques that have become enormously popular for tackling complex problems in 21st century data science we now present the particle prediction problem more specifically within the framework of data driven machine learning 2 data driven machine learning for ocean prediction data driven modeling is built upon the rationale that cause and effect relationships are often discernible among relevant variables and observed outcomes solomatine et al 2008 for example one intuitively expects a relationship between wind speed and sea state data driven models are trained to identify statistical relationships between data sets and later use this learned information to make predictions about cases unseen during training although these models do not explicitly provide insight into the underlying physics of a problem they are useful in situations when knowing an outcome is more important than understanding the relevant physical mechanisms oil spill first responders for instance are more concerned about where oil is and where it is going than why it moves as it does data driven models can also be beneficial when the full physics of a problem are not known when the underlying physics are not easily modeled numerically or when large quantities of relevant data are available from which to learn solomatine et al 2008 considerable effort has been made over several decades in fluid dynamics to use data driven machine learning as alternatives to the computationally expensive task of solving the nonlinear navier stokes equations one increasingly popular method attempts to learn governing partial differential equations from large quantities of numerical model output or experimental data this approach starts with observations of one or more state variables and their time derivatives assembles a library of nonlinear features in the form of polynomial functions and finally identifies the relevant components in the feature library using weights obtained by a machine learning algorithm e g brunton et al 2016 ayed et al 2019 ouala et al 2019 the discovered equations are then integrated in time to forecast future system states other work has focused on learning flow dynamics from snapshot images e g lee and you 2017 mohan and gaitonde 2018 still others develop sophisticated black box machine or deep learning methods to either combine with or altogether bypass conventional models chen et al 2018 wikner et al 2020 these and similar studies in a rapidly growing body of literature boast promising potential for learning and predicting complex flow dynamics but most depend on having observations at resolutions and densities not yet available to the oceanographer an important aspiration of the present investigation is to replace the computationally expensive process of running circulation models with information learned entirely from available observations this requirement necessarily precludes methods that rely on high resolution model output for training perhaps the easiest instruments for the oceanographer to deploy in relation to this problem are lagrangian surface drifters that provide time series of position velocity and acceleration throughout a region of interest given the nature of the problem and the hypothetical data we chose an artificial neural network ann a biologically inspired data driven nonlinear regression model whose notable strength is its theoretical ability to fit any function neural networks have demonstrated promising potential in a variety of complex geophysical problems such as identifying climate feedback mechanisms gonzález et al 2015 weather forecasting dueben and bauer 2018 predicting climate patterns toms et al 2020 and hurricane track prediction moradi kordmahalleh et al 2016 in oceanography neural networks have been developed for locating spilled oil in synthetic aperture radar imagery kubat et al 1998 for increasing the efficiency of parameterization routines in numerical ocean models krasnopolsky et al 2002 for modeling nonlinear wind wave spectra tolman et al 2005 and for ocean eddy identification and tracking franz et al 2018 lguensat et al 2018 here we investigate whether simple anns can learn to use observed velocities of drifting lagrangian particles to predict future velocities our study differs from others in the oceanographic literature that explore machine learning tools in that we develop from scratch and test the simplest possible ann instead of using more sophisticated forms of neural networks from preexisting software packages our rationale behind using a simple feedforward ann is twofold first is the law of parsimony credited to the fourteenth century philosopher william of ockham and supported by modern probability theory which states that given two explanations for some phenomenon the simpler one is more likely to be correct in light of this and following the inspiration of influential pioneering work in neural networks e g elman 1993 we choose to start small and simple the second reason for choosing a simple feed forward network is more practical the decades old black box objection to neural networks is that their performance under the hood or within a chosen machine learning library is physically uninterpretable see e g touretzky and pomerleau 1989 open research questions within the machine learning field related to this include what exactly did the neural network learn and how is it solving this particular problem what do the weights signify what do the neuron activation behaviors mean and what are the patterns learned within the network while these questions are well beyond the scope of the present investigation answers would be of great interest to the oceanography community accustomed to extracting direct physical interpretations from theory driven models one can therefore argue that the more complex the machine learning architecture the more physically abstract its behavior is likely to be and the lower the likelihood that physical interpretability will ever be identified sometimes simpler is better the success of any data driven method fundamentally requires some degree of predictability among the desired output data e g massoud et al 2018 the predictability of ocean lagrangian trajectories has been extensively explored both numerically e g özgökmen et al 2000 rixen et al 2008 and experimentally e g özgökmen et al 2001 coelho et al 2015 özgökmen et al 2000 2001 use simple statistical models that closely resemble anns to show how trajectory prediction on the order of days to weeks can be improved by combining information from nearby lagrangian drifters with knowledge of the mean flow within a region of interest one then wonders whether lagrangian drifter data alone contain enough information from which to predict trajectories if so then an ann ought to be able to learn from lagrangian trajectories without any additional information about the underlying physics this study takes the first step toward answering this question by systematically exploring the inherent predictability of ocean trajectories using simple artificial neural networks a simple test of predictability of a time series is whether or not future states can be predicted given only information about previous states for example the sequence 2 4 6 8 10 x contains enough information for one to predict the value of x to be 12 similarly one can test whether particle trajectories contain inherent predictability by trying to predict where a particle will go next based on where it has already been this is the premise of all regression techniques and is the logical starting point for testing any new time series prediction scheme an early non oceanographic demonstration of this technique in an industrial robotic application was by payeur et al 1995 who designed neural networks to predict position velocity and acceleration of moving objects based on previous observations this work suggests that the sequential velocities of drifting ocean particles may be learned in a similar fashion we therefore pose several questions can a neural network be trained to predict a particle s future velocities using only its previous velocities as input if so how would its predictive skill compare to a more traditional autoregressive model if this predictive task proves challenging can the ann at least perform better than a rudimentary persistence model the first part of this study consists of a series of proof of concept control experiments involving particles in a hierarchy of simulated known flows deliberately engineering the flow field allows us to gradually turn up the complexity of the dynamics which inversely decreases the predictability of lagrangian trajectories in order to systematically determine where predictability collapses of the many tested flow fields three are presented here we then conduct a case study in physically based oceanic flow fields generated by the hybrid coordinate ocean model hycom for a mesoscale eddy in the gulf of mexico to assess how our approach might perform with observational data in a real ocean application two types of anns are developed one predicts a particle s velocity at some future time based only on its previous velocity and another uses as input one day s worth of observations anns are evaluated against predictions from both rudimentary persistence and autoregressive integrated moving average arima models we find that anns predict trajectories of particles not seen during training often with half the forecast error of arima we also consider ann sensitivity to the numbers of hidden layers hidden neurons and training particles and to the prediction window over which forecasts are made importantly our methodology is easily applied to field applications by replacing simulated particle velocities with those from lagrangian drifters anywhere in the ocean our results suggest that data driven anns may offer a missing supplement to traditional theory driven models by providing forecasting tools that learn entirely from objective ground truth observations 3 proof of concept 3 1 methods we first conduct a hierarchy of control experiments consisting of buoyant particles in simulated rotational coherent structures of increasing complexity 1 steady uniform and stationary eddy center of rotation is not moving within the domain 2 steady uniform and drifting eddy center of rotation moves in time and 3 drifting eddy with time dependent sinusoidal pulses of amplitude a and frequency ω added to the mean rotational flow each flow field is scaled according to a realistic mesoscale eddy in the gulf of mexico studied by haza et al 2016 as a known example of interacting mesoscale and submesoscale motions a description of the parameter space for each flow field follows 3 1 1 case 1 stationary mesoscale eddy if eulerian velocity u as a function of space x x y and time t is separated into a time independent mean circulation u and a time dependent fluctuating component u 1 u x t u x u x t then case 1 represents mean eddy flow only with no small scale turbulence that is u 0 and u x t u x velocity components for circular mean flow are given by 2a u x y 2 c o y y c r 2 exp x x c 2 y y c 2 r 2 2b v x y 2 c o x x c r 2 exp x x c 2 y y c 2 r 2 where x y are the zonal and meridional positions of the particle respectively x c y c is the center of the eddy taken as the cartesian origin for convenience r 70 103 m is the radius at which the flow velocity is a characteristic mean velocity u v 0 1 m s 1 and c o 2 6 104 m2 s 1 is a dimensional parameter that sets the mean velocity u v at radius r calculated using eq 2 with x y r and x c y c 0 and where c o 0 produces clockwise flow given the absence of temporal flow variance particles released in case 1 will travel around a perfect circle a representative particle trajectory is shown in fig 1a starting at the solid circle and ending at the 3 1 2 case 2 drifting mesoscale eddy case 2 is set up identically to case 1 except that the center of the eddy x c x c y c varies in time 3 x c m t where m 8 10 3 m s 1 to generate a steady southwesterly drift of approximately 0 7 km d 1 comparable to the observed eddy drift in haza et al 2016 over the course of 14 days particles in this second case traverse a 2d spiral pattern over time as seen in fig 1b 3 1 3 case 3 drifting mesoscale eddy with submesoscale noise case 3 introduces time dependent velocity fluctuations to the drifting eddy of case 2 in the form of periodic pulses with amplitude a 1 5 m s 1 frequency ω 2 π t and period t 4 h 4a u a sin ω t 2 a cos 1 5 ω t 4b v a cos ω t 2 a sin 1 5 ω t adding four unique pulse signals to the mean flow eq 4 simulates the combined signals of multiple coinciding forcing mechanisms that one expects in nature the resulting flow pattern fig 1c can be likened to a mesoscale eddy surrounded by smaller submesoscale circulations it is worth noting that in this simulation all particles remain trapped within the mesoscale circulation though this is seldom true in nature where scales are dynamically interconnected see haza et al 2016 a more realistic scenario of interacting scales of motion will be considered in the case study described in section 4 sensitivity studies have demonstrated that a minimum of o 200 400 drifters are necessary to measure and quantify mesoscale submesoscale interaction dynamics özgökmen et al 2011 2012 the strong dependence of local transport on submesoscales dictates that real ocean applications would ideally involve at least o 300 drifters deployed within o 1 10 km region we therefore chose to release 360 particles in each simulation this allows a neural network to be trained using 270 or 75 see section 3 1 4 of them within the threshold for capturing scale interactions we later consider the inevitable practical challenges of deploying such quantities of drifters by decreasing the number of particles to more operationally realistic quantities in the eddy case study in all three cases particles are evenly distributed around a circle of radius 70 103 m from the center of the eddy and released simultaneously they are advected for 14 days in cases 1 and 3 and for 29 days in case 2 in order to capture the spiral trajectory effect by making a complete revolution around the eddy the trajectories of these particles hereafter the real trajectories are determined by applying a fourth order runge kutta rk4 integrator dormand and prince 1980 to eq 2 with data sampling frequency δ s t we choose δ s t 3 h to be comparable to the frequency of the hycom output used in the case study described in section 4 velocity components u 1 v 1 at each time step are found by substituting new positions x 1 y 1 into eq 2 3 1 4 neural network configurations we now describe the setup of our artificial neural networks relative to the data and to the problem at hand specifics of the neural network architectures development and training process are detailed in appendix we engineer anns to predict a particle s future velocity at some time t o δ p t where subscript p indicates a prediction time window thus for every ann input v t o the target output is v t o δ p t and each v t o v t o δ p t pair constitutes a unique example the choice of δ p t is based more on engineering considerations than on physics if the time series is predictable the ann will learn whatever time interval it is trained with longer prediction time windows lead to flatter error curves over a forecast period but at decreased forecast resolution such decisions are highly problem specific but allow for many possibilities such as creating a system of anns with some making longer forecasts while others are trained to make shorter forecasts we choose δ p t 6 h for two reasons first having a different prediction window than the observational time step i e δ p t δ s t provides a better sense of how well the network learns the data instead of simply predicting the next time step the network must predict two steps into the future the second reason relates to an error propagation problem when making forecasts beyond δ p t in this configuration a 24 h forecast would be made recursively starting with the most recent velocity observations followed by the successive outputs 5 h 24 h x h 18 h h 12 h h 6 h v 0 h where h x represents the ann s hypothesis for the next velocities we seek to minimize this error propagation while still maintaining a prediction window that is small enough to be practical in applications such as oil spill forecasting we refer to anns configured in this manner as one to one anns because one observation is used to predict one future state if velocity v is decomposed into zonal and meridional components u v then each observation becomes a vector containing two attributes each attribute is assigned to an input and an output neuron as shown in fig 2 network weights lines in fig 2 are initiated randomly from a univariate gaussian distribution of mean 0 and variance 1 due to the time dependence and high variability of ocean dynamics we combine a floating window approach to training first proposed by kubat 1989 with a continuous or so called online learning paradigm as follows of the 360 particles in each experiment 75 or 270 are used for training and the remaining 25 are reserved for testing the size of the training data set is fixed to include only the most recent 24 hours worth of observations with 270 training particles and 24 3 8 observations day this amounts to 2160 training examples at any given time observations are weighed at each training step according to how recent they are by assigning to the oldest 12 hours worth of observations a weight of 1 to the next 6 h a weight of 2 and to the most recent 6 h a weight of 3 where the weight indicates how many times the example is duplicated in the training set weighing the observations introduces repetition to the training process and increases the number of training examples to 3780 the network is initially trained for 100 epochs using the observations from day 1 where one epoch indicates one round through every training example subsequent observations from each particle are then added to the training set one time step δ s t at a time to mimic for example a lagrangian drifter reporting its position every n hours old examples are discarded as new data become available and training continues for another 100 epochs each time the training set is updated this happens a total of 104 13 d 8 obs d times for cases 1 and 3 224 times for case 2 thus by the end of the simulation the anns have been trained for a total of either 105 000 or 225 000 epochs such extensive training greatly increases the risk of overfitting to combat this we recognize that each training epoch essentially produces a new version of the ann we therefore select the best model out of 100 at the end of every training session see appendix this allows for three possible scenarios after updating the training set throughout the simulation 1 if the model benefited from all additional training the final version of the model after epoch 100 is retained 2 if training the model for an additional 100 epochs causes the model to overfit such that it performed better before the additional training the original version of the model is retained 3 if the model benefited from some additional training but began to overfit during the training session the version of the model just before overfitting began is retained using an ann with the fewest possible hidden neurons also aids against overfitting since the regression flexibility of an ann is proportional to the number of hidden neurons in addition to the one to one networks we also develop time series anns that take as input a time series of data rather than a single observation this better reflects the physical reality that a particle s next location can often be anticipated from its most recent n positions or velocities under this configuration each particle constitutes an individual training example containing 16 attributes 8 observations per day 2 velocity components this results in a tenfold decrease in the number of training examples available i e from 2160 to 270 at any given time the time series is not weighted for these anns because doing so would increase the number of attributes per example rather than the number of examples thus it does not introduce the same repetition as it does with the one to one networks instead it would require an unnecessarily larger network with more input and hidden neurons while extended forecasts can be made recursively with one to one anns using eq 5 this is not so straightforward with time series anns when δ p t δ s t these anns are engineered to expect the input vector to contain observations with time step δ s t if after training a vector containing a different time step is fed through the network the output will be nonsensical thus in the case where δ p t δ s t if the ann predicts a single future time step v t o δ p t then one would need to interpolate between the predicted velocity and the most recent input velocity in order to update the input time series and feed it back to the ann even more problematically if δ p t δ s t and the ann predicts only one future time step extended forecasts are altogether impossible without extrapolation a solution to this problem is to engineer the time series ann to predict multiple time steps simultaneously where both δ p t and the number of time steps to predict are determined by the specific problem we therefore engineer our time series anns to issue a 24 hr forecast as a set of velocities at t o i δ p t where δ p t 6 h and i 1 2 3 4 as illustrated in fig 3 the learning rate η an increment that controls network weight adjustment during training see appendix is set to 10 1 for one to one and time series anns in cases 1 and 2 trial and error indicated that the complexity of case 3 requires slower learning so η 1 0 2 for both network configurations all anns contain a single hidden layer for simplicity for the one to one anns where each example is composed of two attributes we use 10 hidden neurons while for the time series anns we use 20 hidden neurons due to the larger number of attributes per example because anns are initialized randomly a network may occasionally learn faster or slower depending on where in solution space the initial weights are located we account for this by randomly initializing an ensemble of three unique anns of both configurations for each experiment thus a total of 18 different anns are trained and tested during the three proof of concept simulations all particles are assumed to be released at 00 00 on day 1 as the particles circle the eddy 24 h forecasts for all 90 test particles are issued every midnight starting on day 2 using the most recent version of the network hereafter we refer to the times at which forecasts are initiated as forecast times and the times for which velocities are predicted i e 06 00 12 00 18 00 and 24 00 as prediction times thus each trial produces a total of 13 forecasts in cases 1 and 3 and 28 forecasts in case 2 where each forecast includes four predictions for 90 particles 3 1 5 assessment metrics the most fundamental assessment metric for any prediction model is rudimentary persistence a mere continuation in both space and time of the most recent observation it is commonly used because of its simplicity but it also sets the lowest possible standard because ocean particle prediction is a notoriously difficult problem it is logical to set low expectations at least at first thus we first compare ann prediction errors to those of persistence predictions in each experiment for a far more rigorous metric we employ statistical autoregressive integrated moving average arima models that make predictions based on linear combinations of time series lags and the lagged forecast errors these models contain three parameters that must be tuned for any given time series first the autoregression parameter p indicates the number of lags to be included in the model and is determined by the correlations between an observation and each previous observation an integrated or differencing technique subtracts an observation from the previous observation in an attempt at making the time series stationary a parameter d specifies how many times this differencing is performed finally the moving average parameter q specifies the number of lags to be used in a moving window average that is compared to the forecast residual error in most time series applications the optimal combinations of p d and q are determined by brute force trial and error here however with o 1 0 4 time series to fit this is hardly feasible we instead use a parameter search function auto arima in r from forecast package r 3 4 2 using default arguments to fit a unique arima model to normalized u and v time series for each of the 360 particles like with the anns this is done every midnight and 24 h arima predictions are made for each time series the time series are allowed to grow continuously such that at the beginning of the simulation the arima models are crudely fit on only one days worth of observations while the models on the last day are fit on the full time series let the error between predicted velocity u p v p and the real target values u r v r for all i prediction times 06 12 18 24 h j test particles k forecasts issued throughout the simulation and l ann ensemble members per experiment be defined as 6 e i j k l u i j k l r u i j k l p 2 v i j k l r v i j k l p 2 1 2 averaging over j k and l quantifies the 24 h forecast errors for the entire experiment 7 e i 1 p f m j 1 p k 1 f l 1 m e j k l where p 90 particles f 13 for cases 1 and 3 and f 28 for case 2 and m 3 ensemble members average forecast errors are calculated between the ann predictions and those from both persistence and arima models 3 2 results proof of concept predicted trajectories are generated by splitting successive predicted velocities into 30 min increments using linear interpolation in order to minimize rk4 time step integration errors ann performance for cases 1 2 and 3 are summarized in figs 4 5 and 6 respectively a representative test particle trajectory during the last two days of the simulation is shown in panel a by a solid gray line with forecast times marked by circles the dotted blue and dashed green lines illustrate the trajectories predicted by the one to one and time series anns respectively arima predicted trajectories are indicated by orange dashed dotted lines and persistence predictions by solid red for comparison forecast errors from eqs 6 7 for velocity and position are shown in panels b and c respectively with error bars indicating the spread of the forecasts given by one standard deviation from the mean error the simple rotational flow of case 1 and the spiral trajectories of case 2 proved to be trivial scenarios for both the one to one and the time series anns to learn predicted trajectories from both networks are visually indistinguishable from each other and also from the real trajectory itself figs 4 and 5a this offered stark improvement over the persistence forecasts which as expected for circular trajectories are always tangent to the eddy both networks for case 1 had similar velocity errors of approximately 0 7 0 4 cm s 1 and position errors around 50 50 m at hour 6 increasing to around 450 300 m at hour 24 case 2 velocity error increased from about 0 3 0 3 cm s 1 at hour 6 to about 0 7 0 5 cm s 1 at hour 24 for the one to one ann which corresponded to position errors around 32 30 m at hour 6 increasing to around 740 550 m at hour 24 the time series ann velocity errors were fairly constant around 0 4 0 3 cm s 1 throughout the entire prediction window leading to position errors increasing from around 47 38 m at hour 6 to around 224 216 m at hour 24 the arima ensemble exhibited a smaller standard deviation than the anns especially in case 1 but the average error increased almost linearly throughout the prediction window while the one to one ann behaved similarly the time series ann mean error remained fairly constant in both cases the slight underperformance by the one to one ann relative to the time series ann beyond 12 h can be traced to the first forecast issued 00 00 on day 2 this early version of the one to one ann produced velocity prediction errors at hour 24 of nearly 7 cm s 1 for several particles in case 1 and 4 cm s 1 in case 2 these outliers aside both anns performed equally well in both of these cases case 3 is more interesting this time arima and persistence were comparable and the one to one ann performed just as poorly as both while the time series ann offered notable predictive improvement the trajectory in fig 6a illustrates a loop on day 13 that scales as a submesoscale feature a 180 turn like this might be observed in nature if a change in weather pattern such as the passage of a strong atmospheric front reverses the direction of the surface flow over the course of several hours the one to one ann predicted a curved trajectory blue dots but fell far short of anticipating the sharp curve that the particle undertook the arima model predicted a similar curved trajectory as the one to one ann but the velocities were nearly twice those of the ann this took the particle twice as far away consequently by the end of the 24 h period the particle ended up being about 55 km away from its predicted position this was only mildly better than the persistence prediction which put the particle roughly 75 km away from its actual position in comparison the time series ann green dashes successfully predicted the particle s change in direction on days 13 and 14 with the final location being about 3 km away from the prediction error analysis revealed that the mean velocity error at hour 24 for the time series ann was about 10 9 cm s 1 compared to 52 44 cm s 1 for the one to one ann which led to a 5 fold decrease in position error these results are corroborated by other particles with a variety of trajectory shapes as demonstrated in fig 7 most trajectories contain sections that are relatively easy to predict for example the last few days in fig 7d and other sections that are more challenging such as the loops in the northern and southern regions of the domain overall passing a time series of observations to the network as input significantly improves the ann s predictive ability highlighting the importance of network architecture and problem setup 4 application gulf of mexico mesoscale eddy 4 1 methods to test the anns in realistic ocean flows we set up a case study using output from a high resolution 1 12 hybrid coordinate ocean model hycom for a mesoscale eddy observed in the gulf of mexico during january 2010 see prasad and hogan 2007 and haza et al 2016 for full details of model configuration this anticyclonic mesoscale circulation surrounded by submesoscale structures provides a known example of interacting scales of motion and a realistic manifestation of our proof of concept case 3 particles are advected through the eddy in the same manner as before 360 equally spaced particles released simultaneously at 00h on 15 january 2010 in a circle of radius 70 103 m a rough estimate of the perimeter of the mesoscale circulation particles are advected for 14 days ending on 28 january hycom eulerian velocity fields were generated every 3 h thus spatial and temporal interpolations are necessary when implementing the rk4 integrator to produce trajectories from the model output acceleration fields are calculated between successive velocity output in order to interpolate velocity on time steps smaller than 3 h spatial interpolation for both velocity and acceleration is carried out using cubic 2d interpolation data setup network configurations and training methods are as described in section 3 1 the time series ann results of case 3 raise questions regarding sensitivity to the chosen prediction window if the forecast is extended beyond 24 h will the time series ann perform just as well or will its error climb to those of the other models we investigate this in the context of this realistic eddy by considering 72 h predictions at each forecast time without changing the configuration of our anns the output time series is interpolated to 3 h time steps using linear interpolation and passed back to the network twice as described in section 3 1 4 acknowledging the practical challenges of deploying hundreds of drifters at sea we also consider the sensitivity to the number of test particles by repeating this 72 h forecast simulation releasing a total of 90 and 45 particles here we utilize a 5 fold cross validation technique to assess the skill of the networks in each scenario k fold validation is easily implemented by randomly dividing all particles into k 5 groups without replacement each of these groups is then taken to be unique set of test particles and the remaining particles become the training examples for the given k fold this allows every particle to be predicted once by an ann instead of initializing three unique random anns as above we seed the random number generator so that every ann starts in the same solution space and its evolution depends only on the particles in the respective training set finally we explore different numbers of hidden layers 1 2 and 3 and hidden neurons 10 20 50 100 for one to one 20 50 100 200 for time series anns using 360 particles three uniquely initialized models are trained for each test as before to account for variations in learning due to random initialization we hereafter refer to the hycom velocity output as observations in order to differentiate them from the ann predicted velocities 4 2 results gulf of mexico mesoscale eddy application the gulf of mexico mesoscale eddy provides a far more realistic representation of what was idealized in case 3 above one important difference is that the particles in this scenario are no longer trapped within the mesoscale circulation instead some escape mostly from the northern rim of the eddy while some end up being drawn into the center of the eddy and still others stay around the perimeter of the mesoscale circulation fig 8 a second difference is that the fluctuating velocity component is no longer sinusoidal or at the very least there are enough interacting dynamics that any predictable periodicity is much harder to discern this enables us to test whether or not the time series anns in case 3 were actually learning the superimposed periodic signals if they were then they should struggle to learn these realistic trajectories fig 9 summarizes the error results for the one to one and time series anns trained on these eddy trajectories the arima errors both mean and standard deviation are indistinguishable from persistence and like with the proof of concept case 3 the one to one ann does not offer any improvement over either metric the velocity and position error plots indicate that on average the one to one ann predictions at hour 24 were worse than both of these models the representative test particle shown in fig 9a illustrates how this might happen the one to one ann predicted for day 13 that the particle would continue its slight clockwise curve that it had been tracing the day before instead the particle curved back to the left before moving due west as a result its final position ended up closer to the persistence prediction than the one to one prediction while this seems fluky a similar situation played out on day 14 the time series ann on the other hand better predicted the slightly wavy trajectory this may be because the particle s trajectory during day 12 was also somewhat s shaped the time series ann velocity errors leveled off around 7 2 4 5 cm s 1 and the position errors climbed from around 1 km at hour 6 to around 5 3 3 km at hour 24 this also illustrates the error cascade from velocity to position even if velocity errors stop growing they will still propagate through to positions and affect the entire predicted trajectory arima forecasts for days 13 and 14 were not very different from those of the time series ann interpretation of the arima results will be discussed further in the next section time series ann performance on longer range 72 h forecasts is shown in fig 10 for a total of 360 90 and 45 particles with 5 fold validation these networks were trained with 288 72 and 36 particles respectively the dashed gray lines indicate the forecast iteration times when the model output were used to generate the next 24 h prediction arima models were fit as before using the auto arima routine but full 72 h predictions were made at each forecast time all model errors were calculated using eqs 6 7 fig 10 shows mean error with one standard deviation several notable take aways follow from fig 10 first after the initial 24 h ann mean velocity error steadily increases towards though never surpasses the mean arima model error roughly doubling from almost 10cms 1 at hour 24 to approximately 23cms 1 at hour 72 secondly while ann position error increases 5 fold from approximately 5km at hour 24 to approximately 25km at hour 72 it remains well below that of arima through hour 72 such that even the upper bounds of the ann error envelope are at or only slightly greater than the arima mean position error this is promising because position is often of greater interest to the particle transport problem than velocity third the ann standard deviations remain smaller throughout the prediction window finally for the first half of the prediction window 0 36h the mean error for the anns with 360 particles is slightly lower than the ann with 90 particle and the error spread somewhat smaller as should be expected and similarly for 90 particles versus 45 after hour 36 there is little statistical difference between the three configurations though the 360 particle ann has a smaller position forecast spread throughout the entire window we discuss each of these outcomes in the next section we next investigate whether changing the size number of neurons and depth number of layers of both one to one and time series anns helps the networks learn to predict trajectories in realistic flows let the convention m n describe these networks by the number of hidden layers m and the number of hidden neurons in each layer n to assess the performance of each network configuration we calculate the relative error between the real target values x r and the predicted values x p 8 re i j k l x i j k l r x i j k l p x i j k l r where x r is either the ann output or the persistence prediction and the subscripts are as defined in section 3 2 fig 11 shows relative error ratio between positions derived from persistence predictions and those derived from ann predictions averaged over i j and k 9 ℜ l 1 g p f i 1 g j 1 p k 1 f x i j k real x i j k pers x i j k real x i j k pred where g 4 predictions per forecast p 90 particles f 13 forecasts for the eddy simulation error bars indicate standard error all ratios are 1 indicating that every ann outperformed persistence despite larger networks taking five to six times longer to train no appreciable difference in performance is observed as the anns increase in complexity one explanation for this is that the hyperparameters such as learning rate and number of training epochs were not tuned for each model but rather were kept constant in order to assess the affect of changing either m or n on model performance finally we consider whether or not the neural networks continued to learn throughout the simulations due to the continuous training method fig 12 shows position error from eq 6 averaged over i prediction times and j test particles for the one to one anns blue dots time series ann green dashes and arima model red dash dots the arima model exhibits a clear downward trend from an average position error of almost 9 km at the start of the simulation around 2 km on the last day this can be attributed to the fact that early in the simulation the time series were too short to successfully regress in these scenarios the arima forecasts were nothing more than the time mean of the few available observations as the time series grew this became less of a problem and the arima models acquired greater skill in contrast no significant trends are observed for any of the neural networks though the one to one anns seem to improve from around 9 km on 16 jan to around 5 km by 28 jan overall we conclude that while the majority of the learning by both network configurations took place during the initial training session subsequent training routines kept the networks up to date using the latest information for time varying domains this is equally as important as initial learning 5 discussion we demonstrated preliminary success at predicting particle transport in the ocean using simple artificial neural networks two questions motivated this study first inspired by the ability of human scientists to discern patterns in oceanic flows e g rupolo 2007 we asked can a data driven machine learning technique accomplish a similar task the goal was to extract enough information from ground truth observations to make respectable forecasts instead of starting with primitive equations the second question was can a data driven algorithm learn to make accurate ocean forecasts without knowing anything whatsoever about the underlying physics if so these tools could potentially supplement existing theory driven ogcms the proof of concept experiment provided a starting point for testing idealized ocean trajectory patterns such as simple inertial oscillations e g beron vera et al 2015 gough et al 2016 and interacting scales of motion e g haza et al 2016 the most promising results were the abilities of these simple anns to predict trajectories within and around a realistic gulf of mexico mesoscale eddy we attribute this success at least partially to the continuous learning training scheme whereby training continues every time the oldest observation is replaced with a new one while the more common approach is to train a neural network once using a large data set and then use it to make predictions of new data this would require having in advance an impractically large training set consisting of full trajectories for millions of particles instead we treated the domain as continuously varying in time and allowed the network to keep up with the latest available observations the contrasting performance of one to one and time series anns in realistic flows demonstrated the importance of using theory to guide model development e g faghmous et al 2014 the one to one anns have the operational advantage of only requiring one velocity observation to make a forecast but trained this way they lack any concept of time and can only remember the most recently seen example changing the input to a time series of observations significantly improved performance since the network was able to consider where the particle had been over some period of time this enhanced performance is to be expected given the fundamental nature of time series and is also in agreement with the first notable discussions on handling time and memory in learning paradigms by jordan 1986 and elman 1990 1991 it is reasonable to ask how the time series neural network managed to outperform even arima forecasts over a 24 h prediction window the primary reason for this is that arima models are inherently sensitive to time series length the wide arima forecast spread large standard deviations in figs 6 9 and 10 is due to the time series early in the simulation being too short to be properly regressed e g 16 18 jan in fig 12 in these cases auto arima returned models whose forecasts were merely the time mean of the few available observations at least 72 h of observations were necessary before skill became comparable to the one to one network where it remained for most of the simulation 18 26 jan not until the last three days of the simulation did the arima model have a sufficient number of data points to make predictions similar to those of the time series ann while the arima models could only be fit to individual time series the anns had the advantage of being able to learn from an assortment of unique time series thus both the one to one and time series anns demonstrated greater skill than arima during the first two days of the simulation fig 12 it is worth noting that the skill of both ann configurations came from seeing only 24 h worth of observations at any given time and the majority of the networks learning occurred during the initial training session that is the training leading up to the 06 jan forecast shown in fig 12 anns may therefore be favorable for generating time sensitive forecasts soon after deploying observational drifters whereas one may need to wait several days before a traditional regression model could provide any skilled insight it is well known that model accuracy inevitably breaks down when a learning machine is used to make predictions outside of the domain in which it was trained this is precisely why the predictive capability of our ann erodes beyond 24 h fig 10 this shortcoming might be overcome by modifying our ann to instead predict the full 72 h time window but doing so would require target data at times t o 6 h t o 12 h t o 72 h for each training example in addition to input observations from the previous 24 h thus such a configuration would require 96 h of training data a final caveat of our experimental setup worthy of consideration is that although the anns are evaluated on particles not seen during training predictions are made over the same time period as the training target data this is fundamentally different from time series regression models that do not require future data while both arima and anns are function approximation models the neural network represents a single mapping function developed through exposure to many representative time series in order to describe other similar time series this also explains the relatively small sensitivity to the number of training particles the unseen particles are most likely to be predicted relative to proximate training particles thus our results suggest that one can use only o 100 drifters to forecast where intermediate drifters would go as long as the drifters all sample the same dynamic features this is especially advantageous for the oceanographer since deploying vast arrays of drifters is expensive and often infeasible we conclude by commenting on overfitting an inherent risk that plagues all machine learning paradigms wherein a model learns the training data too well and then fails to extrapolate to new data the anns in proof of concept cases 1 and 2 for example may have been prone to overfitting as evidenced by velocity prediction errors greater than what might be expected for such trivial cases fig 5 and especially fig 4 aside from increasing the size of the training set which is often not possible overfitting can be minimized by decreasing the degrees of freedom of the model or minimizing the amount of training the first strategy is easily accomplished by using the fewest possible hidden neurons on the other hand minimizing the amount of training is less trivial for systems that evolve continuously in time such systems often require continuous learning despite the increased risk of overfitting we tested this by repeating the eddy simulation in the same way as described above except that the time series anns were not trained at all following the initial training on day 1 this resulted in forecast errors that were consistently on par with those of both persistence and the worst arima forecasts throughout the entire simulation not shown confirming that online learning was required ultimately the selection of the best version of the ann at each training session compromised between the need to keep the network up to date with the system dynamics while recognizing that at any given time little or even no additional training may actually be required to summarize anns may be beneficial in real ocean scenarios when i numerous time series are available that collectively contain useful information to assist with forecasting ii available time series are too short for regression models e g at the beginning of a coordinated drifter deployment or iii predicting the transport of material within a local domain is necessary using observations dispersed within the domain e g predicting the spread of an oil plume using a limited collection of drifters 6 conclusions we have demonstrated that particle trajectories in a variety of simulated and modeled oceanic flow regimes can be learned by simple artificial neural networks the fundamental idea is to move away from relying on primitive equations of motion for prediction as these are impossible to initialize and solve for all scales of motion in the ocean instead the goal is to develop a predictive tool that learns from ground truth observational data without any direct physical guidance we started with a proof of concept exploration using a hierarchy of idealized flows and then tested our approach on realistic flows generated by a high resolution hybrid coordinate ocean model for a mesoscale eddy in the northern gulf of mexico this eddy provided a particularly illustrative example of interacting mesoscale and submesoscale dynamics and we showed that our approach to ann training performed well even for these complex flow fields we tested two approaches to ann training in the first configuration the network predicted a drifting particle s velocity at time t o 6 h using only its previous velocity at time t o this approach had the operational advantage of requiring only one velocity observation to make a forecast but predictive performance suffered due to a lack of internal memory within the ann we also developed networks that predicted the drifting particle s velocity at time t o i δ t i 1 2 3 4 based on the last 24 hours worth of velocity measurements this time series ann performed between two and six times better than its one to one counterpart for both velocity and position estimates depending on the complexity of the flow the predictive capabilities of the time series anns can be attributed to several things first was allowing the ann to see and learn from a time series of data second we restricted our training data to the past 24 h of data replacing old observations with the most recent this helped the ann forget regime shifts within the data set such as a particle sharply changing direction or behavior as may result from a strong weather event we adopted a continuous training approach whereby ann training continued whenever new data became available this allowed the ann to always be up to date with the constantly changing domain since real ocean drifters continuously provide new data this training technique does not seem unreasonable in fact we argue that it presents a highly efficient and effective way of utilizing real time data for ocean prediction finally and most importantly the time series ann is able to learn from many independent trajectories and these time series can be as short as 24 h at 3 h increments this is a fundamental advantage over traditional regression models that are time series specific and in the case of ocean particles require at least 72 h of observations before any predictive skill is discernible this study addressed the question of whether a neural network could learn to predict a particle s velocity based only on its previous velocities an important distinction is in order between learning particle trajectories and learning the dynamics of the system in which the particles exist the latter is a much more difficult problem because the sparsity of observational data means that the dynamics of the ocean are seldom known a posteriori learning to interpolate point wise time series is often the best one can do when relying only on observations from an operational standpoint this should not be a concern predicting the spread of an oil spill does not necessarily require learning or even knowing the full dynamics but rather predicting how certain sections of a plume may evolve based on nearby observations the eddy application results suggest there is reason to be optimistic that anns will be able to learn observed trajectories just as well thus future work will involve testing this predictive approach on real drifter data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was made possible by a grant from the gulf of mexico research initiative united states data are publicly available through the gulf of mexico research initiative information data cooperative griidc at https data gulfresearchinitiative org doi 10 7266 n7bc3wjc the authors thank the three anonymous reviewers whose constructive suggestions and recommendations greatly improved this manuscript appendix the artificial neural network a 1 brief overview an artificial neural network ann is a biologically inspired nonlinear regression model that is trained to map input data to desired output values groups of nodes called neurons are arranged in layers such that adjacent layers are fully interconnected by weighted links but no neurons within the same layer are connected to each other see figs 2 and 3 in the text layers in between the input and output layers are commonly referred to as hidden layers because they are inside the network and the intermediate data mappings they produce are usually not known to the network engineer all neurons consist of a trainable bias or zero th weight that is added to the neuron s input signal and an activation function usually chosen from families of functions that return values either on the interval 1 1 or 0 1 e g duch and jankowski 1999 activation functions reduce the dimensionality of the data being passed through successive layers in the ann all weights and biases are initialized as small random numbers we take these from a univariate gaussian distribution of mean 0 and variance 1 training is the process by which solution space is searched for the optimum combination of these weights and biases such that the network output most accurately matches desired target values a common approach is to pass training examples a subset of all available data to the ann one at a time and compare the output to the corresponding target values for each example if the output differs from the target the error is back propagated through the network to quantify each neuron s contribution to the overall error and small adjustments are made to the weights and biases the network is always tested and evaluated on data not seen during training we now describe this training process and the details about the ann configuration used in this study the discussion closely follows kubat 2017 for a comprehensive and illustrative overview of neural networks see also nielsen 2014 a 2 network configuration and training consider a training example x defined as a vector containing n attributes describing that particular example x x 1 x 2 x n each attribute is assigned its own input neuron in the ann thus the one to one ann described in section 3 1 4 has n 2 input neurons where the attributes are horizontal velocity components for the time series ann each example contains n 16 attributes 8 observations 2 velocity components see section 3 1 4 in the text so these networks required 16 input neurons the simplest possible ann contains a single hidden layer and as few hidden neurons as possible we choose 10 hidden neurons for the one to one network and 20 hidden neurons for the time series ann determining these sizes is more art than science and we therefore conduct sensitivity tests in section 4 let the weights connecting the k th attribute to the j th hidden neuron and the j th hidden neuron to the i th output neuron be denoted w k j and w j i respectively similarly let the biases for the k output neurons and the j hidden neurons be denoted b k and b j respectively a sigmoid transfer function of the form a 1 f x 1 1 e σ is chosen for all hidden and output neurons where f x is the neuron output and σ is the weighted sum from the previous layer in the network e g a 2 σ j 1 w 11 x 1 w 21 x 2 w n 1 x n b 1 each example is passed through the network as weighted sums of the attributes to produce an output vector y y 1 y 2 y i a 3 y i f j w j i f k w k j x k b j b i in the case of the one to one anns the output vector is the same length i 2 as the input vector thus these networks also have i 2 output neurons one corresponding to zonal velocity and the other to meridional velocity while the time series anns contained i 8 output neurons 4 predictions 2 velocity components eq a 3 describes the behavior of a simple feed forward neural network note that since eq a 1 returns values on the interval 0 1 the output of the ann will also be on this interval thus all data are normalized to this interval before being presented to the network and all output must be returned to the range of the original data this requires that the minimum and maximum values of the original data set be retained with the ann and used to normalize all future data on which the trained ann might be run to train the ann we first define a target vector r r 1 r 2 r n for each velocity observation here r corresponds to a given particle s velocity at time t o δ p t where t o is the issue time of the forecast and δ p t is the forecast prediction time step each training example is passed through the network eq a 3 and the ann output is compared to the target vector prediction error is then back propagated through the network to quantify each neuron s responsibility to the overall error and the weights and biases are adjusted accordingly a 4a w j i w j i η δ i h j b i b i η δ i a 4b w k j w k j η δ j x k b j b j η δ j where η 0 1 is a small learning rate h j is the output of the j th hidden neuron and δ i δ j are quantifications of the neurons error responsibility a 5a δ i y i 1 y i r i y i a 5b δ j h j 1 h j i δ i w j i presenting the last training example to the ann indicates the completion of one training epoch the ann performance is then evaluated using a cost function whose value is to be minimized during the training process the cost function in this study is taken to be mean squared error a 6 m s e 1 n i 1 n r i y i 2 training continues for a defined number of epochs or until some performance threshold has been achieved once trained the ann is run by presenting new examples one at a time predictions are made using eq a 3 and the optimum weights and biases a 3 final remarks the strength of anns is their theoretical ability to fit any function adding neurons to an ann introduces more weights to adjust and is comparable to introducing higher order terms to a nonlinear regression equation adding additional hidden layers increases the number of times the data are transformed within the model in order to reduce dimensionality as the network attempts to map the input vector to the desired output signal like all regression models however neural networks are prone to overfitting the training data and thereby performing poorly on new data this risk can often be minimized by decreasing the size of the network i e the number of hidden neurons increasing the training data set or by shortening the training time here we adapt the smallest possible ann and treat the completion of each training epoch as producing a new version of the ann and take the best of these that is the one with smallest m s e at the end of each 100 epoch training session another disadvantage of these models is that they often lack interpretability see section 5 nevertheless they can be ideal tools for problems in which one need not know why a particular outcome occurs as discussed in the text this is often true for predicting material transport in the ocean 
