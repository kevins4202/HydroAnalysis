index,text
4460,satellite based products can provide valuable information for model calibrations and evaluations however effectively and efficiently constraining hydrological models with both satellite based information and streamflow measurements remains a challenge here a parallel computing based and spatially stepwise strategy that enables separate optimizations of different objective functions in a spatially distributed and parallel manner was proposed for model calibration with streamflow observations and satellite based evapotranspiration et the new calibration strategy m5 was tested with the soil and water assessment tool swat model in the upper heihe river basin of china a mountainous watershed on the northeastern qinghai tibet plateau the performance of m5 was evaluated and compared with two single variable calibration strategies i e streamflow only calibration and et only calibration and with two multivariate calibration methods i e joint calibration and stepwise calibration results indicate that m5 achieves the best model performance among the five calibration strategies in reproducing temporal variations of streamflow and et moreover m5 improves the simulation of the spatial pattern of et and attains a higher spatial efficiency than the other calibration strategies m5 also exhibits a higher computational efficiency with a magnitude up to 2 times greater than the other calibration strategies due to the application of parallel computing further analysis demonstrates that the new calibration strategy can lead to a synergic relationship between the simulation accuracy of streamflow and et underscoring the added value of satellite based products for model calibrations keywords model calibration remote sensing computational efficiency swat satellite based et heihe river basin 1 introduction hydrological models play an important role in water resource management and are crucial for understanding complex watershed systems model calibration is a prerequisite for achieving reliable hydrological simulations molina navarro et al 2017 hydrological models are usually calibrated against streamflow observations which represent integrated hydrological responses to natural and anthropogenic drivers dembélé et al 2020 streamflow only calibration can ensure the water balance of the studied basin to some extent but does not guarantee the correct representation of internal hydrological processes such as the spatial patterns of evapotranspiration et and soil moisture clark et al 2015 rakovec et al 2016b stisen et al 2018 zink et al 2018 both of which are critical for drought monitoring flood forecasting and land atmosphere feedback investigations a single variable calibration can lead to a pseudo accurate model that produces many behavioral simulations i e the issue of equifinality in inverse modeling beven and freer 2001 rajib et al 2018a wambura et al 2018 and gets the right answers but for the wrong reasons kirchner 2006 at present there seems to be a consensus in the hydrological community that in addition to streamflow measurements some other hydrological observations are warranted to reduce model uncertainties and improve the realism of hydrological models silvestro et al 2015 rakovec et al 2016a baroni et al 2019 over the last few decades remote sensing rs technology has considerably advanced lettenmaier et al 2015 many rs based estimates of hydrological fluxes and states are now available at the global scale with increasing spatiotemporal resolutions tang et al 2009 nijzink et al 2018 satellite based observations considerably improve our understanding of the water and atmospheric cycles munier et al 2014 markonis et al 2019 tapley et al 2019 and promote a variety of hydrometeorological applications brocca et al 2018 sheffield et al 2018 brombacher et al 2020 in particular satellite based observations can provide valuable information for model calibration koch et al 2016 koppa et al 2019 or be incorporated into hydrological models through data assimilation rajib et al 2018b ma et al 2019 azimi et al 2020 in recent years increasing efforts have been made to constrain the parameters of hydrological models using various satellite based products including surface soil moisture and temperature wanders et al 2014 kundu et al 2017 zink et al 2018 et immerzeel and droogers 2008 odusanya et al 2019 zhang et al 2020b total water storage rakovec et al 2016a and snow cover han et al 2019 schattan et al 2020 or some combination of such products rakovec et al 2016b nijzink et al 2018 dembélé et al 2020 the referenced studies demonstrate that satellite based products have great potential to improve hydrological modeling especially with respect to internal hydrological processes for model calibration satellite based products can be used alone lópez lópez et al 2017 ruiz pérez et al 2017 odusanya et al 2019 zhang et al 2020b or in combination with streamflow measurements sutanudjaja et al 2014 rajib et al 2016 li et al 2018 nijzink et al 2018 dembélé et al 2020 nemri and kinnard 2020 however the former method has achieved only limited success since horizontal fluxes cannot be constrained without streamflow observations rajib et al 2018a hence the combined use of satellite based products and streamflow observations is preferred for model calibration a number of previous studies have reported the superiority of multivariate calibration with both streamflow and satellite based observations to the single variable calibration in identifying parameters and reducing model uncertainty rientjes et al 2013 silvestro et al 2015 kundu et al 2017 franco et al 2020 nevertheless effectively and efficiently constraining hydrological models with both satellite based information and streamflow observations is still a challenging task silvestro et al 2015 and many issues persist first inconsistent findings exist regarding the accuracy relationship between streamflow and satellite based variable a number of studies have reported a trade off relationship or a well defined pareto front between the simulation accuracy of the satellite based variable and streamflow wanders et al 2014 herman et al 2018 zink et al 2018 dembélé et al 2020 while some other studies have found a synergic or an irrelevant relationship rajib et al 2016 2018a li et al 2018 these conflicting results might be attributed to the utilization of different satellite based products objective functions and parametrization and calibration strategies rakovec et al 2016a demirel et al 2018 rajib et al 2018a koppa et al 2019 second various strategies including joint simultaneous calibration rajib et al 2016 demirel et al 2018 stepwise calibration sutanudjaja et al 2014 lópez lópez et al 2017 herman et al 2018 and spatially stepwise i e basin by basin calibration niraula et al 2015 molina navarro et al 2017 rajib et al 2018a have been proposed for multivariate model calibration however few studies have made comparisons among these different calibration strategies in terms of their effectiveness and computational efficiency furthermore the modeling results of multivariate calibrations have seldom been compared to the calibrations based solely on satellite based products koppa et al 2019 this lack of research hinders evaluations of the added value obtained from the incorporation of satellite based products third automatic calibration algorithms which are relatively less subjective than manual calibration methods fenicia et al 2007 are typically adopted for multivariate calibrations these algorithms usually demand the model to run thousands of times resulting in a high computational cost especially for high resolution modeling over large domains parallel computing technique can reduce computational burden of automatic calibration schemes rouholahnejad et al 2012 zhang et al 2013 cibin and chaubey 2015 the combination of parallel computing with satellite based products could be useful in computationally efficient estimations of spatially distributed model parameters however this method has not yet received much attention in this study we proposed a parallel computing based and spatially stepwise strategy for model calibration with streamflow observations and satellite based et the new calibration strategy was tested with the soil and water assessment tool swat model in the upper heihe river basin of china a meso scale mountainous watershed on the northeastern qinghai tibet plateau more specifically our objectives were to i evaluate the effectiveness and efficiency of the new calibration strategy ii compare the new calibration strategy with four other calibration methods i e streamflow only et only joint and stepwise calibrations and iii analyze the relationship synergy or trade off between the simulation accuracy of et and streamflow under multivariate calibrations 2 materials and methods 2 1 study area the heihe river basin is the second largest inland river basin in the arid region of northwest china the upstream area of the heihe river basin uhrb lying between 98 5 e and 101 5 e longitude and between 37 5 n and 39 5 n latitude is a meso scale mountainous watershed on the northeastern qinghai tibetan plateau with a drainage area of approximately 10 000 km2 the runoff generated in the uhrb is the major water resource for agricultural and ecological development in the midstream and downstream areas zhang et al 2019 as shown in fig 1 the eastern tributary of the uhrb is monitored by the qilian hydrological station while the western tributary is monitored by the zhama station the two tributaries jointly contribute to streamflow at the basin outlet where the yingluoxia station is located the uhrb has steep terrain with elevations ranging from 1675 to 5013 m above sea level the climate is continental alpine with a mean annual temperature less than 1 and average precipitation of approximately 450 mm year for the 1960 2014 period zhang et al 2018 snow plays an important role in the hydrology of the uhrb the contribution of snowmelt to streamflow is approximately 15 li et al 2019 the primary land use and cover types are grassland and barren land zhang et al 2019 accounting for 69 2 and 24 2 of the uhrb respectively the major soil types include alpine meadow soil alpine chestnut soil subalpine shrub meadow soil and alpine frost desert soil zhang et al 2018 2 2 datasets as listed in table 1 two categories of data were used in this study the first category of data includes the meteorological data the digital elevation model dem the land use and cover map and the soil type map these data were used for the model setup and parametrization observations of precipitation wind speed relative humidity and maximum and minimum temperature at four national level stations marked as red stars in fig 1 were obtained from the national meteorological information center nmic precipitation data measured at four local rainfall gauges marked as green stars in fig 1 were additionally collected from the water resources bulletin of gansu province wrbgp dem data with a spatial resolution of 90 90 m was clipped from the aster global dem and was mainly used for watershed delineation and calculations of subbasin parameters the land use and cover map in the year 2011 was derived from 30 m landsat tm etm images and was collected from the science data center for cold and arid regions sdccar in total seven land use and cover types were classified in this map farmland grassland forest urban land wetland barren land and water bodies the soil type map with a scale of 1 1 000 000 originated from the second state soil survey of china the latest nation wide soil survey zhang et al 2018 the patterns of land use and cover and soil types were mainly used for the generations of the hrus hydrological response units the second category of data includes streamflow observations and satellite based et these data were used for the model calibration and evaluation the daily streamflow observations at the qilian zhama and yingluoxia hydrological stations were obtained from the wrbgp the monthly satellite based et product with a high spatial resolution of 1 1 km was generated via an operational software system etwatch and was collected from the sdccar etwatch is an integrated algorithm of the residual approach and the penman monteith model wu et al 2012 the et product of etwatch is reliable and has been widely used in model evaluations and water balance analyses moiwo et al 2011a 2011b sun et al 2018 zhou et al 2018 2 3 swat swat is a comprehensive process based and semi distributed eco hydrological model neitsch et al 2011 and is one of the most widely used hydrological models worldwide abbaspour et al 2019 the swat model divides a watershed into subbasins and further subdivides each subbasin into multiple hrus the hru is the basic modeling unit that represents a unique combination of land use and cover soil type and terrain slope each hru is assumed to be homogeneous with respect to its hydrologic response the swat model was originally designed to predict influences of land management practices on water sediment and agricultural chemical yields and has now been widely used for a variety of applications such as hydrological impact assessments zhang et al 2015 2019 water accounting delavar et al 2020 and drought and flood modeling tan et al 2020 in this study the arcswat interface an extension of arcgis software was used to set up the swat model version 2012 a threshold drainage area of 200 km2 was selected to divide the study area into 23 subbasins these subbasins were further discretized to 667 hrus by setting the area thresholds of the land use and cover soil and slope to 1 5 and 10 respectively owing to the lack of a uniform criterion these thresholds were mainly determined according to our multiyear modeling experience zhang et al 2015 2016 2018 and the previous swat related studies gassman et al 2007 rajib et al 2016 2018b the thresholds were chosen to ensure a balance between the detailed representation of the spatial heterogeneity of the watershed and computational efficiency five elevation bands were defined for each subbasin to account for orographic influences on precipitation and temperature patterns the penman monteith method was chosen to estimate potential evapotranspiration the soil conservation service curve number scs cn method was selected to simulate surface runoff the variable storage approach was adopted to route channel flow 2 4 sufi 2 optimization algorithm the sequential uncertainty fitting version 2 sufi 2 algorithm abbaspour et al 2004 embedded in the swat cup software was used to calibrate the swat model automatically sufi 2 is a robust tool used for model calibrations and uncertainty analyses based on a global search procedure and can deal with a large lumber of parameters through latin hypercube sampling abbaspour et al 2004 2015 as shown in table a1 thirteen parameters related to the processes of channel routing et and surface and subsurface runoff were selected for model calibration the descriptions and initial ranges of the calibrated parameters as well as their sensitivity ranks to streamflow and et simulations determined through global sensitivity analyses were presented in the table several other parameters that also play important roles in hydrological modeling over the mountainous uhrb include the precipitation lapse rate plaps snowmelt temperature sftm snowmelt base temperature smtmp melt factor for snow on june 21 smfmx melt factor for snow on december 21 smfmn and snowpack temperature lag factor timp the plaps can exert a profound impact on precipitation pattern estimates and was derived based on the linear relationship between the mean annual precipitation and the elevations of rain gauges in and around the uhrb fig 1 owing to the lack of snow observations the snow related parameters were determined using the sufi 2 algorithm with a preliminary iteration of 300 simulations by comparing the streamflow simulations against the observations following the study of rajib et al 2018a and the suggestion of the model developer https groups google com g swat cup c r0lzgpuds70 after determining the snow related parameters and plaps these values were kept constant during the implementations of the five calibration strategies as described in the next section to enable a fair comparison among them 2 5 calibration strategies as shown in fig 2 this study adopted five strategies to calibrate the swat model streamflow only calibration m1 et only calibration m2 joint calibration m3 stepwise calibration m4 and parallel computing based and spatially stepwise calibration m5 in m1 swat was calibrated based solely on streamflow observations while m2 calibrated the model based solely on satellite based et m3 calibrated swat with both streamflow observations and satellite based et in a lumped manner m4 first calibrated swat with satellite based et and then with streamflow observations m5 first calibrated the model separately for the 23 subbasins in a parallel manner based on satellite based et and then calibrated the model for the 3 sub watersheds as delineated by the hydrological stations using the streamflow observations here m1 and m2 served as the benchmarks with which to better evaluate the multivariate calibration strategies m3 m5 m3 and m4 were two commonly used multivariate calibration methods and were selected herein to compare with m5 to measure the degree of performance improvement and to analyze the impact of calibration strategy on the accuracy relationship synergy or trade off between et and streamflow simulations the sufi 2 algorithm embedded in swat cup was executed with a total of 6 iterations to optimize the streamflow related and et related parameters in m1 m2 and m3 we set the number of swat simulations to 300 consistently in each sufi 2 iteration in total there were 300 6 1800 simulations conducted in m1 m2 and m3 with respect to stepwise calibration m4 the sufi 2 algorithm embedded in swat cup was first executed with 3 iterations to calibrate the et related parameters and then with an additional 3 iterations to calibrate the streamflow related parameters similarly there were 300 3 300 3 1800 simulations conducted in m4 in the spatially stepwise calibration m5 the parallel computing based sufi 2 algorithm was first executed with 3 iterations to calibrate et related parameters at the subbasin level and then with an additional 6 iterations 3 for the upstream sub watersheds and 3 for the downstream sub watershed to calibrate the streamflow related parameters at the sub watershed level resulting in 300 9 2700 simulations it should be noted that in m5 all the subbasins of the uhrb were actually calibrated with 6 iterations of sufi 2 instead of with 9 in line with the other calibration strategies this would facilitate the fair comparison of m5 with the other calibration methods all computations were accomplished with a lenovo notebook pc with an intel core i9980h cpu 16 gb of ram and 8 processor cores more details about the calibration strategies were presented in the following subsections 2 5 1 single variable calibration strategies benchmarks m1 and m2 the streamflow only calibration m1 was implemented with sufi 2 by comparing the simulations against the daily streamflow observations of the zhama qilian and yingluoxia stations while the et only calibration m2 was implemented by comparing the simulations against the monthly satellite based et of the 23 subbasins fig 1 the objective functions of m1 and m2 were formatted by combining the objective functions at different hydrological stations or subbasins to a single function as shown in eqs 1 and 2 below 1 f o i 1 n w i kge i stream 2 f e k 1 m w k kge k et 3 kge 1 1 c c 2 1 b e t a 2 1 g a m a 2 4 cc j 1 t s j s mean o j o mean j 1 t s j s mean 2 j 1 n o i o mean 2 5 beta μ s μ o 6 gama σ s σ o where n and m are the total numbers of hydrological stations and subbasins which are 3 and 23 respectively wi is the weight assigned to the kling gupta efficiency kge for streamflow at station i which was set as 1 n wk is the weight assigned to the kge for et at subbasin k which was set as 1 m s and o are the simulations and observations respectively µ and σ represent the mean and standard deviation respectively t is the total number of time steps and j is the time step the kge is a multiple component performance metric proposed by gupta et al 2009 the components of kge include cc beta and gama and represent the linear correlation bias and relative temporal variability between the simulations and observations respectively the et related parameters may not be well constrained in m1 since swat is calibrated solely against streamflow observations we hypothesize that m1 will lead to reasonable streamflow simulations but unreasonable et simulations in contrast the streamflow related parameters may not be well constrained in m2 since swat is calibrated exclusively against satellite based et we hypothesize that m2 will lead to reasonable simulations of et but unreasonable simulations of streamflow 2 5 2 multivariate calibration strategies joint and stepwise calibrations m3 and m4 the joint calibration m3 was conducted with sufi 2 by comparing the simulations of swat against both streamflow observations and satellite based et following the method of madsen 2000 we defined the objective function of m3 as in eq 7 to fairly weigh the objective functions of et and streamflow 7 f mv o e o a o 2 e a e 2 8 a o m a x o m i n e m i n o m i n 9 a e m a x o m i n e m i n e m i n where o 1 f o and e 1 f e ao and ae are the transformation constants assigned to the objective functions of streamflow and et respectively and o m i n and e m i n represent the minimum values of o and e in an initial sufi 2 based iteration with 300 simulations fmv oe ensures that the objective functions of et and streamflow have the same distance to the origin and helps to identify a parameter set that represents a balance point on the pareto front silvestro et al 2015 since the swat model is calibrated against both streamflow observations and satellite based et in m3 we hypothesize that m3 will achieve reasonable streamflow and et simulations simultaneously the stepwise calibration m4 has been widely used in multivariate model calibrations sutanudjaja et al 2014 lópez lópez et al 2017 herman et al 2018 nijzink et al 2018 nemri and kinnard 2020 m4 was performed with sufi 2 by first comparing the simulations against the satellite based et of the 23 subbasins and then against the streamflow observations obtained at the 3 hydrological stations the parameters closely related and sensitive to et simulations including esco epco tlaps and canmx table a1 were first optimized based on the objective function fe eq 2 afterwards the remaining nine streamflow related parameters were optimized using the objective function fo eq 1 m4 calibrates the et related and streamflow related parameters separately and attempts to find a solution that provides the best simulations for both et and streamflow rather than a compromised or balanced solution between et and streamflow as obtained in m3 we hypothesize that m4 will perform better than m3 2 5 3 a parallel computing based and spatially stepwise calibration strategy m5 inspired by the works of cibin and chaubey 2015 rajib et al 2018a and yang et al 2019 a parallel computing based and spatially stepwise calibration strategy m5 was proposed in this study m5 is similar to m4 except in m5 the objective functions of the different subbasins and hydrological stations were optimized in a separate and parallel manner in the first step as illustrated in fig 3 the et related parameters in each of 23 subbasins were calibrated separately and simultaneously using parallel computing based on the objective function fe sub eq 10 in the second step the streamflow related parameters were first calibrated in parallel for the two upstream sub watersheds i e the eastern and western tributaries and then for the downstream sub watershed based on the objective function fo sub eq 11 the stepwise calibration of streamflow related parameters was necessary since the streamflow simulations of the downstream sub watershed depend on those of the upstream sub watersheds if the three sub watersheds are calibrated simultaneously the boundary condition of the optimization problem in the downstream sub watershed will be constantly changing resulting in the non convergence of the optimization algorithm eq 10 and eq 11 are expressed as follows 10 f e s u b kge et i i 1 2 23 11 f o s u b kge stream k k 1 2 3 where kge et s u b is the kge for et at subbasin i and kge stream s u b is the kge for streamflow at the outlet of sub watershed k a flowchart of the implementation of the new calibration strategy with swat is shown in fig 4 the approach consists of two steps i e the calibrations of et related parameters step 01 and streamflow related parameters step 02 at each step the initial ranges of the calibrated parameters the number of iterations m and the number of simulations in the current iteration are first defined the parameter sets of the different simulations are then sampled through the latin hypercube sampling lhs method afterwards the swat model runs in parallel for the current iteration based on the single program multiple data spmd function of the matlab parallel computing toolbox running swat involves the sequential execution of sufi2 make input exe swat edit exe swat exe and sfui2 extract exe these programs update the calibrated parameters in the input files execute the swat model and extract the outputs of interest the asterisk of the program sfui2 extract exe refers to the rch and sub files that summarize the model outputs at the reach and subbasin scales herein the parallel running of swat is similar to the parallel processing module of the swat cup rouholahnejad et al 2012 but it is not licensed finally the sufi 2 algorithm runs in parallel for the different subbasins based on the parfor loop of the matlab parallel computing toolbox the parallel running of sufi 2 involves the sequential execution of sufi2 goal fn m sufi2 95ppu exe sufi2 95ppu beh exe and sufi2 new pars exe these programs calculate the objective function the 95 prediction uncertainty and the new parameter ranges at each subbasin for the next iteration we developed the program sufi2 goal fn m to enable model performance evaluations with any customized objective functions in the second step step 02 the above procedures are repeated with the differences that the streamflow related parameters are optimized and the sub watershed delineated by the hydrological stations are calibrated meanwhile there might be sub steps in step 02 as shown in our case in fig 3 the parallel computing based sufi 2 algorithm is compatible with the swat cup and the calibration results can be viewed directly with swat cup m5 calibrates the et related and streamflow related parameters in each subbasin or sub watershed separately and attempts to find a solution that provides the best simulations for both et and streamflow at the subbasin or sub watershed scale rather than a compromised or balanced solution between et and streamflow and among subbasins as obtained in m3 and m4 we hypothesize that m5 will lead to the best model performance among the five calibration strategies 2 6 evaluation of the calibration strategies the streamflow simulation accuracy of swat was evaluated via the kge and its components including cc beta and gama eqs 3 4 5 and 6 the ranges of the kge cc beta and gama are 1 1 1 0 and 0 respectively the closer the kge and its components are to 1 the better the model performance is the simulation accuracy of et was assessed using the kge and the spatial efficiency spaef proposed by demirel et al 2018 as defined in eq 12 12 spaef e 1 1 c c 2 1 β 2 1 γ 2 13 cc ρ o b s s i m 14 β σ sim μ sim σ obs μ obs 15 γ j 1 g m i n k j l j j 1 m l j where cc is the linear correlation coefficient between the observed and simulated patterns β is the fraction of the coefficient of variation representing the spatial variability γ is the percentage of histogram intersection for a given histogram l of the satellite based et pattern and the histogram k of the simulated pattern and g is the number of bins contained in each histogram the z score of the spatial pattern was used to estimate γ to ensure bias insensitivity and to enable the comparison of two variables with different units koch et al 2018 the spaef ranges from to 1 and the better the performance is the closer the value is to 1 furthermore a combined performance metric fcomb defined as in eq 16 was used to assess the overall performance of swat in reproducing the temporal dynamics of streamflow and et and the spatial pattern of et following the study of hulsman et al 2021 16 f comb 1 1 3 1 f o 2 1 f e 2 1 spaef e 2 where fo fe and spaefe are defined as in eqs 1 2 and 15 respectively fcomb ranges between and 1 with perfect performance if the value reaches 1 in addition an efficiency metric ce defined as the reciprocal value of the run time of the sufi 2 algorithm was used to evaluate computational efficiency of the calibration strategies 3 results 3 1 model performance for streamflow table 2 shows the values of the kge and its components cc gama and beta for the daily streamflow simulations obtained with the different calibration strategies unsurprisingly the et only calibration m2 produces the lowest streamflow simulation accuracy kge 0 among the five strategies at the three hydrological stations in the calibration and validation periods as shown in fig a1 and fig 5 many erroneous flow peaks are generated in the wet season from may to september this can be attributed to the higher values of the parameters cn2 and surlag table a2 in m2 than in the other calibration methods a higher cn2 leads to more surface runoff while a higher surlag induces a more rapid release of surface runoff to the main channel neitsch et al 2011 the streamflow only calibration m1 leads to satisfactory streamflow simulations in the calibration kge 0 73 and validation kge 0 60 periods however some small flow peaks can be seen on the hydrographs of the qilian and zhama stations during the dry and cold period january to march this can be explained by the relatively low value of the tlaps parameter table a2 such a low value would lead to overestimations of temperature and consequently misclassifications of snowfall as rainfall over the mountainous uhrb the joint calibration m3 results in relatively lower streamflow simulation accuracy than m1 especially at the yingluoxia station during the calibration and validation periods as shown in fig a1 b and fig 5 b the hydrograph simulated with m3 is ahead in time with respect to the observed hydrograph which might be due to the higher channel hydraulic conductivity ch k2 the stepwise calibration m4 brings about comparable model performances to those of m1 with kge values higher than 0 75 and 0 60 respectively in the calibration and validation periods the spatially stepwise calibration m5 achieves higher streamflow simulation accuracy than m3 and attains comparable precision to m1 and m4 in the calibration period moreover m5 performs best among the five calibration strategies in the validation period comparing the components of the kge table 2 it is found that the cc value in m5 is comparable to those in m1 m3 and m4 while the beta and gama values are closer to 1 in m5 than those in m1 m3 and m4 this indicates that the better model performance obtained with m5 relative to those obtained with m1 m3 and m4 is mainly due to the lower streamflow simulation biases and the better simulations of the temporal streamflow variations taking a closer look at fig a1 and fig 5 we can find that the rising limbs of the hydrographs tend to be simulated worse than the receding limbs in all of the calibration strategies this is possibly due to the deficiencies of swat in representing the soil freezing thawing and snowmelt processes both of which have a significant impact on streamflow generation at the time of a shift from a cold to a warm period march to may 3 2 model performance for et 3 2 1 temporal agreement between the satellite based et and the simulations fig 6 shows the comparisons of the satellite based et with the simulations obtained with the different calibration strategies at the basin scale as expected the streamflow only calibration m1 leads to the lowest et simulation accuracy among the five calibration strategies with kge values less than 0 10 the simulated et shows a significant overestimation in m1 especially in the wet season from may to september tis results can be attributed to the low value of the esco parameter table a2 the other calibrations m2 m5 generate reasonable et simulations achieving kge values that range from 0 80 to 0 87 nevertheless et tends to be underestimated during the period from january to july in most years in line with our previous study zhang et al 2020a this result might be associated with the non simulated soil freezing thawing processes and inaccurate climatic forcing data the et only calibration m2 achieves the best model performance in the calibration period which is understandable since the model was calibrated solely on et in the validation period we can see that the spatially stepwise calibration method m5 obtains the highest kge indicating a better model performance than the other calibration strategies fig 7 depicts the histograms of the kge for the subbasin level et simulations obtained with the different calibration strategies similarly m1 leads to worse model performances than the other calibration methods with kge less than 0 01 at most of the subbasins the median kge value reaches 0 71 in m2 during the calibration period and this value is higher than those of m3 and m4 in the validation period comparable median kge values are obtained with m1 m3 and m4 the spatially stepwise calibration m5 results in the highest median kge values among the five calibrations the kge values in m5 are 0 76 and 0 78 respectively in the calibration and validation periods moreover comparing the histograms of the kge we can observe that the use of m5 leads to more subbasins with higher kge values than the other calibration strategies regardless the results demonstrate that m5 performs better than the other strategies in reproducing the temporal variations of et 3 2 2 spatial agreement between the simulated and satellite based et patterns fig 8 compares the satellite based et with the simulations obtained with the different calibration strategies at the subbasin scale we can see that the simulated et maps obtained with the calibration strategies m1 to m4 exhibit poor spatial agreement with the satellite based map with spaef values less than 0 15 the simulated et map agrees reasonably well with the satellite based et pattern in the spatially stepwise calibration m5 achieving a higher spaef value than the other calibration strategies at the subbasin scale as shown in fig 8 g k the simulated and satellite based et are closer to the 1 1 line in m5 relative to those in m1 m4 during the calibration and validation periods the results imply that the use of m5 improves the simulation of the spatial pattern of et this is understandable because the et related parameters are optimized in a spatially distributed manner in m5 instead of in a lumped manner as in m1 to m4 which enables divergent optimal parameter sets among the different subbasins as shown in fig a2 the improvement of the subbasin level et simulations can be primarily attributed to the better model performance in the wet season from may to september in the dry season from october to april the subbasin level et simulations show poor agreement with the satellite based et in all of the calibration strategies achieving low cc 0 30 and spaef 0 10 values this can be explained by two reasons one is that the dry season et are of lower magnitudes and temporal variabilities than the wet season et and the optimization algorithm would implicitly give a higher weight to the wet season et simulations the other reason is that the soil freezing thawing processes that occur in the dry season and significantly affect the soil water contents and et are not considered by the swat model 3 3 effectiveness and efficiency of the calibration strategies fig 9 a and b show the values of the combined performance metric fcomb and its components in the calibration and validation periods the spatially stepwise calibration m5 achieves the highest overall model performance among the five calibration strategies with an fcomb value higher than 0 55 moreover the components of fcomb including spaefe fe and fo are consistently higher in m5 than those in the other calibration strategies during the validation period the et only calibration m2 obtains the lowest fcomb value 0 25 mainly due to its poor streamflow simulations the joint m3 and stepwise m4 calibrations have comparable fcomb values that are higher than that of m1 as presented in fig 9 c m5 has the highest computational efficiency ce due the application of parallel computing the improvement of the ce can reach up to 2 times greater in m5 than the other calibration strategies specifically the run time of the sufi 2 algorithm embedded in the swat cup is approximately 9 12 h in m1 to m4 while that of the parallel computing based sufi 2 algorithm is approximately about 4 h in m5 taken together the results demonstrate that the new calibration strategy is more reliable and computationally efficient than the other calibration strategies 4 discussion 4 1 trade off or synergy in accuracy between streamflow and et a comparison of multivariate calibration with single variable calibrations i e the benchmarks will aid in understanding the accuracy relationship trade off or synergy between the calibrated variables this understanding is essential for assessing the added value from the incorporation of satellite based products in model calibrations in the study the joint calibration m3 obtains lower fo and fe values than the benchmarks table 3 implying a trade off in accuracy between the simulated et and streamflow the stepwise calibration m4 has comparable model performances to the benchmarks indicating an irrelevant relationship between the simulation accuracy of streamflow and et the spatially stepwise calibration m5 however achieves higher fo and fe values than the benchmarks implying a synergy in accuracy between streamflow and et simulations we agree with rajib et al 2018a that m3 has only achieved a sub optimal parameter set because it attempts to optimize the different objective functions in a lumped manner thus inducing an averaging effect in other words the calibrated parameter set is optimal for the combined objective function of the objectives at the 23 subbasins and 3 hydrological stations but not for each of them this averaging effect could be ameliorated by the stepwise calibration technique m4 because in m4 the objective functions of streamflow and et are optimized in two separate stages the spatially stepwise calibration m5 enables separate optimizations of the different objective functions in a spatially distributed manner m5 can move beyond the averaging effect and leads to simultaneous improvements in streamflow and et simulations relative to the benchmarks highlighting the added value of the satellite based products for model calibrations 4 2 pros and cons of the new calibration strategy the new calibration strategy m5 has several advantages over the joint and stepwise calibrations m3 and m4 first as mentioned above m5 can move beyond the averaging effect of the calibration with a combined objective function by optimizing each of the objective functions separately moreover m5 has a higher computation efficiency due to the application of parallel computing most importantly m5 enables model calibration in a spatially distributed manner that can effectively use the spatial and temporal information of satellite based products as a result m5 can attain better model performances than the other calibration strategies when reproducing both the temporal variations and spatial patterns of et we admit that m5 has some deficiencies first the implicit assumption of a complementary relationship between streamflow and et might not hold true for streamflow and other satellite based variables as demonstrated by koppa et al 2019 second the parameters associated with different hydrological responses need to be distinguished before the stepwise calibration can be conducted this however is not an easy task and requires a deep understanding of hydrological processes especially for complex hydrological models with hundreds of parameters third m5 may add equifinality to the model since the parameters of each subbasin or sub watershed are calibrated separately subbasins with similar hydrological properties might have quit different optimal parameter sets last m5 is considered as the stepwise calibration method and it attempts to determine a single solution that can best match the available observations it does not provide the alternative combinations of the calibrated parameters that yield equally good results fenicia et al 2007 this leads to great challenges in identifying behavioral simulations and assessing model uncertainties fenicia et al 2007 koppa et al 2019 nevertheless we believe the new calibration strategy would lead to lower parameter and model output uncertainty than other calibration methods sutanudjaja et al 2014 the calibration strategies from m1 to m4 only adopt the temporal information of the observations to use in the model calibrations while m5 utilizes both the temporal and spatial information of the observations to screen out one optimal parameter set in other words m5 imposes more constraints on the parameter optimization problem than the other calibration methods furthermore owing to the separate optimization of the objective functions m5 can avoid the trade offs or balances among the different objective functions this leads to an easier convergence of the objective function to its optimum value in comparison to the other calibration strategies resulting in smaller ranges of parameters and model outputs her and chaubey 2015 rajib et al 2018a additionally m5 has a proportion of the calibrated parameters being fixed at each step allowing smaller degrees of freedom in fitting the measurements 4 3 limitations and uncertainties the study is subject to several limitations and uncertainties first the new calibration strategy was tested with swat using streamflow observations and satellite based et other satellite based products such as soil moisture and snow cover products could also provide valuable information for model calibrations rakovec et al 2016b kundu et al 2017 han et al 2019 but these products were not considered in the current study further efforts are needed to investigate whether the new calibration strategy and the its underlying assumptions are still applicable to streamflow and other satellite based observations second a single satellite based et product was used in this study with the assumption that it is free of errors as has been done in many previous studies immerzeel and droogers 2008 rajib et al 2018a odusanya et al 2019 zhang et al 2020b however satellite based products are inherently subjected to some biases due to their indirect estimates of hydrological variables zink et al 2018 dembélé et al 2020 the limitations of retrieve algorithms and inaccuracies in the meteorological forcings the use of a single satellite based et product therefore adds some uncertainty to our findings these uncertainties might be reduced by using synthesized satellite based products elnashar et al 2020 or by shifting from the utilization of absolute satellite based estimates to relative ones i e the spatial pattern information as has been by many researchers demirel et al 2018 nijzink et al 2018 wambura et al 2018 zink et al 2018 dembélé et al 2020 furthermore monthly satellite based et product instead of the daily et product were used in this study to constrain the swat model the new calibration strategy m5 needs to be further tested with the daily satellite based et product considering the high model sensitivity to fast changing climatic conditions baroni et al 2019 third the choice of objective function has a profound impact on the calibration results molina navarro et al 2017 abbaspour et al 2018 this study formulated the objection functions of the calibration strategies of m1 to m4 by summing the equally weighted kge values at the different hydrological stations or subbasins following the methods of the previous studies abbaspour et al 2015 rajib et al 2016 2018a dembélé et al 2020 however other performance metrics such as the nash sutcliffe efficiency nse and the coefficient of determination r 2 and other weight setting methods such as the inverse error weighting stisen et al 2018 dembélé et al 2020 and the drainage area based approach gong et al 2012 were not tested furthermore this study did not test the calibration strategies with growing numbers of sufi 2 iterations we argue that m5 would still perform better than the other calibration methods even with more sufi 2 iterations for two reasons i the number of iterations set as 6 in the present study is higher than those used in many other studies rajib et al 2016 2018a franco et al 2020 and is high enough for the objective function to converge around the optimal value and ii the advantages of m5 over the other calibration strategies section 4 2 can guarantee its better performance 5 conclusions in this study we proposed a parallel computing based and spatially stepwise strategy to calibrate the swat model with streamflow observations and satellite based et the effectiveness and computational efficiency of the new calibration strategy m5 was demonstrated using the case of the upper heihe river basin of china a meso scale mountainous watershed on the northeastern qinghai tibet plateau the performance of m5 was compared with those of the streamflow only calibration m1 et only calibration m2 joint calibration m3 and stepwise calibration m4 the results indicate that m1 achieves satisfactory streamflow simulations but leads to unreasonable et simulations m2 attains satisfactory et simulations but result in unreasonable streamflow simulations the joint calibration m3 yields reasonable et and streamflows simulations simultaneously and has a higher et simulation accuracy than m1 and a greater streamflow accuracy than m2 the stepwise calibration m4 has a better model performance than m3 due to the alleviation of the tradeoffs between the objective functions of et and streamflow the spatially stepwise calibration m5 generates the best streamflow and et simulations among the five calibration strategies m5 also improves the simulation of the spatial pattern of et and has a higher computational efficiency than the other calibration strategies taken together the results demonstrate that the new calibration strategy is more reliable and computationally efficient than the other calibration methods further analysis implies that m5 can lead to a synergic relationship between the simulation accuracy of et and streamflow highlighting the added value of the satellite based products for model calibrations m5 enables separate optimizations of the objective functions in a spatially distributed and parallel manner so that it can effectively and efficiently use the spatial and temporal information of the satellite based products nevertheless we admit that m5 has some deficiencies such as the implicit assumption of a complementary relationship between the calibrated variables and the need to distinguish between the parameters related to different hydrological responses we further point out the limitations and uncertainties of the study along with the prospects of future work 6 code availability the code used to calibrate swat based on the parallel computing based and spatially stepwise strategy is freely available at https github com hydrors parallelswat credit authorship contribution statement ling zhang conceptualization funding acquisition investigation software methodology resources visualization writing original draft writing review editing yanbo zhao investigation resources visualization writing review editing qimin ma data curation formal analysis writing review editing penglong wang software investigation yingchun ge resources visualization writing review editing wenjun yu investigation validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the major project of china high resolution earth observation system 21 y20b01 9001 19 22 the strategic priority research program of chinese academy of sciences xda20100104 the national natural science foundation of china 41901045 and the cas light of west china program 29y929661 the authors would like to thank dr mehmet c demirel for sharing the code used to calculate the spatial efficiency the careful reviews and constructive comments of the anonymous referees are greatly appreciated appendix a 
4460,satellite based products can provide valuable information for model calibrations and evaluations however effectively and efficiently constraining hydrological models with both satellite based information and streamflow measurements remains a challenge here a parallel computing based and spatially stepwise strategy that enables separate optimizations of different objective functions in a spatially distributed and parallel manner was proposed for model calibration with streamflow observations and satellite based evapotranspiration et the new calibration strategy m5 was tested with the soil and water assessment tool swat model in the upper heihe river basin of china a mountainous watershed on the northeastern qinghai tibet plateau the performance of m5 was evaluated and compared with two single variable calibration strategies i e streamflow only calibration and et only calibration and with two multivariate calibration methods i e joint calibration and stepwise calibration results indicate that m5 achieves the best model performance among the five calibration strategies in reproducing temporal variations of streamflow and et moreover m5 improves the simulation of the spatial pattern of et and attains a higher spatial efficiency than the other calibration strategies m5 also exhibits a higher computational efficiency with a magnitude up to 2 times greater than the other calibration strategies due to the application of parallel computing further analysis demonstrates that the new calibration strategy can lead to a synergic relationship between the simulation accuracy of streamflow and et underscoring the added value of satellite based products for model calibrations keywords model calibration remote sensing computational efficiency swat satellite based et heihe river basin 1 introduction hydrological models play an important role in water resource management and are crucial for understanding complex watershed systems model calibration is a prerequisite for achieving reliable hydrological simulations molina navarro et al 2017 hydrological models are usually calibrated against streamflow observations which represent integrated hydrological responses to natural and anthropogenic drivers dembélé et al 2020 streamflow only calibration can ensure the water balance of the studied basin to some extent but does not guarantee the correct representation of internal hydrological processes such as the spatial patterns of evapotranspiration et and soil moisture clark et al 2015 rakovec et al 2016b stisen et al 2018 zink et al 2018 both of which are critical for drought monitoring flood forecasting and land atmosphere feedback investigations a single variable calibration can lead to a pseudo accurate model that produces many behavioral simulations i e the issue of equifinality in inverse modeling beven and freer 2001 rajib et al 2018a wambura et al 2018 and gets the right answers but for the wrong reasons kirchner 2006 at present there seems to be a consensus in the hydrological community that in addition to streamflow measurements some other hydrological observations are warranted to reduce model uncertainties and improve the realism of hydrological models silvestro et al 2015 rakovec et al 2016a baroni et al 2019 over the last few decades remote sensing rs technology has considerably advanced lettenmaier et al 2015 many rs based estimates of hydrological fluxes and states are now available at the global scale with increasing spatiotemporal resolutions tang et al 2009 nijzink et al 2018 satellite based observations considerably improve our understanding of the water and atmospheric cycles munier et al 2014 markonis et al 2019 tapley et al 2019 and promote a variety of hydrometeorological applications brocca et al 2018 sheffield et al 2018 brombacher et al 2020 in particular satellite based observations can provide valuable information for model calibration koch et al 2016 koppa et al 2019 or be incorporated into hydrological models through data assimilation rajib et al 2018b ma et al 2019 azimi et al 2020 in recent years increasing efforts have been made to constrain the parameters of hydrological models using various satellite based products including surface soil moisture and temperature wanders et al 2014 kundu et al 2017 zink et al 2018 et immerzeel and droogers 2008 odusanya et al 2019 zhang et al 2020b total water storage rakovec et al 2016a and snow cover han et al 2019 schattan et al 2020 or some combination of such products rakovec et al 2016b nijzink et al 2018 dembélé et al 2020 the referenced studies demonstrate that satellite based products have great potential to improve hydrological modeling especially with respect to internal hydrological processes for model calibration satellite based products can be used alone lópez lópez et al 2017 ruiz pérez et al 2017 odusanya et al 2019 zhang et al 2020b or in combination with streamflow measurements sutanudjaja et al 2014 rajib et al 2016 li et al 2018 nijzink et al 2018 dembélé et al 2020 nemri and kinnard 2020 however the former method has achieved only limited success since horizontal fluxes cannot be constrained without streamflow observations rajib et al 2018a hence the combined use of satellite based products and streamflow observations is preferred for model calibration a number of previous studies have reported the superiority of multivariate calibration with both streamflow and satellite based observations to the single variable calibration in identifying parameters and reducing model uncertainty rientjes et al 2013 silvestro et al 2015 kundu et al 2017 franco et al 2020 nevertheless effectively and efficiently constraining hydrological models with both satellite based information and streamflow observations is still a challenging task silvestro et al 2015 and many issues persist first inconsistent findings exist regarding the accuracy relationship between streamflow and satellite based variable a number of studies have reported a trade off relationship or a well defined pareto front between the simulation accuracy of the satellite based variable and streamflow wanders et al 2014 herman et al 2018 zink et al 2018 dembélé et al 2020 while some other studies have found a synergic or an irrelevant relationship rajib et al 2016 2018a li et al 2018 these conflicting results might be attributed to the utilization of different satellite based products objective functions and parametrization and calibration strategies rakovec et al 2016a demirel et al 2018 rajib et al 2018a koppa et al 2019 second various strategies including joint simultaneous calibration rajib et al 2016 demirel et al 2018 stepwise calibration sutanudjaja et al 2014 lópez lópez et al 2017 herman et al 2018 and spatially stepwise i e basin by basin calibration niraula et al 2015 molina navarro et al 2017 rajib et al 2018a have been proposed for multivariate model calibration however few studies have made comparisons among these different calibration strategies in terms of their effectiveness and computational efficiency furthermore the modeling results of multivariate calibrations have seldom been compared to the calibrations based solely on satellite based products koppa et al 2019 this lack of research hinders evaluations of the added value obtained from the incorporation of satellite based products third automatic calibration algorithms which are relatively less subjective than manual calibration methods fenicia et al 2007 are typically adopted for multivariate calibrations these algorithms usually demand the model to run thousands of times resulting in a high computational cost especially for high resolution modeling over large domains parallel computing technique can reduce computational burden of automatic calibration schemes rouholahnejad et al 2012 zhang et al 2013 cibin and chaubey 2015 the combination of parallel computing with satellite based products could be useful in computationally efficient estimations of spatially distributed model parameters however this method has not yet received much attention in this study we proposed a parallel computing based and spatially stepwise strategy for model calibration with streamflow observations and satellite based et the new calibration strategy was tested with the soil and water assessment tool swat model in the upper heihe river basin of china a meso scale mountainous watershed on the northeastern qinghai tibet plateau more specifically our objectives were to i evaluate the effectiveness and efficiency of the new calibration strategy ii compare the new calibration strategy with four other calibration methods i e streamflow only et only joint and stepwise calibrations and iii analyze the relationship synergy or trade off between the simulation accuracy of et and streamflow under multivariate calibrations 2 materials and methods 2 1 study area the heihe river basin is the second largest inland river basin in the arid region of northwest china the upstream area of the heihe river basin uhrb lying between 98 5 e and 101 5 e longitude and between 37 5 n and 39 5 n latitude is a meso scale mountainous watershed on the northeastern qinghai tibetan plateau with a drainage area of approximately 10 000 km2 the runoff generated in the uhrb is the major water resource for agricultural and ecological development in the midstream and downstream areas zhang et al 2019 as shown in fig 1 the eastern tributary of the uhrb is monitored by the qilian hydrological station while the western tributary is monitored by the zhama station the two tributaries jointly contribute to streamflow at the basin outlet where the yingluoxia station is located the uhrb has steep terrain with elevations ranging from 1675 to 5013 m above sea level the climate is continental alpine with a mean annual temperature less than 1 and average precipitation of approximately 450 mm year for the 1960 2014 period zhang et al 2018 snow plays an important role in the hydrology of the uhrb the contribution of snowmelt to streamflow is approximately 15 li et al 2019 the primary land use and cover types are grassland and barren land zhang et al 2019 accounting for 69 2 and 24 2 of the uhrb respectively the major soil types include alpine meadow soil alpine chestnut soil subalpine shrub meadow soil and alpine frost desert soil zhang et al 2018 2 2 datasets as listed in table 1 two categories of data were used in this study the first category of data includes the meteorological data the digital elevation model dem the land use and cover map and the soil type map these data were used for the model setup and parametrization observations of precipitation wind speed relative humidity and maximum and minimum temperature at four national level stations marked as red stars in fig 1 were obtained from the national meteorological information center nmic precipitation data measured at four local rainfall gauges marked as green stars in fig 1 were additionally collected from the water resources bulletin of gansu province wrbgp dem data with a spatial resolution of 90 90 m was clipped from the aster global dem and was mainly used for watershed delineation and calculations of subbasin parameters the land use and cover map in the year 2011 was derived from 30 m landsat tm etm images and was collected from the science data center for cold and arid regions sdccar in total seven land use and cover types were classified in this map farmland grassland forest urban land wetland barren land and water bodies the soil type map with a scale of 1 1 000 000 originated from the second state soil survey of china the latest nation wide soil survey zhang et al 2018 the patterns of land use and cover and soil types were mainly used for the generations of the hrus hydrological response units the second category of data includes streamflow observations and satellite based et these data were used for the model calibration and evaluation the daily streamflow observations at the qilian zhama and yingluoxia hydrological stations were obtained from the wrbgp the monthly satellite based et product with a high spatial resolution of 1 1 km was generated via an operational software system etwatch and was collected from the sdccar etwatch is an integrated algorithm of the residual approach and the penman monteith model wu et al 2012 the et product of etwatch is reliable and has been widely used in model evaluations and water balance analyses moiwo et al 2011a 2011b sun et al 2018 zhou et al 2018 2 3 swat swat is a comprehensive process based and semi distributed eco hydrological model neitsch et al 2011 and is one of the most widely used hydrological models worldwide abbaspour et al 2019 the swat model divides a watershed into subbasins and further subdivides each subbasin into multiple hrus the hru is the basic modeling unit that represents a unique combination of land use and cover soil type and terrain slope each hru is assumed to be homogeneous with respect to its hydrologic response the swat model was originally designed to predict influences of land management practices on water sediment and agricultural chemical yields and has now been widely used for a variety of applications such as hydrological impact assessments zhang et al 2015 2019 water accounting delavar et al 2020 and drought and flood modeling tan et al 2020 in this study the arcswat interface an extension of arcgis software was used to set up the swat model version 2012 a threshold drainage area of 200 km2 was selected to divide the study area into 23 subbasins these subbasins were further discretized to 667 hrus by setting the area thresholds of the land use and cover soil and slope to 1 5 and 10 respectively owing to the lack of a uniform criterion these thresholds were mainly determined according to our multiyear modeling experience zhang et al 2015 2016 2018 and the previous swat related studies gassman et al 2007 rajib et al 2016 2018b the thresholds were chosen to ensure a balance between the detailed representation of the spatial heterogeneity of the watershed and computational efficiency five elevation bands were defined for each subbasin to account for orographic influences on precipitation and temperature patterns the penman monteith method was chosen to estimate potential evapotranspiration the soil conservation service curve number scs cn method was selected to simulate surface runoff the variable storage approach was adopted to route channel flow 2 4 sufi 2 optimization algorithm the sequential uncertainty fitting version 2 sufi 2 algorithm abbaspour et al 2004 embedded in the swat cup software was used to calibrate the swat model automatically sufi 2 is a robust tool used for model calibrations and uncertainty analyses based on a global search procedure and can deal with a large lumber of parameters through latin hypercube sampling abbaspour et al 2004 2015 as shown in table a1 thirteen parameters related to the processes of channel routing et and surface and subsurface runoff were selected for model calibration the descriptions and initial ranges of the calibrated parameters as well as their sensitivity ranks to streamflow and et simulations determined through global sensitivity analyses were presented in the table several other parameters that also play important roles in hydrological modeling over the mountainous uhrb include the precipitation lapse rate plaps snowmelt temperature sftm snowmelt base temperature smtmp melt factor for snow on june 21 smfmx melt factor for snow on december 21 smfmn and snowpack temperature lag factor timp the plaps can exert a profound impact on precipitation pattern estimates and was derived based on the linear relationship between the mean annual precipitation and the elevations of rain gauges in and around the uhrb fig 1 owing to the lack of snow observations the snow related parameters were determined using the sufi 2 algorithm with a preliminary iteration of 300 simulations by comparing the streamflow simulations against the observations following the study of rajib et al 2018a and the suggestion of the model developer https groups google com g swat cup c r0lzgpuds70 after determining the snow related parameters and plaps these values were kept constant during the implementations of the five calibration strategies as described in the next section to enable a fair comparison among them 2 5 calibration strategies as shown in fig 2 this study adopted five strategies to calibrate the swat model streamflow only calibration m1 et only calibration m2 joint calibration m3 stepwise calibration m4 and parallel computing based and spatially stepwise calibration m5 in m1 swat was calibrated based solely on streamflow observations while m2 calibrated the model based solely on satellite based et m3 calibrated swat with both streamflow observations and satellite based et in a lumped manner m4 first calibrated swat with satellite based et and then with streamflow observations m5 first calibrated the model separately for the 23 subbasins in a parallel manner based on satellite based et and then calibrated the model for the 3 sub watersheds as delineated by the hydrological stations using the streamflow observations here m1 and m2 served as the benchmarks with which to better evaluate the multivariate calibration strategies m3 m5 m3 and m4 were two commonly used multivariate calibration methods and were selected herein to compare with m5 to measure the degree of performance improvement and to analyze the impact of calibration strategy on the accuracy relationship synergy or trade off between et and streamflow simulations the sufi 2 algorithm embedded in swat cup was executed with a total of 6 iterations to optimize the streamflow related and et related parameters in m1 m2 and m3 we set the number of swat simulations to 300 consistently in each sufi 2 iteration in total there were 300 6 1800 simulations conducted in m1 m2 and m3 with respect to stepwise calibration m4 the sufi 2 algorithm embedded in swat cup was first executed with 3 iterations to calibrate the et related parameters and then with an additional 3 iterations to calibrate the streamflow related parameters similarly there were 300 3 300 3 1800 simulations conducted in m4 in the spatially stepwise calibration m5 the parallel computing based sufi 2 algorithm was first executed with 3 iterations to calibrate et related parameters at the subbasin level and then with an additional 6 iterations 3 for the upstream sub watersheds and 3 for the downstream sub watershed to calibrate the streamflow related parameters at the sub watershed level resulting in 300 9 2700 simulations it should be noted that in m5 all the subbasins of the uhrb were actually calibrated with 6 iterations of sufi 2 instead of with 9 in line with the other calibration strategies this would facilitate the fair comparison of m5 with the other calibration methods all computations were accomplished with a lenovo notebook pc with an intel core i9980h cpu 16 gb of ram and 8 processor cores more details about the calibration strategies were presented in the following subsections 2 5 1 single variable calibration strategies benchmarks m1 and m2 the streamflow only calibration m1 was implemented with sufi 2 by comparing the simulations against the daily streamflow observations of the zhama qilian and yingluoxia stations while the et only calibration m2 was implemented by comparing the simulations against the monthly satellite based et of the 23 subbasins fig 1 the objective functions of m1 and m2 were formatted by combining the objective functions at different hydrological stations or subbasins to a single function as shown in eqs 1 and 2 below 1 f o i 1 n w i kge i stream 2 f e k 1 m w k kge k et 3 kge 1 1 c c 2 1 b e t a 2 1 g a m a 2 4 cc j 1 t s j s mean o j o mean j 1 t s j s mean 2 j 1 n o i o mean 2 5 beta μ s μ o 6 gama σ s σ o where n and m are the total numbers of hydrological stations and subbasins which are 3 and 23 respectively wi is the weight assigned to the kling gupta efficiency kge for streamflow at station i which was set as 1 n wk is the weight assigned to the kge for et at subbasin k which was set as 1 m s and o are the simulations and observations respectively µ and σ represent the mean and standard deviation respectively t is the total number of time steps and j is the time step the kge is a multiple component performance metric proposed by gupta et al 2009 the components of kge include cc beta and gama and represent the linear correlation bias and relative temporal variability between the simulations and observations respectively the et related parameters may not be well constrained in m1 since swat is calibrated solely against streamflow observations we hypothesize that m1 will lead to reasonable streamflow simulations but unreasonable et simulations in contrast the streamflow related parameters may not be well constrained in m2 since swat is calibrated exclusively against satellite based et we hypothesize that m2 will lead to reasonable simulations of et but unreasonable simulations of streamflow 2 5 2 multivariate calibration strategies joint and stepwise calibrations m3 and m4 the joint calibration m3 was conducted with sufi 2 by comparing the simulations of swat against both streamflow observations and satellite based et following the method of madsen 2000 we defined the objective function of m3 as in eq 7 to fairly weigh the objective functions of et and streamflow 7 f mv o e o a o 2 e a e 2 8 a o m a x o m i n e m i n o m i n 9 a e m a x o m i n e m i n e m i n where o 1 f o and e 1 f e ao and ae are the transformation constants assigned to the objective functions of streamflow and et respectively and o m i n and e m i n represent the minimum values of o and e in an initial sufi 2 based iteration with 300 simulations fmv oe ensures that the objective functions of et and streamflow have the same distance to the origin and helps to identify a parameter set that represents a balance point on the pareto front silvestro et al 2015 since the swat model is calibrated against both streamflow observations and satellite based et in m3 we hypothesize that m3 will achieve reasonable streamflow and et simulations simultaneously the stepwise calibration m4 has been widely used in multivariate model calibrations sutanudjaja et al 2014 lópez lópez et al 2017 herman et al 2018 nijzink et al 2018 nemri and kinnard 2020 m4 was performed with sufi 2 by first comparing the simulations against the satellite based et of the 23 subbasins and then against the streamflow observations obtained at the 3 hydrological stations the parameters closely related and sensitive to et simulations including esco epco tlaps and canmx table a1 were first optimized based on the objective function fe eq 2 afterwards the remaining nine streamflow related parameters were optimized using the objective function fo eq 1 m4 calibrates the et related and streamflow related parameters separately and attempts to find a solution that provides the best simulations for both et and streamflow rather than a compromised or balanced solution between et and streamflow as obtained in m3 we hypothesize that m4 will perform better than m3 2 5 3 a parallel computing based and spatially stepwise calibration strategy m5 inspired by the works of cibin and chaubey 2015 rajib et al 2018a and yang et al 2019 a parallel computing based and spatially stepwise calibration strategy m5 was proposed in this study m5 is similar to m4 except in m5 the objective functions of the different subbasins and hydrological stations were optimized in a separate and parallel manner in the first step as illustrated in fig 3 the et related parameters in each of 23 subbasins were calibrated separately and simultaneously using parallel computing based on the objective function fe sub eq 10 in the second step the streamflow related parameters were first calibrated in parallel for the two upstream sub watersheds i e the eastern and western tributaries and then for the downstream sub watershed based on the objective function fo sub eq 11 the stepwise calibration of streamflow related parameters was necessary since the streamflow simulations of the downstream sub watershed depend on those of the upstream sub watersheds if the three sub watersheds are calibrated simultaneously the boundary condition of the optimization problem in the downstream sub watershed will be constantly changing resulting in the non convergence of the optimization algorithm eq 10 and eq 11 are expressed as follows 10 f e s u b kge et i i 1 2 23 11 f o s u b kge stream k k 1 2 3 where kge et s u b is the kge for et at subbasin i and kge stream s u b is the kge for streamflow at the outlet of sub watershed k a flowchart of the implementation of the new calibration strategy with swat is shown in fig 4 the approach consists of two steps i e the calibrations of et related parameters step 01 and streamflow related parameters step 02 at each step the initial ranges of the calibrated parameters the number of iterations m and the number of simulations in the current iteration are first defined the parameter sets of the different simulations are then sampled through the latin hypercube sampling lhs method afterwards the swat model runs in parallel for the current iteration based on the single program multiple data spmd function of the matlab parallel computing toolbox running swat involves the sequential execution of sufi2 make input exe swat edit exe swat exe and sfui2 extract exe these programs update the calibrated parameters in the input files execute the swat model and extract the outputs of interest the asterisk of the program sfui2 extract exe refers to the rch and sub files that summarize the model outputs at the reach and subbasin scales herein the parallel running of swat is similar to the parallel processing module of the swat cup rouholahnejad et al 2012 but it is not licensed finally the sufi 2 algorithm runs in parallel for the different subbasins based on the parfor loop of the matlab parallel computing toolbox the parallel running of sufi 2 involves the sequential execution of sufi2 goal fn m sufi2 95ppu exe sufi2 95ppu beh exe and sufi2 new pars exe these programs calculate the objective function the 95 prediction uncertainty and the new parameter ranges at each subbasin for the next iteration we developed the program sufi2 goal fn m to enable model performance evaluations with any customized objective functions in the second step step 02 the above procedures are repeated with the differences that the streamflow related parameters are optimized and the sub watershed delineated by the hydrological stations are calibrated meanwhile there might be sub steps in step 02 as shown in our case in fig 3 the parallel computing based sufi 2 algorithm is compatible with the swat cup and the calibration results can be viewed directly with swat cup m5 calibrates the et related and streamflow related parameters in each subbasin or sub watershed separately and attempts to find a solution that provides the best simulations for both et and streamflow at the subbasin or sub watershed scale rather than a compromised or balanced solution between et and streamflow and among subbasins as obtained in m3 and m4 we hypothesize that m5 will lead to the best model performance among the five calibration strategies 2 6 evaluation of the calibration strategies the streamflow simulation accuracy of swat was evaluated via the kge and its components including cc beta and gama eqs 3 4 5 and 6 the ranges of the kge cc beta and gama are 1 1 1 0 and 0 respectively the closer the kge and its components are to 1 the better the model performance is the simulation accuracy of et was assessed using the kge and the spatial efficiency spaef proposed by demirel et al 2018 as defined in eq 12 12 spaef e 1 1 c c 2 1 β 2 1 γ 2 13 cc ρ o b s s i m 14 β σ sim μ sim σ obs μ obs 15 γ j 1 g m i n k j l j j 1 m l j where cc is the linear correlation coefficient between the observed and simulated patterns β is the fraction of the coefficient of variation representing the spatial variability γ is the percentage of histogram intersection for a given histogram l of the satellite based et pattern and the histogram k of the simulated pattern and g is the number of bins contained in each histogram the z score of the spatial pattern was used to estimate γ to ensure bias insensitivity and to enable the comparison of two variables with different units koch et al 2018 the spaef ranges from to 1 and the better the performance is the closer the value is to 1 furthermore a combined performance metric fcomb defined as in eq 16 was used to assess the overall performance of swat in reproducing the temporal dynamics of streamflow and et and the spatial pattern of et following the study of hulsman et al 2021 16 f comb 1 1 3 1 f o 2 1 f e 2 1 spaef e 2 where fo fe and spaefe are defined as in eqs 1 2 and 15 respectively fcomb ranges between and 1 with perfect performance if the value reaches 1 in addition an efficiency metric ce defined as the reciprocal value of the run time of the sufi 2 algorithm was used to evaluate computational efficiency of the calibration strategies 3 results 3 1 model performance for streamflow table 2 shows the values of the kge and its components cc gama and beta for the daily streamflow simulations obtained with the different calibration strategies unsurprisingly the et only calibration m2 produces the lowest streamflow simulation accuracy kge 0 among the five strategies at the three hydrological stations in the calibration and validation periods as shown in fig a1 and fig 5 many erroneous flow peaks are generated in the wet season from may to september this can be attributed to the higher values of the parameters cn2 and surlag table a2 in m2 than in the other calibration methods a higher cn2 leads to more surface runoff while a higher surlag induces a more rapid release of surface runoff to the main channel neitsch et al 2011 the streamflow only calibration m1 leads to satisfactory streamflow simulations in the calibration kge 0 73 and validation kge 0 60 periods however some small flow peaks can be seen on the hydrographs of the qilian and zhama stations during the dry and cold period january to march this can be explained by the relatively low value of the tlaps parameter table a2 such a low value would lead to overestimations of temperature and consequently misclassifications of snowfall as rainfall over the mountainous uhrb the joint calibration m3 results in relatively lower streamflow simulation accuracy than m1 especially at the yingluoxia station during the calibration and validation periods as shown in fig a1 b and fig 5 b the hydrograph simulated with m3 is ahead in time with respect to the observed hydrograph which might be due to the higher channel hydraulic conductivity ch k2 the stepwise calibration m4 brings about comparable model performances to those of m1 with kge values higher than 0 75 and 0 60 respectively in the calibration and validation periods the spatially stepwise calibration m5 achieves higher streamflow simulation accuracy than m3 and attains comparable precision to m1 and m4 in the calibration period moreover m5 performs best among the five calibration strategies in the validation period comparing the components of the kge table 2 it is found that the cc value in m5 is comparable to those in m1 m3 and m4 while the beta and gama values are closer to 1 in m5 than those in m1 m3 and m4 this indicates that the better model performance obtained with m5 relative to those obtained with m1 m3 and m4 is mainly due to the lower streamflow simulation biases and the better simulations of the temporal streamflow variations taking a closer look at fig a1 and fig 5 we can find that the rising limbs of the hydrographs tend to be simulated worse than the receding limbs in all of the calibration strategies this is possibly due to the deficiencies of swat in representing the soil freezing thawing and snowmelt processes both of which have a significant impact on streamflow generation at the time of a shift from a cold to a warm period march to may 3 2 model performance for et 3 2 1 temporal agreement between the satellite based et and the simulations fig 6 shows the comparisons of the satellite based et with the simulations obtained with the different calibration strategies at the basin scale as expected the streamflow only calibration m1 leads to the lowest et simulation accuracy among the five calibration strategies with kge values less than 0 10 the simulated et shows a significant overestimation in m1 especially in the wet season from may to september tis results can be attributed to the low value of the esco parameter table a2 the other calibrations m2 m5 generate reasonable et simulations achieving kge values that range from 0 80 to 0 87 nevertheless et tends to be underestimated during the period from january to july in most years in line with our previous study zhang et al 2020a this result might be associated with the non simulated soil freezing thawing processes and inaccurate climatic forcing data the et only calibration m2 achieves the best model performance in the calibration period which is understandable since the model was calibrated solely on et in the validation period we can see that the spatially stepwise calibration method m5 obtains the highest kge indicating a better model performance than the other calibration strategies fig 7 depicts the histograms of the kge for the subbasin level et simulations obtained with the different calibration strategies similarly m1 leads to worse model performances than the other calibration methods with kge less than 0 01 at most of the subbasins the median kge value reaches 0 71 in m2 during the calibration period and this value is higher than those of m3 and m4 in the validation period comparable median kge values are obtained with m1 m3 and m4 the spatially stepwise calibration m5 results in the highest median kge values among the five calibrations the kge values in m5 are 0 76 and 0 78 respectively in the calibration and validation periods moreover comparing the histograms of the kge we can observe that the use of m5 leads to more subbasins with higher kge values than the other calibration strategies regardless the results demonstrate that m5 performs better than the other strategies in reproducing the temporal variations of et 3 2 2 spatial agreement between the simulated and satellite based et patterns fig 8 compares the satellite based et with the simulations obtained with the different calibration strategies at the subbasin scale we can see that the simulated et maps obtained with the calibration strategies m1 to m4 exhibit poor spatial agreement with the satellite based map with spaef values less than 0 15 the simulated et map agrees reasonably well with the satellite based et pattern in the spatially stepwise calibration m5 achieving a higher spaef value than the other calibration strategies at the subbasin scale as shown in fig 8 g k the simulated and satellite based et are closer to the 1 1 line in m5 relative to those in m1 m4 during the calibration and validation periods the results imply that the use of m5 improves the simulation of the spatial pattern of et this is understandable because the et related parameters are optimized in a spatially distributed manner in m5 instead of in a lumped manner as in m1 to m4 which enables divergent optimal parameter sets among the different subbasins as shown in fig a2 the improvement of the subbasin level et simulations can be primarily attributed to the better model performance in the wet season from may to september in the dry season from october to april the subbasin level et simulations show poor agreement with the satellite based et in all of the calibration strategies achieving low cc 0 30 and spaef 0 10 values this can be explained by two reasons one is that the dry season et are of lower magnitudes and temporal variabilities than the wet season et and the optimization algorithm would implicitly give a higher weight to the wet season et simulations the other reason is that the soil freezing thawing processes that occur in the dry season and significantly affect the soil water contents and et are not considered by the swat model 3 3 effectiveness and efficiency of the calibration strategies fig 9 a and b show the values of the combined performance metric fcomb and its components in the calibration and validation periods the spatially stepwise calibration m5 achieves the highest overall model performance among the five calibration strategies with an fcomb value higher than 0 55 moreover the components of fcomb including spaefe fe and fo are consistently higher in m5 than those in the other calibration strategies during the validation period the et only calibration m2 obtains the lowest fcomb value 0 25 mainly due to its poor streamflow simulations the joint m3 and stepwise m4 calibrations have comparable fcomb values that are higher than that of m1 as presented in fig 9 c m5 has the highest computational efficiency ce due the application of parallel computing the improvement of the ce can reach up to 2 times greater in m5 than the other calibration strategies specifically the run time of the sufi 2 algorithm embedded in the swat cup is approximately 9 12 h in m1 to m4 while that of the parallel computing based sufi 2 algorithm is approximately about 4 h in m5 taken together the results demonstrate that the new calibration strategy is more reliable and computationally efficient than the other calibration strategies 4 discussion 4 1 trade off or synergy in accuracy between streamflow and et a comparison of multivariate calibration with single variable calibrations i e the benchmarks will aid in understanding the accuracy relationship trade off or synergy between the calibrated variables this understanding is essential for assessing the added value from the incorporation of satellite based products in model calibrations in the study the joint calibration m3 obtains lower fo and fe values than the benchmarks table 3 implying a trade off in accuracy between the simulated et and streamflow the stepwise calibration m4 has comparable model performances to the benchmarks indicating an irrelevant relationship between the simulation accuracy of streamflow and et the spatially stepwise calibration m5 however achieves higher fo and fe values than the benchmarks implying a synergy in accuracy between streamflow and et simulations we agree with rajib et al 2018a that m3 has only achieved a sub optimal parameter set because it attempts to optimize the different objective functions in a lumped manner thus inducing an averaging effect in other words the calibrated parameter set is optimal for the combined objective function of the objectives at the 23 subbasins and 3 hydrological stations but not for each of them this averaging effect could be ameliorated by the stepwise calibration technique m4 because in m4 the objective functions of streamflow and et are optimized in two separate stages the spatially stepwise calibration m5 enables separate optimizations of the different objective functions in a spatially distributed manner m5 can move beyond the averaging effect and leads to simultaneous improvements in streamflow and et simulations relative to the benchmarks highlighting the added value of the satellite based products for model calibrations 4 2 pros and cons of the new calibration strategy the new calibration strategy m5 has several advantages over the joint and stepwise calibrations m3 and m4 first as mentioned above m5 can move beyond the averaging effect of the calibration with a combined objective function by optimizing each of the objective functions separately moreover m5 has a higher computation efficiency due to the application of parallel computing most importantly m5 enables model calibration in a spatially distributed manner that can effectively use the spatial and temporal information of satellite based products as a result m5 can attain better model performances than the other calibration strategies when reproducing both the temporal variations and spatial patterns of et we admit that m5 has some deficiencies first the implicit assumption of a complementary relationship between streamflow and et might not hold true for streamflow and other satellite based variables as demonstrated by koppa et al 2019 second the parameters associated with different hydrological responses need to be distinguished before the stepwise calibration can be conducted this however is not an easy task and requires a deep understanding of hydrological processes especially for complex hydrological models with hundreds of parameters third m5 may add equifinality to the model since the parameters of each subbasin or sub watershed are calibrated separately subbasins with similar hydrological properties might have quit different optimal parameter sets last m5 is considered as the stepwise calibration method and it attempts to determine a single solution that can best match the available observations it does not provide the alternative combinations of the calibrated parameters that yield equally good results fenicia et al 2007 this leads to great challenges in identifying behavioral simulations and assessing model uncertainties fenicia et al 2007 koppa et al 2019 nevertheless we believe the new calibration strategy would lead to lower parameter and model output uncertainty than other calibration methods sutanudjaja et al 2014 the calibration strategies from m1 to m4 only adopt the temporal information of the observations to use in the model calibrations while m5 utilizes both the temporal and spatial information of the observations to screen out one optimal parameter set in other words m5 imposes more constraints on the parameter optimization problem than the other calibration methods furthermore owing to the separate optimization of the objective functions m5 can avoid the trade offs or balances among the different objective functions this leads to an easier convergence of the objective function to its optimum value in comparison to the other calibration strategies resulting in smaller ranges of parameters and model outputs her and chaubey 2015 rajib et al 2018a additionally m5 has a proportion of the calibrated parameters being fixed at each step allowing smaller degrees of freedom in fitting the measurements 4 3 limitations and uncertainties the study is subject to several limitations and uncertainties first the new calibration strategy was tested with swat using streamflow observations and satellite based et other satellite based products such as soil moisture and snow cover products could also provide valuable information for model calibrations rakovec et al 2016b kundu et al 2017 han et al 2019 but these products were not considered in the current study further efforts are needed to investigate whether the new calibration strategy and the its underlying assumptions are still applicable to streamflow and other satellite based observations second a single satellite based et product was used in this study with the assumption that it is free of errors as has been done in many previous studies immerzeel and droogers 2008 rajib et al 2018a odusanya et al 2019 zhang et al 2020b however satellite based products are inherently subjected to some biases due to their indirect estimates of hydrological variables zink et al 2018 dembélé et al 2020 the limitations of retrieve algorithms and inaccuracies in the meteorological forcings the use of a single satellite based et product therefore adds some uncertainty to our findings these uncertainties might be reduced by using synthesized satellite based products elnashar et al 2020 or by shifting from the utilization of absolute satellite based estimates to relative ones i e the spatial pattern information as has been by many researchers demirel et al 2018 nijzink et al 2018 wambura et al 2018 zink et al 2018 dembélé et al 2020 furthermore monthly satellite based et product instead of the daily et product were used in this study to constrain the swat model the new calibration strategy m5 needs to be further tested with the daily satellite based et product considering the high model sensitivity to fast changing climatic conditions baroni et al 2019 third the choice of objective function has a profound impact on the calibration results molina navarro et al 2017 abbaspour et al 2018 this study formulated the objection functions of the calibration strategies of m1 to m4 by summing the equally weighted kge values at the different hydrological stations or subbasins following the methods of the previous studies abbaspour et al 2015 rajib et al 2016 2018a dembélé et al 2020 however other performance metrics such as the nash sutcliffe efficiency nse and the coefficient of determination r 2 and other weight setting methods such as the inverse error weighting stisen et al 2018 dembélé et al 2020 and the drainage area based approach gong et al 2012 were not tested furthermore this study did not test the calibration strategies with growing numbers of sufi 2 iterations we argue that m5 would still perform better than the other calibration methods even with more sufi 2 iterations for two reasons i the number of iterations set as 6 in the present study is higher than those used in many other studies rajib et al 2016 2018a franco et al 2020 and is high enough for the objective function to converge around the optimal value and ii the advantages of m5 over the other calibration strategies section 4 2 can guarantee its better performance 5 conclusions in this study we proposed a parallel computing based and spatially stepwise strategy to calibrate the swat model with streamflow observations and satellite based et the effectiveness and computational efficiency of the new calibration strategy m5 was demonstrated using the case of the upper heihe river basin of china a meso scale mountainous watershed on the northeastern qinghai tibet plateau the performance of m5 was compared with those of the streamflow only calibration m1 et only calibration m2 joint calibration m3 and stepwise calibration m4 the results indicate that m1 achieves satisfactory streamflow simulations but leads to unreasonable et simulations m2 attains satisfactory et simulations but result in unreasonable streamflow simulations the joint calibration m3 yields reasonable et and streamflows simulations simultaneously and has a higher et simulation accuracy than m1 and a greater streamflow accuracy than m2 the stepwise calibration m4 has a better model performance than m3 due to the alleviation of the tradeoffs between the objective functions of et and streamflow the spatially stepwise calibration m5 generates the best streamflow and et simulations among the five calibration strategies m5 also improves the simulation of the spatial pattern of et and has a higher computational efficiency than the other calibration strategies taken together the results demonstrate that the new calibration strategy is more reliable and computationally efficient than the other calibration methods further analysis implies that m5 can lead to a synergic relationship between the simulation accuracy of et and streamflow highlighting the added value of the satellite based products for model calibrations m5 enables separate optimizations of the objective functions in a spatially distributed and parallel manner so that it can effectively and efficiently use the spatial and temporal information of the satellite based products nevertheless we admit that m5 has some deficiencies such as the implicit assumption of a complementary relationship between the calibrated variables and the need to distinguish between the parameters related to different hydrological responses we further point out the limitations and uncertainties of the study along with the prospects of future work 6 code availability the code used to calibrate swat based on the parallel computing based and spatially stepwise strategy is freely available at https github com hydrors parallelswat credit authorship contribution statement ling zhang conceptualization funding acquisition investigation software methodology resources visualization writing original draft writing review editing yanbo zhao investigation resources visualization writing review editing qimin ma data curation formal analysis writing review editing penglong wang software investigation yingchun ge resources visualization writing review editing wenjun yu investigation validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the major project of china high resolution earth observation system 21 y20b01 9001 19 22 the strategic priority research program of chinese academy of sciences xda20100104 the national natural science foundation of china 41901045 and the cas light of west china program 29y929661 the authors would like to thank dr mehmet c demirel for sharing the code used to calculate the spatial efficiency the careful reviews and constructive comments of the anonymous referees are greatly appreciated appendix a 
4461,water retention curve wrc is an important parameter for unsaturated soils it is greatly affected by the anisotropy of pore structure as supported by experimental results in the literature so far however the mechanism and theoretical modelling of anisotropy effects have not been investigated these two issues were explored in this study based on two dimensional analysis of soil pores while were approximated as a series of ellipses for simplicity according to experimental results in the literature the pores of anisotropic specimen are more elongated than those of isotropic specimen on average the elongated pore has a higher water retention ability than the round pore when they have the same area as a consequence the water retention ability of anisotropic specimen is higher than that of isotropic specimen on the basis of this mechanism a new wrc model was proposed for isotropic and anisotropic soils to verify the new model it was applied to simulate the wrcs of three soils with isotropic and anisotropic pore structures measured and calculated results were well matched with the coefficient of determination r2 in the range of 0 89 to 0 99 and the root mean square error rmse ranging from 0 009 to 0 073 it is convincingly demonstrated that the new model is able to capture the influence of anisotropy on wrc keywords unsaturated soil water retention anisotropy pore shape 1 introduction soil properties are often anisotropic i e direction dependent for different reasons such as geological deposition and anisotropic stress history the influence of anisotropy on the stiffness strength and permeability has been extensively investigated by previous researchers in the literature however there are limited studies of anisotropy effects on the water retention curve wrc this curve is an important hydrological parameter for seepage analysis used in the agricultural hydrological environmental and geotechnical areas related to the vadose zone lamorski et al 2017 ng and pang 2000 sakai et al 2015 tan et al 2016 walczak et al 2006 zhou et al 2020 ng et al 2020 sivakumar et al 2010 prepared two types of specimens with the same density through isotropic and anisotropic compression at a given suction the anisotropic specimen has a higher degree of saturation than the isotropic one tse 2007 investigated the influence of stress induced anisotropy on the water retention behaviour of a completely decomposed granite three specimens were prepared using the same method and then controlled to the same mean net stress i e the difference between total stress and pore air pressure but different stress ratios i e the ratio of deviator stress and mean net stress for both intact and compacted specimens the equilibrium degree of saturation at a given suction at a larger stress ratio is higher than that at a lower stress ratio similar observation was reported by habasimbi and nishimura 2018 through a series of tests on unsaturated clay the above results consistently suggest that anisotropy would alter the water retention curve of unsaturated soils up to now however there is no theoretical investigation on the wrc of anisotropic soils further studies are required to understand the mechanism of anisotropy effects and to model wrc of anisotropic soils in this study a new water retention model for anisotropic soils was developed the influence of anisotropy on the pore shape and hence the wrc was incorporated the model was verified using experimental results reported in the literature 2 mathematical formulations 2 1 influence of anisotropy on the pore shape and water retention curve wrc of unsaturated soils is mainly governed by its pore characteristics such as the pore orientation pore shape pore size distribution and pore connectivity zhou and ng 2014 among these four factors the first two are directly affected by soil anisotropy the pore orientation does not affect the wrc which is a scalar variable as confirmed by the experimental results of priono et al 2017 hence the present study focuses on the influence of pore shape on the basis of two dimensional analysis for simplicity soil pores are approximated as a series of ellipses based on the results of experimental studies and discrete element method dem simulations in the literature chow et al 2019 yuan et al 2019 gao et al 2020 kang et al 2014 for each ellipse two parameters are required to describe the area and shape independently the area is denoted by a unit cm2 while the shape is characterized by the elongation ratio e unit cm cm 1 defined as follows chow et al 2019 1 e l min l maj where l min and l maj are the lengths unit cm of the minor and major principal axis respectively according to this definition e falls in the range of 0 to 1 when e approaches 0 the pore is very elongated when e is equal to 1 the pore becomes perfectly round by using these two independent variables i e a and e l min and l maj can be determined 2 l min 2 e 0 5 a π 0 5 3 l maj 2 e 0 5 a π 0 5 the values of l min and l maj would affect the water retention curve of this pore there is a critical value of suction s cri unit kpa below which the pore is fully saturated and above which the pore is not able to retain water the value of s cri is calculated using the following equation ng and menzies 2007 4 s cri 2 t s l min 2 t s l maj where t s is the surface tension coefficient unit mn cm 1 of air water interfaces substituting equations 2 and 3 into 4 5 s cri t s a π 0 5 e 0 5 e 0 5 according to equation 5 s cri of a pore is governed by not only its area but also its elongation ratio at a given area s cri is larger when the pore is more elongated i e a smaller e recently gao et al 2020 investigated the characteristic of pore shape of a clay through quantitative scanning electron microscopy sem analysis they found that e of most pores falls in a small range with a dominant value at each state this dominant value is used here to characterize the pore shape of a soil specimen this simplification could keep the formulations simple and minimize the number of model parameters the dominant values of e under isotropic and anisotropic states are denoted by e iso and e ani respectively hence the values of s cri at isotropic state s cri iso and anisotropic state s cri ani can be calculated using equation 5 6 s cri iso t s a π 0 5 e iso 0 5 e iso 0 5 7 s cri ani t s a π 0 5 e ani 0 5 e ani 0 5 to investigate the influence of anisotropy isotropic state can be considered as a reference the value of s cri ani is normalized by s cri iso and defined as β by assuming that soil anisotropy does not alter a equations 6 and 7 suggest that 8 β e ani 0 5 e ani 0 5 e iso 0 5 e iso 0 5 equation 8 suggests that β is governed by the values of e at isotropic and anisotropic states some researchers investigated the influence of anisotropy on e of porous materials oda et al 1985 carried out a series of biaxial compression tests on photoelastic disks they found that the pores became more elongated during the shearing process i e a reduction of e due to the stress induced anisotropy chow et al 2019 studied the pore structure of kaolin clay under one dimensional compression they found that soil pores became more elongated during the anisotropic one dimensional compression similar observations were reported by gao et al 2020 through a series of triaxial compression tests on clay the above results consistently imply that the anisotropic specimen would have a smaller dominant e and hence a higher equilibrium water content at a given suction than the isotropic one 2 2 modelling the water retention behaviour of anisotropic soils many water retention models have been reported in the literature and most of them are able to capture the wrcs of isotropic soils this note uses the model of van genuchten 1980 as an example 9 s r 1 s m 3 m 2 m 1 where s r is the degree of saturation s is the suction unit kpa m 1 m 2 and m 3 are soil parameters parameters m 1 and m 2 are dimensionless and m 3 has a unit of kpa when an isotropic specimen becomes anisotropic its pores would become more elongated i e a smaller e on average as discussed above the scaling factor β in equation 8 can be applied to describe anisotropy effects on the s cri of all pores then equation 9 is revised as follows 10 s r 1 s m 3 i s o 1 β m 2 m 1 equation 10 is the newly derived wrc model for unsaturated soils compared with equation 9 the new model considers the influence of anisotropy explicitly there are four model parameters i e m 1 m 2 m 3 iso and β in the new model note that parameter β represents anisotropic effects on the pore shape and water retention behaviour through equations 8 and 10 respectively it takes the same value in these two equations because equation 10 is derived based on equations 8 and 9 the value is equal to 1 for isotropic soils and larger than 1 for anisotropic soils in addition parameters m 1 m 2 and m 3 iso are assumed to be independent of soil anisotropy wrcs at both isotropic and anisotropic states are required to calibrate these four parameters in this study the calibration is conducted using matlab with two steps first of all equation 10 is applied to fit a wrc of isotropic specimen the values of parameters m 1 m 2 and m 3 iso are determined when the coefficient of determination r 2 is the highest after that the equation is applied to fit a wrc of anisotropic specimen to calibrate parameter β 3 model verification to evaluate capability of the new model it was applied to simulate the wrcs of three soils tested by some previous researchers these soils and their values of model parameters are summarized in table 1 sivakumar et al 2010 investigated the water retention behaviour of kaolin clay two different methods were used to prepare soil specimens including isotropic compaction and anisotropic one dimensional compaction by using each method two slightly compacted specimens void ratio 1 19 and two heavily compacted specimens void ratio 0 99 were prepared fig 1 shows the wrcs of these four specimens with different densities and degrees of anisotropy experimental results in this figure were obtained from tests is a id a is b and id b in sivakumar et al 2010 the computed results by the new model are also included in this figure for comparison it is clear that the anisotropic specimen could retain more water than the isotropic specimen at a given suction at both slightly and heavily compacted conditions this trend is well captured by the proposed model the coefficient of determination r 2 is larger than 0 98 and the root mean square error rmse dimensionless here is below 0 018 for all cases as summarized in table 1 in addition note that the value of β is 1 61 and 1 20 for slightly and heavily compacted specimens respectively the difference is likely attributed to the distribution of pore elongation ratios through dem simulations sufian et al 2019 found that pores in looser specimens were more sensitive to anisotropic stress compression than those in denser specimens this implies that the influence of anisotropy on e and hence the wrc was more significant see equations 8 and 10 in looser specimens than that in denser specimens fig 2 shows the measured and computed wrcs of a completely decomposed granite from hong kong the experimental results were reported by tse 2007 three different specimens were subjected to the same mean net stress of 80 kpa but different stress ratios of 0 0 75 and 1 2 after equilibrium each specimen was dried for measuring the wrc the results illustrate that for the equilibrium water content at a given suction its value at a higher stress ratio i e more anisotropic is higher than that at a lower stress ratio more importantly the influence of stress induced anisotropy was well captured by the new model with r 2 larger than 0 90 and rmse below 0 073 as expected the value of β is larger when the stress ratio is higher see table 1 this is because the pores are more elongated on average due to anisotropic stress condition habasimbi and nishimura 2018 investigated the influence of stress ratio on the wrc of a silt from japan one specimen was tested at isotropic stress condition while the other one was tested at one dimensional straining condition their tests were fitted using the proposed model experimental results in fig 3 are corresponding to the drying wrc at a net stress of 100 kpa reported by habasimbi and nishimura 2018 the measured and computed results are well matched as shown in this figure the values of r 2 are larger than 0 89 and the values of rmse are below 0 011 the results shown in figs 1 through 3 clearly demonstrate that the new model is able to well capture the water retention behaviour of isotropic and anisotropic soils the model capability is closely related to the considerations of pore shape in equations 8 and 10 the proposed wrc model can be applied in seepage analysis for different purposes slope hydrology and its influence on the stability is taken as one example here the stress condition of soils in a slope is often anisotropic and heterogeneous depending on many factors such as the slope geometry and soil property consequently soils in the slope could have different anisotropic pore structures and hence wrcs moreover the degree of anisotropy may evolve under the action of external loading for instance there are many construction activities near slopes in densely populated and hilly cities like hong kong in such cases the proposed model could be used to improve the seepage analysis it should be pointed out that however evolving anisotropy and its influence on the parameter β in equation 10 are complicated more theoretical and experimental studies are necessary prior to the practical implementation and application of the proposed modelling approach 4 summary and conclusion in this study anisotropy effects on the wrc were investigated based on two dimensional analysis of soil pores which were approximated as a series of ellipses for simplicity compared to isotropic specimen the pores of anisotropic specimen are more elongated on average and therefore have a higher equilibrium water content at a given suction on the basis of this mechanism a new water retention model was proposed for anisotropic soils a new variable which is a function of the dominant elongation ratio was newly added in the model of van genuchten 1980 for incorporating anisotropy effects the new model was verified by using experimental results of three different soils for each soil the wrcs of both isotropic and anisotropic specimens were determined by previous researchers the measured and computed results were well matched with r 2 in the range of 0 89 to 0 99 and rmse ranging from 0 009 to 0 073 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported by the national science foundation of china through the research grant 52022004 the authors also would like to thank the research grants council rgc of the hksar for providing financial support through the grants 16210420 and aoe e 603 18 the support from natural science foundation of guangdong province grant no 2018a030310018 is also acknowledged 
4461,water retention curve wrc is an important parameter for unsaturated soils it is greatly affected by the anisotropy of pore structure as supported by experimental results in the literature so far however the mechanism and theoretical modelling of anisotropy effects have not been investigated these two issues were explored in this study based on two dimensional analysis of soil pores while were approximated as a series of ellipses for simplicity according to experimental results in the literature the pores of anisotropic specimen are more elongated than those of isotropic specimen on average the elongated pore has a higher water retention ability than the round pore when they have the same area as a consequence the water retention ability of anisotropic specimen is higher than that of isotropic specimen on the basis of this mechanism a new wrc model was proposed for isotropic and anisotropic soils to verify the new model it was applied to simulate the wrcs of three soils with isotropic and anisotropic pore structures measured and calculated results were well matched with the coefficient of determination r2 in the range of 0 89 to 0 99 and the root mean square error rmse ranging from 0 009 to 0 073 it is convincingly demonstrated that the new model is able to capture the influence of anisotropy on wrc keywords unsaturated soil water retention anisotropy pore shape 1 introduction soil properties are often anisotropic i e direction dependent for different reasons such as geological deposition and anisotropic stress history the influence of anisotropy on the stiffness strength and permeability has been extensively investigated by previous researchers in the literature however there are limited studies of anisotropy effects on the water retention curve wrc this curve is an important hydrological parameter for seepage analysis used in the agricultural hydrological environmental and geotechnical areas related to the vadose zone lamorski et al 2017 ng and pang 2000 sakai et al 2015 tan et al 2016 walczak et al 2006 zhou et al 2020 ng et al 2020 sivakumar et al 2010 prepared two types of specimens with the same density through isotropic and anisotropic compression at a given suction the anisotropic specimen has a higher degree of saturation than the isotropic one tse 2007 investigated the influence of stress induced anisotropy on the water retention behaviour of a completely decomposed granite three specimens were prepared using the same method and then controlled to the same mean net stress i e the difference between total stress and pore air pressure but different stress ratios i e the ratio of deviator stress and mean net stress for both intact and compacted specimens the equilibrium degree of saturation at a given suction at a larger stress ratio is higher than that at a lower stress ratio similar observation was reported by habasimbi and nishimura 2018 through a series of tests on unsaturated clay the above results consistently suggest that anisotropy would alter the water retention curve of unsaturated soils up to now however there is no theoretical investigation on the wrc of anisotropic soils further studies are required to understand the mechanism of anisotropy effects and to model wrc of anisotropic soils in this study a new water retention model for anisotropic soils was developed the influence of anisotropy on the pore shape and hence the wrc was incorporated the model was verified using experimental results reported in the literature 2 mathematical formulations 2 1 influence of anisotropy on the pore shape and water retention curve wrc of unsaturated soils is mainly governed by its pore characteristics such as the pore orientation pore shape pore size distribution and pore connectivity zhou and ng 2014 among these four factors the first two are directly affected by soil anisotropy the pore orientation does not affect the wrc which is a scalar variable as confirmed by the experimental results of priono et al 2017 hence the present study focuses on the influence of pore shape on the basis of two dimensional analysis for simplicity soil pores are approximated as a series of ellipses based on the results of experimental studies and discrete element method dem simulations in the literature chow et al 2019 yuan et al 2019 gao et al 2020 kang et al 2014 for each ellipse two parameters are required to describe the area and shape independently the area is denoted by a unit cm2 while the shape is characterized by the elongation ratio e unit cm cm 1 defined as follows chow et al 2019 1 e l min l maj where l min and l maj are the lengths unit cm of the minor and major principal axis respectively according to this definition e falls in the range of 0 to 1 when e approaches 0 the pore is very elongated when e is equal to 1 the pore becomes perfectly round by using these two independent variables i e a and e l min and l maj can be determined 2 l min 2 e 0 5 a π 0 5 3 l maj 2 e 0 5 a π 0 5 the values of l min and l maj would affect the water retention curve of this pore there is a critical value of suction s cri unit kpa below which the pore is fully saturated and above which the pore is not able to retain water the value of s cri is calculated using the following equation ng and menzies 2007 4 s cri 2 t s l min 2 t s l maj where t s is the surface tension coefficient unit mn cm 1 of air water interfaces substituting equations 2 and 3 into 4 5 s cri t s a π 0 5 e 0 5 e 0 5 according to equation 5 s cri of a pore is governed by not only its area but also its elongation ratio at a given area s cri is larger when the pore is more elongated i e a smaller e recently gao et al 2020 investigated the characteristic of pore shape of a clay through quantitative scanning electron microscopy sem analysis they found that e of most pores falls in a small range with a dominant value at each state this dominant value is used here to characterize the pore shape of a soil specimen this simplification could keep the formulations simple and minimize the number of model parameters the dominant values of e under isotropic and anisotropic states are denoted by e iso and e ani respectively hence the values of s cri at isotropic state s cri iso and anisotropic state s cri ani can be calculated using equation 5 6 s cri iso t s a π 0 5 e iso 0 5 e iso 0 5 7 s cri ani t s a π 0 5 e ani 0 5 e ani 0 5 to investigate the influence of anisotropy isotropic state can be considered as a reference the value of s cri ani is normalized by s cri iso and defined as β by assuming that soil anisotropy does not alter a equations 6 and 7 suggest that 8 β e ani 0 5 e ani 0 5 e iso 0 5 e iso 0 5 equation 8 suggests that β is governed by the values of e at isotropic and anisotropic states some researchers investigated the influence of anisotropy on e of porous materials oda et al 1985 carried out a series of biaxial compression tests on photoelastic disks they found that the pores became more elongated during the shearing process i e a reduction of e due to the stress induced anisotropy chow et al 2019 studied the pore structure of kaolin clay under one dimensional compression they found that soil pores became more elongated during the anisotropic one dimensional compression similar observations were reported by gao et al 2020 through a series of triaxial compression tests on clay the above results consistently imply that the anisotropic specimen would have a smaller dominant e and hence a higher equilibrium water content at a given suction than the isotropic one 2 2 modelling the water retention behaviour of anisotropic soils many water retention models have been reported in the literature and most of them are able to capture the wrcs of isotropic soils this note uses the model of van genuchten 1980 as an example 9 s r 1 s m 3 m 2 m 1 where s r is the degree of saturation s is the suction unit kpa m 1 m 2 and m 3 are soil parameters parameters m 1 and m 2 are dimensionless and m 3 has a unit of kpa when an isotropic specimen becomes anisotropic its pores would become more elongated i e a smaller e on average as discussed above the scaling factor β in equation 8 can be applied to describe anisotropy effects on the s cri of all pores then equation 9 is revised as follows 10 s r 1 s m 3 i s o 1 β m 2 m 1 equation 10 is the newly derived wrc model for unsaturated soils compared with equation 9 the new model considers the influence of anisotropy explicitly there are four model parameters i e m 1 m 2 m 3 iso and β in the new model note that parameter β represents anisotropic effects on the pore shape and water retention behaviour through equations 8 and 10 respectively it takes the same value in these two equations because equation 10 is derived based on equations 8 and 9 the value is equal to 1 for isotropic soils and larger than 1 for anisotropic soils in addition parameters m 1 m 2 and m 3 iso are assumed to be independent of soil anisotropy wrcs at both isotropic and anisotropic states are required to calibrate these four parameters in this study the calibration is conducted using matlab with two steps first of all equation 10 is applied to fit a wrc of isotropic specimen the values of parameters m 1 m 2 and m 3 iso are determined when the coefficient of determination r 2 is the highest after that the equation is applied to fit a wrc of anisotropic specimen to calibrate parameter β 3 model verification to evaluate capability of the new model it was applied to simulate the wrcs of three soils tested by some previous researchers these soils and their values of model parameters are summarized in table 1 sivakumar et al 2010 investigated the water retention behaviour of kaolin clay two different methods were used to prepare soil specimens including isotropic compaction and anisotropic one dimensional compaction by using each method two slightly compacted specimens void ratio 1 19 and two heavily compacted specimens void ratio 0 99 were prepared fig 1 shows the wrcs of these four specimens with different densities and degrees of anisotropy experimental results in this figure were obtained from tests is a id a is b and id b in sivakumar et al 2010 the computed results by the new model are also included in this figure for comparison it is clear that the anisotropic specimen could retain more water than the isotropic specimen at a given suction at both slightly and heavily compacted conditions this trend is well captured by the proposed model the coefficient of determination r 2 is larger than 0 98 and the root mean square error rmse dimensionless here is below 0 018 for all cases as summarized in table 1 in addition note that the value of β is 1 61 and 1 20 for slightly and heavily compacted specimens respectively the difference is likely attributed to the distribution of pore elongation ratios through dem simulations sufian et al 2019 found that pores in looser specimens were more sensitive to anisotropic stress compression than those in denser specimens this implies that the influence of anisotropy on e and hence the wrc was more significant see equations 8 and 10 in looser specimens than that in denser specimens fig 2 shows the measured and computed wrcs of a completely decomposed granite from hong kong the experimental results were reported by tse 2007 three different specimens were subjected to the same mean net stress of 80 kpa but different stress ratios of 0 0 75 and 1 2 after equilibrium each specimen was dried for measuring the wrc the results illustrate that for the equilibrium water content at a given suction its value at a higher stress ratio i e more anisotropic is higher than that at a lower stress ratio more importantly the influence of stress induced anisotropy was well captured by the new model with r 2 larger than 0 90 and rmse below 0 073 as expected the value of β is larger when the stress ratio is higher see table 1 this is because the pores are more elongated on average due to anisotropic stress condition habasimbi and nishimura 2018 investigated the influence of stress ratio on the wrc of a silt from japan one specimen was tested at isotropic stress condition while the other one was tested at one dimensional straining condition their tests were fitted using the proposed model experimental results in fig 3 are corresponding to the drying wrc at a net stress of 100 kpa reported by habasimbi and nishimura 2018 the measured and computed results are well matched as shown in this figure the values of r 2 are larger than 0 89 and the values of rmse are below 0 011 the results shown in figs 1 through 3 clearly demonstrate that the new model is able to well capture the water retention behaviour of isotropic and anisotropic soils the model capability is closely related to the considerations of pore shape in equations 8 and 10 the proposed wrc model can be applied in seepage analysis for different purposes slope hydrology and its influence on the stability is taken as one example here the stress condition of soils in a slope is often anisotropic and heterogeneous depending on many factors such as the slope geometry and soil property consequently soils in the slope could have different anisotropic pore structures and hence wrcs moreover the degree of anisotropy may evolve under the action of external loading for instance there are many construction activities near slopes in densely populated and hilly cities like hong kong in such cases the proposed model could be used to improve the seepage analysis it should be pointed out that however evolving anisotropy and its influence on the parameter β in equation 10 are complicated more theoretical and experimental studies are necessary prior to the practical implementation and application of the proposed modelling approach 4 summary and conclusion in this study anisotropy effects on the wrc were investigated based on two dimensional analysis of soil pores which were approximated as a series of ellipses for simplicity compared to isotropic specimen the pores of anisotropic specimen are more elongated on average and therefore have a higher equilibrium water content at a given suction on the basis of this mechanism a new water retention model was proposed for anisotropic soils a new variable which is a function of the dominant elongation ratio was newly added in the model of van genuchten 1980 for incorporating anisotropy effects the new model was verified by using experimental results of three different soils for each soil the wrcs of both isotropic and anisotropic specimens were determined by previous researchers the measured and computed results were well matched with r 2 in the range of 0 89 to 0 99 and rmse ranging from 0 009 to 0 073 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported by the national science foundation of china through the research grant 52022004 the authors also would like to thank the research grants council rgc of the hksar for providing financial support through the grants 16210420 and aoe e 603 18 the support from natural science foundation of guangdong province grant no 2018a030310018 is also acknowledged 
4462,downscaling of local daily precipitation from large scale climatic variables is required for assessing the impact of climate change on hydrology and water resources this study proposes wavelet transform wt based feed forward neural network ff nn and nonlinear auto regressive with exogenous inputs network narx nn models for downscaling daily precipitation the models are applied to a large river basin the krishna river basin in the indian subcontinent several climatic variables including geo potential heights wind direction vorticity humidity air temperature mean sea level pressure meridional velocity at surface and 500hpa and 850hpa levels are considered based on their statistical correlations the results are evaluated using different performance measures and the ability of the models to capture the extreme events at five selected grid points in different locations having varying climatic characteristics is assessed the performance of the proposed wavelet based models is also compared with that of four different traditional and recent downscaling methods multiple linear regression mlr statistical downscaling model sdsm genetic programming gp and artificial neural networks anns the results reveal that the wavelet based neural network models wt ff nn and wt narx nn are robust compared to the other methods in terms of their ability to capture the regional precipitation patterns and the extreme events the improvement in the wavelet based models can be attributed to their ability to unravel the hidden relationship between the predictors and precipitation it is also observed that there is considerable increase in the correlation between precipitation and the decomposed climatic variables all these results suggest that wavelets aid in unravelling the relationship between local precipitation and large scale climatic variables and improving the overall performance of the downscaling models abbreviations ann nn artificial neural networks neural networks ff nn feed forward neural networks gcm general circulation model gp genetic programming mlr multiple linear regression narx nn nonlinear auto regressive with exogenous inputs neural networks ncep national center for environmental prediction rcm regional climate model sdsm statistical downscaling model wt ff nn wavelet feed forward neural networks wt gp wavelet genetic programming wt mlr wavelet multiple linear regression wt narx nn wavelet nonlinear auto regressive with exogenous inputs neural networks keywords precipitation downscaling general circulation models statistical downscaling genetic programming neural networks wavelets 1 introduction studying the impact of global warming induced climate change on water resources has become a key area of research in recent decades to project future climate change scenarios general circulation models gcms which are computer models working at the global scale are commonly used these models generally produce climate outputs e g precipitation temperature at coarser spatial scales typically having horizontal grid size ranging from 100 km to 600 km such coarse scale climate outputs however are not appropriate for most hydrologic studies since hydrologic studies generally require data at much finer spatial scales corresponding to catchments therefore to properly study the impact of climate change on hydrology and water resources it is required to downscale the coarse scale climate outputs to catchment scale hydrologic variables sehgal et al 2018 wilby et al 1998 downscaling of gcm outputs assumes that the local climate is a function of both the global atmospheric features large scale climatic conditions and the local conditions topography land surface characteristics generally deriving local climatic characteristics from gcm outputs through downscaling is a multistep process involving a multitude of assumptions and approximations the methods for downscaling can be broadly classified into 1 dynamic downscaling and 2 statistical downscaling dynamical downscaling uses an appropriate gcm to generate regional climate models rcms to be used at regional scales srinivas et al 2014 the rcms consider the coarse scale gcm outputs as boundary conditions and detailed physical processes complex terrain and land surface characteristics at regional scale to model the regional scale climate the rcms have better ability in capturing the basic regional patterns in time and space contrary to this statistical downscaling models use empirical relationships wilby et al 1998 1999 between predictors atmospheric processes and predictands local climatic variables such as rainfall to generate the data at finer spatiotemporal resolutions even though dynamical downscaling has shown robust performances it requires large amount of data high level of expertise to analyze the results and is also computationally intensive which make it beyond the reach of institutions in most countries fowler et al 2007 hay and clark 2003 sachindra et al 2014 mahmood et al 2016 the number of experiments for climate scenarios is limited by the intensive computational requirements which is a function of resolution domain size and accuracy further the rcms are limited by systematic errors branching from the gcms as the rcms are strongly dependent on the outputs from the gcms compared to the rcm statistical downscaling methods have gained much wider acceptance due to their simplicity and low computational burden okkan and inan 2015 rashid et al 2015 sachindra et al 2018 sachindra and perera 2016 sehgal et al 2018 however establishing robust linkage between local climatic variables and large scale atmospheric processes requires long historical data hay and clark 2003 depending on the assumed relationship statistical downscaling consists of a heterogeneous group of methods that vary in sophistication and applicability these methods are classified into three categories stochastic weather generators weather classification based approaches and regression based methods stochastic weather generators involve fitting probability distribution function to the variable the weather classification based methods involve classification of the state variables into weather types using the k means algorithm in addition to the stochastic weather generation the weather generator methods are based on the statistical properties of the climatic variables they are sensitive to outliers in the training set and involve the generation of an ensemble of time series and statistical analysis of the results furthermore the weather generator and weather classification methods are data intensive need large computational resources and do not have prediction capability outside the historical data range these issues led the regression based methods to gain popularity especially due to their ease in application sachindra et al 2018 the regression based downscaling methods primarily depend on developing a predictor predictand relationship some of these methods include multiple linear regression mlr sachindra et al 2014 joshi et al 2015 duhan and pandey 2015 ravansalar et al 2017 generalized linear model glm asong et al 2016 beecham et al 2014 artificial neural networks anns yeditha et al 2020 chithra et al 2016 hassan esfahani et al 2015 genetic programming gp coulibaly and burn 2004 sachindra et al 2018 random forests regression rfr hutengs and vohland 2016 pang et al 2017 mishra et al 2017 support vector machines svms kannan and ghosh 2011 srinivas et al 2014 and extreme learning machines elms sachindra et al 2018 zhu et al 2019 these studies have shown that the regression based downscaling methods are robust and accurate in capturing the local climatic patterns further the results from the past studies indicate that techniques based on artificial intelligence ai which take into account the complexity and nonlinearity in the relationship between predictors and predictands perform better in comparison with the other linear methods many past studies kurths et al 2019 guntu et al 2020 maheswaran and khosa 2015 rashid et al 2016 have reported that the relationship between rainfall and atmospheric variables is a function of time scale for instance maheswaran and khosa 2015 studying the cauvery river basin in india showed that the relationship between climatic indices and rainfall varied within the time frequency domain therefore the popular traditional approach to downscaling based on single scale models might not be able to capture the variability at different time scales rashid et al 2016 in light of this limitation with single scale models there has been a special emphasis on the multi scale dynamics of the climate system involving interactions and feedbacks among different processes at different temporal and spatial scales agarwal et al 2017 2018 2019 carey et al 2013 maheswaran and khosa 2012a 2012b addison 2005 torrence and compo 1998 in this regard wavelets have become a highly popular tool with their inherent advantages including time frequency localization and multiscale resolution wavelets decompose the given time series into scale specific components as proxies of the physical processes at those scales several studies kaheil et al 2008 adamowski and sun 2010 maheswaran and khosa 2012a 2012b 2014 2009 rathinasamy et al 2013 niu and sivakumar 2013 beecham et al 2014 shafaei and kisi 2017 nourani et al 2018 nourani and farbouddam 2019 baghanam et al 2019 rashid et al 2018 have shown that wavelet transform based modelling has aided in unravelling the multiscale dynamics in different processes a recent study by sun et al 2019 investigated the efficiency of the wavelet based models over the single stochastic ann regression models for daily streamflow forecasting and reported better performance of the wavelet based models on similar lines ravansalar et al 2017 used wavelet transform based linear genetic programming wlgp models for predicting monthly streamflow in iran they found that the wavelet based models significantly improved the flow prediction in comparison with gp ann and mlr models djerbouai and souag gamane 2016 showed that the accuracy of drought prediction in the karkheh basin in iran improved with wavelet pre processing of inputs when compared to single models without wavelets most of the above studies have combined the wavelets with other models in developing hybrid models for hydrologic forecasting applications even though there has been extensive research on the application of wavelets for hydrologic forecasting there have only been very few studies on the application of wavelets for statistical downscaling among such studies kaheil et al 2008 combined wavelets and svm for downscaling and forecasting evapotranspiration rashid et al 2015 used the wavelet gamlss model for downscaling of precipitation in the onkaparinga river catchment in south australia the results showed that the wt gamlss models were accurate in reproducing the time series properties when compared to the orthodox gamlss models more recently lakhanpal et al 2017 used a wavelet based second order volterra model for downscaling monthly temperature in the krishna river basin in india and found that the wavelet volterra model performed better when compared to other models including anns and mlr sehgal et al 2018 used a similar model for downscaling monthly precipitation using the ncep data and showed that the model results were closer to the local precipitation it is important to note that most of the above mentioned studies have focussed on downscaling of precipitation and other variables at the monthly scale and there have hardly been any studies on evaluating the hybrid models for precipitation downscaling at the daily scale despite the fact that downscaling of precipitation at the daily scale and even sub daily scales is very important especially in the context of extreme events furthermore thus far there has not been any serious attempt to conduct a detailed investigation to compare different methods for downscaling of daily precipitation in view of these the present study proposes the application of wavelet based hybrid methods for statistical downscaling of daily precipitation the proposed wavelet based hybrid models are applied for downscaling daily precipitation in five selected grid points in different locations in the krishna river basin in india with varying climatic characteristics the performances of these models are also compared with some popular models including mlr sdsm gp and single scale ann models 2 study area and data the krishna river basin being the fourth largest basin inthe indian peninsula originates at mahabaleshwar in the state of maharashtra india and joins the bay of bengal at hamasaladeevi near koduru inandhra pradesh fig 1 the catchment of the basin spreads between 73 to 82 e and 13 to 19 n covering a total area of 260 401 km2 fig 2 a d shows the topography mean annual rainfall distribution land use land cover lulc and climate classification respectively of the krishna river basin most of the area of the basin is flat terrain having semi arid characteristics according to köppen climate classification and with agricultural land covering about 78 of the total area the mean annual rainfall of the basin ranges from 420 mm mid parts to 4000 mm western regions with much of it occurring during the southwest monsoon season june october in the present study five selected grid points indicated by numbers 1 to 5 in fig 1 bottom in different locations in the krishna river basin are considered for developing and applying the downscaling methods and for comparing their performances these grids for downscaling of data are selected based on the climatic classification of the region as shown in fig 2 d for these grids precipitation data are made available from the india meteorological department imd for the period from january 1 1901 to december 31 2019 the data are available at the daily resolution in gridded format and have a spatial resolution of 0 25 x 0 25 pai et al 2014 fig 3 shows for example the rainfall distribution at one of the five grids grid 1 with the mean annual precipitation value also indicated therein the data related to the different atmospheric variables were downloaded from the national center for environmental prediction and national center for atmospheric research ncep ncar website https www esrl noaa gov psd data gridded data ncep reanalysis html for the period from january 1 1948 to december 31 2017 these data have a spatial resolution of 2 5 x 2 5 taking into account the climatic characteristics of the krishna river basin including those reported by previous studies sehgal et al 2018 a total of 16 predictors is considered table 1 the predictor variables are selected in such a way that they are strong contributors to the variability of precipitation in the region the influence of temperature wind velocity geopotential heights and humidity on precipitation is very widely known and thus their utility in downscaling precipitation sachindra et al 2018 3 methodology in this study several traditional and modern methods are applied for downscaling daily precipitation in the krishna river basin and for comparing their performances these methods include multiple linear regression mlr statistical downscaling model sdsm genetic programming gp artificial neural networks anns and wavelet neural network wnn hybrid model the first four models are single scale models while the last one is a multiscale model a brief description of these methods is presented next 3 1 single scale models 3 1 1 multiple linear regression mlr multiple linear regression mlr is a form of regression analysis through which a linear relationship between a predictand variable y and multiple predictor variables x1 x2 xn is established a typical linear regression equation takes the form 1 y a0 a1x1 a2x2 a3x3 anxn the parameters a0 a1 an are estimated using the simple least squares method a detailed description of multiple linear regression can be seen in olive 2017 among others 3 1 2 statistical downscaling model sdsm the statistical downscaling model sdsm is described as a hybrid between the multiple linear regression model and a stochastic weather generator wilby et al 2002 wetterhall et al 2006 this method has the advantages of both regression based downscaling and the stochastic weather generators tryhorn and degaetano 2011 a number of studies wilby et al 1999 harpham and wilby 2005 wetterhall et al 2006 have used the sdsm for downscaling precipitation and temperature for different study regions around the world extensive details about the sdsm can be obtained from wilby et al 2002 among others the sdsm has been among the most widely used models for climate change impact studies and therefore can serve as a base model for comparing the performance of other downscaling methods 3 1 3 genetic programming gp genetic programming gp having its roots in genetic algorithm ga is an evolutionary algorithm working on the principles of survival of the fittest and natural selection sachindra et al 2018 sivapragasam et al 2008 however it is slightly different from genetic algorithm in the way it utilises the parse tree structure for searching the best solutions genetic programming uses the bottom up approach and therefore does notinvolve apriori assumptions on the structural relationship between the predictors and the predictand rather it searches for a suitable relationship that is constituted by i mathematical expression ii logical statements or iii a set of mathematical functions and arranged in an unfamiliar pattern maheswaran and khosa 2011 in general the gp implementation has two steps a creation of parse trees which involves identification of the probable set of basic mathematical functions e g cos sin tan summation subtraction exponent multiplication division and power and b formation of the terminal set which includes the predictand and the predictors these steps work iteratively to mimic the underlying process in this study the terminal set includes the ncep variables and the local precipitation and the set of functions used are exp sqrt power minimization of the sum of the squared error is used as the objective function to identify the best model structure here the gp parameters such as the number of generations initial population cross over and mutation rate are altered in such a way to get the best results discipulus tool is used for developing the gp models 3 1 4 artificial neural networks anns artificial neural networks are developed based on the neural system of living entities an ann is an effective tool for understanding the complex nonlinear relationships between inputs and outputs alizadeh et al 2017 in the past anns have been used in the fields of climatology and hydrology for different kinds of applications see kişi 2009 nourani et al 2009 belayneh et al 2014 okkan and fistikoglu 2014 ahmed et al 2015 maheswaran and khosa 2012 and vu et al 2016 for details an ann s ability to learn and simulate based on the inputs given is unparalleled for solving complex problems which singular models cannot perform rumelhart et al 1994 during the training erudition process the network runs multiple times it produces new outputs for each trial by changing the values of weights until the specified minimum error criterion is achieved maheswaran and khosa 2015 various functions such as feed forward neural network ff nn with back propagation generalized regression neural network grnn and nonlinear auto regressive with exogenous inputs narx can be used for capturing the input output relationship out of these the feed forward neural network with back propagation network ff nn and narx neural network narx nn are used here for downscaling precipitation as they have been more widely applied when compared to the other methods for detailed descriptions of these models the readers are referred to haykin 2008 and shanmuganathan 2016 among others the ff nn is a multiple layer network in which the weights are estimated through the propagation of the error gradient among the different anns the ff nn is one of the most efficient methods the ff nn model consists of several neurons that are stacked in layers and connected with each other the first layer and last layer are the input and output layers respectively all other intermediate layers are the hidden layers containing the neurons that carry the information in terms of weight there are many sub phases in the training period particularly adopting a function for activation to propagate signals to the other nodes the ff nn is trained with backpropagation algorithm which is most widely used especially in hydrologic applications if wi represents the vector of weights linking the vectors of input variables xt to the layer i then the output yt at layer i can be defined as 2 y t i f x t i w i where f is the transfer function which is nonlinear in nature the weights calculated using the ith neuron and the transfer function are repeatedly changed until the obtained output satisfies the minimum error criterion a detailed explanation about the functioning of the ff nn is provided in svozil et al 1997 the narx nn is a recurrent dynamic neural network having feedback connection enclosing several layers of the network the narx nn generally works in two different modes parallel and series in the parallel mode the output is fed to the input of the ff nn as part of the standard narx architecture in the series mode however the true output is used as the feedback the input for the narx nn model consists of two parts the external input and the previous output of the network in this study the functional form of the narx nn model is assumed as 3 p t f c t c t 1 c t n where pt is the precipitation state ct is the set of climatic variables on the tth day and ct n is the set of predictor variables on the t n th day the network function f is identified through training mishra and sharma 2018 showed that the model results improved when the long and short term memory in the input climatic variables were taken into consideration while downscaling precipitation because of this in this study lagged input variables are also considered for capturing the long and short term memory of the climatic system while downscaling 3 2 wavelet based models for downscaling 3 2 1 wavelets wavelet transform provides the time frequency representation of signals by transforming the original signal into a series of decompositions using a wavelet function as the basis it is different from fourier transform because of its ability to capture the nonstationary and abrupt features in the time domain grossmann and morlet 1984 rathinasamy et al 2014 discrete wavelet transform dwt and continuous wavelet transform cwt are two types of wavelet transforms the former operates on dyadic scales while the latter operates on all scales and is used for analyzing continuous functions mallat 1999 discrete wavelet transform can be obtained either through the mallat or through the à trous wavelet transform alternatively known as the maximal overlap discrete wavelet transform modwt percival and walden 2000 the elementary idea of modwt is to use any redundant information from the original series to fill the gaps here the original time series is passed through a low pass filter and a high pass filter to obtain the details and the smoothed version respectively maheswaran and khosa 2012a 2012b consider the original time series y t which may also be denoted as so t further smoother versions of so t may be derived using 4 s j t l h l s j 1 t 2 j 1 l in eq 4 j takes values from 1 to j defined by the levels of decomposition and h is a low pass filter with compact support for the haar wavelet function h is defined as 1 2 1 2 similarly for the b3 spline wavelet h can be defined as 1 16 1 4 3 8 1 4 1 16 maheswaran and khosa 2012a 2012b from the smoother version of the y t at levels j and j 1 the detail component of y t at level j is written as 5 d j t s j 1 t s j t where the set d1 d2 dj sj represents the additive wavelet decomposition of the data up to resolution level j and sj is the residual component or the approximation here unlike the classical dwt the decimation is avoided thus resulting in components at different timescales to be of the same length rathinasamy et al 2014 in this study wavelet decomposition of the time series is carried out using the wmtsa toolbox percival and walden 2000 3 3 development of downscaling models fig 4 shows the overall schematic of the methodology used for the models in this study the methodology adopted can be delineated into the following steps 3 3 1 step1 selection of predictors many past studies have emphasized that the selection of the candidate predictors should be made based on their physically relevant relationship with precipitation and availability in both the ncep and gcm outputs ren et al 2018 sehgal et al 2018 in this study a set of candidate predictors is identified separately for each of the five grid points considered by using the pearson correlation coefficient for the period from 1948 to 2017 it is relevant to note at this point that many studies generally use only the pearson correlation coefficient zero lag correlation for selecting the potential predictors however in the present study in addition to the zero lag correlation lagged correlation between the candidate predictors and the observed precipitation is used since many studies kannan and ghosh 2011 mishra et al 2017 have shown the presence of short and long term memory in the relationship between precipitation and climatic variables 3 3 2 step 2 standardization the selected candidate predictors from the ncep are standardized to avoid systematic bias and reduce the instability created by the difference in magnitude in the models in this study the data are standardized by subtracting the mean of the data and dividing by its standard deviation 3 3 3 step 3 model development a single scale models mlr gp ff nn and narx nn once the potential predictors are determined cross correlation function is applied between the potential predictors and precipitation to determine the lag up to which significant correlation 95 exists between the variables based on the cross correlation values the optimal lag values for each of the predictors are obtained the variables with significant cross correlation are considered as the input for the downscaling models and the local precipitation as the output the entire data set is divided into training and validation sets in the ratio of 70 30 the models are trained using the sum of squared errors as the objective function further the model results are evaluated using several performance measures to compare the results b wavelet transform based hybrid models wt mlr wt gp wt ff nn and wt narx nn after selecting the potential predictors the maximal overlap discrete wavelet transform modwt is applied to all the variables to obtain the decomposition at various scales for this purpose a clear understanding about the choice of the mother wavelet and the level of decomposition is required as these factors influence the amount of features captured in the resulting data set a good level of decomposition helps in obtaining the required features to develop a good relationship between the variables whereas unnecessary levels may increase the redundant components based on the approach followed by lakhanpal et al 2017 the optimum level of decomposition is selected as 10 whereas haar wavelet is selected as the mother wavelet based on the works of maheswaran and khosa 2012 and sehgal et al 2014 it must be noted that after decomposition of the input data the number of input variables would be many therefore to reduce the input dimension of the models the cross correlation coefficient at 95 confidence interval is used for selecting the levels that have significant correlation with precipitation maheswaran and khosa 2012a 2012b after selecting the significant levels of the decomposed predictors the models are developed with these as the input predictors and the daily precipitation as the output this would result in a single model with significant variables across all the levels of decomposition as inputs maheswaran and khosa 2012a 2012b in contrast to developing an individual model for each of the levels rashid et al 2016 in this study multiple linear regression mlr genetic programming gp feed forward neural network ff nn and nonlinear auto regressive with exogenous inputs neural networks narx nn models are used for developing the wavelet neural network hybrid models these hybrid models are denoted as wt mlr wt gp wt ff nn and wt narx nn respectively 3 4 performance evaluation measures in this study several performance evaluation measures are used to analyze the results obtained from the downscaling models these include the normalized root mean square error nrmse nash sutcliffe efficiency coefficient nse nash and sutcliffe 1970 coefficient of determination r2 willmott 1982 and percent bias pbias gupta and sorooshian 1999 the mathematical equations for these performance measures are as follows 6 nrmse i 1 n y i obs y i sim 2 n 1 σ obs 7 nse 1 i 1 n y i obs y i sim 2 i 1 n y i obs y mean obs 2 8 r 2 i 1 n y i obs y mean obs y i sim y mean sim i 1 n y i obs y mean obs 2 y i sim y mean sim 2 2 9a pbias μ μ sim μ obs μ obs 9b pbias σ σ sim σ obs σ obs where y i obs is the ith observed data y i sim is the ith simulated data y i m e a n is the mean of the observed data σ obs is the standard deviation of the observed data n is the number of observations and μ sim and σ sim denote the model simulated value of the mean and standard deviation whereas μ obs and σ obs are the corresponding statistics for the observed value if nse and r2 values are close to 0 then the performance of the model in downscaling is considered highly unsatisfactory if nse and r2 values are close to 1 then the model performance is almost perfect if the value of nrmse is low then it is an indication that the downscaled values are similar to the observed values if the value of nrmse is high then the downscaled results are not close to the observed ones if pbias is positive it indicates an over estimation of the values while a negative pbias value indicates an under estimation of the values sachindra et al 2018 showed that the percentage bias can be used as a reliable measure for evaluating the performance of the model results 3 5 extreme precipitation accuracy in this study the ability of the models in capturing the extreme precipitation events is also investigated for this purpose a rainfall event is considered as an extreme precipitation event when the daily precipitation exceeds 50 mm following the guidelines adopted by the world meteorological organization wmo for heavy rain event in the indian subcontinent the accuracy of downscaling in terms of capturing the extreme precipitation is estimated as 10 e x t r e m e p r e c i p i t a t i o n a c c u r a c y observed n o o f e v e n t s g r e a t e r t h a n 50 m m s i m u l a t e d n o o f e v e n t s g r e a t e r t h a n 50 m m observed n o o f e v e n t s g r e a t e r t h a n 50 m m 100 4 analysis and results 4 1 model application in any downscaling strategy the selection of the predictors and its spatial domain plays a major role based on the climatic characteristics of the krishna river basin and reports by some previous studies lakhanpal et al 2017 sehgal et al 2018 a total of 16 probable predictors is considered as listed in table 1 the list of probable predictors considered here is similar to that used by sehgal et al 2018 for the krishna river basin sachindra et al 2018 for australia and yang et al 2017 for a region in china in the present study a spatial domain of 4 4 grid points placed at a spatial resolution of 2 5 in the latitudinal and longitudinal directions respectively is considered this spatial extent is selected based on previous studies for the same region sehgal et al 2018 therefore in the present study spatial averaging of the ncep data is considered to reduce the number of predictor variables this is important since the selection of too many variables would result in an over training of the models due to redundant components sehgal et al 2018 fig 5 shows the cross correlation between the candidate predictors and precipitation for grid 1 considered in this study figures for the remaining four grids are shown in fig s1 in supplementary information it is observed that many of the candidate predictors e g mean sea level pressure meridional velocity component surface air temperature zonal velocity component share a lagged relationship with precipitation showing the presence of long and short term memory in the climate system as observed by mishra et al 2017 it is also observed that all the 16 selected predictors have a significant correlation with precipitation and therefore are considered as potential predictors further for developing separate models for wet and dry periods the entire precipitation data set is categorized into monsoon season and non monsoon season the period from june to october is considered as the monsoon season and the period from november to may is considered as the non monsoon season based on the precipitation occurrence amount for the study area for training and validating the downscaling models the observed precipitation and ncep data are split into two parts with a ratio of 70 30 for each grid and season in the case of the mlr models all the potential predictors are considered for the regression analysis it is observed that the number of potential predictors varies and so separate models for each grid points are developed in the case of sdsm all the probable predictors are considered as the model predictors and the model is developed using the sdsm 5 2 toolbox gonzález rojí et al 2019 for the gp models a population size of 500 is chosen and the mutation and crossover rates are kept at high value following sivapragasam et al 2008 and selle and muttil 2011 the mean squared error mse is used as the model fitness function the mathematical function set is selected so that a meaningful relationship can evolve between the predictand and predictors again separate gp models are developed for each grid and season regarding the ann based models ff nn and narx nn training is carried out using the levenberg marquardt lm algorithm due to its fast convergence accuracy robustness and reliability adamowski and karapataki 2010 adamowski and sun 2010 using a trial and error procedure the optimal number of neurons for each model is selected the optimal number of hidden neurons for each of the models is found using a trial and error procedure the number of hidden neurons is varied from 10 to 20 and the best value is chosen based on the rmse for the wt mlr wt gp wt ff nn and wt narx nn models decomposition of the climatic variable is done using the haar wavelet zhang 2019 each of the predictor variables is decomposed up to 10 levels which produce 11 components of the original data set at different scales to be used as input for wavelet models following the procedure outlined in maheswaran and khosa 2012 4 2 results of performance analysis after the models are trained and validated using the corresponding data to a satisfactory level the performance measures nrmse nse r2 and pbias are computed for each of the five grids for assessing the ability of the models to downscale precipitation figs 6 and s2 in supplementary information present the results of the models for calibration during monsoon and non monsoon seasons respectively fig 7 presents the validation results for the monsoon season similar results are also obtained for the non monsoon season not presented here the results for the calibration set indicate that the wavelet based model shows robust performance and performs better than the other models for all the five grids in both the seasons for the non monsoon season at grid 1 refer to fig s21 the performance measure r2 is the highest for the wavelet based feed forward neural network wt ff nn and wavelet narx neural network wt narx nn models for singular and hybrid machine learning models i e ff nn narx nn gp wt mlr and wt gp the r2 values are 0 62 0 64 0 52 0 51 and 0 59 respectively and the nse values are 0 46 0 45 0 46 0 45 and 0 49 respectively for the same grid the performances of mlr and sdsm are not superior when compared to the machine learning models for the monsoon season and for grid 1 the results in terms of nse r2 show that the wt ff nn and wt narx nn models have values of 0 62 0 79 and 0 55 0 69 respectively whereas the ff nn narx nn gp wt mlr and wt gp models have values of 0 46 0 62 0 45 0 63 0 46 0 52 0 45 0 51 and 0 49 0 59 respectively the performances of mlr and sdsm are comparatively poorer similar trend in the performance of the models is also observed for all the other four grids comparing the performance of the wavelet based models across the five grids the model performance in terms of r2 is relatively poorer for the wet locations grid 5 and grid 3 than for the dry locations grid 1 and grid 2 further for grid 1 during the monsoon season refer to fig 6 nrmse values of the machine learning models ff nn narx nn and gp are 84 31 mm 87 36 mm and 84 48 mm and for the wavelet hybrid models wt mlr wt gp wt ff nn and wt narx nn are 87 54 mm 81 45 mm 82 21 mm and 77 71 mm respectively the results obtained using all the models considered at all the five grids show that the wavelet based downscaled estimates from wt ff nn and wt narx nn are the closest to the observed values the performance measures for the validation period see fig 7 show similar behaviour to that for the calibration period interestingly the wt mlr has higher nrmse when compared to the single scale machine learning models showing the necessity of nonlinear models in capturing the downscaling relationships according to sachindra et al 2018 percentage bias is a reliable measure of the model performance as it enables a reasonable comparison of the ability of the downscaling models as seen from the results refer to figs 6 and 7 the percentage bias in the mean of the model simulations has a similar pattern in both wet and dry places for the calibration period the mlr models show the highest percentage bias mean and the wt narx nn models show the lowest percentage bias it is important to note that for both the dry and wet locations the wavelet based models wt ff nn and wt narx nn have the lowest percentage bias when compared to the other models between the wavelet based hybrid models the wt narx nn models have better performance for all the grids further the percentage bias is negative for most of the models indicating that the models are underestimating the rainfall values the biases in the standard deviation of the rainfall estimates indicate that the performances of the mlr and sdsm for both the seasons refer to figs 6 and s2 are not acceptable for any of the grids as they are high and in the order of 29 to 50 on the contrary the machine learning methods such as ff nn narx nn and gp perform better in capturing the standard deviation among the machine learning models the wavelet based hybrid models yield comparatively less percentage bias in the standard deviation for example for grid 5 fig 6 the percentage bias for the calibration set for the monsoon season are found to be 31 1 25 5 14 4 10 24 12 2 28 54 12 42 1 5 and 2 8 for the mlr sdsm gp ff nn narx nn wt mlr wt gp wt ff nn and wt narx nn models respectively similar results are obtained for the other grids wherein the values of pbias σ are in the range of 6 08 to 18 07 for wt ff nn and wt narx nn models comparing the results across all the five grids most of the models underestimate the standard deviation for relatively dry grids grid 1 and grid 2 and overestimate for the wet grids grid 5 and grid 3 similar findings have also been reported by some past studies sachindra et al 2018 anandhi et al 2008 this might be due to the fact that catchment scale dynamics may depend not just on large scale climatic factors but also on regional factors fig 8 shows the cumulative distribution function cdf for the different model outputs and the observed precipitation at grid 1 cdf analysis is also carried out for the remaining four grids and the results are shown in fig s3 of supplementary information as seen the performance of the mlr models is not satisfactory in capturing the peak extreme rainfall values for almost all the five grids the cdf results obtained from the sdsm and the singular machine learning models ff nn narx nn and gp show that these models have good performance the results also indicate that the hybrid models wt mlr and wt gp produce better results than their singular versions but still fail to capture the observed rainfall pattern at most of the grids especially the extreme values this is particularly the case for grid 2 on the other hand the wavelet based models wt ff nn and wt narx nn perform relatively better in capturing the local rainfall events for all the five grids the cdfs obtained from these models are close to that of the observed rainfall with extreme rainfall events also captured reasonably well between the two wavelet based models there is no significant difference in terms of model performance to further evaluate whether the distributions obtained from the models are statistically similar to the ones obtained from the observed data at 95 confidence level the kolmogorov smirnov nonparametric goodness of fit test is used fig 9 shows the results from the different models for the five selected grids in terms of the p values the results indicate that the wavelet based models have p values greater than 0 05 for all the grids however the mlr models have p values less than 0 05 indicating that they perform poorly in reproducing the distribution of the daily precipitation other models such as gp sdsm ff nn narx nn wt mlr and wt gp perform moderately at some grids and poorly at others all these results show that the wavelet based hybrid models wt ff nn and wt narx nn are robust in capturing the overall trend and pattern in the daily precipitation and perform better when compared to statistical and other traditional machine learning models to further understand the effectiveness of the downscaling models the relationship between the raw precipitation ncep precipitation data set and observed precipitation is examined for this purpose the raw precipitation data from the ncep data for the period from 1948 to 2017 are used fig 10 a shows for instance the scatter plot between the observed precipitation at grid 3 randomly selected and the raw ncep data obtained at the closest location to grid 3 fig 10 b j shows the scatterplots between the observed daily precipitation and the model results for the validation set at grid 3 the scatter plots indicate that the accuracy of the downscaled data is greater than that of the raw data without downscaling further the scatterplots show that the models have the ability to capture the observed rainfall pattern and amount although with varying degrees the mlr and sdsm underestimate the rainfall particularly for values greater than 40 mm the ff nn gp narx nn wt mlr and wt gp models perform moderately in capturing the rainfall events the wavelet based models i e wt ff nn and wt narx nn perform comparatively better in capturing both the extreme events and the moderate low rainfall events with the values closer to the 45 line when compared to that from the other models between the two wavelet based hybrid models considered in this study the wt narx nn outperforms the wt ff nn model in terms of its ability to capture the observed rainfall similar results are also obtained for the other four grids not presented table 2 shows the actual number of extreme events observed and the number of events simulated by the different models along with the percentage accuracy for the validation period the results show that the wavelet based hybrid models capture extreme events with greater accuracy than the other models further it is observed comparing the results among the single scale models that the nn models and gp models outperform the mlr and sdsm for instance for grid 1 the wavelet based models show an accuracy of 78 and 90 whereas the ff nn model and the gp model have an accuracy of 72 and 47 respectively 5 discussion in this study wavelet based hybrid models were proposed for downscaling daily precipitation and their performances were compared with those of some key traditional and other modern methods including mlr sdsm gp and ann models among the nine downscaling methods applied in this study machine learning methods were found to generally outperform the basic mlr and sdsm among the single scale machine learning models the narx nn model outperformed the ff nn model and the gp model the better performance of the narx nn model may be due to its ability in capturing the long and short term memory relationship between the climatic variables and precipitation as also discussed in mishra et al 2017 although the sdsm is the most popular model and has been widely used in numerous studies khan et al 2006 wetterhall et al 2006 tryhorn and degaetano 2011 it was unable to provide robust results in the present study when compared to the gp and ann models the sdsm underestimated the quantile precipitation at almost all the five selected grids this finding is in accordance with the observation reported by chen et al 2011 who showed that an svm based model outperformed the sdsm in the downscaling of daily precipitation interestingly our results showed that the performance of the gp models was reasonable in comparison with the ann models this observation was also made by sachindra et al 2018 while investigating the effectiveness of the downscaling models for monthly precipitation overall our results manifest that the wavelet based neural network models wt ff nn and wt narx nn are robust compared to the traditional and other modern machine learning methods considered in this study this is in accordance with the broader consensus about the performance of the wavelet based hybrid models wherein the application of wavelets enhances the ability of the models to capture the multiscale relationship among the variables for example in a recent study sun et al 2019 revealed that the models with wavelet components performed better than the regular models i e those without wavelet coupling for streamflow forecasting rashid et al 2016 showed that the wavelet based gamlss models resulted in better downscaling estimates for monthly precipitation than the single scale gamlss models in another study maheswaran and khosa 2015 showed that the wavelet based model performed superior to the simple nonlinear models for rainfall forecasting in the present study the wavelet based hybrid models performed better in terms of capturing both the overall precipitation characteristics and the extreme precipitation events further the bias in terms of the standard deviation and mean of the downscaled precipitation was comparatively less for the hybrid models one possible reason for the improvement in the performance of the wavelet based hybrid models could be that the decomposition of the climatic variables at multiple scales unravelled their hidden relationship with precipitation this is evident from the results shown in fig 11 for grid 2 where the values show the correlation between the predictor variables and precipitation with and without wavelet decomposition the correlation between precipitation and mean sea level pressure mslp is 0 08 without applying any decomposition whereas the correlation between decompositions of mslp d4 to d9 and precipitation varies from 0 17 to 0 39 a similar pattern can be observed for several other variables as well e g uas p500 p850 r500 r850 it can be seen that there is a considerable increase in the correlation between precipitation and the decomposition of the climatic variables this shows that wavelet decomposition can help in unravelling the hidden relationship between the predictors and precipitation the decomposition of the atmospheric variables shows higher correlation with precipitation when compared to the correlation obtained between precipitation and atmospheric variables at the original scale similar behaviour was also observed for the other four grids not shown here due to space constraints thus it may be suggested that wavelets aid in unravelling the relationship between local precipitation and large scale climatic variables and improve the overall performance of the downscaling models that are based on them in the present study ncep data was used for model development and validation however the proposed methodology can be used for gcm data by using the gcm data for model development instead of ncep data several studies including those by sehgal et al 2018 and lakhanpal et al 2017 have shown that the ncep data and gcm data more specifically canesm are in good agreement for the krishna river basin therefore one may conclude from this study that the approach can be extended for the gcm data 6 conclusions in this study nine downscaling models including mlr sdsm gp ann and wavelet based hybrid models were developed and applied for downscaling daily precipitation at five selected grid points in different locations in the krishna river basin in india using the ncep data and the imd data over the period 1948 2017 calibration and validation of the models were carried out based on the results obtained for the five selected grids in terms of several performance measures nrmse nse r2 and pbias and their ability to capture the extreme events it was observed that the wavelet based hybrid models performed better when compared with the other traditional mlr and sdsm and machine learning models gp and ann the wavelet based models wt ff nn and wt narx nn showed better ability in capturing the extreme rainfall events and the error associated with the downscaled precipitation was low the outcomes of this study indicate the utility of wavelets in improving the performance of the hybrid models for downscaling of precipitation and other hydrologic analyses nevertheless the present outcomes regarding the performance of the wavelet based hybrid models need to be further supported and confirmed through their applications to other regions across india and around the globe having different climatic conditions and meteorological factors credit authorship contribution statement yeditha pavan kumar conceptualization data curation formal analysis investigation methodology writing original draft rathinasamy maheswaran conceptualization data curation formal analysis funding acquisition investigation methodology resources software supervision validation visualization writing original draft writing review editing ankit agarwal investigation methodology resources software validation visualization writing original draft writing review editing bellie sivakumar validation visualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements rm gratefully acknowledges the funding received through the inspire faculty award ifa 12 eng 28 from the department of science and technology india aa acknowledges the funding support provided by the indian institute of technology roorkee through faculty initiation grant number iitr sric 1808 f i g and coprepare project funded by ugc and daad under the igp 2020 2024 bs acknowledges the support from the iit bombay seed grant rd 0519 irccsh0 027 the authors would like to thank the three anonymous reviewers for their constructive comments and useful suggestions on an earlier version of the manuscript which led to significant improvements to the presentation of the work appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126373 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4462,downscaling of local daily precipitation from large scale climatic variables is required for assessing the impact of climate change on hydrology and water resources this study proposes wavelet transform wt based feed forward neural network ff nn and nonlinear auto regressive with exogenous inputs network narx nn models for downscaling daily precipitation the models are applied to a large river basin the krishna river basin in the indian subcontinent several climatic variables including geo potential heights wind direction vorticity humidity air temperature mean sea level pressure meridional velocity at surface and 500hpa and 850hpa levels are considered based on their statistical correlations the results are evaluated using different performance measures and the ability of the models to capture the extreme events at five selected grid points in different locations having varying climatic characteristics is assessed the performance of the proposed wavelet based models is also compared with that of four different traditional and recent downscaling methods multiple linear regression mlr statistical downscaling model sdsm genetic programming gp and artificial neural networks anns the results reveal that the wavelet based neural network models wt ff nn and wt narx nn are robust compared to the other methods in terms of their ability to capture the regional precipitation patterns and the extreme events the improvement in the wavelet based models can be attributed to their ability to unravel the hidden relationship between the predictors and precipitation it is also observed that there is considerable increase in the correlation between precipitation and the decomposed climatic variables all these results suggest that wavelets aid in unravelling the relationship between local precipitation and large scale climatic variables and improving the overall performance of the downscaling models abbreviations ann nn artificial neural networks neural networks ff nn feed forward neural networks gcm general circulation model gp genetic programming mlr multiple linear regression narx nn nonlinear auto regressive with exogenous inputs neural networks ncep national center for environmental prediction rcm regional climate model sdsm statistical downscaling model wt ff nn wavelet feed forward neural networks wt gp wavelet genetic programming wt mlr wavelet multiple linear regression wt narx nn wavelet nonlinear auto regressive with exogenous inputs neural networks keywords precipitation downscaling general circulation models statistical downscaling genetic programming neural networks wavelets 1 introduction studying the impact of global warming induced climate change on water resources has become a key area of research in recent decades to project future climate change scenarios general circulation models gcms which are computer models working at the global scale are commonly used these models generally produce climate outputs e g precipitation temperature at coarser spatial scales typically having horizontal grid size ranging from 100 km to 600 km such coarse scale climate outputs however are not appropriate for most hydrologic studies since hydrologic studies generally require data at much finer spatial scales corresponding to catchments therefore to properly study the impact of climate change on hydrology and water resources it is required to downscale the coarse scale climate outputs to catchment scale hydrologic variables sehgal et al 2018 wilby et al 1998 downscaling of gcm outputs assumes that the local climate is a function of both the global atmospheric features large scale climatic conditions and the local conditions topography land surface characteristics generally deriving local climatic characteristics from gcm outputs through downscaling is a multistep process involving a multitude of assumptions and approximations the methods for downscaling can be broadly classified into 1 dynamic downscaling and 2 statistical downscaling dynamical downscaling uses an appropriate gcm to generate regional climate models rcms to be used at regional scales srinivas et al 2014 the rcms consider the coarse scale gcm outputs as boundary conditions and detailed physical processes complex terrain and land surface characteristics at regional scale to model the regional scale climate the rcms have better ability in capturing the basic regional patterns in time and space contrary to this statistical downscaling models use empirical relationships wilby et al 1998 1999 between predictors atmospheric processes and predictands local climatic variables such as rainfall to generate the data at finer spatiotemporal resolutions even though dynamical downscaling has shown robust performances it requires large amount of data high level of expertise to analyze the results and is also computationally intensive which make it beyond the reach of institutions in most countries fowler et al 2007 hay and clark 2003 sachindra et al 2014 mahmood et al 2016 the number of experiments for climate scenarios is limited by the intensive computational requirements which is a function of resolution domain size and accuracy further the rcms are limited by systematic errors branching from the gcms as the rcms are strongly dependent on the outputs from the gcms compared to the rcm statistical downscaling methods have gained much wider acceptance due to their simplicity and low computational burden okkan and inan 2015 rashid et al 2015 sachindra et al 2018 sachindra and perera 2016 sehgal et al 2018 however establishing robust linkage between local climatic variables and large scale atmospheric processes requires long historical data hay and clark 2003 depending on the assumed relationship statistical downscaling consists of a heterogeneous group of methods that vary in sophistication and applicability these methods are classified into three categories stochastic weather generators weather classification based approaches and regression based methods stochastic weather generators involve fitting probability distribution function to the variable the weather classification based methods involve classification of the state variables into weather types using the k means algorithm in addition to the stochastic weather generation the weather generator methods are based on the statistical properties of the climatic variables they are sensitive to outliers in the training set and involve the generation of an ensemble of time series and statistical analysis of the results furthermore the weather generator and weather classification methods are data intensive need large computational resources and do not have prediction capability outside the historical data range these issues led the regression based methods to gain popularity especially due to their ease in application sachindra et al 2018 the regression based downscaling methods primarily depend on developing a predictor predictand relationship some of these methods include multiple linear regression mlr sachindra et al 2014 joshi et al 2015 duhan and pandey 2015 ravansalar et al 2017 generalized linear model glm asong et al 2016 beecham et al 2014 artificial neural networks anns yeditha et al 2020 chithra et al 2016 hassan esfahani et al 2015 genetic programming gp coulibaly and burn 2004 sachindra et al 2018 random forests regression rfr hutengs and vohland 2016 pang et al 2017 mishra et al 2017 support vector machines svms kannan and ghosh 2011 srinivas et al 2014 and extreme learning machines elms sachindra et al 2018 zhu et al 2019 these studies have shown that the regression based downscaling methods are robust and accurate in capturing the local climatic patterns further the results from the past studies indicate that techniques based on artificial intelligence ai which take into account the complexity and nonlinearity in the relationship between predictors and predictands perform better in comparison with the other linear methods many past studies kurths et al 2019 guntu et al 2020 maheswaran and khosa 2015 rashid et al 2016 have reported that the relationship between rainfall and atmospheric variables is a function of time scale for instance maheswaran and khosa 2015 studying the cauvery river basin in india showed that the relationship between climatic indices and rainfall varied within the time frequency domain therefore the popular traditional approach to downscaling based on single scale models might not be able to capture the variability at different time scales rashid et al 2016 in light of this limitation with single scale models there has been a special emphasis on the multi scale dynamics of the climate system involving interactions and feedbacks among different processes at different temporal and spatial scales agarwal et al 2017 2018 2019 carey et al 2013 maheswaran and khosa 2012a 2012b addison 2005 torrence and compo 1998 in this regard wavelets have become a highly popular tool with their inherent advantages including time frequency localization and multiscale resolution wavelets decompose the given time series into scale specific components as proxies of the physical processes at those scales several studies kaheil et al 2008 adamowski and sun 2010 maheswaran and khosa 2012a 2012b 2014 2009 rathinasamy et al 2013 niu and sivakumar 2013 beecham et al 2014 shafaei and kisi 2017 nourani et al 2018 nourani and farbouddam 2019 baghanam et al 2019 rashid et al 2018 have shown that wavelet transform based modelling has aided in unravelling the multiscale dynamics in different processes a recent study by sun et al 2019 investigated the efficiency of the wavelet based models over the single stochastic ann regression models for daily streamflow forecasting and reported better performance of the wavelet based models on similar lines ravansalar et al 2017 used wavelet transform based linear genetic programming wlgp models for predicting monthly streamflow in iran they found that the wavelet based models significantly improved the flow prediction in comparison with gp ann and mlr models djerbouai and souag gamane 2016 showed that the accuracy of drought prediction in the karkheh basin in iran improved with wavelet pre processing of inputs when compared to single models without wavelets most of the above studies have combined the wavelets with other models in developing hybrid models for hydrologic forecasting applications even though there has been extensive research on the application of wavelets for hydrologic forecasting there have only been very few studies on the application of wavelets for statistical downscaling among such studies kaheil et al 2008 combined wavelets and svm for downscaling and forecasting evapotranspiration rashid et al 2015 used the wavelet gamlss model for downscaling of precipitation in the onkaparinga river catchment in south australia the results showed that the wt gamlss models were accurate in reproducing the time series properties when compared to the orthodox gamlss models more recently lakhanpal et al 2017 used a wavelet based second order volterra model for downscaling monthly temperature in the krishna river basin in india and found that the wavelet volterra model performed better when compared to other models including anns and mlr sehgal et al 2018 used a similar model for downscaling monthly precipitation using the ncep data and showed that the model results were closer to the local precipitation it is important to note that most of the above mentioned studies have focussed on downscaling of precipitation and other variables at the monthly scale and there have hardly been any studies on evaluating the hybrid models for precipitation downscaling at the daily scale despite the fact that downscaling of precipitation at the daily scale and even sub daily scales is very important especially in the context of extreme events furthermore thus far there has not been any serious attempt to conduct a detailed investigation to compare different methods for downscaling of daily precipitation in view of these the present study proposes the application of wavelet based hybrid methods for statistical downscaling of daily precipitation the proposed wavelet based hybrid models are applied for downscaling daily precipitation in five selected grid points in different locations in the krishna river basin in india with varying climatic characteristics the performances of these models are also compared with some popular models including mlr sdsm gp and single scale ann models 2 study area and data the krishna river basin being the fourth largest basin inthe indian peninsula originates at mahabaleshwar in the state of maharashtra india and joins the bay of bengal at hamasaladeevi near koduru inandhra pradesh fig 1 the catchment of the basin spreads between 73 to 82 e and 13 to 19 n covering a total area of 260 401 km2 fig 2 a d shows the topography mean annual rainfall distribution land use land cover lulc and climate classification respectively of the krishna river basin most of the area of the basin is flat terrain having semi arid characteristics according to köppen climate classification and with agricultural land covering about 78 of the total area the mean annual rainfall of the basin ranges from 420 mm mid parts to 4000 mm western regions with much of it occurring during the southwest monsoon season june october in the present study five selected grid points indicated by numbers 1 to 5 in fig 1 bottom in different locations in the krishna river basin are considered for developing and applying the downscaling methods and for comparing their performances these grids for downscaling of data are selected based on the climatic classification of the region as shown in fig 2 d for these grids precipitation data are made available from the india meteorological department imd for the period from january 1 1901 to december 31 2019 the data are available at the daily resolution in gridded format and have a spatial resolution of 0 25 x 0 25 pai et al 2014 fig 3 shows for example the rainfall distribution at one of the five grids grid 1 with the mean annual precipitation value also indicated therein the data related to the different atmospheric variables were downloaded from the national center for environmental prediction and national center for atmospheric research ncep ncar website https www esrl noaa gov psd data gridded data ncep reanalysis html for the period from january 1 1948 to december 31 2017 these data have a spatial resolution of 2 5 x 2 5 taking into account the climatic characteristics of the krishna river basin including those reported by previous studies sehgal et al 2018 a total of 16 predictors is considered table 1 the predictor variables are selected in such a way that they are strong contributors to the variability of precipitation in the region the influence of temperature wind velocity geopotential heights and humidity on precipitation is very widely known and thus their utility in downscaling precipitation sachindra et al 2018 3 methodology in this study several traditional and modern methods are applied for downscaling daily precipitation in the krishna river basin and for comparing their performances these methods include multiple linear regression mlr statistical downscaling model sdsm genetic programming gp artificial neural networks anns and wavelet neural network wnn hybrid model the first four models are single scale models while the last one is a multiscale model a brief description of these methods is presented next 3 1 single scale models 3 1 1 multiple linear regression mlr multiple linear regression mlr is a form of regression analysis through which a linear relationship between a predictand variable y and multiple predictor variables x1 x2 xn is established a typical linear regression equation takes the form 1 y a0 a1x1 a2x2 a3x3 anxn the parameters a0 a1 an are estimated using the simple least squares method a detailed description of multiple linear regression can be seen in olive 2017 among others 3 1 2 statistical downscaling model sdsm the statistical downscaling model sdsm is described as a hybrid between the multiple linear regression model and a stochastic weather generator wilby et al 2002 wetterhall et al 2006 this method has the advantages of both regression based downscaling and the stochastic weather generators tryhorn and degaetano 2011 a number of studies wilby et al 1999 harpham and wilby 2005 wetterhall et al 2006 have used the sdsm for downscaling precipitation and temperature for different study regions around the world extensive details about the sdsm can be obtained from wilby et al 2002 among others the sdsm has been among the most widely used models for climate change impact studies and therefore can serve as a base model for comparing the performance of other downscaling methods 3 1 3 genetic programming gp genetic programming gp having its roots in genetic algorithm ga is an evolutionary algorithm working on the principles of survival of the fittest and natural selection sachindra et al 2018 sivapragasam et al 2008 however it is slightly different from genetic algorithm in the way it utilises the parse tree structure for searching the best solutions genetic programming uses the bottom up approach and therefore does notinvolve apriori assumptions on the structural relationship between the predictors and the predictand rather it searches for a suitable relationship that is constituted by i mathematical expression ii logical statements or iii a set of mathematical functions and arranged in an unfamiliar pattern maheswaran and khosa 2011 in general the gp implementation has two steps a creation of parse trees which involves identification of the probable set of basic mathematical functions e g cos sin tan summation subtraction exponent multiplication division and power and b formation of the terminal set which includes the predictand and the predictors these steps work iteratively to mimic the underlying process in this study the terminal set includes the ncep variables and the local precipitation and the set of functions used are exp sqrt power minimization of the sum of the squared error is used as the objective function to identify the best model structure here the gp parameters such as the number of generations initial population cross over and mutation rate are altered in such a way to get the best results discipulus tool is used for developing the gp models 3 1 4 artificial neural networks anns artificial neural networks are developed based on the neural system of living entities an ann is an effective tool for understanding the complex nonlinear relationships between inputs and outputs alizadeh et al 2017 in the past anns have been used in the fields of climatology and hydrology for different kinds of applications see kişi 2009 nourani et al 2009 belayneh et al 2014 okkan and fistikoglu 2014 ahmed et al 2015 maheswaran and khosa 2012 and vu et al 2016 for details an ann s ability to learn and simulate based on the inputs given is unparalleled for solving complex problems which singular models cannot perform rumelhart et al 1994 during the training erudition process the network runs multiple times it produces new outputs for each trial by changing the values of weights until the specified minimum error criterion is achieved maheswaran and khosa 2015 various functions such as feed forward neural network ff nn with back propagation generalized regression neural network grnn and nonlinear auto regressive with exogenous inputs narx can be used for capturing the input output relationship out of these the feed forward neural network with back propagation network ff nn and narx neural network narx nn are used here for downscaling precipitation as they have been more widely applied when compared to the other methods for detailed descriptions of these models the readers are referred to haykin 2008 and shanmuganathan 2016 among others the ff nn is a multiple layer network in which the weights are estimated through the propagation of the error gradient among the different anns the ff nn is one of the most efficient methods the ff nn model consists of several neurons that are stacked in layers and connected with each other the first layer and last layer are the input and output layers respectively all other intermediate layers are the hidden layers containing the neurons that carry the information in terms of weight there are many sub phases in the training period particularly adopting a function for activation to propagate signals to the other nodes the ff nn is trained with backpropagation algorithm which is most widely used especially in hydrologic applications if wi represents the vector of weights linking the vectors of input variables xt to the layer i then the output yt at layer i can be defined as 2 y t i f x t i w i where f is the transfer function which is nonlinear in nature the weights calculated using the ith neuron and the transfer function are repeatedly changed until the obtained output satisfies the minimum error criterion a detailed explanation about the functioning of the ff nn is provided in svozil et al 1997 the narx nn is a recurrent dynamic neural network having feedback connection enclosing several layers of the network the narx nn generally works in two different modes parallel and series in the parallel mode the output is fed to the input of the ff nn as part of the standard narx architecture in the series mode however the true output is used as the feedback the input for the narx nn model consists of two parts the external input and the previous output of the network in this study the functional form of the narx nn model is assumed as 3 p t f c t c t 1 c t n where pt is the precipitation state ct is the set of climatic variables on the tth day and ct n is the set of predictor variables on the t n th day the network function f is identified through training mishra and sharma 2018 showed that the model results improved when the long and short term memory in the input climatic variables were taken into consideration while downscaling precipitation because of this in this study lagged input variables are also considered for capturing the long and short term memory of the climatic system while downscaling 3 2 wavelet based models for downscaling 3 2 1 wavelets wavelet transform provides the time frequency representation of signals by transforming the original signal into a series of decompositions using a wavelet function as the basis it is different from fourier transform because of its ability to capture the nonstationary and abrupt features in the time domain grossmann and morlet 1984 rathinasamy et al 2014 discrete wavelet transform dwt and continuous wavelet transform cwt are two types of wavelet transforms the former operates on dyadic scales while the latter operates on all scales and is used for analyzing continuous functions mallat 1999 discrete wavelet transform can be obtained either through the mallat or through the à trous wavelet transform alternatively known as the maximal overlap discrete wavelet transform modwt percival and walden 2000 the elementary idea of modwt is to use any redundant information from the original series to fill the gaps here the original time series is passed through a low pass filter and a high pass filter to obtain the details and the smoothed version respectively maheswaran and khosa 2012a 2012b consider the original time series y t which may also be denoted as so t further smoother versions of so t may be derived using 4 s j t l h l s j 1 t 2 j 1 l in eq 4 j takes values from 1 to j defined by the levels of decomposition and h is a low pass filter with compact support for the haar wavelet function h is defined as 1 2 1 2 similarly for the b3 spline wavelet h can be defined as 1 16 1 4 3 8 1 4 1 16 maheswaran and khosa 2012a 2012b from the smoother version of the y t at levels j and j 1 the detail component of y t at level j is written as 5 d j t s j 1 t s j t where the set d1 d2 dj sj represents the additive wavelet decomposition of the data up to resolution level j and sj is the residual component or the approximation here unlike the classical dwt the decimation is avoided thus resulting in components at different timescales to be of the same length rathinasamy et al 2014 in this study wavelet decomposition of the time series is carried out using the wmtsa toolbox percival and walden 2000 3 3 development of downscaling models fig 4 shows the overall schematic of the methodology used for the models in this study the methodology adopted can be delineated into the following steps 3 3 1 step1 selection of predictors many past studies have emphasized that the selection of the candidate predictors should be made based on their physically relevant relationship with precipitation and availability in both the ncep and gcm outputs ren et al 2018 sehgal et al 2018 in this study a set of candidate predictors is identified separately for each of the five grid points considered by using the pearson correlation coefficient for the period from 1948 to 2017 it is relevant to note at this point that many studies generally use only the pearson correlation coefficient zero lag correlation for selecting the potential predictors however in the present study in addition to the zero lag correlation lagged correlation between the candidate predictors and the observed precipitation is used since many studies kannan and ghosh 2011 mishra et al 2017 have shown the presence of short and long term memory in the relationship between precipitation and climatic variables 3 3 2 step 2 standardization the selected candidate predictors from the ncep are standardized to avoid systematic bias and reduce the instability created by the difference in magnitude in the models in this study the data are standardized by subtracting the mean of the data and dividing by its standard deviation 3 3 3 step 3 model development a single scale models mlr gp ff nn and narx nn once the potential predictors are determined cross correlation function is applied between the potential predictors and precipitation to determine the lag up to which significant correlation 95 exists between the variables based on the cross correlation values the optimal lag values for each of the predictors are obtained the variables with significant cross correlation are considered as the input for the downscaling models and the local precipitation as the output the entire data set is divided into training and validation sets in the ratio of 70 30 the models are trained using the sum of squared errors as the objective function further the model results are evaluated using several performance measures to compare the results b wavelet transform based hybrid models wt mlr wt gp wt ff nn and wt narx nn after selecting the potential predictors the maximal overlap discrete wavelet transform modwt is applied to all the variables to obtain the decomposition at various scales for this purpose a clear understanding about the choice of the mother wavelet and the level of decomposition is required as these factors influence the amount of features captured in the resulting data set a good level of decomposition helps in obtaining the required features to develop a good relationship between the variables whereas unnecessary levels may increase the redundant components based on the approach followed by lakhanpal et al 2017 the optimum level of decomposition is selected as 10 whereas haar wavelet is selected as the mother wavelet based on the works of maheswaran and khosa 2012 and sehgal et al 2014 it must be noted that after decomposition of the input data the number of input variables would be many therefore to reduce the input dimension of the models the cross correlation coefficient at 95 confidence interval is used for selecting the levels that have significant correlation with precipitation maheswaran and khosa 2012a 2012b after selecting the significant levels of the decomposed predictors the models are developed with these as the input predictors and the daily precipitation as the output this would result in a single model with significant variables across all the levels of decomposition as inputs maheswaran and khosa 2012a 2012b in contrast to developing an individual model for each of the levels rashid et al 2016 in this study multiple linear regression mlr genetic programming gp feed forward neural network ff nn and nonlinear auto regressive with exogenous inputs neural networks narx nn models are used for developing the wavelet neural network hybrid models these hybrid models are denoted as wt mlr wt gp wt ff nn and wt narx nn respectively 3 4 performance evaluation measures in this study several performance evaluation measures are used to analyze the results obtained from the downscaling models these include the normalized root mean square error nrmse nash sutcliffe efficiency coefficient nse nash and sutcliffe 1970 coefficient of determination r2 willmott 1982 and percent bias pbias gupta and sorooshian 1999 the mathematical equations for these performance measures are as follows 6 nrmse i 1 n y i obs y i sim 2 n 1 σ obs 7 nse 1 i 1 n y i obs y i sim 2 i 1 n y i obs y mean obs 2 8 r 2 i 1 n y i obs y mean obs y i sim y mean sim i 1 n y i obs y mean obs 2 y i sim y mean sim 2 2 9a pbias μ μ sim μ obs μ obs 9b pbias σ σ sim σ obs σ obs where y i obs is the ith observed data y i sim is the ith simulated data y i m e a n is the mean of the observed data σ obs is the standard deviation of the observed data n is the number of observations and μ sim and σ sim denote the model simulated value of the mean and standard deviation whereas μ obs and σ obs are the corresponding statistics for the observed value if nse and r2 values are close to 0 then the performance of the model in downscaling is considered highly unsatisfactory if nse and r2 values are close to 1 then the model performance is almost perfect if the value of nrmse is low then it is an indication that the downscaled values are similar to the observed values if the value of nrmse is high then the downscaled results are not close to the observed ones if pbias is positive it indicates an over estimation of the values while a negative pbias value indicates an under estimation of the values sachindra et al 2018 showed that the percentage bias can be used as a reliable measure for evaluating the performance of the model results 3 5 extreme precipitation accuracy in this study the ability of the models in capturing the extreme precipitation events is also investigated for this purpose a rainfall event is considered as an extreme precipitation event when the daily precipitation exceeds 50 mm following the guidelines adopted by the world meteorological organization wmo for heavy rain event in the indian subcontinent the accuracy of downscaling in terms of capturing the extreme precipitation is estimated as 10 e x t r e m e p r e c i p i t a t i o n a c c u r a c y observed n o o f e v e n t s g r e a t e r t h a n 50 m m s i m u l a t e d n o o f e v e n t s g r e a t e r t h a n 50 m m observed n o o f e v e n t s g r e a t e r t h a n 50 m m 100 4 analysis and results 4 1 model application in any downscaling strategy the selection of the predictors and its spatial domain plays a major role based on the climatic characteristics of the krishna river basin and reports by some previous studies lakhanpal et al 2017 sehgal et al 2018 a total of 16 probable predictors is considered as listed in table 1 the list of probable predictors considered here is similar to that used by sehgal et al 2018 for the krishna river basin sachindra et al 2018 for australia and yang et al 2017 for a region in china in the present study a spatial domain of 4 4 grid points placed at a spatial resolution of 2 5 in the latitudinal and longitudinal directions respectively is considered this spatial extent is selected based on previous studies for the same region sehgal et al 2018 therefore in the present study spatial averaging of the ncep data is considered to reduce the number of predictor variables this is important since the selection of too many variables would result in an over training of the models due to redundant components sehgal et al 2018 fig 5 shows the cross correlation between the candidate predictors and precipitation for grid 1 considered in this study figures for the remaining four grids are shown in fig s1 in supplementary information it is observed that many of the candidate predictors e g mean sea level pressure meridional velocity component surface air temperature zonal velocity component share a lagged relationship with precipitation showing the presence of long and short term memory in the climate system as observed by mishra et al 2017 it is also observed that all the 16 selected predictors have a significant correlation with precipitation and therefore are considered as potential predictors further for developing separate models for wet and dry periods the entire precipitation data set is categorized into monsoon season and non monsoon season the period from june to october is considered as the monsoon season and the period from november to may is considered as the non monsoon season based on the precipitation occurrence amount for the study area for training and validating the downscaling models the observed precipitation and ncep data are split into two parts with a ratio of 70 30 for each grid and season in the case of the mlr models all the potential predictors are considered for the regression analysis it is observed that the number of potential predictors varies and so separate models for each grid points are developed in the case of sdsm all the probable predictors are considered as the model predictors and the model is developed using the sdsm 5 2 toolbox gonzález rojí et al 2019 for the gp models a population size of 500 is chosen and the mutation and crossover rates are kept at high value following sivapragasam et al 2008 and selle and muttil 2011 the mean squared error mse is used as the model fitness function the mathematical function set is selected so that a meaningful relationship can evolve between the predictand and predictors again separate gp models are developed for each grid and season regarding the ann based models ff nn and narx nn training is carried out using the levenberg marquardt lm algorithm due to its fast convergence accuracy robustness and reliability adamowski and karapataki 2010 adamowski and sun 2010 using a trial and error procedure the optimal number of neurons for each model is selected the optimal number of hidden neurons for each of the models is found using a trial and error procedure the number of hidden neurons is varied from 10 to 20 and the best value is chosen based on the rmse for the wt mlr wt gp wt ff nn and wt narx nn models decomposition of the climatic variable is done using the haar wavelet zhang 2019 each of the predictor variables is decomposed up to 10 levels which produce 11 components of the original data set at different scales to be used as input for wavelet models following the procedure outlined in maheswaran and khosa 2012 4 2 results of performance analysis after the models are trained and validated using the corresponding data to a satisfactory level the performance measures nrmse nse r2 and pbias are computed for each of the five grids for assessing the ability of the models to downscale precipitation figs 6 and s2 in supplementary information present the results of the models for calibration during monsoon and non monsoon seasons respectively fig 7 presents the validation results for the monsoon season similar results are also obtained for the non monsoon season not presented here the results for the calibration set indicate that the wavelet based model shows robust performance and performs better than the other models for all the five grids in both the seasons for the non monsoon season at grid 1 refer to fig s21 the performance measure r2 is the highest for the wavelet based feed forward neural network wt ff nn and wavelet narx neural network wt narx nn models for singular and hybrid machine learning models i e ff nn narx nn gp wt mlr and wt gp the r2 values are 0 62 0 64 0 52 0 51 and 0 59 respectively and the nse values are 0 46 0 45 0 46 0 45 and 0 49 respectively for the same grid the performances of mlr and sdsm are not superior when compared to the machine learning models for the monsoon season and for grid 1 the results in terms of nse r2 show that the wt ff nn and wt narx nn models have values of 0 62 0 79 and 0 55 0 69 respectively whereas the ff nn narx nn gp wt mlr and wt gp models have values of 0 46 0 62 0 45 0 63 0 46 0 52 0 45 0 51 and 0 49 0 59 respectively the performances of mlr and sdsm are comparatively poorer similar trend in the performance of the models is also observed for all the other four grids comparing the performance of the wavelet based models across the five grids the model performance in terms of r2 is relatively poorer for the wet locations grid 5 and grid 3 than for the dry locations grid 1 and grid 2 further for grid 1 during the monsoon season refer to fig 6 nrmse values of the machine learning models ff nn narx nn and gp are 84 31 mm 87 36 mm and 84 48 mm and for the wavelet hybrid models wt mlr wt gp wt ff nn and wt narx nn are 87 54 mm 81 45 mm 82 21 mm and 77 71 mm respectively the results obtained using all the models considered at all the five grids show that the wavelet based downscaled estimates from wt ff nn and wt narx nn are the closest to the observed values the performance measures for the validation period see fig 7 show similar behaviour to that for the calibration period interestingly the wt mlr has higher nrmse when compared to the single scale machine learning models showing the necessity of nonlinear models in capturing the downscaling relationships according to sachindra et al 2018 percentage bias is a reliable measure of the model performance as it enables a reasonable comparison of the ability of the downscaling models as seen from the results refer to figs 6 and 7 the percentage bias in the mean of the model simulations has a similar pattern in both wet and dry places for the calibration period the mlr models show the highest percentage bias mean and the wt narx nn models show the lowest percentage bias it is important to note that for both the dry and wet locations the wavelet based models wt ff nn and wt narx nn have the lowest percentage bias when compared to the other models between the wavelet based hybrid models the wt narx nn models have better performance for all the grids further the percentage bias is negative for most of the models indicating that the models are underestimating the rainfall values the biases in the standard deviation of the rainfall estimates indicate that the performances of the mlr and sdsm for both the seasons refer to figs 6 and s2 are not acceptable for any of the grids as they are high and in the order of 29 to 50 on the contrary the machine learning methods such as ff nn narx nn and gp perform better in capturing the standard deviation among the machine learning models the wavelet based hybrid models yield comparatively less percentage bias in the standard deviation for example for grid 5 fig 6 the percentage bias for the calibration set for the monsoon season are found to be 31 1 25 5 14 4 10 24 12 2 28 54 12 42 1 5 and 2 8 for the mlr sdsm gp ff nn narx nn wt mlr wt gp wt ff nn and wt narx nn models respectively similar results are obtained for the other grids wherein the values of pbias σ are in the range of 6 08 to 18 07 for wt ff nn and wt narx nn models comparing the results across all the five grids most of the models underestimate the standard deviation for relatively dry grids grid 1 and grid 2 and overestimate for the wet grids grid 5 and grid 3 similar findings have also been reported by some past studies sachindra et al 2018 anandhi et al 2008 this might be due to the fact that catchment scale dynamics may depend not just on large scale climatic factors but also on regional factors fig 8 shows the cumulative distribution function cdf for the different model outputs and the observed precipitation at grid 1 cdf analysis is also carried out for the remaining four grids and the results are shown in fig s3 of supplementary information as seen the performance of the mlr models is not satisfactory in capturing the peak extreme rainfall values for almost all the five grids the cdf results obtained from the sdsm and the singular machine learning models ff nn narx nn and gp show that these models have good performance the results also indicate that the hybrid models wt mlr and wt gp produce better results than their singular versions but still fail to capture the observed rainfall pattern at most of the grids especially the extreme values this is particularly the case for grid 2 on the other hand the wavelet based models wt ff nn and wt narx nn perform relatively better in capturing the local rainfall events for all the five grids the cdfs obtained from these models are close to that of the observed rainfall with extreme rainfall events also captured reasonably well between the two wavelet based models there is no significant difference in terms of model performance to further evaluate whether the distributions obtained from the models are statistically similar to the ones obtained from the observed data at 95 confidence level the kolmogorov smirnov nonparametric goodness of fit test is used fig 9 shows the results from the different models for the five selected grids in terms of the p values the results indicate that the wavelet based models have p values greater than 0 05 for all the grids however the mlr models have p values less than 0 05 indicating that they perform poorly in reproducing the distribution of the daily precipitation other models such as gp sdsm ff nn narx nn wt mlr and wt gp perform moderately at some grids and poorly at others all these results show that the wavelet based hybrid models wt ff nn and wt narx nn are robust in capturing the overall trend and pattern in the daily precipitation and perform better when compared to statistical and other traditional machine learning models to further understand the effectiveness of the downscaling models the relationship between the raw precipitation ncep precipitation data set and observed precipitation is examined for this purpose the raw precipitation data from the ncep data for the period from 1948 to 2017 are used fig 10 a shows for instance the scatter plot between the observed precipitation at grid 3 randomly selected and the raw ncep data obtained at the closest location to grid 3 fig 10 b j shows the scatterplots between the observed daily precipitation and the model results for the validation set at grid 3 the scatter plots indicate that the accuracy of the downscaled data is greater than that of the raw data without downscaling further the scatterplots show that the models have the ability to capture the observed rainfall pattern and amount although with varying degrees the mlr and sdsm underestimate the rainfall particularly for values greater than 40 mm the ff nn gp narx nn wt mlr and wt gp models perform moderately in capturing the rainfall events the wavelet based models i e wt ff nn and wt narx nn perform comparatively better in capturing both the extreme events and the moderate low rainfall events with the values closer to the 45 line when compared to that from the other models between the two wavelet based hybrid models considered in this study the wt narx nn outperforms the wt ff nn model in terms of its ability to capture the observed rainfall similar results are also obtained for the other four grids not presented table 2 shows the actual number of extreme events observed and the number of events simulated by the different models along with the percentage accuracy for the validation period the results show that the wavelet based hybrid models capture extreme events with greater accuracy than the other models further it is observed comparing the results among the single scale models that the nn models and gp models outperform the mlr and sdsm for instance for grid 1 the wavelet based models show an accuracy of 78 and 90 whereas the ff nn model and the gp model have an accuracy of 72 and 47 respectively 5 discussion in this study wavelet based hybrid models were proposed for downscaling daily precipitation and their performances were compared with those of some key traditional and other modern methods including mlr sdsm gp and ann models among the nine downscaling methods applied in this study machine learning methods were found to generally outperform the basic mlr and sdsm among the single scale machine learning models the narx nn model outperformed the ff nn model and the gp model the better performance of the narx nn model may be due to its ability in capturing the long and short term memory relationship between the climatic variables and precipitation as also discussed in mishra et al 2017 although the sdsm is the most popular model and has been widely used in numerous studies khan et al 2006 wetterhall et al 2006 tryhorn and degaetano 2011 it was unable to provide robust results in the present study when compared to the gp and ann models the sdsm underestimated the quantile precipitation at almost all the five selected grids this finding is in accordance with the observation reported by chen et al 2011 who showed that an svm based model outperformed the sdsm in the downscaling of daily precipitation interestingly our results showed that the performance of the gp models was reasonable in comparison with the ann models this observation was also made by sachindra et al 2018 while investigating the effectiveness of the downscaling models for monthly precipitation overall our results manifest that the wavelet based neural network models wt ff nn and wt narx nn are robust compared to the traditional and other modern machine learning methods considered in this study this is in accordance with the broader consensus about the performance of the wavelet based hybrid models wherein the application of wavelets enhances the ability of the models to capture the multiscale relationship among the variables for example in a recent study sun et al 2019 revealed that the models with wavelet components performed better than the regular models i e those without wavelet coupling for streamflow forecasting rashid et al 2016 showed that the wavelet based gamlss models resulted in better downscaling estimates for monthly precipitation than the single scale gamlss models in another study maheswaran and khosa 2015 showed that the wavelet based model performed superior to the simple nonlinear models for rainfall forecasting in the present study the wavelet based hybrid models performed better in terms of capturing both the overall precipitation characteristics and the extreme precipitation events further the bias in terms of the standard deviation and mean of the downscaled precipitation was comparatively less for the hybrid models one possible reason for the improvement in the performance of the wavelet based hybrid models could be that the decomposition of the climatic variables at multiple scales unravelled their hidden relationship with precipitation this is evident from the results shown in fig 11 for grid 2 where the values show the correlation between the predictor variables and precipitation with and without wavelet decomposition the correlation between precipitation and mean sea level pressure mslp is 0 08 without applying any decomposition whereas the correlation between decompositions of mslp d4 to d9 and precipitation varies from 0 17 to 0 39 a similar pattern can be observed for several other variables as well e g uas p500 p850 r500 r850 it can be seen that there is a considerable increase in the correlation between precipitation and the decomposition of the climatic variables this shows that wavelet decomposition can help in unravelling the hidden relationship between the predictors and precipitation the decomposition of the atmospheric variables shows higher correlation with precipitation when compared to the correlation obtained between precipitation and atmospheric variables at the original scale similar behaviour was also observed for the other four grids not shown here due to space constraints thus it may be suggested that wavelets aid in unravelling the relationship between local precipitation and large scale climatic variables and improve the overall performance of the downscaling models that are based on them in the present study ncep data was used for model development and validation however the proposed methodology can be used for gcm data by using the gcm data for model development instead of ncep data several studies including those by sehgal et al 2018 and lakhanpal et al 2017 have shown that the ncep data and gcm data more specifically canesm are in good agreement for the krishna river basin therefore one may conclude from this study that the approach can be extended for the gcm data 6 conclusions in this study nine downscaling models including mlr sdsm gp ann and wavelet based hybrid models were developed and applied for downscaling daily precipitation at five selected grid points in different locations in the krishna river basin in india using the ncep data and the imd data over the period 1948 2017 calibration and validation of the models were carried out based on the results obtained for the five selected grids in terms of several performance measures nrmse nse r2 and pbias and their ability to capture the extreme events it was observed that the wavelet based hybrid models performed better when compared with the other traditional mlr and sdsm and machine learning models gp and ann the wavelet based models wt ff nn and wt narx nn showed better ability in capturing the extreme rainfall events and the error associated with the downscaled precipitation was low the outcomes of this study indicate the utility of wavelets in improving the performance of the hybrid models for downscaling of precipitation and other hydrologic analyses nevertheless the present outcomes regarding the performance of the wavelet based hybrid models need to be further supported and confirmed through their applications to other regions across india and around the globe having different climatic conditions and meteorological factors credit authorship contribution statement yeditha pavan kumar conceptualization data curation formal analysis investigation methodology writing original draft rathinasamy maheswaran conceptualization data curation formal analysis funding acquisition investigation methodology resources software supervision validation visualization writing original draft writing review editing ankit agarwal investigation methodology resources software validation visualization writing original draft writing review editing bellie sivakumar validation visualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements rm gratefully acknowledges the funding received through the inspire faculty award ifa 12 eng 28 from the department of science and technology india aa acknowledges the funding support provided by the indian institute of technology roorkee through faculty initiation grant number iitr sric 1808 f i g and coprepare project funded by ugc and daad under the igp 2020 2024 bs acknowledges the support from the iit bombay seed grant rd 0519 irccsh0 027 the authors would like to thank the three anonymous reviewers for their constructive comments and useful suggestions on an earlier version of the manuscript which led to significant improvements to the presentation of the work appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126373 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4463,cloud model theory provides a reliable method to effectively solve the problem of uncertainty associated with lake water quality assessments to accurately match water quality parameters obtained from water samples and water quality standards water quality parameters from water samples and water quality class levels were used as inputs to a reverse cloud generator algorithm to derive corresponding sample and level clouds a multidimensional shape position similarity cloud model mspscm was then developed to accurately evaluate lake water quality by considering shape and position similarities between the sample and level clouds using monthly water quality monitoring data from 2017 to 2019 spatiotemporal variability of water quality parameters of nansi lake in shandong province was analyzed and the mspscm was used to further study the spatiotemporal variability of water pollution in nansi lake results showed that total nitrogen and total phosphorus were the main sources of pollution in nansi lake except for the severe pollution of the upper lake and its inflow waters class v water quality standard in november 2017 and september 2019 nansi lake waters meet class iii water quality standard and are suitable for drinking after being treated by a sewage treatment plant concentration of residential areas and industries around the upper lake is relatively high large quantities of pollutants are discharged into the upper lake resulting in considerably severe pollution of the upper lake class iv difference between water quality of nansi lake and that of its inflow water indicates that the purification ability of nansi lake should not be underestimated in addition compared with the existing cloud model used to evaluate lake water quality the mspscm can more accurately reflect lake water quality and provides a more flexible and effective method for lake water quality evaluation keywords cloud model lake water quality cloud generator multidimensional shape position similarity cloud model spatiotemporal variability of water quality 1 introduction lakes play a very important role in freshwater storage in surface ecosystems wang et al 2019 wu et al 2016 yao et al 2019a rivers supply water to lakes and are the main transport channels of pollutants into lakes han et al 2020 wang et al 2019 yang et al 2016 lake pollution is becoming increasingly severe because of the long timescale of the water cycle difficulty to protect water resources from pollution high vulnerability of ecosystems and strong dependence of human economic development on lake water resources egessa et al 2020 nong et al 2020 yang et al 2016 yao et al 2020 lake water quality assessment may be understood as a multi criteria decision making process with quantitative water quality parameters as input and qualitative assessment as output wang et al 2018 wu et al 2017 yao et al 2019a the process contains two types of uncertainties 1 accuracy of water quality data and monitoring methods are constantly changing under the influence of hydrodynamic and biochemical factors which introduces random uncertainty into water quality evaluation norris and thoms 1999 2 water quality is affected by many water quality parameters and the nonlinear and complex relationships among parameters introduce fuzzy uncertainty into the determination of the degree of water pollution yan et al 2017 development of efficient water quality evaluation methods to accurately identify spatiotemporal characteristics is key to comprehensive improvement of lake water and its environment since the popularization of the water quality assessment index archibald 1972 many researchers have developed assessment methods taking into account uncertainties involved in lake water quality assessment these methods can be roughly divided into three categories 1 numerical methods based on multivariate statistical analysis such as nemerow pollution index npi chen et al 2017 principal component analysis pca platikanov et al 2019 and analytic hierarchy process ahp singh et al 2019 2 methods based on fuzzy set theory and grey system theory mainly including fuzzy mathematics evaluation method fmem zhang et al 2018 and grey system evaluation method gsem zhang et al 2018 3 methods based on artificial intelligence such as back propagation artificial neural network bp ann lu et al 2016 support vector machine svm ji et al 2017 and long short term memory network lstm wang et al 2019 while useful for water quality assessment these methods lack comprehensive consideration of the different uncertainties involved in water quality assessment and their applicability is limited to specific conditions the cloud model proposed by li et al 2009 can convert qualitative descriptions into quantitative values effectively mitigate impacts of uncertainties on accuracy of evaluation results and overcome the limits of the traditional water quality assessment methods described above wang et al 2016a using a forward and a reverse cloud generator the model converts qualitative concepts to quantities generating the quantitative cloud characteristics of expectation ex entropy en and hyper entropy he wang et al 2014 in the model clouds are formed from cloud droplets which are distributed in the domain space within the maximum and minimum boundaries fig a1 the forward cloud generator derives the degree of water pollution from qualitative information of water quality status while the reverse cloud generator calculates the eigenvalues of ex en and he from values of water quality parameters obtained from samples yao et al 2019a expectation is the expected spatial distribution of cloud droplets in the region and is a quantitative characterization of water quality entropy represents the degree of dispersion of cloud droplets and the range of cloud droplets in the domain space and is an effective measure of the uncertainty associated with the qualitative concept of water quality hyper entropy is a measure of entropy uncertainty which reflects the degree of condensation between cloud droplets ren et al 2017 the normal cloud model is the most basic version of the cloud model with normal gaussian membership function it has been widely used in lake water quality assessments wang et al 2016a 2016b yang and wang 2020 yao et al 2019a however uncertainties from monitoring methods or environmental parameters are not taken into account when uncertainties are only reflected by cloud characteristics wang et al 2016a 2016b the multidimensional similarity cloud model proposed by yao et al 2019a effectively solves this problem it uses values of the cloud characteristics of each evaluation index to form a vector and calculates the cosine of the angle between the vectors to determine water quality class level however when ex is far greater than en and he en and he are ignored which may lead to large deviations in evaluation results in fact similarity measurements of connotation and extension of the concept of qualitative description are included in cloud model similarity measurements yang and wang 2020 in lake water quality evaluations based on normal cloud models connotation refers to interval range of water quality classes that is cloud position extension refers to uncertainty associated with water quality class that is cloud shape similarity or sameness between clouds is determined by similarity between cloud shapes and positions however the similarity cloud model proposed by yao et al 2019a lacks detailed consideration of similarity measurements of cloud shapes thus in this study we used the random weighting method yao et al 2019b to randomize water quality monitoring data to reduce the impact of uncertainties on accuracy of water quality evaluations from this we developed a multidimensional shape position similarity cloud model mspscm that considers similarity measurements of cloud positions and shapes to capture water quality quantitatively and accurately the objectives of this study are 1 to develop the mspscm to comprehensively consider similarity measurements of cloud positions and shapes to capture lake water quality status more accurately 2 to apply the model to explore the spatiotemporal variability of water quality of a lake and its inflow and accurately identify the main factors underlying lake water quality deterioration and main pollution areas generally this study aims to propose a model that can accurately quantify lake water quality and thus provide more reliable technical support for effective monitoring and control of lake water quality 2 materials and methods 2 1 study area nansi lake is in the northern part of huaihe basin 116 34 117 21 e 34 37 35 20 n and extends along the northwest southeast axis see fig 1 it is 126 km long in the north south direction and 5 25 km wide in the east west direction lake waters cover an area of 1266 km2 average water depth is about 1 46 m annual average air temperature is 14 2 c average annual precipitation is about 700 mm precipitation between june and september accounts for about 70 of total annual precipitation yao et al 2019a since the completion of the erji dam in 1960 nansi lake has been divided into upper and lower lakes with areas of 602 and 664 km2 respectively catchment area of the upper lake is 88 4 of the total catchment area of nansi lake while that of the lower lake 11 6 of the total catchment area indicating that the upper lake is a larger source of pollution than the lower lake li et al 2020 nansi lake is an important water transport channel and storage lake on the east route of the south to north water transfer project er snwtp acceleration of urbanization decline of water storage capacity and environmental degradation have led to increased pollution of nansi lake with inflow being the main pollution source ren et al 2020 consequently a comprehensive study on water quality variation of the lake and its inflow can provide an improved scientific basis for the effective control of water pollution in nansi lake 2 2 data collection monthly water quality monitoring data of nansi lake between 2017 and 2019 were used they were collected from 5 monitoring stations on nansi lake and 13 stations at the entrance to the lake see fig 1 following the structural pollution characteristics of nansi lake watershed and the water quality indicators used along the er snwtp ma et al 2020 we examined the parameters of dissolved oxygen do chemical oxygen demand codcr ammonia nitrogen nh3 n total nitrogen tn and total phosphorus tp nansi lake is a shallow lake on the plain with an average water depth of 2 m therefore sampling was conducted at 0 5 m below the water surface li et al 2020 coordinates of sampling stations were recorded using a portable global positioning system the sampling boat was downstream of the sampling station to avoid water samples being polluted by bottom sediments stirred up by the boat thus ensuring sample quality cao et al 2017 sampler and sample container were first washed with sampling water the container was labeled using a self adhesive label sampling was repeated three times at each monitoring station to reduce deviations caused by the sampling process nong et al 2020 portable refrigerators were used to store water samples under cold and dark conditions samples were transferred immediately to a laboratory near the monitoring station li et al 2020 following the standard methods for the examination of water and wastewater han et al 2020 do was determined using a portable dissolved oxygen analyzer jpb 607a codcr was determined using the potassium dichromate method nh3 n was determined by salicylic acid spectrophotometry tn and tp were determined by peroxide potassium sulfate ultraviolet spectrophotometry and ammonium molybdate spectrophotometric method 2 3 development of the mspscm for assessing water quality the process for developing the mspscm is shown in fig 2 firstly the random entropy weight method yao et al 2019b was used to preprocess collected data to minimize impact of uncertainties in water quality assessment on accuracy of evaluation results then the processed data and boundary values of the parameters for each class were taken as the inputs of their respective reverse cloud generators and the quantitative cloud characteristics of water samples and water quality classes were obtained respectively sample clouds and class level clouds were generated by the forward cloud generator ren et al 2017 finally lakes water quality status are determined by comprehensively considering position and shape similarities between sample and level clouds of water quality parameters thereby avoiding large deviations between evaluation results and field quantities caused by the large differences between ex en and he in the existing multidimensional similarity cloud model yao et al 2019a 2 3 1 determination of sample cloud characteristics of water quality parameters the random weighting method was used to maximize the integrity of the information from water monitoring data and effectively reduce impacts of random or uncontrolled uncertainties associated with monitoring methods and environmental characteristics on the accuracy of monitoring data yao et al 2019a let the sample matrix of water quality parameters be x ij n m and the random matrix be r ij k n r a n d k n then the randomized water quality parameters sample matrix r ij k m can be obtained by dimensionless processing of x ij n m and r ij k n as follows 1 r ij k m r ij k n x ij n m i 1 n x ij n m where m is the number of water quality parameters n is the sample number of water quality parameters k is the number of random weighting times the processed water quality monitoring data were then used as inputs for the reverse cloud generator li et al 2020 to derive cloud characteristics of the water samples as follows 2 e x j 1 k i 1 k r ij e n j π 2 1 k i 1 k r ij e x j i 1 2 k j 1 2 m h e j s j 2 e n 2 j where s j denotes the variance of j th water quality parameter and s j 1 k 1 i 1 k r ij e x j 2 2 3 2 determination of level cloud characteristics of water quality parameters the environmental quality standards for surface water in china available at http www cnemc cn jcgf shj 200801 t20080128 647287 shtml divides water quality into six class levels class i v and beyond class v and defines the threshold range of water quality parameters for each class see table a1 class i indicates good water quality and the surface water can be used for drinking after simple treatment such as filtration and disinfection class ii indicates slightly polluted water which can be used for drinking after conventional treatment such as precipitation filtration and disinfection class iii indicates moderately polluted water which can only be used for drinking after being treated by potable water treatment plants class iv indicates considerably polluted water which is suitable for industrial and entertainment purposes and unsuitable for direct contact with the human body class v indicates severely polluted water which is suitable for agricultural irrigation water beyond class v is extremely severely polluted and has no use su et al 2017 the maximum and minimum boundary values of each parameter were derived using the method proposed by wang et al 2016a then the boundary values of each water quality parameter were used as inputs for the corresponding reverse cloud generator wang et al 2014 to obtain the quantitative cloud characteristics see table 1 for each water quality parameter under different water quality class levels as follows 3 ex b min b max 2 e n b max b min 6 h e k e n where b max and b min are the maximum and minimum values of each class level boundary respectively k is an adjustment coefficient used to adjust the condensation degree of cloud droplets in level clouds and is generally set to 0 1 liu et al 2014 2 3 3 determination of water quality class level lake water quality class level was determined on the basis of the similarity between sample and class level clouds generated by quantitative cloud characteristics ex en and he of water quality samples and classes yao et al 2019a similarity between sample and level clouds is determined by shape and position similarities between the clouds shape similarity depends on en and he which is reflected in the geometric similarity between sample and level clouds as shown in fig 3 a for normal cloud c 1 let the area enclosed by the expected curve and abscissa be s 1 ϕ z 1 be the probability density function of the standard normal distribution z 1 x e x 1 e n 1 then s 1 2 π e n 1 1 2 π e n 1 exp x e x 1 2 2 e n 1 2 d x 2 π e n 1 ϕ z 1 d x 2 π e n 1 similarly the area m s 1 enclosed by the maximum boundary curve of c 1 and the abscissa is 2 π e n 1 3 h e 1 thus the area s 2 enclosed by the expected curve and abscissa of normal cloud c 2 is 2 π e n 2 and the area m s 2 enclosed by the expected curve and abscissa of c 2 is 2 π e n 2 3 h e 2 μ 1 s 1 m s 1 and μ 2 s 2 m s 2 are the uncertainty of c 1 and c 2 respectively and represent shape similarity consequently let the normal cloud of the i th ake water sample be c i and the normal cloud of the j th water quality class be c j then shape similarity between the sample and level clouds is calculated as follows 4 si m s c i c j ξ 1 5 min e n i ξ 3 h e i ξ e n i ξ e n j ξ 3 h e j ξ e n j ξ ξ 1 5 max e n i ξ 3 h e i ξ e n i ξ e n j ξ 3 h e j ξ e n j ξ where e n i ξ h e i ξ denote the entropy and hyper entropy of the i th sample of the ξ th water quality parameter and e n j ξ h e j ξ denote the entropy and hyper entropy of the j th water quality class of the ξ th water quality parameter position similarity depends on ex which is the similarity difference caused by the movement of sample and level clouds as shown in fig 3 b following the 3en rule of normal distribution ren et al 2017 the interval ex 3en ex 3en can represent the range of normal clouds the overlapping interval length of the abscissa of normal clouds c 1 and c 2 is l 1 min e x 1 3 e n 1 e x 2 3 e n 2 max e x 1 3 e n 1 e x 2 3 e n 2 and the total length of their abscissa interval is l 2 max e x 1 3 e n 1 e x 2 3 e n 2 min e x 1 3 e n 1 e x 2 3 e n 2 following set theory the ratio of l 1 to l 2 represents position similarity between normal clouds c 1 and c 2 thus position similarity between sample and level clouds is calculated as follows 5 si m p c i c j ξ 1 5 min e x i ξ 3 e n i ξ e x j ξ 3 e n j ξ max e x i ξ 3 e n i ξ e x j ξ 3 e n j ξ ξ 1 5 max e x i ξ 3 e n i ξ e x j ξ 3 e n j ξ min e x i ξ 3 e n i ξ e x j ξ 3 e n j ξ where e x i ξ e n i ξ denote expectation and entropy of the i th sample of the ξ th water quality parameter and e x j ξ e n j ξ denote expectation and entropy of the j th water quality class of ξ th water quality parameter for a sample cloud and level cloud with the same shape similarity between the clouds changes with distance between the clouds similarly if positions of the sample and level clouds coincide similarity changes with change in shape thus establishing a comprehensive similarity variable between the two clouds si m c c i c j on the basis of shape and position similarities between the sample and level clouds is the key to accurate assessment of water quality si m c c i c j is derived as follows 6 si m c c i c j si m s c i c j s i m p c i c j note ec and mbc denote expectation curve and maximum boundary curve respectively 3 results and discussion 3 1 analysis of water quality parameters to accurately obtain the main factors affecting the water quality of the study area annual average concentrations and variability of water quality parameters were derived by using the collected data from 2017 to 2019 see table 2 in addition differences between annual average concentrations of water quality parameters at 18 monitoring sections were tested using one way analysis of variance one way anova varol 2019 see fig 4 statistical variables of do concentration in table 2 show that do in 2017 2019 mainly fluctuated between class i and ii and exhibited an upward trend indicating that the study area has a good aquatic oxygen environment there were statistically relatively differences between average do concentrations at different monitoring sections one way anova p 0 05 see fig 4 concentrations at monitoring sections in the upper lake were lower than those at the lower lake which may be because of lake self purification and purification effects of the erji dam yao et al 2019a statistical variables of codcr concentration in table 2 show that codcr in 2017 2019 was mainly in class iii which indicates that the water in the study area was moderately polluted by organic matter and could only be used for drinking after being processed by a sewage treatment plant there were statistically relatively differences between average codcr concentrations at different monitoring sections one way anova p 0 05 see fig 4 especially for the upper lake average codcr concentrations at monitoring sections in the upper lake 15 mg l codcr 20 mg l were in class iii except that at s14 codcr 14 64 mg l were in class ii however differences between average codcr concentrations at different monitoring sections in the lower lake were statistically insignificant this is mainly because of the small number rivers entering the lower lake weak circulation in the lake and accumulation of organic pollutants in the upper lake li et al 2020 statistical variables of nh3 n concentration in table 2 show that nh3 n in 2017 2019 was mainly in class ii which indicates that the water in the study area was slightly polluted by nh3 n from organic matter and could only be used for drinking after conventional treatment there were statistically significant differences between average nh3 n concentrations at different monitoring sections one way anova p 0 01 see fig 4 concentrations of nh3 n in the upper lake were basically higher than those in the lower lake and the highest nh3 n concentrations only reached the class iii nh3 n s18 0 70 mg l 1 00 mg l this indicates that water quality in nansi lake was less affected by nh3 n organic pollutants statistical variables of tn concentration in table 2 indicate that tn in 2017 2019 mainly fluctuated between class iii and v which indicates that nansi lake was severely polluted by nitrogen containing organic matter and was faced with the threat of eutrophication aggravation there were statistically significant differences between average tn concentrations at different monitoring sections one way anova p 0 01 see fig 4 average tn concentrations were in class iii at s1 and s2 and class iv at s4 and s5 reflecting that nitrogenous organic pollutants in the lower lake were more than that in the upper lake moreover average tn concentrations were in class iii at s7 s10 s11 and s12 class iv at s8 and s13 class v at s9 s16 and s17 and beyond class v at s6 s14 s15 and s18 indicating that nansi lake inflow waters especially the lower lake inflow waters were severely polluted by organic matter containing nitrogen this is mainly because of large amounts nitrogen containing pollutants such as chemical fertilizers and livestock manure discharged into the inflow rivers of nansi lake which were affected by water erosion and sediment microbial degradation in the migration process resulting in a decrease in the concentration of these pollutants in nansi lake qu et al 2020 statistical variables of tp concentration in table 2 indicate that tp of nansi lake and its inflow in 2017 2019 mainly fluctuated between class iii and iv class ii and iii respectively the main reason is that the low circulation in nansi lake and high flow rate in its inflow water resulting in most of the phosphorus containing pollutants carried by the inflow being concentrated in nansi lake li et al 2020 there were statistically significant differences between average tp concentrations at different monitoring stations one way anova p 0 01 see fig 4 average tp concentrations were in class iv at s1 and s2 and class iii at s4 and s5 which reflects that the phosphorus containing pollutants of the upper lake is more than that of the lower lake average tp concentrations were in class ii at 6 out of 10 60 upper lake inflow sections and at 2 out of 3 67 s16 and s17 lower lake inflow sections class iii at 4 out of 10 40 upper lake inflow sections and class iv at s18 tp 0 235 mg l 0 20 mg l indicating that it is necessary to focus on strengthening the treatment of phosphorus containing pollutants into the inflow rivers through s18 differences between tp in nansi lake and tp in inflow rivers indicate that spatial variability of tp in nansi lake is mainly affected by p2o5 content in the bottom sediments and that impact of tp carried by inflow rivers was relatively small on tp concentration in nansi lake yao et al 2019a 3 2 spatiotemporal variability of water quality from the mspscm since the actual circumstance of lake water quality is determined by the comprehensive effect of various water quality parameters the monthly time series of water quality parameters of 18 water quality monitoring sections from january 2017 to december 2019 were used as the input to the mspscm to evaluate spatiotemporal variability of water quality in the study area 3 2 1 spatiotemporal variability of water quality on a monthly scale to obtain the periods and regions with serious water pollution monthly water quality monitoring data from four monitoring areas the upper lake monitoring sections s1 s3 its inflow sections s6 s15 the lower lake s4 and s5 and its inflow s16 s18 were used as inputs to the mspscm to calculate the comprehensive similarity degree between water quality monitoring regions and water quality classes see fig 5 to determine the water quality status of each monitoring area for different periods fig 5 a b shows that from january 2017 to december 2019 water quality of the upper lake had the highest comprehensive similarity degree with class iii and that of its inflow had the highest comprehensive similarity degree with class iv this indicates that the upper lake was moderately polluted its water was only suitable for drinking after being treated by a sewage treatment plant ma et al 2020 however upper lake inflow was more severely polluted mainly because of the residential areas and industries concentrated around nansi lake and discharge of domestic and industrial sewage through the inflow water ren et al 2020 upper lake inflow was suitable for industrial and entertainment purposes and unsuitable for direct contact with the human body su et al 2017 the difference between water quality of the upper lake and that of its inflow shows that the upper lake plays an important role in water purification in november 2017 and september 2019 upper lake and its inflow had the highest comprehensive similarity degree with class v indicating that these monitoring regions were severely polluted during these two months this may be because during the dry season november 2017 when there is little precipitation upper lake inflow is mainly derived from domestic and industrial sewage yao et al 2019a during high water september 2019 when rivers transport sediments into the lake at high rates large amounts of nitrogen containing organic pollutants are discharged into the lake li et al 2020 excess tn concentration resulted in severe water pollution in the upper lake and its inflow refer to the section 3 1 during these two months fig 5 c d shows that during the study period water quality of the lower lake and its inflow had the highest comprehensive similarity degree with class iii indicating that these monitoring regions were also moderately polluted in november 2017 and september 2019 water quality of the lower lake and its inflow had the highest comprehensive similarity degree with class iv indicating that these monitoring regions were considerably polluted during these two months the reasons underlying this peak in pollution are the same as those that affected the upper lake during the same period during these two months water quality in the upper lake was higher than that in the lower lake which may be because upper lake water is purified as it flows into the lower lake and the erji dam regulates upper lake water quality yao et al 2019a domestic sewage and industrial wastewater were gathered in the inflow of upper lake resulting in water quality of lower lake inflow being higher than that of the upper lake li et al 2020 3 2 2 spatiotemporal variability of annual water quality to further identify the major sources of serious water pollution annual water quality monitoring data from monitoring sections s1 s18 were used as inputs to the mspscm to calculate comprehensive similarity degree between water quality monitoring sections and water quality classes as shown in fig 6 to determine the water quality status of each monitoring section fig 6 shows that in 2017 water quality at s1 s2 and s3 had the highest comprehensive similarity degree with class v class iv and class iii respectively this indicates that water quality of the upper lake was gradually improved from upstream to downstream this is mainly because many rivers flow into the upper lake discharging large amounts of tn into the upper lake near s1 tn concentrations at s6 s14 and s15 monitoring sections were in class v see section 3 1 however the upper lake has a rapid flow rate yao et al 2019a and strong self purification ability ma et al 2020 resulting in improvement of water quality as the water flows downstream water quality at s6 s8 s9 s12 and s15 had the highest comprehensive similarity degree with class iv indicating that rivers flowing through these sections were the major sources of serious water pollution of the upper lake in 2017 water quality at s7 s10 s11 s13 and s14 had the highest comprehensive similarity degree with class v indicating that rivers flowing through these sections were the major sources of water quality deterioration of upper lake inflow in november 2017 moreover water quality at s17 and s18 had the highest comprehensive similarity degree with class iv reflecting that rivers flowing through these sections were the major sources of water quality deterioration of lower lake inflow in november 2017 in 2018 water quality at s1 s2 and s3 had the highest comprehensive similarity degree with class iv class iii and class iii respectively these results are consistent with the spatial variability of upper lake water quality in 2017 for upper lake inflow water quality at s11 had the highest comprehensive similarity degree with class iii that of other sections s6 s10 and s12 s15 had the highest comprehensive similarity degree with class iv indicating that rivers flowing through s6 s10 and s12 s15 were the major sources affecting the water quality of the upper lake thus strengthening pollution control of the rivers flowing through sections s6 s10 and s12 s15 is the key to comprehensively improve upper lake water quality for the lower lake and its inflow water quality at s4 s5 s16 and s17 had the highest comprehensive similarity degree with class iii that at s18 had the highest comprehensive similarity degree with class iv indicating that the river flowing through s18 was the major source that threaten the water quality of lower lake in 2019 water quality at s1 and s2 had the highest comprehensive similarity degree with class iv that at s3 had the highest comprehensive similarity degree with class iii which was similar to upper lake water quality spatial variability during 2017 2018 water quality at s9 s13 and s14 had the highest comprehensive similarity degree with class iv indicating that rivers flowing through these sections were the major sources of serious water pollution in upper lake inflow in 2019 see fig 5 b water quality at s6 s8 s10 s12 and s15 had the highest comprehensive similarity degree with class v rivers flowing through these sections were the major sources of water quality deterioration in the upper lake inflow in september 2019 water quality at s4 s5 s16 and s17 had the highest comprehensive similarity degree with class iii that at s18 had the highest comprehensive similarity degree with class iv indicating that river flowing through s18 was the major source of water quality deterioration in the lower lake inflow in september 2019 3 3 advantages and limitations of the mspscm for evaluating water quality our results show that the mspscm developed in this study can accurately capture the evolution of lake water quality and locate severely polluted areas compared with the existing multidimensional normal cloud model mncm wang et al 2016b and multidimensional similarity cloud model mscm yao et al 2019a the mspscm developed in this study effectively mitigates uncertainties associated with lake water quality assessments and comprehensively considers shape and position similarities between sample cloud and level cloud to determine lake water quality class level which allows differences between water quality to be identified more accurately water quality data from 12 typical lakes in china wang et al 2016b table 3 were used to verify that mspscm results can more accurately reflect field conditions than mncm and mscm results of the mspscm results 9 out of 12 75 are identical to mncm results and 10 out of 12 83 are identical to mscm results mspscm results are highly consistent with lake water quality status in the field which fully reflects the reliability of the mspscm results from the different models differ for gantang and west lakes with large areas of algal bloom instead of total algal bloom in these lakes wang et al 2016a indicating that most of the these lakes has relatively serious water pollution water quality in gantang and west lakes is more in line with class v verifying that mspscm results can more accurately reflect lake water quality than mncm and mscm results determination of lake water quality class usually depends on the threshold of each class for each water quality parameter yao et al 2019a different lakes are affected by different natural factors such as precipitation insolation domestic pollution industrial wastewater discharge and other human factors as a result there are different water quality class thresholds for different lake water quality parameters although the mspscm makes up for the deficiencies of mncm and mscm like mncm and mscm the mspscm is also limited because it determines the degree of pollution of different lakes using the national standard water quality parameter class thresholds which may lead to large deviations between evaluation results and field quantities consequently in future research it is urgent to develop a system which defines class thresholds that can accurately reflect field conditions 4 conclusion to improve the match between water quality parameters obtained from samples and water quality standards in multidimensional cloud model studies of lake water quality assessments a reverse cloud generator was used to process sample data and water quality class level interval to generate corresponding sample clouds and level clouds and a mspscm was established from the shape and position similarities between sample and level clouds spatiotemporal variability of five selected water quality parameters and water quality status of nansi lake from january 2017 to december 2019 were explored using the proposed mspscm method results from the mspscm were compared with those from existing mncm and mscm methods the main conclusions are as follows 1 during the water quality monitoring period from 2017 to 2019 do codcr and nh3 n concentrations in the water quality monitoring sections of nansi lake and its inflow did not exceed class iii which met the minimum standards of water quality in nansi lake however tn concentrations at s1 s3 and tp concentrations at s4 s5 were in class iv indicating that nansi lake has been threatened by excessive nitrogen and phosphorus organic pollutants to aggravate eutrophication moreover tn concentrations at s6 s14 s15 and s18 beyond class v and tp concentration at s18 was in class iv indicating that the excessive nitrogen and phosphorus organic pollutants in the inflow rivers flowing through these sections were the main factors threatening the water quality of nansi lake the local environmental protection departments should strengthen the control of the sewage discharge of these inflow rivers to ensure the water quality of nansi lake 2 in 2017 2019 the monthly variation of water quality in monitoring regions shows that the water quality of upper lake lower lake and its inflow were concentrated in class iii upper lake inflow was concentrated in class iv especially in november 2017 and september 2019 the water quality of upper lake and its inflow reached class v indicating that the upper lake inflow was the major source of water quality deterioration in nansi lake annual variation of water quality in monitoring sections shows that 6 out of 10 60 monitoring sections in the upper lake inflow were seriously polluted especially water quality at s6 s8 s12 and s15 changed from class iv to class v it is urgent to investigate the pollution sources at these sections to achieve targeted treatment of water quality in nansi lake 3 comparisons between cmscm mncm and mscm results show that mspscm results can more accurately reflect lake water quality than mncm and mscm results the mspscm method provides scientific and reliable technical support for obtaining accurate spatiotemporal characteristics of water pollution in lakes from different time scales declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the national natural science foundation of china grant no 51879006 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126379 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4463,cloud model theory provides a reliable method to effectively solve the problem of uncertainty associated with lake water quality assessments to accurately match water quality parameters obtained from water samples and water quality standards water quality parameters from water samples and water quality class levels were used as inputs to a reverse cloud generator algorithm to derive corresponding sample and level clouds a multidimensional shape position similarity cloud model mspscm was then developed to accurately evaluate lake water quality by considering shape and position similarities between the sample and level clouds using monthly water quality monitoring data from 2017 to 2019 spatiotemporal variability of water quality parameters of nansi lake in shandong province was analyzed and the mspscm was used to further study the spatiotemporal variability of water pollution in nansi lake results showed that total nitrogen and total phosphorus were the main sources of pollution in nansi lake except for the severe pollution of the upper lake and its inflow waters class v water quality standard in november 2017 and september 2019 nansi lake waters meet class iii water quality standard and are suitable for drinking after being treated by a sewage treatment plant concentration of residential areas and industries around the upper lake is relatively high large quantities of pollutants are discharged into the upper lake resulting in considerably severe pollution of the upper lake class iv difference between water quality of nansi lake and that of its inflow water indicates that the purification ability of nansi lake should not be underestimated in addition compared with the existing cloud model used to evaluate lake water quality the mspscm can more accurately reflect lake water quality and provides a more flexible and effective method for lake water quality evaluation keywords cloud model lake water quality cloud generator multidimensional shape position similarity cloud model spatiotemporal variability of water quality 1 introduction lakes play a very important role in freshwater storage in surface ecosystems wang et al 2019 wu et al 2016 yao et al 2019a rivers supply water to lakes and are the main transport channels of pollutants into lakes han et al 2020 wang et al 2019 yang et al 2016 lake pollution is becoming increasingly severe because of the long timescale of the water cycle difficulty to protect water resources from pollution high vulnerability of ecosystems and strong dependence of human economic development on lake water resources egessa et al 2020 nong et al 2020 yang et al 2016 yao et al 2020 lake water quality assessment may be understood as a multi criteria decision making process with quantitative water quality parameters as input and qualitative assessment as output wang et al 2018 wu et al 2017 yao et al 2019a the process contains two types of uncertainties 1 accuracy of water quality data and monitoring methods are constantly changing under the influence of hydrodynamic and biochemical factors which introduces random uncertainty into water quality evaluation norris and thoms 1999 2 water quality is affected by many water quality parameters and the nonlinear and complex relationships among parameters introduce fuzzy uncertainty into the determination of the degree of water pollution yan et al 2017 development of efficient water quality evaluation methods to accurately identify spatiotemporal characteristics is key to comprehensive improvement of lake water and its environment since the popularization of the water quality assessment index archibald 1972 many researchers have developed assessment methods taking into account uncertainties involved in lake water quality assessment these methods can be roughly divided into three categories 1 numerical methods based on multivariate statistical analysis such as nemerow pollution index npi chen et al 2017 principal component analysis pca platikanov et al 2019 and analytic hierarchy process ahp singh et al 2019 2 methods based on fuzzy set theory and grey system theory mainly including fuzzy mathematics evaluation method fmem zhang et al 2018 and grey system evaluation method gsem zhang et al 2018 3 methods based on artificial intelligence such as back propagation artificial neural network bp ann lu et al 2016 support vector machine svm ji et al 2017 and long short term memory network lstm wang et al 2019 while useful for water quality assessment these methods lack comprehensive consideration of the different uncertainties involved in water quality assessment and their applicability is limited to specific conditions the cloud model proposed by li et al 2009 can convert qualitative descriptions into quantitative values effectively mitigate impacts of uncertainties on accuracy of evaluation results and overcome the limits of the traditional water quality assessment methods described above wang et al 2016a using a forward and a reverse cloud generator the model converts qualitative concepts to quantities generating the quantitative cloud characteristics of expectation ex entropy en and hyper entropy he wang et al 2014 in the model clouds are formed from cloud droplets which are distributed in the domain space within the maximum and minimum boundaries fig a1 the forward cloud generator derives the degree of water pollution from qualitative information of water quality status while the reverse cloud generator calculates the eigenvalues of ex en and he from values of water quality parameters obtained from samples yao et al 2019a expectation is the expected spatial distribution of cloud droplets in the region and is a quantitative characterization of water quality entropy represents the degree of dispersion of cloud droplets and the range of cloud droplets in the domain space and is an effective measure of the uncertainty associated with the qualitative concept of water quality hyper entropy is a measure of entropy uncertainty which reflects the degree of condensation between cloud droplets ren et al 2017 the normal cloud model is the most basic version of the cloud model with normal gaussian membership function it has been widely used in lake water quality assessments wang et al 2016a 2016b yang and wang 2020 yao et al 2019a however uncertainties from monitoring methods or environmental parameters are not taken into account when uncertainties are only reflected by cloud characteristics wang et al 2016a 2016b the multidimensional similarity cloud model proposed by yao et al 2019a effectively solves this problem it uses values of the cloud characteristics of each evaluation index to form a vector and calculates the cosine of the angle between the vectors to determine water quality class level however when ex is far greater than en and he en and he are ignored which may lead to large deviations in evaluation results in fact similarity measurements of connotation and extension of the concept of qualitative description are included in cloud model similarity measurements yang and wang 2020 in lake water quality evaluations based on normal cloud models connotation refers to interval range of water quality classes that is cloud position extension refers to uncertainty associated with water quality class that is cloud shape similarity or sameness between clouds is determined by similarity between cloud shapes and positions however the similarity cloud model proposed by yao et al 2019a lacks detailed consideration of similarity measurements of cloud shapes thus in this study we used the random weighting method yao et al 2019b to randomize water quality monitoring data to reduce the impact of uncertainties on accuracy of water quality evaluations from this we developed a multidimensional shape position similarity cloud model mspscm that considers similarity measurements of cloud positions and shapes to capture water quality quantitatively and accurately the objectives of this study are 1 to develop the mspscm to comprehensively consider similarity measurements of cloud positions and shapes to capture lake water quality status more accurately 2 to apply the model to explore the spatiotemporal variability of water quality of a lake and its inflow and accurately identify the main factors underlying lake water quality deterioration and main pollution areas generally this study aims to propose a model that can accurately quantify lake water quality and thus provide more reliable technical support for effective monitoring and control of lake water quality 2 materials and methods 2 1 study area nansi lake is in the northern part of huaihe basin 116 34 117 21 e 34 37 35 20 n and extends along the northwest southeast axis see fig 1 it is 126 km long in the north south direction and 5 25 km wide in the east west direction lake waters cover an area of 1266 km2 average water depth is about 1 46 m annual average air temperature is 14 2 c average annual precipitation is about 700 mm precipitation between june and september accounts for about 70 of total annual precipitation yao et al 2019a since the completion of the erji dam in 1960 nansi lake has been divided into upper and lower lakes with areas of 602 and 664 km2 respectively catchment area of the upper lake is 88 4 of the total catchment area of nansi lake while that of the lower lake 11 6 of the total catchment area indicating that the upper lake is a larger source of pollution than the lower lake li et al 2020 nansi lake is an important water transport channel and storage lake on the east route of the south to north water transfer project er snwtp acceleration of urbanization decline of water storage capacity and environmental degradation have led to increased pollution of nansi lake with inflow being the main pollution source ren et al 2020 consequently a comprehensive study on water quality variation of the lake and its inflow can provide an improved scientific basis for the effective control of water pollution in nansi lake 2 2 data collection monthly water quality monitoring data of nansi lake between 2017 and 2019 were used they were collected from 5 monitoring stations on nansi lake and 13 stations at the entrance to the lake see fig 1 following the structural pollution characteristics of nansi lake watershed and the water quality indicators used along the er snwtp ma et al 2020 we examined the parameters of dissolved oxygen do chemical oxygen demand codcr ammonia nitrogen nh3 n total nitrogen tn and total phosphorus tp nansi lake is a shallow lake on the plain with an average water depth of 2 m therefore sampling was conducted at 0 5 m below the water surface li et al 2020 coordinates of sampling stations were recorded using a portable global positioning system the sampling boat was downstream of the sampling station to avoid water samples being polluted by bottom sediments stirred up by the boat thus ensuring sample quality cao et al 2017 sampler and sample container were first washed with sampling water the container was labeled using a self adhesive label sampling was repeated three times at each monitoring station to reduce deviations caused by the sampling process nong et al 2020 portable refrigerators were used to store water samples under cold and dark conditions samples were transferred immediately to a laboratory near the monitoring station li et al 2020 following the standard methods for the examination of water and wastewater han et al 2020 do was determined using a portable dissolved oxygen analyzer jpb 607a codcr was determined using the potassium dichromate method nh3 n was determined by salicylic acid spectrophotometry tn and tp were determined by peroxide potassium sulfate ultraviolet spectrophotometry and ammonium molybdate spectrophotometric method 2 3 development of the mspscm for assessing water quality the process for developing the mspscm is shown in fig 2 firstly the random entropy weight method yao et al 2019b was used to preprocess collected data to minimize impact of uncertainties in water quality assessment on accuracy of evaluation results then the processed data and boundary values of the parameters for each class were taken as the inputs of their respective reverse cloud generators and the quantitative cloud characteristics of water samples and water quality classes were obtained respectively sample clouds and class level clouds were generated by the forward cloud generator ren et al 2017 finally lakes water quality status are determined by comprehensively considering position and shape similarities between sample and level clouds of water quality parameters thereby avoiding large deviations between evaluation results and field quantities caused by the large differences between ex en and he in the existing multidimensional similarity cloud model yao et al 2019a 2 3 1 determination of sample cloud characteristics of water quality parameters the random weighting method was used to maximize the integrity of the information from water monitoring data and effectively reduce impacts of random or uncontrolled uncertainties associated with monitoring methods and environmental characteristics on the accuracy of monitoring data yao et al 2019a let the sample matrix of water quality parameters be x ij n m and the random matrix be r ij k n r a n d k n then the randomized water quality parameters sample matrix r ij k m can be obtained by dimensionless processing of x ij n m and r ij k n as follows 1 r ij k m r ij k n x ij n m i 1 n x ij n m where m is the number of water quality parameters n is the sample number of water quality parameters k is the number of random weighting times the processed water quality monitoring data were then used as inputs for the reverse cloud generator li et al 2020 to derive cloud characteristics of the water samples as follows 2 e x j 1 k i 1 k r ij e n j π 2 1 k i 1 k r ij e x j i 1 2 k j 1 2 m h e j s j 2 e n 2 j where s j denotes the variance of j th water quality parameter and s j 1 k 1 i 1 k r ij e x j 2 2 3 2 determination of level cloud characteristics of water quality parameters the environmental quality standards for surface water in china available at http www cnemc cn jcgf shj 200801 t20080128 647287 shtml divides water quality into six class levels class i v and beyond class v and defines the threshold range of water quality parameters for each class see table a1 class i indicates good water quality and the surface water can be used for drinking after simple treatment such as filtration and disinfection class ii indicates slightly polluted water which can be used for drinking after conventional treatment such as precipitation filtration and disinfection class iii indicates moderately polluted water which can only be used for drinking after being treated by potable water treatment plants class iv indicates considerably polluted water which is suitable for industrial and entertainment purposes and unsuitable for direct contact with the human body class v indicates severely polluted water which is suitable for agricultural irrigation water beyond class v is extremely severely polluted and has no use su et al 2017 the maximum and minimum boundary values of each parameter were derived using the method proposed by wang et al 2016a then the boundary values of each water quality parameter were used as inputs for the corresponding reverse cloud generator wang et al 2014 to obtain the quantitative cloud characteristics see table 1 for each water quality parameter under different water quality class levels as follows 3 ex b min b max 2 e n b max b min 6 h e k e n where b max and b min are the maximum and minimum values of each class level boundary respectively k is an adjustment coefficient used to adjust the condensation degree of cloud droplets in level clouds and is generally set to 0 1 liu et al 2014 2 3 3 determination of water quality class level lake water quality class level was determined on the basis of the similarity between sample and class level clouds generated by quantitative cloud characteristics ex en and he of water quality samples and classes yao et al 2019a similarity between sample and level clouds is determined by shape and position similarities between the clouds shape similarity depends on en and he which is reflected in the geometric similarity between sample and level clouds as shown in fig 3 a for normal cloud c 1 let the area enclosed by the expected curve and abscissa be s 1 ϕ z 1 be the probability density function of the standard normal distribution z 1 x e x 1 e n 1 then s 1 2 π e n 1 1 2 π e n 1 exp x e x 1 2 2 e n 1 2 d x 2 π e n 1 ϕ z 1 d x 2 π e n 1 similarly the area m s 1 enclosed by the maximum boundary curve of c 1 and the abscissa is 2 π e n 1 3 h e 1 thus the area s 2 enclosed by the expected curve and abscissa of normal cloud c 2 is 2 π e n 2 and the area m s 2 enclosed by the expected curve and abscissa of c 2 is 2 π e n 2 3 h e 2 μ 1 s 1 m s 1 and μ 2 s 2 m s 2 are the uncertainty of c 1 and c 2 respectively and represent shape similarity consequently let the normal cloud of the i th ake water sample be c i and the normal cloud of the j th water quality class be c j then shape similarity between the sample and level clouds is calculated as follows 4 si m s c i c j ξ 1 5 min e n i ξ 3 h e i ξ e n i ξ e n j ξ 3 h e j ξ e n j ξ ξ 1 5 max e n i ξ 3 h e i ξ e n i ξ e n j ξ 3 h e j ξ e n j ξ where e n i ξ h e i ξ denote the entropy and hyper entropy of the i th sample of the ξ th water quality parameter and e n j ξ h e j ξ denote the entropy and hyper entropy of the j th water quality class of the ξ th water quality parameter position similarity depends on ex which is the similarity difference caused by the movement of sample and level clouds as shown in fig 3 b following the 3en rule of normal distribution ren et al 2017 the interval ex 3en ex 3en can represent the range of normal clouds the overlapping interval length of the abscissa of normal clouds c 1 and c 2 is l 1 min e x 1 3 e n 1 e x 2 3 e n 2 max e x 1 3 e n 1 e x 2 3 e n 2 and the total length of their abscissa interval is l 2 max e x 1 3 e n 1 e x 2 3 e n 2 min e x 1 3 e n 1 e x 2 3 e n 2 following set theory the ratio of l 1 to l 2 represents position similarity between normal clouds c 1 and c 2 thus position similarity between sample and level clouds is calculated as follows 5 si m p c i c j ξ 1 5 min e x i ξ 3 e n i ξ e x j ξ 3 e n j ξ max e x i ξ 3 e n i ξ e x j ξ 3 e n j ξ ξ 1 5 max e x i ξ 3 e n i ξ e x j ξ 3 e n j ξ min e x i ξ 3 e n i ξ e x j ξ 3 e n j ξ where e x i ξ e n i ξ denote expectation and entropy of the i th sample of the ξ th water quality parameter and e x j ξ e n j ξ denote expectation and entropy of the j th water quality class of ξ th water quality parameter for a sample cloud and level cloud with the same shape similarity between the clouds changes with distance between the clouds similarly if positions of the sample and level clouds coincide similarity changes with change in shape thus establishing a comprehensive similarity variable between the two clouds si m c c i c j on the basis of shape and position similarities between the sample and level clouds is the key to accurate assessment of water quality si m c c i c j is derived as follows 6 si m c c i c j si m s c i c j s i m p c i c j note ec and mbc denote expectation curve and maximum boundary curve respectively 3 results and discussion 3 1 analysis of water quality parameters to accurately obtain the main factors affecting the water quality of the study area annual average concentrations and variability of water quality parameters were derived by using the collected data from 2017 to 2019 see table 2 in addition differences between annual average concentrations of water quality parameters at 18 monitoring sections were tested using one way analysis of variance one way anova varol 2019 see fig 4 statistical variables of do concentration in table 2 show that do in 2017 2019 mainly fluctuated between class i and ii and exhibited an upward trend indicating that the study area has a good aquatic oxygen environment there were statistically relatively differences between average do concentrations at different monitoring sections one way anova p 0 05 see fig 4 concentrations at monitoring sections in the upper lake were lower than those at the lower lake which may be because of lake self purification and purification effects of the erji dam yao et al 2019a statistical variables of codcr concentration in table 2 show that codcr in 2017 2019 was mainly in class iii which indicates that the water in the study area was moderately polluted by organic matter and could only be used for drinking after being processed by a sewage treatment plant there were statistically relatively differences between average codcr concentrations at different monitoring sections one way anova p 0 05 see fig 4 especially for the upper lake average codcr concentrations at monitoring sections in the upper lake 15 mg l codcr 20 mg l were in class iii except that at s14 codcr 14 64 mg l were in class ii however differences between average codcr concentrations at different monitoring sections in the lower lake were statistically insignificant this is mainly because of the small number rivers entering the lower lake weak circulation in the lake and accumulation of organic pollutants in the upper lake li et al 2020 statistical variables of nh3 n concentration in table 2 show that nh3 n in 2017 2019 was mainly in class ii which indicates that the water in the study area was slightly polluted by nh3 n from organic matter and could only be used for drinking after conventional treatment there were statistically significant differences between average nh3 n concentrations at different monitoring sections one way anova p 0 01 see fig 4 concentrations of nh3 n in the upper lake were basically higher than those in the lower lake and the highest nh3 n concentrations only reached the class iii nh3 n s18 0 70 mg l 1 00 mg l this indicates that water quality in nansi lake was less affected by nh3 n organic pollutants statistical variables of tn concentration in table 2 indicate that tn in 2017 2019 mainly fluctuated between class iii and v which indicates that nansi lake was severely polluted by nitrogen containing organic matter and was faced with the threat of eutrophication aggravation there were statistically significant differences between average tn concentrations at different monitoring sections one way anova p 0 01 see fig 4 average tn concentrations were in class iii at s1 and s2 and class iv at s4 and s5 reflecting that nitrogenous organic pollutants in the lower lake were more than that in the upper lake moreover average tn concentrations were in class iii at s7 s10 s11 and s12 class iv at s8 and s13 class v at s9 s16 and s17 and beyond class v at s6 s14 s15 and s18 indicating that nansi lake inflow waters especially the lower lake inflow waters were severely polluted by organic matter containing nitrogen this is mainly because of large amounts nitrogen containing pollutants such as chemical fertilizers and livestock manure discharged into the inflow rivers of nansi lake which were affected by water erosion and sediment microbial degradation in the migration process resulting in a decrease in the concentration of these pollutants in nansi lake qu et al 2020 statistical variables of tp concentration in table 2 indicate that tp of nansi lake and its inflow in 2017 2019 mainly fluctuated between class iii and iv class ii and iii respectively the main reason is that the low circulation in nansi lake and high flow rate in its inflow water resulting in most of the phosphorus containing pollutants carried by the inflow being concentrated in nansi lake li et al 2020 there were statistically significant differences between average tp concentrations at different monitoring stations one way anova p 0 01 see fig 4 average tp concentrations were in class iv at s1 and s2 and class iii at s4 and s5 which reflects that the phosphorus containing pollutants of the upper lake is more than that of the lower lake average tp concentrations were in class ii at 6 out of 10 60 upper lake inflow sections and at 2 out of 3 67 s16 and s17 lower lake inflow sections class iii at 4 out of 10 40 upper lake inflow sections and class iv at s18 tp 0 235 mg l 0 20 mg l indicating that it is necessary to focus on strengthening the treatment of phosphorus containing pollutants into the inflow rivers through s18 differences between tp in nansi lake and tp in inflow rivers indicate that spatial variability of tp in nansi lake is mainly affected by p2o5 content in the bottom sediments and that impact of tp carried by inflow rivers was relatively small on tp concentration in nansi lake yao et al 2019a 3 2 spatiotemporal variability of water quality from the mspscm since the actual circumstance of lake water quality is determined by the comprehensive effect of various water quality parameters the monthly time series of water quality parameters of 18 water quality monitoring sections from january 2017 to december 2019 were used as the input to the mspscm to evaluate spatiotemporal variability of water quality in the study area 3 2 1 spatiotemporal variability of water quality on a monthly scale to obtain the periods and regions with serious water pollution monthly water quality monitoring data from four monitoring areas the upper lake monitoring sections s1 s3 its inflow sections s6 s15 the lower lake s4 and s5 and its inflow s16 s18 were used as inputs to the mspscm to calculate the comprehensive similarity degree between water quality monitoring regions and water quality classes see fig 5 to determine the water quality status of each monitoring area for different periods fig 5 a b shows that from january 2017 to december 2019 water quality of the upper lake had the highest comprehensive similarity degree with class iii and that of its inflow had the highest comprehensive similarity degree with class iv this indicates that the upper lake was moderately polluted its water was only suitable for drinking after being treated by a sewage treatment plant ma et al 2020 however upper lake inflow was more severely polluted mainly because of the residential areas and industries concentrated around nansi lake and discharge of domestic and industrial sewage through the inflow water ren et al 2020 upper lake inflow was suitable for industrial and entertainment purposes and unsuitable for direct contact with the human body su et al 2017 the difference between water quality of the upper lake and that of its inflow shows that the upper lake plays an important role in water purification in november 2017 and september 2019 upper lake and its inflow had the highest comprehensive similarity degree with class v indicating that these monitoring regions were severely polluted during these two months this may be because during the dry season november 2017 when there is little precipitation upper lake inflow is mainly derived from domestic and industrial sewage yao et al 2019a during high water september 2019 when rivers transport sediments into the lake at high rates large amounts of nitrogen containing organic pollutants are discharged into the lake li et al 2020 excess tn concentration resulted in severe water pollution in the upper lake and its inflow refer to the section 3 1 during these two months fig 5 c d shows that during the study period water quality of the lower lake and its inflow had the highest comprehensive similarity degree with class iii indicating that these monitoring regions were also moderately polluted in november 2017 and september 2019 water quality of the lower lake and its inflow had the highest comprehensive similarity degree with class iv indicating that these monitoring regions were considerably polluted during these two months the reasons underlying this peak in pollution are the same as those that affected the upper lake during the same period during these two months water quality in the upper lake was higher than that in the lower lake which may be because upper lake water is purified as it flows into the lower lake and the erji dam regulates upper lake water quality yao et al 2019a domestic sewage and industrial wastewater were gathered in the inflow of upper lake resulting in water quality of lower lake inflow being higher than that of the upper lake li et al 2020 3 2 2 spatiotemporal variability of annual water quality to further identify the major sources of serious water pollution annual water quality monitoring data from monitoring sections s1 s18 were used as inputs to the mspscm to calculate comprehensive similarity degree between water quality monitoring sections and water quality classes as shown in fig 6 to determine the water quality status of each monitoring section fig 6 shows that in 2017 water quality at s1 s2 and s3 had the highest comprehensive similarity degree with class v class iv and class iii respectively this indicates that water quality of the upper lake was gradually improved from upstream to downstream this is mainly because many rivers flow into the upper lake discharging large amounts of tn into the upper lake near s1 tn concentrations at s6 s14 and s15 monitoring sections were in class v see section 3 1 however the upper lake has a rapid flow rate yao et al 2019a and strong self purification ability ma et al 2020 resulting in improvement of water quality as the water flows downstream water quality at s6 s8 s9 s12 and s15 had the highest comprehensive similarity degree with class iv indicating that rivers flowing through these sections were the major sources of serious water pollution of the upper lake in 2017 water quality at s7 s10 s11 s13 and s14 had the highest comprehensive similarity degree with class v indicating that rivers flowing through these sections were the major sources of water quality deterioration of upper lake inflow in november 2017 moreover water quality at s17 and s18 had the highest comprehensive similarity degree with class iv reflecting that rivers flowing through these sections were the major sources of water quality deterioration of lower lake inflow in november 2017 in 2018 water quality at s1 s2 and s3 had the highest comprehensive similarity degree with class iv class iii and class iii respectively these results are consistent with the spatial variability of upper lake water quality in 2017 for upper lake inflow water quality at s11 had the highest comprehensive similarity degree with class iii that of other sections s6 s10 and s12 s15 had the highest comprehensive similarity degree with class iv indicating that rivers flowing through s6 s10 and s12 s15 were the major sources affecting the water quality of the upper lake thus strengthening pollution control of the rivers flowing through sections s6 s10 and s12 s15 is the key to comprehensively improve upper lake water quality for the lower lake and its inflow water quality at s4 s5 s16 and s17 had the highest comprehensive similarity degree with class iii that at s18 had the highest comprehensive similarity degree with class iv indicating that the river flowing through s18 was the major source that threaten the water quality of lower lake in 2019 water quality at s1 and s2 had the highest comprehensive similarity degree with class iv that at s3 had the highest comprehensive similarity degree with class iii which was similar to upper lake water quality spatial variability during 2017 2018 water quality at s9 s13 and s14 had the highest comprehensive similarity degree with class iv indicating that rivers flowing through these sections were the major sources of serious water pollution in upper lake inflow in 2019 see fig 5 b water quality at s6 s8 s10 s12 and s15 had the highest comprehensive similarity degree with class v rivers flowing through these sections were the major sources of water quality deterioration in the upper lake inflow in september 2019 water quality at s4 s5 s16 and s17 had the highest comprehensive similarity degree with class iii that at s18 had the highest comprehensive similarity degree with class iv indicating that river flowing through s18 was the major source of water quality deterioration in the lower lake inflow in september 2019 3 3 advantages and limitations of the mspscm for evaluating water quality our results show that the mspscm developed in this study can accurately capture the evolution of lake water quality and locate severely polluted areas compared with the existing multidimensional normal cloud model mncm wang et al 2016b and multidimensional similarity cloud model mscm yao et al 2019a the mspscm developed in this study effectively mitigates uncertainties associated with lake water quality assessments and comprehensively considers shape and position similarities between sample cloud and level cloud to determine lake water quality class level which allows differences between water quality to be identified more accurately water quality data from 12 typical lakes in china wang et al 2016b table 3 were used to verify that mspscm results can more accurately reflect field conditions than mncm and mscm results of the mspscm results 9 out of 12 75 are identical to mncm results and 10 out of 12 83 are identical to mscm results mspscm results are highly consistent with lake water quality status in the field which fully reflects the reliability of the mspscm results from the different models differ for gantang and west lakes with large areas of algal bloom instead of total algal bloom in these lakes wang et al 2016a indicating that most of the these lakes has relatively serious water pollution water quality in gantang and west lakes is more in line with class v verifying that mspscm results can more accurately reflect lake water quality than mncm and mscm results determination of lake water quality class usually depends on the threshold of each class for each water quality parameter yao et al 2019a different lakes are affected by different natural factors such as precipitation insolation domestic pollution industrial wastewater discharge and other human factors as a result there are different water quality class thresholds for different lake water quality parameters although the mspscm makes up for the deficiencies of mncm and mscm like mncm and mscm the mspscm is also limited because it determines the degree of pollution of different lakes using the national standard water quality parameter class thresholds which may lead to large deviations between evaluation results and field quantities consequently in future research it is urgent to develop a system which defines class thresholds that can accurately reflect field conditions 4 conclusion to improve the match between water quality parameters obtained from samples and water quality standards in multidimensional cloud model studies of lake water quality assessments a reverse cloud generator was used to process sample data and water quality class level interval to generate corresponding sample clouds and level clouds and a mspscm was established from the shape and position similarities between sample and level clouds spatiotemporal variability of five selected water quality parameters and water quality status of nansi lake from january 2017 to december 2019 were explored using the proposed mspscm method results from the mspscm were compared with those from existing mncm and mscm methods the main conclusions are as follows 1 during the water quality monitoring period from 2017 to 2019 do codcr and nh3 n concentrations in the water quality monitoring sections of nansi lake and its inflow did not exceed class iii which met the minimum standards of water quality in nansi lake however tn concentrations at s1 s3 and tp concentrations at s4 s5 were in class iv indicating that nansi lake has been threatened by excessive nitrogen and phosphorus organic pollutants to aggravate eutrophication moreover tn concentrations at s6 s14 s15 and s18 beyond class v and tp concentration at s18 was in class iv indicating that the excessive nitrogen and phosphorus organic pollutants in the inflow rivers flowing through these sections were the main factors threatening the water quality of nansi lake the local environmental protection departments should strengthen the control of the sewage discharge of these inflow rivers to ensure the water quality of nansi lake 2 in 2017 2019 the monthly variation of water quality in monitoring regions shows that the water quality of upper lake lower lake and its inflow were concentrated in class iii upper lake inflow was concentrated in class iv especially in november 2017 and september 2019 the water quality of upper lake and its inflow reached class v indicating that the upper lake inflow was the major source of water quality deterioration in nansi lake annual variation of water quality in monitoring sections shows that 6 out of 10 60 monitoring sections in the upper lake inflow were seriously polluted especially water quality at s6 s8 s12 and s15 changed from class iv to class v it is urgent to investigate the pollution sources at these sections to achieve targeted treatment of water quality in nansi lake 3 comparisons between cmscm mncm and mscm results show that mspscm results can more accurately reflect lake water quality than mncm and mscm results the mspscm method provides scientific and reliable technical support for obtaining accurate spatiotemporal characteristics of water pollution in lakes from different time scales declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the national natural science foundation of china grant no 51879006 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126379 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4464,short term water demand forecasting stwdf is the foundation stone in the derivation of an optimal plan for controlling water supply systems deep learning dl approaches provide the most accurate solutions for this purpose however they suffer from complexity problem due to the massive number of parameters in addition to the high forecasting error at the extreme points in this work an effective method to alleviate the error at these points is proposed it is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them to our knowledge this is the first work that considers the problem related to the extreme points moreover the water demand forecasting model proposed in this work is a novel dl model with relatively low complexity the basic model uses the gated recurrent unit gru to handle the sequential relationship in the historical demand data while an unsupervised classification method k means is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters real data obtained from two different water plants in china are used to train and verify the model proposed the prediction results and the comparison with the state of the art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy furthermore it is found that extending the data set significantly reduces the error by about 30 however it increases the training time keywords deep learning neural networks gated recurrent unit unsupervised classification water demand forecasting extreme points 1 introduction water scarcity has become a threat to humankind in recent decades many efforts in all possible directions are being made to compensate for this growing problem northey et al 2016 gonzález zeas et al 2019 the major reliable strategies for that include water treatment zinatloo ajabshir et al 2020 water desalination and optimization of water management systems nanotechnology is the most powerful technology employed for water treatment where researchers have done impressive work zinatloo ajabshir et al 2020 zinatloo ajabshir et al 2017 moshtaghi et al 2016 on the other hand stwdf is the foundation stone of the optimization of water management systems therefore numerous researchers have directed their efforts towards this problem nowadays deep learning dl is the most dominant approach which provides the most promising solutions to a myriad of critical problems to mention a few examples zhou 2020 has proposed deep learning method for forecasting the water quality in the presence of missing data friedel et al 2020 have compared four machine learning methods to predict groundwater redox status in the agriculturally dominated regions of new zealand wang et al 2020 have used a deep belief network to forecast the depth of snow over in alaska flood prediction is another crucial problem that utilizes deep learning approaches an encoder decoder based on long short term memory lstm is proposed by kao et al 2020 for multi step ahead flood forecasting while flood susceptibility modeling using deep learning is investigated in many studies such as pham et al 2021 bui et al 2020 lstm is also used by ni et al 2020 for streamflow and rainfall prediction where two lstm based models are built one combined wavelet network with lstm and the other combined convolutional network with lstm to achieve better performance stwdf is one of these problems that benefit most from dl to develop effective methods however some challenges might impede the success of dl based solutions model complexity the accumulative error when forecasting multi steps and the significant prediction error at the extreme points are some of these challenges that still need investigation model complexity and the size of the model in particular become serious constraints when using federated learning approach and bring some extra challenges to knowledge transformation technologies model complexity includes two types time complexity and space complexity time complexity is brought on by the time required to train the model and by the data size required for training the place complexity problem usually comes to the surface when the model has a massive number of parameters which increases the model size as well as the training time the second challenge is the accumulative error problem which affects the multi step prediction of water demand when relying on the historical data of water demand for stwdf the predicted values are involved in predicting the following values thus the prediction error is compounded by the use of inaccurate values of the predicted water demand the third challenge is the significant error at the extreme points which occur as a normal reflection of the nonlinearity of daily water demand these points are recognized as periods where water demand is dramatically different from the average demand in the adjacent periods making it difficult for the model to approximate the demand at these points a few examples of extreme points are illustrated in figs 7 and 8 the error at these points is usually unacceptable and may lead to severe problems in the distribution system in literature complexity problem was not a critical issue when statistical methods such as auto regression integrated moving average arima method and the seasonal version of it sarima in addition to markov chain are used for stwdf pandey et al 2021 however their accuracy is not sufficiently satisfactory caiado 2010 has achieved 11 of mean square percentage error for one day prediction and 13 2 for 7 days ahead prediction by combining sarima with the generalized autoregressive conditional heteroscedasticity method for daily wdf arandia et al 2016 also have achieved 4 21 of mean absolute percentage error mape for 15 min prediction of water demand in dublin spain by involving some data assimilation technique to improve the performance of srima method however their proposal does not work well with hourly prediction where the best mape they have got is 38 12 brentan et al 2017 have built their prediction model based on svr and fourier methods for on line prediction of hourly water demand their model achieves mape of 3 41 for one step prediction with the expansion in the application of machine learning zhou et al 2020 he et al 2019 and artificial neural networks anns salloom et al 2020 yu et al 2020 he et al 2017 several studies have proved that machine learning methods outdo the stochastic and the probabilistic models for wdf gagliardi et al 2017 has proved the anns overcome markov chain based models in terms of forecasting accuracy herrera et al 2010 and bai et al 2015 have proved the efficiency of support vector regression method for hourly wdf guo et al 2018 has compared statistical methods and conventional anns with deep learning method and proved that the dl methods give more accurate results when predicting water demand for short horizon in fact most researchers focus on improving prediction accuracy without paying too much attention to model complexity furthermore a new trend that exacerbates this problem has started looming on the horizon recently where some researchers comprise many machine learning models in one system then they chose a different one for different prediction periods based on probabilistic methods ambrosio et al 2019 have used a combination of multilayer perceptron svm elm random forests and adaptive neural fuzzy inference systems for hourly wdf antunes et al 2018 have investigated combining svr ann k nearest neighbours and random forest regression for real time wdf this strategy is meant to use the most accurate model in the most suitable prediction period however it requires a massive amount of computations and memory to save the parameters of all models and system configuration setting the error at extreme points also contributes to the worsening of the prediction accuracy however it has not attracted researchers attention guo et al 2018 are the first to point out this problem in their work but they have not provided any solution the accumulative error problem may occur when predicting several steps ahead based on the historical data of water demand some researchers tried to solve this problem individually by building a new neural network model and train it to make the predicted values approach the real ones obviously this method increases the parameter of the system dramatically in fact the accumulative error problem can be mitigated by involving several factors besides the historical data papageorgiou et al 2015 kley holsteg and ziel 2020 so the impact of the predicted values in the input can be reduced efficiently many factors influence water demand level dias et al 2018 but only factors such as meteorological conditions and day type which have weekly or daily distinguishable changes have real impacts on stwdf romano and kapelan 2014 however managers still rely on the historical data of water demand for stwdf and ignore the other possible factors because of the difficulties of gathering data about them in real time particularly in the 15 min interval moreover some available information about some factors such as meteorological information are not sufficiently accurate for short time prediction rayner et al 2005 making them unreliable in this work we propose a novel dl model for stwdf it is built based on the gated recurrent unit gru and unsupervised classification method k means involving data classification as a prior step has two major benefits i it helps with creating new features to compensate for the leak of reliable features which reflects positively on the prediction accuracy and the accumulative error ii it creates a relationship between data of different days which an ann can be easily approximated with a small number of parameters which in turn enhances the space complexity of the models additionally we investigate using a novel technique to alleviate the nonlinearity at the extreme points and reduce the error it depends on inserting virtual data between the actual data so that the nonlinearity at these points is drastically declined the contribution of this paper can be summarized as follows a novel dl model that provides a high prediction accuracy for both one step 15 min and multi step 96 steps ahead prediction is proposed it is built based on gru neural network supported by an unsupervised classification step to enhance the accuracy a new technique for mitigating the prediction error at extreme points is proposed and investigated the accumulative error problem which occurs in multi step prediction is mitigated by means of classification step which establishes new features to rely on in the prediction a comparison with the state of the art is carried out to show the effectiveness of our proposed methods in terms of accuracy and model complexity the rest of this paper is organized as follows section 2 describe the equipment used to carry out this research section 3 explains the research methodology and the methods proposed in this work section 4 clarifies the results of this research including the structure of the prediction model and the evaluation results while section 5 provide an intensive discussion to illustrate the underlying cause of these results section 6 illustrates the significance of the results section 7 includes the conclusion and the planned future works 2 research equipment and tools the machine used to carry out this research including classification step training the dl models prediction process and verification and evaluation of the proposed method is an asus laptop with an intel core i7 processor four real cores with a speed of 2 6 ghz each the installed ram is 16 gb all models are built using python 3 6 programming language over anaconda platform keras library and tensorflow backend are used to build the dl models due to their availability and convenience so anyone can simply redesign the models in a short time in fact the proposed method for stwdf does not contain complicated steps that require very powerful hardware ten years of water demand data do not exceed 1 gb if saved in a csv file which means that small storage space is sufficient to store the required data all calculations can be achieved using a cpu with the specification mentioned above as the minimum requirement the time spent on prediction and training using the aforementioned specifications are listed in the results section thus the proposed model is achievable in the industrial field by the currently available model 3 methodology firstly the water demand is collected every 15 min from the water distributing system and the database is updated every 24 h one step prediction scenario and multi step prediction scenario are considered as described in section 3 2 k means method is applied to classify the data based on their numerical distance into mclasses as described in section 3 3 the number mis determined using elbow method then demand readings and the classes are organized in vectors v t c 1 c 2 c m each vector contains the demand reading v t in addition to the value of each class that related to the corresponding demand value c 1 c 2 c m where the value of the class c j is determined as in eq 2 the last 96 values of water demand are used to predict the demand in the following 15 min in scenario 1 prediction is made by passing 96 vectors to the dense block this block handles each vector individually and results in one value for each vector thus the output of the dense block contains 96 values next these 96 values are passed to the gru block the output of the gru block is the desired demand value in scenario 2 the prediction is achieved iteratively by repeating scenario 1 for 96 times the result of each iteration is used in the next iteration the prediction model consists of the dense block and the gru block as explained in section 3 4 the final structure of the prediction model is clarified in section 4 1 and shown in fig 4 the model is trained on the training data set as described in section 3 6 in order to solve the problem of the massive error at the extreme points the data set is extended by injecting virtual values between each two adjacent actual values such that the linearity is increased the proposed method is described in section 3 5 the accuracy of the proposed methods is evaluated based on the mean absolute error mae and the mean absolute percentage error mape the complexity of the model is assessed based on the akaike information criterion aic the complete evaluation methodology is described in section 3 8 3 1 water demand data the data used in this research are collected in 2016 from two different district metering areas dmas in changzhou city in china guo et al 2018 fig 1 shows the location of the two dmas on the map as two red spots dma1 is a residential area with about 13000 residences and a few commercial buildings while the second area dma2 is an industrial area with a population of about 8500 and 300 factories the data set contains 25000 measured demand values for each dma each day is divided into 96 duration each of 15 min and the total water demand is measured for the whole dma at the end of each duration the time recorded in the database is the end of each duration the database is updated every 24 h i e the actual values of water demand for each day are not available until the end of the day table 1 lists the statistical information of the data used in this research data set is divided into two sets training set and testing set the training set is used to train the models it contains 22500 records during training 15 of the training data is used for validation the testing set contains 2500 records used to test the accuracy of the models after training ends 3 2 prediction scenarios this work targets the problem of short term water demand forecasting based on the historical data of water demand each prediction step is done for one prediction period which is 15 min in length water demand values of the last 96 periods i e the demands of the previous 24 h are employed to achieve each prediction step two forecasting scenarios are considered i scenario 1 is a one step forecasting scenario where the actual values of the required historical data are assumed to be available when the prediction starts ii scenario 2 is a multi step forecasting scenario where 96 steps each one is of 15 min are forecasted iteratively in this scenario the actual values are available only for the first prediction step while the required data for the following steps contain recently predicted values in addition to the available previous actual values when they are available in fact this is the realistic scenario since the database considered in this research will be updated at the end of each day 3 3 data classification and feature building the essential step of building an ann model for any purpose is identifying the inputs to that model in this research the historical water demand data is the only available data for prediction we choose to use the demand values of the last 96 periods t 1 t 2 t 96 for stwdf for each prediction period t in order to compensate for the lack of features the available data is classified into mclasses then the created classes are used as new features since there is no clue about the possible features to simplify the method a simple version of the algorithm k means is applied to implement an unsupervised classification of the historical data of water demand based on the numerical distance between values the number mshould be identified carefully due to the fact that large mundermines the benefit of classification on the other hand small mincreases the within cluster square error also called distortion yang and sinaga 2019 elbow method is used widely in the literature and in this research to determine the optimal number of classes to be used according to this method the best value of mis the smallest number that guarantees a small distortion the distortion is calculated based on the following equation yang and sinaga 2019 1 sse i 1 n j 1 m w i j x i c j 2 2 where n is the total number of data samples and i is the sample index in the class j m is the total number of classes and jis the class s index w i j is calculated as follows 2 w i j 1 x i class j 0 otherwise the initial center of each class j is determined randomly as in the following formula 3 cj randn 1 m std mean where randn a m generates an array of a m items each of them is a random float sampled from the normal distribution std and mean are the standard deviations and the mean of each data set respectively they are involved in 3 in order to guarantee that the created centers represent the whole data 3 4 dl model design by classifying the data new relationships between them are created where data belongs to the same classes has a high level of similarity to get the benefits of classification the designed dl model should be able to approximate the relationship between each value and the classes additionally the model should be able to approximate the sequential relationship between water demands in this research 96 records of the historical data are used to achieve every prediction step considering these requirements the model comprises two blocks the dense block that works upon the relationship of the water demand value with the classes and the gru block that works upon the sequential relationship between water demand data thus the proposed model called data classification based neural network model or dcgru model in the rest of this paper 3 4 1 dense block the input of the model takes the form of a 96 m 1 matrix 96 is number of the rows where each row contains m 1 values which are one demand value and the corresponding values of the created classes then the input is passed into three fully connected layers of the type time distributed dense time distributed dense layer is a dense layer that accepts two dimensional input and applies the activation function to every row separately and the output size equals the number of the rows in the input thus the output of this part of the model is a vector of 96 values each of them implicitly represents the water demand value of one record and its relationship to the created classes when choosing the activation functions for layer 1 and layer 2 we consider getting a positive number at the output of each layer may accelerate the convergence while an amplifier function is needed in the output layer the number of units in each dense layer is determined using grid search taking into account that maintaining a small number of units in each dense layer is preferred in order to arrive at a simple neural network model 3 4 2 gru block the output of the dense block is a sequence of 96 values it is passed into the gru block which contains a hidden gru layer and one gru cell represents the output layer of the dcgru model the number of units in the hidden gru layer is chosen to what suggested in the previous research chung et al 2014 the input vector elements are passed to the gru layer consecutively one item at a time the output of the gru layer is calculated based on three activation functions two inner functions and one for the output the sigmoid function is employed in several works in the literature xu et al 2019 as an inner activation function for gru layers while the hyperbolic tangent tanh is used as an output activation function these three functions are applied to the current input and the output of the previous unit based on the following equations deng et al 2019 4 z t σ g w z x t u z h t 1 b z 5 r t σ g w r x t u r h t 1 b r 6 h t 1 z t h t 1 σ h w h x t u h r t h t 1 b h where x t is the input vector h t is the output vector z t is the update gate vector r t is the reset gate vector and w u and bare the trainable parameters matrices and vectors while σ g and σ h are the sigmoid and the hyperbolic tangent functions respectively the hyperparameters of the proposed model structure are listed in table 5 3 5 expanding the data set in order to alleviate the error at the extreme points we suggest expanding the data set in fact the problem shows up because of the high difference between the water demand value at the extreme points and that at the points around them as shown in fig 7 and 8 to solve this problem we try to reduce this difference by setting a number ρ of virtual water demand values between every two consecutive actual values the new values are inserted between every two consecutive actual values next the new values are classified based on the k means method in practice the major problem of the method presented is the determination of the suitable ρ to be inserted into the data set at first glance increasing the number of virtual values seems to be suitable for reducing the error at the extreme points however giving it more thought the gru layer remembers the sequential changes between input data not data itself and reflects that on the weights of the model thus increasing the length of the linear sequence between two actual demand values reflects negatively on the next step of forecasting which exacerbates the error moreover inserting many virtual values enlarges the input which in turn enlarges the training and forecasting time the value of ρ is determined experimentally taking into account the aforementioned thoughts the input of the model is also expanded to comprise the actual water measurements and the virtual water values for a full day i e the input size becomes 96 ρ 1 the structure of the prediction model does not change however we will use the abbreviation edcgru to refer to the dcgru model applied to the expanded data set in the rest of this article when predicting water demand for the next period in scenario 1 all virtual values should be predicted one by one and classified then included in the input data to achieve one prediction step in scenario 2 96 ρ 1 values of water demand should be predicted consecutively every ρ 1 predicted value is considered a forecasting result of the corresponding forecasting period 3 6 training all models are trained under the same circumstances using the same equipment and the same data the parameters of all models are initialized based on the xavier uniform initializer it initializes the weights with random numbers picked from a uniform distribution within an interval limited by values related to the number of input and output weights of the corresponding layer the designed model is a compound of two blocks dense block and gru block each block uses different activation functions besides each block acts upon different kinds of features therefore a different trainable parameter of the model requires a different learning strategy adam training algorithm is used to train all models in this research it trains every weight in the neural network using different learning rates according to its previous changes which match our learning strategy desired also adam algorithm distinguishes itself from the others because it is a quick trainer and memory conservative sun et al 2019 which is one goal of this research mini batch technique is used during the training in this technique the prediction error is calculated for a small batch of prediction steps and then the average error is backpropagated to adjust models weights and biases in order to guarantee that the training data are not delivered to the model in a meaningful order the training data is shuffled after every epoch although all models are trained similarly however different training parameters are required for a different model to result in good prediction accuracy table 4 lists the training parameters for every model 3 6 1 training parameters of the dcgru model for this model the learning rate is chosen to what is recommended in kingma and ba 2014 however other values around 0 001 were examined and found that 0 002 is the best learning rate for dma1 while 0 001 is the best learning rate for dma2 β 1 and β 2 are set to 0 9 and 0 999 respectively for both dmas in order to avoid overfitting the early stop technique is used with a tolerance of two consecutive epochs i e the training stops automatically when the prediction error of training data keep descending while the error of validation data starts ascending for two consecutive epochs the final number of epochs is 15 and 18 for dma1 and dma2 respectively the batch size is 100 for both dmas 3 6 2 training parameters of the edcgru model by expanding the input the initial values of the input layer changes thus different values for the training parameter are required the learning rate is scheduled to start at 0 002 and decreases by 50 per cent every 5 epochs for both dmas epochs number is determined using the early stop technique with a tolerance of 4 epochs the final epochs numbers are 17 and 21 for dma1 and dma2 respectively batch size is set to 100 3 7 comparison methodology to show the effectiveness of the proposed methods we compare our model with two models the first one is the basic forecasting model referred to as bgru model it is similar to the dcgru model but it does not involve the classification step through this comparison the benefits of the classification step to enhance the prediction accuracy is unveiled the second one represents the state of the art to show the superiority of the proposed model in terms of accuracy and space complexity the grun model proposed by guo et al 2018 is chosen to compare with for the following reasons authors present comprehensive research about the efficiency of the deep learning method and provide strong evidence about the superiority of deep learning over other possible methods thus comparing the model proposed in this work with the grun model is sufficient to prove that our model is better than those in the state of the art they also consider the two forecasting scenarios that are considered in this research moreover they use the same data sets we use for training validation and testing the structures of these two models are illustrated in the section 3 7 1 and section 3 7 2 additionally to increase the scientific value of this work a comparison with the results described in previous publications is listed in table 7 the publications that consider svm or neural network based methods are included due to their superiority in stwdf over other methods we compare this work with six works from the literature a brief description of these works is listed here du et al 2021 combine discrete wavelet transform dwt and principal component analysis pca with lstm for one step daily wdf antunes et al 2018 use svr ann k nearest neighbors and random forest regression for one step prediction of daily water demand then they choose the best result based on probabilistic methods brentan et al 2017 combine svm and fourier method for one step prediction of daily water demand mouatadid and adamawski 2017 use eml for one step prediction of daily water demand candelieri 2017 uses clustering to enhance svm method for one step prediction of hourly water demand tiwari et al 2016 use elm with wavelet ann for one step daily wdf readers can refer to the original works for more details 3 7 1 basic gru forecasting model bgru bgru model composed of the gru block of the cdgru model proposed in section 3 4 2 the input of the bgru model is a list of 96 items v t 1 v t 2 v t 96 which represent water demand in the last 96 duration t 1 t 2 t 96 where t is the index of the duration for which the water demand is forecasted it takes the shape of 96 1 the hidden layer is a gru layer with 32 units the output layer is one gru unit the structure of this model is shown in fig 2 adam training algorithm is used for training this model with a learning rate of 0 001 for both dmas batch size is 100 and the final epochs numbers are 19 and 23 for dma1 and dma2 respectively all other training parameters are similar to that of the dcgru model listed in table 4 prediction is made in the same way as when using the dcgru model however in scenario 2 the output of the current prediction step is included in the input of the next prediction step without any prior classification 3 7 2 grun forecasting model in this section we provide a brief description of the method and the grun model structure readers can refer to the original work in guo et al 2018 for more details in this work the authors extract three features out of these data based on the time recent time near time and distant time they select a small number of water demand measurements 5 values for each feature table 2 shows the selected demand values for each feature where t is the desired time point where water demand should be predicted the structure of the grun model includes three gru layers each layer acts upon one of the three features the outputs of the three layers are merged and used as input to seven dense layers that used to discover the influence of each feature on the output the researchers of the work changed the activation functions of the reset gate and update gate into relu function instead of sigmoid function for forecasting scenario 2 the forecasting accuracy decreases as the dependence on the predicted water demand values increases in order to avoid that the authors added a correction module to reduce the forecasting error this module consists of one dense layer with 96 units the parameters of the grun model and the correction model structure are listed in table 3 the grun model is trained using adam with the same data set described in section 3 1 and the same equipment all training parameters are listed in table 4 3 8 evaluation methodology each model is evaluated based on i forecasting accuracy ii the model space complexity and iii computational load including training and forecasting time i forecasting accuracy is evaluated for individual forecasting scenarios based on the mean absolute error mae and mean absolute percentage error mape the testing data are used for evaluation ii the model complexity is calculated based on the akaike information criterion aic aic is a reliable tool for selecting the best between several competing statistical models depends on the number of variables and output error of the models when applying the same data set the model with the lowest aic is deemed to be the best aic for a neural network model can be calculated by the following general equation seghouane 2011 7 aic n ln rss n 2 k where n represents the number of observations k is the number of model s variables which are usually the weights and the biases of a neural network model and rss represents the sum of square error of the model output when 1 n k 40 then aic needs a bias adjustment in this case the value of aic can be calculated by the following equation panchal et al 2010 8 aic n ln rss n 2 k 2 k k 1 n k 1 in this research aic is calculated using 8 based on the whole data where n 25000 observations the number of variables for each model is listed in table 6 iii training time is measured during training where training data and validation data are employed forecasting time is measured for each prediction scenario individually using the testing data in scenario 1 time is measured for one step prediction while in scenario 2 time is measured for 96 step prediction the measurement is repeated 1000 times and the average time is recorded 4 results 4 1 dcgru model structure fig 3 shows the distortion of water demand data in terms of the number of classes it is clear that 4 is the most suitable number of classes for both data sets of dma1 and dma2 based on what described in the methodology section 3 2 the resulted model consists of two blocks as shown in fig 4 i dense block contains an input layer with a shape of 96 5 1 and two hidden layers all layers are of type time distributed dense the input layer and the two hidden layers contain 50 10 and 1 units respectively the activation function for both the input and the first hidden layer is relu function while a linear function is used in the second hidden layer ii gru block contains two layers gru layer 1 contains 32 gru cells each cell has the sigmoid function for both of its inner activation functions and the tanhfunction for the output function gru layer 2 is the output layer of the dcgru model it contains only one gru cell with sigmoid and linear functions as its inner and output activation functions respectively fig 4 illustrates the structure of the dcgru model and table 5 lists the hyperparameters of this model after expanding the data the input of the model changes based on ρ it is found that ρ 1 enhances the prediction accuracy the most thus only one virtual value is added between every two consecutive actual values in prediction scenario 1 two consecutive values are predicted the first one is the virtual value and the second is the actual value for this prediction step in scenario 2 196 steps of prediction should be made every second value is considered the actual value of the corresponding period 4 2 prediction and comparison results figs 5 and 6 illustrates a comparison between bgru dcgru and the state of the art model grun in terms of forecasting accuracy in scenario 1 for dma1 and dma2 respectively while figs 7 and 8 illustrate similar information in scenario 2 table 6 lists the numerical results of the comparison the dcgru model proposed in this work achieves the best forecasting accuracy between the three mentioned models it achieves 1 26 and 1 31 of mape in scenario 1 for dma1 and dma2 respectively and 1 50 and 1 38 in scenario 2 for dma1 and dma2 respectively table 7 shows that our model achieves the best mape compared with the state of the art the mape rises to 2 49 and 3 58 when using the bgru model for dma1 and dma2 respectively in forecasting scenario 1 and to 5 11 and 7 03 in scenario 2 the grun model achieves a comparable accuracy in the state of the art works as illustrated in table 7 its mape reaches 2 06 and 2 46 in scenario 1 and 4 33 and 4 96 in scenario 2 the compression with the results of the previous works which listed in table 7 shows that comparable accuracy is achieved by recurrent neural network based methods mainly lstm and gru neural network the preprocessing step of the data makes a slight difference in their results guo et al who did not add any preprocessing step have achieved 2 06 of mape while du et al who used pca and dwt to process the data before the prediction have achieved 1 83 the number of parameters k of each model which represents the number of weights and biases depends on the structure of each model thus dcgru and edcgru have the same number of parameters which is 4187 while grun has 30403 parameters and bgru has 3366 parameters before expanding the data set the small number of parameters and the small error accomplished by the dcgru model enhances the aic for this model where it reaches 20633 and 29396 for dma1 and dma2 respectively compared with 172225 and 176019 for the grun model these numbers unveil the superiority of our model in terms of space complexity on the other hand training the dcgru model takes about 192 s and 234 s for dma1 and dma2 respectively training the grun model takes around 80 s and 105 s for dma1 and dma2 respectively which means that the grun model is better than our model in terms of time complexity after expanding the data set the forecasting accuracy becomes much better than before with the same number of parameters making it the best in terms of space complexity edcgru model has 9176 and 11873 of aic for dma1 and dma2 and achieves 0 98 and 0 99 of mape for dma1 and dma2 respectively in scenario 1 and 1 03 and 1 04 for dma1 and dma2 respectively in scenario 2 however it has the worst time complexity which basically caused by the large input size training the model needs 850 s and 1055 s for dma1 and dma2 respectively figs 9 and 10 show prediction results and error before and after expanding the data in scenario 1 for dma1 and dma2 respectively while figs 11 and 12 illustrate similar information in scenario 2 5 discussion in this work a new ann structure for stwdf is investigated aiming at minimising the number of the trainable parameters while maintaining a level of forecasting accuracy not less than that reported in the previous works the proposed dcgru model achieves this goal successfully as shown in the results the complexity of our model is reduced effectively compared to what achieved in the state of the art this is attributable to two factors i using gru cell instead of lstm cell as done in some works in literature van houdt et al 2020 mu et al 2020 the gru cell contains nine parameters as shown in eqs 4 6 while the lstm cell contains ten parameters zhang et al 2020 xie et al 2020 ii utilizing the classification to find the connection between water demand in different days devises a chance to handle this relationship via an ann unit with fewer parameters unlike the grun model where guo et al 2018 uses three gru layers to handle the same relationship adding too many parameters to the model structure although the bgru model has the minimum number of parameters as illustrated in table 6 it can not reach a sufficient prediction accuracy in either of our scenarios as a consequence aic of bgru is significantly larger than that of dcgru thanks again to the classification step which enhances the dcgru model accuracy by creating additional useful features so that our model achieves the best accuracy compared to the previous works with reasonable complexity as the comparison results in table 7 clarify furthermore the created relationship between data from the same class plays an influential role in diminishing the influence of involving the predicted values in prediction in scenario 2 thus no further modifications are required to solve the accumulative error problem on the contrary the grun model needs a correction model to solve this problem which adds many parameters and increase the complexity computational load is influenced by the structure of the model as well as the input size the larger the input size the heavier the load although the design of the bgru model is simple it can not converge quickly thus more training epochs are required to arrive at the best accuracy as shown in table 4 this explains why bgru has longer training time than dcgru despite it has fewer parameters and both have the same input size on the other hand grun has the fastest training process due to the tiny input size compared to the other three models where it has input size of 15 records compared with 96 for dcgru and bgru and 192 for edcgru models although the use of full day water demand measurements as an input for the dcgru model increases the computational time it guarantees good performance in scenario 2 the edcgru model has the lowest mae for both dmas in both scenarios which emphasizes that expanding the data impacts the overall accuracy by increasing the linearity between the sequential data the benefits of expanding the data are graphically illustrated in figs 9 and 12 however this expansion reflects negatively on the computational load because of increment in the input size thus both training and forecasting time dramatically rise the training time differs based on the dma because every dma has a different pattern within the data which may take different time to approximate in contrast training time does not change when forecasting scenario changes because each model is trained once and used for both scenarios on the other hand forecasting time depends only on the forecasting process making it change based on the forecasting scenario and remain the same for different dmas 6 significance of results the prediction models proposed in this research provide accurate forecasting of water demand where the mae of both cdgru and ecdgru does not exceed 1 m 3 15 min in scenario 1 and 1 2 m 3 15 min is scenario 2 such an accuracy guarantees accurate pumping which in turn secures a satisfactory service while reducing the risk of sabotaging the pipes by high pressure and reduce the cost of maintenance additionally water demand data are confidential data because they are rich with private information thus training a machine learning model on data sets of different areas requires following a training strategy that respects this privacy this strategy could be federated learning li et al 2020 federated learning is an approach where the model is trained on different data sets located in different databases without moving the data thus several transitions are required to complete the training to ease the transitions a small size model is preferred knowledge transformation is another practical approach that aims to reduce training time using a dl model already trained on similar data and retrain it or retrain part of it on the required date this approach also requires transferring the trained model with its parameters to the target data set this work result in a dl model that is at least six times smaller than what proposed in the literature for stwdf scientifically the method proposed to enhance the accuracy at the extreme points can be applied for any other time series problem to reduce the nonlinearity of the data in fact the majority of data series such as wind speed data pollution rate data and daily temperature suffer from high nonlinearity adding virtual data in between the actual data in the way proposed in this research raises the linearity between data which lower the prediction error however it still needs more investigation regarding the training time where increasing the data size results in higher training time to reach an acceptable accuracy 7 conclusion this research investigates a novel dl model for stwdf and proposes a novel strategy for mitigating the error at the extreme points the main goal of the new design is to minimize the place complexity of the model while keeping high accuracy levels in two forecasting scenarios one step and multi step forecasting scenarios in addition to reducing the accumulative error problem in multi step forecasting scenario to enhance the performance of the model the historical data of water demand is classified into four classes such a step creates more relationships between data to enhance the prediction accuracy and relieves the reliance on the sequential relationship and minimizes the accumulative error moreover this prior clustering process reforms the data shape such that a simple ann model can approximate additional characteristic of the data a comparison between the proposed model and the bgru model exposes the benefit of the classification step to lower the error while a comparison with state of the art shows that the proposed design guarantees a remarkable improvement where the complexity of the model is reduced 6 times of what achieved in the literature while preserving the best prediction accuracy 98 69 and 98 5 for scenario 1 and scenario 2 respectively on the other hand expanding the data set adding virtual values within the data to reduce the nonlinearity at extreme points is proved to be effective and reliable it is found that adding more virtual values within the data does not necessarily lead to better accuracy at extreme points this is due to the accumulative error that grows when relying on more predicted virtual values to forecast one actual value this work found that one virtual value can reduce the error at extreme points by 30 of its original value nevertheless the model presented in this work relies on water demand history making it vulnerable to abnormalities in water demand this problem will be considered in future work in addition to optimizing the model structure and the training parameters based on an evolutionary method credit authorship contribution statement tony salloom conceptualization formal analysis investigation software writing original draft okyay kaynak data curation methodology validation writing review editing supervision wei he funding acquisition writing review editing resources supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to acknowledge the helpful comments of prof shuming liu of tsinghua university china throughout our research and sharing their data set with us this work was supported by the national natural science foundation of china under grant 62073031 62061160371 and the fundamental research funds for the china central universities of ustb under grant frf tp 19 001c2 
4464,short term water demand forecasting stwdf is the foundation stone in the derivation of an optimal plan for controlling water supply systems deep learning dl approaches provide the most accurate solutions for this purpose however they suffer from complexity problem due to the massive number of parameters in addition to the high forecasting error at the extreme points in this work an effective method to alleviate the error at these points is proposed it is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them to our knowledge this is the first work that considers the problem related to the extreme points moreover the water demand forecasting model proposed in this work is a novel dl model with relatively low complexity the basic model uses the gated recurrent unit gru to handle the sequential relationship in the historical demand data while an unsupervised classification method k means is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters real data obtained from two different water plants in china are used to train and verify the model proposed the prediction results and the comparison with the state of the art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy furthermore it is found that extending the data set significantly reduces the error by about 30 however it increases the training time keywords deep learning neural networks gated recurrent unit unsupervised classification water demand forecasting extreme points 1 introduction water scarcity has become a threat to humankind in recent decades many efforts in all possible directions are being made to compensate for this growing problem northey et al 2016 gonzález zeas et al 2019 the major reliable strategies for that include water treatment zinatloo ajabshir et al 2020 water desalination and optimization of water management systems nanotechnology is the most powerful technology employed for water treatment where researchers have done impressive work zinatloo ajabshir et al 2020 zinatloo ajabshir et al 2017 moshtaghi et al 2016 on the other hand stwdf is the foundation stone of the optimization of water management systems therefore numerous researchers have directed their efforts towards this problem nowadays deep learning dl is the most dominant approach which provides the most promising solutions to a myriad of critical problems to mention a few examples zhou 2020 has proposed deep learning method for forecasting the water quality in the presence of missing data friedel et al 2020 have compared four machine learning methods to predict groundwater redox status in the agriculturally dominated regions of new zealand wang et al 2020 have used a deep belief network to forecast the depth of snow over in alaska flood prediction is another crucial problem that utilizes deep learning approaches an encoder decoder based on long short term memory lstm is proposed by kao et al 2020 for multi step ahead flood forecasting while flood susceptibility modeling using deep learning is investigated in many studies such as pham et al 2021 bui et al 2020 lstm is also used by ni et al 2020 for streamflow and rainfall prediction where two lstm based models are built one combined wavelet network with lstm and the other combined convolutional network with lstm to achieve better performance stwdf is one of these problems that benefit most from dl to develop effective methods however some challenges might impede the success of dl based solutions model complexity the accumulative error when forecasting multi steps and the significant prediction error at the extreme points are some of these challenges that still need investigation model complexity and the size of the model in particular become serious constraints when using federated learning approach and bring some extra challenges to knowledge transformation technologies model complexity includes two types time complexity and space complexity time complexity is brought on by the time required to train the model and by the data size required for training the place complexity problem usually comes to the surface when the model has a massive number of parameters which increases the model size as well as the training time the second challenge is the accumulative error problem which affects the multi step prediction of water demand when relying on the historical data of water demand for stwdf the predicted values are involved in predicting the following values thus the prediction error is compounded by the use of inaccurate values of the predicted water demand the third challenge is the significant error at the extreme points which occur as a normal reflection of the nonlinearity of daily water demand these points are recognized as periods where water demand is dramatically different from the average demand in the adjacent periods making it difficult for the model to approximate the demand at these points a few examples of extreme points are illustrated in figs 7 and 8 the error at these points is usually unacceptable and may lead to severe problems in the distribution system in literature complexity problem was not a critical issue when statistical methods such as auto regression integrated moving average arima method and the seasonal version of it sarima in addition to markov chain are used for stwdf pandey et al 2021 however their accuracy is not sufficiently satisfactory caiado 2010 has achieved 11 of mean square percentage error for one day prediction and 13 2 for 7 days ahead prediction by combining sarima with the generalized autoregressive conditional heteroscedasticity method for daily wdf arandia et al 2016 also have achieved 4 21 of mean absolute percentage error mape for 15 min prediction of water demand in dublin spain by involving some data assimilation technique to improve the performance of srima method however their proposal does not work well with hourly prediction where the best mape they have got is 38 12 brentan et al 2017 have built their prediction model based on svr and fourier methods for on line prediction of hourly water demand their model achieves mape of 3 41 for one step prediction with the expansion in the application of machine learning zhou et al 2020 he et al 2019 and artificial neural networks anns salloom et al 2020 yu et al 2020 he et al 2017 several studies have proved that machine learning methods outdo the stochastic and the probabilistic models for wdf gagliardi et al 2017 has proved the anns overcome markov chain based models in terms of forecasting accuracy herrera et al 2010 and bai et al 2015 have proved the efficiency of support vector regression method for hourly wdf guo et al 2018 has compared statistical methods and conventional anns with deep learning method and proved that the dl methods give more accurate results when predicting water demand for short horizon in fact most researchers focus on improving prediction accuracy without paying too much attention to model complexity furthermore a new trend that exacerbates this problem has started looming on the horizon recently where some researchers comprise many machine learning models in one system then they chose a different one for different prediction periods based on probabilistic methods ambrosio et al 2019 have used a combination of multilayer perceptron svm elm random forests and adaptive neural fuzzy inference systems for hourly wdf antunes et al 2018 have investigated combining svr ann k nearest neighbours and random forest regression for real time wdf this strategy is meant to use the most accurate model in the most suitable prediction period however it requires a massive amount of computations and memory to save the parameters of all models and system configuration setting the error at extreme points also contributes to the worsening of the prediction accuracy however it has not attracted researchers attention guo et al 2018 are the first to point out this problem in their work but they have not provided any solution the accumulative error problem may occur when predicting several steps ahead based on the historical data of water demand some researchers tried to solve this problem individually by building a new neural network model and train it to make the predicted values approach the real ones obviously this method increases the parameter of the system dramatically in fact the accumulative error problem can be mitigated by involving several factors besides the historical data papageorgiou et al 2015 kley holsteg and ziel 2020 so the impact of the predicted values in the input can be reduced efficiently many factors influence water demand level dias et al 2018 but only factors such as meteorological conditions and day type which have weekly or daily distinguishable changes have real impacts on stwdf romano and kapelan 2014 however managers still rely on the historical data of water demand for stwdf and ignore the other possible factors because of the difficulties of gathering data about them in real time particularly in the 15 min interval moreover some available information about some factors such as meteorological information are not sufficiently accurate for short time prediction rayner et al 2005 making them unreliable in this work we propose a novel dl model for stwdf it is built based on the gated recurrent unit gru and unsupervised classification method k means involving data classification as a prior step has two major benefits i it helps with creating new features to compensate for the leak of reliable features which reflects positively on the prediction accuracy and the accumulative error ii it creates a relationship between data of different days which an ann can be easily approximated with a small number of parameters which in turn enhances the space complexity of the models additionally we investigate using a novel technique to alleviate the nonlinearity at the extreme points and reduce the error it depends on inserting virtual data between the actual data so that the nonlinearity at these points is drastically declined the contribution of this paper can be summarized as follows a novel dl model that provides a high prediction accuracy for both one step 15 min and multi step 96 steps ahead prediction is proposed it is built based on gru neural network supported by an unsupervised classification step to enhance the accuracy a new technique for mitigating the prediction error at extreme points is proposed and investigated the accumulative error problem which occurs in multi step prediction is mitigated by means of classification step which establishes new features to rely on in the prediction a comparison with the state of the art is carried out to show the effectiveness of our proposed methods in terms of accuracy and model complexity the rest of this paper is organized as follows section 2 describe the equipment used to carry out this research section 3 explains the research methodology and the methods proposed in this work section 4 clarifies the results of this research including the structure of the prediction model and the evaluation results while section 5 provide an intensive discussion to illustrate the underlying cause of these results section 6 illustrates the significance of the results section 7 includes the conclusion and the planned future works 2 research equipment and tools the machine used to carry out this research including classification step training the dl models prediction process and verification and evaluation of the proposed method is an asus laptop with an intel core i7 processor four real cores with a speed of 2 6 ghz each the installed ram is 16 gb all models are built using python 3 6 programming language over anaconda platform keras library and tensorflow backend are used to build the dl models due to their availability and convenience so anyone can simply redesign the models in a short time in fact the proposed method for stwdf does not contain complicated steps that require very powerful hardware ten years of water demand data do not exceed 1 gb if saved in a csv file which means that small storage space is sufficient to store the required data all calculations can be achieved using a cpu with the specification mentioned above as the minimum requirement the time spent on prediction and training using the aforementioned specifications are listed in the results section thus the proposed model is achievable in the industrial field by the currently available model 3 methodology firstly the water demand is collected every 15 min from the water distributing system and the database is updated every 24 h one step prediction scenario and multi step prediction scenario are considered as described in section 3 2 k means method is applied to classify the data based on their numerical distance into mclasses as described in section 3 3 the number mis determined using elbow method then demand readings and the classes are organized in vectors v t c 1 c 2 c m each vector contains the demand reading v t in addition to the value of each class that related to the corresponding demand value c 1 c 2 c m where the value of the class c j is determined as in eq 2 the last 96 values of water demand are used to predict the demand in the following 15 min in scenario 1 prediction is made by passing 96 vectors to the dense block this block handles each vector individually and results in one value for each vector thus the output of the dense block contains 96 values next these 96 values are passed to the gru block the output of the gru block is the desired demand value in scenario 2 the prediction is achieved iteratively by repeating scenario 1 for 96 times the result of each iteration is used in the next iteration the prediction model consists of the dense block and the gru block as explained in section 3 4 the final structure of the prediction model is clarified in section 4 1 and shown in fig 4 the model is trained on the training data set as described in section 3 6 in order to solve the problem of the massive error at the extreme points the data set is extended by injecting virtual values between each two adjacent actual values such that the linearity is increased the proposed method is described in section 3 5 the accuracy of the proposed methods is evaluated based on the mean absolute error mae and the mean absolute percentage error mape the complexity of the model is assessed based on the akaike information criterion aic the complete evaluation methodology is described in section 3 8 3 1 water demand data the data used in this research are collected in 2016 from two different district metering areas dmas in changzhou city in china guo et al 2018 fig 1 shows the location of the two dmas on the map as two red spots dma1 is a residential area with about 13000 residences and a few commercial buildings while the second area dma2 is an industrial area with a population of about 8500 and 300 factories the data set contains 25000 measured demand values for each dma each day is divided into 96 duration each of 15 min and the total water demand is measured for the whole dma at the end of each duration the time recorded in the database is the end of each duration the database is updated every 24 h i e the actual values of water demand for each day are not available until the end of the day table 1 lists the statistical information of the data used in this research data set is divided into two sets training set and testing set the training set is used to train the models it contains 22500 records during training 15 of the training data is used for validation the testing set contains 2500 records used to test the accuracy of the models after training ends 3 2 prediction scenarios this work targets the problem of short term water demand forecasting based on the historical data of water demand each prediction step is done for one prediction period which is 15 min in length water demand values of the last 96 periods i e the demands of the previous 24 h are employed to achieve each prediction step two forecasting scenarios are considered i scenario 1 is a one step forecasting scenario where the actual values of the required historical data are assumed to be available when the prediction starts ii scenario 2 is a multi step forecasting scenario where 96 steps each one is of 15 min are forecasted iteratively in this scenario the actual values are available only for the first prediction step while the required data for the following steps contain recently predicted values in addition to the available previous actual values when they are available in fact this is the realistic scenario since the database considered in this research will be updated at the end of each day 3 3 data classification and feature building the essential step of building an ann model for any purpose is identifying the inputs to that model in this research the historical water demand data is the only available data for prediction we choose to use the demand values of the last 96 periods t 1 t 2 t 96 for stwdf for each prediction period t in order to compensate for the lack of features the available data is classified into mclasses then the created classes are used as new features since there is no clue about the possible features to simplify the method a simple version of the algorithm k means is applied to implement an unsupervised classification of the historical data of water demand based on the numerical distance between values the number mshould be identified carefully due to the fact that large mundermines the benefit of classification on the other hand small mincreases the within cluster square error also called distortion yang and sinaga 2019 elbow method is used widely in the literature and in this research to determine the optimal number of classes to be used according to this method the best value of mis the smallest number that guarantees a small distortion the distortion is calculated based on the following equation yang and sinaga 2019 1 sse i 1 n j 1 m w i j x i c j 2 2 where n is the total number of data samples and i is the sample index in the class j m is the total number of classes and jis the class s index w i j is calculated as follows 2 w i j 1 x i class j 0 otherwise the initial center of each class j is determined randomly as in the following formula 3 cj randn 1 m std mean where randn a m generates an array of a m items each of them is a random float sampled from the normal distribution std and mean are the standard deviations and the mean of each data set respectively they are involved in 3 in order to guarantee that the created centers represent the whole data 3 4 dl model design by classifying the data new relationships between them are created where data belongs to the same classes has a high level of similarity to get the benefits of classification the designed dl model should be able to approximate the relationship between each value and the classes additionally the model should be able to approximate the sequential relationship between water demands in this research 96 records of the historical data are used to achieve every prediction step considering these requirements the model comprises two blocks the dense block that works upon the relationship of the water demand value with the classes and the gru block that works upon the sequential relationship between water demand data thus the proposed model called data classification based neural network model or dcgru model in the rest of this paper 3 4 1 dense block the input of the model takes the form of a 96 m 1 matrix 96 is number of the rows where each row contains m 1 values which are one demand value and the corresponding values of the created classes then the input is passed into three fully connected layers of the type time distributed dense time distributed dense layer is a dense layer that accepts two dimensional input and applies the activation function to every row separately and the output size equals the number of the rows in the input thus the output of this part of the model is a vector of 96 values each of them implicitly represents the water demand value of one record and its relationship to the created classes when choosing the activation functions for layer 1 and layer 2 we consider getting a positive number at the output of each layer may accelerate the convergence while an amplifier function is needed in the output layer the number of units in each dense layer is determined using grid search taking into account that maintaining a small number of units in each dense layer is preferred in order to arrive at a simple neural network model 3 4 2 gru block the output of the dense block is a sequence of 96 values it is passed into the gru block which contains a hidden gru layer and one gru cell represents the output layer of the dcgru model the number of units in the hidden gru layer is chosen to what suggested in the previous research chung et al 2014 the input vector elements are passed to the gru layer consecutively one item at a time the output of the gru layer is calculated based on three activation functions two inner functions and one for the output the sigmoid function is employed in several works in the literature xu et al 2019 as an inner activation function for gru layers while the hyperbolic tangent tanh is used as an output activation function these three functions are applied to the current input and the output of the previous unit based on the following equations deng et al 2019 4 z t σ g w z x t u z h t 1 b z 5 r t σ g w r x t u r h t 1 b r 6 h t 1 z t h t 1 σ h w h x t u h r t h t 1 b h where x t is the input vector h t is the output vector z t is the update gate vector r t is the reset gate vector and w u and bare the trainable parameters matrices and vectors while σ g and σ h are the sigmoid and the hyperbolic tangent functions respectively the hyperparameters of the proposed model structure are listed in table 5 3 5 expanding the data set in order to alleviate the error at the extreme points we suggest expanding the data set in fact the problem shows up because of the high difference between the water demand value at the extreme points and that at the points around them as shown in fig 7 and 8 to solve this problem we try to reduce this difference by setting a number ρ of virtual water demand values between every two consecutive actual values the new values are inserted between every two consecutive actual values next the new values are classified based on the k means method in practice the major problem of the method presented is the determination of the suitable ρ to be inserted into the data set at first glance increasing the number of virtual values seems to be suitable for reducing the error at the extreme points however giving it more thought the gru layer remembers the sequential changes between input data not data itself and reflects that on the weights of the model thus increasing the length of the linear sequence between two actual demand values reflects negatively on the next step of forecasting which exacerbates the error moreover inserting many virtual values enlarges the input which in turn enlarges the training and forecasting time the value of ρ is determined experimentally taking into account the aforementioned thoughts the input of the model is also expanded to comprise the actual water measurements and the virtual water values for a full day i e the input size becomes 96 ρ 1 the structure of the prediction model does not change however we will use the abbreviation edcgru to refer to the dcgru model applied to the expanded data set in the rest of this article when predicting water demand for the next period in scenario 1 all virtual values should be predicted one by one and classified then included in the input data to achieve one prediction step in scenario 2 96 ρ 1 values of water demand should be predicted consecutively every ρ 1 predicted value is considered a forecasting result of the corresponding forecasting period 3 6 training all models are trained under the same circumstances using the same equipment and the same data the parameters of all models are initialized based on the xavier uniform initializer it initializes the weights with random numbers picked from a uniform distribution within an interval limited by values related to the number of input and output weights of the corresponding layer the designed model is a compound of two blocks dense block and gru block each block uses different activation functions besides each block acts upon different kinds of features therefore a different trainable parameter of the model requires a different learning strategy adam training algorithm is used to train all models in this research it trains every weight in the neural network using different learning rates according to its previous changes which match our learning strategy desired also adam algorithm distinguishes itself from the others because it is a quick trainer and memory conservative sun et al 2019 which is one goal of this research mini batch technique is used during the training in this technique the prediction error is calculated for a small batch of prediction steps and then the average error is backpropagated to adjust models weights and biases in order to guarantee that the training data are not delivered to the model in a meaningful order the training data is shuffled after every epoch although all models are trained similarly however different training parameters are required for a different model to result in good prediction accuracy table 4 lists the training parameters for every model 3 6 1 training parameters of the dcgru model for this model the learning rate is chosen to what is recommended in kingma and ba 2014 however other values around 0 001 were examined and found that 0 002 is the best learning rate for dma1 while 0 001 is the best learning rate for dma2 β 1 and β 2 are set to 0 9 and 0 999 respectively for both dmas in order to avoid overfitting the early stop technique is used with a tolerance of two consecutive epochs i e the training stops automatically when the prediction error of training data keep descending while the error of validation data starts ascending for two consecutive epochs the final number of epochs is 15 and 18 for dma1 and dma2 respectively the batch size is 100 for both dmas 3 6 2 training parameters of the edcgru model by expanding the input the initial values of the input layer changes thus different values for the training parameter are required the learning rate is scheduled to start at 0 002 and decreases by 50 per cent every 5 epochs for both dmas epochs number is determined using the early stop technique with a tolerance of 4 epochs the final epochs numbers are 17 and 21 for dma1 and dma2 respectively batch size is set to 100 3 7 comparison methodology to show the effectiveness of the proposed methods we compare our model with two models the first one is the basic forecasting model referred to as bgru model it is similar to the dcgru model but it does not involve the classification step through this comparison the benefits of the classification step to enhance the prediction accuracy is unveiled the second one represents the state of the art to show the superiority of the proposed model in terms of accuracy and space complexity the grun model proposed by guo et al 2018 is chosen to compare with for the following reasons authors present comprehensive research about the efficiency of the deep learning method and provide strong evidence about the superiority of deep learning over other possible methods thus comparing the model proposed in this work with the grun model is sufficient to prove that our model is better than those in the state of the art they also consider the two forecasting scenarios that are considered in this research moreover they use the same data sets we use for training validation and testing the structures of these two models are illustrated in the section 3 7 1 and section 3 7 2 additionally to increase the scientific value of this work a comparison with the results described in previous publications is listed in table 7 the publications that consider svm or neural network based methods are included due to their superiority in stwdf over other methods we compare this work with six works from the literature a brief description of these works is listed here du et al 2021 combine discrete wavelet transform dwt and principal component analysis pca with lstm for one step daily wdf antunes et al 2018 use svr ann k nearest neighbors and random forest regression for one step prediction of daily water demand then they choose the best result based on probabilistic methods brentan et al 2017 combine svm and fourier method for one step prediction of daily water demand mouatadid and adamawski 2017 use eml for one step prediction of daily water demand candelieri 2017 uses clustering to enhance svm method for one step prediction of hourly water demand tiwari et al 2016 use elm with wavelet ann for one step daily wdf readers can refer to the original works for more details 3 7 1 basic gru forecasting model bgru bgru model composed of the gru block of the cdgru model proposed in section 3 4 2 the input of the bgru model is a list of 96 items v t 1 v t 2 v t 96 which represent water demand in the last 96 duration t 1 t 2 t 96 where t is the index of the duration for which the water demand is forecasted it takes the shape of 96 1 the hidden layer is a gru layer with 32 units the output layer is one gru unit the structure of this model is shown in fig 2 adam training algorithm is used for training this model with a learning rate of 0 001 for both dmas batch size is 100 and the final epochs numbers are 19 and 23 for dma1 and dma2 respectively all other training parameters are similar to that of the dcgru model listed in table 4 prediction is made in the same way as when using the dcgru model however in scenario 2 the output of the current prediction step is included in the input of the next prediction step without any prior classification 3 7 2 grun forecasting model in this section we provide a brief description of the method and the grun model structure readers can refer to the original work in guo et al 2018 for more details in this work the authors extract three features out of these data based on the time recent time near time and distant time they select a small number of water demand measurements 5 values for each feature table 2 shows the selected demand values for each feature where t is the desired time point where water demand should be predicted the structure of the grun model includes three gru layers each layer acts upon one of the three features the outputs of the three layers are merged and used as input to seven dense layers that used to discover the influence of each feature on the output the researchers of the work changed the activation functions of the reset gate and update gate into relu function instead of sigmoid function for forecasting scenario 2 the forecasting accuracy decreases as the dependence on the predicted water demand values increases in order to avoid that the authors added a correction module to reduce the forecasting error this module consists of one dense layer with 96 units the parameters of the grun model and the correction model structure are listed in table 3 the grun model is trained using adam with the same data set described in section 3 1 and the same equipment all training parameters are listed in table 4 3 8 evaluation methodology each model is evaluated based on i forecasting accuracy ii the model space complexity and iii computational load including training and forecasting time i forecasting accuracy is evaluated for individual forecasting scenarios based on the mean absolute error mae and mean absolute percentage error mape the testing data are used for evaluation ii the model complexity is calculated based on the akaike information criterion aic aic is a reliable tool for selecting the best between several competing statistical models depends on the number of variables and output error of the models when applying the same data set the model with the lowest aic is deemed to be the best aic for a neural network model can be calculated by the following general equation seghouane 2011 7 aic n ln rss n 2 k where n represents the number of observations k is the number of model s variables which are usually the weights and the biases of a neural network model and rss represents the sum of square error of the model output when 1 n k 40 then aic needs a bias adjustment in this case the value of aic can be calculated by the following equation panchal et al 2010 8 aic n ln rss n 2 k 2 k k 1 n k 1 in this research aic is calculated using 8 based on the whole data where n 25000 observations the number of variables for each model is listed in table 6 iii training time is measured during training where training data and validation data are employed forecasting time is measured for each prediction scenario individually using the testing data in scenario 1 time is measured for one step prediction while in scenario 2 time is measured for 96 step prediction the measurement is repeated 1000 times and the average time is recorded 4 results 4 1 dcgru model structure fig 3 shows the distortion of water demand data in terms of the number of classes it is clear that 4 is the most suitable number of classes for both data sets of dma1 and dma2 based on what described in the methodology section 3 2 the resulted model consists of two blocks as shown in fig 4 i dense block contains an input layer with a shape of 96 5 1 and two hidden layers all layers are of type time distributed dense the input layer and the two hidden layers contain 50 10 and 1 units respectively the activation function for both the input and the first hidden layer is relu function while a linear function is used in the second hidden layer ii gru block contains two layers gru layer 1 contains 32 gru cells each cell has the sigmoid function for both of its inner activation functions and the tanhfunction for the output function gru layer 2 is the output layer of the dcgru model it contains only one gru cell with sigmoid and linear functions as its inner and output activation functions respectively fig 4 illustrates the structure of the dcgru model and table 5 lists the hyperparameters of this model after expanding the data the input of the model changes based on ρ it is found that ρ 1 enhances the prediction accuracy the most thus only one virtual value is added between every two consecutive actual values in prediction scenario 1 two consecutive values are predicted the first one is the virtual value and the second is the actual value for this prediction step in scenario 2 196 steps of prediction should be made every second value is considered the actual value of the corresponding period 4 2 prediction and comparison results figs 5 and 6 illustrates a comparison between bgru dcgru and the state of the art model grun in terms of forecasting accuracy in scenario 1 for dma1 and dma2 respectively while figs 7 and 8 illustrate similar information in scenario 2 table 6 lists the numerical results of the comparison the dcgru model proposed in this work achieves the best forecasting accuracy between the three mentioned models it achieves 1 26 and 1 31 of mape in scenario 1 for dma1 and dma2 respectively and 1 50 and 1 38 in scenario 2 for dma1 and dma2 respectively table 7 shows that our model achieves the best mape compared with the state of the art the mape rises to 2 49 and 3 58 when using the bgru model for dma1 and dma2 respectively in forecasting scenario 1 and to 5 11 and 7 03 in scenario 2 the grun model achieves a comparable accuracy in the state of the art works as illustrated in table 7 its mape reaches 2 06 and 2 46 in scenario 1 and 4 33 and 4 96 in scenario 2 the compression with the results of the previous works which listed in table 7 shows that comparable accuracy is achieved by recurrent neural network based methods mainly lstm and gru neural network the preprocessing step of the data makes a slight difference in their results guo et al who did not add any preprocessing step have achieved 2 06 of mape while du et al who used pca and dwt to process the data before the prediction have achieved 1 83 the number of parameters k of each model which represents the number of weights and biases depends on the structure of each model thus dcgru and edcgru have the same number of parameters which is 4187 while grun has 30403 parameters and bgru has 3366 parameters before expanding the data set the small number of parameters and the small error accomplished by the dcgru model enhances the aic for this model where it reaches 20633 and 29396 for dma1 and dma2 respectively compared with 172225 and 176019 for the grun model these numbers unveil the superiority of our model in terms of space complexity on the other hand training the dcgru model takes about 192 s and 234 s for dma1 and dma2 respectively training the grun model takes around 80 s and 105 s for dma1 and dma2 respectively which means that the grun model is better than our model in terms of time complexity after expanding the data set the forecasting accuracy becomes much better than before with the same number of parameters making it the best in terms of space complexity edcgru model has 9176 and 11873 of aic for dma1 and dma2 and achieves 0 98 and 0 99 of mape for dma1 and dma2 respectively in scenario 1 and 1 03 and 1 04 for dma1 and dma2 respectively in scenario 2 however it has the worst time complexity which basically caused by the large input size training the model needs 850 s and 1055 s for dma1 and dma2 respectively figs 9 and 10 show prediction results and error before and after expanding the data in scenario 1 for dma1 and dma2 respectively while figs 11 and 12 illustrate similar information in scenario 2 5 discussion in this work a new ann structure for stwdf is investigated aiming at minimising the number of the trainable parameters while maintaining a level of forecasting accuracy not less than that reported in the previous works the proposed dcgru model achieves this goal successfully as shown in the results the complexity of our model is reduced effectively compared to what achieved in the state of the art this is attributable to two factors i using gru cell instead of lstm cell as done in some works in literature van houdt et al 2020 mu et al 2020 the gru cell contains nine parameters as shown in eqs 4 6 while the lstm cell contains ten parameters zhang et al 2020 xie et al 2020 ii utilizing the classification to find the connection between water demand in different days devises a chance to handle this relationship via an ann unit with fewer parameters unlike the grun model where guo et al 2018 uses three gru layers to handle the same relationship adding too many parameters to the model structure although the bgru model has the minimum number of parameters as illustrated in table 6 it can not reach a sufficient prediction accuracy in either of our scenarios as a consequence aic of bgru is significantly larger than that of dcgru thanks again to the classification step which enhances the dcgru model accuracy by creating additional useful features so that our model achieves the best accuracy compared to the previous works with reasonable complexity as the comparison results in table 7 clarify furthermore the created relationship between data from the same class plays an influential role in diminishing the influence of involving the predicted values in prediction in scenario 2 thus no further modifications are required to solve the accumulative error problem on the contrary the grun model needs a correction model to solve this problem which adds many parameters and increase the complexity computational load is influenced by the structure of the model as well as the input size the larger the input size the heavier the load although the design of the bgru model is simple it can not converge quickly thus more training epochs are required to arrive at the best accuracy as shown in table 4 this explains why bgru has longer training time than dcgru despite it has fewer parameters and both have the same input size on the other hand grun has the fastest training process due to the tiny input size compared to the other three models where it has input size of 15 records compared with 96 for dcgru and bgru and 192 for edcgru models although the use of full day water demand measurements as an input for the dcgru model increases the computational time it guarantees good performance in scenario 2 the edcgru model has the lowest mae for both dmas in both scenarios which emphasizes that expanding the data impacts the overall accuracy by increasing the linearity between the sequential data the benefits of expanding the data are graphically illustrated in figs 9 and 12 however this expansion reflects negatively on the computational load because of increment in the input size thus both training and forecasting time dramatically rise the training time differs based on the dma because every dma has a different pattern within the data which may take different time to approximate in contrast training time does not change when forecasting scenario changes because each model is trained once and used for both scenarios on the other hand forecasting time depends only on the forecasting process making it change based on the forecasting scenario and remain the same for different dmas 6 significance of results the prediction models proposed in this research provide accurate forecasting of water demand where the mae of both cdgru and ecdgru does not exceed 1 m 3 15 min in scenario 1 and 1 2 m 3 15 min is scenario 2 such an accuracy guarantees accurate pumping which in turn secures a satisfactory service while reducing the risk of sabotaging the pipes by high pressure and reduce the cost of maintenance additionally water demand data are confidential data because they are rich with private information thus training a machine learning model on data sets of different areas requires following a training strategy that respects this privacy this strategy could be federated learning li et al 2020 federated learning is an approach where the model is trained on different data sets located in different databases without moving the data thus several transitions are required to complete the training to ease the transitions a small size model is preferred knowledge transformation is another practical approach that aims to reduce training time using a dl model already trained on similar data and retrain it or retrain part of it on the required date this approach also requires transferring the trained model with its parameters to the target data set this work result in a dl model that is at least six times smaller than what proposed in the literature for stwdf scientifically the method proposed to enhance the accuracy at the extreme points can be applied for any other time series problem to reduce the nonlinearity of the data in fact the majority of data series such as wind speed data pollution rate data and daily temperature suffer from high nonlinearity adding virtual data in between the actual data in the way proposed in this research raises the linearity between data which lower the prediction error however it still needs more investigation regarding the training time where increasing the data size results in higher training time to reach an acceptable accuracy 7 conclusion this research investigates a novel dl model for stwdf and proposes a novel strategy for mitigating the error at the extreme points the main goal of the new design is to minimize the place complexity of the model while keeping high accuracy levels in two forecasting scenarios one step and multi step forecasting scenarios in addition to reducing the accumulative error problem in multi step forecasting scenario to enhance the performance of the model the historical data of water demand is classified into four classes such a step creates more relationships between data to enhance the prediction accuracy and relieves the reliance on the sequential relationship and minimizes the accumulative error moreover this prior clustering process reforms the data shape such that a simple ann model can approximate additional characteristic of the data a comparison between the proposed model and the bgru model exposes the benefit of the classification step to lower the error while a comparison with state of the art shows that the proposed design guarantees a remarkable improvement where the complexity of the model is reduced 6 times of what achieved in the literature while preserving the best prediction accuracy 98 69 and 98 5 for scenario 1 and scenario 2 respectively on the other hand expanding the data set adding virtual values within the data to reduce the nonlinearity at extreme points is proved to be effective and reliable it is found that adding more virtual values within the data does not necessarily lead to better accuracy at extreme points this is due to the accumulative error that grows when relying on more predicted virtual values to forecast one actual value this work found that one virtual value can reduce the error at extreme points by 30 of its original value nevertheless the model presented in this work relies on water demand history making it vulnerable to abnormalities in water demand this problem will be considered in future work in addition to optimizing the model structure and the training parameters based on an evolutionary method credit authorship contribution statement tony salloom conceptualization formal analysis investigation software writing original draft okyay kaynak data curation methodology validation writing review editing supervision wei he funding acquisition writing review editing resources supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to acknowledge the helpful comments of prof shuming liu of tsinghua university china throughout our research and sharing their data set with us this work was supported by the national natural science foundation of china under grant 62073031 62061160371 and the fundamental research funds for the china central universities of ustb under grant frf tp 19 001c2 
