index,text
5280,by integrating disciplinary sub models coupled hydrological models allow the exchange of output and input fluxes among their sub models while such coupling of models can mirror the conceptual representation of the water cycle potential uncertainty propagation and aggregation across the sub models may limit their overall performance there are limited studies dealing with uncertainty in coupled hydrological models due to the high computational needs the absence of detailed data and the lack of efficient uncertainty propagation frameworks this study presents an effective uncertainty propagation framework using a combination of statistical techniques multi variable calibration and parallel computation the framework was tested using a synthetic mathematical coupled model and a real world coupled surface water prms and subsurface modflow model for the synthetic coupled model the framework has shown its effectiveness to reveal the interplay of input variables quantify the uncertainties within each sub model and track their propagation through the coupled modeling system for the prms modflow model the framework has demonstrated how uncertainty in input precipitation surface water and subsurface water sub models influences the different segments of a hydrograph the results also indicate that improved predictions of high flow require a better quantification of input uncertainty in contrast baseflow and recession flow uncertainties largely depend on the subsurface and surface water sub models respectively the presented framework in addition to providing relatively comprehensive uncertainty information on the integrated model outputs it helps to identify the potential sources of uncertainty that can be used for further model improvement and guide new data collection campaigns keywords uncertainty propagation coupled models hydrological model uncetainty hydrologic modeling integrated models abbreviations nsmc null space monte carlo ws winding stairs algorithm for uncertainty propagation modflow modular three dimensional groundwater flow model prms precipitation runoff modeling system gsflow the coupled groundwater and surface water model ua uncertainty analysis pest model independent parameter estimation and uncertainty analysis tool beopest the parallelized version of pest bmv bottom marginal variance tmv top marginal variance 1 introduction the recent advances in process understanding and computational capacity have led to the emergence of numerous coupled hydrologic models including coupled surface subsurface hydrology panday and huyakorn 2004 markstrom et al 2008 surface subsurface atmosphere maxwell et al 2011 tian et al 2012 davison et al 2015 and water management and hydrological models adam et al 2015 bellin et al 2016 malek et al 2017 compared to single discipline based models these coupled models have resulted in better process representations mimicking the natural processes couplings can be achieved in two ways i sequential one way coupling and ii tight two ways coupling sequential coupling entails process and flux being passing in one direction from one sub model to the other without feedback on the other hand tight coupling involves process and flux exchanges in both directions performance improvements due to tight coupling of models can be found in pellerin et al 2004 mölders and raabe 1997 sridhar et al 2018 zabel and mauser 2013 while an extended review of integrated models can be found in laniak et al 2013 and maxwell et al 2014 the efficacy of coupled models to improve process representation however still needs further research one of the challenges is the uncertainties that stem from each sub model input data and their interplay these uncertainties might amplify or compensate to each other as they propagate through the integrated coupled model impacting the overall model performance and the subsequent decision making in addition the theoretical bias variance trade off yeh and yoon 1981 concept suggests that as we build complex coupled models through the incorporation of more sub models the variance component of the overall uncertainty increases while the bias decreases prompting the development of coupled models as a balancing act this established concept of bias variance trade off calls for rigorous uncertainty quantification propagation and reduction in coupled models uncertainty analysis in complex coupled models is limited however studies based on computationally inexpensive coupled models had demonstrated the potential constraints and benefits of the analysis in a sequentially integrated rainfall runoff sewer system and water quality model willems 2008 illustrated one way forward propagation of uncertainty from input rainfall to the final water quality model using variance decomposition and monte carlo simulations the study found that the water quality model is the highest source of uncertainty that needs more data and model improvement similarly bates et al 2003 applied a bayesian approach to propagate uncertainty across models and to identify the highest source of uncertainty within a sequentially integrated risk assessment model magombeyi and taigbenu 2014 also used a monte carlo analysis to illustrate the value of uncertainty propagation by identifying the highest source of uncertainty in integrated hydrology agronomy and socioeconomic model for computationally expensive and complex models williams and maxwell 2011 showed how uncertainty stemming from subsurface conductivity field propagates to influence surface water fluxes and soil moisture estimates however their study tested a few realizations of the uncertain conductivity field to demonstrate the propagation of uncertainty than a full uncertainty analysis that considers the different sources of uncertainties the limited number of studies that deal with uncertainty analysis in coupled complex models particularly tightly coupled models demonstrate the need for the development of an effective and comprehensive uncertainty analysis framework the core challenge in meeting this need is the high computational demand of running a coupled model and the multiple model runs required to perform uncertainty analysis using formal e g bayesian statistics or informal e g monte carlo analysis methods the formal schemes usually rely on markov chain monte carlo mcmc methods for sampling vrugt and ter braak 2011 while the informal monte carlo techniques employ sampling methods such as the latin hypercube technique applying mcmc based schemes in coupled models can be computationally prohibitive since their sampling is sequential in contrast although monte carlo samplings can be parallelized to save computational time their application in high dimensional space may suffer in sampling the optimal parameter regions in this study we developed a model independent framework to investigate uncertainty propagation in coupled models the framework has three components i breaking down of a high dimensional and complex coupled model into its sub models to execute them concurrently sellar et al 1996 ii the application of null space monte carlo nsmc to calibrate and quantify sub model uncertainties and iii input and sub models uncertainty propagation using the winding stairs ws algorithm besides saving computational time the concurrent setup enables a straightforward application of multi variable calibration as suggested by gupta et al 2008 e g streamflow baseflow and groundwater head nsmc is an informal monte carlo based uncertainty analysis ua technique tonkin and doherty 2009 however it is different from a typical monte carlo based sampling as the nsmc uses singular value decomposition to ensure the generated samples calibrate the model while spanning the null space this makes nsmc effective not only in saving computational time but also in avoiding samples that are not constrained by the calibrated space this allows nsmc to focus its sampling around the optimal parameter region than randomly sampling nonoptimal regions ws is a variance based uncertainty propagation sensitivity analysis method jansen et al 1994 compared to other variance based uncertainty propagation schemes ws is proved to be computationally superior chan et al 2000 thus the computational efficiency and effectiveness of these methods allow the framework to be applicable in computationally expensive coupled models 2 methodology the presented framework fig 1 seeks to investigate the relative contributions and interplay of the input data and sub models uncertainties on the coupled output it relies on efficient statistical schemes parallel computation and multivariable calibration first a coupled model is reformulated from sequential to a concurrent setup to enable parallel execution of the individual sub models next using the result from the concurrent setup and input uncertainty analysis beopest a parallel version of pest doherty 2010 is used to estimate the optimal parameters and recover the associated parameter uncertainties of each sub model finally the uncertainty from each sub model along with the input uncertainty is used in the winding stairs algorithm to propagate the combined uncertainty and quantify the relative contributions of the different sources of uncertainty on the coupled model output e g streamflow 2 1 gsflow model although any coupled integrated model can be considered we used the usgs s coupled groundwater and surface water flow model gsflow markstrom et al 2008 as one of our test cases gsflow is a publicly available coupled hydrological model with a widespread user base and proven performance for various hydrologic modeling applications essaid and hill 2014 hassan et al 2014 gsflow simulates surface and subsurface processes through the coupling of the surface water model prms markstromet al 2015 and the subsurface process model modflow harbaugh 2005 the coupling is enabled through the unsaturated zone flow uzf package niswonger et al 2006 and stream routing sfr2 package niswonger and prudic 2005 of modflow and the soil zone sz package of prms the surface and subsurface processes are coupled through interacting fluxes in both ways surface to subsurface and subsurface to surface fig 2 the water balance equations in gsflow are formulated as follows 1 p e t q q g δ s which encompasses precipitation p evapotranspiration et streamflow q groundwater sink qg and change in storage δ s the total streamflow q consists of hortonian runoff saturation excess runoff interflow and baseflow components while the total evapotranspiration consists of evapotranspiration from soil unsaturated and saturated zones the first three components of the streamflow are simulated by prms while the baseflow is simulated by modlow the coupling fluxes between the two sub models are percolation from the soil zone of prms to modflow and exfiltration from modflow to prms the coupling in the soil zone of prms is expressed as follows 2 p e exf gw et s q i q d r p δ s s where p e is the precipitation in excess of surface loses and abstraction and exf gw is groundwater exfiltration that occurs when the water table level exceeds the soil zone base the two fluxes represent inputs to the soil zone the outputs in the zone include evapotranspiration from the soil zone ets interflow q i dunnian runoff q d and percolation to the unsaturated zone r p the final term δ s s represents the difference between input and output fluxes in the soil zone the water balance simulated in modflow has two parts i the unsaturated zone uzf package and ii the remaining packages of modflow for the saturated subsurface groundwater zone in the uzf the water balance is simulated as 3 r p r g et p δ s p where rg is the recharge passed to modflow through uzf after satisfying the demand of the unsaturated zone evapotranspiration e t p δ s p is the storage change in the zone the groundwater zone water balance is formulated as follows 4 r g et g exf gw q b δ s g where et g is the evapotranspiration from the groundwater zone q b is the baseflow and δ s g represents the change in storage in the groundwater zone the detailed description and the computational formulation of each flux is presented in markstrom et al 2008 2 2 concurrent integration of models in a typical integrated hydrologic model the output of one sub model becomes an input to other sub models sequentially or concurrently the existing setup of the gsflow model shown in fig 2 uses sequential coupling which is not efficient in executing the individual models for calibration and quantification of uncertainty as these require one of the models to complete its simulation before the other model to start its simulation in addition the gsflow set up lacks an input interface to accommodate exfiltration as an external input into the prms subcomponent we have reformulated the gsflow coupling as shown in fig 3 by adopting a concurrent coupling sellar et al 1996 du and chen 2002 which allows the two sub models to be executed simultaneously starting from an initial estimate of each coupling variable r p and exf following their executions the outputs from each sub model are exchanged this step is iterated until the evaluation by the optimizer confirms convergence of the coupling variables between consecutive iterations the concurrent setup allows model calibration and uncertainty analysis to be undertaken independently within each sub model while making sure that the coupling variables have converged by the optimizer the computational cost of running a high dimensional coupled model can be prohibitive for automatic calibration thus traditionally most coupled models are calibrated manually using expert assisted trial and error attempts that do not guarantee a reliable estimate of the parameters in contrast the concurrent approach which enables the sub models to read their input variables directly allows for the application of automatic calibration and uncertainty analysis techniques for each sub model as a result in the concurrent architecture 1 the computational time of the integrated model becomes equivalent to the time required to complete the computationally expensive sub model 2 each model can be calibrated separately and 3 multi variable calibration can be accommodated these make the concurrent architecture to be advantageous in quantifying and propagating uncertainty in integrated models reformulating a tightly coupled model into a concurrent set up can be achieved relatively easily if the sub models of a coupled model can be executed in a standalone mode the current version of the gsflow model does not allow modflow and prms models to be executed independently since the prms model cannot receive exfiltration as input hence in our case study reformulating the gsflow for concurrent runs was necessary to execute the modflow and prms models in parallel and achieve computational efficiency most of the recent developments in integrated modeling including the earth system models adopted a standalone model execution in their integration to allow flexibility in model development and executions 2 3 model calibration and uncertainty quantification the direct application of bayesian methods moges et al 2016 2018 vrugt and ter braak 2011 to parameter estimation and uncertainty analysis in coupled complex models are limited by their large number of model parameters potential correlations among the parameters and the high computational expense subsequently most bayesian based model calibration and uncertainty quantification are commonly applied for computationally less expensive single models with limited number of parameters in this study the least square based model calibration and null space monte carlo uncertainty analysis tonkin and doherty 2009 were applied null space monte carlo under pest doherty 2010 was used to perform model calibration and uncertainty analysis pest employs a nonlinear optimization algorithm that iteratively moves towards a parameter set that minimizes the objective function ϕ which is often the weighted sum of the difference between observed h and simulated x p values as shown below 5 ϕ h x p t w h x p where x is the coefficient or jacobian matrix representing a linearized model p is the parameter vector and w is a diagonal matrix containing the observation weights for linear models eq 5 can be solved in a single step if the problem has a unique solution in the case of nonlinear models the equation is solved iteratively using the following parameter upgrade vector doherty 2010 6 δ p x t w x 1 x t w h x p if the inversion involves a large number of parameters the term x t q x also known as the hessian matrix is not invertible due to the potential presence of correlation structures within x which is the case in most environmental models pest can be instructed to use singular value decomposition svd with a user specified truncation level to calibrate high dimensional models in a numerically stable and computationally efficient fashion the svd for the coefficient matrix χ w x is formulated as follows 7 χ u s v t where u is a matrix that is comprised of orthogonal unit vectors spanning the model output space while v is a matrix of unit orthogonal vectors spanning the model parameter space s is a diagonal matrix with singular values by specifying a partition on the singular values eq 7 can be rewritten as 8 χ u 1 s 1 v t 1 u 2 s 2 v t 2 where the matrix s 2 is composed of zero or close to zero singular values while s 1 has non zero singular values v 1 and u 1 are composed of the vectors corresponding to s 1 while v 2 and u 2 correspond to s 2 the orthogonal vectors in v 1 and v 2 together span the parameter space the solution to the calibration objective function in eq 5 becomes 9 p v 1 s 1 1 u t 1 w h where p is the estimate of the true parameter p given that the observed values h and the actual system are related as 10 w h χ p w ε subsequently eq 9 can be formulated as 11 p v 1 v 1 t p v 1 w s 1 1 u t 1 ε thus noting the relationship for the orthogonal matrix v i v v t v 1 v 1 t v 2 v 2 t the error incurred in the parameter estimation can be computed as 12 p p v 2 v 2 t p v 1 s 1 1 u t 1 ε from eq 12 the error in the parameter estimation consists of two components the first term is the result of the parameter simplification null space whereas the second term is the result of the measurement error associated with the calibration data the null space monte carlo approach employed by pest to perform calibration constrained monte carlo uncertainty analysis capitalizes on the relationship in eq 11 it retains the calibrated parameter and augments it by null space parameter fields to generate calibration constrained parameter realizations the procedure involves 1 generating a random parameter field p s with a covariance matrix of c p that is estimated from a prior parameter field based on expert knowledge 2 subtracting the calibrated parameter p from the generated random parameter p s 3 projecting the difference p s p into the null space by multiplying it with v 2 v 2 t and 4 adding the projected difference to the calibrated parameter to get a new calibration constrained parameter sets p n e w which have the same magnitudes as the optimal values of the parameters plus values from the null space that represent the potential errors in estimated parameters values 13 p n e w v 2 v 2 t p s p p since the optimal and null space parameter values are results of model calibration the above approach saves significant computational time required for exploring model uncertainty as it reduces the number of multiple model runs required to recover the parameter uncertainty the null space monte carlo nsmc is applicable only to ill posed problems where null spaces exist where the information content of the observation is insufficient to support parameter calibration for an overdetermined case where there are more observations or information content in the observations than model parameters calibration constrained monte carlo can be directly undertaken based on the post calibration parameter covariance matrix this covariance matrix decomposed through cholesky factorization is used to generate parameter fields that reveal the post calibration parameter space but for most hydrological models the information content of observed data is low compared to the calibrated parameters making the nsmc approach widely applicable 2 4 input uncertainty the importance of accounting input uncertainty particularly related to precipitation measurements in hydrological modeling is noted by kavetski et al 2006 ajami et al 2007 balin et al 2010 vrugt et al 2008 and mcmillan et al 2011 a constant multiplying factor or parameter is commonly used to account for the precipitation uncertainties during model calibration in this study we considered two scenarios with different levels of precipitation uncertainty following oudin et al 2006 to investigate the impact of precipitation input uncertainty on the integrated simulation of subsurface and surface water model in line with the multiplying factor approach oudin et al 2006 suggested the following equation to represent precipitation input errors 14 p j p e x p σ n 0 1 σ 2 2 where σ is a multiplying factor that determines the level of uncertainty oudin et al 2006 suggested σ to be between 0 and 0 5 for no and maximum uncertainty in precipitation respectively we have considered two values of σ 0 1 and 0 2 to investigate 1 propagation of input uncertainty 2 the interplay of input uncertainty with sub model s uncertainties and 3 the effect of an increase in the level of input uncertainty on streamflow simulation fig 4 shows two random samples drawn from the two levels of σ considered as shown in the figure and eq 14 the input uncertainty is heteroscedastic with the level of uncertainty increasing with the magnitude of the observed precipitation 2 5 winding stairs ws the ws algorithm is briefly described as follows while the detailed formulations of the algorithm can be found in jansen et al 1994 rossing et al 1994 and chan et al 2000 suppose f is a function of two random input variables u and v f can be represented as 15 f u v f o f u u f v v f uv u v where f o is the best estimate of f when both u and v are unknown f u u is the adjustments to f o when u is fixed and f v v is the adjustment to f o when v is fixed the final term f uv u v represents the remaining difference between f and adjusted f o at the core of ws is eq 15 the equation is sobol s decomposition of a model f into summands of increasing dimensionality sobol 2003 this approach is also referred to as high dimensional model representation and used for a variance based global sensitivity analysis although variance based techniques are widely used in model sensitivity and uncertainty analysis they have two limitations i their assumption that variance captures the full uncertainty and ii their high computational cost pianosi et al 2016 although variance alone might not capture the entire pdf of a response variable methods that go beyond quantifying variance incur high computational costs for which model simplification using different surrogates are usually needed dell oca et al 2017 compared to the standard implementation of sobol s approach the ws method has a significant computational advantage which reduces the computational cost by more than half chan et al 2000 this is achieved as ws uses a single model realization multiple time this makes the ws method a viable alternative for computationally expensive coupled models which are the focus of this study in addition ws leads to two metrics that allow evaluation of the role of sub model interactions in influencing uncertainty propagation following sobol 2003 the variance of f can be decomposed as follows 16 v a r f u v v a r f u u v a r f v v v a r f uv u v jansen et al 1994 defines two variance metrics following equation 16 the top marginal variance tmv and the bottom marginal variance bmv associated with each input factor random variable the top marginal variance of u is defined as the reduction in the total variance due to knowing u while v is unknown the gain of knowing u which is var f u u whereas the bottom marginal variance of u is the variance that remains when u is unknown the price of not knowing u i e var f u u v a r f uv u v which includes the interaction with v thus if there is no interaction between the random variables u and v the bmv and tmv of the variables become the same in order to calculate the top and bottom marginal variances jansen et al 1994 proposed a sampling algorithm known as winding stairs ws the following equation describes the formulation of the ws algorithm for an integrated model f which contains three sub models m i 17 q f m 1 m 2 m 3 where q is the integrated model output the ws algorithm is a variance based monte carlo approach for uncertainty propagation it differs from the regular monte carlo sampling in which the ws samples are drawn sequentially from the marginal distributions of the input variables which in our case are the parameters of surface and subsurface models and the precipitation equation 18 shows the sampling procedure of ws for demonstration 18 q 1 q 2 q 3 q 4 q 5 q 6 q 7 q 8 q 9 q 10 q 11 q 12 f m 11 m 21 m 31 f m 11 m 22 m 31 f m 11 m 22 m 32 f m 12 m 22 m 32 f m 12 m 23 m 32 f m 12 m 23 m 33 f m 13 m 23 m 33 f m 13 m 24 m 33 f m 13 m 24 m 34 f m 14 m 24 m 34 f m 14 m 25 m 34 f m 14 m 25 m 35 the suffix i in m i j indicates the sub model index while j represents the parameter sample draws from model m i parameter space as shown in the equation a total of 12 parameters are drawn from the three models with each sub model i sampled four times in a cyclic winding fashion for example as shown in the first row of the right hand side of eq 18 the ws sampling randomly starts from a certain set of model parameters for each sub model and the integrated model simulation results in q 1 then only the second model parameters are changed from m 21 to m 22 resulting in the model output of q 2 ending the first cycle the parameters from the third model are changed from m 31 to m 32 resulting in the model output of q 3 in order to proceed to the next cycle of the ws and generate q 4 the parameters of model 1 are sampled from m 11 to m 12 and the cycle continues repeating the above steps the sub models parameters within each column are assumed to be independent thus the total variance in the output v a r q is calculated from each column the bottom marginal variance of m 2 is calculated as 19 b m v m 2 1 2 v a r c o l 1 c o l 2 while the top marginal variance of m 2 is calculated as 20 t m v m 2 v a r q 0 5 v a r c o l 2 s h i f t c o l 1 where s h i f t c o l 1 represented column 1 shifted one step downward in this study ws jansen et al 1994 is used to quantify and propagate uncertainty based on sequentially drawn model parameters and input uncertainty sets the parameter sets were drawn from their distributions obtained from the null space monte carlo analysis for instance the models m1 m2 and m3 may represent the precipitation model given in eq 14 prms and modflow respectively for the prms we considered 13 parameters to be uncertain following the prms manual while for modflow we considered the conductivity field to be the main source of uncertainty although the ws parameter draws are sequential model executions based on the draws do not require to be sequential thus the approach is computationally less expensive since the drawn samples can be executed in parallel in addition the ws propagation scheme accounts for the interaction of sub models in influencing outputs of the coupled model these features set ws apart from the traditional monte carlo based uncertainty propagation schemes used in demirel et al 2013 quintero et al 2012 zappa et al 2011 making it attractive for uncertainty analysis and propagation of computationally demanding coupled models in general the ws analysis can be used in answering the following key questions how does uncertainty in sub models parameters and input variables propagate into the integrated model output which sub model or input variable controls which segment of a hydrograph consequently the answer to these questions can guide model improvement data collection and assess output reliability ws was applied following model calibration however ws or other sensitivity analysis techniques can be conducted before model calibration to help identify parameters relevance for model calibration and improve understanding of model parameters and their interactions song et al 2015 hutcheson and mcadams 2010 3 case studies by choosing two separate case studies we intend to examine how successfully the proposed framework is to quantify and propagate uncertainty in integrated models in the first case we tested the methodology on a synthetic coupled system consisting of a linear and nonlinear functions in the second case study the coupled hydrological model gsflow is considered for a small watershed in california these two numerical experiments provide a known reference to evaluate the accuracy and relevance of the framework in quantifying uncertainty propagation in coupled models in our design of the test cases we tried to emulate actual coupled hydrological models test case 1 represents a typical coupled model while test case 2 represents a specific surface subsurface model coupling in both cases both the observables and simulated variables derived observation are used to calibrate and infer model parameter posteriors conventionally only observable variables are used to calibrate a model while derived observation non observable variables can be used as model forcing in order to allow multi variable calibration speed up convergence and better constrain model calibration we used both observable and derived observation variables in our calibration for well defined inverse modeling problems with sufficient observable data for calibration it is not needed to use derived observation moreover since derived observation data are required to run the sub models in their standalone mode their independent estimates are commonly available for instance inputs to modflow such as percolation or recharge are often estimated based on precipitation soil type and land use data westenbroek et al 2018 westenbroek et al 2010 as such the calibration of the coupled models will benefit from the use of such data it is important to note that percolation refers to the input flux entering the unsaturated zone as in eq 2 while recharge refers to the input flux entering the saturated zone groundwater as in eq 3 in both test cases in emulating precipitation input and its uncertainty we perturbed precipitation inputs with a known noise level while similar perturbation is conducted to reflect the estimation uncertainty of percolation in test case 2 such design helps to evaluate and determine whether the proposed framework can capture the prescribed input noise propagation into the model output in an actual watershed modeling precipitation input uncertainties can be obtained from reanalysis data sets or be estimated independently using a bayesian scheme at a catchment level balin et al 2010 3 1 synthetic coupled model synthetic case studies provide a controlled platform to examine the effectiveness of the framework and establish a baseline evaluation under a simple test case this test case demands less computational time and allows detailed model scrutiny besides since the exact solution is known it will help us to evaluate the error incurred accurately as shown in fig 5 the test case has two sub models with the first sub model eq 21a being a nonlinear function of the parameters while the second eq 22a being a linear function of the parameters the coupling variables are represented in eqs 21b and 22b the actual values of the parameters are shown in table 1 100 data points are randomly generated for y and z using the parameters in table 1 random noise is added to both output variables y and z to represent observation errors in the data used for calibration the noise level is n 0 2 and n 0 3 5 for y and z respectively in order to deal with input errors the constant noise of n 0 2 is added on each input value x s in formulating test case 1 we chose a systematic design in that 1 the data length is short 2 the submodels are linear nonlinear with a tractable known derivative and 3 the noise levels assigned to the different uncertainty sources being equal and different the first criterion enables evaluation of the framework in a limited data scenario while the second condition allows evaluation of the results consistency with an analytical understanding of uncertainty quantification the last condition is used to evaluate the implication of different and equal noise levels for different sub models as such in this test case we focused on the implication of the different noise levels in sub model outputs linearity nonlinearity of the sub models and their derivative characteristics in uncertainty propagation within a limited data length 100 points availability for calibration we have performed additional experiments that are presented in the si document 21a y a x 1 exp b x 2 c u 21b v a x 1 c x 2 22a z d x 3 2 e x 1 f v 22b u dx 3 v 3 2 hydrological case study the second case study deals with the sagehen watershed which drains 27 km2 basin area in the northern california fig 6 the watershed has been used as an experimental catchment for the gsflow model development the model data and simulation results are adopted from the gsflow user manual the sagehen catchment is a closed basin precluding inflows other than precipitation the dominant hydrological processes are snowmelt in the spring and precipitation driven flow during the summer groundwater flows contribute to baseflow the annual mean precipitation in the catchment is 880 mm while the mean temperature is 5 9 c the watershed encompasses numerous springs fed from groundwater rademacher et al 2005 the input data to the model include observed precipitation temperature soil texture land use classes and soil depth which are all available from the model website markstrom et al 2008 the data used for calibration include simulated streamflow at the watershed outlet groundwater head observations close to the spring locations and the percolation from the entire catchment the percolation data is the standard user specified input to modflow s uzf package aggregated from cell by cell value to catchment level lumped sum similarly if the framework is to be applied to other coupled models input to sub models can be used as calibration data in order to account for measurement errors random noise was incorporated in all calibration datasets section 4 2 the streamflow and percolation data were used for the surface water model calibration while the groundwater heads and baseflow data were used for groundwater model calibration the ten spring locations shown in fig 6 were used to extract observed head measurements the use of the four calibration datasets direct runoff percolation baseflow and groundwater head ensures improved constraining of the calibration process multi variable calibration the sagehen catchment groundwater system is modeled by two vertical layers each with 81 by 73 finite difference cells the cell s dimension is 90 by 90 m while the vertical depths of the cells vary based on topography the surface water flow model prms is discretized using hrus hydrologic response units which results in 128 hrus based on soil texture land use and topography of the watershed the modflow s cells and prms s hrus interact based on their location cells falling within an hru inheriting the same flux while cells falling on the boundary taking a proportional flux equivalent to their areal proportion table 2 shows the 12 parameters considered for the calibration of the surface water hydrology in prms the parameter ranges shown in the table are adopted from markstrom et al 2008 the hydraulic conductivity used to characterize the subsurface flow is considered for calibration pilot points doherty 2003 are employed to calibrate the hydraulic conductivity thirty pilot points are considered with four conductivity zones fig 6 an exponential variogram is used to interpolate hydraulic conductivity from the pilot points to the conductivity field of the discretized catchment 4 results and analysis 4 1 result from the synthetic coupled model case 1 in this case study the convergence of the coupled model was monitored by checking the subsequent values of the coupling variables u and v after each iteration convergence was assumed when the maximum difference in the coupling variable is below 10e 4 in a consecutive iteration in each iteration sub model 1 was calibrated based on the output variable y and coupling variable v while sub model 2 is calibrated based on the output variable z for our initial estimate of the coupling variables u and v the coupled model has converged within 10 iteration steps i e the subsequent values of u and v in iteration 9 and 10 becomes less than 10e 4 table 3 compares the calibrated and the actual parameters and shows that the calibrated parameters are closely comparable to the actual parameters the biggest difference is for parameter d yet the actual parameter 0 23 is well within the parameter uncertainty range as shown in fig 8 fig 7 presents the actual and simulated values of the model outputs and the coupling variables after the calibration the figure confirms that the calibrated parameters have resulted in a close correspondence between the actual and the simulated values comparing the calibration performance between the two model output variables z and y the simulation error is higher in z than in y which can be attributed to the noise incurred z with n 0 3 5 and y with n 0 2 in the design of the case study subsequently it is also observed that the noise in q y z is high that is inherited from z as discussed earlier in general it is recommended to use svd and apply null space monte carlo to quantify the parameter uncertainty when the condition number the ratio between the smallest and largest eigen value of the model parameters matrix is greater than 10e 7 the calibration process for this test case resulted in a minimum condition number of 10e 4 for sub model 2 thus rather than null space monte carlo we used the post calibration covariance matrix directly to perform calibration constrained monte carlo and quantify the parameter uncertainty fig 8 shows the parameter uncertainties for each sub model bivariate plots can be referred to in fig s7 despite sub model 2 being nonlinear model while sub model 1 is a linear model in terms of their parameters sub model 2 has the highest parameter uncertainty which can be attributed to the higher measurement noise introduced in z than y fig 9 shows the uncertainty propagated to the total output variable q y z the highest contributions for both bottom marginal variance bmv and top marginal variance tmv are incurred by sub model 2 compared to sub model 1 for lower quantile 25 medium quantile 50 and higher quantile 75 of the coupled model output variable q the uncertainties in q are dominated by the uncertainty that resulted from input variables xs and sub model 2 the uncertainty propagated on q from the input variables is close to the uncertainty contributed from sub model 2 in the lower quantiles the figure also shows that the uncertainty contribution from the input variables gradually decrease as q increases whereas the uncertainty contribution from sub model 2 increases with q this can be attributed to the fixed level of uncertainty introduced in the input variables which makes the uncertainty level and contribution from input variables to be relatively higher when the magnitudes of input variables are small and gradually decrease with increased input values the difference between tmv and bmv is small for the input as well as for the two sub models this small difference indicates that the effect of interactions among the uncertainties of input and the two sub models in influencing the output is relatively low another observation from fig 9 is it shows that there is a higher uncertainty contribution from the input variables than sub model 1 though they have the same noise level n 0 2 this is expected considering the derivative of the output variable with respect to the input variables and parameters of sub model 1 the derivative with respect to the input variables is higher than the derivative with respect to sub model 1 parameters this further shows the effectiveness of the framework despite the nonlinearity of sub model 1 the uncertainty contribution of the linear model sub model 2 to the coupled model output is higher not only the uncertainty contribution is higher but also the accuracy of the calibrated parameter is low for sub model 2 the sources of this higher uncertainty in sub model 2 can be attributed to the noise in the calibration data z and the error propagated from sub model 1 through the coupling variable v however as fig 7 shows the uncertainty estimate in the coupling variable v is minimum therefore the uncertainty in q can mostly be attributed to the noise stemming from the calibration data z than from the propagation of input v moreover a separate test with reduced noise in the calibration variable z with n 0 1 resulted in better accuracy of the calibrated parameters of sub model 2 at the same time the uncertainty propagated from sub model 2 becomes lower in addition to showing how the different sources of uncertainty interplay the results from this synthetic case also highlighted the potential impact of observed data used for calibration and the need to use accurate and different observed data related to the model outputs for the model calibration we have conducted additional tests to further evaluate the impact of sample size and noise level 1 sample size increased from 100 to 500 and 2 noise level doubled for the sub models sub model 1 n 0 4 and sub model 2 n 0 7 respectively while keeping the input noise to be n 0 2 as figs s1 to s3 in the supporting document show an increase in sample size from 100 data points to 500 data points leads to reduced parameter uncertainty during calibration and limited uncertainty propagation to the overall output q y z in the second experiment doubling the noise level in y and z to n 0 4 and n 0 7 lead to high parameter uncertainty and increased uncertainty in the overall model output figs s4 and s5 similarly higher uncertainty propagation is shown to stem from z than y fig s6 these experiments demonstrated that the framework can capture what is expected from theory increase data length may reduce parameter uncertainty whereas increased noise may lead to high uncertainty furthermore an additional experiment is conducted to evaluate the implication of calibration performed based on only y and z observables and v y and z v being a derived observation estimate the results of using v y and z are already discussed figs 7 to 9 the result based on only the observables is similar to figs 7 to 9 however the convergence speed under the use of y and z is dependent on the initial estimates of u and v the coupling variables in contrast the convergence in the case of v y and z is independent of the initial estimates of u and v the use of v as an additional calibration variable is considered since inputs to coupled models sub models are mostly available as forcing to standalone sub models or can be estimated for instance to run modlfow as a standalone model recharge percolation forcing data can be estimated using methods found in healey and scanlon 2010 such percolation recharge estimates can be used as an additional calibration data v to have a better convergence furthermore their estimation variance can also be used to determine the weight of each estimated value in the null space monte carlo calibration moreover our experiment to evaluate a full analysis i e calibrating the entire coupled model at once with the same calibration variables as the concurrent one lead to a singular matrix indicating that the full analysis is an ill posed problem thus svd or tikhonov regularization techniques are required to derive a meaningful parameter estimation and uncertainty analysis this regularization need by the full analysis implies that breaking the coupled model into concurrent sub models is a beneficial approach the full analysis where the couple models are calibrated as a single model is ill posed because of the existence of parameter correlation the correlation stems from the last term in sub model 2 eq 22a where the term f v f a x 1 c x 2 this term leads to the correlation of parameters a c and f that results in a singular matrix this is emblematic of the potential equifinality problem in coupled models resulting from estimating the entire parameter at once however contrary to the full analysis in the concurrent set up where the individual models are calibrated in parallel these parameters are not correlated as v is a direct external input to sub model 2 and is not a function of sub model 1 parameters this setup may result in conditional convergences depending on the initial estimates of u and v or the use of v as part of the objective function in summary the above rigorous analysis demonstrated that the proposed framework effectively propagated the estimated parameter and input data uncertainties and identified the major source of uncertainty for the integrated model output q thus the analysis is informative and can guide further model improvements and data collection 4 2 result for the sagehen catchment the first step in this test case was the reformulation of the tightly coupled gsflow model into a concurrent setup the reformulation includes modifying the gsflow source codes data input and output architectures and the coupling temporal scale the reformulation of input and output data exchanges liberated the tightly coupled approach and render the surface and subsurface components to be standalone executed independently and access to the coupling variables externally the original gsflow s coupling of surface and subsurface fluxes is done at each time step while the modified gsflow s coupling is done after independent completions of the surface and subsurface models for the entire simulation period this difference is only a computational matter as the concurrent set up also evaluates the convergence of the coupling variables at each grid cell for every time step and ensuring the satisfaction of mass balance by each model fig 10 shows a comparison of the original gsflow model and the concurrent arrangement from fig 10 it is observed that there is a very good correspondence between the original and modified gsflow models the direct runoff water table level and percolation results from the concurrent set up with known or true model parameters values were used as the basis to generate the calibration data different levels of gaussian noise accounting for observation uncertainty were incorporated into these calibration data the level of uncertainty in the water table observations was assumed to be a normal distribution with a standard deviation of 1 and a mean of 0 in the case of direct runoff the uncertainty considered was a normal distribution with a mean of 0 and a standard deviation of 2 cfs and 5 cfs for flows less than and greater 10 cfs respectively the percolation data uncertainty was considered to be similar to that of the direct runoff as it is a product of prms n 0 2 and n 0 5 depending on the magnitude of percolation being less or greater than 10 direct runoff can be estimated using standard baseflow separation techniques the calibration of both prms and modflow were done using the svd approach since the condition number is greater than 10e 7 which makes their jacobian matrix to be singular pest provides a utility that can aid in determining the number of singular values and differentiating the solution space from the null space doherty and hunt 2009 for the prms out of the 12 parameters 8 parameters belong to the calibration solution space whereas for the modflow 20 out of the 30 parameters belong to the solution space fig 11 presents the calibration results from prms the direct runoff hydrograph and the percolation data the figure shows that there is a good correspondence between the observed generated using known parameters and the modeled data the nash sutcliffe coefficients for the calibration in both cases are close to 0 9 similarly fig 12 shows the calibration result relating the observed and simulated water table the figure indicates a good correspondence between the two following the calibration of both the surface and sub surface models we applied the calibration constrained null space monte carlo analysis to recover the parameter uncertainty of each sub model the procedure follows the formulation in equation 13 null space monte carlo fig 13 shows the parameter uncertainty result of prms which indicates narrower uncertainty bands compared to the parameter bounds recommended in the gsflow model literature markstrom et al 2008 out of these parameter uncertainties we sampled 5000 parameter sets for the ws analysis of the uncertainty propagation from prms to the streamflow predictions similarly 5000 calibration constrained conductivity fields are drawn from the estimated hydraulic conductive field to quantify the uncertainty contribution by modflow to the streamflow predictions fig 14 demonstrates 6 representative samples from the 5000 conductivity field realizations considered the figure shows that the conductivity field is variable with the main variability occurring along with the stream network a seasonal analysis of the streamflow result was considered to determine the relative influence of uncertainties from input precipitation prms and modflow sub models to this effect a flow duration curve was first generated to illustrate the level of uncertainty in the four seasons winter djf spring mam summer jja and fall son a common exceedance level was chosen for consistency across the seasons in computing bmv and tmv for each uncertainty contributor the exceedance levels identified to represent low flow medium flow and high flow in each season were 25 60 and 95 respectively the relative contribution of uncertainty to streamflow at each exceedance level is presented in figs 15 and 16 the percentage bar plots in the four seasons shown in figs 15 and 16 reveal that the influence of each uncertainty source varies across seasons as each season has different forcing data including temperature and precipitation the uncertainty contribution from input precipitation is the highest in spring and winter while the contribution from modflow is the highest during the summer figs 15 and 16 prms s contribution to the higher quantile of streamflow uncertainty is high during the fall season and while the second highest during winter and spring seasons the winter plot shows that the contribution from input precipitation increases with the magnitude of streamflow whereas the summer plot shows that its contribution decreases with an increase in streamflow magnitude comparing figs 15 and 16 with the different level of input uncertainty σ 0 1 and σ 0 2 it can be noted that an increase in input precipitation uncertainty leads to increased contribution from input uncertainty on the streamflow estimation uncertainty particularly on the higher quantiles of the streamflow this implies that model based peak flow decision making requires relatively accurate precipitation data measurements the propagation of uncertainty from precipitation data to the streamflow predictions happens without much smoothing buffering from either modflow or prms across the seasons as the patterns of the contribution from prms and modflow are similar in the two plots furthermore the figures also show that a very similar result in terms of both bmv and tmv as discussed above this is an indication of less interaction among the sub models and the input precipitation uncertainties in influencing streamflow estimates in order to further distinguish the relative contributions of the different sources of uncertainty in different segments of the hydrograph we had temporally refined the seasonal analysis into a daily time scale this temporal refinement allows to zoom into the time scale of hydrological processes such as peak flow generation e g hortonian and dunnian and recession flows that have a time scale lower than baseflow as a result understanding the flow dynamics at the daily scale is relevant for further data collection and model conceptual improvements the daily interplay of the different sources of uncertainty is shown in fig 17 illustrating the contribution of the different sources of uncertainty on the different segments of the hydrograph in the absence of precipitation e g summer days 270th to 350th the dominant sources of uncertainty in streamflow are the subsurface component whereas in recession periods e g days 140th to 170th the dominant contributor to uncertainty is prms however in periods of high precipitation the uncertainty in streamflow is dominated by input precipitation uncertainty on the other hand in periods where there is lower precipitation e g days 30th 80th day the uncertainty sources show a relatively close influence among the three sources furthermore comparing the two scenarios of input uncertainty with σ 0 1 and σ 0 2 the increase in input uncertainty has 1 further dominated the periods where input uncertainty was the highest contributor and 2 this dominance is mostly compensated by the relative decrease in uncertainty contribution from prms this interplay indicates that input uncertainty plays a critical role in the direct runoff hydrograph than the baseflow hydrograph therefore as suggested in the seasonal disaggregation scale peak flow based analysis and decisions require to account for input uncertainties the results from figs 15 17 are consistent with the model s conceptual perception that high flows are sensitive to input precipitation baseflow is controlled by the subsurface model and recession flows are mostly a function of the surface model the analysis however provided quantitative evidence to this perception which in return enables for a better quantification and communication of uncertainty as well as improve further model development and data collection for instance it was shown that accurate measurements of precipitation data particularly during higher magnitude storms are of importance for flood modelers while soil water characteristics and interflow parameterization of prms need further analysis for a better representation of recession flows 5 discussion a framework that includes i concurrent coupling of surface and groundwater models ii multi variable calibration and uncertainty quantification using nsmc and iii a computationally efficient uncertainty propagation scheme ws for tightly coupled models is presented among the three parts the multi variable calibration is widely used in hydrologic literature gupta et al 2008 however the concept of concurrent coupling and ws are relatively new in hydrology to deal with the uncertainty of coupled models thus these can be considered as the novel contribution of this study since the presented framework is modular different methods can be used in place of the existing elements of the framework for instance bayesian analysis formal ua method can be applied in place of the null space monte carlo analysis to calibrate and quantify sub model uncertainty in a comparative analysis keating et al 2010 indicated that both nsmc and bayesian ua performed well and arrive at an equivalent result here the null space monte carlo approach informal ua method is preferred to the bayesian methods for its computational efficiency and capability to incorporate multi variable calibration that results in a reasonable estimate of predictive uncertainty there is however a tradeoff in gaining the computational efficiency at the expense of losing formality and uncertainty estimation accuracy that could be gained using a full bayesian scheme similarly different uncertainty propagation schemes sensitivity analysis methods can be employed in place of ws the uncertainty propagation method we used is inherently similar with sensitivity analysis methods uncertainty propagation uses a known uncertainty of a parameter or a model to quantify how it propagates to model outputs on the other hand sensitivity analysis uses similar methods without the of knowing model or parameter uncertainty to quantify how they influence model outputs we chose ws because of its computational efficiency and parallel implementation a comparative study by chan et al 2000 has shown that ws can reduce the computational cost by more than half compared to the sobol method that is widely used for sensitivity analysis the other computationally frugal sensitivity analysis method is a derivative based ua hill et al 2015 as indicated in the result section above the ws result is consistent with the derivative expectation however for further exploration and comparison depending on model problem complexity and data availability other alternative sensitivity analysis methods can be used in place of ws such alternative uncertainty propagation methods include i derivative based ii regional sensitivity analysis and iii multiple moment based techniques a detailed review of these methods can be found in pianosi et al 2016 we have used a coupling flux v in calibrating the coupled models this flux can include precipitation radiation and evaporation in the coupled atmospheric hydrologic model wagner et al 2016 larsen et al 2016 or it can be recharge percolation in coupled surface subsurface models e g gsflow observation or estimation of these variables v is required as a prerequisite to run the sub models independently for example to simulate groundwater flow recharge percolation data obtained from different estimation methods are typically used as forcing or input such an independently estimated and available dataset can be used to constrain further the calibration of the integrated groundwater and surface water models where recharge percolation has become a coupling flux the use of such estimated variables derived observations in model calibration as soft or proxy data besides direct observations is an established approach in hydrological modeling seibert and mcdonnell 2002 arnold et al 2015 jang et al 2018 zhang et al 2011 revilla romero et al 2015 in the hypothetical case study with known inputs and outputs the values of v are obtained from the known simple linear model v ax1 x2 while we used the percolation estimate of the known gsflow model in the second test case to distinguish between the estimated v and the other observed calibration variables we suggest the use of different weights that reflect the confidence uncertainty in the estimation of v however since the focus of this work is to develop a general framework of uncertainty quantification and propagation in integrated models future works can compare the performance of the framework under different types of the coupling flux v i e estimated observed and a mixture of estimated and observed cases this can be accomplished for example by applying the framework to the integrated models presented by wagner et al 2016 and larsen et al 2016 where all types of the coupling fluxes are available two major uncertainty sources are not dealt with in this study i model structural uncertainty and ii measurement errors observation uncertainty the significance of these sources of uncertainty is demonstrated in numerous contributions for instance rojas et al 2008 and butts et al 2004 have noted model structural uncertainty is a major source of uncertainty among input parameter and measurement observation uncertainties on the other hand mcmillan et al 2012 and coxon et al 2015 have demonstrated the implication of measurement uncertainty as such accounting these uncertainty sources in future studies is necessary furthermore precipitation input uncertainty can be explored rather than prescribing them as in test case 2 in this regard balin et al 2010 can be a guide on how to quantify precipitation uncertainty independently 6 conclusion the study presented a new modeling framework that quantifies uncertainty and its propagation within the integrated hydrological models the framework was tested using two case studies first a mathematical case study was setup to test and validate the framework before applying it to a computationally expensive coupled hydrological model the results from the mathematical case study demonstrated the ability of the framework to quantify uncertainty propagation and to reveal which sub model is the highest contributor to uncertainty under different scenarios in the second case study the framework was tested in the coupled model of prms and modflow we analyzed the interactions and interplay of uncertainties from the surface model subsurface model and input precipitation data on streamflow estimation at the sagehen catchment the analysis revealed which segment of the hydrograph uncertainty is controlled by which sub model or input uncertainty for instance during the winter and spring seasons more than 50 of the uncertainty across the low medium and high flows comes from input precipitation in contrast during the summer more than 80 of the uncertainty in low and medium flows is attributed to the sub surface model while more than 60 of the uncertainty in medium and high flows during the fall are propagated from the surface water model such results can be used to direct further enhancement of the integrated model performance credit authorship contribution statement edom moges methodology investigation yonas demissie methodology supervision hongyi li supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financially supported by the department of defense s strategic environmental research and development program serdp under contact w912hq15c0023 contributions by h li were supported by the office of science of the u s department of energy through the energy exascale earth system modeling e3sm project of earth system modeling program the data for test case 2 can be found in the gsflow model webpage https www usgs gov software coupled ground water and surface water flow model gsflow while the numerical design for test case 1 is explained in the text appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 125341 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5280,by integrating disciplinary sub models coupled hydrological models allow the exchange of output and input fluxes among their sub models while such coupling of models can mirror the conceptual representation of the water cycle potential uncertainty propagation and aggregation across the sub models may limit their overall performance there are limited studies dealing with uncertainty in coupled hydrological models due to the high computational needs the absence of detailed data and the lack of efficient uncertainty propagation frameworks this study presents an effective uncertainty propagation framework using a combination of statistical techniques multi variable calibration and parallel computation the framework was tested using a synthetic mathematical coupled model and a real world coupled surface water prms and subsurface modflow model for the synthetic coupled model the framework has shown its effectiveness to reveal the interplay of input variables quantify the uncertainties within each sub model and track their propagation through the coupled modeling system for the prms modflow model the framework has demonstrated how uncertainty in input precipitation surface water and subsurface water sub models influences the different segments of a hydrograph the results also indicate that improved predictions of high flow require a better quantification of input uncertainty in contrast baseflow and recession flow uncertainties largely depend on the subsurface and surface water sub models respectively the presented framework in addition to providing relatively comprehensive uncertainty information on the integrated model outputs it helps to identify the potential sources of uncertainty that can be used for further model improvement and guide new data collection campaigns keywords uncertainty propagation coupled models hydrological model uncetainty hydrologic modeling integrated models abbreviations nsmc null space monte carlo ws winding stairs algorithm for uncertainty propagation modflow modular three dimensional groundwater flow model prms precipitation runoff modeling system gsflow the coupled groundwater and surface water model ua uncertainty analysis pest model independent parameter estimation and uncertainty analysis tool beopest the parallelized version of pest bmv bottom marginal variance tmv top marginal variance 1 introduction the recent advances in process understanding and computational capacity have led to the emergence of numerous coupled hydrologic models including coupled surface subsurface hydrology panday and huyakorn 2004 markstrom et al 2008 surface subsurface atmosphere maxwell et al 2011 tian et al 2012 davison et al 2015 and water management and hydrological models adam et al 2015 bellin et al 2016 malek et al 2017 compared to single discipline based models these coupled models have resulted in better process representations mimicking the natural processes couplings can be achieved in two ways i sequential one way coupling and ii tight two ways coupling sequential coupling entails process and flux being passing in one direction from one sub model to the other without feedback on the other hand tight coupling involves process and flux exchanges in both directions performance improvements due to tight coupling of models can be found in pellerin et al 2004 mölders and raabe 1997 sridhar et al 2018 zabel and mauser 2013 while an extended review of integrated models can be found in laniak et al 2013 and maxwell et al 2014 the efficacy of coupled models to improve process representation however still needs further research one of the challenges is the uncertainties that stem from each sub model input data and their interplay these uncertainties might amplify or compensate to each other as they propagate through the integrated coupled model impacting the overall model performance and the subsequent decision making in addition the theoretical bias variance trade off yeh and yoon 1981 concept suggests that as we build complex coupled models through the incorporation of more sub models the variance component of the overall uncertainty increases while the bias decreases prompting the development of coupled models as a balancing act this established concept of bias variance trade off calls for rigorous uncertainty quantification propagation and reduction in coupled models uncertainty analysis in complex coupled models is limited however studies based on computationally inexpensive coupled models had demonstrated the potential constraints and benefits of the analysis in a sequentially integrated rainfall runoff sewer system and water quality model willems 2008 illustrated one way forward propagation of uncertainty from input rainfall to the final water quality model using variance decomposition and monte carlo simulations the study found that the water quality model is the highest source of uncertainty that needs more data and model improvement similarly bates et al 2003 applied a bayesian approach to propagate uncertainty across models and to identify the highest source of uncertainty within a sequentially integrated risk assessment model magombeyi and taigbenu 2014 also used a monte carlo analysis to illustrate the value of uncertainty propagation by identifying the highest source of uncertainty in integrated hydrology agronomy and socioeconomic model for computationally expensive and complex models williams and maxwell 2011 showed how uncertainty stemming from subsurface conductivity field propagates to influence surface water fluxes and soil moisture estimates however their study tested a few realizations of the uncertain conductivity field to demonstrate the propagation of uncertainty than a full uncertainty analysis that considers the different sources of uncertainties the limited number of studies that deal with uncertainty analysis in coupled complex models particularly tightly coupled models demonstrate the need for the development of an effective and comprehensive uncertainty analysis framework the core challenge in meeting this need is the high computational demand of running a coupled model and the multiple model runs required to perform uncertainty analysis using formal e g bayesian statistics or informal e g monte carlo analysis methods the formal schemes usually rely on markov chain monte carlo mcmc methods for sampling vrugt and ter braak 2011 while the informal monte carlo techniques employ sampling methods such as the latin hypercube technique applying mcmc based schemes in coupled models can be computationally prohibitive since their sampling is sequential in contrast although monte carlo samplings can be parallelized to save computational time their application in high dimensional space may suffer in sampling the optimal parameter regions in this study we developed a model independent framework to investigate uncertainty propagation in coupled models the framework has three components i breaking down of a high dimensional and complex coupled model into its sub models to execute them concurrently sellar et al 1996 ii the application of null space monte carlo nsmc to calibrate and quantify sub model uncertainties and iii input and sub models uncertainty propagation using the winding stairs ws algorithm besides saving computational time the concurrent setup enables a straightforward application of multi variable calibration as suggested by gupta et al 2008 e g streamflow baseflow and groundwater head nsmc is an informal monte carlo based uncertainty analysis ua technique tonkin and doherty 2009 however it is different from a typical monte carlo based sampling as the nsmc uses singular value decomposition to ensure the generated samples calibrate the model while spanning the null space this makes nsmc effective not only in saving computational time but also in avoiding samples that are not constrained by the calibrated space this allows nsmc to focus its sampling around the optimal parameter region than randomly sampling nonoptimal regions ws is a variance based uncertainty propagation sensitivity analysis method jansen et al 1994 compared to other variance based uncertainty propagation schemes ws is proved to be computationally superior chan et al 2000 thus the computational efficiency and effectiveness of these methods allow the framework to be applicable in computationally expensive coupled models 2 methodology the presented framework fig 1 seeks to investigate the relative contributions and interplay of the input data and sub models uncertainties on the coupled output it relies on efficient statistical schemes parallel computation and multivariable calibration first a coupled model is reformulated from sequential to a concurrent setup to enable parallel execution of the individual sub models next using the result from the concurrent setup and input uncertainty analysis beopest a parallel version of pest doherty 2010 is used to estimate the optimal parameters and recover the associated parameter uncertainties of each sub model finally the uncertainty from each sub model along with the input uncertainty is used in the winding stairs algorithm to propagate the combined uncertainty and quantify the relative contributions of the different sources of uncertainty on the coupled model output e g streamflow 2 1 gsflow model although any coupled integrated model can be considered we used the usgs s coupled groundwater and surface water flow model gsflow markstrom et al 2008 as one of our test cases gsflow is a publicly available coupled hydrological model with a widespread user base and proven performance for various hydrologic modeling applications essaid and hill 2014 hassan et al 2014 gsflow simulates surface and subsurface processes through the coupling of the surface water model prms markstromet al 2015 and the subsurface process model modflow harbaugh 2005 the coupling is enabled through the unsaturated zone flow uzf package niswonger et al 2006 and stream routing sfr2 package niswonger and prudic 2005 of modflow and the soil zone sz package of prms the surface and subsurface processes are coupled through interacting fluxes in both ways surface to subsurface and subsurface to surface fig 2 the water balance equations in gsflow are formulated as follows 1 p e t q q g δ s which encompasses precipitation p evapotranspiration et streamflow q groundwater sink qg and change in storage δ s the total streamflow q consists of hortonian runoff saturation excess runoff interflow and baseflow components while the total evapotranspiration consists of evapotranspiration from soil unsaturated and saturated zones the first three components of the streamflow are simulated by prms while the baseflow is simulated by modlow the coupling fluxes between the two sub models are percolation from the soil zone of prms to modflow and exfiltration from modflow to prms the coupling in the soil zone of prms is expressed as follows 2 p e exf gw et s q i q d r p δ s s where p e is the precipitation in excess of surface loses and abstraction and exf gw is groundwater exfiltration that occurs when the water table level exceeds the soil zone base the two fluxes represent inputs to the soil zone the outputs in the zone include evapotranspiration from the soil zone ets interflow q i dunnian runoff q d and percolation to the unsaturated zone r p the final term δ s s represents the difference between input and output fluxes in the soil zone the water balance simulated in modflow has two parts i the unsaturated zone uzf package and ii the remaining packages of modflow for the saturated subsurface groundwater zone in the uzf the water balance is simulated as 3 r p r g et p δ s p where rg is the recharge passed to modflow through uzf after satisfying the demand of the unsaturated zone evapotranspiration e t p δ s p is the storage change in the zone the groundwater zone water balance is formulated as follows 4 r g et g exf gw q b δ s g where et g is the evapotranspiration from the groundwater zone q b is the baseflow and δ s g represents the change in storage in the groundwater zone the detailed description and the computational formulation of each flux is presented in markstrom et al 2008 2 2 concurrent integration of models in a typical integrated hydrologic model the output of one sub model becomes an input to other sub models sequentially or concurrently the existing setup of the gsflow model shown in fig 2 uses sequential coupling which is not efficient in executing the individual models for calibration and quantification of uncertainty as these require one of the models to complete its simulation before the other model to start its simulation in addition the gsflow set up lacks an input interface to accommodate exfiltration as an external input into the prms subcomponent we have reformulated the gsflow coupling as shown in fig 3 by adopting a concurrent coupling sellar et al 1996 du and chen 2002 which allows the two sub models to be executed simultaneously starting from an initial estimate of each coupling variable r p and exf following their executions the outputs from each sub model are exchanged this step is iterated until the evaluation by the optimizer confirms convergence of the coupling variables between consecutive iterations the concurrent setup allows model calibration and uncertainty analysis to be undertaken independently within each sub model while making sure that the coupling variables have converged by the optimizer the computational cost of running a high dimensional coupled model can be prohibitive for automatic calibration thus traditionally most coupled models are calibrated manually using expert assisted trial and error attempts that do not guarantee a reliable estimate of the parameters in contrast the concurrent approach which enables the sub models to read their input variables directly allows for the application of automatic calibration and uncertainty analysis techniques for each sub model as a result in the concurrent architecture 1 the computational time of the integrated model becomes equivalent to the time required to complete the computationally expensive sub model 2 each model can be calibrated separately and 3 multi variable calibration can be accommodated these make the concurrent architecture to be advantageous in quantifying and propagating uncertainty in integrated models reformulating a tightly coupled model into a concurrent set up can be achieved relatively easily if the sub models of a coupled model can be executed in a standalone mode the current version of the gsflow model does not allow modflow and prms models to be executed independently since the prms model cannot receive exfiltration as input hence in our case study reformulating the gsflow for concurrent runs was necessary to execute the modflow and prms models in parallel and achieve computational efficiency most of the recent developments in integrated modeling including the earth system models adopted a standalone model execution in their integration to allow flexibility in model development and executions 2 3 model calibration and uncertainty quantification the direct application of bayesian methods moges et al 2016 2018 vrugt and ter braak 2011 to parameter estimation and uncertainty analysis in coupled complex models are limited by their large number of model parameters potential correlations among the parameters and the high computational expense subsequently most bayesian based model calibration and uncertainty quantification are commonly applied for computationally less expensive single models with limited number of parameters in this study the least square based model calibration and null space monte carlo uncertainty analysis tonkin and doherty 2009 were applied null space monte carlo under pest doherty 2010 was used to perform model calibration and uncertainty analysis pest employs a nonlinear optimization algorithm that iteratively moves towards a parameter set that minimizes the objective function ϕ which is often the weighted sum of the difference between observed h and simulated x p values as shown below 5 ϕ h x p t w h x p where x is the coefficient or jacobian matrix representing a linearized model p is the parameter vector and w is a diagonal matrix containing the observation weights for linear models eq 5 can be solved in a single step if the problem has a unique solution in the case of nonlinear models the equation is solved iteratively using the following parameter upgrade vector doherty 2010 6 δ p x t w x 1 x t w h x p if the inversion involves a large number of parameters the term x t q x also known as the hessian matrix is not invertible due to the potential presence of correlation structures within x which is the case in most environmental models pest can be instructed to use singular value decomposition svd with a user specified truncation level to calibrate high dimensional models in a numerically stable and computationally efficient fashion the svd for the coefficient matrix χ w x is formulated as follows 7 χ u s v t where u is a matrix that is comprised of orthogonal unit vectors spanning the model output space while v is a matrix of unit orthogonal vectors spanning the model parameter space s is a diagonal matrix with singular values by specifying a partition on the singular values eq 7 can be rewritten as 8 χ u 1 s 1 v t 1 u 2 s 2 v t 2 where the matrix s 2 is composed of zero or close to zero singular values while s 1 has non zero singular values v 1 and u 1 are composed of the vectors corresponding to s 1 while v 2 and u 2 correspond to s 2 the orthogonal vectors in v 1 and v 2 together span the parameter space the solution to the calibration objective function in eq 5 becomes 9 p v 1 s 1 1 u t 1 w h where p is the estimate of the true parameter p given that the observed values h and the actual system are related as 10 w h χ p w ε subsequently eq 9 can be formulated as 11 p v 1 v 1 t p v 1 w s 1 1 u t 1 ε thus noting the relationship for the orthogonal matrix v i v v t v 1 v 1 t v 2 v 2 t the error incurred in the parameter estimation can be computed as 12 p p v 2 v 2 t p v 1 s 1 1 u t 1 ε from eq 12 the error in the parameter estimation consists of two components the first term is the result of the parameter simplification null space whereas the second term is the result of the measurement error associated with the calibration data the null space monte carlo approach employed by pest to perform calibration constrained monte carlo uncertainty analysis capitalizes on the relationship in eq 11 it retains the calibrated parameter and augments it by null space parameter fields to generate calibration constrained parameter realizations the procedure involves 1 generating a random parameter field p s with a covariance matrix of c p that is estimated from a prior parameter field based on expert knowledge 2 subtracting the calibrated parameter p from the generated random parameter p s 3 projecting the difference p s p into the null space by multiplying it with v 2 v 2 t and 4 adding the projected difference to the calibrated parameter to get a new calibration constrained parameter sets p n e w which have the same magnitudes as the optimal values of the parameters plus values from the null space that represent the potential errors in estimated parameters values 13 p n e w v 2 v 2 t p s p p since the optimal and null space parameter values are results of model calibration the above approach saves significant computational time required for exploring model uncertainty as it reduces the number of multiple model runs required to recover the parameter uncertainty the null space monte carlo nsmc is applicable only to ill posed problems where null spaces exist where the information content of the observation is insufficient to support parameter calibration for an overdetermined case where there are more observations or information content in the observations than model parameters calibration constrained monte carlo can be directly undertaken based on the post calibration parameter covariance matrix this covariance matrix decomposed through cholesky factorization is used to generate parameter fields that reveal the post calibration parameter space but for most hydrological models the information content of observed data is low compared to the calibrated parameters making the nsmc approach widely applicable 2 4 input uncertainty the importance of accounting input uncertainty particularly related to precipitation measurements in hydrological modeling is noted by kavetski et al 2006 ajami et al 2007 balin et al 2010 vrugt et al 2008 and mcmillan et al 2011 a constant multiplying factor or parameter is commonly used to account for the precipitation uncertainties during model calibration in this study we considered two scenarios with different levels of precipitation uncertainty following oudin et al 2006 to investigate the impact of precipitation input uncertainty on the integrated simulation of subsurface and surface water model in line with the multiplying factor approach oudin et al 2006 suggested the following equation to represent precipitation input errors 14 p j p e x p σ n 0 1 σ 2 2 where σ is a multiplying factor that determines the level of uncertainty oudin et al 2006 suggested σ to be between 0 and 0 5 for no and maximum uncertainty in precipitation respectively we have considered two values of σ 0 1 and 0 2 to investigate 1 propagation of input uncertainty 2 the interplay of input uncertainty with sub model s uncertainties and 3 the effect of an increase in the level of input uncertainty on streamflow simulation fig 4 shows two random samples drawn from the two levels of σ considered as shown in the figure and eq 14 the input uncertainty is heteroscedastic with the level of uncertainty increasing with the magnitude of the observed precipitation 2 5 winding stairs ws the ws algorithm is briefly described as follows while the detailed formulations of the algorithm can be found in jansen et al 1994 rossing et al 1994 and chan et al 2000 suppose f is a function of two random input variables u and v f can be represented as 15 f u v f o f u u f v v f uv u v where f o is the best estimate of f when both u and v are unknown f u u is the adjustments to f o when u is fixed and f v v is the adjustment to f o when v is fixed the final term f uv u v represents the remaining difference between f and adjusted f o at the core of ws is eq 15 the equation is sobol s decomposition of a model f into summands of increasing dimensionality sobol 2003 this approach is also referred to as high dimensional model representation and used for a variance based global sensitivity analysis although variance based techniques are widely used in model sensitivity and uncertainty analysis they have two limitations i their assumption that variance captures the full uncertainty and ii their high computational cost pianosi et al 2016 although variance alone might not capture the entire pdf of a response variable methods that go beyond quantifying variance incur high computational costs for which model simplification using different surrogates are usually needed dell oca et al 2017 compared to the standard implementation of sobol s approach the ws method has a significant computational advantage which reduces the computational cost by more than half chan et al 2000 this is achieved as ws uses a single model realization multiple time this makes the ws method a viable alternative for computationally expensive coupled models which are the focus of this study in addition ws leads to two metrics that allow evaluation of the role of sub model interactions in influencing uncertainty propagation following sobol 2003 the variance of f can be decomposed as follows 16 v a r f u v v a r f u u v a r f v v v a r f uv u v jansen et al 1994 defines two variance metrics following equation 16 the top marginal variance tmv and the bottom marginal variance bmv associated with each input factor random variable the top marginal variance of u is defined as the reduction in the total variance due to knowing u while v is unknown the gain of knowing u which is var f u u whereas the bottom marginal variance of u is the variance that remains when u is unknown the price of not knowing u i e var f u u v a r f uv u v which includes the interaction with v thus if there is no interaction between the random variables u and v the bmv and tmv of the variables become the same in order to calculate the top and bottom marginal variances jansen et al 1994 proposed a sampling algorithm known as winding stairs ws the following equation describes the formulation of the ws algorithm for an integrated model f which contains three sub models m i 17 q f m 1 m 2 m 3 where q is the integrated model output the ws algorithm is a variance based monte carlo approach for uncertainty propagation it differs from the regular monte carlo sampling in which the ws samples are drawn sequentially from the marginal distributions of the input variables which in our case are the parameters of surface and subsurface models and the precipitation equation 18 shows the sampling procedure of ws for demonstration 18 q 1 q 2 q 3 q 4 q 5 q 6 q 7 q 8 q 9 q 10 q 11 q 12 f m 11 m 21 m 31 f m 11 m 22 m 31 f m 11 m 22 m 32 f m 12 m 22 m 32 f m 12 m 23 m 32 f m 12 m 23 m 33 f m 13 m 23 m 33 f m 13 m 24 m 33 f m 13 m 24 m 34 f m 14 m 24 m 34 f m 14 m 25 m 34 f m 14 m 25 m 35 the suffix i in m i j indicates the sub model index while j represents the parameter sample draws from model m i parameter space as shown in the equation a total of 12 parameters are drawn from the three models with each sub model i sampled four times in a cyclic winding fashion for example as shown in the first row of the right hand side of eq 18 the ws sampling randomly starts from a certain set of model parameters for each sub model and the integrated model simulation results in q 1 then only the second model parameters are changed from m 21 to m 22 resulting in the model output of q 2 ending the first cycle the parameters from the third model are changed from m 31 to m 32 resulting in the model output of q 3 in order to proceed to the next cycle of the ws and generate q 4 the parameters of model 1 are sampled from m 11 to m 12 and the cycle continues repeating the above steps the sub models parameters within each column are assumed to be independent thus the total variance in the output v a r q is calculated from each column the bottom marginal variance of m 2 is calculated as 19 b m v m 2 1 2 v a r c o l 1 c o l 2 while the top marginal variance of m 2 is calculated as 20 t m v m 2 v a r q 0 5 v a r c o l 2 s h i f t c o l 1 where s h i f t c o l 1 represented column 1 shifted one step downward in this study ws jansen et al 1994 is used to quantify and propagate uncertainty based on sequentially drawn model parameters and input uncertainty sets the parameter sets were drawn from their distributions obtained from the null space monte carlo analysis for instance the models m1 m2 and m3 may represent the precipitation model given in eq 14 prms and modflow respectively for the prms we considered 13 parameters to be uncertain following the prms manual while for modflow we considered the conductivity field to be the main source of uncertainty although the ws parameter draws are sequential model executions based on the draws do not require to be sequential thus the approach is computationally less expensive since the drawn samples can be executed in parallel in addition the ws propagation scheme accounts for the interaction of sub models in influencing outputs of the coupled model these features set ws apart from the traditional monte carlo based uncertainty propagation schemes used in demirel et al 2013 quintero et al 2012 zappa et al 2011 making it attractive for uncertainty analysis and propagation of computationally demanding coupled models in general the ws analysis can be used in answering the following key questions how does uncertainty in sub models parameters and input variables propagate into the integrated model output which sub model or input variable controls which segment of a hydrograph consequently the answer to these questions can guide model improvement data collection and assess output reliability ws was applied following model calibration however ws or other sensitivity analysis techniques can be conducted before model calibration to help identify parameters relevance for model calibration and improve understanding of model parameters and their interactions song et al 2015 hutcheson and mcadams 2010 3 case studies by choosing two separate case studies we intend to examine how successfully the proposed framework is to quantify and propagate uncertainty in integrated models in the first case we tested the methodology on a synthetic coupled system consisting of a linear and nonlinear functions in the second case study the coupled hydrological model gsflow is considered for a small watershed in california these two numerical experiments provide a known reference to evaluate the accuracy and relevance of the framework in quantifying uncertainty propagation in coupled models in our design of the test cases we tried to emulate actual coupled hydrological models test case 1 represents a typical coupled model while test case 2 represents a specific surface subsurface model coupling in both cases both the observables and simulated variables derived observation are used to calibrate and infer model parameter posteriors conventionally only observable variables are used to calibrate a model while derived observation non observable variables can be used as model forcing in order to allow multi variable calibration speed up convergence and better constrain model calibration we used both observable and derived observation variables in our calibration for well defined inverse modeling problems with sufficient observable data for calibration it is not needed to use derived observation moreover since derived observation data are required to run the sub models in their standalone mode their independent estimates are commonly available for instance inputs to modflow such as percolation or recharge are often estimated based on precipitation soil type and land use data westenbroek et al 2018 westenbroek et al 2010 as such the calibration of the coupled models will benefit from the use of such data it is important to note that percolation refers to the input flux entering the unsaturated zone as in eq 2 while recharge refers to the input flux entering the saturated zone groundwater as in eq 3 in both test cases in emulating precipitation input and its uncertainty we perturbed precipitation inputs with a known noise level while similar perturbation is conducted to reflect the estimation uncertainty of percolation in test case 2 such design helps to evaluate and determine whether the proposed framework can capture the prescribed input noise propagation into the model output in an actual watershed modeling precipitation input uncertainties can be obtained from reanalysis data sets or be estimated independently using a bayesian scheme at a catchment level balin et al 2010 3 1 synthetic coupled model synthetic case studies provide a controlled platform to examine the effectiveness of the framework and establish a baseline evaluation under a simple test case this test case demands less computational time and allows detailed model scrutiny besides since the exact solution is known it will help us to evaluate the error incurred accurately as shown in fig 5 the test case has two sub models with the first sub model eq 21a being a nonlinear function of the parameters while the second eq 22a being a linear function of the parameters the coupling variables are represented in eqs 21b and 22b the actual values of the parameters are shown in table 1 100 data points are randomly generated for y and z using the parameters in table 1 random noise is added to both output variables y and z to represent observation errors in the data used for calibration the noise level is n 0 2 and n 0 3 5 for y and z respectively in order to deal with input errors the constant noise of n 0 2 is added on each input value x s in formulating test case 1 we chose a systematic design in that 1 the data length is short 2 the submodels are linear nonlinear with a tractable known derivative and 3 the noise levels assigned to the different uncertainty sources being equal and different the first criterion enables evaluation of the framework in a limited data scenario while the second condition allows evaluation of the results consistency with an analytical understanding of uncertainty quantification the last condition is used to evaluate the implication of different and equal noise levels for different sub models as such in this test case we focused on the implication of the different noise levels in sub model outputs linearity nonlinearity of the sub models and their derivative characteristics in uncertainty propagation within a limited data length 100 points availability for calibration we have performed additional experiments that are presented in the si document 21a y a x 1 exp b x 2 c u 21b v a x 1 c x 2 22a z d x 3 2 e x 1 f v 22b u dx 3 v 3 2 hydrological case study the second case study deals with the sagehen watershed which drains 27 km2 basin area in the northern california fig 6 the watershed has been used as an experimental catchment for the gsflow model development the model data and simulation results are adopted from the gsflow user manual the sagehen catchment is a closed basin precluding inflows other than precipitation the dominant hydrological processes are snowmelt in the spring and precipitation driven flow during the summer groundwater flows contribute to baseflow the annual mean precipitation in the catchment is 880 mm while the mean temperature is 5 9 c the watershed encompasses numerous springs fed from groundwater rademacher et al 2005 the input data to the model include observed precipitation temperature soil texture land use classes and soil depth which are all available from the model website markstrom et al 2008 the data used for calibration include simulated streamflow at the watershed outlet groundwater head observations close to the spring locations and the percolation from the entire catchment the percolation data is the standard user specified input to modflow s uzf package aggregated from cell by cell value to catchment level lumped sum similarly if the framework is to be applied to other coupled models input to sub models can be used as calibration data in order to account for measurement errors random noise was incorporated in all calibration datasets section 4 2 the streamflow and percolation data were used for the surface water model calibration while the groundwater heads and baseflow data were used for groundwater model calibration the ten spring locations shown in fig 6 were used to extract observed head measurements the use of the four calibration datasets direct runoff percolation baseflow and groundwater head ensures improved constraining of the calibration process multi variable calibration the sagehen catchment groundwater system is modeled by two vertical layers each with 81 by 73 finite difference cells the cell s dimension is 90 by 90 m while the vertical depths of the cells vary based on topography the surface water flow model prms is discretized using hrus hydrologic response units which results in 128 hrus based on soil texture land use and topography of the watershed the modflow s cells and prms s hrus interact based on their location cells falling within an hru inheriting the same flux while cells falling on the boundary taking a proportional flux equivalent to their areal proportion table 2 shows the 12 parameters considered for the calibration of the surface water hydrology in prms the parameter ranges shown in the table are adopted from markstrom et al 2008 the hydraulic conductivity used to characterize the subsurface flow is considered for calibration pilot points doherty 2003 are employed to calibrate the hydraulic conductivity thirty pilot points are considered with four conductivity zones fig 6 an exponential variogram is used to interpolate hydraulic conductivity from the pilot points to the conductivity field of the discretized catchment 4 results and analysis 4 1 result from the synthetic coupled model case 1 in this case study the convergence of the coupled model was monitored by checking the subsequent values of the coupling variables u and v after each iteration convergence was assumed when the maximum difference in the coupling variable is below 10e 4 in a consecutive iteration in each iteration sub model 1 was calibrated based on the output variable y and coupling variable v while sub model 2 is calibrated based on the output variable z for our initial estimate of the coupling variables u and v the coupled model has converged within 10 iteration steps i e the subsequent values of u and v in iteration 9 and 10 becomes less than 10e 4 table 3 compares the calibrated and the actual parameters and shows that the calibrated parameters are closely comparable to the actual parameters the biggest difference is for parameter d yet the actual parameter 0 23 is well within the parameter uncertainty range as shown in fig 8 fig 7 presents the actual and simulated values of the model outputs and the coupling variables after the calibration the figure confirms that the calibrated parameters have resulted in a close correspondence between the actual and the simulated values comparing the calibration performance between the two model output variables z and y the simulation error is higher in z than in y which can be attributed to the noise incurred z with n 0 3 5 and y with n 0 2 in the design of the case study subsequently it is also observed that the noise in q y z is high that is inherited from z as discussed earlier in general it is recommended to use svd and apply null space monte carlo to quantify the parameter uncertainty when the condition number the ratio between the smallest and largest eigen value of the model parameters matrix is greater than 10e 7 the calibration process for this test case resulted in a minimum condition number of 10e 4 for sub model 2 thus rather than null space monte carlo we used the post calibration covariance matrix directly to perform calibration constrained monte carlo and quantify the parameter uncertainty fig 8 shows the parameter uncertainties for each sub model bivariate plots can be referred to in fig s7 despite sub model 2 being nonlinear model while sub model 1 is a linear model in terms of their parameters sub model 2 has the highest parameter uncertainty which can be attributed to the higher measurement noise introduced in z than y fig 9 shows the uncertainty propagated to the total output variable q y z the highest contributions for both bottom marginal variance bmv and top marginal variance tmv are incurred by sub model 2 compared to sub model 1 for lower quantile 25 medium quantile 50 and higher quantile 75 of the coupled model output variable q the uncertainties in q are dominated by the uncertainty that resulted from input variables xs and sub model 2 the uncertainty propagated on q from the input variables is close to the uncertainty contributed from sub model 2 in the lower quantiles the figure also shows that the uncertainty contribution from the input variables gradually decrease as q increases whereas the uncertainty contribution from sub model 2 increases with q this can be attributed to the fixed level of uncertainty introduced in the input variables which makes the uncertainty level and contribution from input variables to be relatively higher when the magnitudes of input variables are small and gradually decrease with increased input values the difference between tmv and bmv is small for the input as well as for the two sub models this small difference indicates that the effect of interactions among the uncertainties of input and the two sub models in influencing the output is relatively low another observation from fig 9 is it shows that there is a higher uncertainty contribution from the input variables than sub model 1 though they have the same noise level n 0 2 this is expected considering the derivative of the output variable with respect to the input variables and parameters of sub model 1 the derivative with respect to the input variables is higher than the derivative with respect to sub model 1 parameters this further shows the effectiveness of the framework despite the nonlinearity of sub model 1 the uncertainty contribution of the linear model sub model 2 to the coupled model output is higher not only the uncertainty contribution is higher but also the accuracy of the calibrated parameter is low for sub model 2 the sources of this higher uncertainty in sub model 2 can be attributed to the noise in the calibration data z and the error propagated from sub model 1 through the coupling variable v however as fig 7 shows the uncertainty estimate in the coupling variable v is minimum therefore the uncertainty in q can mostly be attributed to the noise stemming from the calibration data z than from the propagation of input v moreover a separate test with reduced noise in the calibration variable z with n 0 1 resulted in better accuracy of the calibrated parameters of sub model 2 at the same time the uncertainty propagated from sub model 2 becomes lower in addition to showing how the different sources of uncertainty interplay the results from this synthetic case also highlighted the potential impact of observed data used for calibration and the need to use accurate and different observed data related to the model outputs for the model calibration we have conducted additional tests to further evaluate the impact of sample size and noise level 1 sample size increased from 100 to 500 and 2 noise level doubled for the sub models sub model 1 n 0 4 and sub model 2 n 0 7 respectively while keeping the input noise to be n 0 2 as figs s1 to s3 in the supporting document show an increase in sample size from 100 data points to 500 data points leads to reduced parameter uncertainty during calibration and limited uncertainty propagation to the overall output q y z in the second experiment doubling the noise level in y and z to n 0 4 and n 0 7 lead to high parameter uncertainty and increased uncertainty in the overall model output figs s4 and s5 similarly higher uncertainty propagation is shown to stem from z than y fig s6 these experiments demonstrated that the framework can capture what is expected from theory increase data length may reduce parameter uncertainty whereas increased noise may lead to high uncertainty furthermore an additional experiment is conducted to evaluate the implication of calibration performed based on only y and z observables and v y and z v being a derived observation estimate the results of using v y and z are already discussed figs 7 to 9 the result based on only the observables is similar to figs 7 to 9 however the convergence speed under the use of y and z is dependent on the initial estimates of u and v the coupling variables in contrast the convergence in the case of v y and z is independent of the initial estimates of u and v the use of v as an additional calibration variable is considered since inputs to coupled models sub models are mostly available as forcing to standalone sub models or can be estimated for instance to run modlfow as a standalone model recharge percolation forcing data can be estimated using methods found in healey and scanlon 2010 such percolation recharge estimates can be used as an additional calibration data v to have a better convergence furthermore their estimation variance can also be used to determine the weight of each estimated value in the null space monte carlo calibration moreover our experiment to evaluate a full analysis i e calibrating the entire coupled model at once with the same calibration variables as the concurrent one lead to a singular matrix indicating that the full analysis is an ill posed problem thus svd or tikhonov regularization techniques are required to derive a meaningful parameter estimation and uncertainty analysis this regularization need by the full analysis implies that breaking the coupled model into concurrent sub models is a beneficial approach the full analysis where the couple models are calibrated as a single model is ill posed because of the existence of parameter correlation the correlation stems from the last term in sub model 2 eq 22a where the term f v f a x 1 c x 2 this term leads to the correlation of parameters a c and f that results in a singular matrix this is emblematic of the potential equifinality problem in coupled models resulting from estimating the entire parameter at once however contrary to the full analysis in the concurrent set up where the individual models are calibrated in parallel these parameters are not correlated as v is a direct external input to sub model 2 and is not a function of sub model 1 parameters this setup may result in conditional convergences depending on the initial estimates of u and v or the use of v as part of the objective function in summary the above rigorous analysis demonstrated that the proposed framework effectively propagated the estimated parameter and input data uncertainties and identified the major source of uncertainty for the integrated model output q thus the analysis is informative and can guide further model improvements and data collection 4 2 result for the sagehen catchment the first step in this test case was the reformulation of the tightly coupled gsflow model into a concurrent setup the reformulation includes modifying the gsflow source codes data input and output architectures and the coupling temporal scale the reformulation of input and output data exchanges liberated the tightly coupled approach and render the surface and subsurface components to be standalone executed independently and access to the coupling variables externally the original gsflow s coupling of surface and subsurface fluxes is done at each time step while the modified gsflow s coupling is done after independent completions of the surface and subsurface models for the entire simulation period this difference is only a computational matter as the concurrent set up also evaluates the convergence of the coupling variables at each grid cell for every time step and ensuring the satisfaction of mass balance by each model fig 10 shows a comparison of the original gsflow model and the concurrent arrangement from fig 10 it is observed that there is a very good correspondence between the original and modified gsflow models the direct runoff water table level and percolation results from the concurrent set up with known or true model parameters values were used as the basis to generate the calibration data different levels of gaussian noise accounting for observation uncertainty were incorporated into these calibration data the level of uncertainty in the water table observations was assumed to be a normal distribution with a standard deviation of 1 and a mean of 0 in the case of direct runoff the uncertainty considered was a normal distribution with a mean of 0 and a standard deviation of 2 cfs and 5 cfs for flows less than and greater 10 cfs respectively the percolation data uncertainty was considered to be similar to that of the direct runoff as it is a product of prms n 0 2 and n 0 5 depending on the magnitude of percolation being less or greater than 10 direct runoff can be estimated using standard baseflow separation techniques the calibration of both prms and modflow were done using the svd approach since the condition number is greater than 10e 7 which makes their jacobian matrix to be singular pest provides a utility that can aid in determining the number of singular values and differentiating the solution space from the null space doherty and hunt 2009 for the prms out of the 12 parameters 8 parameters belong to the calibration solution space whereas for the modflow 20 out of the 30 parameters belong to the solution space fig 11 presents the calibration results from prms the direct runoff hydrograph and the percolation data the figure shows that there is a good correspondence between the observed generated using known parameters and the modeled data the nash sutcliffe coefficients for the calibration in both cases are close to 0 9 similarly fig 12 shows the calibration result relating the observed and simulated water table the figure indicates a good correspondence between the two following the calibration of both the surface and sub surface models we applied the calibration constrained null space monte carlo analysis to recover the parameter uncertainty of each sub model the procedure follows the formulation in equation 13 null space monte carlo fig 13 shows the parameter uncertainty result of prms which indicates narrower uncertainty bands compared to the parameter bounds recommended in the gsflow model literature markstrom et al 2008 out of these parameter uncertainties we sampled 5000 parameter sets for the ws analysis of the uncertainty propagation from prms to the streamflow predictions similarly 5000 calibration constrained conductivity fields are drawn from the estimated hydraulic conductive field to quantify the uncertainty contribution by modflow to the streamflow predictions fig 14 demonstrates 6 representative samples from the 5000 conductivity field realizations considered the figure shows that the conductivity field is variable with the main variability occurring along with the stream network a seasonal analysis of the streamflow result was considered to determine the relative influence of uncertainties from input precipitation prms and modflow sub models to this effect a flow duration curve was first generated to illustrate the level of uncertainty in the four seasons winter djf spring mam summer jja and fall son a common exceedance level was chosen for consistency across the seasons in computing bmv and tmv for each uncertainty contributor the exceedance levels identified to represent low flow medium flow and high flow in each season were 25 60 and 95 respectively the relative contribution of uncertainty to streamflow at each exceedance level is presented in figs 15 and 16 the percentage bar plots in the four seasons shown in figs 15 and 16 reveal that the influence of each uncertainty source varies across seasons as each season has different forcing data including temperature and precipitation the uncertainty contribution from input precipitation is the highest in spring and winter while the contribution from modflow is the highest during the summer figs 15 and 16 prms s contribution to the higher quantile of streamflow uncertainty is high during the fall season and while the second highest during winter and spring seasons the winter plot shows that the contribution from input precipitation increases with the magnitude of streamflow whereas the summer plot shows that its contribution decreases with an increase in streamflow magnitude comparing figs 15 and 16 with the different level of input uncertainty σ 0 1 and σ 0 2 it can be noted that an increase in input precipitation uncertainty leads to increased contribution from input uncertainty on the streamflow estimation uncertainty particularly on the higher quantiles of the streamflow this implies that model based peak flow decision making requires relatively accurate precipitation data measurements the propagation of uncertainty from precipitation data to the streamflow predictions happens without much smoothing buffering from either modflow or prms across the seasons as the patterns of the contribution from prms and modflow are similar in the two plots furthermore the figures also show that a very similar result in terms of both bmv and tmv as discussed above this is an indication of less interaction among the sub models and the input precipitation uncertainties in influencing streamflow estimates in order to further distinguish the relative contributions of the different sources of uncertainty in different segments of the hydrograph we had temporally refined the seasonal analysis into a daily time scale this temporal refinement allows to zoom into the time scale of hydrological processes such as peak flow generation e g hortonian and dunnian and recession flows that have a time scale lower than baseflow as a result understanding the flow dynamics at the daily scale is relevant for further data collection and model conceptual improvements the daily interplay of the different sources of uncertainty is shown in fig 17 illustrating the contribution of the different sources of uncertainty on the different segments of the hydrograph in the absence of precipitation e g summer days 270th to 350th the dominant sources of uncertainty in streamflow are the subsurface component whereas in recession periods e g days 140th to 170th the dominant contributor to uncertainty is prms however in periods of high precipitation the uncertainty in streamflow is dominated by input precipitation uncertainty on the other hand in periods where there is lower precipitation e g days 30th 80th day the uncertainty sources show a relatively close influence among the three sources furthermore comparing the two scenarios of input uncertainty with σ 0 1 and σ 0 2 the increase in input uncertainty has 1 further dominated the periods where input uncertainty was the highest contributor and 2 this dominance is mostly compensated by the relative decrease in uncertainty contribution from prms this interplay indicates that input uncertainty plays a critical role in the direct runoff hydrograph than the baseflow hydrograph therefore as suggested in the seasonal disaggregation scale peak flow based analysis and decisions require to account for input uncertainties the results from figs 15 17 are consistent with the model s conceptual perception that high flows are sensitive to input precipitation baseflow is controlled by the subsurface model and recession flows are mostly a function of the surface model the analysis however provided quantitative evidence to this perception which in return enables for a better quantification and communication of uncertainty as well as improve further model development and data collection for instance it was shown that accurate measurements of precipitation data particularly during higher magnitude storms are of importance for flood modelers while soil water characteristics and interflow parameterization of prms need further analysis for a better representation of recession flows 5 discussion a framework that includes i concurrent coupling of surface and groundwater models ii multi variable calibration and uncertainty quantification using nsmc and iii a computationally efficient uncertainty propagation scheme ws for tightly coupled models is presented among the three parts the multi variable calibration is widely used in hydrologic literature gupta et al 2008 however the concept of concurrent coupling and ws are relatively new in hydrology to deal with the uncertainty of coupled models thus these can be considered as the novel contribution of this study since the presented framework is modular different methods can be used in place of the existing elements of the framework for instance bayesian analysis formal ua method can be applied in place of the null space monte carlo analysis to calibrate and quantify sub model uncertainty in a comparative analysis keating et al 2010 indicated that both nsmc and bayesian ua performed well and arrive at an equivalent result here the null space monte carlo approach informal ua method is preferred to the bayesian methods for its computational efficiency and capability to incorporate multi variable calibration that results in a reasonable estimate of predictive uncertainty there is however a tradeoff in gaining the computational efficiency at the expense of losing formality and uncertainty estimation accuracy that could be gained using a full bayesian scheme similarly different uncertainty propagation schemes sensitivity analysis methods can be employed in place of ws the uncertainty propagation method we used is inherently similar with sensitivity analysis methods uncertainty propagation uses a known uncertainty of a parameter or a model to quantify how it propagates to model outputs on the other hand sensitivity analysis uses similar methods without the of knowing model or parameter uncertainty to quantify how they influence model outputs we chose ws because of its computational efficiency and parallel implementation a comparative study by chan et al 2000 has shown that ws can reduce the computational cost by more than half compared to the sobol method that is widely used for sensitivity analysis the other computationally frugal sensitivity analysis method is a derivative based ua hill et al 2015 as indicated in the result section above the ws result is consistent with the derivative expectation however for further exploration and comparison depending on model problem complexity and data availability other alternative sensitivity analysis methods can be used in place of ws such alternative uncertainty propagation methods include i derivative based ii regional sensitivity analysis and iii multiple moment based techniques a detailed review of these methods can be found in pianosi et al 2016 we have used a coupling flux v in calibrating the coupled models this flux can include precipitation radiation and evaporation in the coupled atmospheric hydrologic model wagner et al 2016 larsen et al 2016 or it can be recharge percolation in coupled surface subsurface models e g gsflow observation or estimation of these variables v is required as a prerequisite to run the sub models independently for example to simulate groundwater flow recharge percolation data obtained from different estimation methods are typically used as forcing or input such an independently estimated and available dataset can be used to constrain further the calibration of the integrated groundwater and surface water models where recharge percolation has become a coupling flux the use of such estimated variables derived observations in model calibration as soft or proxy data besides direct observations is an established approach in hydrological modeling seibert and mcdonnell 2002 arnold et al 2015 jang et al 2018 zhang et al 2011 revilla romero et al 2015 in the hypothetical case study with known inputs and outputs the values of v are obtained from the known simple linear model v ax1 x2 while we used the percolation estimate of the known gsflow model in the second test case to distinguish between the estimated v and the other observed calibration variables we suggest the use of different weights that reflect the confidence uncertainty in the estimation of v however since the focus of this work is to develop a general framework of uncertainty quantification and propagation in integrated models future works can compare the performance of the framework under different types of the coupling flux v i e estimated observed and a mixture of estimated and observed cases this can be accomplished for example by applying the framework to the integrated models presented by wagner et al 2016 and larsen et al 2016 where all types of the coupling fluxes are available two major uncertainty sources are not dealt with in this study i model structural uncertainty and ii measurement errors observation uncertainty the significance of these sources of uncertainty is demonstrated in numerous contributions for instance rojas et al 2008 and butts et al 2004 have noted model structural uncertainty is a major source of uncertainty among input parameter and measurement observation uncertainties on the other hand mcmillan et al 2012 and coxon et al 2015 have demonstrated the implication of measurement uncertainty as such accounting these uncertainty sources in future studies is necessary furthermore precipitation input uncertainty can be explored rather than prescribing them as in test case 2 in this regard balin et al 2010 can be a guide on how to quantify precipitation uncertainty independently 6 conclusion the study presented a new modeling framework that quantifies uncertainty and its propagation within the integrated hydrological models the framework was tested using two case studies first a mathematical case study was setup to test and validate the framework before applying it to a computationally expensive coupled hydrological model the results from the mathematical case study demonstrated the ability of the framework to quantify uncertainty propagation and to reveal which sub model is the highest contributor to uncertainty under different scenarios in the second case study the framework was tested in the coupled model of prms and modflow we analyzed the interactions and interplay of uncertainties from the surface model subsurface model and input precipitation data on streamflow estimation at the sagehen catchment the analysis revealed which segment of the hydrograph uncertainty is controlled by which sub model or input uncertainty for instance during the winter and spring seasons more than 50 of the uncertainty across the low medium and high flows comes from input precipitation in contrast during the summer more than 80 of the uncertainty in low and medium flows is attributed to the sub surface model while more than 60 of the uncertainty in medium and high flows during the fall are propagated from the surface water model such results can be used to direct further enhancement of the integrated model performance credit authorship contribution statement edom moges methodology investigation yonas demissie methodology supervision hongyi li supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financially supported by the department of defense s strategic environmental research and development program serdp under contact w912hq15c0023 contributions by h li were supported by the office of science of the u s department of energy through the energy exascale earth system modeling e3sm project of earth system modeling program the data for test case 2 can be found in the gsflow model webpage https www usgs gov software coupled ground water and surface water flow model gsflow while the numerical design for test case 1 is explained in the text appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 125341 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5281,the uncertainty of initial water level being above the designed flood limited water level and the uncertainty of inflow caused by flood forecast error can cause flood risk in real time reservoir flood operation a risk analysis model for reservoir flood regulation under two dimensional uncertainties based on bayesian network is proposed the nodes of bayesian network are determined and the network structure is established with expert knowledge the parameter learning is conducted with the training samples obtained from monte carlo simulation thereafter through the prior probability inference without posterior information and the posterior probability inference with given posterior information the variation of flood risk is analyzed under singular factor uncertainty and two factors uncertainties the case study of xianghongdian reservoir using a flood of 100 years return period indicates the risk resulted from inflow uncertainty is greater than that of the uncertainty of initial water level there is a certain complementarity between the uncertainties of inflow and initial water level and the combination risk is between the results of two single factor risk levels moreover bayesian network is able to conduct bi directional inferences and infer the probability distribution of any other node which has practical value for risk assessment and control of reservoir flood control operation keywords reservoir floodwater utilization uncertainty assessment flood regulation risk bayesian network 1 introduction during flood seasons reservoir water level is controlled below the designed flood limited water level flwl to ensure adequate flood storage for regulating design flood or check flood before arrival of floods chen et al 2013 peng et al 2017 tan et al 2017 zhang et al 2018 however long term practice shows that the operation mode of strictly controlling reservoir water level below flwl may increase spilled water which affects water supplying guarantee rate in dry inflow span and reduces economic benefits yun and singh 2008 zhou et al 2014 mo et al 2018 to resolve the contradiction between reservoir flood control and water conservation in flood season and improve comprehensive benefits of reservoir research on dynamic control of flwl dc flwl got focus in the last few decades duren and beard 1972 draper and lund 2004 li et al 2010 liu et al 2011 zhou et al 2014 ding et al 2015 ding et al 2017 dc flwl of reservoir varies the flwl in flood season based on the real time flood rainfall reservoir status conditions and the forecast results under the premise of not lowering the flood control standard of the reservoir with the development of computer radar satellite and other advanced technologies and the improvement of numerical weather forecast dc flwl through the method of pre refill and pre release based on forecasts draper and lund 2004 ding et al 2015 hui et al 2016 li et al 2010 zhou et al 2018 has been systematically investigated at receding flood stages partial of flood water is retained pre refill within reservoir and results storage surplus to the reservoir flwl and the reservoir water level is reduced to the flwl or even below by pre release before next flood occurs yun and singh 2008 the excess storage of reservoir can be determined by the real time flood forecast information diao and wang 2011 zhang et al 2011 zhou et al 2018 it can also be determined synthetically according to weather forecast and flood forecast results zhou et al 2005 there are certain risks in floodwater utilization because of the unavoidable forecast error resulting in exceedance in the designed flwl due to a biased forecast thus causing flood risk to reservoir and downstream li et al 2010 developed a dynamic control operation model that considers inflow uncertainty consisting of pre release refill operation and risk analysis modules and the case study showed that the dc flwl can increase hydropower benefits without increasing flood control risk ding et al 2015 established a two stage reservoir operation model for dc flwl and derived optimal hedging rules according to forecast level and acceptable risk for real time reservoir operation in flood season in this paper the uncertainty of dc flwl considering both rainless day forecast error through weather forecast and real time flood forecast error through hydrological forecast is quantified with the probability distribution of the excess storage through joint probability distribution derivation and the bayesian network analysis method is adopted to study further on the flood risks of reservoir and its downstream under a specific operation mode with consideration of the uncertainty of initial water level coupling with the subsequent flood forecast error bayesian network is a kind of probability inferencing network combining probability theory and graph theory pearl 1988 it can decompose the joint distribution into several probability distributions with lower complexity by using the conditional independence relationship between variables so as to reduce the model complexity and improve the reasoning efficiency and is widely used in the research field of artificial intelligence at present lewis 1998 dhall et al 2020 kang and he 2011 proposed a risk analyzing model for water transfer project based on bayesian network which could be used for real time risk assessment and decision support chen et al 2019 applied the dynamic bayesian network to risk analysis for real time flood control operation of a multi reservoir system the framework of this paper is as follows section 2 elaborates the risk factors of reservoir flood control operation including the uncertainty of initial water level and the uncertainty of flood forecast section 3 establishes the bayesian network structure of reservoir flood regulation and obtains the network parameters based on training samples from mc simulation then conducts probability inference using the trained network and uses the reasoning results for risk calculation finally the model is applied to xianghongdian reservoir in huai river basin of china and the results are analyzed and discussed fig 1 shows the framework of the proposed model 2 risk sources of reservoir flood regulation assuming that reservoir implements dc flwl for floodwater utilization in the earlier recession stage there is uncertainty in the initial water level for the next flood besides there exist many uncertainties involved in the flood control operation of the reservoir chen et al 2015 xu et al 2019 such as the uncertainty in hydrological forecast of flood flow uncertainty in hydraulic conditions of discharge capacity and uncertainty in water level storage curve etc flood forecast error is considered as the primary risk factor in the study of reservoir flood control risk since the forecasted flood process is the basis of flood control decision making uncertainties in discharge capacity and water level storage curve are not considered in this paper fig 2 presents the schematic of the risk factors in flood regulating there is an error distribution in the initial water level reservoir storage and there is also an error distribution in the forecasted flood process forming two uncertain risk sources of initial water level and forecast error 2 1 uncertainty of initial regulating condition the method of pre refill and pre release is to store a certain amount of floodwater in flood control capacity based on rainfall forecast and flood forecast information the excess water storage is released through pre release for floodwater utilization in rainless periods and pre release for flood control in the early periods of flood there are errors in weather forecast and flood forecast inevitably we set d as the rainless day considering uncertainty q fl t as the reservoir inflow in early flood periods considering uncertainty the pre release water considering uncertainty can be then expressed as 1 w q m q in d δ t 1 t 1 τ α 0 q s q fl t δ t 2 where qm is the maximum water flow for utilization of the reservoir q in is the average recession flow in the forecasted rainless periods d α 0 represents the allowed maximum discount coefficient of downstream safety conveyance discharge α 0 1 qs is the safety discharge of the reservoir downstream δ t 1 is the time interval of rainless forecast δ t 2 is the time interval of flood forecast an improved truncated gaussian distribution appendix a is proposed to describe the error of forecasted rainless day d and the normal distribution is taken to describe the error of forecasted flood q fl t then the distribution densities of the excess storage corresponding to 1 7 forecasted rainless day are obtained through the related theory of mathematical statistics the specific derivation is shown in appendix b the density function is expressed as follows 2 f w w a 1 a 2 exp w μ 1 μ 2 2 2 σ 3 2 0 w 1 exp x σ 1 2 w μ 2 σ 2 2 μ 1 σ 3 2 2 2 σ 1 σ 2 σ 3 2 d x y 0 a 2 0 w 1 exp w x μ 2 2 2 σ 2 2 d x where a 1 μ 1 σ 1 y 0 are the distribution parameters of pre release water for floodwater utilization in rainless periods w 1 is the upper boundary of pre release water for floodwater utilization a 2 μ 2 σ 2 are the distribution parameters of pre release water for flood control in flood periods σ 3 σ 1 2 σ 2 2 thus the initial reservoir storage of the next flood considering uncertainty can be calculated with the following equation 3 v 0 v x w s w where vx denotes the corresponding storage of reservoir designed flwl ws is the upper boundary of the excess storage determined in dc flwl it can be known from eq 3 that v 0 is a monotonic function of w the distribution density of v 0 can be derived since the density function of w has been obtained degroot and schervish 2011 set v 0 g w then w g 1 v 0 v x w s v 0 and the distribution density of v 0 is 4 f v 0 v 0 f w g 1 v 0 d g 1 v 0 d v 0 f w v x w s v 0 from eq 4 the distribution of initial reservoir storage of the next flood can be derived under the premise of conducting floodwater utilization in the recession stage of the previous flood that is the uncertainty of initial condition caused by the excess storage 2 2 uncertainty of flood forecast errors exist in flood forecast because of uncertainties in observed hydrological and meteorological information model structure and model parameters which formulates the forecasted flood flow as continuous stochastic processes under the condition that the forecast is unbiased the forecasted process can be treated as the mean value of the stochastic processes and the forecasted flow during each time period in the process obeys normal distribution diao et al 2007 chen et al 2015 the reservoir inflow considering uncertainty q t can be expressed as eq 5 for the single reservoir flood control system containing the downstream flood control point the lateral inflow at downstream considering uncertainty iu t should be also taken into account expressed as eq 6 5 q t q t 1 ɛ q t 6 iu t i u t 1 ɛ iu t where q t iu t are the forecasted reservoir inflow and lateral inflow at time t respectively ɛ q t ɛ iu t are the relative errors of q t iu t ɛ q t and ɛ iu t follow normal distribution thus q t and iu t follow normal distribution as well the distribution parameters of ɛ q t ɛ iu t can be estimated according to the historical flood forecast results 3 risk analysis model for flood regulation based on bayesian network 3 1 basic principles of bayesian network bayesian network bn is a theory that uses directed graph to describe probability relation it can be used to deal with the uncertainties caused by the conditional correlation among different variables it consists of the following parts 1 model structure model structure properties are represented by a directed acyclic graph dag nodes in dag correspond to variables in model directed arcs represent the conditional dependency between variables 2 related parameters model parameters refer to the conditional probability table cpt specified for each variable cpt specifies the conditional probability for each instance of the variable risk analysis for reservoir flood control is in essence a problem to solve the joint probability distribution function pdf of multiple variables it is very complex to deal with uncertainty directly using the joint distribution of numerous variables bn can decompose the joint distribution into the products of several probability distributions with lower complexity using chain rule and the conditional independence relationship between variables expressing as 7 p x 1 x n i 1 n p x i π x i where p x 1 x n is the joint distribution of n variables π x i represents parent nodes of xi p x i π x i is the conditional probability distribution of xi given π x i when π x i p x i π x i is then the marginal distribution p xi 3 2 establishment and inference of the bayesian network the bayesian network model for flood risk analysis is built through the following three steps 1 structure learning which determines the network structure according to the conditional dependence between the stochastic variables as the relationship among the variables in the flood control operation of a reservoir is explicit this study establishes the graph structure by expert knowledge 2 parameter learning which gets the prior probability distributions of root nodes and the conditional probability distributions of non root nodes using a learning algorithm this study performs parameter learning through the maximum likelihood estimation mle method 3 probabilistic reasoning that is given the evidence of some nodes to infer the probability distributions of any other nodes it can perform bi directional inferences i e prediction infer from cause to effect and diagnosis infer from effect to cause 3 2 1 structure learning the basic principle of reservoir flood routing is the water balance equation expressed as eq 8 8 v t 1 v t q t q t 1 2 q t q t 1 2 δ t the flood flow at downstream flood control point at time t qc t is the sum of reservoir outflow q t and lateral inflow iu t as shown in eq 9 9 qc t i u t q t set the number of time intervals of reservoir flood routing as t the initial time is 0 then t 0 1 t in the above equations according to eq 8 and eq 9 the nodes in the network and the relationship between them can be determined since the uncertainty of initial reservoir storage is considered for convenience of expression the flow variables in bn are taken as average values in a time period variables are shown as follows v t 0 reservoir storage at the beginning of time period t v t 1 reservoir storage at the end of time period t h t 0 reservoir water level at the beginning of time period t h t 1 reservoir water level at the end of time period t q t q t 1 q t 2 average reservoir inflow in time period t q t q t 1 q t 2 average reservoir outflow in time period t i u t iu t 1 i u t 2 average lateral inflow in time period t q c t qc t 1 q c t 2 average flow at downstream control point in time period t the established structure of bn for reservoir flood control operation according to the logical relationship is shown as fig 3 3 2 2 parameter learning 1 generating the training samples mc method is used to provide data inputs for bn due to the high complexity of the density function of the initial reservoir storage we discretize it by numerical method and generate the random samples of v 0 v 1 0 according to the discretized probabilities the samples of reservoir inflow and lateral inflow are generated through the latin hypercube sampling lhs method huang et al 2018 then the optimal flood control model of a reservoir is established which is to minimize the maximum reservoir outflow with consideration of the constraints of water balance the upper and lower limits of water level the upper and lower limits of outflow discharge capacity and variation of outflows between adjacent times as the following equations 10 min max q t s t v t 1 v t 0 q t q t δ t h t h t h t q t q t q t q t q t max q t q t 1 δ q m substitute the generated samples into the above model we can get the corresponding samples of reservoir outflow reservoir storage and water level and the samples of flow at downstream control point can be obtained through eq 9 1 discretizing the variables it is generally assumed that all variables are discrete variables or continuous variables following normal distribution in parameter learning of bn monti and cooper 1998 however the risk factors are not all subjected to normal distribution and the influence of reservoir flood regulation on the distributions of variables should be considered thus we discretize the continuous variables identified in section 3 2 1 using the equal distance method the equal distance method is an unsupervised algorithm for discretization chen et al 2019 we specify the number of the discrete intervals as k and divide the range of the variable x into k intervals the width of each interval is equal to x max x min k the more the number of intervals the higher the discretization precision of the variable but at the same time the computing cost will increase the interval number k should be determined according to the number of training samples the value range of the variables and the precision of the required results sensitivity analysis can also be conducted through changing the value of k the k value can be sought that has the least impact on the uncertainty calculation under the premise of the allowable computation amount and time 2 parameter learning through mle method based on the discretized training samples the mle method which is suitable for complete data is adopted to perform parameter learning of bn as expressed in eq 11 11 θ ml arg max θ l x θ arg max θ ln p x θ where θ refers to the parameter of bn θ ml is the maximum likelihood estimation of the parameter θ l x θ is the likelihood function 3 2 3 probability inference inference is the process of answering query through calculation for a trained bn the prior probability inference can be made when the posterior knowledge is acquired the posterior probability inference can be performed in the posterior probability problem the known variable is usually called evidence variable denoted as e and its value is denoted as e the variable whose posterior probability is needed to be computed is called query variable denoted as m then the posterior distribution to be computed is p m e e the calculation principle is bayes theorem as shown in eq 12 12 p m e e p e e m p m p e e where p m is the prior probability of m p e e m is the likelihood probability according to the risk decision making needs of reservoir flood routing the prediction reasoning from cause to effect and diagnosis reasoning from effect to cause are conducted in this paper prediction can conduct risk assessment according to the risk factors that is to analyze the possible consequences of the reservoir and downstream and their probabilities under the influence of various risk factors diagnosis can infer the occurrence probabilities of the risk factors based on risk results for example given the value of the maximum water level of reservoir or the maximum flow at downstream control point it calculates the risk causes reversely and provides the possible values of the risk causes and the corresponding probabilities so as to analyze the uncertainty level of each risk factor under the results which provides valuable information for flood control risk decision making 1 prediction given the initial reservoir storage reservoir inflow and lateral inflow calculate the probability distributions of the reservoir water level and the river flow at downstream control point 2 diagnosis given the reservoir water level or the river flow at downstream control point calculate the probability distributions of the initial reservoir storage reservoir inflow and lateral inflow in this paper the joint tree propagation algorithm shafer and shenoy 1990 is used for the posterior probability inference of bn 3 3 risk calculation based on the probability distributions of the random variables inferred by bn the risk value is derived through calculating the cumulative probability of exceeding the threshold of the acceptable safety value set the reservoir safety water level as hs and the safety flow at downstream control point as qcs then the flood risk can be defined from the reservoir itself and the downstream control point expressed as follows 13 p r t p h t h s i 1 m h t p i h t i h s 14 p d t p q c t q c s i 1 m q c t p i q c t i q c s where pr t is the reservoir risk in time period t m h t is the number counted as the values of water level nodes exceeding hs p i h t i h s is the probability value of h t i h s pd t is the risk of downstream control point in time period t m q c t is the number counted as the values of downstream flow nodes exceeding qcs p i q c t i q c s is the probability value of q c t i q c s 4 case study 4 1 introduction to the study area the xianghongdian reservoir which locates in upstream of the west pi river huai river in china is a large reservoir used for flood control irrigation and electricity generation the reservoir basin covers an area of 1431 km2 and has a mean annual precipitation of 1465 mm the reservoir flwl is 125 m and the corresponding storage is 1227 15 106m3 the design flood water level is 140 98 m and the check flood water level is 143 37 m the downstream flood control point is jinzhai with a safety discharge of 2500 m3 s the huai river basin and the xianghongdian reservoir are shown in fig 4 wan et al 2019 and fig 5 presents the forecasted reservoir inflow and lateral inflow between reservoir and downstream control point of a flood the time interval is 1 h the flood magnitude is determined to be 100 years return period through frequency analysis 4 2 data preparation based on the forecasted rainless day 1 7 adopted for excess storage in the recession of the previous flood the level b flood forecast accuracy and the maximum discount coefficient of safety discharge α 0 0 8 the distribution density of v 0 can be computed according to eq 4 and the results are shown in fig 6 xianghongdian flood control system uses xin anjiang model for flood forecast and the forecast accuracy can reach level b it can be known that the probability of forecast relative error distributing within allowable error ɛ 0 is 70 85 according to the qualified rate of level b accuracy of 70 85 taking ɛ 0 20 we can then get the standard deviation of relative error σ 0 19 according to p ɛ ɛ 0 70 thus the forecast relative errors of reservoir inflow and lateral inflow obey n 0 0 192 based on mc simulation method 1000 random samples of v 0 corresponding to forecasted rainless day 1 7 and reservoir inflow qt lateral inflow iut are generated according to their probability distributions substituting them into the optimal flood control model of a reservoir 1000 samples of vt ht qt and qct can be then obtained thus the data input into bn are all acquired 4 3 results and discussion in the deterministic operation process without considering uncertainties which is calculated with the designed flwl as initial water level and the process of the forecasted inflow the maximum reservoir water level is 137 22 m and the maximum flow at downstream control point is 1709 m3 s three cases are taken to analyze the influence of uncertainties on the calculation results of flood regulation case 1 only considering the uncertainty of initial water level the training data are deterministic samples of qt iut forecasted mean value random samples of v 0 and random samples of qt vt t 0 ht qct obtained by flood regulation case 2 only considering the uncertainty of flood forecast the training data are deterministic samples of v 0 vx random samples of qt iut and random samples of qt vt t 0 ht qct obtained by flood regulation case 3 simultaneously considering the uncertainties of initial water level and flood forecast the training data are random samples of v 0 qt iut and random samples of qt vt t 0 ht qct obtained by flood regulation the bayesian network can get the probability distribution of each variable at each time due to limited space the paper only presents the results of the maximum reservoir water level and the maximum flow at downstream control point 4 3 1 prior probability inference for the trained bn the prior probability inference can be performed to obtain the possible values of the maximum reservoir water level and the maximum flow at downstream control point as well as their possibilities of occurrence the corresponding risk rates can be calculated based on eq 13 and eq 14 if mc simulation method is used the risk rate will be the ratio of times of exceeding the acceptable safety value to total simulation times the results of bn prior probability reasoning in case 1 are compared with those of mc method to verify the learning and reasoning effect of bn taking forecasted rainless day 7 as an example the comparison of the risk curves of reservoir maximum water level is shown in fig 7 the vertical line in the figure represents the original maximum water level 137 22 m in deterministic operation the results of bn reasoning are consistent with those of mc simulation which shows that bn reasoning is effective the risk of the maximum flow at downstream control point is 0 because the uncertainty of initial water level is small and the reservoir outflow samples obtained from the optimal operation model are the same therefore we only analyze the reservoir risk of case 1 in the rest of the paper next the prior probability inference and risk calculation are carried out for case 1 case 2 and case 3 respectively case 1 the training data of case 1 are input into bn to learn the corresponding network parameters the prior probability inference is performed and the risk values corresponding to forecasted rainless day 1 7 are calculated through eq 13 fig 8 shows the risks of the reservoir under different safety water level when hs is greater than the original maximum water level 137 22 m in deterministic operation the more forecasted rainless day is adopted the larger the risk value is this is because the forecast accuracy decreases with the increase of forecasted rainless day if 7 rainless day is adopted the probability of the highest water level exceeding 138 1 m is 0 but if 1 rainless day is adopted the probability of the highest water level exceeding 137 4 m is 0 for hs below 137 22 m the trend is not clear due to the randomness of the training samples and the discretization precision of the continuous variables case 2 the training data of case 2 are input into bn to learn the corresponding network parameters and the prior probability inference is performed the risk results are shown in fig 9 from which we can know that the risk values decrease with larger safety values when the reservoir safety value of hs equal to the original maximum water level 137 22 m fig 9 a and the downstream safety value of qcs equal to the original maximum downstream flow 1709 m3 s fig 9 b the risk values reach 50 because of the normal distributions of risk sources case 3 the training data of case 3 are input into bn to learn the corresponding network parameters and the prior probability inference is performed the risk results are shown in fig 10 fig 10 a shows that there is little difference in different risk curves corresponding to different forecasted rainless days adopted for excess storage compared with fig 8 it can be seen that after adding the risk source of flood forecast uncertainty the risk difference caused by different errors of initial water level is much less than that only considering the risk source of initial water level uncertainty which means that the uncertainties of initial water level and flood forecast have the effect of offsetting and compensating for the combined risk because of the initial water level uncertainty has no effect on reservoir outflow fig 10 b is the same with fig 9 b taking forecasted rainless day 7 as an example the comparison of the risk curves of case 1 2 3 is shown in fig 11 from which we can see 1 under the same safety water level 137 22 m the risks only considering the inflow uncertainty case 2 and simultaneously considering two uncertainties case 3 are much greater than that only considering the initial water level uncertainty case 1 it shows that the flood forecast uncertainty is the main risk factor in contrast the uncertainty of the initial water level affects the risk at a certain level 2 compared with only considering the inflow uncertainty case 2 the risks that simultaneously considering two uncertainties case 3 are smaller which shows that the flood regulation risk caused by flood forecast error can be reduced by adding the uncertainty factor of the initial water level this is because there is a great probability that the initial water level is lower than the designed flwl which can compensate and offset the risk caused by forecast error when the posterior information is unknown prior probability inference of bayesian network can get the probability distribution of any node in the network according to the historical statistical information so as to calculate the risk value compared with monte carlo method it has the advantages of intelligence comprehensiveness and systematicness 4 3 2 posterior probability inference as illustrated in section3 2 3 the posterior probability inference can be divided into prediction and diagnosis in order to show the difference of reasoning results under different evidences low medium and high levels of uncertainty l1 l2 and l3 respectively are used to describe the given evidences the initial reservoir storage v 0 the peak discharge of reservoir inflow q max the peak discharge of lateral inflow iu max the maximum reservoir water level h max and the maximum flow at downstream control point qc max the values of these evidence variables under three uncertainty levels are set as follows l1 v 0 is set as the reservoir storage corresponding to the designed flwl vx q max iu max are set as the forecasted values q max iu max h max qc max are set as the maximum water level and the maximum downstream flow in the deterministic operation process l2 v 0 h max qc max are set as the mean values of l1 and l3 q max iu max are set as the forecasted value with the forecast relative error of one time standard deviation i e q max 1 σ iu max 1 σ l3 v 0 h max qc max are set as the maximum value in their simulation samples q max iu max are set as the forecasted value with the forecast relative error of twice standard deviation i e q max 1 2σ iu max 1 2σ taking the forecasted rainless day 7 in case 3 as an example the values of evidence variables under each uncertainty level are presented in table 1 1 prediction given v 0 q max iu max predict ht qct the risks obtained from prediction of case 1 2 3 under the low uncertainty level are shown in fig 12 it can be seen from fig 12 that the variation of risk values between water levels considering only the initial water level factor case 1 is the largest and considering two factors case 3 and only considering the inflow factor case 2 are much smaller besides the risk values of case 3 are smaller than those of case 2 this means under the same posterior information the uncertainty of flood operation results caused by the only factor of initial water level uncertainty is the smallest that is the impact on risk is the least the uncertainties of flood operation results caused by the only factor of inflow uncertainty and the comprehensive two factors are larger i e the two cases have a significant impact on risk then we can see the influence of different values of evidence variables on the prediction results given the values of v 0 corresponding to l1 l2 l3 and the values of q max iu max corresponding to l1 the flood operation results of case 3 are predicted as well as their probabilities of occurrence as shown in fig 13 the risk incremental rate predicted by the low medium and high uncertainty levels also increases successively which is quite reasonable the prediction results under different uncertainty levels of evidence variables are shown in tables 2 4 which present the value interval corresponding to the highest probability of the predicted variable and its probability we can observe some general trends from the tables the variable values of maximum probability increase with the uncertainty level and the probability values in case 2 and case 3 are lower than those in case 1 this is because the inflow uncertainty is considered in these two cases which has a greater impact on the uncertainty of flood control results 1 diagnosis given h max qc max diagnose v 0 qt iut the posterior probability distributions diagnosed from different levels of uncertainties are shown in figs 14 16 taking forecasted rainless day 7 as an example the higher the uncertainty level of h max qc max is i e the larger the given values of maximum reservoir water level and maximum downstream flow the larger the values of probability distribution range of v 0 q max iu max obtained by diagnosis reasoning are the posterior probability distributions of v 0 in case 1 are concentrated fig 14 which indicate that the uncertainty of initial water level has little impact on the reservoir flood control risk the distribution of q max in fig 15 is scattered when the uncertainty level is l1 or l2 fig 15 a compared with fig 15 b it can be seen that the main uncertainty source of risk is the reservoir inflow peak q max taking the highest uncertainty level l3 as an example for analysis since the corresponding value of each evidence variable is the maximum value in the simulation samples which is equivalent to the most extreme risk situation the diagnosis results are also extreme the maximum probability values are close to the maximum values in simulation samples of q max and iu max and the probability distributions are sharp thin and concentrated as shown in fig 16 b and c yet the probability distribution of v 0 is wide in fig 16 a which illustrates that the main reason for this extreme risk event is q max and iu max and the impact of v 0 is relatively small the posterior probability inference through the trained bn can re evaluate the risk results after the introduction of new observed posterior information it can assess risks intelligently and automatically and display risk results in real time which is of great guiding significance to the decision making of flood control and profit promotion of the reservoir in flood season 5 conclusions reservoir dc flwl through the method of pre refill and pre release based on forecasts is the most extensive and effective way to alleviate the conflict between flood control and water conservation in flood season currently however dc flwl not only alleviates water shortage but also brings certain risk to reservoir flood control in this paper bayesian network is used to analyze the influence on the reservoir flood control operation of the uncertainty of initial water level caused by floodwater utilization the uncertainty of flood forecast caused by the forecast error and the interactions using a flood of 100 years return period for case study the main conclusions are as follows 1 the uncertainty of flood forecast is the main risk factor in comparison the uncertainty of the initial water level has less influence on the risk besides the risk caused by flood forecast error will be reduced after adding the uncertainty factor of the initial water level 2 the posterior probability inference of bn can realize bi directional inferences that is prediction from cause to effect and diagnosis from effect to cause in real time flood operation the probability distribution of any other node can be inferred given the posterior information of some nodes which has a good application prospect in risk assessment and decision making support 3 in the posterior probability distributions obtained by predictive reasoning the uncertainty of flood operation results caused by the single factor of initial water level uncertainty is the lowest and that caused by the single factor of inflow uncertainty is the highest while the results of two uncertain factors are in the middle range which are closer to the results of singular inflow uncertainty factor in addition the predicted risks increase with the level of uncertainty 4 in diagnostic reasoning the higher uncertainty level of the evidence variables i e the greater the given values of maximum reservoir water level and maximum downstream flow corresponds to increase in the values of probability distribution range of the query variables the initial reservoir storage reservoir inflow and lateral inflow the proposed methodology introduces the risk source of floodwater utilization and coupling the flood forecast error the coordination and transformation between the water use benefit and the flood control risk of reservoir can be realized in flood season bayesian network as a new risk control method its bi directional inferences have good application prospects 1 establishing a bi directional risk evaluation mode for reservoir operation 2 assessing the real time risk evolution according to the real time forecast information and reservoir state 3 realizing risk informed decision making according to the risk values of the network nodes credit authorship contribution statement qingwen lu methodology software writing original draft ping an zhong conceptualization supervision funding acquisition bin xu formal analysis visualization feilin zhu validation project administration yufei ma writing review editing han wang data curation sunyu xu data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national key r d program of china grant number 2017yfc0405606 the national natural science foundation of china grant number 51909062 the national natural science foundation of jiangsu province china grant number bk20180509 the china postdoctoral science foundation grant number 2019m661715 appendix a the rainless day forecast error δ d is defined as below a 1 δ d d d in which d is the actual rainless day d is the predicted rainless day using the rainfall forecast results from tigge in this paper δ d has a certain thick tail property compared with theoretical gaussian distribution moreover it has a distribution range d 1 d 2 and d 1 d d 2 d m d dm is the longest natural continuous rainless day taken as 30 in this case thus the improved truncated gaussian distribution for rainless day forecast error is a 2 f δ d a e δ d δ d c 2 2 ω 2 y 0 d 1 d 2 a e δ d δ d c 2 2 ω 2 y 0 d δ d setting a a d 1 d 2 a e δ d δ d c 2 2 ω 2 y 0 d δ d y 0 y 0 d 1 d 2 a e δ d δ d c 2 2 ω 2 y 0 d δ d it is assumed there is no systematic deviation in rainless day forecast i e δ d c 0 then the above equation can be simplified as a 3 f δ d a e δ d 2 2 ω 2 y 0 δ d d 1 d 2 appendix b the excess storage can be divided into pre release water for floodwater utilization w 1 and pre release water for flood control w 2 b 1 w w 1 w 2 q m q in δ t 1 d δ d t 1 τ α 0 q s q fl t 1 ɛ t δ t 2 where ɛ t is the flood forecast relative error at time t t τ which follows the normal distribution n 0 σ 2 t q fl t is the forecasted reservoir inflow at time t using the design flood of 100 years return period in this case as the next flood occurs several days later cannot forecast nowadays setting w 1 as random variable x and q m q in δ t 1 b q m q in δ t 1 d c we have b 2 x g δ d b δ d c the inverse function of eq b 2 is δ d g 1 x x c b taking the derivative of x we can get d g 1 x dx 1 b hence b 3 f x x f δ d g 1 x d g 1 x dx 1 b a e x c 2 2 b ω 2 y 0 the equation above is the distribution density function of pre release water for floodwater utilization we can get x b d 1 c b d 2 c since δ d d 1 d 2 and b d 1 c 0 through d 1 d setting b d 2 c w 1 then x 0 w 1 setting w 2 as random variable y then y is a linear combination of ɛ t then y n μ 2 σ 2 2 b 4 μ 2 t 1 τ α 0 q s q fl t δ t 2 b 5 σ 2 2 t 1 τ q fl t δ t 2 2 σ 2 t then the distribution density function of pre release water for flood control is b 6 f y y 1 2 π σ 2 e y μ 2 2 2 σ 2 2 setting total excess storage w as random variable z then z x y regarding weather forecast and flood forecast independent of each other i e x y are independent of each other the distribution of w can be then deduced degroot and schervish 2011 to simplify the following derivation process setting a b a 1 c μ 1 b ω σ 1 y 0 b y 0 1 2 π σ 2 a 2 then the distribution density functions of x y are respectively b 7 f x x a 1 e x μ 1 2 2 σ 1 2 y 0 x 0 w 1 b 8 f y y a 2 e y μ 2 2 2 σ 2 2 in this case the distribution function of z is b 9 f z z x y z f x y d x d y 0 w 1 z x f x y d y d x where f x y represents the joint density function of x y if x y are independent of each other f x y f x x f y y taking the derivative of eq b 9 with respect to z the distribution density function of z can be achieved as below b 10 f z z 0 w 1 d dz z x f x y d y d x 0 w 1 f x z x d x 0 w 1 f x x f y z x d x substituting eq b 7 eq b 8 into eq b 10 to get b 11 f z z a 1 a 2 0 w 1 exp x μ 1 2 2 σ 1 2 z x μ 2 2 2 σ 2 2 d x y 0 a 2 0 w 1 exp z x μ 2 2 2 σ 2 2 d x 0 w 1 exp x μ 1 2 2 σ 1 2 z x μ 2 2 2 σ 2 2 d x in eq b 11 can be reorganized to b 12 0 w 1 exp σ 2 2 x μ 1 2 σ 1 2 z x μ 2 2 2 σ 1 2 σ 2 2 d x 0 w 1 exp σ 2 2 x 2 2 x μ 1 μ 1 2 σ 1 2 z 2 x 2 μ 2 2 2 x z 2 z μ 2 2 x μ 2 2 σ 1 2 σ 2 2 d x 0 w 1 exp x 2 σ 1 2 σ 2 2 2 x σ 2 2 μ 1 σ 1 2 z μ 2 σ 1 2 z 2 μ 2 2 2 z μ 2 σ 2 2 μ 1 2 2 σ 1 2 σ 2 2 d x setting σ 3 σ 1 2 σ 2 2 the equation above can be rearranged as b 13 0 w 1 exp x 2 2 x σ 2 2 μ 1 σ 1 2 z μ 2 σ 3 2 σ 1 2 z 2 μ 2 2 2 z μ 2 σ 2 2 μ 1 2 σ 3 2 2 σ 1 σ 2 σ 3 2 d x 0 w 1 exp x σ 2 2 μ 1 σ 1 2 z μ 2 σ 3 2 2 σ 2 2 μ 1 σ 1 2 z μ 2 σ 3 2 2 σ 1 2 z μ 2 2 σ 2 2 μ 1 2 σ 3 2 2 σ 1 σ 2 σ 3 2 d x 0 w 1 exp σ 3 2 σ 1 2 z μ 2 2 σ 2 2 μ 1 2 σ 2 2 μ 1 σ 1 2 z μ 2 2 2 σ 3 2 σ 1 σ 2 2 exp x σ 2 2 μ 1 σ 1 2 z μ 2 σ 3 2 2 2 σ 1 σ 2 σ 3 2 d x in the above equation exp σ 3 2 σ 1 2 z μ 2 2 σ 2 2 μ 1 2 σ 2 2 μ 1 σ 1 2 z μ 2 2 2 σ 3 2 σ 1 σ 2 2 can be deduced more b 14 exp σ 1 2 σ 2 2 σ 1 2 z μ 2 2 σ 2 2 μ 1 2 σ 2 2 μ 1 σ 1 2 z μ 2 2 2 σ 3 2 σ 1 σ 2 2 exp σ 1 4 z μ 2 2 σ 1 σ 2 2 μ 1 2 σ 1 σ 2 2 z μ 2 2 σ 2 4 μ 1 2 σ 2 4 μ 1 2 2 σ 1 σ 2 2 μ 1 z μ 2 σ 1 4 z μ 2 2 2 σ 3 2 σ 1 σ 2 2 exp μ 1 2 z μ 2 2 2 μ 1 z μ 2 2 σ 3 2 exp z μ 1 μ 2 2 2 σ 3 2 putting eq b 14 into eq b 13 eq b 15 can be achieved b 15 0 w 1 exp x μ 1 2 2 σ 1 2 z x μ 2 2 2 σ 2 2 d x exp z μ 1 μ 2 2 2 σ 3 2 0 w 1 exp x σ 1 2 z μ 2 σ 2 2 μ 1 σ 3 2 2 2 σ 1 σ 2 σ 3 2 d x putting eq b 15 into eq b 11 we can get b 16 f z z a 1 a 2 exp z μ 1 μ 2 2 2 σ 3 2 0 w 1 exp x σ 1 2 z μ 2 σ 2 2 μ 1 σ 3 2 2 2 σ 1 σ 2 σ 3 2 d x y 0 a 2 0 w 1 exp z x μ 2 2 2 σ 2 2 d x 
5281,the uncertainty of initial water level being above the designed flood limited water level and the uncertainty of inflow caused by flood forecast error can cause flood risk in real time reservoir flood operation a risk analysis model for reservoir flood regulation under two dimensional uncertainties based on bayesian network is proposed the nodes of bayesian network are determined and the network structure is established with expert knowledge the parameter learning is conducted with the training samples obtained from monte carlo simulation thereafter through the prior probability inference without posterior information and the posterior probability inference with given posterior information the variation of flood risk is analyzed under singular factor uncertainty and two factors uncertainties the case study of xianghongdian reservoir using a flood of 100 years return period indicates the risk resulted from inflow uncertainty is greater than that of the uncertainty of initial water level there is a certain complementarity between the uncertainties of inflow and initial water level and the combination risk is between the results of two single factor risk levels moreover bayesian network is able to conduct bi directional inferences and infer the probability distribution of any other node which has practical value for risk assessment and control of reservoir flood control operation keywords reservoir floodwater utilization uncertainty assessment flood regulation risk bayesian network 1 introduction during flood seasons reservoir water level is controlled below the designed flood limited water level flwl to ensure adequate flood storage for regulating design flood or check flood before arrival of floods chen et al 2013 peng et al 2017 tan et al 2017 zhang et al 2018 however long term practice shows that the operation mode of strictly controlling reservoir water level below flwl may increase spilled water which affects water supplying guarantee rate in dry inflow span and reduces economic benefits yun and singh 2008 zhou et al 2014 mo et al 2018 to resolve the contradiction between reservoir flood control and water conservation in flood season and improve comprehensive benefits of reservoir research on dynamic control of flwl dc flwl got focus in the last few decades duren and beard 1972 draper and lund 2004 li et al 2010 liu et al 2011 zhou et al 2014 ding et al 2015 ding et al 2017 dc flwl of reservoir varies the flwl in flood season based on the real time flood rainfall reservoir status conditions and the forecast results under the premise of not lowering the flood control standard of the reservoir with the development of computer radar satellite and other advanced technologies and the improvement of numerical weather forecast dc flwl through the method of pre refill and pre release based on forecasts draper and lund 2004 ding et al 2015 hui et al 2016 li et al 2010 zhou et al 2018 has been systematically investigated at receding flood stages partial of flood water is retained pre refill within reservoir and results storage surplus to the reservoir flwl and the reservoir water level is reduced to the flwl or even below by pre release before next flood occurs yun and singh 2008 the excess storage of reservoir can be determined by the real time flood forecast information diao and wang 2011 zhang et al 2011 zhou et al 2018 it can also be determined synthetically according to weather forecast and flood forecast results zhou et al 2005 there are certain risks in floodwater utilization because of the unavoidable forecast error resulting in exceedance in the designed flwl due to a biased forecast thus causing flood risk to reservoir and downstream li et al 2010 developed a dynamic control operation model that considers inflow uncertainty consisting of pre release refill operation and risk analysis modules and the case study showed that the dc flwl can increase hydropower benefits without increasing flood control risk ding et al 2015 established a two stage reservoir operation model for dc flwl and derived optimal hedging rules according to forecast level and acceptable risk for real time reservoir operation in flood season in this paper the uncertainty of dc flwl considering both rainless day forecast error through weather forecast and real time flood forecast error through hydrological forecast is quantified with the probability distribution of the excess storage through joint probability distribution derivation and the bayesian network analysis method is adopted to study further on the flood risks of reservoir and its downstream under a specific operation mode with consideration of the uncertainty of initial water level coupling with the subsequent flood forecast error bayesian network is a kind of probability inferencing network combining probability theory and graph theory pearl 1988 it can decompose the joint distribution into several probability distributions with lower complexity by using the conditional independence relationship between variables so as to reduce the model complexity and improve the reasoning efficiency and is widely used in the research field of artificial intelligence at present lewis 1998 dhall et al 2020 kang and he 2011 proposed a risk analyzing model for water transfer project based on bayesian network which could be used for real time risk assessment and decision support chen et al 2019 applied the dynamic bayesian network to risk analysis for real time flood control operation of a multi reservoir system the framework of this paper is as follows section 2 elaborates the risk factors of reservoir flood control operation including the uncertainty of initial water level and the uncertainty of flood forecast section 3 establishes the bayesian network structure of reservoir flood regulation and obtains the network parameters based on training samples from mc simulation then conducts probability inference using the trained network and uses the reasoning results for risk calculation finally the model is applied to xianghongdian reservoir in huai river basin of china and the results are analyzed and discussed fig 1 shows the framework of the proposed model 2 risk sources of reservoir flood regulation assuming that reservoir implements dc flwl for floodwater utilization in the earlier recession stage there is uncertainty in the initial water level for the next flood besides there exist many uncertainties involved in the flood control operation of the reservoir chen et al 2015 xu et al 2019 such as the uncertainty in hydrological forecast of flood flow uncertainty in hydraulic conditions of discharge capacity and uncertainty in water level storage curve etc flood forecast error is considered as the primary risk factor in the study of reservoir flood control risk since the forecasted flood process is the basis of flood control decision making uncertainties in discharge capacity and water level storage curve are not considered in this paper fig 2 presents the schematic of the risk factors in flood regulating there is an error distribution in the initial water level reservoir storage and there is also an error distribution in the forecasted flood process forming two uncertain risk sources of initial water level and forecast error 2 1 uncertainty of initial regulating condition the method of pre refill and pre release is to store a certain amount of floodwater in flood control capacity based on rainfall forecast and flood forecast information the excess water storage is released through pre release for floodwater utilization in rainless periods and pre release for flood control in the early periods of flood there are errors in weather forecast and flood forecast inevitably we set d as the rainless day considering uncertainty q fl t as the reservoir inflow in early flood periods considering uncertainty the pre release water considering uncertainty can be then expressed as 1 w q m q in d δ t 1 t 1 τ α 0 q s q fl t δ t 2 where qm is the maximum water flow for utilization of the reservoir q in is the average recession flow in the forecasted rainless periods d α 0 represents the allowed maximum discount coefficient of downstream safety conveyance discharge α 0 1 qs is the safety discharge of the reservoir downstream δ t 1 is the time interval of rainless forecast δ t 2 is the time interval of flood forecast an improved truncated gaussian distribution appendix a is proposed to describe the error of forecasted rainless day d and the normal distribution is taken to describe the error of forecasted flood q fl t then the distribution densities of the excess storage corresponding to 1 7 forecasted rainless day are obtained through the related theory of mathematical statistics the specific derivation is shown in appendix b the density function is expressed as follows 2 f w w a 1 a 2 exp w μ 1 μ 2 2 2 σ 3 2 0 w 1 exp x σ 1 2 w μ 2 σ 2 2 μ 1 σ 3 2 2 2 σ 1 σ 2 σ 3 2 d x y 0 a 2 0 w 1 exp w x μ 2 2 2 σ 2 2 d x where a 1 μ 1 σ 1 y 0 are the distribution parameters of pre release water for floodwater utilization in rainless periods w 1 is the upper boundary of pre release water for floodwater utilization a 2 μ 2 σ 2 are the distribution parameters of pre release water for flood control in flood periods σ 3 σ 1 2 σ 2 2 thus the initial reservoir storage of the next flood considering uncertainty can be calculated with the following equation 3 v 0 v x w s w where vx denotes the corresponding storage of reservoir designed flwl ws is the upper boundary of the excess storage determined in dc flwl it can be known from eq 3 that v 0 is a monotonic function of w the distribution density of v 0 can be derived since the density function of w has been obtained degroot and schervish 2011 set v 0 g w then w g 1 v 0 v x w s v 0 and the distribution density of v 0 is 4 f v 0 v 0 f w g 1 v 0 d g 1 v 0 d v 0 f w v x w s v 0 from eq 4 the distribution of initial reservoir storage of the next flood can be derived under the premise of conducting floodwater utilization in the recession stage of the previous flood that is the uncertainty of initial condition caused by the excess storage 2 2 uncertainty of flood forecast errors exist in flood forecast because of uncertainties in observed hydrological and meteorological information model structure and model parameters which formulates the forecasted flood flow as continuous stochastic processes under the condition that the forecast is unbiased the forecasted process can be treated as the mean value of the stochastic processes and the forecasted flow during each time period in the process obeys normal distribution diao et al 2007 chen et al 2015 the reservoir inflow considering uncertainty q t can be expressed as eq 5 for the single reservoir flood control system containing the downstream flood control point the lateral inflow at downstream considering uncertainty iu t should be also taken into account expressed as eq 6 5 q t q t 1 ɛ q t 6 iu t i u t 1 ɛ iu t where q t iu t are the forecasted reservoir inflow and lateral inflow at time t respectively ɛ q t ɛ iu t are the relative errors of q t iu t ɛ q t and ɛ iu t follow normal distribution thus q t and iu t follow normal distribution as well the distribution parameters of ɛ q t ɛ iu t can be estimated according to the historical flood forecast results 3 risk analysis model for flood regulation based on bayesian network 3 1 basic principles of bayesian network bayesian network bn is a theory that uses directed graph to describe probability relation it can be used to deal with the uncertainties caused by the conditional correlation among different variables it consists of the following parts 1 model structure model structure properties are represented by a directed acyclic graph dag nodes in dag correspond to variables in model directed arcs represent the conditional dependency between variables 2 related parameters model parameters refer to the conditional probability table cpt specified for each variable cpt specifies the conditional probability for each instance of the variable risk analysis for reservoir flood control is in essence a problem to solve the joint probability distribution function pdf of multiple variables it is very complex to deal with uncertainty directly using the joint distribution of numerous variables bn can decompose the joint distribution into the products of several probability distributions with lower complexity using chain rule and the conditional independence relationship between variables expressing as 7 p x 1 x n i 1 n p x i π x i where p x 1 x n is the joint distribution of n variables π x i represents parent nodes of xi p x i π x i is the conditional probability distribution of xi given π x i when π x i p x i π x i is then the marginal distribution p xi 3 2 establishment and inference of the bayesian network the bayesian network model for flood risk analysis is built through the following three steps 1 structure learning which determines the network structure according to the conditional dependence between the stochastic variables as the relationship among the variables in the flood control operation of a reservoir is explicit this study establishes the graph structure by expert knowledge 2 parameter learning which gets the prior probability distributions of root nodes and the conditional probability distributions of non root nodes using a learning algorithm this study performs parameter learning through the maximum likelihood estimation mle method 3 probabilistic reasoning that is given the evidence of some nodes to infer the probability distributions of any other nodes it can perform bi directional inferences i e prediction infer from cause to effect and diagnosis infer from effect to cause 3 2 1 structure learning the basic principle of reservoir flood routing is the water balance equation expressed as eq 8 8 v t 1 v t q t q t 1 2 q t q t 1 2 δ t the flood flow at downstream flood control point at time t qc t is the sum of reservoir outflow q t and lateral inflow iu t as shown in eq 9 9 qc t i u t q t set the number of time intervals of reservoir flood routing as t the initial time is 0 then t 0 1 t in the above equations according to eq 8 and eq 9 the nodes in the network and the relationship between them can be determined since the uncertainty of initial reservoir storage is considered for convenience of expression the flow variables in bn are taken as average values in a time period variables are shown as follows v t 0 reservoir storage at the beginning of time period t v t 1 reservoir storage at the end of time period t h t 0 reservoir water level at the beginning of time period t h t 1 reservoir water level at the end of time period t q t q t 1 q t 2 average reservoir inflow in time period t q t q t 1 q t 2 average reservoir outflow in time period t i u t iu t 1 i u t 2 average lateral inflow in time period t q c t qc t 1 q c t 2 average flow at downstream control point in time period t the established structure of bn for reservoir flood control operation according to the logical relationship is shown as fig 3 3 2 2 parameter learning 1 generating the training samples mc method is used to provide data inputs for bn due to the high complexity of the density function of the initial reservoir storage we discretize it by numerical method and generate the random samples of v 0 v 1 0 according to the discretized probabilities the samples of reservoir inflow and lateral inflow are generated through the latin hypercube sampling lhs method huang et al 2018 then the optimal flood control model of a reservoir is established which is to minimize the maximum reservoir outflow with consideration of the constraints of water balance the upper and lower limits of water level the upper and lower limits of outflow discharge capacity and variation of outflows between adjacent times as the following equations 10 min max q t s t v t 1 v t 0 q t q t δ t h t h t h t q t q t q t q t q t max q t q t 1 δ q m substitute the generated samples into the above model we can get the corresponding samples of reservoir outflow reservoir storage and water level and the samples of flow at downstream control point can be obtained through eq 9 1 discretizing the variables it is generally assumed that all variables are discrete variables or continuous variables following normal distribution in parameter learning of bn monti and cooper 1998 however the risk factors are not all subjected to normal distribution and the influence of reservoir flood regulation on the distributions of variables should be considered thus we discretize the continuous variables identified in section 3 2 1 using the equal distance method the equal distance method is an unsupervised algorithm for discretization chen et al 2019 we specify the number of the discrete intervals as k and divide the range of the variable x into k intervals the width of each interval is equal to x max x min k the more the number of intervals the higher the discretization precision of the variable but at the same time the computing cost will increase the interval number k should be determined according to the number of training samples the value range of the variables and the precision of the required results sensitivity analysis can also be conducted through changing the value of k the k value can be sought that has the least impact on the uncertainty calculation under the premise of the allowable computation amount and time 2 parameter learning through mle method based on the discretized training samples the mle method which is suitable for complete data is adopted to perform parameter learning of bn as expressed in eq 11 11 θ ml arg max θ l x θ arg max θ ln p x θ where θ refers to the parameter of bn θ ml is the maximum likelihood estimation of the parameter θ l x θ is the likelihood function 3 2 3 probability inference inference is the process of answering query through calculation for a trained bn the prior probability inference can be made when the posterior knowledge is acquired the posterior probability inference can be performed in the posterior probability problem the known variable is usually called evidence variable denoted as e and its value is denoted as e the variable whose posterior probability is needed to be computed is called query variable denoted as m then the posterior distribution to be computed is p m e e the calculation principle is bayes theorem as shown in eq 12 12 p m e e p e e m p m p e e where p m is the prior probability of m p e e m is the likelihood probability according to the risk decision making needs of reservoir flood routing the prediction reasoning from cause to effect and diagnosis reasoning from effect to cause are conducted in this paper prediction can conduct risk assessment according to the risk factors that is to analyze the possible consequences of the reservoir and downstream and their probabilities under the influence of various risk factors diagnosis can infer the occurrence probabilities of the risk factors based on risk results for example given the value of the maximum water level of reservoir or the maximum flow at downstream control point it calculates the risk causes reversely and provides the possible values of the risk causes and the corresponding probabilities so as to analyze the uncertainty level of each risk factor under the results which provides valuable information for flood control risk decision making 1 prediction given the initial reservoir storage reservoir inflow and lateral inflow calculate the probability distributions of the reservoir water level and the river flow at downstream control point 2 diagnosis given the reservoir water level or the river flow at downstream control point calculate the probability distributions of the initial reservoir storage reservoir inflow and lateral inflow in this paper the joint tree propagation algorithm shafer and shenoy 1990 is used for the posterior probability inference of bn 3 3 risk calculation based on the probability distributions of the random variables inferred by bn the risk value is derived through calculating the cumulative probability of exceeding the threshold of the acceptable safety value set the reservoir safety water level as hs and the safety flow at downstream control point as qcs then the flood risk can be defined from the reservoir itself and the downstream control point expressed as follows 13 p r t p h t h s i 1 m h t p i h t i h s 14 p d t p q c t q c s i 1 m q c t p i q c t i q c s where pr t is the reservoir risk in time period t m h t is the number counted as the values of water level nodes exceeding hs p i h t i h s is the probability value of h t i h s pd t is the risk of downstream control point in time period t m q c t is the number counted as the values of downstream flow nodes exceeding qcs p i q c t i q c s is the probability value of q c t i q c s 4 case study 4 1 introduction to the study area the xianghongdian reservoir which locates in upstream of the west pi river huai river in china is a large reservoir used for flood control irrigation and electricity generation the reservoir basin covers an area of 1431 km2 and has a mean annual precipitation of 1465 mm the reservoir flwl is 125 m and the corresponding storage is 1227 15 106m3 the design flood water level is 140 98 m and the check flood water level is 143 37 m the downstream flood control point is jinzhai with a safety discharge of 2500 m3 s the huai river basin and the xianghongdian reservoir are shown in fig 4 wan et al 2019 and fig 5 presents the forecasted reservoir inflow and lateral inflow between reservoir and downstream control point of a flood the time interval is 1 h the flood magnitude is determined to be 100 years return period through frequency analysis 4 2 data preparation based on the forecasted rainless day 1 7 adopted for excess storage in the recession of the previous flood the level b flood forecast accuracy and the maximum discount coefficient of safety discharge α 0 0 8 the distribution density of v 0 can be computed according to eq 4 and the results are shown in fig 6 xianghongdian flood control system uses xin anjiang model for flood forecast and the forecast accuracy can reach level b it can be known that the probability of forecast relative error distributing within allowable error ɛ 0 is 70 85 according to the qualified rate of level b accuracy of 70 85 taking ɛ 0 20 we can then get the standard deviation of relative error σ 0 19 according to p ɛ ɛ 0 70 thus the forecast relative errors of reservoir inflow and lateral inflow obey n 0 0 192 based on mc simulation method 1000 random samples of v 0 corresponding to forecasted rainless day 1 7 and reservoir inflow qt lateral inflow iut are generated according to their probability distributions substituting them into the optimal flood control model of a reservoir 1000 samples of vt ht qt and qct can be then obtained thus the data input into bn are all acquired 4 3 results and discussion in the deterministic operation process without considering uncertainties which is calculated with the designed flwl as initial water level and the process of the forecasted inflow the maximum reservoir water level is 137 22 m and the maximum flow at downstream control point is 1709 m3 s three cases are taken to analyze the influence of uncertainties on the calculation results of flood regulation case 1 only considering the uncertainty of initial water level the training data are deterministic samples of qt iut forecasted mean value random samples of v 0 and random samples of qt vt t 0 ht qct obtained by flood regulation case 2 only considering the uncertainty of flood forecast the training data are deterministic samples of v 0 vx random samples of qt iut and random samples of qt vt t 0 ht qct obtained by flood regulation case 3 simultaneously considering the uncertainties of initial water level and flood forecast the training data are random samples of v 0 qt iut and random samples of qt vt t 0 ht qct obtained by flood regulation the bayesian network can get the probability distribution of each variable at each time due to limited space the paper only presents the results of the maximum reservoir water level and the maximum flow at downstream control point 4 3 1 prior probability inference for the trained bn the prior probability inference can be performed to obtain the possible values of the maximum reservoir water level and the maximum flow at downstream control point as well as their possibilities of occurrence the corresponding risk rates can be calculated based on eq 13 and eq 14 if mc simulation method is used the risk rate will be the ratio of times of exceeding the acceptable safety value to total simulation times the results of bn prior probability reasoning in case 1 are compared with those of mc method to verify the learning and reasoning effect of bn taking forecasted rainless day 7 as an example the comparison of the risk curves of reservoir maximum water level is shown in fig 7 the vertical line in the figure represents the original maximum water level 137 22 m in deterministic operation the results of bn reasoning are consistent with those of mc simulation which shows that bn reasoning is effective the risk of the maximum flow at downstream control point is 0 because the uncertainty of initial water level is small and the reservoir outflow samples obtained from the optimal operation model are the same therefore we only analyze the reservoir risk of case 1 in the rest of the paper next the prior probability inference and risk calculation are carried out for case 1 case 2 and case 3 respectively case 1 the training data of case 1 are input into bn to learn the corresponding network parameters the prior probability inference is performed and the risk values corresponding to forecasted rainless day 1 7 are calculated through eq 13 fig 8 shows the risks of the reservoir under different safety water level when hs is greater than the original maximum water level 137 22 m in deterministic operation the more forecasted rainless day is adopted the larger the risk value is this is because the forecast accuracy decreases with the increase of forecasted rainless day if 7 rainless day is adopted the probability of the highest water level exceeding 138 1 m is 0 but if 1 rainless day is adopted the probability of the highest water level exceeding 137 4 m is 0 for hs below 137 22 m the trend is not clear due to the randomness of the training samples and the discretization precision of the continuous variables case 2 the training data of case 2 are input into bn to learn the corresponding network parameters and the prior probability inference is performed the risk results are shown in fig 9 from which we can know that the risk values decrease with larger safety values when the reservoir safety value of hs equal to the original maximum water level 137 22 m fig 9 a and the downstream safety value of qcs equal to the original maximum downstream flow 1709 m3 s fig 9 b the risk values reach 50 because of the normal distributions of risk sources case 3 the training data of case 3 are input into bn to learn the corresponding network parameters and the prior probability inference is performed the risk results are shown in fig 10 fig 10 a shows that there is little difference in different risk curves corresponding to different forecasted rainless days adopted for excess storage compared with fig 8 it can be seen that after adding the risk source of flood forecast uncertainty the risk difference caused by different errors of initial water level is much less than that only considering the risk source of initial water level uncertainty which means that the uncertainties of initial water level and flood forecast have the effect of offsetting and compensating for the combined risk because of the initial water level uncertainty has no effect on reservoir outflow fig 10 b is the same with fig 9 b taking forecasted rainless day 7 as an example the comparison of the risk curves of case 1 2 3 is shown in fig 11 from which we can see 1 under the same safety water level 137 22 m the risks only considering the inflow uncertainty case 2 and simultaneously considering two uncertainties case 3 are much greater than that only considering the initial water level uncertainty case 1 it shows that the flood forecast uncertainty is the main risk factor in contrast the uncertainty of the initial water level affects the risk at a certain level 2 compared with only considering the inflow uncertainty case 2 the risks that simultaneously considering two uncertainties case 3 are smaller which shows that the flood regulation risk caused by flood forecast error can be reduced by adding the uncertainty factor of the initial water level this is because there is a great probability that the initial water level is lower than the designed flwl which can compensate and offset the risk caused by forecast error when the posterior information is unknown prior probability inference of bayesian network can get the probability distribution of any node in the network according to the historical statistical information so as to calculate the risk value compared with monte carlo method it has the advantages of intelligence comprehensiveness and systematicness 4 3 2 posterior probability inference as illustrated in section3 2 3 the posterior probability inference can be divided into prediction and diagnosis in order to show the difference of reasoning results under different evidences low medium and high levels of uncertainty l1 l2 and l3 respectively are used to describe the given evidences the initial reservoir storage v 0 the peak discharge of reservoir inflow q max the peak discharge of lateral inflow iu max the maximum reservoir water level h max and the maximum flow at downstream control point qc max the values of these evidence variables under three uncertainty levels are set as follows l1 v 0 is set as the reservoir storage corresponding to the designed flwl vx q max iu max are set as the forecasted values q max iu max h max qc max are set as the maximum water level and the maximum downstream flow in the deterministic operation process l2 v 0 h max qc max are set as the mean values of l1 and l3 q max iu max are set as the forecasted value with the forecast relative error of one time standard deviation i e q max 1 σ iu max 1 σ l3 v 0 h max qc max are set as the maximum value in their simulation samples q max iu max are set as the forecasted value with the forecast relative error of twice standard deviation i e q max 1 2σ iu max 1 2σ taking the forecasted rainless day 7 in case 3 as an example the values of evidence variables under each uncertainty level are presented in table 1 1 prediction given v 0 q max iu max predict ht qct the risks obtained from prediction of case 1 2 3 under the low uncertainty level are shown in fig 12 it can be seen from fig 12 that the variation of risk values between water levels considering only the initial water level factor case 1 is the largest and considering two factors case 3 and only considering the inflow factor case 2 are much smaller besides the risk values of case 3 are smaller than those of case 2 this means under the same posterior information the uncertainty of flood operation results caused by the only factor of initial water level uncertainty is the smallest that is the impact on risk is the least the uncertainties of flood operation results caused by the only factor of inflow uncertainty and the comprehensive two factors are larger i e the two cases have a significant impact on risk then we can see the influence of different values of evidence variables on the prediction results given the values of v 0 corresponding to l1 l2 l3 and the values of q max iu max corresponding to l1 the flood operation results of case 3 are predicted as well as their probabilities of occurrence as shown in fig 13 the risk incremental rate predicted by the low medium and high uncertainty levels also increases successively which is quite reasonable the prediction results under different uncertainty levels of evidence variables are shown in tables 2 4 which present the value interval corresponding to the highest probability of the predicted variable and its probability we can observe some general trends from the tables the variable values of maximum probability increase with the uncertainty level and the probability values in case 2 and case 3 are lower than those in case 1 this is because the inflow uncertainty is considered in these two cases which has a greater impact on the uncertainty of flood control results 1 diagnosis given h max qc max diagnose v 0 qt iut the posterior probability distributions diagnosed from different levels of uncertainties are shown in figs 14 16 taking forecasted rainless day 7 as an example the higher the uncertainty level of h max qc max is i e the larger the given values of maximum reservoir water level and maximum downstream flow the larger the values of probability distribution range of v 0 q max iu max obtained by diagnosis reasoning are the posterior probability distributions of v 0 in case 1 are concentrated fig 14 which indicate that the uncertainty of initial water level has little impact on the reservoir flood control risk the distribution of q max in fig 15 is scattered when the uncertainty level is l1 or l2 fig 15 a compared with fig 15 b it can be seen that the main uncertainty source of risk is the reservoir inflow peak q max taking the highest uncertainty level l3 as an example for analysis since the corresponding value of each evidence variable is the maximum value in the simulation samples which is equivalent to the most extreme risk situation the diagnosis results are also extreme the maximum probability values are close to the maximum values in simulation samples of q max and iu max and the probability distributions are sharp thin and concentrated as shown in fig 16 b and c yet the probability distribution of v 0 is wide in fig 16 a which illustrates that the main reason for this extreme risk event is q max and iu max and the impact of v 0 is relatively small the posterior probability inference through the trained bn can re evaluate the risk results after the introduction of new observed posterior information it can assess risks intelligently and automatically and display risk results in real time which is of great guiding significance to the decision making of flood control and profit promotion of the reservoir in flood season 5 conclusions reservoir dc flwl through the method of pre refill and pre release based on forecasts is the most extensive and effective way to alleviate the conflict between flood control and water conservation in flood season currently however dc flwl not only alleviates water shortage but also brings certain risk to reservoir flood control in this paper bayesian network is used to analyze the influence on the reservoir flood control operation of the uncertainty of initial water level caused by floodwater utilization the uncertainty of flood forecast caused by the forecast error and the interactions using a flood of 100 years return period for case study the main conclusions are as follows 1 the uncertainty of flood forecast is the main risk factor in comparison the uncertainty of the initial water level has less influence on the risk besides the risk caused by flood forecast error will be reduced after adding the uncertainty factor of the initial water level 2 the posterior probability inference of bn can realize bi directional inferences that is prediction from cause to effect and diagnosis from effect to cause in real time flood operation the probability distribution of any other node can be inferred given the posterior information of some nodes which has a good application prospect in risk assessment and decision making support 3 in the posterior probability distributions obtained by predictive reasoning the uncertainty of flood operation results caused by the single factor of initial water level uncertainty is the lowest and that caused by the single factor of inflow uncertainty is the highest while the results of two uncertain factors are in the middle range which are closer to the results of singular inflow uncertainty factor in addition the predicted risks increase with the level of uncertainty 4 in diagnostic reasoning the higher uncertainty level of the evidence variables i e the greater the given values of maximum reservoir water level and maximum downstream flow corresponds to increase in the values of probability distribution range of the query variables the initial reservoir storage reservoir inflow and lateral inflow the proposed methodology introduces the risk source of floodwater utilization and coupling the flood forecast error the coordination and transformation between the water use benefit and the flood control risk of reservoir can be realized in flood season bayesian network as a new risk control method its bi directional inferences have good application prospects 1 establishing a bi directional risk evaluation mode for reservoir operation 2 assessing the real time risk evolution according to the real time forecast information and reservoir state 3 realizing risk informed decision making according to the risk values of the network nodes credit authorship contribution statement qingwen lu methodology software writing original draft ping an zhong conceptualization supervision funding acquisition bin xu formal analysis visualization feilin zhu validation project administration yufei ma writing review editing han wang data curation sunyu xu data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national key r d program of china grant number 2017yfc0405606 the national natural science foundation of china grant number 51909062 the national natural science foundation of jiangsu province china grant number bk20180509 the china postdoctoral science foundation grant number 2019m661715 appendix a the rainless day forecast error δ d is defined as below a 1 δ d d d in which d is the actual rainless day d is the predicted rainless day using the rainfall forecast results from tigge in this paper δ d has a certain thick tail property compared with theoretical gaussian distribution moreover it has a distribution range d 1 d 2 and d 1 d d 2 d m d dm is the longest natural continuous rainless day taken as 30 in this case thus the improved truncated gaussian distribution for rainless day forecast error is a 2 f δ d a e δ d δ d c 2 2 ω 2 y 0 d 1 d 2 a e δ d δ d c 2 2 ω 2 y 0 d δ d setting a a d 1 d 2 a e δ d δ d c 2 2 ω 2 y 0 d δ d y 0 y 0 d 1 d 2 a e δ d δ d c 2 2 ω 2 y 0 d δ d it is assumed there is no systematic deviation in rainless day forecast i e δ d c 0 then the above equation can be simplified as a 3 f δ d a e δ d 2 2 ω 2 y 0 δ d d 1 d 2 appendix b the excess storage can be divided into pre release water for floodwater utilization w 1 and pre release water for flood control w 2 b 1 w w 1 w 2 q m q in δ t 1 d δ d t 1 τ α 0 q s q fl t 1 ɛ t δ t 2 where ɛ t is the flood forecast relative error at time t t τ which follows the normal distribution n 0 σ 2 t q fl t is the forecasted reservoir inflow at time t using the design flood of 100 years return period in this case as the next flood occurs several days later cannot forecast nowadays setting w 1 as random variable x and q m q in δ t 1 b q m q in δ t 1 d c we have b 2 x g δ d b δ d c the inverse function of eq b 2 is δ d g 1 x x c b taking the derivative of x we can get d g 1 x dx 1 b hence b 3 f x x f δ d g 1 x d g 1 x dx 1 b a e x c 2 2 b ω 2 y 0 the equation above is the distribution density function of pre release water for floodwater utilization we can get x b d 1 c b d 2 c since δ d d 1 d 2 and b d 1 c 0 through d 1 d setting b d 2 c w 1 then x 0 w 1 setting w 2 as random variable y then y is a linear combination of ɛ t then y n μ 2 σ 2 2 b 4 μ 2 t 1 τ α 0 q s q fl t δ t 2 b 5 σ 2 2 t 1 τ q fl t δ t 2 2 σ 2 t then the distribution density function of pre release water for flood control is b 6 f y y 1 2 π σ 2 e y μ 2 2 2 σ 2 2 setting total excess storage w as random variable z then z x y regarding weather forecast and flood forecast independent of each other i e x y are independent of each other the distribution of w can be then deduced degroot and schervish 2011 to simplify the following derivation process setting a b a 1 c μ 1 b ω σ 1 y 0 b y 0 1 2 π σ 2 a 2 then the distribution density functions of x y are respectively b 7 f x x a 1 e x μ 1 2 2 σ 1 2 y 0 x 0 w 1 b 8 f y y a 2 e y μ 2 2 2 σ 2 2 in this case the distribution function of z is b 9 f z z x y z f x y d x d y 0 w 1 z x f x y d y d x where f x y represents the joint density function of x y if x y are independent of each other f x y f x x f y y taking the derivative of eq b 9 with respect to z the distribution density function of z can be achieved as below b 10 f z z 0 w 1 d dz z x f x y d y d x 0 w 1 f x z x d x 0 w 1 f x x f y z x d x substituting eq b 7 eq b 8 into eq b 10 to get b 11 f z z a 1 a 2 0 w 1 exp x μ 1 2 2 σ 1 2 z x μ 2 2 2 σ 2 2 d x y 0 a 2 0 w 1 exp z x μ 2 2 2 σ 2 2 d x 0 w 1 exp x μ 1 2 2 σ 1 2 z x μ 2 2 2 σ 2 2 d x in eq b 11 can be reorganized to b 12 0 w 1 exp σ 2 2 x μ 1 2 σ 1 2 z x μ 2 2 2 σ 1 2 σ 2 2 d x 0 w 1 exp σ 2 2 x 2 2 x μ 1 μ 1 2 σ 1 2 z 2 x 2 μ 2 2 2 x z 2 z μ 2 2 x μ 2 2 σ 1 2 σ 2 2 d x 0 w 1 exp x 2 σ 1 2 σ 2 2 2 x σ 2 2 μ 1 σ 1 2 z μ 2 σ 1 2 z 2 μ 2 2 2 z μ 2 σ 2 2 μ 1 2 2 σ 1 2 σ 2 2 d x setting σ 3 σ 1 2 σ 2 2 the equation above can be rearranged as b 13 0 w 1 exp x 2 2 x σ 2 2 μ 1 σ 1 2 z μ 2 σ 3 2 σ 1 2 z 2 μ 2 2 2 z μ 2 σ 2 2 μ 1 2 σ 3 2 2 σ 1 σ 2 σ 3 2 d x 0 w 1 exp x σ 2 2 μ 1 σ 1 2 z μ 2 σ 3 2 2 σ 2 2 μ 1 σ 1 2 z μ 2 σ 3 2 2 σ 1 2 z μ 2 2 σ 2 2 μ 1 2 σ 3 2 2 σ 1 σ 2 σ 3 2 d x 0 w 1 exp σ 3 2 σ 1 2 z μ 2 2 σ 2 2 μ 1 2 σ 2 2 μ 1 σ 1 2 z μ 2 2 2 σ 3 2 σ 1 σ 2 2 exp x σ 2 2 μ 1 σ 1 2 z μ 2 σ 3 2 2 2 σ 1 σ 2 σ 3 2 d x in the above equation exp σ 3 2 σ 1 2 z μ 2 2 σ 2 2 μ 1 2 σ 2 2 μ 1 σ 1 2 z μ 2 2 2 σ 3 2 σ 1 σ 2 2 can be deduced more b 14 exp σ 1 2 σ 2 2 σ 1 2 z μ 2 2 σ 2 2 μ 1 2 σ 2 2 μ 1 σ 1 2 z μ 2 2 2 σ 3 2 σ 1 σ 2 2 exp σ 1 4 z μ 2 2 σ 1 σ 2 2 μ 1 2 σ 1 σ 2 2 z μ 2 2 σ 2 4 μ 1 2 σ 2 4 μ 1 2 2 σ 1 σ 2 2 μ 1 z μ 2 σ 1 4 z μ 2 2 2 σ 3 2 σ 1 σ 2 2 exp μ 1 2 z μ 2 2 2 μ 1 z μ 2 2 σ 3 2 exp z μ 1 μ 2 2 2 σ 3 2 putting eq b 14 into eq b 13 eq b 15 can be achieved b 15 0 w 1 exp x μ 1 2 2 σ 1 2 z x μ 2 2 2 σ 2 2 d x exp z μ 1 μ 2 2 2 σ 3 2 0 w 1 exp x σ 1 2 z μ 2 σ 2 2 μ 1 σ 3 2 2 2 σ 1 σ 2 σ 3 2 d x putting eq b 15 into eq b 11 we can get b 16 f z z a 1 a 2 exp z μ 1 μ 2 2 2 σ 3 2 0 w 1 exp x σ 1 2 z μ 2 σ 2 2 μ 1 σ 3 2 2 2 σ 1 σ 2 σ 3 2 d x y 0 a 2 0 w 1 exp z x μ 2 2 2 σ 2 2 d x 
5282,it is still very challenging to enhance the accuracy and stability of daily runoff forecasts especially several days ahead owing to the non linearity of the forecasted processes here we hypothesize that short lag time has a significant impact on forecasting results thus we incorporate short previous time steps into long short term memory lstm and develop the self attentive long short term memory sa lstm in sa lstm the self attention mechanism is used to model interdependencies within short previous time steps sa lstm is evaluated at eight runoff datasets the experimental results demonstrate that compared with state of art benchmark models sa lstm achieves the best performance the rmses of sa lstm are at least 2 3 smaller than that of the second best model at the seventh day the nses and nse in of sa lstm are at least 4 6 and 6 4 higher than those of the second best model at the seventh day furthermore sa lstm also surpasses the baseline methods for base mean and peak flows the superiority of sa lstm can be attributed to its exploitation of information in short lag time keywords long short term memory runoff forecast attention self attentive long short term memory 1 introduction runoff forecasting is of great importance for water resource management such as flood protection di baldassarre et al 2010 blöschl et al 2019 drought mitigation zhang et al 2012 yuan et al 2019 reservoir operation chen et al 2016 wu et al 2018 and ecosystem conservation palmer and ruhi 2019 a runoff forecasting model is a simplified representation of a real hydrologic system sorooshian et al 2008 hrachowitz et al 2013 soulsby et al 2016 gao et al 2018 2019 runoff forecasting models are mainly classified into three categories conceptual models physically based models and empirical models beven 2011 liu et al 2019 both conceptual models and physically based models are process based models which to some extent explicitly describe and quantify hydrological processes and the water balance hundecha and bárdossy 2004 umakhanthan and ball 2005 yu et al 2006 samaniego et al 2010 birkel and soulsby 2015 seibert et al 2018 process based models have long played a dominant role in the hydrological community because of their transparent model structure and parameterization as well as their satisfactory performance in well gauged basins and model transferability yang and musiake 2003 sivapalan and blöschl 2017 gao et al 2017 unfortunately process based models have many complex limiting factors such as slope and soil moisture sorooshian and gupta 1995 thus their performance is restricted to solve these problems empirical models also known as black box models are applied in runoff forecasting empirical models estimate input output relationships from a statistical viewpoint using data driven methods shen 2018 liu et al 2018 at the beginning of the development of empirical models linear data driven models such as autoregressive moving average arma and autoregressive integrated moving average arima were always used to predict runoff time series carlson et al 1970 salas et al 1985 montanari et al 1997 noakes et al 1985 these models are linearly consequently they have relatively more interpretability and less flexibility james et al 2013 with the development of machine learning many nonlinear models have been employed in runoff forecasting typical nonlinear models include but are not limited to artificial neural network ann riad et al 2004 khalil et al 2005 support vector machine svm liong and sivapragasam 2002 asefa et al 2006 papacharalampous et al 2019 support vector regression svr maity et al 2010 luo et al 2019 random forest rf huang et al 2019 tyralis et al 2019 quantile regression forests qrf whan and schmeits 2018 bhuiyan et al 2018 2019 extreme gradient boosting xgboost zhang et al 2019 cisty and soldanova 2018 kinematic wave kw adaptive neuro fuzzy inference system anfis and nonlinear prediction nlp chua and wong 2011 laio et al 2003 he et al 2014 feed forward back propagation neural network ffbp generalized regression neural networks grnn conjugate gradient cascade correlation and radial basis function rbf neural network methods mehr et al 2015 kişi 2007 yaseen et al 2016 although these methods have achieved some improvements over linear data driven models they still belong to the shallow learning category and have limited practicability xie et al 2019 wavelet based methods are another popular tool in runoff forecasting with wavelet a time series can be decomposed into high and low frequency sub time series which offer a coherent structure to a data driven model quilty and adamowski 2018 thus wavelet based time series forecasts become popular unfortunately some wavelet based forecasts cannot be used in real world scenarios because they adopt future data to perform forecasting du et al 2017 to dispel doubt wavelet data driven forecasting framework wddff is proposed to demonstrate the superiority of wavelet by employing boundary corrected wavelet and scaling coefficients quilty and adamowski 2018 ensemble wavelet stochastic data driven forecasting framework is proposed by using variable selection methods and data driven models including multiple linear regression and extreme learning machines quilty et al 2019 in the last several years deep learning has been applied to runoff forecasting and has achieved considerable success shen 2018 assem et al 2017 established deep convolutional neural networks to forecast water flow and water level based on 30 year maximum temperature minimum temperature and run off data sets tian et al 2018 compared four recurrent neural networks rnns in runoff forecasting they found that the nonlinear autoregressive exogenous inputs neural network narx and long short term memory lstm perform better than the other two models especially in small catchments in xie et al 2019 variational mode decomposition was used to decompose a daily runoff series and a partial autocorrelation function pacf was applied to determine the input variables of each subsequence then the improved particle swarm optimization algorithm was combined with the deep belief network model to predict each subsequence and finally reconstruct the ensemble forecasting result although deep learning models greatly improve runoff forecasting accuracy the temporal dependencies between elements in a time series are still not well exploited temporal dependencies reflect the relevance of the variation in time series data across different time frequencies thus temporal dependencies could enhance model accuracy song et al 2018 based on the hypothesis that short previous time steps are more correlated than long previous time steps with prediction results short lag time are introduced to our new model i e self attentive long short term memory sa lstm where lag time is the number of previous time steps using as inputs ni et al 2020 in addition self attention mechanisms are integrated into sa lstm to exploit the temporal dependencies within short lag time dependencies at distant positions are difficult to learn hochreiter et al 2001 to tackle this problem self attention mechanisms have been utilized in sequential modelling in other fields such as machine translation and image captioning luong et al 2015 bahdanau et al 2014 these mechanisms explicitly yield global dependencies between input and output vaswani et al 2017 among these attention mechanisms the self attention mechanism is highlighted because it can compute representations of its input and output relating different positions of a single sequence for example global and local attention can be combined with lstm and rnn for machine translation and abstractive summarization zoph and knight 2016 luong et al 2015 mashlakov et al 2019 applied two attention heads to rnn for battery state of charge forecasting however there have been few works using self attention mechanisms for runoff forecasting until now in this paper the sa lstm model is proposed for runoff forecasting the main contributions of sa lstm are listed as follows 1 dual inputs i e long previous time series and short previous time series are employed to enhance the forecast accuracy short previous time series are integrated into lstm based runoff forecasting to strengthen the important information of short previous time series 2 the self attention mechanism without decoder and encoder is introduced to model temporal dependencies between previous time steps for improving runoff forecasting 3 sa lstm is compared with four benchmark models i e the svr lstm xgboost chen and guestrin 2016 and light gradient boosting machine lightgbm ke et al 2017 models through forecasting runoff with 1 7 days of lead time the experimental results including both the base flow and peak flow results verify the effectiveness of sa lstm the next section introduces the baseline methods used in this paper section 3 shows a case study and section 4 presents the results and discussions conclusions are given in section 5 2 methodologies 2 1 long short term memory lstm suppose w f w i and w o represent the weight matrices from the forget input and output gates to the input u f u i and u o represent the matrix of weight from forget input and output gates to the hidden b f b i and b o represent the forget input and output gate bias vectors then the lstm unit can be written as follows 1 f t σ w f x t u f h t 1 b f 2 i t σ w i x t u i h t 1 b i 3 o t σ w o x t u o h t 1 b o 4 c t tanh w c x t u c h t 1 b c 5 c t f t c t 1 i t c t 6 h t o t tanh c t where σ is a nonlinear activation function such as the sigmoid tanh and rectified linear unit relu relu is applied in this study f i o and c are the forget input output gates and the cell state vectors at time t respectively all of those variables have the same size as the cell output vector h is the element wise multiplication of the vectors fig 1 shows the structure of lstm 2 2 self attentive long short term memory sa lstm short previous time series have a larger impact than the long counterparts on forecasting results and thus will be more helpful for forecasting consequently sa lstm takes short previous time series in parallel to long previous time series as input suppose the lag number of the long lag time is t the lag number of the short lag time is t where t t and t is a hyperparameter the short previous time series are mapped to long previous temporal representation by correlation then the most representative features and temporal patterns of short previous time series are captured in this study the self attention mechanism is applied in runoff forecasting to calculate the interdependency of runoff on different days self attention was originally designed for natural language processing tasks such as machine translation and abstractive summarization paulus et al 2017 the self attention module has a strong capability to extract features and capture the dependencies between different time steps for each learned representation of a univariate series the self attention module learns its relationship with other learned representations including itself this mechanism allows the hidden layer to focus on critical information by assigning different attention weights to the hidden layer of the neural network in general the self attention function can be described as mapping a query and a set of key value pairs to an output where the query keys values and the output are all vectors the output is computed as a weighted sum of the values where the weight for each position is computed as the inner product between the query and keys at every other position in the time series suppose the hidden layer output vector of lstm h h 1 h 2 h t is selected as the input of the self attention mechanism the context vector v t is the sum of h i multiplication α i where α i is the attention weight of h i 7 v t i 1 t α i h i the attention weight α i of each hidden layer output vector is computed by 8 α i exp e i i 1 t exp e i 9 i 1 t α i 1 10 e i tanh w h h i b h e i 1 1 where w h and b h are the weight matrix and the bias of h i respectively the value of w h and b h are adjusted in training process although learning dependencies within the time sequence are helpful for forecasting the lengths of the forward and backward paths deteriorate the ability to learn such dependencies shorter paths of forward and backward signals make it easier to learn dependencies for more accurate dependencies scaled dot product attention is embedded in sa lstm vaswani et al 2017 as follows 11 attention q k v s o f t max q k t d v where d is the dimension of keys q k and v represent the matrix of query key and value respectively q is the previous decoder state k and v are the hidden states of the encoder the scaled dot product attention mechanism is faster than the additive attention mechanism in the training process through linear transformation scaled dot product attention is used for queries keys and values the relevance is computed in the process of scaled dot product attention when q k v special multi head attention is named self attention as illustrated in fig 2 two branches are designed where the upper branch is for the long previous time series and the lower branch is for the short previous time series the features of either branch are deeply shared with the other two sets of high level features are then aggregated to generate the final forecast results at the upper branch lstm is used first and then a self attention module is employed note that we utilize a self attention module without a sequence transduction encoder and decoder the encoder reads and encodes the data source into a fixed length vector and then the decoder outputs results from the encoded vector the encoder and decoder are jointly trained to maximize the correct probability the potential assumption with the encoder and decoder is that the neural network needs to compress all the necessary information of the data source into a fixed length vector fortunately our inputs have a fixed length bahdanau et al 2014 therefore the encoder and decoder are abandoned in the lower branch lstm with linear transformation is first used then the correlation between the output and short previous time series is computed and yields a t t dimensional relevance matrix the relevance matrix multiplies the output of the first step and gives a t 1 vector after linear transformation the output is concatenated with the result of the first branch finally the output runs through a tangent and a linear transformation module to generate the final forecast all of the weights can be updated automatically in the training process 2 3 performance metrics the performances are evaluated by the root mean square error rmse mean absolute error mae nash sutcliffe efficiency coefficient nse nse of logarithmic flow nse in and correlation coefficient r these criteria are defined as follows 12 rmse 1 n i 1 n q i o q i f 2 13 mae 1 n i 1 n q i o q i f 14 nse 1 i 1 n q i o q i f 2 i 1 n q i o q o 2 15 n s e i n 1 i 1 n i n q i o i n q i f 2 i 1 n i n q i o i n q o 2 16 r i 1 n q i o q o q i f q f i 1 n q i o q o 2 i 1 n q i f q f 2 where n is the number of data pairs q i o and q i f are the observation and forecast of runoff at time i respectively q o and q f are the mean observed and forecasted runoff respectively a significance test also needs to be performed to confirm the correlation between two variables using the p value pearson correlation test when the rmse and mae approach zero and r approaches one the forecast model prediction is accurate nse is in the range of to one when the value of nse is above 0 9 the fit of the model is perfect a value of nse in the range 0 8 0 9 is considered to indicate a fairly good model while a value of nse less than 0 8 indicates that the model is unsatisfactory shamseldin 1997 nse in reflects the accuracy at base flow in runoff forecasting gao et al 2013 3 case study 3 1 study area and data the mississippi river is the fourth longest river in the world and the longest river in the united states of america the basin area of the mississippi river is 3 22 million km2 and contains 41 of the land area of the country the missouri river and ohio river divide the mississippi river into an upper reach a middle reach and a lower reach the annual average runoff of the mississippi river is 16 792 m3 s and the peak runoff is 86 791 m3 s although the american government has constructed many flood control projects historically the mississippi river basin floods often therefore it is significant to forecast the daily runoff of the mississippi river in this study data from eight stations grand rapids aitkin st louis thebes near ulm near wolf point garrison dam and sioux city on the main stream and tributary of the mississippi river were collected to verify the models the grand rapids and aitkin stations are on the upper reach of the mississippi river and the st louis and thebes stations are on the middle reach of the mississippi river the near ulm near wolf point garrison dam and sioux city stations are on the missouri river a tributary of the mississippi river fig 3 shows the location of these eight stations as well as the mississippi river basin including the main stream and part of the tributary stream of the mississippi river daily runoff data m3 s at these eight stations were obtained from the united states geological survey https waterdata usgs gov nwis sw the data periods for the grand rapids aitkin st louis and thebes stations ranged from october 3 1948 to december 31 1997 17 987 days from march 1 1945 to november 2 1991 17 048 days from december 1 1931 to september 16 2002 25 858 days and from october 1 1939 to october 21 2000 22 293 days respectively the data periods for the near ulm near wolf point garrison dam and sioux city stations ranged from july 31 1957 to december 19 1986 10 734 days from january 19 1951 to december 10 1986 13 110 days from october 1 1969 to march 12 2009 14 408 days and from march 9 1961 to december 18 1992 11 608 days respectively a statistical description of runoff at these eight stations is shown in table 1 the largest range between the maximum and the minimum runoff was at the st louis station when considering the ratio between the standard deviation and average runoff the aitkin station had the largest runoff variation and the garrison dam station was the most stable station in this study the first 80 of runoff data in each station are used for training and the rest of the data are used for validation 3 2 open source software this study relies on open source libraries including scipy numpy van der walt et al 2011 math and pandas mckinney 2010 statsmodels seabold and perktold 2010 is used to compute the acf and pacf pytorch 1 2 0 and python 3 6 are employed for deep learning methods including lstm and sa lstm the xgboost lightgbm and sklearn pedregosa et al 2011 packages are used to implement xgboost lightgbm and svr respectively matplotlib hunter 2007 is used to draw figures the packages are installed by using thecommand line on ubuntu 16 04 system all the experiments are conducted on a workstation equipped with an intel i7 4960x cpu a 32 gb rams and a nvidia gtx geforce 1080 ti gpu 3 3 forecasting factor selection the selection of input variables is vital for runoff forecasting models in this paper pacf and autocorrelation function acf are used to calculate the correlation between runoff and the potential forecasting factors the partial autocorrelation functions and autocorrelation functions with respect to various lag numbers at eight stations are shown in figs 4 and 5 respectively different stations have significant partial autocorrelations at different time lags grand rapids aitkin st louis thebes near ulm station near wolf point station garrison dam station and sioux city have lag numbers of 2 3 5 5 5 2 2 and 4 respectively at the thebes station fig 4 d runoff demonstrates a high autocorrelation at time lag 5 which is also the longest time lag considering pacf and acf comprehensively for the convenience of comparison we use the longest time lag i e 5 days as the input of the models 3 4 network structure optimization the learning rate of our model is set to 0 01 as in xie et al 2019 and the maximum number of iterations is 10 000 adam optimizer is chosen as the optimizer andrychowicz et al 2016 rana et al 2019 the numbers of hidden neurons and hidden layers as well as the short lag time are optimized in turn first the model is initialized with an input of 5 as the long lag time based on the analysis in section 3 3 and then forecasting results for different numbers of days ahead with hidden neurons ranging from 3 to 15 are calculated the results of the grand rapids station are shown in table 2 as an example when the number of hidden neurons is 5 r is 0 905 nse is 0 803 nse in is 0 803 rmse is 8 664 and mae is 5 883 the r nse and nse in values are the highest and the rmse and mae are the lowest in all of the results in short the sa lstm model has the best performance with 5 hidden neurons therefore the number of hidden neurons is set to 5 in the sa lstm model then the number of hidden layers is varied from 1 to 6 with an interval of 1 the seven day ahead forecast performances with different numbers of hidden layers at the grand rapids station are shown in table 3 when the number of hidden layers is 3 r is 0 905 nse is 0 803 nse in is 0 803 rmse is 8 664 and mae is 5 883 the r nse nse in and rmse indicate the best performance among all of the structures the mae indicates the second best performance after the structure with 6 hidden layers therefore the number of hidden layers is set to 3 finally the short lag time in the sa lstm model is set from 2 days to 4 days and the performances of the seventh day forecasts at the grand rapids aitkin st louis and thebes stations are shown in table 4 when the short lag time is 3 days the seventh day forecast has the highest accuracy at the aitkin st louis and thebes stations at the grand rapids station the short lag time of 3 day has the best performance according to r and rmse however the short lag time of 4 day has the best performance according to nse nse in and mae consequently the short lag time is set to 3 day 4 results and discussion in section 4 1 the sa lstm model is compared with other models the comparisons of the forecasting errors of peak and base flows on the seventh day are shown in sections 4 2 and 4 3 respectively in section 4 4 the epoch v s loss and epoch v s nse of sa lstm model are shown meanwhile twelve time series are resampled at each station to verify the performance of sa lstm model further are shown in section 4 5 4 1 comparisons of runoff forecasting performance between sa lstm and other models to verify the performance of the sa lstm model four forecast models namely the lstm svr xgboost and lightgbm models are compared in this study xgboost and lightgbm are set with the default parameters in the paper svr in the scikit learn package is used with the default parameters 4 1 1 comparisons with performance metrics the performance comparisons at the eight stations are shown in tables 5 12 with various evaluation criteria the rmse and mae of the svr model are several times greater than those of the other methods at the eight stations at the grand rapids station the rmse of the sa lstm model for the 7 day ahead forecast is 7 9 and 9 3 smaller than that of the lightgbm and xgboost models respectively at the near wolf point and the sioux city stations the rmses of the sa lstm for the 7 day ahead forecast are 6 9 and 5 4 smaller than that of the lightgbm model respectively at the other five stations the rmses of the sa lstm model are more than 10 lower than those of the lightgbm and xgboost models compared with the lstm models the maes of sa lstm are 5 1 31 0 13 8 16 5 6 7 15 1 16 0 and 13 6 smaller at the eight stations furthermore the rmses are 7 8 22 9 11 2 13 7 11 7 16 8 9 5 and 19 1 smaller at the eight stations this indicates that the self attention mechanism significantly increases the total forecast accuracy the values of r are smaller than 0 9 at the seventh day using the lightgbm xgboost and svr models at seven stations the r values of all stations are smaller than 0 9 at seventh day using lstm model in comparison the r values of sa lstm are higher than 0 9 on the seventh day at seven stations the p values of all of the models are close to zero the nse of svr becomes negative on later forecast days the values of nse using lightgbm xgboost and lstm significantly decrease after the third day by incorporating the short time sequence with a self attention mechanism sa lstm performs much better than the other models for the first four days of lead time the nse values are greater than 0 9 at seven stations in general the improvement of nse by the sa lstm model can reach 15 compared with the lstm model in short these results demonstrate that sa lstm improves the accuracy of 1 7 day ahead forecasts as the lead time increases the accuracy of all models declines but the accuracy of sa lstm decreases much less than that of the other models the superiority of sa lstm over lstm verifies the importance of short lag time for forecasting which also means that there is high interdependency between short previous time series 4 1 2 comparisons of forecast results for different days ahead figs 6 11 show the forecasts of all models for one day the four days and the seven days ahead at the eight stations more details of the peak flow forecast results are shown in the local enlarged image figs 6 and 7 show that all the models perform well in the forecast and fit of most of the runoff forecasts except for the svr model fig 6 a and b show that the forecasting capability of the lstm model is inferior to that of sa lstm at the end of the runoff series this result demonstrates the superiority of the self attention mechanism figs 8 and 9 show the predicted fourth day forecasting results of all models at the eight stations it is obvious that the predicted results are poorer than the results for the first day and the svr model also has the worst performance among all models the results of all models have a slight lag behind the observed value fig 8 a illustrates that the lstm model has a poor forecasting ability at the end of the runoff series the sa lstm lstm lightgbm and xgboost models perform well in forecasting fig 8 b shows that the lstm model performs worse than sa lstm at the start of the runoff series fig 8 c and d show that all models have great abilities in capturing most of the runoff except for the svr model fig 9 c and d show that the lightgbm and xgboost models perform worse than sa lstm model figs 10 and 11 show the forecasting results of all models for seven days ahead at the eight stations it is obvious that the forecasting results are the worst compared to the other days ahead the runoff series of all models have a relatively large lag behind the observed runoff series the lag times are ranked as follows sa lstm lightgbm xgboost lstm svr this ranking means that the sa lstm model has the best forecast ability fig 10 a illustrates that the svr model underestimates the value of runoff most of the time many forecast results are negative values which are set to zero in fig 10 a the svr model overestimates most of the runoff as shown in fig 10 b d and fig 11 other models have good performances for most times as shown in figs 10 and 11 4 1 3 comparisons of scatter plots on the seventh day fig 12 shows forecasted and observed daily runoff scatter plots of the five models on the seventh day at the eight stations fig 12 a h shows the grand rapids aitkin st louis thebes near ulm near wolf point garrison dam and sioux city stations respectively it is obvious that the fitting line of the sa lstm model in fig 12 is closest to the ideal fitting line the fitting line of the svr model has the largest errors among these models at each station all four models overestimate the runoff when runoff is low and underestimate the runoff when runoff is high when the value of the base flow is low the sa lstm model outperforms the baseline models as shown in fig 12 a when the value of the base flow is high the sa lstm model demonstrates significant improvements in daily runoff forecasting compared with the other models as shown in fig 12 b d fig 12 g shows that lstm and lightgbm models overestimate most of runoff 4 2 comparisons of forecast errors of peak values peak flow forecasting is important in water resource management torrential rain and snow melting in a short time can bring about peak flows notably peak flow is difficult to forecast tables 13 20 show the 15 peak flow forecast results of the five models on the seventh day at each station the absolute relative error are is applied to evaluate the performance of the models 17 are q f q o q o where q o and q f are the observation and forecast of runoff respectively tables 13 17 and 19 20 demonstrate that sa lstm has the best accuracy among all the models according to the mean are the mean ares of the sa lstm model are 22 2 14 6 27 5 18 4 18 3 9 7 and 8 3 at the grand rapids aitkin st louis thebes near ulm garrison dam and sioux city stations respectively the mean are values of the sa lstm model are reduced by approximately 6 75 at the eight stations compared with the mean are values of the lstm model among all models the sa lstm model has the lowest maximum are and minimum are at the grand rapids station at the aitkin and thebes stations among all models the sa lstm model has the lowest minimum are and the second lowest maximum are next to the svr model the performance of the svr model is unstable at the near wolf point station the svr model has the best performance however the mean ares of the svr model at the grand rapids and sioux city station are the highest and are much greater than the mean are values of the other models the performances of lightgbm and xgboost are stable according to figs 6 11 sa lstm has a better ability than lstm svr lightgbm and xgboost to capture the peak values for example the local enlarged image of fig 6 b shows that the svr and lstm models underestimate the peak values and the xgboost and lightgbm models slightly overestimate the peak flow the local enlarged image of fig 6 d reveals that the sa lstm model is effective in peak flow forecasts when the basic runoff is high while the lightgbm xgboost and svr models underestimate the peak values the local enlarged images of fig 8 c and d reveal that all models underestimate the value of the extreme peak flow and the forecast result of the sa lstm model is closest to the observed value because of overestimating the most of runoff svr model fit peak value well as shown in fig 11 b 4 3 comparisons of base flow forecast accuracy it can be seen in figs 6 and 7 d that although all the models perform well in runoff forecasting the svr model has a poor fit at base flow fig 8 illustrates that the sa lstm lstm lightgbm and xgboost models perform well at base flow fig 8 a shows that the svr model underestimates the base flow fig 8 b d and fig 9 show that the forecast result of the svr model is obviously higher than the observed value at base flow thus the svr model has an insufficient runoff forecasting ability fig 10 a shows that the svr model underestimates most of the base flow and many forecast results are negative the negative values are set to zero fig 10 b d and fig 11 show that the svr model overestimates the runoff at base flow in contrast the sa lstm model is the most stable and accurate model and fits most of the series at base flow as demonstrated in fig 10 all models have a slight lag behind the observed runoff at base flow as show in fig 10 the value of nse in is above 0 9 in the first two days of forecasting using the lightgbm xgboost and lstm models at the seven stations except the near wolf point station however the value of nse in decreases rapidly on the seventh day the values of nse in using the xgboost and lightgbm models at the eight stations are less than 0 8 and the value of the nse in using the lstm model is less than 0 6 at the aitkin station as shown in table 6 the sa lstm model has better performances than the other models in the first five days the nse in values are greater than 0 9 at the four stations as shown in tables 6 8 and table 12 the nse in values of all stations are greater than 0 7 for the 1 7 day forecast on the seventh day the attention mechanism enhances nse in by approximately 17 2 at the eight stations compared with the lstm model the improvement is higher than that of nse this reflects that the attention mechanism improves the performance for the base flow 4 4 epoch v s loss nse the epoch v s loss and epoch v s nse are shown in fig 13 and 14 the loss decreases fast within the first one hundred training epochs after the first one hundred training epochs the loss reduces slowly and the value of loss reaches 12 69 eventually the nse increases fast within the first forty training epochs after the first forty training epochs the nse increases slowly and the value of nse reaches 0 97 eventually 4 5 extended experiments data are resampled from the whole training data the first time series is the first half of the dataset the remaining time series are sequentially added with the following one twelfth of dataset finally we get 12 new training series based on a time series the seven day ahead mean performances of twelve time series at each station are shown in table 21 the mean performance of sa lstm is better than that of other four models at each station the values of r are all above 0 85 the nse of sa lstm model are above 0 8 at half of the stations the nse and nse in of other four models are below 0 7 mostly compared with lstm model sa lstm model decrease at least 2 6 value of rmses the value of maes are reduced 11 3 21 4 15 3 13 4 5 9 9 7 2 2 3 0 at eight stations respectively the svr model has the worst performance although the values of r are all above 0 8 the values of nse and nse in become negative at half of the stations the rmses and maes of svr model are much greater than that of other models the seven day ahead forecast performance of twelve time series at grand rapid station are shown in table 22 as an example the sa lstm model has best accuracy at every datasets and more than half of nse and nse in are greater than 0 8 the rmse of sa lstm model are reduced 4 at least compared with that of the second best model the mae of sa lstm model are more than 10 smaller than other models at five datasets 5 conclusions owing to the nonlinearity of runoff conventional empirical models for runoff forecasting require the integration of more information and techniques in this paper a short lag time is introduced and a more effective model is developed to enhance the performance of daily runoff forecasting based on the lstm model the sa lstm model employs a self attention mechanism to obtain more interdependent information within the short previous time series for runoff forecasting the sa lstm model is verified for daily runoff forecasting at eight stations on the mississippi river the results of this model are compared with those of the lightgbm xgboost lstm and svr models in 1 to 7 day ahead forecasts the experimental results indicate that the sa lstm model improves the total forecast accuracy and yields improved performances for base and peak values compared to the other models the effectiveness of the sa lstm model stems from 1 the introduction and exploitation of short previous time series which are highly correlated with forecasting result and 2 the exploitation of the short previous time series with a self attention mechanism however middle and long term forecasting is still difficult in hydrology in future studies we will try to extend the forecast time and improve the accuracy of peak base flows credit authorship contribution statement xi chen writing review editing methodology software conceptualization jiaxu huang writing original draft visualization data curation conceptualization zhen han software validation hongkai gao writing review editing methodology min liu writing review editing zhiqiang li writing review editing xiaoping liu writing review editing qingli li writing review editing honggang qi writing review editing yonggui huang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is supported by the national key research and development project of china 2017yfe0100700 in part by the national natural science foundation of china grant no 41871340 41801036 41911530191 in part by the shanghai key laboratory of multidimensional information processing east china normal university no 2020key003 and in part by the institute of eco chongming ecnu iec 201902 the fundamental research funds for the central universities 
5282,it is still very challenging to enhance the accuracy and stability of daily runoff forecasts especially several days ahead owing to the non linearity of the forecasted processes here we hypothesize that short lag time has a significant impact on forecasting results thus we incorporate short previous time steps into long short term memory lstm and develop the self attentive long short term memory sa lstm in sa lstm the self attention mechanism is used to model interdependencies within short previous time steps sa lstm is evaluated at eight runoff datasets the experimental results demonstrate that compared with state of art benchmark models sa lstm achieves the best performance the rmses of sa lstm are at least 2 3 smaller than that of the second best model at the seventh day the nses and nse in of sa lstm are at least 4 6 and 6 4 higher than those of the second best model at the seventh day furthermore sa lstm also surpasses the baseline methods for base mean and peak flows the superiority of sa lstm can be attributed to its exploitation of information in short lag time keywords long short term memory runoff forecast attention self attentive long short term memory 1 introduction runoff forecasting is of great importance for water resource management such as flood protection di baldassarre et al 2010 blöschl et al 2019 drought mitigation zhang et al 2012 yuan et al 2019 reservoir operation chen et al 2016 wu et al 2018 and ecosystem conservation palmer and ruhi 2019 a runoff forecasting model is a simplified representation of a real hydrologic system sorooshian et al 2008 hrachowitz et al 2013 soulsby et al 2016 gao et al 2018 2019 runoff forecasting models are mainly classified into three categories conceptual models physically based models and empirical models beven 2011 liu et al 2019 both conceptual models and physically based models are process based models which to some extent explicitly describe and quantify hydrological processes and the water balance hundecha and bárdossy 2004 umakhanthan and ball 2005 yu et al 2006 samaniego et al 2010 birkel and soulsby 2015 seibert et al 2018 process based models have long played a dominant role in the hydrological community because of their transparent model structure and parameterization as well as their satisfactory performance in well gauged basins and model transferability yang and musiake 2003 sivapalan and blöschl 2017 gao et al 2017 unfortunately process based models have many complex limiting factors such as slope and soil moisture sorooshian and gupta 1995 thus their performance is restricted to solve these problems empirical models also known as black box models are applied in runoff forecasting empirical models estimate input output relationships from a statistical viewpoint using data driven methods shen 2018 liu et al 2018 at the beginning of the development of empirical models linear data driven models such as autoregressive moving average arma and autoregressive integrated moving average arima were always used to predict runoff time series carlson et al 1970 salas et al 1985 montanari et al 1997 noakes et al 1985 these models are linearly consequently they have relatively more interpretability and less flexibility james et al 2013 with the development of machine learning many nonlinear models have been employed in runoff forecasting typical nonlinear models include but are not limited to artificial neural network ann riad et al 2004 khalil et al 2005 support vector machine svm liong and sivapragasam 2002 asefa et al 2006 papacharalampous et al 2019 support vector regression svr maity et al 2010 luo et al 2019 random forest rf huang et al 2019 tyralis et al 2019 quantile regression forests qrf whan and schmeits 2018 bhuiyan et al 2018 2019 extreme gradient boosting xgboost zhang et al 2019 cisty and soldanova 2018 kinematic wave kw adaptive neuro fuzzy inference system anfis and nonlinear prediction nlp chua and wong 2011 laio et al 2003 he et al 2014 feed forward back propagation neural network ffbp generalized regression neural networks grnn conjugate gradient cascade correlation and radial basis function rbf neural network methods mehr et al 2015 kişi 2007 yaseen et al 2016 although these methods have achieved some improvements over linear data driven models they still belong to the shallow learning category and have limited practicability xie et al 2019 wavelet based methods are another popular tool in runoff forecasting with wavelet a time series can be decomposed into high and low frequency sub time series which offer a coherent structure to a data driven model quilty and adamowski 2018 thus wavelet based time series forecasts become popular unfortunately some wavelet based forecasts cannot be used in real world scenarios because they adopt future data to perform forecasting du et al 2017 to dispel doubt wavelet data driven forecasting framework wddff is proposed to demonstrate the superiority of wavelet by employing boundary corrected wavelet and scaling coefficients quilty and adamowski 2018 ensemble wavelet stochastic data driven forecasting framework is proposed by using variable selection methods and data driven models including multiple linear regression and extreme learning machines quilty et al 2019 in the last several years deep learning has been applied to runoff forecasting and has achieved considerable success shen 2018 assem et al 2017 established deep convolutional neural networks to forecast water flow and water level based on 30 year maximum temperature minimum temperature and run off data sets tian et al 2018 compared four recurrent neural networks rnns in runoff forecasting they found that the nonlinear autoregressive exogenous inputs neural network narx and long short term memory lstm perform better than the other two models especially in small catchments in xie et al 2019 variational mode decomposition was used to decompose a daily runoff series and a partial autocorrelation function pacf was applied to determine the input variables of each subsequence then the improved particle swarm optimization algorithm was combined with the deep belief network model to predict each subsequence and finally reconstruct the ensemble forecasting result although deep learning models greatly improve runoff forecasting accuracy the temporal dependencies between elements in a time series are still not well exploited temporal dependencies reflect the relevance of the variation in time series data across different time frequencies thus temporal dependencies could enhance model accuracy song et al 2018 based on the hypothesis that short previous time steps are more correlated than long previous time steps with prediction results short lag time are introduced to our new model i e self attentive long short term memory sa lstm where lag time is the number of previous time steps using as inputs ni et al 2020 in addition self attention mechanisms are integrated into sa lstm to exploit the temporal dependencies within short lag time dependencies at distant positions are difficult to learn hochreiter et al 2001 to tackle this problem self attention mechanisms have been utilized in sequential modelling in other fields such as machine translation and image captioning luong et al 2015 bahdanau et al 2014 these mechanisms explicitly yield global dependencies between input and output vaswani et al 2017 among these attention mechanisms the self attention mechanism is highlighted because it can compute representations of its input and output relating different positions of a single sequence for example global and local attention can be combined with lstm and rnn for machine translation and abstractive summarization zoph and knight 2016 luong et al 2015 mashlakov et al 2019 applied two attention heads to rnn for battery state of charge forecasting however there have been few works using self attention mechanisms for runoff forecasting until now in this paper the sa lstm model is proposed for runoff forecasting the main contributions of sa lstm are listed as follows 1 dual inputs i e long previous time series and short previous time series are employed to enhance the forecast accuracy short previous time series are integrated into lstm based runoff forecasting to strengthen the important information of short previous time series 2 the self attention mechanism without decoder and encoder is introduced to model temporal dependencies between previous time steps for improving runoff forecasting 3 sa lstm is compared with four benchmark models i e the svr lstm xgboost chen and guestrin 2016 and light gradient boosting machine lightgbm ke et al 2017 models through forecasting runoff with 1 7 days of lead time the experimental results including both the base flow and peak flow results verify the effectiveness of sa lstm the next section introduces the baseline methods used in this paper section 3 shows a case study and section 4 presents the results and discussions conclusions are given in section 5 2 methodologies 2 1 long short term memory lstm suppose w f w i and w o represent the weight matrices from the forget input and output gates to the input u f u i and u o represent the matrix of weight from forget input and output gates to the hidden b f b i and b o represent the forget input and output gate bias vectors then the lstm unit can be written as follows 1 f t σ w f x t u f h t 1 b f 2 i t σ w i x t u i h t 1 b i 3 o t σ w o x t u o h t 1 b o 4 c t tanh w c x t u c h t 1 b c 5 c t f t c t 1 i t c t 6 h t o t tanh c t where σ is a nonlinear activation function such as the sigmoid tanh and rectified linear unit relu relu is applied in this study f i o and c are the forget input output gates and the cell state vectors at time t respectively all of those variables have the same size as the cell output vector h is the element wise multiplication of the vectors fig 1 shows the structure of lstm 2 2 self attentive long short term memory sa lstm short previous time series have a larger impact than the long counterparts on forecasting results and thus will be more helpful for forecasting consequently sa lstm takes short previous time series in parallel to long previous time series as input suppose the lag number of the long lag time is t the lag number of the short lag time is t where t t and t is a hyperparameter the short previous time series are mapped to long previous temporal representation by correlation then the most representative features and temporal patterns of short previous time series are captured in this study the self attention mechanism is applied in runoff forecasting to calculate the interdependency of runoff on different days self attention was originally designed for natural language processing tasks such as machine translation and abstractive summarization paulus et al 2017 the self attention module has a strong capability to extract features and capture the dependencies between different time steps for each learned representation of a univariate series the self attention module learns its relationship with other learned representations including itself this mechanism allows the hidden layer to focus on critical information by assigning different attention weights to the hidden layer of the neural network in general the self attention function can be described as mapping a query and a set of key value pairs to an output where the query keys values and the output are all vectors the output is computed as a weighted sum of the values where the weight for each position is computed as the inner product between the query and keys at every other position in the time series suppose the hidden layer output vector of lstm h h 1 h 2 h t is selected as the input of the self attention mechanism the context vector v t is the sum of h i multiplication α i where α i is the attention weight of h i 7 v t i 1 t α i h i the attention weight α i of each hidden layer output vector is computed by 8 α i exp e i i 1 t exp e i 9 i 1 t α i 1 10 e i tanh w h h i b h e i 1 1 where w h and b h are the weight matrix and the bias of h i respectively the value of w h and b h are adjusted in training process although learning dependencies within the time sequence are helpful for forecasting the lengths of the forward and backward paths deteriorate the ability to learn such dependencies shorter paths of forward and backward signals make it easier to learn dependencies for more accurate dependencies scaled dot product attention is embedded in sa lstm vaswani et al 2017 as follows 11 attention q k v s o f t max q k t d v where d is the dimension of keys q k and v represent the matrix of query key and value respectively q is the previous decoder state k and v are the hidden states of the encoder the scaled dot product attention mechanism is faster than the additive attention mechanism in the training process through linear transformation scaled dot product attention is used for queries keys and values the relevance is computed in the process of scaled dot product attention when q k v special multi head attention is named self attention as illustrated in fig 2 two branches are designed where the upper branch is for the long previous time series and the lower branch is for the short previous time series the features of either branch are deeply shared with the other two sets of high level features are then aggregated to generate the final forecast results at the upper branch lstm is used first and then a self attention module is employed note that we utilize a self attention module without a sequence transduction encoder and decoder the encoder reads and encodes the data source into a fixed length vector and then the decoder outputs results from the encoded vector the encoder and decoder are jointly trained to maximize the correct probability the potential assumption with the encoder and decoder is that the neural network needs to compress all the necessary information of the data source into a fixed length vector fortunately our inputs have a fixed length bahdanau et al 2014 therefore the encoder and decoder are abandoned in the lower branch lstm with linear transformation is first used then the correlation between the output and short previous time series is computed and yields a t t dimensional relevance matrix the relevance matrix multiplies the output of the first step and gives a t 1 vector after linear transformation the output is concatenated with the result of the first branch finally the output runs through a tangent and a linear transformation module to generate the final forecast all of the weights can be updated automatically in the training process 2 3 performance metrics the performances are evaluated by the root mean square error rmse mean absolute error mae nash sutcliffe efficiency coefficient nse nse of logarithmic flow nse in and correlation coefficient r these criteria are defined as follows 12 rmse 1 n i 1 n q i o q i f 2 13 mae 1 n i 1 n q i o q i f 14 nse 1 i 1 n q i o q i f 2 i 1 n q i o q o 2 15 n s e i n 1 i 1 n i n q i o i n q i f 2 i 1 n i n q i o i n q o 2 16 r i 1 n q i o q o q i f q f i 1 n q i o q o 2 i 1 n q i f q f 2 where n is the number of data pairs q i o and q i f are the observation and forecast of runoff at time i respectively q o and q f are the mean observed and forecasted runoff respectively a significance test also needs to be performed to confirm the correlation between two variables using the p value pearson correlation test when the rmse and mae approach zero and r approaches one the forecast model prediction is accurate nse is in the range of to one when the value of nse is above 0 9 the fit of the model is perfect a value of nse in the range 0 8 0 9 is considered to indicate a fairly good model while a value of nse less than 0 8 indicates that the model is unsatisfactory shamseldin 1997 nse in reflects the accuracy at base flow in runoff forecasting gao et al 2013 3 case study 3 1 study area and data the mississippi river is the fourth longest river in the world and the longest river in the united states of america the basin area of the mississippi river is 3 22 million km2 and contains 41 of the land area of the country the missouri river and ohio river divide the mississippi river into an upper reach a middle reach and a lower reach the annual average runoff of the mississippi river is 16 792 m3 s and the peak runoff is 86 791 m3 s although the american government has constructed many flood control projects historically the mississippi river basin floods often therefore it is significant to forecast the daily runoff of the mississippi river in this study data from eight stations grand rapids aitkin st louis thebes near ulm near wolf point garrison dam and sioux city on the main stream and tributary of the mississippi river were collected to verify the models the grand rapids and aitkin stations are on the upper reach of the mississippi river and the st louis and thebes stations are on the middle reach of the mississippi river the near ulm near wolf point garrison dam and sioux city stations are on the missouri river a tributary of the mississippi river fig 3 shows the location of these eight stations as well as the mississippi river basin including the main stream and part of the tributary stream of the mississippi river daily runoff data m3 s at these eight stations were obtained from the united states geological survey https waterdata usgs gov nwis sw the data periods for the grand rapids aitkin st louis and thebes stations ranged from october 3 1948 to december 31 1997 17 987 days from march 1 1945 to november 2 1991 17 048 days from december 1 1931 to september 16 2002 25 858 days and from october 1 1939 to october 21 2000 22 293 days respectively the data periods for the near ulm near wolf point garrison dam and sioux city stations ranged from july 31 1957 to december 19 1986 10 734 days from january 19 1951 to december 10 1986 13 110 days from october 1 1969 to march 12 2009 14 408 days and from march 9 1961 to december 18 1992 11 608 days respectively a statistical description of runoff at these eight stations is shown in table 1 the largest range between the maximum and the minimum runoff was at the st louis station when considering the ratio between the standard deviation and average runoff the aitkin station had the largest runoff variation and the garrison dam station was the most stable station in this study the first 80 of runoff data in each station are used for training and the rest of the data are used for validation 3 2 open source software this study relies on open source libraries including scipy numpy van der walt et al 2011 math and pandas mckinney 2010 statsmodels seabold and perktold 2010 is used to compute the acf and pacf pytorch 1 2 0 and python 3 6 are employed for deep learning methods including lstm and sa lstm the xgboost lightgbm and sklearn pedregosa et al 2011 packages are used to implement xgboost lightgbm and svr respectively matplotlib hunter 2007 is used to draw figures the packages are installed by using thecommand line on ubuntu 16 04 system all the experiments are conducted on a workstation equipped with an intel i7 4960x cpu a 32 gb rams and a nvidia gtx geforce 1080 ti gpu 3 3 forecasting factor selection the selection of input variables is vital for runoff forecasting models in this paper pacf and autocorrelation function acf are used to calculate the correlation between runoff and the potential forecasting factors the partial autocorrelation functions and autocorrelation functions with respect to various lag numbers at eight stations are shown in figs 4 and 5 respectively different stations have significant partial autocorrelations at different time lags grand rapids aitkin st louis thebes near ulm station near wolf point station garrison dam station and sioux city have lag numbers of 2 3 5 5 5 2 2 and 4 respectively at the thebes station fig 4 d runoff demonstrates a high autocorrelation at time lag 5 which is also the longest time lag considering pacf and acf comprehensively for the convenience of comparison we use the longest time lag i e 5 days as the input of the models 3 4 network structure optimization the learning rate of our model is set to 0 01 as in xie et al 2019 and the maximum number of iterations is 10 000 adam optimizer is chosen as the optimizer andrychowicz et al 2016 rana et al 2019 the numbers of hidden neurons and hidden layers as well as the short lag time are optimized in turn first the model is initialized with an input of 5 as the long lag time based on the analysis in section 3 3 and then forecasting results for different numbers of days ahead with hidden neurons ranging from 3 to 15 are calculated the results of the grand rapids station are shown in table 2 as an example when the number of hidden neurons is 5 r is 0 905 nse is 0 803 nse in is 0 803 rmse is 8 664 and mae is 5 883 the r nse and nse in values are the highest and the rmse and mae are the lowest in all of the results in short the sa lstm model has the best performance with 5 hidden neurons therefore the number of hidden neurons is set to 5 in the sa lstm model then the number of hidden layers is varied from 1 to 6 with an interval of 1 the seven day ahead forecast performances with different numbers of hidden layers at the grand rapids station are shown in table 3 when the number of hidden layers is 3 r is 0 905 nse is 0 803 nse in is 0 803 rmse is 8 664 and mae is 5 883 the r nse nse in and rmse indicate the best performance among all of the structures the mae indicates the second best performance after the structure with 6 hidden layers therefore the number of hidden layers is set to 3 finally the short lag time in the sa lstm model is set from 2 days to 4 days and the performances of the seventh day forecasts at the grand rapids aitkin st louis and thebes stations are shown in table 4 when the short lag time is 3 days the seventh day forecast has the highest accuracy at the aitkin st louis and thebes stations at the grand rapids station the short lag time of 3 day has the best performance according to r and rmse however the short lag time of 4 day has the best performance according to nse nse in and mae consequently the short lag time is set to 3 day 4 results and discussion in section 4 1 the sa lstm model is compared with other models the comparisons of the forecasting errors of peak and base flows on the seventh day are shown in sections 4 2 and 4 3 respectively in section 4 4 the epoch v s loss and epoch v s nse of sa lstm model are shown meanwhile twelve time series are resampled at each station to verify the performance of sa lstm model further are shown in section 4 5 4 1 comparisons of runoff forecasting performance between sa lstm and other models to verify the performance of the sa lstm model four forecast models namely the lstm svr xgboost and lightgbm models are compared in this study xgboost and lightgbm are set with the default parameters in the paper svr in the scikit learn package is used with the default parameters 4 1 1 comparisons with performance metrics the performance comparisons at the eight stations are shown in tables 5 12 with various evaluation criteria the rmse and mae of the svr model are several times greater than those of the other methods at the eight stations at the grand rapids station the rmse of the sa lstm model for the 7 day ahead forecast is 7 9 and 9 3 smaller than that of the lightgbm and xgboost models respectively at the near wolf point and the sioux city stations the rmses of the sa lstm for the 7 day ahead forecast are 6 9 and 5 4 smaller than that of the lightgbm model respectively at the other five stations the rmses of the sa lstm model are more than 10 lower than those of the lightgbm and xgboost models compared with the lstm models the maes of sa lstm are 5 1 31 0 13 8 16 5 6 7 15 1 16 0 and 13 6 smaller at the eight stations furthermore the rmses are 7 8 22 9 11 2 13 7 11 7 16 8 9 5 and 19 1 smaller at the eight stations this indicates that the self attention mechanism significantly increases the total forecast accuracy the values of r are smaller than 0 9 at the seventh day using the lightgbm xgboost and svr models at seven stations the r values of all stations are smaller than 0 9 at seventh day using lstm model in comparison the r values of sa lstm are higher than 0 9 on the seventh day at seven stations the p values of all of the models are close to zero the nse of svr becomes negative on later forecast days the values of nse using lightgbm xgboost and lstm significantly decrease after the third day by incorporating the short time sequence with a self attention mechanism sa lstm performs much better than the other models for the first four days of lead time the nse values are greater than 0 9 at seven stations in general the improvement of nse by the sa lstm model can reach 15 compared with the lstm model in short these results demonstrate that sa lstm improves the accuracy of 1 7 day ahead forecasts as the lead time increases the accuracy of all models declines but the accuracy of sa lstm decreases much less than that of the other models the superiority of sa lstm over lstm verifies the importance of short lag time for forecasting which also means that there is high interdependency between short previous time series 4 1 2 comparisons of forecast results for different days ahead figs 6 11 show the forecasts of all models for one day the four days and the seven days ahead at the eight stations more details of the peak flow forecast results are shown in the local enlarged image figs 6 and 7 show that all the models perform well in the forecast and fit of most of the runoff forecasts except for the svr model fig 6 a and b show that the forecasting capability of the lstm model is inferior to that of sa lstm at the end of the runoff series this result demonstrates the superiority of the self attention mechanism figs 8 and 9 show the predicted fourth day forecasting results of all models at the eight stations it is obvious that the predicted results are poorer than the results for the first day and the svr model also has the worst performance among all models the results of all models have a slight lag behind the observed value fig 8 a illustrates that the lstm model has a poor forecasting ability at the end of the runoff series the sa lstm lstm lightgbm and xgboost models perform well in forecasting fig 8 b shows that the lstm model performs worse than sa lstm at the start of the runoff series fig 8 c and d show that all models have great abilities in capturing most of the runoff except for the svr model fig 9 c and d show that the lightgbm and xgboost models perform worse than sa lstm model figs 10 and 11 show the forecasting results of all models for seven days ahead at the eight stations it is obvious that the forecasting results are the worst compared to the other days ahead the runoff series of all models have a relatively large lag behind the observed runoff series the lag times are ranked as follows sa lstm lightgbm xgboost lstm svr this ranking means that the sa lstm model has the best forecast ability fig 10 a illustrates that the svr model underestimates the value of runoff most of the time many forecast results are negative values which are set to zero in fig 10 a the svr model overestimates most of the runoff as shown in fig 10 b d and fig 11 other models have good performances for most times as shown in figs 10 and 11 4 1 3 comparisons of scatter plots on the seventh day fig 12 shows forecasted and observed daily runoff scatter plots of the five models on the seventh day at the eight stations fig 12 a h shows the grand rapids aitkin st louis thebes near ulm near wolf point garrison dam and sioux city stations respectively it is obvious that the fitting line of the sa lstm model in fig 12 is closest to the ideal fitting line the fitting line of the svr model has the largest errors among these models at each station all four models overestimate the runoff when runoff is low and underestimate the runoff when runoff is high when the value of the base flow is low the sa lstm model outperforms the baseline models as shown in fig 12 a when the value of the base flow is high the sa lstm model demonstrates significant improvements in daily runoff forecasting compared with the other models as shown in fig 12 b d fig 12 g shows that lstm and lightgbm models overestimate most of runoff 4 2 comparisons of forecast errors of peak values peak flow forecasting is important in water resource management torrential rain and snow melting in a short time can bring about peak flows notably peak flow is difficult to forecast tables 13 20 show the 15 peak flow forecast results of the five models on the seventh day at each station the absolute relative error are is applied to evaluate the performance of the models 17 are q f q o q o where q o and q f are the observation and forecast of runoff respectively tables 13 17 and 19 20 demonstrate that sa lstm has the best accuracy among all the models according to the mean are the mean ares of the sa lstm model are 22 2 14 6 27 5 18 4 18 3 9 7 and 8 3 at the grand rapids aitkin st louis thebes near ulm garrison dam and sioux city stations respectively the mean are values of the sa lstm model are reduced by approximately 6 75 at the eight stations compared with the mean are values of the lstm model among all models the sa lstm model has the lowest maximum are and minimum are at the grand rapids station at the aitkin and thebes stations among all models the sa lstm model has the lowest minimum are and the second lowest maximum are next to the svr model the performance of the svr model is unstable at the near wolf point station the svr model has the best performance however the mean ares of the svr model at the grand rapids and sioux city station are the highest and are much greater than the mean are values of the other models the performances of lightgbm and xgboost are stable according to figs 6 11 sa lstm has a better ability than lstm svr lightgbm and xgboost to capture the peak values for example the local enlarged image of fig 6 b shows that the svr and lstm models underestimate the peak values and the xgboost and lightgbm models slightly overestimate the peak flow the local enlarged image of fig 6 d reveals that the sa lstm model is effective in peak flow forecasts when the basic runoff is high while the lightgbm xgboost and svr models underestimate the peak values the local enlarged images of fig 8 c and d reveal that all models underestimate the value of the extreme peak flow and the forecast result of the sa lstm model is closest to the observed value because of overestimating the most of runoff svr model fit peak value well as shown in fig 11 b 4 3 comparisons of base flow forecast accuracy it can be seen in figs 6 and 7 d that although all the models perform well in runoff forecasting the svr model has a poor fit at base flow fig 8 illustrates that the sa lstm lstm lightgbm and xgboost models perform well at base flow fig 8 a shows that the svr model underestimates the base flow fig 8 b d and fig 9 show that the forecast result of the svr model is obviously higher than the observed value at base flow thus the svr model has an insufficient runoff forecasting ability fig 10 a shows that the svr model underestimates most of the base flow and many forecast results are negative the negative values are set to zero fig 10 b d and fig 11 show that the svr model overestimates the runoff at base flow in contrast the sa lstm model is the most stable and accurate model and fits most of the series at base flow as demonstrated in fig 10 all models have a slight lag behind the observed runoff at base flow as show in fig 10 the value of nse in is above 0 9 in the first two days of forecasting using the lightgbm xgboost and lstm models at the seven stations except the near wolf point station however the value of nse in decreases rapidly on the seventh day the values of nse in using the xgboost and lightgbm models at the eight stations are less than 0 8 and the value of the nse in using the lstm model is less than 0 6 at the aitkin station as shown in table 6 the sa lstm model has better performances than the other models in the first five days the nse in values are greater than 0 9 at the four stations as shown in tables 6 8 and table 12 the nse in values of all stations are greater than 0 7 for the 1 7 day forecast on the seventh day the attention mechanism enhances nse in by approximately 17 2 at the eight stations compared with the lstm model the improvement is higher than that of nse this reflects that the attention mechanism improves the performance for the base flow 4 4 epoch v s loss nse the epoch v s loss and epoch v s nse are shown in fig 13 and 14 the loss decreases fast within the first one hundred training epochs after the first one hundred training epochs the loss reduces slowly and the value of loss reaches 12 69 eventually the nse increases fast within the first forty training epochs after the first forty training epochs the nse increases slowly and the value of nse reaches 0 97 eventually 4 5 extended experiments data are resampled from the whole training data the first time series is the first half of the dataset the remaining time series are sequentially added with the following one twelfth of dataset finally we get 12 new training series based on a time series the seven day ahead mean performances of twelve time series at each station are shown in table 21 the mean performance of sa lstm is better than that of other four models at each station the values of r are all above 0 85 the nse of sa lstm model are above 0 8 at half of the stations the nse and nse in of other four models are below 0 7 mostly compared with lstm model sa lstm model decrease at least 2 6 value of rmses the value of maes are reduced 11 3 21 4 15 3 13 4 5 9 9 7 2 2 3 0 at eight stations respectively the svr model has the worst performance although the values of r are all above 0 8 the values of nse and nse in become negative at half of the stations the rmses and maes of svr model are much greater than that of other models the seven day ahead forecast performance of twelve time series at grand rapid station are shown in table 22 as an example the sa lstm model has best accuracy at every datasets and more than half of nse and nse in are greater than 0 8 the rmse of sa lstm model are reduced 4 at least compared with that of the second best model the mae of sa lstm model are more than 10 smaller than other models at five datasets 5 conclusions owing to the nonlinearity of runoff conventional empirical models for runoff forecasting require the integration of more information and techniques in this paper a short lag time is introduced and a more effective model is developed to enhance the performance of daily runoff forecasting based on the lstm model the sa lstm model employs a self attention mechanism to obtain more interdependent information within the short previous time series for runoff forecasting the sa lstm model is verified for daily runoff forecasting at eight stations on the mississippi river the results of this model are compared with those of the lightgbm xgboost lstm and svr models in 1 to 7 day ahead forecasts the experimental results indicate that the sa lstm model improves the total forecast accuracy and yields improved performances for base and peak values compared to the other models the effectiveness of the sa lstm model stems from 1 the introduction and exploitation of short previous time series which are highly correlated with forecasting result and 2 the exploitation of the short previous time series with a self attention mechanism however middle and long term forecasting is still difficult in hydrology in future studies we will try to extend the forecast time and improve the accuracy of peak base flows credit authorship contribution statement xi chen writing review editing methodology software conceptualization jiaxu huang writing original draft visualization data curation conceptualization zhen han software validation hongkai gao writing review editing methodology min liu writing review editing zhiqiang li writing review editing xiaoping liu writing review editing qingli li writing review editing honggang qi writing review editing yonggui huang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is supported by the national key research and development project of china 2017yfe0100700 in part by the national natural science foundation of china grant no 41871340 41801036 41911530191 in part by the shanghai key laboratory of multidimensional information processing east china normal university no 2020key003 and in part by the institute of eco chongming ecnu iec 201902 the fundamental research funds for the central universities 
5283,many countries experiences floods due to heavy flows carried by trans boundary rivers hydrological modelling in un gauged catchments and trans boundary rivers has become a challenging task for hydrological modellers this research aims at estimating river discharges computed solely using satellite images with no ground based or a prior information at all it is also aimed at testing the applicability of approach in very well defined channel to braided channels and in very wide range of discharge conditions the approach works owing to principle of hydraulic geometry of natural rivers monitoring these discharges in upstream areas will help in forecasting flows in the downstream areas in this paper an attempt is made to estimate the river discharge using river flow width information derived from temporal satellite remote sensing data alone the approach is based on many stations hydraulic geometry amhg principle through genetic algorithm technique the technique is demonstrated in four major rivers in india for various discharge values using temporal remote sensing data of indian remote sensing satellites irs and landsat computed discharges are validated with observed discharges in all the identified river reaches using statistical method and found to have very good match this novel approach is useful globally for quantifying river discharges with continuous imaging of the rivers and has strong advantage in flood early warning and water resources assessment keywords temporal satellite data river hydraulic geometry genetic algorithm automatic river width extraction discharge estimation flood early warning 1 introduction for many catchments in the world river gauge measurements are rare or non existent thus limiting current understanding of water losses along river courses habitat changes and flood risk dingman 2007 hunger and doell 2008 satellites in contrast provide spatially dense coverage globally attracting calls for a global river discharge mapping capacity from space rhodes 1978 the possibility of monitoring river discharge by satellite has not been adequately studied hitherto because of the difficulty in making sufficiently precise measurements of the water surface since the successful launch of commercial satellites with very high resolution sensors it has become possible to derive ground information from satellite data zhang et al 2004 however previous efforts to estimate river discharge from remotely sensed observations have all required inclusion of some form of ancillary ground based information such as gauge measurements bathymetric surveys and or calibrated hydrology models that are simply unavailable for most of the planet mersel et al 2013 to remove this dependence on ground based information gleason and smith 2014 demonstrated the methodology in estimating river discharge derived solely from multiple satellite images of a river with no ground based or a priori information using at many stations hydraulic geometry amhg principle satellites in contrast provide spatially dense coverage globally attracting calls for a global river discharge mapping capacity from space smith and pavelsky 2008 however previous efforts to estimate river discharge from remotely sensed observations have all required inclusion of some form of ancillary ground based information such as gauge measurements bathymetric surveys and or calibrated hydrology models that are simply unavailable for most of the planet brakenridge et al 2012 smith 1997 to remove this dependence on ground based river discharge m3 s information it is found that useful estimates may be derived solely from multiple satellite images of a river through use of river s hydraulic geometry amhg which holds that a river s paired at a station hydraulic geometry ahg parameters are log linearly related along a river an associated amhg discharge retrieval method uses only remotely sensed cross sectional river top width as an input to an unconstrained optimization of width ahg via a genetic algorithm gleason and smith 2014 amhg effectively halves the number of parameters required by traditional hydraulic geometry thus paving the way for remote estimation of a single remaining parameter and thus river discharge through repeated satellite image observations along a river course the approach is a recent novel method and gaining importance due to practical importance in measuring river discharges in hilly terrains and in trans boundaries the main objective of the study is to estimate river discharges through hydraulic geometry principle and genetic algorithm technique using temporal remote sensing data and to test the applicability of approach in very well defined channel to braided channels and in very wide range of discharge conditions it is also aimed at selecting a suitable method for river width extraction using satellite data to demonstrate the technique four river reaches of major indian rivers are selected each in the godavari krishna mahanadi and the brahmaputra aerial extent of these rivers are 0 312 0 257 0 1412 and 0 712 million sq km respectively in each river 8 to 10 km river stretches are selected wide range of hydraulic river hydraulic geometry and no lateral stream joins in between geographic location of all four rivers and the selected channel locations are shown in the fig 1 a and b 2 a novel theory of river hydraulic geometry some hydraulic characteristics of stream channels like depth width velocity and suspended load are measured quantitatively and vary with discharge as simple power functions at a given river cross section similar variations in relation to discharge exist among the cross sections along the length of a river under the condition that discharge at all points is equal in frequency of occurrence knighton 1974 leopold and maddock 1953 the functions derived for a given cross section and among various cross sections along the river differ only in numerical values of coefficients and exponents measurement of hydraulic factors which help to determine the shape of natural stream channels like depth width velocity and sediment load vary with discharge as a simple power function ferguson 1986 the study of interrelations between them is termed as hydraulic geometry it can be stated that the relation of discharge to other hydraulic factors in natural river cross sections can be expressed as leopold and maddock 1953 1 w a q b ie log w b log q log a 2 d c q f ie log d f log q log c 3 v k q m ie log v m log q log k where q is the discharge w d v represents water surface width mean depth and mean velocity respectively the letters b f m a c and k are numerical constants because width depth and velocity are each a function of discharge as described by eqs 1 3 the three equations can immediately be related to one another through the identity 4 q area velocity or q w d v substituting from the above 5 q a q b c q f k q m or 6 q a c k q b f m it follows therefore that b f m 1 0 and a c k 1 0 2 1 concept of at a station hydraulic geometry ahg the term at a station means at a given cross section and ahg indicates that every cross section along a river exhibits power law behaviour and hydraulic parameters are interrelated to discharge as shown in eqs 1 3 the term ahg can be easily understood by establishing a relationship between width depth or velocity and discharge measurements at frequent intervals of time or at multiple cross sections along a river relation between log q and log w plotted at a temporal scale of a given period using data from hydrodynamic simulation may be hec ras gives the ahg parameters a and b according to width ahg the parameters a and b are fixed for the cross section and using observed widths derived from temporal remote sensing data discharges can be computed using eq 1 2 2 concept of at many stations hydraulic geometry amhg study on hydraulic geometry can reveal the response of river systems to basin attributes and the trends in channel change ran et al 2012 the concept of amhg is applicable to a reach along the river such that discharge along that reach is constant and the same can be estimated based on the correlation found between ahg parameters at one cross section to another in spite of widths and depths varying along the reach at different cross sections considering the law of mass conservation the broad approach of amhg is described in the fig 2 2 2 1 mathematical discussion of the width amhg equation based on the log linear relationship between a and b the width amhg for some user defined length of river is approximately defined as follows 7 f a xa x 2 x n e b x 1 x 2 x n where the subscripts x1 x2 xn correspond to spatially indexed cross section locations up to n total cross sections along the river a and b are the classic site specific ahg parameters at each cross section and f and e are river specific constants defining the intercept and slope respectively of the empirical log linear amhg relationship calculated from all ahg a and b pairs f is proposed to be approximated to w x where w x s the spatially indexed river surface width a quantity that remains relatively constant in log space allowing for the log linear behaviour of amhg substituting w x or f rewriting equation 7 yields another formulation for width based amhg 8 b x 1 log e log a x 1 log e log w x e is constant and unique to each river or a reach of river and its units change at a cross section with changing b exponents it does not correspond to any known hydrologic quantity substituting slope for 1 log e in eq 8 gives 9 b x s l o p e log a x s l o p e log w x and 10 intercept s l o p e log w x a strong congruity has been observed between slope and intercept values obtained from eq 8 and from the observed best fit line plotted between log a and b using observed field widths and discharge ahg equations exhibit amhg behaviour despite differences in how coefficients and exponents of these equations vary with river scale and cross sectional geometry this suggests that although the classical ahg relationships do summarize local site specific channel conditions they also exist within a larger river wide amhg and are not fully independent of each other gleason and smith 2014 gleason and wang 2015 2 2 2 estimation of e parameter from satellite images to estimate parameter e simultaneous river width data at multiple locations along the river is measured from satellite images and it s been observed that the difference of the squares if maximum and minimum observed widths at all cross sections display power law behaviour with maximum observed widths i e 11 max w x 1 x 2 xn p max w x 1 x 2 xn 2 min w x 1 x 2 xn 2 y where p and y are derived empirically by fitting equation 8 to the maximum minimum width differences for all cross sections eq 8 can be rewritten in log space as 12 y log w xmax logp log w xmax 2 w xmin 2 it s also noted that exponent y in eq 11 and e in eq 7 has near equivalence when computed using simultaneous measured river width datasets although a mathematical proof for this congruity has not yet been found but e can still be approximated to y based on the following factors because the log linear form of amhg has a constant slope it may be said that 13 slope δ b δ log a rewriting equation 13 between two points as 14 slope b x 2 b x 1 log a x 2 a x 1 where x1 and x2 refer to any pair of cross sections along river and using power law equation 1 and eq 14 can be rewritten as 15 slope b x 2 b x 1 log w x 2 q x 2 bx 2 w x 1 q x 1 bx 1 for a given mass conserved reach q is constant therefore q x 1 q x 2 implifying the eq 15 as 16 slope b x 2 b x 1 log w x 2 w x 1 b x 2 b x 1 log q noting that log p is constant in eq 12 it s been observed that the numerator and denominator values in eqs 16 and 12 act as equal and in opposite directions hence a relationship is established between observed amhg slopes and width based proxy y a second supporting factor is that the exponent y remains constant when random subsamples of cross sections are used to generate the power law in eq 11 based on these two factors the value of exponent y can be substituted as slope in eq 9 with a negative sign for the known values of e eq 9 can be solved for a wide range of a and b values 2 2 3 river width measurement using temporal remote sensing data since the only input required for river discharge estimation in the genetic algorithm technique is river width the widths measured have to be very accurate various methods of river mask extraction using the satellite data has been examined thoroughly fisher et al 2013 nicholas 2013 feyisa et al 2014 there are various tools developed for auto extraction of river widths by creating binary water masks or the widths can be measured manually in arc gis in either of the case there is a chance of 5 10 error which has to be adjusted by sheer vision or by knowledge of field survey data extracting river mask from remote sensing data is more challenging task where shadow and urban areas are more predominant in satellite images river water flow extents were extracted by applying different water indices using image processing algorithms after examining various approaches spectral severability approach is used for water mask extraction in this study once water mask river active channel is extracted from remote sensing images computing river widths exactly in perpendicular to the direction of flow is very important in this scenario widths were measured using the rivwidth tool pavelsky and smith 2008 the tool is developed on interactive data language idl code platform and uses environment for visualising images envi for visualization purposes the input required is binary water mask along which the tool draws a centreline and width is measured perpendicular to the centreline of the river river masks are prepared using digital image processing techniques river discharge data was obtained from the central water commission india for all the selected four river channel locations for the estimation of river widths medium resolution satellite data is used from resourcesat liss iii 23 m resolution and open sources landsat 30 m resolution cloud free satellite scenes are selected covering wide range of discharges in all the four rivers more than 240 satellite images are used covering all the four river stretches river masks were prepared from all the 240 satellite images river widths at regular intervals within the selected river reach and perpendicular to the direction of flow were measured using all the selected satellite images using rivwidth tool an attempt was made to compare ahg and amhg approaches with the field observed discharges a test reach in the mahanadi river just below hirakud dam was selected to validate these approaches for the period of august 25 to september 04 2006 required input parameters were derived using 10 m carto dem and satellite images discharges during the mentioned period were computed using ahg and amhg approaches and compared with the field measured discharges the best identified approach is subsequently used in estimating river discharges using genetic algorithm technique 3 genetic algorithm technique for river discharge estimation the genetic algorithm ga technique is applied to obtain optimal parameter values of the standard rating curve model rcm for predicting in real time event based flow discharge hydrographs at sites receiving significant inflows tayfur et al 2009 genetic algorithm ga is a numerical optimization method that mimics the process of natural selection mutation and reproduction through the interaction of basic units termed genes and chromosomes a chromosome contains a set of genes where each gene is a specific parameter needed to implement a procedure or solve a problem deb et al 2002 sastry et al 2005 in our case ga is run between consecutive two cross sections with an objective is to conserve discharge between the two cross sections ten chromosomes are populated with two pair of a and b values which are randomly drawn from the amhg solution space any chromosome that is discarded for generating an unreasonably large or small discharge is replaced by a new chromosome randomly drawn from the solution space until all 10 chromosomes pass the discharge filter discharge filter is the range formed using maximum and minimum allowable discharge calculated as follows max discharge max observed width 10 max depth 5 max velocity min discharge min observed width 0 5 min depth 0 1 min velocity chromosomes are then ranked based on their fitness which is calculated as the percentage difference in discharge between the two cross sections each chromosome undergoes the processes of crossover and mutation based on ga parameterization in crossover specific genes from certain chromosomes are exchanged mimicking the mingling of dna in sexual reproduction in mutation there is a small chance the mutation rate here set at 0 1 that a random gene is randomly altered the altered chromosomes are then re ranked according to their fitness discharge conservation and the five least fit chromosomes die out and are replaced with another five chromosomes populated with random genes drawn from the amhg solution space following this process of initial selection crossover mutation and final selection and regeneration the initial 10 chromosomes are said to have completed one generation the ga was run for 50 generations at the end of which we got one pair of a b values for each cross section the entire process above is then re run for more than 50 times for better results since the objective is to conserve discharge at any cross section within the reach we permute the above process with 90 combinations of cross sections since we have ten cross sections in our reach and each time the ga is run for two cross sections at the end of these 90 ga s we get 4500 pair of a and b values which have to be aggregated cross section wise and median of all 4500 pairs of a and b cross section wise give us 10 pairs of a and b values at that cross section median a and median b values computed are checked for one parameter nature of a and b using eq 9 i e using median a and b are calculated from eq 9 and this combining with observed width gives a discharge value similarly using median b a is calculated from eq 9 and another discharge value is obtained it is found that the two values are nearly the same hence by using any of the one of the combinations of a b described above discharges are calculated at each cross section and the median of these values give the final reach average discharge value 3 1 nash sutcliffe model efficiency coefficient the nash sutcliffe model efficiency coefficient is used to assess the predictive power of hydrological models and methods it can be expressed as 17 nse 1 t 1 t q m t q 0 t 2 t 1 t q 0 t q 0 2 where q0 is the mean observed discharges and qm is modelled discharge and q 0 t s observed discharge at time t nash sutcliffe model efficiency coefficient is calculated to validate computed discharges with observed discharges at all test sites 3 2 results and discussions field measured discharge data of all the mentioned test sites were obtained from the central water commission india for calibration and validation of computed discharges discharges during august 25 to september 04 2006 computed in a test reach in the mahanadi river just below hirakud dam using ahg and amhg approaches and its comparison with the field measured discharges are shown in the fig 3 the computed average flow during the mentioned period using ahg and amhg approaches are found to be 7071 and 5358 m3 s respectively whereas the average observed flow is 5942 m3 s nash coefficient computed between observed and computed flows using ahg and amhg approaches are found to be 0 696 and 0 803 respectively the results revealed that amhg approach is more accurate than the ahg approach the reason may be due to that geometry at multiple sections are considered within the reach while computing discharges using amhg approach and ahg is an inherent part of amhg principle different image processing algorithms were tested to extract the water extend in the river within the selected channels normalised water indexing method is found to be good in extracting the water layer from the optical remote sensing images similarly different software algorithms were examined to compute river widths perpendicular to the river flow at regular interval using the derived water masks it is found that rivwidth tool is suitable and accurate in computing the river widths from multiple satellite images river widths in the selected reaches in all the four rivers varies from 200 m to 1500 m at different flow discharges resolution of the satellite data medium resolution used in the study is found to be optimum considering the maximum error of one pixel in estimating the river widths finer resolution satellite data may be required to extract water widths more accurately in narrow rivers extracting river mask from remote sensing data is more challenging task where shadow and urban areas are more predominant in satellite images optimised river hydraulic geometry parameters a and b for all the four rivers were computed through genetic algorithm technique using river hydraulic geometry of all the four rivers river specific parameters w and e for the entire selected river reaches were computed using the eqs 7 and 8 with the optimised a b parameters and plotted in a logarithmic scale for all cross sections it is found the relationship between w and e on a logarithmic scale varies linearly at all cross sections slopes and intercepts of these lines were computed using the eqs 9 and 10 relationship between a b of all the cross sections in the godavari river is shown in the fig 4 mathematical relation between w e parameters of the godavari river is shown in the fig 5 as an example flow discharges at all cross sections in all the four river channels were computed through ga technique using river hydraulic geometry mean of the computed flows are compared with the observed discharges of the same day and time of the satellite pass time step the range of these computations covers the wide range of discharges in each river comparison of observed and computed discharges are show in hydrographic and scatter plot forms in the figs 6 and 7 respectively at perur gauge station of the godavari river nash coefficient between observed and computed discharges is found to be 0 95 the river godavari experiences floods when the flow exceeds discharge more than 30000 m3 s it is interesting to note that river follows the hydraulic geometry even up to 35 000 m3 sec discharge range comparison of observed and computed discharges are shown in hydrographic and scatter plot forms in the figs 8 and 9 respectively at tikarapara gauge station of the mahanadi river nash coefficient between observed and computed discharges is found to be 0 85 the river godavari experiences floods when the flow exceeds discharge more than 20 000 m3 s it is observed that computed discharges has good match up to the discharge of 18 000 m3 s and deviation is found to be more when discharge exceeds 20 000 m3 s as the river from this it is evident that this river flows don t follow the hydraulic geometry during high flows observed and computed discharges of krisha river at wadenapalli gauge stations are plotted in hydrographic and scatter forms as shown in the figs 10 and 11 respectively and nash coefficient between observed and computed discharges is found to be 0 89 floods in the krishna river is a rare phenomenon as the maximum flows are dammed in the upstream portion of the river it is observed that computed discharges has good match up to the discharge of 6000 m3 s the river experienced floods in 2009 unfortunately no cloud free optical satellite data and coverage of microwave data is not available covering the river reach during the peak flood situation brahmaputra river is one of the most frequent flood prone rivers in india observed and computed discharges of brahmaputra river at passighat gauge stations are plotted in hydrographic and scatter forms as shown in the figs 12 and 13 respectively and nash coefficient between observed and computed discharges is found to be 0 86 it is observed that computed discharges have good match up to the discharge of 12 000 m3 s unfortunately no cloud free optical satellite data and coverage of microwave data is not available covering the river reach during the peak flood situations from the dem analysis it is found that the river doesn t follow hydraulic geometry during high flows from these rivers it can be inferred that discharges in the godavari can be computed more accurately even up to the order of 35 000 m3 s because of well defined hydraulic geometry of the river in case of brahmaputra river computed discharges are deviating more with observed values when discharge exceeds 12 000 m3 s this may be due to braidedness of the river channel hence the study revealed that selecting a river channel location is very important in achieving the desired accuracy from this study it is noticed that selecting minimum and maximum discharge range of rivers is also an important criterion in this approach the procedure may be operationally used to estimate river discharges through genetic algorithm technique using temporal remote sensing data of similar resolution from multiple satellite by selecting suitable reach locations where river hydraulic geometry is well defined 4 conclusions this is fundamentally a different automated approach for quantitative river discharge estimation using temporal remote sensing remote sensing images of river that is enabled by advancing a classic theory of river hydraulics through genetic algorithm technique as no ground based information is required the approach holds promise for water resources assessment flood forecasting and addressing scientific problems through global mapping of river flow this approach solves the technical limitations associated with un gauged catchments and trans boundary flows this study also revealed that amhg is more accurate than ahg principle as the widths of river at multiple cross sections is considered and ahg is inherent part of amhg principle error on estimating river widths also will be minimized in amhg principle accuracy in estimating river widths is an important parameter in this study the adopted method should be able to improve water extraction accuracy by increasing spectral separability between water and non water surfaces particularly in areas with shadows and urban backgrounds that are often major causes of low classification accuracy from this study it is found that satellite based river discharge estimation using river hydraulic geometry is more accurate even up to error less than 10 where river follows perfect hydraulic geometry the approach doesn t fit in artificial channels where the profile is rectangle in nature the approach is found to be more suitable during low to moderate flows in the river error is more significant when the river is in the state of floods as the floodplains doesn t follow perfect hydraulic geometry selecting minimum and maximum discharge range of rivers is an important criterion in this approach this can be decided based on the long term historic discharge data of gauged rivers and by hit and trail method in case of ungauged rivers when the observed and computed discharges of mahanadi at tikarpara gauge stations are compared the deviation of peak flow is found to be nearly 30 as the river experience floods at this discharge range further research may be is required to make use of this novel approach for computing flows even at high stages of the rivers availability of cloud free optical data during monsoon period is a limitation for operational use of the study microwave satellite data may suffice this limitation subject to its cost and its frequency of coverage over the required rivers may be fine to medium resolution 5 to 10 m resolution satellite data is required to estimate river widths to estimate flows in narrow rivers more accurately genetic algorithm technique is more suitable where the geomorphological changes of rivers are insignificant the procedure may be operationally used to estimate river discharges through genetic algorithm technique using temporal remote sensing data of similar resolution from multiple satellite by selecting suitable reach locations where river hydraulic geometry is well defined credit authorship contribution statement k h v durga rao conceptualization methodology formal analysis writing original draft a shravya software validation data curation v k dadhwal supervision visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author sincerely acknowledges the support and guidance given by the director national remote sensing centre nrsc and deputy director remote sensing applications rsa nrsc support and cooperation given by dr v v rao group director water resources group nrsc is greatly acknowledged the first author acknowledges the support of ms ketaki agarwal nrsc during initial phase of the project the authors acknowledge the central water commission india for providing observed discharge data of various gauge sites 
5283,many countries experiences floods due to heavy flows carried by trans boundary rivers hydrological modelling in un gauged catchments and trans boundary rivers has become a challenging task for hydrological modellers this research aims at estimating river discharges computed solely using satellite images with no ground based or a prior information at all it is also aimed at testing the applicability of approach in very well defined channel to braided channels and in very wide range of discharge conditions the approach works owing to principle of hydraulic geometry of natural rivers monitoring these discharges in upstream areas will help in forecasting flows in the downstream areas in this paper an attempt is made to estimate the river discharge using river flow width information derived from temporal satellite remote sensing data alone the approach is based on many stations hydraulic geometry amhg principle through genetic algorithm technique the technique is demonstrated in four major rivers in india for various discharge values using temporal remote sensing data of indian remote sensing satellites irs and landsat computed discharges are validated with observed discharges in all the identified river reaches using statistical method and found to have very good match this novel approach is useful globally for quantifying river discharges with continuous imaging of the rivers and has strong advantage in flood early warning and water resources assessment keywords temporal satellite data river hydraulic geometry genetic algorithm automatic river width extraction discharge estimation flood early warning 1 introduction for many catchments in the world river gauge measurements are rare or non existent thus limiting current understanding of water losses along river courses habitat changes and flood risk dingman 2007 hunger and doell 2008 satellites in contrast provide spatially dense coverage globally attracting calls for a global river discharge mapping capacity from space rhodes 1978 the possibility of monitoring river discharge by satellite has not been adequately studied hitherto because of the difficulty in making sufficiently precise measurements of the water surface since the successful launch of commercial satellites with very high resolution sensors it has become possible to derive ground information from satellite data zhang et al 2004 however previous efforts to estimate river discharge from remotely sensed observations have all required inclusion of some form of ancillary ground based information such as gauge measurements bathymetric surveys and or calibrated hydrology models that are simply unavailable for most of the planet mersel et al 2013 to remove this dependence on ground based information gleason and smith 2014 demonstrated the methodology in estimating river discharge derived solely from multiple satellite images of a river with no ground based or a priori information using at many stations hydraulic geometry amhg principle satellites in contrast provide spatially dense coverage globally attracting calls for a global river discharge mapping capacity from space smith and pavelsky 2008 however previous efforts to estimate river discharge from remotely sensed observations have all required inclusion of some form of ancillary ground based information such as gauge measurements bathymetric surveys and or calibrated hydrology models that are simply unavailable for most of the planet brakenridge et al 2012 smith 1997 to remove this dependence on ground based river discharge m3 s information it is found that useful estimates may be derived solely from multiple satellite images of a river through use of river s hydraulic geometry amhg which holds that a river s paired at a station hydraulic geometry ahg parameters are log linearly related along a river an associated amhg discharge retrieval method uses only remotely sensed cross sectional river top width as an input to an unconstrained optimization of width ahg via a genetic algorithm gleason and smith 2014 amhg effectively halves the number of parameters required by traditional hydraulic geometry thus paving the way for remote estimation of a single remaining parameter and thus river discharge through repeated satellite image observations along a river course the approach is a recent novel method and gaining importance due to practical importance in measuring river discharges in hilly terrains and in trans boundaries the main objective of the study is to estimate river discharges through hydraulic geometry principle and genetic algorithm technique using temporal remote sensing data and to test the applicability of approach in very well defined channel to braided channels and in very wide range of discharge conditions it is also aimed at selecting a suitable method for river width extraction using satellite data to demonstrate the technique four river reaches of major indian rivers are selected each in the godavari krishna mahanadi and the brahmaputra aerial extent of these rivers are 0 312 0 257 0 1412 and 0 712 million sq km respectively in each river 8 to 10 km river stretches are selected wide range of hydraulic river hydraulic geometry and no lateral stream joins in between geographic location of all four rivers and the selected channel locations are shown in the fig 1 a and b 2 a novel theory of river hydraulic geometry some hydraulic characteristics of stream channels like depth width velocity and suspended load are measured quantitatively and vary with discharge as simple power functions at a given river cross section similar variations in relation to discharge exist among the cross sections along the length of a river under the condition that discharge at all points is equal in frequency of occurrence knighton 1974 leopold and maddock 1953 the functions derived for a given cross section and among various cross sections along the river differ only in numerical values of coefficients and exponents measurement of hydraulic factors which help to determine the shape of natural stream channels like depth width velocity and sediment load vary with discharge as a simple power function ferguson 1986 the study of interrelations between them is termed as hydraulic geometry it can be stated that the relation of discharge to other hydraulic factors in natural river cross sections can be expressed as leopold and maddock 1953 1 w a q b ie log w b log q log a 2 d c q f ie log d f log q log c 3 v k q m ie log v m log q log k where q is the discharge w d v represents water surface width mean depth and mean velocity respectively the letters b f m a c and k are numerical constants because width depth and velocity are each a function of discharge as described by eqs 1 3 the three equations can immediately be related to one another through the identity 4 q area velocity or q w d v substituting from the above 5 q a q b c q f k q m or 6 q a c k q b f m it follows therefore that b f m 1 0 and a c k 1 0 2 1 concept of at a station hydraulic geometry ahg the term at a station means at a given cross section and ahg indicates that every cross section along a river exhibits power law behaviour and hydraulic parameters are interrelated to discharge as shown in eqs 1 3 the term ahg can be easily understood by establishing a relationship between width depth or velocity and discharge measurements at frequent intervals of time or at multiple cross sections along a river relation between log q and log w plotted at a temporal scale of a given period using data from hydrodynamic simulation may be hec ras gives the ahg parameters a and b according to width ahg the parameters a and b are fixed for the cross section and using observed widths derived from temporal remote sensing data discharges can be computed using eq 1 2 2 concept of at many stations hydraulic geometry amhg study on hydraulic geometry can reveal the response of river systems to basin attributes and the trends in channel change ran et al 2012 the concept of amhg is applicable to a reach along the river such that discharge along that reach is constant and the same can be estimated based on the correlation found between ahg parameters at one cross section to another in spite of widths and depths varying along the reach at different cross sections considering the law of mass conservation the broad approach of amhg is described in the fig 2 2 2 1 mathematical discussion of the width amhg equation based on the log linear relationship between a and b the width amhg for some user defined length of river is approximately defined as follows 7 f a xa x 2 x n e b x 1 x 2 x n where the subscripts x1 x2 xn correspond to spatially indexed cross section locations up to n total cross sections along the river a and b are the classic site specific ahg parameters at each cross section and f and e are river specific constants defining the intercept and slope respectively of the empirical log linear amhg relationship calculated from all ahg a and b pairs f is proposed to be approximated to w x where w x s the spatially indexed river surface width a quantity that remains relatively constant in log space allowing for the log linear behaviour of amhg substituting w x or f rewriting equation 7 yields another formulation for width based amhg 8 b x 1 log e log a x 1 log e log w x e is constant and unique to each river or a reach of river and its units change at a cross section with changing b exponents it does not correspond to any known hydrologic quantity substituting slope for 1 log e in eq 8 gives 9 b x s l o p e log a x s l o p e log w x and 10 intercept s l o p e log w x a strong congruity has been observed between slope and intercept values obtained from eq 8 and from the observed best fit line plotted between log a and b using observed field widths and discharge ahg equations exhibit amhg behaviour despite differences in how coefficients and exponents of these equations vary with river scale and cross sectional geometry this suggests that although the classical ahg relationships do summarize local site specific channel conditions they also exist within a larger river wide amhg and are not fully independent of each other gleason and smith 2014 gleason and wang 2015 2 2 2 estimation of e parameter from satellite images to estimate parameter e simultaneous river width data at multiple locations along the river is measured from satellite images and it s been observed that the difference of the squares if maximum and minimum observed widths at all cross sections display power law behaviour with maximum observed widths i e 11 max w x 1 x 2 xn p max w x 1 x 2 xn 2 min w x 1 x 2 xn 2 y where p and y are derived empirically by fitting equation 8 to the maximum minimum width differences for all cross sections eq 8 can be rewritten in log space as 12 y log w xmax logp log w xmax 2 w xmin 2 it s also noted that exponent y in eq 11 and e in eq 7 has near equivalence when computed using simultaneous measured river width datasets although a mathematical proof for this congruity has not yet been found but e can still be approximated to y based on the following factors because the log linear form of amhg has a constant slope it may be said that 13 slope δ b δ log a rewriting equation 13 between two points as 14 slope b x 2 b x 1 log a x 2 a x 1 where x1 and x2 refer to any pair of cross sections along river and using power law equation 1 and eq 14 can be rewritten as 15 slope b x 2 b x 1 log w x 2 q x 2 bx 2 w x 1 q x 1 bx 1 for a given mass conserved reach q is constant therefore q x 1 q x 2 implifying the eq 15 as 16 slope b x 2 b x 1 log w x 2 w x 1 b x 2 b x 1 log q noting that log p is constant in eq 12 it s been observed that the numerator and denominator values in eqs 16 and 12 act as equal and in opposite directions hence a relationship is established between observed amhg slopes and width based proxy y a second supporting factor is that the exponent y remains constant when random subsamples of cross sections are used to generate the power law in eq 11 based on these two factors the value of exponent y can be substituted as slope in eq 9 with a negative sign for the known values of e eq 9 can be solved for a wide range of a and b values 2 2 3 river width measurement using temporal remote sensing data since the only input required for river discharge estimation in the genetic algorithm technique is river width the widths measured have to be very accurate various methods of river mask extraction using the satellite data has been examined thoroughly fisher et al 2013 nicholas 2013 feyisa et al 2014 there are various tools developed for auto extraction of river widths by creating binary water masks or the widths can be measured manually in arc gis in either of the case there is a chance of 5 10 error which has to be adjusted by sheer vision or by knowledge of field survey data extracting river mask from remote sensing data is more challenging task where shadow and urban areas are more predominant in satellite images river water flow extents were extracted by applying different water indices using image processing algorithms after examining various approaches spectral severability approach is used for water mask extraction in this study once water mask river active channel is extracted from remote sensing images computing river widths exactly in perpendicular to the direction of flow is very important in this scenario widths were measured using the rivwidth tool pavelsky and smith 2008 the tool is developed on interactive data language idl code platform and uses environment for visualising images envi for visualization purposes the input required is binary water mask along which the tool draws a centreline and width is measured perpendicular to the centreline of the river river masks are prepared using digital image processing techniques river discharge data was obtained from the central water commission india for all the selected four river channel locations for the estimation of river widths medium resolution satellite data is used from resourcesat liss iii 23 m resolution and open sources landsat 30 m resolution cloud free satellite scenes are selected covering wide range of discharges in all the four rivers more than 240 satellite images are used covering all the four river stretches river masks were prepared from all the 240 satellite images river widths at regular intervals within the selected river reach and perpendicular to the direction of flow were measured using all the selected satellite images using rivwidth tool an attempt was made to compare ahg and amhg approaches with the field observed discharges a test reach in the mahanadi river just below hirakud dam was selected to validate these approaches for the period of august 25 to september 04 2006 required input parameters were derived using 10 m carto dem and satellite images discharges during the mentioned period were computed using ahg and amhg approaches and compared with the field measured discharges the best identified approach is subsequently used in estimating river discharges using genetic algorithm technique 3 genetic algorithm technique for river discharge estimation the genetic algorithm ga technique is applied to obtain optimal parameter values of the standard rating curve model rcm for predicting in real time event based flow discharge hydrographs at sites receiving significant inflows tayfur et al 2009 genetic algorithm ga is a numerical optimization method that mimics the process of natural selection mutation and reproduction through the interaction of basic units termed genes and chromosomes a chromosome contains a set of genes where each gene is a specific parameter needed to implement a procedure or solve a problem deb et al 2002 sastry et al 2005 in our case ga is run between consecutive two cross sections with an objective is to conserve discharge between the two cross sections ten chromosomes are populated with two pair of a and b values which are randomly drawn from the amhg solution space any chromosome that is discarded for generating an unreasonably large or small discharge is replaced by a new chromosome randomly drawn from the solution space until all 10 chromosomes pass the discharge filter discharge filter is the range formed using maximum and minimum allowable discharge calculated as follows max discharge max observed width 10 max depth 5 max velocity min discharge min observed width 0 5 min depth 0 1 min velocity chromosomes are then ranked based on their fitness which is calculated as the percentage difference in discharge between the two cross sections each chromosome undergoes the processes of crossover and mutation based on ga parameterization in crossover specific genes from certain chromosomes are exchanged mimicking the mingling of dna in sexual reproduction in mutation there is a small chance the mutation rate here set at 0 1 that a random gene is randomly altered the altered chromosomes are then re ranked according to their fitness discharge conservation and the five least fit chromosomes die out and are replaced with another five chromosomes populated with random genes drawn from the amhg solution space following this process of initial selection crossover mutation and final selection and regeneration the initial 10 chromosomes are said to have completed one generation the ga was run for 50 generations at the end of which we got one pair of a b values for each cross section the entire process above is then re run for more than 50 times for better results since the objective is to conserve discharge at any cross section within the reach we permute the above process with 90 combinations of cross sections since we have ten cross sections in our reach and each time the ga is run for two cross sections at the end of these 90 ga s we get 4500 pair of a and b values which have to be aggregated cross section wise and median of all 4500 pairs of a and b cross section wise give us 10 pairs of a and b values at that cross section median a and median b values computed are checked for one parameter nature of a and b using eq 9 i e using median a and b are calculated from eq 9 and this combining with observed width gives a discharge value similarly using median b a is calculated from eq 9 and another discharge value is obtained it is found that the two values are nearly the same hence by using any of the one of the combinations of a b described above discharges are calculated at each cross section and the median of these values give the final reach average discharge value 3 1 nash sutcliffe model efficiency coefficient the nash sutcliffe model efficiency coefficient is used to assess the predictive power of hydrological models and methods it can be expressed as 17 nse 1 t 1 t q m t q 0 t 2 t 1 t q 0 t q 0 2 where q0 is the mean observed discharges and qm is modelled discharge and q 0 t s observed discharge at time t nash sutcliffe model efficiency coefficient is calculated to validate computed discharges with observed discharges at all test sites 3 2 results and discussions field measured discharge data of all the mentioned test sites were obtained from the central water commission india for calibration and validation of computed discharges discharges during august 25 to september 04 2006 computed in a test reach in the mahanadi river just below hirakud dam using ahg and amhg approaches and its comparison with the field measured discharges are shown in the fig 3 the computed average flow during the mentioned period using ahg and amhg approaches are found to be 7071 and 5358 m3 s respectively whereas the average observed flow is 5942 m3 s nash coefficient computed between observed and computed flows using ahg and amhg approaches are found to be 0 696 and 0 803 respectively the results revealed that amhg approach is more accurate than the ahg approach the reason may be due to that geometry at multiple sections are considered within the reach while computing discharges using amhg approach and ahg is an inherent part of amhg principle different image processing algorithms were tested to extract the water extend in the river within the selected channels normalised water indexing method is found to be good in extracting the water layer from the optical remote sensing images similarly different software algorithms were examined to compute river widths perpendicular to the river flow at regular interval using the derived water masks it is found that rivwidth tool is suitable and accurate in computing the river widths from multiple satellite images river widths in the selected reaches in all the four rivers varies from 200 m to 1500 m at different flow discharges resolution of the satellite data medium resolution used in the study is found to be optimum considering the maximum error of one pixel in estimating the river widths finer resolution satellite data may be required to extract water widths more accurately in narrow rivers extracting river mask from remote sensing data is more challenging task where shadow and urban areas are more predominant in satellite images optimised river hydraulic geometry parameters a and b for all the four rivers were computed through genetic algorithm technique using river hydraulic geometry of all the four rivers river specific parameters w and e for the entire selected river reaches were computed using the eqs 7 and 8 with the optimised a b parameters and plotted in a logarithmic scale for all cross sections it is found the relationship between w and e on a logarithmic scale varies linearly at all cross sections slopes and intercepts of these lines were computed using the eqs 9 and 10 relationship between a b of all the cross sections in the godavari river is shown in the fig 4 mathematical relation between w e parameters of the godavari river is shown in the fig 5 as an example flow discharges at all cross sections in all the four river channels were computed through ga technique using river hydraulic geometry mean of the computed flows are compared with the observed discharges of the same day and time of the satellite pass time step the range of these computations covers the wide range of discharges in each river comparison of observed and computed discharges are show in hydrographic and scatter plot forms in the figs 6 and 7 respectively at perur gauge station of the godavari river nash coefficient between observed and computed discharges is found to be 0 95 the river godavari experiences floods when the flow exceeds discharge more than 30000 m3 s it is interesting to note that river follows the hydraulic geometry even up to 35 000 m3 sec discharge range comparison of observed and computed discharges are shown in hydrographic and scatter plot forms in the figs 8 and 9 respectively at tikarapara gauge station of the mahanadi river nash coefficient between observed and computed discharges is found to be 0 85 the river godavari experiences floods when the flow exceeds discharge more than 20 000 m3 s it is observed that computed discharges has good match up to the discharge of 18 000 m3 s and deviation is found to be more when discharge exceeds 20 000 m3 s as the river from this it is evident that this river flows don t follow the hydraulic geometry during high flows observed and computed discharges of krisha river at wadenapalli gauge stations are plotted in hydrographic and scatter forms as shown in the figs 10 and 11 respectively and nash coefficient between observed and computed discharges is found to be 0 89 floods in the krishna river is a rare phenomenon as the maximum flows are dammed in the upstream portion of the river it is observed that computed discharges has good match up to the discharge of 6000 m3 s the river experienced floods in 2009 unfortunately no cloud free optical satellite data and coverage of microwave data is not available covering the river reach during the peak flood situation brahmaputra river is one of the most frequent flood prone rivers in india observed and computed discharges of brahmaputra river at passighat gauge stations are plotted in hydrographic and scatter forms as shown in the figs 12 and 13 respectively and nash coefficient between observed and computed discharges is found to be 0 86 it is observed that computed discharges have good match up to the discharge of 12 000 m3 s unfortunately no cloud free optical satellite data and coverage of microwave data is not available covering the river reach during the peak flood situations from the dem analysis it is found that the river doesn t follow hydraulic geometry during high flows from these rivers it can be inferred that discharges in the godavari can be computed more accurately even up to the order of 35 000 m3 s because of well defined hydraulic geometry of the river in case of brahmaputra river computed discharges are deviating more with observed values when discharge exceeds 12 000 m3 s this may be due to braidedness of the river channel hence the study revealed that selecting a river channel location is very important in achieving the desired accuracy from this study it is noticed that selecting minimum and maximum discharge range of rivers is also an important criterion in this approach the procedure may be operationally used to estimate river discharges through genetic algorithm technique using temporal remote sensing data of similar resolution from multiple satellite by selecting suitable reach locations where river hydraulic geometry is well defined 4 conclusions this is fundamentally a different automated approach for quantitative river discharge estimation using temporal remote sensing remote sensing images of river that is enabled by advancing a classic theory of river hydraulics through genetic algorithm technique as no ground based information is required the approach holds promise for water resources assessment flood forecasting and addressing scientific problems through global mapping of river flow this approach solves the technical limitations associated with un gauged catchments and trans boundary flows this study also revealed that amhg is more accurate than ahg principle as the widths of river at multiple cross sections is considered and ahg is inherent part of amhg principle error on estimating river widths also will be minimized in amhg principle accuracy in estimating river widths is an important parameter in this study the adopted method should be able to improve water extraction accuracy by increasing spectral separability between water and non water surfaces particularly in areas with shadows and urban backgrounds that are often major causes of low classification accuracy from this study it is found that satellite based river discharge estimation using river hydraulic geometry is more accurate even up to error less than 10 where river follows perfect hydraulic geometry the approach doesn t fit in artificial channels where the profile is rectangle in nature the approach is found to be more suitable during low to moderate flows in the river error is more significant when the river is in the state of floods as the floodplains doesn t follow perfect hydraulic geometry selecting minimum and maximum discharge range of rivers is an important criterion in this approach this can be decided based on the long term historic discharge data of gauged rivers and by hit and trail method in case of ungauged rivers when the observed and computed discharges of mahanadi at tikarpara gauge stations are compared the deviation of peak flow is found to be nearly 30 as the river experience floods at this discharge range further research may be is required to make use of this novel approach for computing flows even at high stages of the rivers availability of cloud free optical data during monsoon period is a limitation for operational use of the study microwave satellite data may suffice this limitation subject to its cost and its frequency of coverage over the required rivers may be fine to medium resolution 5 to 10 m resolution satellite data is required to estimate river widths to estimate flows in narrow rivers more accurately genetic algorithm technique is more suitable where the geomorphological changes of rivers are insignificant the procedure may be operationally used to estimate river discharges through genetic algorithm technique using temporal remote sensing data of similar resolution from multiple satellite by selecting suitable reach locations where river hydraulic geometry is well defined credit authorship contribution statement k h v durga rao conceptualization methodology formal analysis writing original draft a shravya software validation data curation v k dadhwal supervision visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author sincerely acknowledges the support and guidance given by the director national remote sensing centre nrsc and deputy director remote sensing applications rsa nrsc support and cooperation given by dr v v rao group director water resources group nrsc is greatly acknowledged the first author acknowledges the support of ms ketaki agarwal nrsc during initial phase of the project the authors acknowledge the central water commission india for providing observed discharge data of various gauge sites 
5284,groundwater withdrawn in a multi aquifer aquitard system maas may cause vertical cross flow supplement and result in groundwater drawdown and soil deformation in both aquifers and aquitards this study investigates the responses of groundwater and deep soils to dewatering in the maas through statistical and numerical analyses field data of ten pumping tests conducted in the maas in shanghai urban area are collected and a statistical analysis for groundwater drawdown and ground settlement is performed field measurements indicate that the drawdowns are distributed linearly along radial distance in logarithmic coordinates however the ground settlements are disproportional to the drawdowns and show a trilinear distribution along the radial direction to examine the inconsistent distributions of the groundwater drawdown and the ground settlement representative dewatering models are developed numerically and solved explicitly by the coupled biot s three dimensional consolidation theory numerical results of dewatering in maas are compared to those obtained by a non coupling formula moreover effect of a micro confined aquifer on drawdown and soil deformation is discussed it is found that soil compression in each stratum is closely related to the decline of the pore pressure whilst small expansions are observed in early dewatering stages due to the hydro mechanical coupling effect the ground surface settlement near the pumping well obtained by the coupled numerical method is significantly smaller than that by the non coupling formula owing to horizontal and vertical cross flow supplement an additional aquifer embedded in aquitard above the pumping aquifer can relief both the groundwater drawdown and ground surface settlement significantly keywords dewatering multi aquifer aquitard system coupled hydro mechanical analysis groundwater drawdown stratum deformation 1 introduction in the past decade dewatering in confined aquifers required for deep excavations increased dramatically with the rapid development of underground exploitation in coastal regions of china huang and han 2009 chen et al 2013 tan and wang 2013a b shen et al 2013 wang et al 2013b wen et al 2017 xu et al 2017 li et al 2017 2018 2019 xu et al 2019 wu et al 2020 according to the principle of effective stress developed by terzaghi depressurization in confined aquifers will inevitably increase the effective stress of soil consequently causing soil compression sheng and sloan 2003 peng et al 2011 pujades et al 2014a b 2016 wang et al 2017 2019a b in a metropolitan area where the density of constructions is high if the dewatering takes place in a thick sand stratum with very high piezometric pressure excessive ground settlement may occur shen et al 2015 lyu et al 2016 wang et al 2018a b zeng et al 2019a b zheng et al 2019 li et al 2020 and potentially damage surrounding infrastructures e g building basements shield tunnels pipe lines etc chen and xiang 2006 xiang et al 2008 omer 2012 wu et al 2019 to ensure the safety of existing facilities it is essential to understand responses of groundwater and deep soil layers to dewatering in confined aquifers regarding dewatering in a confined aquifer many studies have been carried out through field measurements lu et al 2015 wang et al 2012 tan et al 2014 zhu et al 2015 zheng et al 2018 numerical simulations majumdar et al 2008 wang et al 2013a lin et al 2010 zhang et al 2018 zeng et al 2018 and theoretical analyses theis 1935 cooper and jacob 1946 sethi 2011 some researchers focused on drawdown distributions outside of foundation pits with sealing curtains partially inserted into the aquifer shen et al 2013 wang et al 2013b xu et al 2014 wu et al 2015a b pujades et al 2016 wen et al 2017 and others paid attention to the ground surface settlements budhu and adiyaman 2010 lopez fernandez et al 2013 niu et al 2013 omer 2012 shen and xu 2011 as well as dewatering induced adverse impacts on buildings and structures chen and xiang 2006 yoo et al 2012 in most related studies the ground settlement was roughly calculated by the use of the uncoupled method in which a seepage analysis software was adopted to get the changes of pore pressures in aquifers and aquitards the ground settlement could be calculated by the formula δ z s se δ p b where δ z is the settlement s se is the portion of the specific storage coefficient associated to soil deformation b is the thickness of the porous medium and δ p is the drop in pressure head in this formula the effect of horizontal deformation on the vertical settlement is ignored which is only suitable for one dimensional problems for a better understanding of the three dimensional soil deformation responses hydro mechanical coupling analyses had been carried out using biot s consolidation theory zheng et al 2014 investigated the settlement mechanism of soils induced by local pressure relief in a confined aquifer and found the maximum soil settlement appears at the top of the stratum more recently pujades et al 2017 reported that settlements were disproportional to drawdown inside a circle with a radius equal to 0 7 times the thickness of the pumped porous medium and proposed a calculation procedure to estimate the settlements around the pumping wells however these findings and proposed method came from a simplified model with only one stratum a single aquifer considering the interactions between aquifers and aquitards the responses of groundwater and deep soil layers will be more complicate to dewatering in multi aquifer aquitard system maas as reported in our primary study wang et al 2019a b the proposed method might be inapplicable in maas and formulas for estimating the deep soil deformations need further examination in this study a statistical analysis of groundwater drawdown and ground surface settlement of ten pumping tests in shanghai urban area is conducted for further examination hydro mechanical coupling analysis using the biot s three dimensional consolidation theory is performed numerical results of ground surface settlement are compared to that obtained by a non coupling formula based on the effective stress principle developed by terzaghi moreover effect of micro confined aquifer on drawdown and soil deformation is discussed 2 statistical analysis 2 1 description of confined aquifers in shanghai shanghai is located on the deltaic deposits of the yangtze river the typical quaternary deposits in shanghai are composed of one artesian aquifer and five confined aquifer layers in addition a micro confined aquifer is distributed sporadically at the branch channel above the first confined aquifer the groundwater system is a typical alternated multi aquifer aquitard system maas as shown in fig 1 since the buried depths of the micro confined aquifer and the first confined aquifer aqi are relatively shallow with high water heads it has a significant influence on the underground constructions such as deep excavations therefore many studies have been carried out regarding the dewatering in these two confined aquifers shen and xu 2011 tan et al 2014 wang et al 2018a b wang et al 2019a b the micro confined aquifer is distributed sporadically at the branch channel and estuarine bay to the southeast as shown in fig 2 the top surface is buried at the depth of 15 m 22 m below the ground surface with the thickness of 2 m 20 m the first confined aquifer is widely distributed in shanghai as shown in fig 3 the top surface is generally 15 m 54 m below the ground surface with the thickness of 3 m 40 m the buried depth of this confined aquifer increases significantly from north to south and east and the thickness changes greatly considering spatial variability of hydraulic characteristics of these two confined aquifers many in situ pumping tests were performed prior to underground construction to examine the hydraulic connections and hydraulic parameters in this study field data of ten pumping tests are collected to investigate the groundwater drawdown and ground surface settlement caused by dewatering in confined aquifers 2 2 database establishment a total of 5 cases of in situ pumping tests in the micro confined aquifer are collected where the locations of test sites are shown in fig 2 the dewatering aquifer lies within the layers of sandy silt and silty sand layers ⑤2 and ⑤3 2 table 1 lists the detail information of the collected pumping tests in the micro confined aquifer moreover a total of 5 cases of in situ pumping test in the 1st confined aquifer are collected where the locations of test sites are shown in fig 3 the detail information of the pumping tests in the 1st confined aquifer is list in table 2 note that in case 5 there is a hydraulic connection between the first confined aquifer ⑦ and second confined aquifer ⑨ during pumping tests the groundwater drawdown and ground surface settlement were measured by the use of observation wells and settlement surveying instruments since the responses of the groundwater and deep soil layers to dewatering in maas are complicated only groundwater drawdown and ground settlement at typical stages are presented and discussed the final drawdowns and corresponding settlements at center of pumping wells are listed in tables 1 and 2 2 3 analysis of groundwater drawdown fig 4 shows the drawdown distribution along the radial distance where the drawdown δ h is normalized by that at center of pumping wells δ h c and the radial distance r is normalized by the buried depth of the top surface of the confined aquifers d i 0 as can be seen from the figure the drawdowns caused by dewatering in both the micro confined aquifer and the 1st confined aquifer are linearly distributed along the radial distance under logarithmic coordinates moreover the theim results are added to the figures for comparison where the parameters for the theim formula are listed in tables 1 and 2 it can be seen from fig 4 a that the theim results of groundwater drawdown in micro confined aquifer agree well with the filed measurements however small discrepancies can be observed from fig 4 b between the theim results and field measurements in the 1st confined aquifer this might be attributed to the deviation of soil parameters it is known that the groundwater drawdown is sensitive to the hydraulic conductivity while this parameter can rarely be accurately determined by field tests or labortory tests although dewatering induced drawdown in confined aquifers might be affected by many factors such as hydraulic connection hydraulic parameters insert depth of pumping wells etc field data confirm the results of theim equation of steady flow in the fully penetrating well 2 4 analysis of ground surface settlements fig 5 depicts the distribution of ground surface settlement along the radial distance where the ground settlement δ v is normalized by that at the center of pumping wells δ vc and the radial distance r is normalized by the buried depth of the confined aquifers d i 0 it can be seen from fig 5 a and b that the normalized ground settlement caused by dewatering in micro confined aquifer and the 1st confined aquifer have similar distributions in both figures the normalized settlement points show a concentrated distribution however these points cannot be fitted by a single line as the drawdown it should be divided into three fitting lines with reflection points located at r 0 4 d i 0 and r 1 5 d i 0 it is seen that the ground settlements near the pumping wells and far away from the pumping wells decreases slight with the increase of radial distance whilst the settlement changes dramatically with radial distance in the range of 0 4 d i 0 r 1 5 d i 0 based on the analyses above it can be tentatively concluded that the groundwater drawdown has a linear distribution under logarithmic coordinates while the ground settlement is proportional to the drawdown this finding is similar to the numerical results reported by pujades et al 2017 for better understanding the responses of groundwater and deep soil layers to dewatering in maas hydro mechanical coupling analyses are carried out and numerical results are analyzed in association with the those obtain by a non coupling formula 3 hydro mechanical coupling method the variables involved in the description of fluid flow through porous media are pore pressure saturation and the three components of the specific discharge vector these variables are related by the fluid mass balance equation darcy s law for fluid transport a constitutive equation specifying the fluid response to changing in pore pressure saturation and volumetric strains and an equation of state relating pore pressure to saturation in the unsaturated range pore pressure influences are involved in mechanical constitutive laws to complete the fluid mechanical coupling assuming that the volumetric strains are known substitution of the mass balance equation into the fluid constitutive relation using darcy s law yields a differential equation in terms of pore pressure and saturation that may be solved for particular geometries properties boundaries and initial conditions 3 1 governing differential equations a confined aquifer of large horizontal extent is characterized by its initial pore pressure and initial isotropic stress water is pumped from the well at a constant rate q per unit depth the elastic porous medium is homogeneous and isotropic in the same layers and the flow of groundwater is governed by darcy s law transient effects are linked to the compressibility of water and the soil matrix a cylindrical system of coordinates is adopted with the z axis being vertically upward substitution of the transport law in the fluid mass balance equation gives 1 p t m k 2 p α ε kk t where k is the hydraulic conductivity p is the pore pressure m is the biot modulus and α is the biot coefficient the elastic relations between effective stresses and strains are small strain 2 σ kk σ kk 0 α p p 0 α 1 ε kk where σ kk is the principal stresses ε kk is the principal strains σ kk 0 and p 0 are the initial values of the principal stresses and pore pressure respectively and α 1 k 4 3 g partial differentiation with respect to time of the eq 2 3 σ kk t α p t α 1 ε kk t substituting eq 3 into eq 1 yields the diffusion equation for pore pressure p 4 p t c 2 p α α 1 s σ kk t where c k s is the diffusion coefficient s 1 m α 2 α 1 is the storage coefficient for an axisymmetric problem the laplacian of p can be expressed as 5 2 p 2 p r 2 1 r p r 2 p z 2 the initial condition to solve this partial differential equation is 6 p r θ z t 0 p 0 r θ z where p 0 r θ z is the initial pore pressure at the point r θ z the boundary conditions are 7 lim r p r θ z p 0 r θ z 8 lim r 0 2 π r p r q k 3 2 influence range for the sake of calculation convenience a constant head boundary is selected in the plane wu et al 2016 the influence radius of the well r can be initially calculated from 9 r 10 s w k where s w is the drawdown of the pumping well k is the hydraulic conductivity of the soil to eliminate the effects of the boundaries on the water level and stratum deformation additional trail simulations should be carried out by increasing model dimension 3 3 initial boundary conditions boundary conditions for coupled hydro mechanical analysis includes mechanical boundary condition and hydraulic boundary condition as for the mechanical boundary condition the soil movements are set to be free at the top of the model and are restrained in all directions u x u y u z 0 at the bottom the soil movements in all vertical boundaries are constrained in its own normal directions and free in other directions during the simulation for the hydraulic boundary condition hydraulic head at the top of the model is set to be free and the model bottom is set to be impervious boundary conditions for four vertical planes are fixed hydraulic head which is set as equal to the initial hydraulic head of each layer pumping is simulated by applying a prescribed flow rate on the gridpoints of the screen in the confined aquifer 3 4 input parameters the elastic perfectly plastic constitutive law with the mohr coulomb yielding criteria is adopted to describe the soil behavior the soil is assumed to be isotropic the behavior of clay layers was assumed to be undrained and drained strength parameters of soil were adopted in sand layers the unit weight void ratio and hydraulic conductivity can be determined from experiments and the elastic modulus e was calculated from the shear modulus g and the shear modulus can was calculated based on the formula suggested by lim et al 2010 g 0 5g 0 0 5 20 2z mpa where g 0 is small strain shear modulus z is buried depth of soil soil cohesion can be taken as the undrained shear strength s u where s u can be calculated from the following equation s u 20 2z kpa dong et al 2016 3 5 validation in this section the hydro mechanical coupling method is applied to simulate a field pumping test of the sk building excavation in shanghai numerical results are compared with the monitoring data to verify its reliability of the numerical method and rationality of the input parameters according to the geotechnical investigation report a paleochannel crossed the investigated area the stratum was mainly composed of saturated clay silt and sandy soils within the depth of 150 m the profiles of the soil layers and maas are shown in fig 6 it is seen that the maas within the depth of 80 m at the site was composed of a phreatic aquifer in the shallow soil layer a micro confined aquifer in layers ⑤2 1 ⑤2 3 and ⑤3 2 and the second confined aquifer in the layer ⑨ the piezometric head of the micro confined aquifer ranged from 3 to 11 m below the ground surface bgs during the geotechnical investigation the piezometric head of the micro confined aquifer in the layer ⑤2 3 was about 5 26 m bgs a pumping well pw1 and an observation well ow1 were installed within the confined aquifer in the layer ⑤2 3 the structures of the pumping well is shown in fig 6 the maximum depth of the pumping well was 32 m and the burial depth of filter ranged from 27 to 31 m bgs the bore diameter was 600 mm and the well pipe was made of steel pipe with a diameter of 273 mm the burial depth of the observation well and the filter were the same as those of the pumping well with a hole diameter of 160 mm and the well pipe was made of pvc with a diameter of 70 mm in order to monitor the dewatering induced adverse impact on the surrounding environment settlement measurement points were installed near the pumping well fig 7 shows the layouts of pumping well observation well and ground settlement monitoring points the pumping continued for 48 h with a constant discharge rate of 2 7 m3 h the pumping test is simulated by the presented coupling method in order to eliminate the influence of the boundary conditions on the calculation results the model takes the center point of the pumping well as the center with an expansion of 325 m outwards the model range is 650 m 650 m in plane and 80 m in depth the initial hydraulic head of the phreatic aquifer is set to the ground surface and initial hydraulic head of the confined aquifers the micro confined aquifer and the second confined aquifer are set as 5 26 m bgs the hydraulic heads at four vertical planes are set to be constant and equal to the initial values the elastic perfectly plastic constitutive model and mohr coulomb criterion are used in the numerical analysis the unit weight void ratio and hydraulic conductivity is determined from experiments and the elastic modulus e is calculated from the shear modulus g the cohesion the friction angle and the void ratio are determined based on laboratory tests of the soil properties table 3 lists the input soil parameters for the numerical simulation fig 8 shows the comparison between the observed and calculated groundwater drawdown in observation well g1 during the pumping test at the beginning of the test a small discrepancy can be observed between the observed and calculated values which may be resulted from the discreteness and heterogeneity of the soil with the proceeding of pumping the agreement becomes much better and finally the calculated drawdown is in line with the field data fig 9 compares the observed and calculated ground surface settlements during the test it can be seen that the both calculated and observed settlements increase gradually with the elapsed pumping time most of the calculated values are in good agreement with the field data while small discrepancies can be observed between some of the calculations and field data this might be attributed to precision of measuring instrument since the maximum settlement in this test was small zhang et al 2017a examined the accuracy of this coupled numerical method for groundwater drawdown and soil deformations by a field pumping recharge test they found that the hydro mechanical coupling analysis method is appropriate for studying the groundwater drawdown in confined aquifer and the vertical deformation of soil induced by dewatering in a confined aquifer 4 hydro mechanical coupling analysis the coupling numerical method is adopted to examine the inconsistent between groundwater drawdown and deep soil deformation caused by dewatering in maas to simplify the study a representative model with the phreatic aquifer and the 1st confined aquifer is developed considering the pumping test adopted in the validation simulation is a special case a new model is developed with representative stratum configuration and soil parameters to investigate the responses of soil and groundwater to pumping 4 1 model description the typical hydrological configuration of shanghai soft soil deposits includes one phreatic aquifer two confined aquifers and two aquitards within the depth of 80 m as shown in fig 10 the phreatic aquifer aq0 is located in the shallow layer with a thickness of 15 m it consists of silty clay and muddy silty clay the first confined aquifer aqi is composed of sandy silt and silt with a thickness of 10 m the two aquifers are separated by the first aquitard adi which is located at 15 m 30 m below the ground surface and mainly composed of silty clay and clay under the first confined aquifer there are the second aquitard adⅱ and the second confined aquifer aqii with an identical thickness of 20 m the initial hydraulic water level of the phreatic water is assumed to be at the ground surface and the piezometric head of the confined aquifer is assumed to be 5 0 m below the ground surface to better understand the development of drawdown and deformation a series of monitoring points a e are set to the top the bottom and the middle of each layer in this study it is assumed that the pumping well w1 is a fully penetrated well with a pumping rate of 21 7 m3 h and has a screen of 10 m in length equal to the thickness of the confined aquifer assuming that the maximum drawdown depth is 25 m k in the confined aquifer is 1 10 10 5 m s and r calculated from eq 9 is 244 m to eliminate the effects of the boundaries on the water level and stratum deformation several trial simulations have been carried out by increasing model dimension finally the whole calculation area of the model is set to be 1600 m by 1600 m in plane and 80 m in depth the finite difference mesh of the numerical analysis accounts for five layer strata in the vertical direction it is divided into 33 layers fig 11 the total numbers of nodes and elements in the model are 171 394 and 161 700 respectively the initial hydraulic heads of the phreatic aquifer and the confined aquifer are set to be at ground surface and 5 0 m below the ground surface respectively the top head boundary condition is free and the other head boundaries are constant hydraulic head boundaries whose head equals the initial hydraulic head parameter values used in the fdm model table 4 are representative for typical strata in shanghai zhang et al 2017a wu et al 2016 4 2 results and discussion 4 2 1 characteristics of groundwater drawdown since the model is composed of multiple aquifers and aquitards with different permeability difference of drawdown variations in these strata are expected fig 12 illustrates the developments of the groundwater drawdown under the semi log coordinate system it can be seen from this figure that responses of groundwater in the multi aquifer strata can be divided into four stages i e s1 s4 as shown in fig 12 stage ⅰ s1 at the beginning of the pumping since dewatering is performed in aqi the groundwater drawdown in this aquifer increases significantly and continuously however the drawdowns in other strata can be neglected this stage has a duration of 1 104s stage ⅱ s2 in this stage drawdowns also appear at points b and d in the aquitards it can be inferred that the pore pressure in aquitards are affected by drawdown in the confined aquifer at the end of this stage the water level in the confined aquifer reaches a stable state whilst the drawdown in phreatic aquifer can be neglected stage iii s3 in this stage a vertical leakage recharge occurs and the water level of the phreatic aquifer begins to decline it can be inferred that the pumped water comes from both aquifers and aquitards in this stage the drawdowns in two aquitards reach constant values stage ⅳ s4 finally groundwater flow in all strata reach a stable state and pumped water comes from all the strata 4 2 2 characteristics of vertical deformation it is known that drawdown caused by pumping in a confined aquifer will increase the soil effective stress and result in soil compression zhang et al 2017b pujades et al 2017 the soil compression can be roughly calculated by the use of the formula δ z s se δ p b as mentioned before in this study the soil deformation is obtained by the hydro mechanical coupling method fig 13 plots the variations of the average strain ε z of each stratum under a semi log coordinate system where the average strain ε z is defined as the mean vertical strain and can be obtained dividing the vertical soil compression s si by the stratum thickness h i in each soil layer the positive values of ε z represent the expansion and negative values represent the compression it is seen that variations of ε z are closely associated with the changes of groundwater drawdowns as shown in fig 13 a and b at stage ⅰ s1 soil compression in the confined aquifer increases significantly because of the noticeable increase of drawdown in the confined aquifer however small expansions are observed in the rest layers at r 0 as shown in fig 13 a these expansions are mainly attributed to the fact that the sudden pumping in the confined aquifer results in an unloading effect which leads to the local expansions in the adjacent aquitards in stage ⅱ s2 the soil compression in the confined aquifer tends to be stable since the drawdown reaches a stable state at the end of this stage the soil in aquitard i and aquitard ii begins to compress as the pumping continues during stage iii s3 the expansion in the phreatic aquifer decreases and the stratum compression increases rapidly owing to the decline of water head of the phreatic aquifer the compression in two aquitards stabilize at the end of this stage finally the stratum compression and drawdown in all layers reach a stable state in stage ⅳ s4 fig 14 demonstrates the contours of soil vertical displacement at the end of the four typical stages where a positive value represents uplift and negative value represents settlement it is seen that the vertical deformation is symmetrically distributed and varies with the pumping time at stage i s1 the deformation is concentrated near the pumping well in the confined aquifer as shown in fig 14 a the maximum settlement is located at the top of the confined aquifer and local small heave is observed in aquitard ⅱ as the pumping continues both the region and the value of soil deformation increase significantly and the local heave in aquitard ii decreases gradually and disappears in stage iii s3 as shown in fig 14 b d the locations of the maximum soil vertical deformations move upward from the top of the confined aquifer in stage i s1 to the ground surface in stage iv s4 zheng et al 2014 interpreted this phenomenon by the soil arching effect at stage i local pressure relief in the confined aquifer near the pumping well results soil arching due to the shear strength of soil it reduces the settlements in the upper layer therefore the maximum vertical deformation appears at the top of the confined aquifer as the pumping continues from stages ⅱ to ⅳ s2 s4 the soil arching moves upward meanwhile the upper layer releases groundwater and results in soil compression the settlements in the upper layer increase and the maximum values move up to the ground surface gradually 4 2 3 characteristics of horizontal deformation it is widely accepted that dewatering in confined aquifer will result in both vertical and horizontal soil deformations due to the seepage force and soil deformation coordination previous studies indicated that the effect of dewatering induced horizontal deformation on surroundings cannot be neglected zhang et al 2017b pujades et al 2017 fig 15 demonstrates the distribution of horizontal deformations in different soil layers at the end stage 4 s4 where negative value represents the deformation toward the pumping well as can be seen from fig 15 horizontal deformations of each soil layer show similar characteristic the horizontal deformations near the well increase rapidly and then decrease slowly with increase of the radial distance the maximum horizontal displacement of each layer is located within 1 0d 3 0d where d is the buried depth of the confined aquifer fig 16 shows the profiles of the horizontal deformation along with the depth at r 50 m in each stage where the negative value represents the deformation towards the pumping well it is seen that the horizontal deformation increases with the pumping process the maximum horizontal deformation is located at the ground surface with a value of about 1 8 mm while the maximum ground surface settlement is about 10 mm in addition the maximum horizontal and vertical deformation in aqi are 1 2 mm and 5 9 mm respectively it is seen that the pumping induced horizontal deformation cannot be ignored since it has the same order of magnitude as the vertical deformation 4 3 comparative analysis in practical engineering the ground surface settlement can be roughly calculated by the following non coupling formula based on the principle of effective stress 10 δ v ψ i 1 i s i ψ i 1 i δ p i e si h i ψ i 1 i γ w δ h i e si h i where i is the number of the soil layers from the bottom layer to ground surface s i is the vertical deformation mm ψ is an empirical coefficient and is suggested to be 0 2 as stated in the research of mohurd 2002 δ p i is the change of pore pressure pa e si is the compression modulus pa h i is the thickness m γ w is the weight of water n m 3 δ h i is the groundwater drawdown m fig 17 presents a comparison of the ground surface settlement distributions between the numerical results and the calculated values where the groundwater drawdowns used in eq 10 come from the numerical results it can be seen that for the area outside the radius of 3d where d is the buried depth of the first confined aquifer the calculated values are in line with the numerical results however a discrepancy is observed from the area inside the radius of 3d and the deviation increase with decrease of the radius zheng et al 2014 pointed out that local pumping in the confined aquifer will result in soil arching due to soil shear strength which will decrease the stratum settlements near the pumping well the soil arching effect decreases with increase of the distance away from the pumping well however this effect cannot be addressed in the non coupling formula i e eq 10 to improve the accuracy of eq 10 the empirical coefficient ψ needs to be improved the modefied coefficient denoted as ψ can be obtained by dividing the expression as 11 ψ δ nv i 1 i γ w δ h i e si h i where δ nv is the ground surface settlement obtained by the coupled numerical simulation fig 18 plots the distribution of the modified empirical coefficient ψ with distance from the pumping well for the area outside the circle with a radius of 3d the modified coefficient ψ is set to be a constant value of 0 2 which is equal to the specified value from mohurd 2002 whist for the area inside the circle with a radius of 3d ψ is linearly distributed in a semi log coordinate system 12 ψ 0 07 lg r 0 064 r 3 d ψ 0 2 r 3 d where r is the radius from the pumping well a comparison is presented to prove the applicability of modified ψ for deep soil deformation fig 19 shows the comparison of the soil vertical deformation between the numerical results and the calculated values by using the modified coefficient at different distances it can be seen that the calculated vertical deformation with modified coefficient ψ agrees well with the numerical results it is evident that non coupling formula with modified coefficient ψ can be used for deep soil deformation caused by dewatering in confined aquifer note that for the soil deformation near the well a small deviation is observed between the calculated and the numerical settlement from 40 m to 60 m below the ground surface this deviation is mainly attributed to the small expansion of the soil below the confined aquifer during dewatering 5 effect of the micro confined aquifer 5 1 model description the aforementioned analyses denoted as case ⅰ neglect the existence of the micro confined aquifer above the 1st confined aquifer in this section a thin micro confined aquifer is embedded within the aquitard i denoted as case ⅱ fig 20 presents the section view of the strata with a micro confined aquifer the micro confined aquifer is located within the adⅰ and the range is set to 20 m 22 5 m below the ground surface the strata are assumed to be isotropic and homogeneous the hydraulic conductivity of this layer is about 5 79e 06 m s the monitoring points including point a e are located at different layers and they are used to monitor the drawdowns and vertical soil displacement in the different layers to examine the influences of the micro confined aquifer on stratum deformations and groundwater drawdowns the numerical simulation performed in case ⅱ is completely identical to that in case ⅰ except for the additional embedded micro confined aquifer in case ⅱ 5 2 results and discussion fig 21 illustrates the development of drawdown at the typical points during the dewatering it is seen that the drawdown at points a and b in case ii is obviously smaller than in case i and drawdown at point b shows a larger deviation than that at point a as shown in fig 21 a this is due to the fact that the micro confined aquifer has a better permeability than that of adi thus the drawdown can be relieved significantly because the groundwater in that aquifer can be supplied horizontally a smaller drawdown in the micro confined aquifer indicates a smaller water head difference between the micro confined aquifer and phreatic aquifer consequently the drawdown in the phreatic aquifer can also be relieved in contrast the influence of the micro confined aquifer on the drawdowns of aqi and adii can be neglected as shown in fig 21 b fig 22 presents the comparison of soil deformation between case ⅰ and case ⅱ fig 22 a and b show the compression variation of stratum above the aqi similar to the responses of the drawdowns the presence of the micro confined aquifer in case ⅱ can mitigate the vertical deformation and stratum compression in the layers above the pumping aquifer as expected the influence of the micro confined on vertical deformations of strata below the adi can be neglected fig 23 presents the comparison of horizontal deformation profiles at r 50 m in stage 4 between case i and case ii where the negative values represent the deformation towards the pumping well it can be seen that the horizontal deformations in these two cases show similar characteristics where the maximum value is located at the ground surface and the deformation decreases with the increase of the depth however the magnitude of the horizontal deformation in case2 is obviously smaller than that in case1 which may be attributed to the following two reasons i the presence of aqi0 reduces the decrease of the hydraulic head in the soil layers above aqi which results in the decrease of seepage force in these soil layers and ii comparing with the deformation in case1 the smaller vertical compression of the soil layers above aqi in case2 results in a smaller soil horizontal deformation due to the coordination of soil deformation therefore the presence of a micro confined aquifer can effectively relieve the pumping induced soil horizontal deformation 6 conclusions in this study a coupled hydro mechanical analysis is conducted to investigate the groundwater drawdown and deformation in maas to pumping in a confined aquifer the drawdown and deformation processes in each stratum are analyzed the ground surface settlement is compared to that obtained by a non coupling method based on the effective stress principle developed by terzaghi the following conclusions can be drawn 1 the groundwater drawdown caused by dewatering in confined aquifer shows a linear distribution along with the radial distance under logarithmic coordinates which is consistent with the theoretical distribution of the complete penetration well theory however the ground surface settlement is fitted by three straight lines with reflection points located at r 0 4 d i 0 and r 1 5 d i 0 where d i 0 is the buried depth of the top surface of the pumping aquifer this discrepancy is mainly attributed to the hydro mechanical coupling effect caused by dewatering 2 drawdown responses of multi aquifer aquitard system can be divided into four typical stages soil deformation in each stratum is closely related to the decline of the water head however local expansions can be observed in early dewatering in aquitards and phreatic aquifer due to soil arching with the proceeding of dewatering the expansion phenomenon disappears and soil compression occurs in all the strata finally both groundwater flow and the soil deformation in each stratum reaches a stable state 3 the non coupling formula provides a good estimation of ground settlement outside the circle with a radius three times of the buried depth of pumping aquifer however it overestimates the ground settlement near the well and the deviation increases with the decrease of the radial distance to improve its applicability a modification of the empirical coefficient is suggested which varies linearly with the radial distance from the pumping well under logarithmic coordinates the non coupling formula with the modified coefficient can provide a good settlement estimation of deep soil layers 4 due to horizontal and vertical cross flow supplement the presence of the micro confined aquifer can relief the groundwater drawdowns of phreatic aquifer and aquitard above the pumping aquifer consequently both the soil compression and horizontal displacement in these strata can be relieved however due to the low hydraulic conductivity and large thickness of the aquitard the vertical cross flow supplement to the pumping aquifer is insignificant as a result the influence of the micro confined aquifer on drawdowns and vertical deformations of strata below that aquitard can be neglected credit authorship contribution statement ming guang li conceptualization formal analysis funding acquisition jin jian chen funding acquisition supervision xiao he xia formal analysis supervision yang qing zhang methodology formal analysis da fa wang writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was substantially supported by the national natural science foundation of china grant nos 41727802 41602283 and 41977216 and by the shanghai rising star program grant no 19qc1400800 the authors are grateful to these programs special thanks also go to the anonymous peer reviewers whose comments greatly improved the rigorousness and quality of this paper 
5284,groundwater withdrawn in a multi aquifer aquitard system maas may cause vertical cross flow supplement and result in groundwater drawdown and soil deformation in both aquifers and aquitards this study investigates the responses of groundwater and deep soils to dewatering in the maas through statistical and numerical analyses field data of ten pumping tests conducted in the maas in shanghai urban area are collected and a statistical analysis for groundwater drawdown and ground settlement is performed field measurements indicate that the drawdowns are distributed linearly along radial distance in logarithmic coordinates however the ground settlements are disproportional to the drawdowns and show a trilinear distribution along the radial direction to examine the inconsistent distributions of the groundwater drawdown and the ground settlement representative dewatering models are developed numerically and solved explicitly by the coupled biot s three dimensional consolidation theory numerical results of dewatering in maas are compared to those obtained by a non coupling formula moreover effect of a micro confined aquifer on drawdown and soil deformation is discussed it is found that soil compression in each stratum is closely related to the decline of the pore pressure whilst small expansions are observed in early dewatering stages due to the hydro mechanical coupling effect the ground surface settlement near the pumping well obtained by the coupled numerical method is significantly smaller than that by the non coupling formula owing to horizontal and vertical cross flow supplement an additional aquifer embedded in aquitard above the pumping aquifer can relief both the groundwater drawdown and ground surface settlement significantly keywords dewatering multi aquifer aquitard system coupled hydro mechanical analysis groundwater drawdown stratum deformation 1 introduction in the past decade dewatering in confined aquifers required for deep excavations increased dramatically with the rapid development of underground exploitation in coastal regions of china huang and han 2009 chen et al 2013 tan and wang 2013a b shen et al 2013 wang et al 2013b wen et al 2017 xu et al 2017 li et al 2017 2018 2019 xu et al 2019 wu et al 2020 according to the principle of effective stress developed by terzaghi depressurization in confined aquifers will inevitably increase the effective stress of soil consequently causing soil compression sheng and sloan 2003 peng et al 2011 pujades et al 2014a b 2016 wang et al 2017 2019a b in a metropolitan area where the density of constructions is high if the dewatering takes place in a thick sand stratum with very high piezometric pressure excessive ground settlement may occur shen et al 2015 lyu et al 2016 wang et al 2018a b zeng et al 2019a b zheng et al 2019 li et al 2020 and potentially damage surrounding infrastructures e g building basements shield tunnels pipe lines etc chen and xiang 2006 xiang et al 2008 omer 2012 wu et al 2019 to ensure the safety of existing facilities it is essential to understand responses of groundwater and deep soil layers to dewatering in confined aquifers regarding dewatering in a confined aquifer many studies have been carried out through field measurements lu et al 2015 wang et al 2012 tan et al 2014 zhu et al 2015 zheng et al 2018 numerical simulations majumdar et al 2008 wang et al 2013a lin et al 2010 zhang et al 2018 zeng et al 2018 and theoretical analyses theis 1935 cooper and jacob 1946 sethi 2011 some researchers focused on drawdown distributions outside of foundation pits with sealing curtains partially inserted into the aquifer shen et al 2013 wang et al 2013b xu et al 2014 wu et al 2015a b pujades et al 2016 wen et al 2017 and others paid attention to the ground surface settlements budhu and adiyaman 2010 lopez fernandez et al 2013 niu et al 2013 omer 2012 shen and xu 2011 as well as dewatering induced adverse impacts on buildings and structures chen and xiang 2006 yoo et al 2012 in most related studies the ground settlement was roughly calculated by the use of the uncoupled method in which a seepage analysis software was adopted to get the changes of pore pressures in aquifers and aquitards the ground settlement could be calculated by the formula δ z s se δ p b where δ z is the settlement s se is the portion of the specific storage coefficient associated to soil deformation b is the thickness of the porous medium and δ p is the drop in pressure head in this formula the effect of horizontal deformation on the vertical settlement is ignored which is only suitable for one dimensional problems for a better understanding of the three dimensional soil deformation responses hydro mechanical coupling analyses had been carried out using biot s consolidation theory zheng et al 2014 investigated the settlement mechanism of soils induced by local pressure relief in a confined aquifer and found the maximum soil settlement appears at the top of the stratum more recently pujades et al 2017 reported that settlements were disproportional to drawdown inside a circle with a radius equal to 0 7 times the thickness of the pumped porous medium and proposed a calculation procedure to estimate the settlements around the pumping wells however these findings and proposed method came from a simplified model with only one stratum a single aquifer considering the interactions between aquifers and aquitards the responses of groundwater and deep soil layers will be more complicate to dewatering in multi aquifer aquitard system maas as reported in our primary study wang et al 2019a b the proposed method might be inapplicable in maas and formulas for estimating the deep soil deformations need further examination in this study a statistical analysis of groundwater drawdown and ground surface settlement of ten pumping tests in shanghai urban area is conducted for further examination hydro mechanical coupling analysis using the biot s three dimensional consolidation theory is performed numerical results of ground surface settlement are compared to that obtained by a non coupling formula based on the effective stress principle developed by terzaghi moreover effect of micro confined aquifer on drawdown and soil deformation is discussed 2 statistical analysis 2 1 description of confined aquifers in shanghai shanghai is located on the deltaic deposits of the yangtze river the typical quaternary deposits in shanghai are composed of one artesian aquifer and five confined aquifer layers in addition a micro confined aquifer is distributed sporadically at the branch channel above the first confined aquifer the groundwater system is a typical alternated multi aquifer aquitard system maas as shown in fig 1 since the buried depths of the micro confined aquifer and the first confined aquifer aqi are relatively shallow with high water heads it has a significant influence on the underground constructions such as deep excavations therefore many studies have been carried out regarding the dewatering in these two confined aquifers shen and xu 2011 tan et al 2014 wang et al 2018a b wang et al 2019a b the micro confined aquifer is distributed sporadically at the branch channel and estuarine bay to the southeast as shown in fig 2 the top surface is buried at the depth of 15 m 22 m below the ground surface with the thickness of 2 m 20 m the first confined aquifer is widely distributed in shanghai as shown in fig 3 the top surface is generally 15 m 54 m below the ground surface with the thickness of 3 m 40 m the buried depth of this confined aquifer increases significantly from north to south and east and the thickness changes greatly considering spatial variability of hydraulic characteristics of these two confined aquifers many in situ pumping tests were performed prior to underground construction to examine the hydraulic connections and hydraulic parameters in this study field data of ten pumping tests are collected to investigate the groundwater drawdown and ground surface settlement caused by dewatering in confined aquifers 2 2 database establishment a total of 5 cases of in situ pumping tests in the micro confined aquifer are collected where the locations of test sites are shown in fig 2 the dewatering aquifer lies within the layers of sandy silt and silty sand layers ⑤2 and ⑤3 2 table 1 lists the detail information of the collected pumping tests in the micro confined aquifer moreover a total of 5 cases of in situ pumping test in the 1st confined aquifer are collected where the locations of test sites are shown in fig 3 the detail information of the pumping tests in the 1st confined aquifer is list in table 2 note that in case 5 there is a hydraulic connection between the first confined aquifer ⑦ and second confined aquifer ⑨ during pumping tests the groundwater drawdown and ground surface settlement were measured by the use of observation wells and settlement surveying instruments since the responses of the groundwater and deep soil layers to dewatering in maas are complicated only groundwater drawdown and ground settlement at typical stages are presented and discussed the final drawdowns and corresponding settlements at center of pumping wells are listed in tables 1 and 2 2 3 analysis of groundwater drawdown fig 4 shows the drawdown distribution along the radial distance where the drawdown δ h is normalized by that at center of pumping wells δ h c and the radial distance r is normalized by the buried depth of the top surface of the confined aquifers d i 0 as can be seen from the figure the drawdowns caused by dewatering in both the micro confined aquifer and the 1st confined aquifer are linearly distributed along the radial distance under logarithmic coordinates moreover the theim results are added to the figures for comparison where the parameters for the theim formula are listed in tables 1 and 2 it can be seen from fig 4 a that the theim results of groundwater drawdown in micro confined aquifer agree well with the filed measurements however small discrepancies can be observed from fig 4 b between the theim results and field measurements in the 1st confined aquifer this might be attributed to the deviation of soil parameters it is known that the groundwater drawdown is sensitive to the hydraulic conductivity while this parameter can rarely be accurately determined by field tests or labortory tests although dewatering induced drawdown in confined aquifers might be affected by many factors such as hydraulic connection hydraulic parameters insert depth of pumping wells etc field data confirm the results of theim equation of steady flow in the fully penetrating well 2 4 analysis of ground surface settlements fig 5 depicts the distribution of ground surface settlement along the radial distance where the ground settlement δ v is normalized by that at the center of pumping wells δ vc and the radial distance r is normalized by the buried depth of the confined aquifers d i 0 it can be seen from fig 5 a and b that the normalized ground settlement caused by dewatering in micro confined aquifer and the 1st confined aquifer have similar distributions in both figures the normalized settlement points show a concentrated distribution however these points cannot be fitted by a single line as the drawdown it should be divided into three fitting lines with reflection points located at r 0 4 d i 0 and r 1 5 d i 0 it is seen that the ground settlements near the pumping wells and far away from the pumping wells decreases slight with the increase of radial distance whilst the settlement changes dramatically with radial distance in the range of 0 4 d i 0 r 1 5 d i 0 based on the analyses above it can be tentatively concluded that the groundwater drawdown has a linear distribution under logarithmic coordinates while the ground settlement is proportional to the drawdown this finding is similar to the numerical results reported by pujades et al 2017 for better understanding the responses of groundwater and deep soil layers to dewatering in maas hydro mechanical coupling analyses are carried out and numerical results are analyzed in association with the those obtain by a non coupling formula 3 hydro mechanical coupling method the variables involved in the description of fluid flow through porous media are pore pressure saturation and the three components of the specific discharge vector these variables are related by the fluid mass balance equation darcy s law for fluid transport a constitutive equation specifying the fluid response to changing in pore pressure saturation and volumetric strains and an equation of state relating pore pressure to saturation in the unsaturated range pore pressure influences are involved in mechanical constitutive laws to complete the fluid mechanical coupling assuming that the volumetric strains are known substitution of the mass balance equation into the fluid constitutive relation using darcy s law yields a differential equation in terms of pore pressure and saturation that may be solved for particular geometries properties boundaries and initial conditions 3 1 governing differential equations a confined aquifer of large horizontal extent is characterized by its initial pore pressure and initial isotropic stress water is pumped from the well at a constant rate q per unit depth the elastic porous medium is homogeneous and isotropic in the same layers and the flow of groundwater is governed by darcy s law transient effects are linked to the compressibility of water and the soil matrix a cylindrical system of coordinates is adopted with the z axis being vertically upward substitution of the transport law in the fluid mass balance equation gives 1 p t m k 2 p α ε kk t where k is the hydraulic conductivity p is the pore pressure m is the biot modulus and α is the biot coefficient the elastic relations between effective stresses and strains are small strain 2 σ kk σ kk 0 α p p 0 α 1 ε kk where σ kk is the principal stresses ε kk is the principal strains σ kk 0 and p 0 are the initial values of the principal stresses and pore pressure respectively and α 1 k 4 3 g partial differentiation with respect to time of the eq 2 3 σ kk t α p t α 1 ε kk t substituting eq 3 into eq 1 yields the diffusion equation for pore pressure p 4 p t c 2 p α α 1 s σ kk t where c k s is the diffusion coefficient s 1 m α 2 α 1 is the storage coefficient for an axisymmetric problem the laplacian of p can be expressed as 5 2 p 2 p r 2 1 r p r 2 p z 2 the initial condition to solve this partial differential equation is 6 p r θ z t 0 p 0 r θ z where p 0 r θ z is the initial pore pressure at the point r θ z the boundary conditions are 7 lim r p r θ z p 0 r θ z 8 lim r 0 2 π r p r q k 3 2 influence range for the sake of calculation convenience a constant head boundary is selected in the plane wu et al 2016 the influence radius of the well r can be initially calculated from 9 r 10 s w k where s w is the drawdown of the pumping well k is the hydraulic conductivity of the soil to eliminate the effects of the boundaries on the water level and stratum deformation additional trail simulations should be carried out by increasing model dimension 3 3 initial boundary conditions boundary conditions for coupled hydro mechanical analysis includes mechanical boundary condition and hydraulic boundary condition as for the mechanical boundary condition the soil movements are set to be free at the top of the model and are restrained in all directions u x u y u z 0 at the bottom the soil movements in all vertical boundaries are constrained in its own normal directions and free in other directions during the simulation for the hydraulic boundary condition hydraulic head at the top of the model is set to be free and the model bottom is set to be impervious boundary conditions for four vertical planes are fixed hydraulic head which is set as equal to the initial hydraulic head of each layer pumping is simulated by applying a prescribed flow rate on the gridpoints of the screen in the confined aquifer 3 4 input parameters the elastic perfectly plastic constitutive law with the mohr coulomb yielding criteria is adopted to describe the soil behavior the soil is assumed to be isotropic the behavior of clay layers was assumed to be undrained and drained strength parameters of soil were adopted in sand layers the unit weight void ratio and hydraulic conductivity can be determined from experiments and the elastic modulus e was calculated from the shear modulus g and the shear modulus can was calculated based on the formula suggested by lim et al 2010 g 0 5g 0 0 5 20 2z mpa where g 0 is small strain shear modulus z is buried depth of soil soil cohesion can be taken as the undrained shear strength s u where s u can be calculated from the following equation s u 20 2z kpa dong et al 2016 3 5 validation in this section the hydro mechanical coupling method is applied to simulate a field pumping test of the sk building excavation in shanghai numerical results are compared with the monitoring data to verify its reliability of the numerical method and rationality of the input parameters according to the geotechnical investigation report a paleochannel crossed the investigated area the stratum was mainly composed of saturated clay silt and sandy soils within the depth of 150 m the profiles of the soil layers and maas are shown in fig 6 it is seen that the maas within the depth of 80 m at the site was composed of a phreatic aquifer in the shallow soil layer a micro confined aquifer in layers ⑤2 1 ⑤2 3 and ⑤3 2 and the second confined aquifer in the layer ⑨ the piezometric head of the micro confined aquifer ranged from 3 to 11 m below the ground surface bgs during the geotechnical investigation the piezometric head of the micro confined aquifer in the layer ⑤2 3 was about 5 26 m bgs a pumping well pw1 and an observation well ow1 were installed within the confined aquifer in the layer ⑤2 3 the structures of the pumping well is shown in fig 6 the maximum depth of the pumping well was 32 m and the burial depth of filter ranged from 27 to 31 m bgs the bore diameter was 600 mm and the well pipe was made of steel pipe with a diameter of 273 mm the burial depth of the observation well and the filter were the same as those of the pumping well with a hole diameter of 160 mm and the well pipe was made of pvc with a diameter of 70 mm in order to monitor the dewatering induced adverse impact on the surrounding environment settlement measurement points were installed near the pumping well fig 7 shows the layouts of pumping well observation well and ground settlement monitoring points the pumping continued for 48 h with a constant discharge rate of 2 7 m3 h the pumping test is simulated by the presented coupling method in order to eliminate the influence of the boundary conditions on the calculation results the model takes the center point of the pumping well as the center with an expansion of 325 m outwards the model range is 650 m 650 m in plane and 80 m in depth the initial hydraulic head of the phreatic aquifer is set to the ground surface and initial hydraulic head of the confined aquifers the micro confined aquifer and the second confined aquifer are set as 5 26 m bgs the hydraulic heads at four vertical planes are set to be constant and equal to the initial values the elastic perfectly plastic constitutive model and mohr coulomb criterion are used in the numerical analysis the unit weight void ratio and hydraulic conductivity is determined from experiments and the elastic modulus e is calculated from the shear modulus g the cohesion the friction angle and the void ratio are determined based on laboratory tests of the soil properties table 3 lists the input soil parameters for the numerical simulation fig 8 shows the comparison between the observed and calculated groundwater drawdown in observation well g1 during the pumping test at the beginning of the test a small discrepancy can be observed between the observed and calculated values which may be resulted from the discreteness and heterogeneity of the soil with the proceeding of pumping the agreement becomes much better and finally the calculated drawdown is in line with the field data fig 9 compares the observed and calculated ground surface settlements during the test it can be seen that the both calculated and observed settlements increase gradually with the elapsed pumping time most of the calculated values are in good agreement with the field data while small discrepancies can be observed between some of the calculations and field data this might be attributed to precision of measuring instrument since the maximum settlement in this test was small zhang et al 2017a examined the accuracy of this coupled numerical method for groundwater drawdown and soil deformations by a field pumping recharge test they found that the hydro mechanical coupling analysis method is appropriate for studying the groundwater drawdown in confined aquifer and the vertical deformation of soil induced by dewatering in a confined aquifer 4 hydro mechanical coupling analysis the coupling numerical method is adopted to examine the inconsistent between groundwater drawdown and deep soil deformation caused by dewatering in maas to simplify the study a representative model with the phreatic aquifer and the 1st confined aquifer is developed considering the pumping test adopted in the validation simulation is a special case a new model is developed with representative stratum configuration and soil parameters to investigate the responses of soil and groundwater to pumping 4 1 model description the typical hydrological configuration of shanghai soft soil deposits includes one phreatic aquifer two confined aquifers and two aquitards within the depth of 80 m as shown in fig 10 the phreatic aquifer aq0 is located in the shallow layer with a thickness of 15 m it consists of silty clay and muddy silty clay the first confined aquifer aqi is composed of sandy silt and silt with a thickness of 10 m the two aquifers are separated by the first aquitard adi which is located at 15 m 30 m below the ground surface and mainly composed of silty clay and clay under the first confined aquifer there are the second aquitard adⅱ and the second confined aquifer aqii with an identical thickness of 20 m the initial hydraulic water level of the phreatic water is assumed to be at the ground surface and the piezometric head of the confined aquifer is assumed to be 5 0 m below the ground surface to better understand the development of drawdown and deformation a series of monitoring points a e are set to the top the bottom and the middle of each layer in this study it is assumed that the pumping well w1 is a fully penetrated well with a pumping rate of 21 7 m3 h and has a screen of 10 m in length equal to the thickness of the confined aquifer assuming that the maximum drawdown depth is 25 m k in the confined aquifer is 1 10 10 5 m s and r calculated from eq 9 is 244 m to eliminate the effects of the boundaries on the water level and stratum deformation several trial simulations have been carried out by increasing model dimension finally the whole calculation area of the model is set to be 1600 m by 1600 m in plane and 80 m in depth the finite difference mesh of the numerical analysis accounts for five layer strata in the vertical direction it is divided into 33 layers fig 11 the total numbers of nodes and elements in the model are 171 394 and 161 700 respectively the initial hydraulic heads of the phreatic aquifer and the confined aquifer are set to be at ground surface and 5 0 m below the ground surface respectively the top head boundary condition is free and the other head boundaries are constant hydraulic head boundaries whose head equals the initial hydraulic head parameter values used in the fdm model table 4 are representative for typical strata in shanghai zhang et al 2017a wu et al 2016 4 2 results and discussion 4 2 1 characteristics of groundwater drawdown since the model is composed of multiple aquifers and aquitards with different permeability difference of drawdown variations in these strata are expected fig 12 illustrates the developments of the groundwater drawdown under the semi log coordinate system it can be seen from this figure that responses of groundwater in the multi aquifer strata can be divided into four stages i e s1 s4 as shown in fig 12 stage ⅰ s1 at the beginning of the pumping since dewatering is performed in aqi the groundwater drawdown in this aquifer increases significantly and continuously however the drawdowns in other strata can be neglected this stage has a duration of 1 104s stage ⅱ s2 in this stage drawdowns also appear at points b and d in the aquitards it can be inferred that the pore pressure in aquitards are affected by drawdown in the confined aquifer at the end of this stage the water level in the confined aquifer reaches a stable state whilst the drawdown in phreatic aquifer can be neglected stage iii s3 in this stage a vertical leakage recharge occurs and the water level of the phreatic aquifer begins to decline it can be inferred that the pumped water comes from both aquifers and aquitards in this stage the drawdowns in two aquitards reach constant values stage ⅳ s4 finally groundwater flow in all strata reach a stable state and pumped water comes from all the strata 4 2 2 characteristics of vertical deformation it is known that drawdown caused by pumping in a confined aquifer will increase the soil effective stress and result in soil compression zhang et al 2017b pujades et al 2017 the soil compression can be roughly calculated by the use of the formula δ z s se δ p b as mentioned before in this study the soil deformation is obtained by the hydro mechanical coupling method fig 13 plots the variations of the average strain ε z of each stratum under a semi log coordinate system where the average strain ε z is defined as the mean vertical strain and can be obtained dividing the vertical soil compression s si by the stratum thickness h i in each soil layer the positive values of ε z represent the expansion and negative values represent the compression it is seen that variations of ε z are closely associated with the changes of groundwater drawdowns as shown in fig 13 a and b at stage ⅰ s1 soil compression in the confined aquifer increases significantly because of the noticeable increase of drawdown in the confined aquifer however small expansions are observed in the rest layers at r 0 as shown in fig 13 a these expansions are mainly attributed to the fact that the sudden pumping in the confined aquifer results in an unloading effect which leads to the local expansions in the adjacent aquitards in stage ⅱ s2 the soil compression in the confined aquifer tends to be stable since the drawdown reaches a stable state at the end of this stage the soil in aquitard i and aquitard ii begins to compress as the pumping continues during stage iii s3 the expansion in the phreatic aquifer decreases and the stratum compression increases rapidly owing to the decline of water head of the phreatic aquifer the compression in two aquitards stabilize at the end of this stage finally the stratum compression and drawdown in all layers reach a stable state in stage ⅳ s4 fig 14 demonstrates the contours of soil vertical displacement at the end of the four typical stages where a positive value represents uplift and negative value represents settlement it is seen that the vertical deformation is symmetrically distributed and varies with the pumping time at stage i s1 the deformation is concentrated near the pumping well in the confined aquifer as shown in fig 14 a the maximum settlement is located at the top of the confined aquifer and local small heave is observed in aquitard ⅱ as the pumping continues both the region and the value of soil deformation increase significantly and the local heave in aquitard ii decreases gradually and disappears in stage iii s3 as shown in fig 14 b d the locations of the maximum soil vertical deformations move upward from the top of the confined aquifer in stage i s1 to the ground surface in stage iv s4 zheng et al 2014 interpreted this phenomenon by the soil arching effect at stage i local pressure relief in the confined aquifer near the pumping well results soil arching due to the shear strength of soil it reduces the settlements in the upper layer therefore the maximum vertical deformation appears at the top of the confined aquifer as the pumping continues from stages ⅱ to ⅳ s2 s4 the soil arching moves upward meanwhile the upper layer releases groundwater and results in soil compression the settlements in the upper layer increase and the maximum values move up to the ground surface gradually 4 2 3 characteristics of horizontal deformation it is widely accepted that dewatering in confined aquifer will result in both vertical and horizontal soil deformations due to the seepage force and soil deformation coordination previous studies indicated that the effect of dewatering induced horizontal deformation on surroundings cannot be neglected zhang et al 2017b pujades et al 2017 fig 15 demonstrates the distribution of horizontal deformations in different soil layers at the end stage 4 s4 where negative value represents the deformation toward the pumping well as can be seen from fig 15 horizontal deformations of each soil layer show similar characteristic the horizontal deformations near the well increase rapidly and then decrease slowly with increase of the radial distance the maximum horizontal displacement of each layer is located within 1 0d 3 0d where d is the buried depth of the confined aquifer fig 16 shows the profiles of the horizontal deformation along with the depth at r 50 m in each stage where the negative value represents the deformation towards the pumping well it is seen that the horizontal deformation increases with the pumping process the maximum horizontal deformation is located at the ground surface with a value of about 1 8 mm while the maximum ground surface settlement is about 10 mm in addition the maximum horizontal and vertical deformation in aqi are 1 2 mm and 5 9 mm respectively it is seen that the pumping induced horizontal deformation cannot be ignored since it has the same order of magnitude as the vertical deformation 4 3 comparative analysis in practical engineering the ground surface settlement can be roughly calculated by the following non coupling formula based on the principle of effective stress 10 δ v ψ i 1 i s i ψ i 1 i δ p i e si h i ψ i 1 i γ w δ h i e si h i where i is the number of the soil layers from the bottom layer to ground surface s i is the vertical deformation mm ψ is an empirical coefficient and is suggested to be 0 2 as stated in the research of mohurd 2002 δ p i is the change of pore pressure pa e si is the compression modulus pa h i is the thickness m γ w is the weight of water n m 3 δ h i is the groundwater drawdown m fig 17 presents a comparison of the ground surface settlement distributions between the numerical results and the calculated values where the groundwater drawdowns used in eq 10 come from the numerical results it can be seen that for the area outside the radius of 3d where d is the buried depth of the first confined aquifer the calculated values are in line with the numerical results however a discrepancy is observed from the area inside the radius of 3d and the deviation increase with decrease of the radius zheng et al 2014 pointed out that local pumping in the confined aquifer will result in soil arching due to soil shear strength which will decrease the stratum settlements near the pumping well the soil arching effect decreases with increase of the distance away from the pumping well however this effect cannot be addressed in the non coupling formula i e eq 10 to improve the accuracy of eq 10 the empirical coefficient ψ needs to be improved the modefied coefficient denoted as ψ can be obtained by dividing the expression as 11 ψ δ nv i 1 i γ w δ h i e si h i where δ nv is the ground surface settlement obtained by the coupled numerical simulation fig 18 plots the distribution of the modified empirical coefficient ψ with distance from the pumping well for the area outside the circle with a radius of 3d the modified coefficient ψ is set to be a constant value of 0 2 which is equal to the specified value from mohurd 2002 whist for the area inside the circle with a radius of 3d ψ is linearly distributed in a semi log coordinate system 12 ψ 0 07 lg r 0 064 r 3 d ψ 0 2 r 3 d where r is the radius from the pumping well a comparison is presented to prove the applicability of modified ψ for deep soil deformation fig 19 shows the comparison of the soil vertical deformation between the numerical results and the calculated values by using the modified coefficient at different distances it can be seen that the calculated vertical deformation with modified coefficient ψ agrees well with the numerical results it is evident that non coupling formula with modified coefficient ψ can be used for deep soil deformation caused by dewatering in confined aquifer note that for the soil deformation near the well a small deviation is observed between the calculated and the numerical settlement from 40 m to 60 m below the ground surface this deviation is mainly attributed to the small expansion of the soil below the confined aquifer during dewatering 5 effect of the micro confined aquifer 5 1 model description the aforementioned analyses denoted as case ⅰ neglect the existence of the micro confined aquifer above the 1st confined aquifer in this section a thin micro confined aquifer is embedded within the aquitard i denoted as case ⅱ fig 20 presents the section view of the strata with a micro confined aquifer the micro confined aquifer is located within the adⅰ and the range is set to 20 m 22 5 m below the ground surface the strata are assumed to be isotropic and homogeneous the hydraulic conductivity of this layer is about 5 79e 06 m s the monitoring points including point a e are located at different layers and they are used to monitor the drawdowns and vertical soil displacement in the different layers to examine the influences of the micro confined aquifer on stratum deformations and groundwater drawdowns the numerical simulation performed in case ⅱ is completely identical to that in case ⅰ except for the additional embedded micro confined aquifer in case ⅱ 5 2 results and discussion fig 21 illustrates the development of drawdown at the typical points during the dewatering it is seen that the drawdown at points a and b in case ii is obviously smaller than in case i and drawdown at point b shows a larger deviation than that at point a as shown in fig 21 a this is due to the fact that the micro confined aquifer has a better permeability than that of adi thus the drawdown can be relieved significantly because the groundwater in that aquifer can be supplied horizontally a smaller drawdown in the micro confined aquifer indicates a smaller water head difference between the micro confined aquifer and phreatic aquifer consequently the drawdown in the phreatic aquifer can also be relieved in contrast the influence of the micro confined aquifer on the drawdowns of aqi and adii can be neglected as shown in fig 21 b fig 22 presents the comparison of soil deformation between case ⅰ and case ⅱ fig 22 a and b show the compression variation of stratum above the aqi similar to the responses of the drawdowns the presence of the micro confined aquifer in case ⅱ can mitigate the vertical deformation and stratum compression in the layers above the pumping aquifer as expected the influence of the micro confined on vertical deformations of strata below the adi can be neglected fig 23 presents the comparison of horizontal deformation profiles at r 50 m in stage 4 between case i and case ii where the negative values represent the deformation towards the pumping well it can be seen that the horizontal deformations in these two cases show similar characteristics where the maximum value is located at the ground surface and the deformation decreases with the increase of the depth however the magnitude of the horizontal deformation in case2 is obviously smaller than that in case1 which may be attributed to the following two reasons i the presence of aqi0 reduces the decrease of the hydraulic head in the soil layers above aqi which results in the decrease of seepage force in these soil layers and ii comparing with the deformation in case1 the smaller vertical compression of the soil layers above aqi in case2 results in a smaller soil horizontal deformation due to the coordination of soil deformation therefore the presence of a micro confined aquifer can effectively relieve the pumping induced soil horizontal deformation 6 conclusions in this study a coupled hydro mechanical analysis is conducted to investigate the groundwater drawdown and deformation in maas to pumping in a confined aquifer the drawdown and deformation processes in each stratum are analyzed the ground surface settlement is compared to that obtained by a non coupling method based on the effective stress principle developed by terzaghi the following conclusions can be drawn 1 the groundwater drawdown caused by dewatering in confined aquifer shows a linear distribution along with the radial distance under logarithmic coordinates which is consistent with the theoretical distribution of the complete penetration well theory however the ground surface settlement is fitted by three straight lines with reflection points located at r 0 4 d i 0 and r 1 5 d i 0 where d i 0 is the buried depth of the top surface of the pumping aquifer this discrepancy is mainly attributed to the hydro mechanical coupling effect caused by dewatering 2 drawdown responses of multi aquifer aquitard system can be divided into four typical stages soil deformation in each stratum is closely related to the decline of the water head however local expansions can be observed in early dewatering in aquitards and phreatic aquifer due to soil arching with the proceeding of dewatering the expansion phenomenon disappears and soil compression occurs in all the strata finally both groundwater flow and the soil deformation in each stratum reaches a stable state 3 the non coupling formula provides a good estimation of ground settlement outside the circle with a radius three times of the buried depth of pumping aquifer however it overestimates the ground settlement near the well and the deviation increases with the decrease of the radial distance to improve its applicability a modification of the empirical coefficient is suggested which varies linearly with the radial distance from the pumping well under logarithmic coordinates the non coupling formula with the modified coefficient can provide a good settlement estimation of deep soil layers 4 due to horizontal and vertical cross flow supplement the presence of the micro confined aquifer can relief the groundwater drawdowns of phreatic aquifer and aquitard above the pumping aquifer consequently both the soil compression and horizontal displacement in these strata can be relieved however due to the low hydraulic conductivity and large thickness of the aquitard the vertical cross flow supplement to the pumping aquifer is insignificant as a result the influence of the micro confined aquifer on drawdowns and vertical deformations of strata below that aquitard can be neglected credit authorship contribution statement ming guang li conceptualization formal analysis funding acquisition jin jian chen funding acquisition supervision xiao he xia formal analysis supervision yang qing zhang methodology formal analysis da fa wang writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was substantially supported by the national natural science foundation of china grant nos 41727802 41602283 and 41977216 and by the shanghai rising star program grant no 19qc1400800 the authors are grateful to these programs special thanks also go to the anonymous peer reviewers whose comments greatly improved the rigorousness and quality of this paper 
