index,text
7820,study region the lenguazaque river basin is a 290 km2 headwater catchment of the fuquene lake watershed in the colombian andes regional stakeholders are optimizing water allocations for all users given numerous quantity related issues such as its simultaneous use for conservation agriculture and coal mining study focus developing hydrological models in headwater catchments is challenging especially in developing countries where technical resources and data are limited this study addresses these challenges by proposing strategies to develop reliable yet mathematically simple models requiring fewer inputs than data intensive alternatives three stages are proposed focused on 1 assessing hydrological data 2 preparing datasets for modeling and 3 developing models in daily and sub daily resolutions the last stage is grounded on a data based mechanistic modeling approach and includes a novel combination of baseflow separation with digital filters and multi objective optimization principles having a hydrophysical meaning obtained models are suitable for simulating alternative scenarios new hydrological insights for the region resulting daily models achieved an acceptable performance significantly better than a semi distributed model the performance of sub daily models was sub optimal yet it can be improved by enhancing the quality of sub daily datasets and the efficiency of computational algorithms due to their mathematical nature the proposed strategies are expected to be applicable to other headwater catchments where improving water management is an imperative need graphical abstract ga1 keywords headwater hydrology data based mechanistic modeling rainfall runoff baseflow estimation transfer functions system identification data availability data and codes are available as a mendeley data link cited in appendix e and in the references repository of rainfall streamflow datasets and models for the lenguazaque river basin cundinamarca colombia original data mendeley data 1 introduction limited water availability and poor water quality have become major challenges all around the world as a result a high proportion of the global population faces water insecurity vörösmarty et al 2010 due to the complexity of the issue hydrological models have been evolving to better understand and represent different elements of the water cycle beven 2019 wagener et al 2010 at the catchment scale the purposes of these models include estimating relationships between rainfall and runoff events predicting streamflow and floods and assessing the impacts of a changing land and climate on water resources song et al 2015 when consistent solid and well supported these models are particularly appropriate for water resources management policymaking and simulation of alternative scenarios often timely and inexpensively considering the computational capabilities of current computers moriasi et al 2007 the reliability of a hydrological model at the catchment scale depends traditionally on its ability to represent local hydrology especially when its purpose is to examine alternative scenarios sivapalan and young 2005 two different yet not mutually exclusive techniques are commonly used to achieve such representation also known as the upward and downward techniques ratto et al 2007 while the first is predominant and focuses on building detailed models including most processes occurring on smaller scales the second centers on assembling parametrically efficient conceptual models from observed data and their dominant modes of response young 2013 under a different paradigm a third technique based on machine and deep learning algorithms has more recently proven outstanding performance when reproducing observed streamflow in gauged and ungauged catchments kratzert et al 2018 2019 nearing et al 2021 despite the benefits the aforementioned techniques still have limited predictability under changing conditions lack physical meaning in explaining underlying processes and are highly dependent on the availability of good quality training datasets beven 2020 despite permanent development and improvement of these approaches implementing reliable hydrological models in headwater catchments remains challenging note that these catchments are generally under protected and poorly instrumented despite their abundance and importance for ecosystems storing carbon and regulating streamflow and water quality downstream alexander et al 2007 beckman and wohl 2014 kelleher et al 2015 under the upward technique distributed physically based models demand high amounts of information and computational resources that even when available in these catchments are subject to parametric uncertainty and equifinality ala aho et al 2017 kelleher et al 2017 concerning the downward technique achieving a parsimonious description of headwater hydrology is complex given the usual heterogeneity and poor characterization of these territories yet significant efforts to achieve such a description in headwaters have been made such as the use of groundwater data and simple models to explain hydrograph components kosugi et al 2011 a first order differential equation of precipitation evapotranspiration and water storage to compute discharge kirchner 2009 and tracer experiments to estimate water balances nauditt et al 2017 tetzlaff et al 2011 wenninger et al 2008 conversely applications of the third technique i e based on machine and deep learning algorithms are still limited in these territories as shown by a few experiments to map and classify their water bodies liu et al 2021 șerban et al 2020 and to predict the hydraulic conductivity of their soil pinto et al 2019 developing countries face additional challenges when implementing headwater hydrological models these nations generally put greater pressure on their headstreams than other territories as their mountains are home to more than 600 million inhabitants and have restricted economies information and scientific capacity chapra 2019 messerli et al 2004 viviroli et al 2007 these restrictions are reflected in scarce yet heterogeneous hydrological data especially given the low number of on site gauging stations rodríguez et al 2020 which often comprise both digital and analog data collection systems and record information at various resolutions burboa et al 2020 pizarro tapia et al 2020 in this study we proposed a modeling framework to address the aforementioned shortcomings when modeling headwater catchments in data scarce regions such a framework follows the previously described downward technique and is based on the data based mechanistic modeling dbm approach young and beven 1994 specifically the framework employs a novel combination of the dbm approach with the separation of baseflow with digital filters eckhardt 2005 to the best of our knowledge this integration has not been proposed previously and is directed to select models that balance simplicity accuracy and low data requirements while capable of examining different scenarios particularly the proposed framework can help researchers to explore headwater hydrology in data scarce regions as well as environmental agencies and practitioners to improve water quantity management programs 2 materials and methods the overview of the proposed framework strategies is presented in fig 1 in summary it comprises three main stages and seven activities to address scarcity and heterogeneity of information when developing hydrological models in headwater catchments the first stage focuses on identifying monitoring stations operated by local stakeholders obtaining daily and sub daily rainfall and streamflow time series from these stations and performing preliminary assessments on the data it is important to note that the coexistence of digital and hardcopy hydrological records implies careful digitization as the quality of paper charts limits the final resolution for sub daily values nevertheless such efforts aim to build an understanding of hydrology in the catchment of interest and to obtain time series that are complete coherent and of good quality the second stage focuses on preparing information for modeling this preparation includes separating streamflow into baseflow and surface runoff and using recession constants and digital filters to estimate the water balance in the catchment this balance is expressed in terms of the water entering e g precipitation and leaving e g baseflow surface runoff evapotranspiration percolation the system the water balance and time series obtained during the first two stages support the subsequent model development and interpretation stage the final stage uses the dbm approach and multi objective optimization principles which will be further discussed in the next sections of the paper 2 1 stage i information analyses 2 1 1 information inventory generally in developing countries the location of hydrological gauging stations can be identified in local environmental agencies bulletins datasets from these stations are either obtained through formal requests or directly downloaded from agencies websites in some cases digital time series are easier to utilize while hardcopy records and charts usually need to be requested scanned and digitized depending on the number of charts and their quality digitization can be done either manually or assisted by software packages to improve efficiency fuentes et al 2018 rohatgi 2020 sušin and peer 2018 in any case professional judgment is essential to select an appropriate tool to secure adequate translation of pluviographs and hydrographs into digital time series and define a reliable time step when necessary computing streamflow from water level measurements can be done with rating curves or hydraulic models depending on data availability finally it is important to build an indexed information inventory to simplify data management and analysis 2 1 2 spatiotemporal distribution the distribution of rainfall and streamflow data in time and space can be described in boxplots and maps of the catchment of interest respectively specifically boxplots nicely display annual rainfall and streamflow regimes that are important for understanding catchment hydrology turning to the maps they are useful to illustrate locations of all rainfall and streamflow gauges in the region of interest and their areas of influence for precipitation its spatial distribution is determined with the isohyetal method given its relative simplicity and capability to represent rainfall variability in mountainous areas anderson 2002 regarding streamflow a digital elevation model dem is employed to compute drainage areas for each station e g nasa and meti 2012 boxplots isohyets and drainage areas are well known in hydrology e g chow et al 1988 shaw et al 2010 and can be generated using both proprietary and open source software packages e g matlab arcgis python r qgis 2 1 3 assessment of data quality consistency and validity quality consistency and validity of the information can be assessed considering the previous spatiotemporal distribution along with percentages of missing data and well known techniques in hydrology these techniques include computing and assessing mass and double mass curves widely described in related literature e g searcy and hardison 1960 shaw et al 2010 tukey 1977 additional metrics may be considered depending on the complexity of the case which are also described in several references e g kottegoda 1980 machiwal and jha 2012 then statistical tools are employed to identify and remove potential outliers from time series which are replaced following traditional procedures in hydrology a full description of these tools and procedures can be found in several sources as well e g chow et al 1988 kottegoda and rosso 2008 shaw et al 2010 in appendix a we describe the specific techniques used in this study in more detail 2 2 stage ii data preparation 2 2 1 estimation of baseflow and surface runoff streamflow time series are separated into their surface runoff and baseflow components using a digital filter expressed as eckhardt 2005 nejadhashemi et al 2004 1 b k r b k 1 1 r 2 s k s k 1 where s k represents streamflow observed in discrete time k s k 1 is streamflow observed one unit of time before k 1 and b k and b k 1 represent baseflow in the same discrete times in this expression the value of b k is restricted as less or equal than s k and r is the streamflow recession constant defined as nathan and mcmahon 1990 2 r e β q t q 0 1 t where β is constant t is time q 0 is the magnitude of an initial peak discharge recorded at a given location and q t represents the recession of such discharge at time t given the structure of eq 2 q t is an exponential function of time thus β and r can be estimated with regression of an exponential model please note that such regressions are included in several software packages e g excel matlab r when data are available for a set of streamflow peaks and recessions recorded at a streamflow gauge the regression is performed on a master recession curve built with this set here the curve is specifically constructed following the matching strip method nathan and mcmahon 1990 once r is known baseflow b k is computed with the digital filter and surface runoff r k is estimated as the difference between s k and b k it is important to highlight that this filter is suggested since it has been capable of reproducing baseflow measurements in several catchments and results obtained with traditional graphical methods arnold and allen 1999 it is also suitable when the information is limited because it only requires streamflow observations 2 2 2 estimation of water balances considering previous definitions and p k as precipitation entering the catchment in discrete time k a mass balance leads to the following fractions as simple estimates of water quantities in a headwater catchment 3 f 1 k 1 n s k k 1 n p k 1 f 2 4 f 2 k 1 n et k per k k 1 n p k 1 f 1 5 f 3 k 1 n b k k 1 n s k 1 f 4 6 f 4 k 1 n r k k 1 n s k 1 f 3 where f 1 and f 2 are fractions of input rainfall leaving the catchment through streamflow and other hydrological mechanisms e g percolation and evapotranspiration and f 3 and f 4 represent the portions of streamflow draining as baseflow and surface runoff additionally n is the length of the time series and s k p k b k r k et k and per k represent streamflow precipitation baseflow surface runoff evapotranspiration and percolation at discrete time k respectively please note that all these terms are required in the same units thus when needed unit conversions use the catchment area as commonly done in hydrology 2 3 stage iii model development 2 3 1 description of the modeling framework under a dbm approach surface runoff can be modeled as single input single output siso or multiple input single output miso transfer functions described as follows camacho et al 2002 mcintyre et al 2011 young 2005 7 r k b z i a z i c k p k 8 r k b 1 z i a z i c k p k 1 1 b j z i a z i c k p k j j where r k represents surface runoff in the outlet of the catchment at discrete time k p k represents a unique input of precipitation at time k and terms p k 1 1 to p k j j describe multiple rainfall inputs at time k j j complementarily and j are discrete time delays respectively from the single and multiple rainfall inputs to the outlet of the catchment j is an integer identifier for each of these multiple inputs c k is a factor accounting for a possible nonlinear response of the model and a b and b j are polynomials defined as young 2005 9 a z i 1 a 1 z 1 a n z n 10 b z i b 0 b 1 z 1 b 2 z 2 b m z m 11 b j z i b j 0 b j 1 z 1 b j 2 z 2 b j l j z l j where n m and l j are the orders of the polynomials a 1 a n b 0 b m and b j 0 b j l j are their characteristic constants and r k is as previously specified here z i is the backward shift operator defined as young 2005 12 z i r k r k i where i is the discrete lag of the runoff time series based on these definitions models for surface runoff can be developed from data collected at single or multiple precipitation gauges by finding appropriate values for c k n m l j 1 j a 1 a n b 0 b m and b j 0 b j l j such values should lead the models to reproduce observations of surface runoff leaving the catchment in the case of c k table 1 shows seven possible alternatives to consider a possible nonlinear response of the catchment thus finding the best alternative requires identifying an alternative that leads to the best results for the remaining values they can be determined with instrumental variable algorithms designed explicitly for this purpose such as those included in the rivid and getpar functions of the captain toolbox taylor et al 2007 2018 once all values are determined eq 1 can be combined with eqs 7 and 8 to describe the general equations for the siso and miso models as follows noting that streamflow s k comprises baseflow b k and surface runoff r k components 13 s k r b k 1 1 r 2 s k s k 1 b z i a z i c k 14 s k r b k 1 1 r 2 s k s k 1 b 1 z i a z i c k p k 1 1 b j z i a z i c k p k j j where the multiple rainfall inputs p k 1 to p k j come from the complete time series obtained in previous stages for j available gauging stations meanwhile the unique rainfall input p k is computed as a weighted average of these series using the areas of influence of each gauging station as weighting factors these areas are extracted from isohyet maps also obtained before using the line with the closest value to the average rainfall between neighbor stations as a division to compute such area 2 3 2 model calibration validation and hydrological interpretation under the proposed framework the process designed to calibrate validate and interpret the modeling results hydrologically is described in fig 2 as shown in the diagram complete time series streamflow components and water balance results are employed with isohyetal areas to compute the single input of precipitation as the weighted average described before note that inputs of the diagram come from stages i and ii previously detailed in fig 1 then for daily and sub daily time steps calibration and validation periods are divided evenly calibration continues by establishing search ranges for n m and for siso models and for n l j and j for miso in both cases integer values between 1 and 5 can be used for the orders of the polynomials n m and l j meanwhile the ranges for the delays and j should encompass typical values of time in discrete units that water takes to travel from available rainfall gauges to the outlet of the catchment finally it is also necessary to define the possible values of c k to be considered as alternatives in which they can represent a possible nonlinear response of the catchment these alternatives can be selected as a sub set of those presented in table 1 for all these values of c k the rivid function of the captain toolbox taylor et al 2007 2018 is employed to identify suitable models for all combinations within the search range then the getpar function is used to extract and store their corresponding parameter values i e a 1 a n b 0 b m and b j 0 b j l j note that sub daily time series are orders of magnitude larger than daily datasets and therefore present data processing challenges for instance longer computing times when running the rivid and getpar functions are required and the and j delay ranges comprise more values in this resolution thus the number of possible combinations to search for potential models is exponentially higher and requires long computing times or the use of high performance computing clusters which can be cost prohibitive as a result sub daily calibration datasets enter an additional process in the diagram where individual rainfall runoff events are selected chosen manually in this case the length of these events brings the size of the datasets closer to those with the daily time step after running the rivid and getpar functions a set of candidate models comprising all combinations in the search range is obtained this means that the values of a 1 a n and b 0 b m are found for all combinations of n m and in the case of siso models while magnitudes of a 1 a n and b j 0 b j l j are identified for all n l j and j in the case of miso please note that the expressions describing these models i e eqs 9 11 can have a maximum of five terms given the search ranges suggested before among all candidate models the best in each category i e siso and miso is selected from those being accurate and parametrically efficient while preserving the mass of water here these desirable model attributes are measured with the young identification criterion yic nash sutcliffe efficiency nse and percent bias pbias which are widely defined and applied in hydrology moriasi et al 2007 young and beven 1994 viewed as a multi objective optimization problem the pareto front contains the set of models performing better under these three metrics thus this set is computed from the three metrics of all candidate models with the algorithm included in the monte carlo analysis toolbox wagener and kollat 2007 note that other multi objective optimization tools are also suitable for this purpose then all models in the set having imaginary roots in polynomial a z i in eq 9 are discarded this is because avoiding such roots allows decomposing the models into several interconnected first order transfer functions facilitating their interpretation young 1992 from the remaining models the best is selected as the one with the highest score defined as 15 score nse 1 e yic 1 abs pbias 100 3 where nse yic and pbias are as previously stated and abs is the absolute value function please note that the score is a simple average where yic and pbias are transformed to be comparable with nse thus the score of a model approaches one when it preserves water volumes while being accurate and parametrically efficient also note that the nse and pbias only measure accuracy and mass conservation respectively while the yic balances accuracy and parametric efficiency at the same time for this reason the score gives more weight to the accuracy of the best model compared to its parametric efficiency and mass conservation once the structure and parameters of the model are calibrated by making this choice validation is performed following the sampling split method klemeš 1986 specifically the best models are used to simulate the catchment response to precipitation inputs in the validation dataset under these new conditions model performance is evaluated again based on its ability to reproduce streamflow observations this time only the nse and pbias are employed since the structures and parameters of the model do not change thus its parametric efficiency does not change either finally calibration and validation datasets are switched and the calibration process is repeated when similar values for both parameters and performance metrics are obtained after this iteration the model can be generally considered acceptable klemeš 1986 having a validated model it is subsequently interpreted in terms of its first order transfer function components as mentioned before avoiding imaginary roots in the polynomial a z i facilitates decomposing models initially having up to five terms in eqs 9 11 into these individual components particularly mathematical operations such as partial fraction expansion allow this type of decomposition young 1992 once decomposed the best models are written in terms of their components each having a characteristic time delay steady state gain and residence time for first order functions such delay is given by the value of i in z i while the steady state gain ssg and a residence time t r are computed as young et al 1996 16 ssg b 1 a 1 17 t r 1 ln a 1 where a 1 and b are the first order constants of the polynomials described in eqs 9 11 accordingly each component is interpreted as a reservoir with a residence time t r receiving an input of precipitation the output of this reservoir is connected to a linear channel with a delay defined by the value of i in z i in addition the ssg is related to the catchment area through the volumes of water entering and leaving the system such volumes are estimated with the previous analysis made with eqs 3 6 it is important to highlight that discarding models with imaginary roots in a z i and separating baseflow beforehand are key stages to simplify the interpretation process this is because separating high order transfer functions into first order components guarantees that values for ssg and t r can be computed and interpreted in addition having baseflow as a separate component makes these functions describe the surface runoff component only this is especially convenient when interpreting the individual responses of the catchment to each of the multiple precipitation inputs in the miso model 2 4 description of the case study the lenguazaque river basin is a headwater catchment located in south america s northern andes in the region of cundinamarca colombia fig 3 the catchment elevation is over 2600 m a s l and it lies specifically on the country s eastern cordillera in an area of 290 km2 consisting of cretaceous and tertiary rock and a combination of lacustrine fluvioglacial colluvial and alluvial deposits bogotá a et al 2011 car and corpoboyacá 2017 about 60 of this area is used for agriculture nearly 20 is occupied by forests and native vegetation of paramo and coal extraction takes place underground fernandez 2018 with a mean slope of 22 and a time of concentration estimated at 2 5 h in local studies the average discharge is around 1 4 m3s 1 car and corpoboyacá 2017 concerning the catchment climate its average temperature is 13 5 c and its annual average precipitation and evapotranspiration are estimated as 900 mm and 500 mm respectively car and corpoboyacá 2017 the area of interest is also a headwater of the larger fuquene lake watershed 1750 km2 a hot spot of freshwater biodiversity supplying water to more than 500 000 citizens where several issues have been documented examples include severe pollution loss of water volume in the receiving lake and replacement of paramo vegetation for other crops fernandez et al 2018a rubiano et al 2006 valderrama et al 2018 the lenguazaque river basin represents this region fairly well since it exemplifies these issues the basin comprises the largest area of paramo essential for regional water supply and storage now with significant portions of vegetation either replaced or affected by agricultural and mining activities additionally domestic and mining wastewaters are discharged without appropriate treatment mcintyre et al 2018 fernandez et al 2018b since such characteristics are not exclusive to this headwater watershed rubiano et al 2006 choosing it as a case study facilitates future upscaling to the other headstreams it is worth mentioning that the semi distributed physically based soil and water assessment tool swat model was previously calibrated for this area reproducing monthly streamflows between 2003 and 2015 with an nse and pbias of 0 56 and 0 001 respectively fernandez and camacho 2019 this is a significant baseline to consider in this study 3 results 3 1 stage i information analyses 3 1 1 information inventory relevant hydrometeorological stations are presented in fig 4 currently operated by the environmental agency in the area of interest car 2020 complementarily a summary of the information available at these stations is provided in appendix b table b 1 as shown rainfall and streamflow gauges are scattered across the basin and daily time series are significantly more continuous than those in sub daily resolution specifically the daily records comprise ten years of information with generally small percentages of missing values while sub daily data are limited to three years recorded in fewer sites and with longer gaps it is worth mentioning that almost 17 of daily streamflow data missing for station s2 is mainly due to a long gap with no recordings from january to june 2009 yet this percentage decreases below 1 afterward finally it is important to acknowledge the role of the environmental agency car which was the source of all original datasets in the case study while daily time series were provided as digital formats most sub daily records are available as hardcopy paper charts that need to be scanned and digitized only a few stations recently installed store data digitally and automatically such as s6 in the case study recording discharge at ten minute intervals 3 1 2 spatiotemporal distribution of rainfall and streamflow the resulting boxplots and maps are presented in figs b 1 and b 2 respectively appendix b the monthly boxplots reveal bimodal rainfall and streamflow regimes with wet periods around april and november and dry periods between december to february and june to september daily diagrams show this trend more clearly since they comprise more continuous time series although most sub daily plots also show the bimodal trends even when comprising sparser data this is not the case for station s8 where high resolution records are inconsistently steady and thus not appropriate for model development turning to spatial distribution isohyets presented in the maps show that daily precipitation gauges cover most sub basins well at the same time the drainage area of the tibita river is not represented sub daily with the same precision fig b 2 a b 2 b complementarily daily streamflow data comprise most streams and drainage areas while sub daily values are limited to the larger areas represented by stations s6 and s8 fig b 2 c table b 1 3 1 3 assessment of data quality consistency and validity mass and double mass curves are displayed in figs b 3 and b 4 respectively appendix b daily cumulative rainfall reveals s5 as the site receiving more precipitation followed by s2 and s1 with comparable volumes and s7 with the lowest amounts these curves show significant rainfall changes between 2009 and 2012 and more linear trends afterward fig b 3 a which is consistent with most cumulative streamflow plots fig b 3 b note that the latter exhibit data discrepancies at s8 since 2015 which is due to hydraulic adjustments performed then rendering subsequent records invalid complementarily linear trends observed for stations s1 and s7 in double mass curves indicate their rainfall records are generally consistent while some slope shifts in stations s2 and s5 indicate possible inconsistencies especially before 2013 when cumulative precipitation lies below 3000 mm fig b 4 a concerning sub daily data cumulative rainfall matches daily trends closely at s1 while values at s5 and s7 are significantly lower fig b 3 a similarly cumulative streamflow resembles daily values at s6 except when data are missing yet records are completely different at s8 fig b 3 b slope changes in double mass curves also suggest rainfall series incongruence especially at s7 in summary daily data are more consistent and of better quality while limitations of sub daily information restrict the reliability of models developed at this higher resolution after managing outliers and missing data for all stations the resulting from complete time series are displayed in fig b 5 more details on the results are presented in table b 2 appendix b the plots show that precipitation data are generally continuous in both resolutions while streamflow series are restricted especially with sub daily data fig b 5f g this means it is possible to develop models for any drainage area with a daily time step yet higher resolution models are limited to the region draining at s6 therefore modeling efforts center in this region allowing subsequent model comparison in both resolutions fig 5 shows the complete time series for this drainage area and is thus considered for modeling 3 2 stage ii data preparation 3 2 1 estimation of baseflow and surface runoff master recession curves and recession constants computed from observations at s6 are presented in fig 6 as shown recessions adjust well to eq 2 when these constants are 0 971 and 0 993 for daily and sub daily models respectively thus these are the values of r used in eq 1 to compute baseflow and surface runoff nevertheless ranges are provided as a reference for possible sensitivity and uncertainty analyses as well as comparison with field measurements 3 2 2 estimation of water balances results of the water balance are summarized in table b 3 as presented daily data indicate almost 30 of input precipitation leaves the drainage area of s2 as streamflow while this value is about 20 for s3 and between 12 and 15 for s4 and s6 this means that small and steep areas in the upper catchment have higher streamflow and less water leaving through percolation and evapotranspiration in proportion to their size compared to larger areas in the lower catchment the higher streamflow is likely due to lower population pastures and economic activities within the smaller sub catchments specifically at s6 where modeling was performed the balance indicates that 65 of the water leaving as streamflow is related to baseflow and the remaining portion is surface runoff note that sub daily data indicate these values are 80 and 20 respectively although these estimates are subject to more limited and less reliable data as previously mentioned 3 3 stage iii model development results of the previously mentioned swat model calibrated between 2003 and 2015 for the same catchment are presented in fig 7 this is the baseline scenario for the present study the model spatial inputs include elevation slope land use and soil type based on these inputs the catchment was divided into eight sub basins and about 1200 hydrologic response units hrus the high number of hrus responds to the unique combinations of the slope land uses and soil types in this area meanwhile the model considered daily precipitation temperature relative humidity solar radiation and wind speed as hydrometeorological inputs accordingly the model was set up to simulate monthly streamflow in station s6 as a response to these inputs swat simulates flow by routing water entering the catchment through two mechanisms arnold et al 2012a 2012b first water reaching the main channel of each sub basin is computed from a water balance for each hru second water in the channels is routed through the river network using either the variable storage or the muskingum method under this semi distributed scheme monthly streamflows were calibrated at station s6 using the sufi 2 algorithm in swat cup abbaspour 2015 a calibration and sensitivity analysis tool for swat such calibration involved 33 parameters identified in the literature creech et al 2015 francesconi et al 2016 among them only a few were found sensitive in the study area such as the hydraulic conductivity of soil sol k and the aquifer percolation coefficient rchrg dp the first three years of the simulation were considered a warm up period since the initial water contents in the catchment were unknown more details on the results of this model and the calibration process are available in previous studies fernandez 2018 fernandez and camacho 2019 3 3 1 daily resolution results obtained with the best siso and miso models are displayed in fig 8 and their corresponding structures parameters and performance are summarized in table 2 an analysis of the uncertainty of these structures and parameters is included in appendix d it is important to note that only the first four values of c k in table 1 were tested as possible alternatives to consider a nonlinear response of the catchment as shown graphically the response of both models is practically equivalent yet performance metrics suggest that the siso is a slightly better predictor during calibration the scores for both models are similar however individual metrics indicate the siso is more parametrically efficient while the miso is a better estimator of observed data and water volumes this is because the siso has a lower yic a metric indicating good fitness and parsimonious parametrization when highly negative in contrast the miso has a higher nse and a pbias closer to zero nevertheless the siso outperformed the miso during validation as its nse and pbias were better while keeping a simple structure in addition both models were significantly more accurate than the swat baseline when results were aggregated monthly however a comparison of these models with swat requires considering additional aspects which will be discussed further in the next section finally note that similar structures parameters and performance metrics were found when calibration and validation datasets were switched see table c 1 in appendix c thus the resulting siso and miso models as described in table 2 can be considered acceptable according to the sampling split method used for model validation klemeš 1986 from these results and eqs 13 and 14 the best siso model and the surface runoff component of the best miso model can be respectively written as 18 s k 0 97 b k 1 1 0 97 2 s k s k 1 0 05 1 0 6 z 1 s k p k 19 r k 0 014 0 018 z 1 1 0 5 z 1 s k p k s 1 0 018 1 0 5 z 1 s k p k s 2 0 005 1 0 5 z 1 s k p k s 5 0 005 1 0 5 z 1 s k p k 1 s 7 where s 1 s 2 s 5 and s 7 refer to the four stations measuring rainfall within the study area from these equations and the values provided in table 2 the models can also be interpreted hydrologically as follows the siso behaves as a 254 km2 reservoir receiving a unique precipitation input and discharging from a single output with negligible delay this means that on average the catchment responds to a precipitation event by increasing its output streamflow immediately yet the input hydrograph attenuates for almost two days before returning to baseflow conditions meanwhile the miso model comprises four systems connected in parallel each receiving a different precipitation input the first system includes two reservoirs with an aggregated area of 122 km2 while the second third and fourth comprise individual ponds with 71 21 and 40 km2 respectively here all ponds attenuate input hydrographs for 1 3 days and the delay of their outputs is negligible in most cases except for the fourth pond which is connected to a linear channel with a delay of one day this suggests that the response of the catchment i e increase of output discharge to precipitation events measured at stations s1 s2 and s5 starts the same day while for rainfall recorded at s7 it starts one day later note that both models use s k to account for the nonlinear response of surface runoff while their baseflow component is the same 3 3 2 sub daily resolution as in the previous case table 3 summarizes the structures characteristic values and performance metrics for the best siso and miso models and figs 9 and 10 show their corresponding simulation results compared to observations mathematically these models are respectively written as 20 s k 0 99 b k 1 1 0 99 2 s k s k 1 0 09 z 2 1 1 6 z 1 0 66 z 2 r k p k 21 r k 0 007 z 48 1 1 70 z 1 0 71 z 2 p k s 1 0 11 1 1 70 z 1 0 71 z 2 p k s 5 0 26 z 10 0 23 z 11 1 1 70 z 1 0 71 z 2 p k s 7 where all terms are as defined previously as shown the framework led to finding models capable of accurately representing individual events however there is still room for improvement specifically for individual events the best siso and miso models preserved water volumes well with a pbias close to zero in all cases meanwhile the siso model had a reasonable performance for all events nse 0 65 whereas the miso performed exceptionally for two events nse 0 95 and sub optimally for the other two nse 0 25 when simulating the complete calibration and validation periods the siso model captured the moments when the catchment responded to precipitation events yet the magnitudes of such response were mostly underestimated for the miso model the timing of the response was also captured to some extent yet it was overestimated for many precipitation events and underestimated for the highest streamflow peaks this results in models with a pbias magnitude far from zero reasonable nse values for total streamflow 0 5 yet inaccurate values for the surface runoff component nse 0 2 4 discussion the interpretation of daily models provides valuable insights into the hydrology of the catchment on the one hand areas found with the miso model have magnitudes resembling those estimated with the isohyetal method see fig b 2a this suggests that methods used for the miso model could provide an objective estimate of the spatial distribution of rainfall without making any prior calculations it is important to note that more research is required to determine the accuracy of this approach on the other hand values of delay and residence time estimated with the siso and miso models are valuable to characterize how fast the catchment responds to recorded precipitation events please note that these values provide more detailed information about such response compared to the currently available estimates of the time of concentration in the area namely a unique value of 2 5 h car and corpoboyacá 2017 thus the identified values may be the basis for the subsequent development of local warning and forecast systems especially given the precision of the implemented models to this end the analysis of uncertainty presented in appendix d is also valuable in addition it is important to consider possible reasons why the siso model outperforms the miso and why the current approach outperforms the swat baseline in the first case the time step used for modeling may be the main reason for the siso model outperforming the miso model as stated the delays and residence times identified for both models are less than one and two days respectively moreover travel times between stations s4 and s8 were estimated as less than 10 h under typical streamflow conditions fernandez 2018 such small response times make detecting individual delays for each precipitation input in daily time step resolution ineffective in improving the simulations of output discharge thus the sub daily character of water travel times in the catchment also supports the need for developing the models in higher resolution in the second case the uncertainty of the swat model input data and parameters are important to be acknowledged as previously stated implementing swat required significant amounts of information even when collected these data are highly uncertain because of the challenges when translating local studies on soils and land use to the required format by swat in addition parameters are uncertain since only a few were identifiable in the model despite their solid scientific foundation thus the present approach reduces such uncertainty by requiring fewer parameters and only precipitation and streamflow time series another reason for outperforming swat is related to the different mechanisms used to consider water storage in the catchment leading to a possible nonlinear response here only the first four values of c k in table 1 were tested as proxies of said storage all coming directly from observations meanwhile swat completely simulates the storage through the previously explained water balances and routing mechanisms thus using observations instead of simulated storage values also improves performance please note that such performance makes the present approach highly effective in identifying appropriate parameters to model the catchment s hydrology for this reason once identified future efforts should focus on testing alternative simulations to account for the storage of the catchment the antecedent precipitation and moisture indexes presented in table 1 can be considered among these alternatives such efforts would also allow the present approach to be practical for making predictions and supporting long term planning in the same way that swat is typically employed for this purpose joint implementation of this data based mechanistic approach with process based models can be useful as previously described in related literature e g ratto et al 2007 sivapalan and young 2005 young 2013 regarding the sub daily models several reasons may have led to their sub optimal performance firstly using a sample of the individual event was not sufficient to describe the catchment response as recorded in complete sub daily time series this may be due to the unique characteristics of each event which could not be extrapolated to the entire period possible alternatives to address this issue include increasing the number of events in the sample using more sophisticated statistical and automated approaches to extract individual events from sub daily time series and using more efficient computational strategies such strategies may comprise the design of more efficient algorithms using open source and cloud based high performance computing and promoting interdisciplinary work particularly involving both hydrologists and computer scientists may be a suitable alternative to increase the access and appropriate use of such computing resources a second reason is the lower quality of sub daily datasets as previously identified for this study nevertheless as mentioned for the daily models the importance of developing sub daily models relies on the sub daily nature of water travel times in the catchment thus promoting collaboration with local stakeholders to continue their sub daily hydrologic monitoring and improve the quality of their datasets over time are potential solutions to this issue in any case efforts made in the present study to implement models in sub daily resolution could serve as a baseline for future improvements accordingly the datasets and code are available in an online repository fernandez 2022 5 conclusion in summary the development of hydrological models in developing countries faces major challenges this study proposed a set of strategies to address these challenges especially in highly disturbed headwater catchments the overall results from the case study showed the reliability and suitability of the proposed approach in a real world scenario such approach is likely applicable to other catchments with similar physiographical characteristics even outside of the andes this is because the methods employed are mathematically simple require low amounts of data and computational power and have a sound hydrological meaning the proposed framework could be especially useful in headstreams of developing countries where water management is limited by data financial and scientific restrictions and where improved management is urgently needed the hydrological models developed under the proposed framework performed better than the data intensive semi distributed model previously setup and calibrated for the same headwater catchment in colombia meanwhile the uncertainty level of the present approach is less than the data intensive semi distributed model as the proposed approach uses observed data as a proxy of the water storage in the catchment moreover although modeling daily streamflow was successful long sub daily simulations are still far from acceptable despite a set of well reproduced short individual events thus improving the quality of sub daily data and computational methods to calibrate models with larger high resolution series is still needed considering results from the case study future work can take several directions the first is directed to the simulation of headwater hydrology under non stationary conditions given the highly dynamic character of these catchments although we focused on constant parameter models for stationary periods as the simplest case these models are the base for developing time variable parameter models possibly under the downward technique keeping the same parameters now variable in time these models should be suitable for evaluating alternative scenarios during longer non stationary periods the second concerns originated from studying the relationship between parameter values and changes in land use this should be addressed through the analysis of satellite imagery for example and would broaden the physical meaning of the models a third direction could be toward a better understanding of headwaters baseflow although the digital filter employed was proven appropriate in several watersheds its performance in headwater catchments still needs to be verified conducting baseflow field measurements in headwater catchments should be useful for this purpose once again many developing countries support vast populations near their headwaters with numerous limitations thus improving water management in these catchments is an imperative need and this work is intended as a tool to help fulfilling this purpose credit authorship contribution statement nicolas fernandez conceptualization methodology software validation formal analysis investigation data curation writing original draft visualization luis a camacho conceptualization methodology resources writing review editing visualization supervision project administration funding acquisition a pouyan nejadhashemi resources writing review editing visualization project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author acknowledges anna raschke for her help proofreading the initial version of the manuscript he also acknowledges the civil and environmental engineering department and the vice presidency of research at universidad de los andes as well as the biosystems and agricultural engineering department at michigan state university for the financial support during his doctoral research he also thanks the two anonymous reviewers whose thorough evaluation led to the final version of this manuscript appendix a supporting information datasets and code are stored in an online repository fernandez 2022 supplementary materials associated with this article can be found in the online version at doi 10 1016 j ejrh 2022 101243 appendix a supplementary material supplementary material 
7820,study region the lenguazaque river basin is a 290 km2 headwater catchment of the fuquene lake watershed in the colombian andes regional stakeholders are optimizing water allocations for all users given numerous quantity related issues such as its simultaneous use for conservation agriculture and coal mining study focus developing hydrological models in headwater catchments is challenging especially in developing countries where technical resources and data are limited this study addresses these challenges by proposing strategies to develop reliable yet mathematically simple models requiring fewer inputs than data intensive alternatives three stages are proposed focused on 1 assessing hydrological data 2 preparing datasets for modeling and 3 developing models in daily and sub daily resolutions the last stage is grounded on a data based mechanistic modeling approach and includes a novel combination of baseflow separation with digital filters and multi objective optimization principles having a hydrophysical meaning obtained models are suitable for simulating alternative scenarios new hydrological insights for the region resulting daily models achieved an acceptable performance significantly better than a semi distributed model the performance of sub daily models was sub optimal yet it can be improved by enhancing the quality of sub daily datasets and the efficiency of computational algorithms due to their mathematical nature the proposed strategies are expected to be applicable to other headwater catchments where improving water management is an imperative need graphical abstract ga1 keywords headwater hydrology data based mechanistic modeling rainfall runoff baseflow estimation transfer functions system identification data availability data and codes are available as a mendeley data link cited in appendix e and in the references repository of rainfall streamflow datasets and models for the lenguazaque river basin cundinamarca colombia original data mendeley data 1 introduction limited water availability and poor water quality have become major challenges all around the world as a result a high proportion of the global population faces water insecurity vörösmarty et al 2010 due to the complexity of the issue hydrological models have been evolving to better understand and represent different elements of the water cycle beven 2019 wagener et al 2010 at the catchment scale the purposes of these models include estimating relationships between rainfall and runoff events predicting streamflow and floods and assessing the impacts of a changing land and climate on water resources song et al 2015 when consistent solid and well supported these models are particularly appropriate for water resources management policymaking and simulation of alternative scenarios often timely and inexpensively considering the computational capabilities of current computers moriasi et al 2007 the reliability of a hydrological model at the catchment scale depends traditionally on its ability to represent local hydrology especially when its purpose is to examine alternative scenarios sivapalan and young 2005 two different yet not mutually exclusive techniques are commonly used to achieve such representation also known as the upward and downward techniques ratto et al 2007 while the first is predominant and focuses on building detailed models including most processes occurring on smaller scales the second centers on assembling parametrically efficient conceptual models from observed data and their dominant modes of response young 2013 under a different paradigm a third technique based on machine and deep learning algorithms has more recently proven outstanding performance when reproducing observed streamflow in gauged and ungauged catchments kratzert et al 2018 2019 nearing et al 2021 despite the benefits the aforementioned techniques still have limited predictability under changing conditions lack physical meaning in explaining underlying processes and are highly dependent on the availability of good quality training datasets beven 2020 despite permanent development and improvement of these approaches implementing reliable hydrological models in headwater catchments remains challenging note that these catchments are generally under protected and poorly instrumented despite their abundance and importance for ecosystems storing carbon and regulating streamflow and water quality downstream alexander et al 2007 beckman and wohl 2014 kelleher et al 2015 under the upward technique distributed physically based models demand high amounts of information and computational resources that even when available in these catchments are subject to parametric uncertainty and equifinality ala aho et al 2017 kelleher et al 2017 concerning the downward technique achieving a parsimonious description of headwater hydrology is complex given the usual heterogeneity and poor characterization of these territories yet significant efforts to achieve such a description in headwaters have been made such as the use of groundwater data and simple models to explain hydrograph components kosugi et al 2011 a first order differential equation of precipitation evapotranspiration and water storage to compute discharge kirchner 2009 and tracer experiments to estimate water balances nauditt et al 2017 tetzlaff et al 2011 wenninger et al 2008 conversely applications of the third technique i e based on machine and deep learning algorithms are still limited in these territories as shown by a few experiments to map and classify their water bodies liu et al 2021 șerban et al 2020 and to predict the hydraulic conductivity of their soil pinto et al 2019 developing countries face additional challenges when implementing headwater hydrological models these nations generally put greater pressure on their headstreams than other territories as their mountains are home to more than 600 million inhabitants and have restricted economies information and scientific capacity chapra 2019 messerli et al 2004 viviroli et al 2007 these restrictions are reflected in scarce yet heterogeneous hydrological data especially given the low number of on site gauging stations rodríguez et al 2020 which often comprise both digital and analog data collection systems and record information at various resolutions burboa et al 2020 pizarro tapia et al 2020 in this study we proposed a modeling framework to address the aforementioned shortcomings when modeling headwater catchments in data scarce regions such a framework follows the previously described downward technique and is based on the data based mechanistic modeling dbm approach young and beven 1994 specifically the framework employs a novel combination of the dbm approach with the separation of baseflow with digital filters eckhardt 2005 to the best of our knowledge this integration has not been proposed previously and is directed to select models that balance simplicity accuracy and low data requirements while capable of examining different scenarios particularly the proposed framework can help researchers to explore headwater hydrology in data scarce regions as well as environmental agencies and practitioners to improve water quantity management programs 2 materials and methods the overview of the proposed framework strategies is presented in fig 1 in summary it comprises three main stages and seven activities to address scarcity and heterogeneity of information when developing hydrological models in headwater catchments the first stage focuses on identifying monitoring stations operated by local stakeholders obtaining daily and sub daily rainfall and streamflow time series from these stations and performing preliminary assessments on the data it is important to note that the coexistence of digital and hardcopy hydrological records implies careful digitization as the quality of paper charts limits the final resolution for sub daily values nevertheless such efforts aim to build an understanding of hydrology in the catchment of interest and to obtain time series that are complete coherent and of good quality the second stage focuses on preparing information for modeling this preparation includes separating streamflow into baseflow and surface runoff and using recession constants and digital filters to estimate the water balance in the catchment this balance is expressed in terms of the water entering e g precipitation and leaving e g baseflow surface runoff evapotranspiration percolation the system the water balance and time series obtained during the first two stages support the subsequent model development and interpretation stage the final stage uses the dbm approach and multi objective optimization principles which will be further discussed in the next sections of the paper 2 1 stage i information analyses 2 1 1 information inventory generally in developing countries the location of hydrological gauging stations can be identified in local environmental agencies bulletins datasets from these stations are either obtained through formal requests or directly downloaded from agencies websites in some cases digital time series are easier to utilize while hardcopy records and charts usually need to be requested scanned and digitized depending on the number of charts and their quality digitization can be done either manually or assisted by software packages to improve efficiency fuentes et al 2018 rohatgi 2020 sušin and peer 2018 in any case professional judgment is essential to select an appropriate tool to secure adequate translation of pluviographs and hydrographs into digital time series and define a reliable time step when necessary computing streamflow from water level measurements can be done with rating curves or hydraulic models depending on data availability finally it is important to build an indexed information inventory to simplify data management and analysis 2 1 2 spatiotemporal distribution the distribution of rainfall and streamflow data in time and space can be described in boxplots and maps of the catchment of interest respectively specifically boxplots nicely display annual rainfall and streamflow regimes that are important for understanding catchment hydrology turning to the maps they are useful to illustrate locations of all rainfall and streamflow gauges in the region of interest and their areas of influence for precipitation its spatial distribution is determined with the isohyetal method given its relative simplicity and capability to represent rainfall variability in mountainous areas anderson 2002 regarding streamflow a digital elevation model dem is employed to compute drainage areas for each station e g nasa and meti 2012 boxplots isohyets and drainage areas are well known in hydrology e g chow et al 1988 shaw et al 2010 and can be generated using both proprietary and open source software packages e g matlab arcgis python r qgis 2 1 3 assessment of data quality consistency and validity quality consistency and validity of the information can be assessed considering the previous spatiotemporal distribution along with percentages of missing data and well known techniques in hydrology these techniques include computing and assessing mass and double mass curves widely described in related literature e g searcy and hardison 1960 shaw et al 2010 tukey 1977 additional metrics may be considered depending on the complexity of the case which are also described in several references e g kottegoda 1980 machiwal and jha 2012 then statistical tools are employed to identify and remove potential outliers from time series which are replaced following traditional procedures in hydrology a full description of these tools and procedures can be found in several sources as well e g chow et al 1988 kottegoda and rosso 2008 shaw et al 2010 in appendix a we describe the specific techniques used in this study in more detail 2 2 stage ii data preparation 2 2 1 estimation of baseflow and surface runoff streamflow time series are separated into their surface runoff and baseflow components using a digital filter expressed as eckhardt 2005 nejadhashemi et al 2004 1 b k r b k 1 1 r 2 s k s k 1 where s k represents streamflow observed in discrete time k s k 1 is streamflow observed one unit of time before k 1 and b k and b k 1 represent baseflow in the same discrete times in this expression the value of b k is restricted as less or equal than s k and r is the streamflow recession constant defined as nathan and mcmahon 1990 2 r e β q t q 0 1 t where β is constant t is time q 0 is the magnitude of an initial peak discharge recorded at a given location and q t represents the recession of such discharge at time t given the structure of eq 2 q t is an exponential function of time thus β and r can be estimated with regression of an exponential model please note that such regressions are included in several software packages e g excel matlab r when data are available for a set of streamflow peaks and recessions recorded at a streamflow gauge the regression is performed on a master recession curve built with this set here the curve is specifically constructed following the matching strip method nathan and mcmahon 1990 once r is known baseflow b k is computed with the digital filter and surface runoff r k is estimated as the difference between s k and b k it is important to highlight that this filter is suggested since it has been capable of reproducing baseflow measurements in several catchments and results obtained with traditional graphical methods arnold and allen 1999 it is also suitable when the information is limited because it only requires streamflow observations 2 2 2 estimation of water balances considering previous definitions and p k as precipitation entering the catchment in discrete time k a mass balance leads to the following fractions as simple estimates of water quantities in a headwater catchment 3 f 1 k 1 n s k k 1 n p k 1 f 2 4 f 2 k 1 n et k per k k 1 n p k 1 f 1 5 f 3 k 1 n b k k 1 n s k 1 f 4 6 f 4 k 1 n r k k 1 n s k 1 f 3 where f 1 and f 2 are fractions of input rainfall leaving the catchment through streamflow and other hydrological mechanisms e g percolation and evapotranspiration and f 3 and f 4 represent the portions of streamflow draining as baseflow and surface runoff additionally n is the length of the time series and s k p k b k r k et k and per k represent streamflow precipitation baseflow surface runoff evapotranspiration and percolation at discrete time k respectively please note that all these terms are required in the same units thus when needed unit conversions use the catchment area as commonly done in hydrology 2 3 stage iii model development 2 3 1 description of the modeling framework under a dbm approach surface runoff can be modeled as single input single output siso or multiple input single output miso transfer functions described as follows camacho et al 2002 mcintyre et al 2011 young 2005 7 r k b z i a z i c k p k 8 r k b 1 z i a z i c k p k 1 1 b j z i a z i c k p k j j where r k represents surface runoff in the outlet of the catchment at discrete time k p k represents a unique input of precipitation at time k and terms p k 1 1 to p k j j describe multiple rainfall inputs at time k j j complementarily and j are discrete time delays respectively from the single and multiple rainfall inputs to the outlet of the catchment j is an integer identifier for each of these multiple inputs c k is a factor accounting for a possible nonlinear response of the model and a b and b j are polynomials defined as young 2005 9 a z i 1 a 1 z 1 a n z n 10 b z i b 0 b 1 z 1 b 2 z 2 b m z m 11 b j z i b j 0 b j 1 z 1 b j 2 z 2 b j l j z l j where n m and l j are the orders of the polynomials a 1 a n b 0 b m and b j 0 b j l j are their characteristic constants and r k is as previously specified here z i is the backward shift operator defined as young 2005 12 z i r k r k i where i is the discrete lag of the runoff time series based on these definitions models for surface runoff can be developed from data collected at single or multiple precipitation gauges by finding appropriate values for c k n m l j 1 j a 1 a n b 0 b m and b j 0 b j l j such values should lead the models to reproduce observations of surface runoff leaving the catchment in the case of c k table 1 shows seven possible alternatives to consider a possible nonlinear response of the catchment thus finding the best alternative requires identifying an alternative that leads to the best results for the remaining values they can be determined with instrumental variable algorithms designed explicitly for this purpose such as those included in the rivid and getpar functions of the captain toolbox taylor et al 2007 2018 once all values are determined eq 1 can be combined with eqs 7 and 8 to describe the general equations for the siso and miso models as follows noting that streamflow s k comprises baseflow b k and surface runoff r k components 13 s k r b k 1 1 r 2 s k s k 1 b z i a z i c k 14 s k r b k 1 1 r 2 s k s k 1 b 1 z i a z i c k p k 1 1 b j z i a z i c k p k j j where the multiple rainfall inputs p k 1 to p k j come from the complete time series obtained in previous stages for j available gauging stations meanwhile the unique rainfall input p k is computed as a weighted average of these series using the areas of influence of each gauging station as weighting factors these areas are extracted from isohyet maps also obtained before using the line with the closest value to the average rainfall between neighbor stations as a division to compute such area 2 3 2 model calibration validation and hydrological interpretation under the proposed framework the process designed to calibrate validate and interpret the modeling results hydrologically is described in fig 2 as shown in the diagram complete time series streamflow components and water balance results are employed with isohyetal areas to compute the single input of precipitation as the weighted average described before note that inputs of the diagram come from stages i and ii previously detailed in fig 1 then for daily and sub daily time steps calibration and validation periods are divided evenly calibration continues by establishing search ranges for n m and for siso models and for n l j and j for miso in both cases integer values between 1 and 5 can be used for the orders of the polynomials n m and l j meanwhile the ranges for the delays and j should encompass typical values of time in discrete units that water takes to travel from available rainfall gauges to the outlet of the catchment finally it is also necessary to define the possible values of c k to be considered as alternatives in which they can represent a possible nonlinear response of the catchment these alternatives can be selected as a sub set of those presented in table 1 for all these values of c k the rivid function of the captain toolbox taylor et al 2007 2018 is employed to identify suitable models for all combinations within the search range then the getpar function is used to extract and store their corresponding parameter values i e a 1 a n b 0 b m and b j 0 b j l j note that sub daily time series are orders of magnitude larger than daily datasets and therefore present data processing challenges for instance longer computing times when running the rivid and getpar functions are required and the and j delay ranges comprise more values in this resolution thus the number of possible combinations to search for potential models is exponentially higher and requires long computing times or the use of high performance computing clusters which can be cost prohibitive as a result sub daily calibration datasets enter an additional process in the diagram where individual rainfall runoff events are selected chosen manually in this case the length of these events brings the size of the datasets closer to those with the daily time step after running the rivid and getpar functions a set of candidate models comprising all combinations in the search range is obtained this means that the values of a 1 a n and b 0 b m are found for all combinations of n m and in the case of siso models while magnitudes of a 1 a n and b j 0 b j l j are identified for all n l j and j in the case of miso please note that the expressions describing these models i e eqs 9 11 can have a maximum of five terms given the search ranges suggested before among all candidate models the best in each category i e siso and miso is selected from those being accurate and parametrically efficient while preserving the mass of water here these desirable model attributes are measured with the young identification criterion yic nash sutcliffe efficiency nse and percent bias pbias which are widely defined and applied in hydrology moriasi et al 2007 young and beven 1994 viewed as a multi objective optimization problem the pareto front contains the set of models performing better under these three metrics thus this set is computed from the three metrics of all candidate models with the algorithm included in the monte carlo analysis toolbox wagener and kollat 2007 note that other multi objective optimization tools are also suitable for this purpose then all models in the set having imaginary roots in polynomial a z i in eq 9 are discarded this is because avoiding such roots allows decomposing the models into several interconnected first order transfer functions facilitating their interpretation young 1992 from the remaining models the best is selected as the one with the highest score defined as 15 score nse 1 e yic 1 abs pbias 100 3 where nse yic and pbias are as previously stated and abs is the absolute value function please note that the score is a simple average where yic and pbias are transformed to be comparable with nse thus the score of a model approaches one when it preserves water volumes while being accurate and parametrically efficient also note that the nse and pbias only measure accuracy and mass conservation respectively while the yic balances accuracy and parametric efficiency at the same time for this reason the score gives more weight to the accuracy of the best model compared to its parametric efficiency and mass conservation once the structure and parameters of the model are calibrated by making this choice validation is performed following the sampling split method klemeš 1986 specifically the best models are used to simulate the catchment response to precipitation inputs in the validation dataset under these new conditions model performance is evaluated again based on its ability to reproduce streamflow observations this time only the nse and pbias are employed since the structures and parameters of the model do not change thus its parametric efficiency does not change either finally calibration and validation datasets are switched and the calibration process is repeated when similar values for both parameters and performance metrics are obtained after this iteration the model can be generally considered acceptable klemeš 1986 having a validated model it is subsequently interpreted in terms of its first order transfer function components as mentioned before avoiding imaginary roots in the polynomial a z i facilitates decomposing models initially having up to five terms in eqs 9 11 into these individual components particularly mathematical operations such as partial fraction expansion allow this type of decomposition young 1992 once decomposed the best models are written in terms of their components each having a characteristic time delay steady state gain and residence time for first order functions such delay is given by the value of i in z i while the steady state gain ssg and a residence time t r are computed as young et al 1996 16 ssg b 1 a 1 17 t r 1 ln a 1 where a 1 and b are the first order constants of the polynomials described in eqs 9 11 accordingly each component is interpreted as a reservoir with a residence time t r receiving an input of precipitation the output of this reservoir is connected to a linear channel with a delay defined by the value of i in z i in addition the ssg is related to the catchment area through the volumes of water entering and leaving the system such volumes are estimated with the previous analysis made with eqs 3 6 it is important to highlight that discarding models with imaginary roots in a z i and separating baseflow beforehand are key stages to simplify the interpretation process this is because separating high order transfer functions into first order components guarantees that values for ssg and t r can be computed and interpreted in addition having baseflow as a separate component makes these functions describe the surface runoff component only this is especially convenient when interpreting the individual responses of the catchment to each of the multiple precipitation inputs in the miso model 2 4 description of the case study the lenguazaque river basin is a headwater catchment located in south america s northern andes in the region of cundinamarca colombia fig 3 the catchment elevation is over 2600 m a s l and it lies specifically on the country s eastern cordillera in an area of 290 km2 consisting of cretaceous and tertiary rock and a combination of lacustrine fluvioglacial colluvial and alluvial deposits bogotá a et al 2011 car and corpoboyacá 2017 about 60 of this area is used for agriculture nearly 20 is occupied by forests and native vegetation of paramo and coal extraction takes place underground fernandez 2018 with a mean slope of 22 and a time of concentration estimated at 2 5 h in local studies the average discharge is around 1 4 m3s 1 car and corpoboyacá 2017 concerning the catchment climate its average temperature is 13 5 c and its annual average precipitation and evapotranspiration are estimated as 900 mm and 500 mm respectively car and corpoboyacá 2017 the area of interest is also a headwater of the larger fuquene lake watershed 1750 km2 a hot spot of freshwater biodiversity supplying water to more than 500 000 citizens where several issues have been documented examples include severe pollution loss of water volume in the receiving lake and replacement of paramo vegetation for other crops fernandez et al 2018a rubiano et al 2006 valderrama et al 2018 the lenguazaque river basin represents this region fairly well since it exemplifies these issues the basin comprises the largest area of paramo essential for regional water supply and storage now with significant portions of vegetation either replaced or affected by agricultural and mining activities additionally domestic and mining wastewaters are discharged without appropriate treatment mcintyre et al 2018 fernandez et al 2018b since such characteristics are not exclusive to this headwater watershed rubiano et al 2006 choosing it as a case study facilitates future upscaling to the other headstreams it is worth mentioning that the semi distributed physically based soil and water assessment tool swat model was previously calibrated for this area reproducing monthly streamflows between 2003 and 2015 with an nse and pbias of 0 56 and 0 001 respectively fernandez and camacho 2019 this is a significant baseline to consider in this study 3 results 3 1 stage i information analyses 3 1 1 information inventory relevant hydrometeorological stations are presented in fig 4 currently operated by the environmental agency in the area of interest car 2020 complementarily a summary of the information available at these stations is provided in appendix b table b 1 as shown rainfall and streamflow gauges are scattered across the basin and daily time series are significantly more continuous than those in sub daily resolution specifically the daily records comprise ten years of information with generally small percentages of missing values while sub daily data are limited to three years recorded in fewer sites and with longer gaps it is worth mentioning that almost 17 of daily streamflow data missing for station s2 is mainly due to a long gap with no recordings from january to june 2009 yet this percentage decreases below 1 afterward finally it is important to acknowledge the role of the environmental agency car which was the source of all original datasets in the case study while daily time series were provided as digital formats most sub daily records are available as hardcopy paper charts that need to be scanned and digitized only a few stations recently installed store data digitally and automatically such as s6 in the case study recording discharge at ten minute intervals 3 1 2 spatiotemporal distribution of rainfall and streamflow the resulting boxplots and maps are presented in figs b 1 and b 2 respectively appendix b the monthly boxplots reveal bimodal rainfall and streamflow regimes with wet periods around april and november and dry periods between december to february and june to september daily diagrams show this trend more clearly since they comprise more continuous time series although most sub daily plots also show the bimodal trends even when comprising sparser data this is not the case for station s8 where high resolution records are inconsistently steady and thus not appropriate for model development turning to spatial distribution isohyets presented in the maps show that daily precipitation gauges cover most sub basins well at the same time the drainage area of the tibita river is not represented sub daily with the same precision fig b 2 a b 2 b complementarily daily streamflow data comprise most streams and drainage areas while sub daily values are limited to the larger areas represented by stations s6 and s8 fig b 2 c table b 1 3 1 3 assessment of data quality consistency and validity mass and double mass curves are displayed in figs b 3 and b 4 respectively appendix b daily cumulative rainfall reveals s5 as the site receiving more precipitation followed by s2 and s1 with comparable volumes and s7 with the lowest amounts these curves show significant rainfall changes between 2009 and 2012 and more linear trends afterward fig b 3 a which is consistent with most cumulative streamflow plots fig b 3 b note that the latter exhibit data discrepancies at s8 since 2015 which is due to hydraulic adjustments performed then rendering subsequent records invalid complementarily linear trends observed for stations s1 and s7 in double mass curves indicate their rainfall records are generally consistent while some slope shifts in stations s2 and s5 indicate possible inconsistencies especially before 2013 when cumulative precipitation lies below 3000 mm fig b 4 a concerning sub daily data cumulative rainfall matches daily trends closely at s1 while values at s5 and s7 are significantly lower fig b 3 a similarly cumulative streamflow resembles daily values at s6 except when data are missing yet records are completely different at s8 fig b 3 b slope changes in double mass curves also suggest rainfall series incongruence especially at s7 in summary daily data are more consistent and of better quality while limitations of sub daily information restrict the reliability of models developed at this higher resolution after managing outliers and missing data for all stations the resulting from complete time series are displayed in fig b 5 more details on the results are presented in table b 2 appendix b the plots show that precipitation data are generally continuous in both resolutions while streamflow series are restricted especially with sub daily data fig b 5f g this means it is possible to develop models for any drainage area with a daily time step yet higher resolution models are limited to the region draining at s6 therefore modeling efforts center in this region allowing subsequent model comparison in both resolutions fig 5 shows the complete time series for this drainage area and is thus considered for modeling 3 2 stage ii data preparation 3 2 1 estimation of baseflow and surface runoff master recession curves and recession constants computed from observations at s6 are presented in fig 6 as shown recessions adjust well to eq 2 when these constants are 0 971 and 0 993 for daily and sub daily models respectively thus these are the values of r used in eq 1 to compute baseflow and surface runoff nevertheless ranges are provided as a reference for possible sensitivity and uncertainty analyses as well as comparison with field measurements 3 2 2 estimation of water balances results of the water balance are summarized in table b 3 as presented daily data indicate almost 30 of input precipitation leaves the drainage area of s2 as streamflow while this value is about 20 for s3 and between 12 and 15 for s4 and s6 this means that small and steep areas in the upper catchment have higher streamflow and less water leaving through percolation and evapotranspiration in proportion to their size compared to larger areas in the lower catchment the higher streamflow is likely due to lower population pastures and economic activities within the smaller sub catchments specifically at s6 where modeling was performed the balance indicates that 65 of the water leaving as streamflow is related to baseflow and the remaining portion is surface runoff note that sub daily data indicate these values are 80 and 20 respectively although these estimates are subject to more limited and less reliable data as previously mentioned 3 3 stage iii model development results of the previously mentioned swat model calibrated between 2003 and 2015 for the same catchment are presented in fig 7 this is the baseline scenario for the present study the model spatial inputs include elevation slope land use and soil type based on these inputs the catchment was divided into eight sub basins and about 1200 hydrologic response units hrus the high number of hrus responds to the unique combinations of the slope land uses and soil types in this area meanwhile the model considered daily precipitation temperature relative humidity solar radiation and wind speed as hydrometeorological inputs accordingly the model was set up to simulate monthly streamflow in station s6 as a response to these inputs swat simulates flow by routing water entering the catchment through two mechanisms arnold et al 2012a 2012b first water reaching the main channel of each sub basin is computed from a water balance for each hru second water in the channels is routed through the river network using either the variable storage or the muskingum method under this semi distributed scheme monthly streamflows were calibrated at station s6 using the sufi 2 algorithm in swat cup abbaspour 2015 a calibration and sensitivity analysis tool for swat such calibration involved 33 parameters identified in the literature creech et al 2015 francesconi et al 2016 among them only a few were found sensitive in the study area such as the hydraulic conductivity of soil sol k and the aquifer percolation coefficient rchrg dp the first three years of the simulation were considered a warm up period since the initial water contents in the catchment were unknown more details on the results of this model and the calibration process are available in previous studies fernandez 2018 fernandez and camacho 2019 3 3 1 daily resolution results obtained with the best siso and miso models are displayed in fig 8 and their corresponding structures parameters and performance are summarized in table 2 an analysis of the uncertainty of these structures and parameters is included in appendix d it is important to note that only the first four values of c k in table 1 were tested as possible alternatives to consider a nonlinear response of the catchment as shown graphically the response of both models is practically equivalent yet performance metrics suggest that the siso is a slightly better predictor during calibration the scores for both models are similar however individual metrics indicate the siso is more parametrically efficient while the miso is a better estimator of observed data and water volumes this is because the siso has a lower yic a metric indicating good fitness and parsimonious parametrization when highly negative in contrast the miso has a higher nse and a pbias closer to zero nevertheless the siso outperformed the miso during validation as its nse and pbias were better while keeping a simple structure in addition both models were significantly more accurate than the swat baseline when results were aggregated monthly however a comparison of these models with swat requires considering additional aspects which will be discussed further in the next section finally note that similar structures parameters and performance metrics were found when calibration and validation datasets were switched see table c 1 in appendix c thus the resulting siso and miso models as described in table 2 can be considered acceptable according to the sampling split method used for model validation klemeš 1986 from these results and eqs 13 and 14 the best siso model and the surface runoff component of the best miso model can be respectively written as 18 s k 0 97 b k 1 1 0 97 2 s k s k 1 0 05 1 0 6 z 1 s k p k 19 r k 0 014 0 018 z 1 1 0 5 z 1 s k p k s 1 0 018 1 0 5 z 1 s k p k s 2 0 005 1 0 5 z 1 s k p k s 5 0 005 1 0 5 z 1 s k p k 1 s 7 where s 1 s 2 s 5 and s 7 refer to the four stations measuring rainfall within the study area from these equations and the values provided in table 2 the models can also be interpreted hydrologically as follows the siso behaves as a 254 km2 reservoir receiving a unique precipitation input and discharging from a single output with negligible delay this means that on average the catchment responds to a precipitation event by increasing its output streamflow immediately yet the input hydrograph attenuates for almost two days before returning to baseflow conditions meanwhile the miso model comprises four systems connected in parallel each receiving a different precipitation input the first system includes two reservoirs with an aggregated area of 122 km2 while the second third and fourth comprise individual ponds with 71 21 and 40 km2 respectively here all ponds attenuate input hydrographs for 1 3 days and the delay of their outputs is negligible in most cases except for the fourth pond which is connected to a linear channel with a delay of one day this suggests that the response of the catchment i e increase of output discharge to precipitation events measured at stations s1 s2 and s5 starts the same day while for rainfall recorded at s7 it starts one day later note that both models use s k to account for the nonlinear response of surface runoff while their baseflow component is the same 3 3 2 sub daily resolution as in the previous case table 3 summarizes the structures characteristic values and performance metrics for the best siso and miso models and figs 9 and 10 show their corresponding simulation results compared to observations mathematically these models are respectively written as 20 s k 0 99 b k 1 1 0 99 2 s k s k 1 0 09 z 2 1 1 6 z 1 0 66 z 2 r k p k 21 r k 0 007 z 48 1 1 70 z 1 0 71 z 2 p k s 1 0 11 1 1 70 z 1 0 71 z 2 p k s 5 0 26 z 10 0 23 z 11 1 1 70 z 1 0 71 z 2 p k s 7 where all terms are as defined previously as shown the framework led to finding models capable of accurately representing individual events however there is still room for improvement specifically for individual events the best siso and miso models preserved water volumes well with a pbias close to zero in all cases meanwhile the siso model had a reasonable performance for all events nse 0 65 whereas the miso performed exceptionally for two events nse 0 95 and sub optimally for the other two nse 0 25 when simulating the complete calibration and validation periods the siso model captured the moments when the catchment responded to precipitation events yet the magnitudes of such response were mostly underestimated for the miso model the timing of the response was also captured to some extent yet it was overestimated for many precipitation events and underestimated for the highest streamflow peaks this results in models with a pbias magnitude far from zero reasonable nse values for total streamflow 0 5 yet inaccurate values for the surface runoff component nse 0 2 4 discussion the interpretation of daily models provides valuable insights into the hydrology of the catchment on the one hand areas found with the miso model have magnitudes resembling those estimated with the isohyetal method see fig b 2a this suggests that methods used for the miso model could provide an objective estimate of the spatial distribution of rainfall without making any prior calculations it is important to note that more research is required to determine the accuracy of this approach on the other hand values of delay and residence time estimated with the siso and miso models are valuable to characterize how fast the catchment responds to recorded precipitation events please note that these values provide more detailed information about such response compared to the currently available estimates of the time of concentration in the area namely a unique value of 2 5 h car and corpoboyacá 2017 thus the identified values may be the basis for the subsequent development of local warning and forecast systems especially given the precision of the implemented models to this end the analysis of uncertainty presented in appendix d is also valuable in addition it is important to consider possible reasons why the siso model outperforms the miso and why the current approach outperforms the swat baseline in the first case the time step used for modeling may be the main reason for the siso model outperforming the miso model as stated the delays and residence times identified for both models are less than one and two days respectively moreover travel times between stations s4 and s8 were estimated as less than 10 h under typical streamflow conditions fernandez 2018 such small response times make detecting individual delays for each precipitation input in daily time step resolution ineffective in improving the simulations of output discharge thus the sub daily character of water travel times in the catchment also supports the need for developing the models in higher resolution in the second case the uncertainty of the swat model input data and parameters are important to be acknowledged as previously stated implementing swat required significant amounts of information even when collected these data are highly uncertain because of the challenges when translating local studies on soils and land use to the required format by swat in addition parameters are uncertain since only a few were identifiable in the model despite their solid scientific foundation thus the present approach reduces such uncertainty by requiring fewer parameters and only precipitation and streamflow time series another reason for outperforming swat is related to the different mechanisms used to consider water storage in the catchment leading to a possible nonlinear response here only the first four values of c k in table 1 were tested as proxies of said storage all coming directly from observations meanwhile swat completely simulates the storage through the previously explained water balances and routing mechanisms thus using observations instead of simulated storage values also improves performance please note that such performance makes the present approach highly effective in identifying appropriate parameters to model the catchment s hydrology for this reason once identified future efforts should focus on testing alternative simulations to account for the storage of the catchment the antecedent precipitation and moisture indexes presented in table 1 can be considered among these alternatives such efforts would also allow the present approach to be practical for making predictions and supporting long term planning in the same way that swat is typically employed for this purpose joint implementation of this data based mechanistic approach with process based models can be useful as previously described in related literature e g ratto et al 2007 sivapalan and young 2005 young 2013 regarding the sub daily models several reasons may have led to their sub optimal performance firstly using a sample of the individual event was not sufficient to describe the catchment response as recorded in complete sub daily time series this may be due to the unique characteristics of each event which could not be extrapolated to the entire period possible alternatives to address this issue include increasing the number of events in the sample using more sophisticated statistical and automated approaches to extract individual events from sub daily time series and using more efficient computational strategies such strategies may comprise the design of more efficient algorithms using open source and cloud based high performance computing and promoting interdisciplinary work particularly involving both hydrologists and computer scientists may be a suitable alternative to increase the access and appropriate use of such computing resources a second reason is the lower quality of sub daily datasets as previously identified for this study nevertheless as mentioned for the daily models the importance of developing sub daily models relies on the sub daily nature of water travel times in the catchment thus promoting collaboration with local stakeholders to continue their sub daily hydrologic monitoring and improve the quality of their datasets over time are potential solutions to this issue in any case efforts made in the present study to implement models in sub daily resolution could serve as a baseline for future improvements accordingly the datasets and code are available in an online repository fernandez 2022 5 conclusion in summary the development of hydrological models in developing countries faces major challenges this study proposed a set of strategies to address these challenges especially in highly disturbed headwater catchments the overall results from the case study showed the reliability and suitability of the proposed approach in a real world scenario such approach is likely applicable to other catchments with similar physiographical characteristics even outside of the andes this is because the methods employed are mathematically simple require low amounts of data and computational power and have a sound hydrological meaning the proposed framework could be especially useful in headstreams of developing countries where water management is limited by data financial and scientific restrictions and where improved management is urgently needed the hydrological models developed under the proposed framework performed better than the data intensive semi distributed model previously setup and calibrated for the same headwater catchment in colombia meanwhile the uncertainty level of the present approach is less than the data intensive semi distributed model as the proposed approach uses observed data as a proxy of the water storage in the catchment moreover although modeling daily streamflow was successful long sub daily simulations are still far from acceptable despite a set of well reproduced short individual events thus improving the quality of sub daily data and computational methods to calibrate models with larger high resolution series is still needed considering results from the case study future work can take several directions the first is directed to the simulation of headwater hydrology under non stationary conditions given the highly dynamic character of these catchments although we focused on constant parameter models for stationary periods as the simplest case these models are the base for developing time variable parameter models possibly under the downward technique keeping the same parameters now variable in time these models should be suitable for evaluating alternative scenarios during longer non stationary periods the second concerns originated from studying the relationship between parameter values and changes in land use this should be addressed through the analysis of satellite imagery for example and would broaden the physical meaning of the models a third direction could be toward a better understanding of headwaters baseflow although the digital filter employed was proven appropriate in several watersheds its performance in headwater catchments still needs to be verified conducting baseflow field measurements in headwater catchments should be useful for this purpose once again many developing countries support vast populations near their headwaters with numerous limitations thus improving water management in these catchments is an imperative need and this work is intended as a tool to help fulfilling this purpose credit authorship contribution statement nicolas fernandez conceptualization methodology software validation formal analysis investigation data curation writing original draft visualization luis a camacho conceptualization methodology resources writing review editing visualization supervision project administration funding acquisition a pouyan nejadhashemi resources writing review editing visualization project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author acknowledges anna raschke for her help proofreading the initial version of the manuscript he also acknowledges the civil and environmental engineering department and the vice presidency of research at universidad de los andes as well as the biosystems and agricultural engineering department at michigan state university for the financial support during his doctoral research he also thanks the two anonymous reviewers whose thorough evaluation led to the final version of this manuscript appendix a supporting information datasets and code are stored in an online repository fernandez 2022 supplementary materials associated with this article can be found in the online version at doi 10 1016 j ejrh 2022 101243 appendix a supplementary material supplementary material 
7821,study region the touws river in the klein karoo region of south africa study focus this study sought to improve the understanding of pool dynamics along non perennial rivers nprs by utilising the water balance approach to assess the water fluxes that influence pool dynamics in the touws river the water balance model made use of various in situ and satellite derived data new hydrological insights the analysis of the water losses from the pool showed that most of the water was lost through evaporation the interaction between the pool and groundwater is dependent on the water levels as the pool loses water to the subsurface up to a certain depth then it starts gaining when the wolverfontein 2 pool is full it can retained water for approximately 258 days without having a surface water inflow a water balance model was established and it simulated the water levels with a high correlation of 0 9 this model was also evaluated in the neighbouring pools and while it simulated the water levels of the upstream pool well this was not the case for the downstream pool when remote sensing derived rainfall and evaporation data were used in the model the simulated water levels had a slightly lower correlation of 0 7 with the observed water levels overall the remotely sensing based monthly fluxes estimates could not provide the detailed pool information that was required for the water balance errors may have arisen or they may have been inherited from any of the three remotely sensed parameters namely the surface area the rainfall or the evaporation although remote sensing did not provide detailed information it is worth noting that it provides baseline information on the pool dynamics overall this work underscores the relevance of multisource data and the water balance it helps to better understand the pool dynamics and it will help with the better management of nprs keywords dryland areas pool hydrodynamics hydrological water balance river ponds temporary rivers water budget data availability the authors do not have permission to share data 1 introduction non perennial rivers nprs comprise all rivers that cease to flow for certain periods of the year these occur globally and across all climatic zones and biomes messager et al 2021 and their occurrence is increasing due to climate change social economic uses and land use effects for some of the nprs when flows cease water occurs in pools along these rivers these pools are of importance for aquatic life as refugia and surrounding communities as a source of water for livestock garden watering and domestic use maswanganye et al 2022 pools occurring along nprs have been recognised for their ecological importance ilhéu et al 2020 marshall et al 2016 sheldon et al 2010 there is a relationship between the ecological state of pools and their hydrology for example bonada et al 2020 found that larger pools tend to have a higher species richness and abundance because of this pools are often considered when determining environmental flows there is however limited information about the nature and causes of spatiotemporal variations of water storage in pools bonada et al 2020 bourke et al 2020 shanafield et al 2021 this knowledge gap constrains the formulation of appropriate management measures consequently management decisions are made by extrapolating knowledge based on the spatiotemporal variations of water storage in lakes bonada et al 2020 maswanganye et al 2021 shanafield et al 2021 recommended the need to improve the understanding of the persistence of pools and how they are impacted by climatic shifts and groundwater abstractions furthermore the communities that utilise these pools need information for allocation and planning purposes for example how long will it take for them to dry up ali et al 2015 routine monitoring of water storage in pools along nprs has not been included in most national hydrological monitoring systems partly because these systems are often considered to have low value rodríguez lozano et al 2020 and due to the absence of adequate financial resources there are also physical limitations as some of these pools are not easily accessible and some may disappear after flow events depending on river bed material hattingh 2020 maswanganye et al 2022 however very few studies have shown that remote sensing can provide useful information about these pools including their spatial distribution and size maswanganye et al 2021 seaton et al 2020 maswanganye et al 2022 found that river flows are the major controlling factor of pool dynamics and suggested that rainfall is important for delaying the drying out of pools in the semi arid and arid environments of south africa however the study also expressed that there is a need to assess pools in detail in order to gain a better understanding of their hydrodynamics several methods can be applied for assessing the pool water fluxes these methods include direct measurements labaugh et al 2016 as well as linear and multiple regression stasik et al 2020 although direct measurements are accurate the limited availability of data on some components remains challenging due to the complexity associated with field measurements and monitoring for instance it is challenging to quantify the interaction between groundwater and pools the linear and multiple regression methods also require data and are easy to use but difficulties are experienced with non linear and non stationary systems li et al 2016 seo et al 2015 to overcome this issue more complex process based models are used such as deterministic and stochastic models while artificial intelligence ai and machine learning have also been used to assist with the complexity of the water systems seo et al 2015 these models may have difficulty estimating beyond the data ranges that are used for training and they may be difficult to interpret due to hidden processes layers talebizadeh and moridnejad 2011 while environmental tracers can also be used to qualify the water sources in a pool as in hamilton et al 2020 in some cases water types cannot be separated by a hydrochemical analysis bourke et al 2020 the water balance approach has been widely used to represent and predict changes in water storage of water bodies ali et al 2015 this approach is based on the law of conservation and has been used to understand water fluxes that influence water body dynamics and simulate the water availability in hydrological systems such as lakes and wetlands gronewold et al 2020 mbanguka et al 2016 the water balance like other methods requires data or estimates of each of the hydrological components evaporation precipitation surface water in and outflows and groundwater in and outflows however the advantage of the water balance is that it can be used to estimate an unknown component of the water balance equation this component is usually the groundwater in and outflows which are difficult to measure directly e g xiao et al 2018 for instance parsons and vermeulen 2017 found that 16 9 and 83 1 of the water lost by the groenvlei lake were due to groundwater outflows and evaporation respectively in addition the water balance method can be used to predict the responses of pools to changes in inputs or outflows this information can also be used to predict how development for example building a dam will alter the hydrology of a water body although the water balance has been applied to understand the dynamics of water bodies across the globe it has not been used to understand pools along nprs maswanganye et al 2022 and bourke et al 2020 suggest that the water balance approach can assist in improving the understanding of pool dynamics which could be useful in the management of nprs and their contributing catchments therefore this study aims to improve the understanding of pool dynamics or water storage changes in pools along non perennial rivers nprs in the semi arid environments of the karoo region of south africa the study uses the water balance method to assess water fluxes that influence the pool dynamics in addition because most areas with these pools may not have the required data for the water balance approach this study also explores the potential of using open access remotely sensed data in the water balance model 2 material and methods 2 1 study area description the study was conducted in the pools occurring along the touws river which is located in the karoo region in south africa fig 1 the touws river is 140 km long and with channel widths of about 200 m the channel has a sandy gravel substrate above the adolpaspoort shale formation the entire catchment is 6 280 km2 but this study is confined to a site where the catchment area is 5750 km2 the catchment is covered mainly in natural vegetation predominantly shrubland and fynbos with some parts of the floodplain being used for agricultural purposes the mean annual rainfall is 244 mm year grenfell et al 2021 however the study site received rainfall amounting to 112 mm year in 2018 91 mm year in 2019 and 182 mm year in 2020 there is no distinct wet dry seasons maswanganye et al 2022 according to petersen et al 2017 the catchment has a mean annual runoff mar of 16 32 mm3 year or 2 5 mm year the available observed flow data for the touws river shows that the river can go for years without having a flow fig 2 2 2 pool description the study investigated three pools located along a 1 2 km stretch of the touws river in the plathuis area at wolverfontein fig 1 the pool on the upstream end referred to as wolverfontein 1 ww1 is located at 33 641726 s and 20 965985 e and had a maximum area of 10 045 m2 the second pool wolverfontein 2 ww2 is 700 m downstream of ww1 located at 33 639076 s and 20 975719 e and with a maximum area of 17 742 m2 the third pool is 450 m downstream of ww2 at 33 642918 s and 20 982405 e and is referred to as touwsberg twb this third pool had a maximum area of 15 722 m2 the study focuses mainly on wolverfontein 2 ww2 pool which is situated along the left bank that is hilly with exposed bedrock while the right bank has a sparsely vegetated floodplain fig 1c and d according to hattingh 2020 the ww2 pool has a substrate of predominantly fine sand this pool has a maximum depth of 1 7 m the ww2 pool nearly dried up during the 2016 2019 drought during flow events these pools connect they are accessible and they persist for long enough to sustain some form of life aquatic vegetation and animal community as described in zacharias and zamparas 2010 furthermore these pools are located close to the flow occurrence rainfall and groundwater level observation points 2 3 data collection and analyses 2 3 1 in situ data a water balance analysis of water storage in pools requires data on rainfall evaporation rates pool storage inflows and outflows of both river water and groundwater the surface area data were obtained from the global positioning system gps measurements collected along the edges of the pools using a hand held gps and a staff gauge was used to measure the water levels during the field visits table 1 a solinst water level logger m3001 m5 and logging at hourly intervals was installed in each pool to measure water levels water levels in ww2 were measured for two years 2019 2021 while this was done for a year 2020 2021 in ww1 and twb pools data from this rain gauge were used for water balance analysis two boreholes for monitoring changes in the depth to the water table were drilled on the left bank 200 m upstream of ww2 this site was the closest to ww2 that a drilling rig could access because of the hilly terrain adjacent to ww2 the two boreholes had depths of 25 m and 60 m a water level data logger logging at hourly intervals was installed in each borehole weather data were required for estimating evaporation rates using the penman method data from the closest weather station owned by the agricultural research council were used this station is located 27 km south east of the study pools rainfall and flow occurrence obtained from the citizen science monitoring programme whereby farmers neighboring the pools collected these data the rainfall data were collected using non recording rain gauges manual notes on flow occurrence absence and presence were recorded by event one farmer is located within the study site 600 m from the ww2 pool fig 1 and the other is located one km upstream of the study site the relationship rating curves between the surface area depth and volume were determined in order to be able to convert between these measurements the volume of the pool was estimated based on the following equations which were derived using 3d analyst on arcgis and by using the differential global positioning system dgps points and continuous water level measurements eq 1 and 2 the following relationships were specifically derived and used for the ww2 pool 1 h 0 00009 a r 0 99 2 v 0 00005a2 0 1415 a 18 83 r 0 99 where h is the depth of water in a pool in metres v is the volume of water stored in m3 and a is the area of the pool in m2 2 3 2 remote sensing data since in situ data on water balance components of non perennial pools are often unavailable the study explored the use of readily and freely available remotely sensed data for water balance analysis of water storage in pools along touws river evaporation data were obtained from the moderate resolution imaging spectroradiometer modis 16 pet product as described by mu et al 2011 2007 the data were downloaded from the appears website https lpdaacsvc cr usgs gov appeears modis 16 evapotranspiration data are widely and commonly used and jovanovic et al 2015 found that modis 16 evapotranspiration and potential evapotranspiration estimates over south africa s landscape were satisfactory however they cautioned that the spatial resolution can limit its potential for small scale use some studies have argued that modis 16 derived pet is suited for small scale application astuti et al 2022 bugan et al 2020 utilised the modis 16 dataset in a hydrological model at a catchment level and concluded that the dataset has the potential to be used in data scarce regions based on the findings of these previous studies the study explored the use of this it was assumed that modis 16 pet would provide the closest satellite derived and freely accessible estimates satellite derived rainfall estimates were obtained from the climate hazards infrared precipitation with stations chirps product as described by funk et al 2015 which was downloaded from the climate engine website https app climateengine com climateengine many studies such as those of maswanganye 2018 plessis and kibii 2021 and pitman and bailey 2021 have suggested that chirps can be used in the absence of in situ data to obtain the surface area of the pool from remote sensing data sentinel 2 images were downloaded and the modified normalized difference water index mndwi was computed to distinguish the water areas pixels from the non water areas shadows in the imagery were classified using the random forest technique and were used to mask out their effect on the derived mndwi water pixels maswanganye et al 2022 twenty four monthly sentinel 2 images close to the end of each month from august 2019 to august 2021 were used the relationship between the surface area and the water depth obtained from the bathymetric survey presented in the in situ data analysis section was used to convert the remotely sensing derived surface area to water depth and then compared with the observed water levels 2 3 3 pool dynamics data analyses this study first examined the water depths of the focal pool ww2 in relation to the water fluxes in order to gain an insight into the water gains and losses the time to empty and the probability of the pool drying out were then determined the water balance model was established by using in situ data which will be explained in the next section the water balance model calibrated using ww2 data was tested on the other two pools ww1 and twb satellite derived rainfall and evaporation estimates were incorporated into the model by substituting the observed rainfall and evaporation which resulted in an in situ and remote sensing hybrid water balance this model does not consider the groundwater in and outflows fig 3 the fully remote sensing based analysis used the changes from the surface area that were obtained from the sentinel 2 images and the satellite derived rainfall and evaporation the performance of all the models was evaluated by using the actual water levels measured of the pools fig 3 illustrates the methodological flow of the analyses in this study the time to empty te was defined as the time it takes for a pool to completely drain out the water from being full this is based on the water loss rate of the pool and assumes that there are no surface water inflows into the pool 1 te s max l wl where smax is the maximum water level in meters and lwl is the average water loss per day in meters which is obtained from assessing the observed water levels the probability of the pool drying out is the chance of finding the pool dry which is calculated based on the dry period no streamflow duration exceeding the time to empty while considering that the rainfall over the pool can reduce the number of days that the pool will be dry in this study this was calculated by using the 30 year flow occurrence and rainfall data because there is no long term data on the other water balance components 2 3 4 water balance analysis water storage in the pool is described by the following water balance eq 2 and illustration 4 fig 4 2 s t s t 1 p t e t q i n t q o u t t g i n t g o u t t where s t is storage at the end of time period t t being a daily interval p t is volume of rainfall over the pool e t is volume of water evaporated from the pool during the day t qin t is river inflows into the pool qout t is surface outflow from the pool gin t is groundwater discharge into the pool gout t is groundwater recharge from the pool daily rainfall p t data obtained from the nearby homestead was used to estimate volume of rainfall over the pool using the following relation 3 p t p t a t where a t is the surface area of the pool obtained using the relationship between surface area and water storage evaporation from the pool e t was estimated similarly to p t with evaporation rates derived using the penman 1948 method based on weather station data as it is a commonly used method for estimating open water evaporation mbanguka et al 2016 yihdego and webb 2018 based on empirical observations the study assumed that when river inflows are occurring continuously then the pool fills and during that period inflows will equal outflows from the pool thus qin t qout t during this period although the pool remains full some of the inflowing water will contribute to subsurface water around and beneath the pool the influence of the pool recharging subsurface water will materialise when no surface inflows occur after the inflows have ceased the amount of water flowing from the pool into the subsurface material will depend on the area of the pool or volume of water in storage thus gout t was assumed to be described by the following relationship 4 g o u t t a s t s 1 b where s1 is the volume of water in the pool below which there will be no positive hydraulic gradient into the subsurface material the volume of water in the pool can also be represented by the depth of water in the pool we assumed that gout t will be a function of the depth of water in the pool 5 g i n t c h where c is a coefficient the model was built specifically for the ww2 pool by using the above water balance approach and its equation and assumptions were transferred to the ww1 and twb pools only two adjustments were made the initial water level starting point and the maximum water level as these pools were not of equal size these pools are very close to the ww2 pool therefore it was assumed that they have the same hydroclimatic conditions 2 4 statistical analysis in order to evaluate the performance of the water balance analysis the following statistics were used the mean error me the mean absolute error mae the correlation coefficient r and the paired t test were used the mean error me which is also called bias measures the average of the estimation error this considers the direction of the errors eq 6 the me ranges from negative infinity to positive infinity and has a perfect score of 0 a positive score indicates that the model is over estimating while a negative score indicates that it is under estimating on average however with the me a perfect score can be achieved when the over and under estimation compensate each other hence the mean absolute error mae was used to provide a true estimation error eq 7 and the me was used to derive the direction of the error 6 me 1 n i 1 n h obs i h sim i 7 mae 1 n i 1 n h obs i h sim i where h obs i is the observed water level h sim i is the simulated water level and n is total number of data points a t test eq 8 was used to determine whether there is a significant difference between means of the observed and simulated water levels 8 t x y n x 2 y 2 x y 2 n 1 where t is the t statistic x is the observed water level mean y is the modelled water level and n is the total number of data points a paired t test assumes that the data sets are continuous that they follow a normal distribution that the mean is a good measure of the central tendency and that the two samples are paired helsel et al 2020 to assess the relationship between the simulated and observed water levels at different time steps daily monthly a correlation coefficient eq 9 was used the correlation ranges from 1 to 1 with 1 being a perfect relationship and 0 meaning that there is no relationship between the observed and the simulated values 9 r n oe o e n o 2 o 2 n e 2 e 2 where o is the observed water level measured by a logger e is the simulated water level and n is the number of score pairs of scores the mean error mean absolute error t test and correlation coefficient were also used to assess the transferability of the model to a pool that is upstream and downstream of ww2 3 results 3 1 water level assessment the water balance analysis shows that the major gains in water level were due to river flow occurrences and that the minor gains were due to the rainfall received over the pool fig 5 high losses always followed the high gain episodes which suggests that water losses might be a function of the water level the depth to water of the shallow and deep boreholes shows no significant changes in relation to the pool water levels nor to the occurrence of flows however the water level data between 2020 02 07 2020 07 31 was missing due to a stolen logger during the covid 19 hard lockdown period 3 1 1 assessment of the water losses from the pool the observed pool water level data from august 2019 to august 2021 suggests that the pool loses approximately 0 2 m per month or 2 4 m per year the losses are high during the southern hemisphere summer 0 29 m month and low during the winter months 0 09 m month fig 6 this indicates that when the pool is full it can last on average for 258 days 8 5 months without any inflows this pool loses 0 7 m year more than the estimated penman evaporation rates the difference may be attributed to water lost through seepage into the subsurface material fig a2 in supplementary material when the volume of water in storage or the water surface area of the pool is large evaporation losses will be large similarly with a large pool bed covered with water and if the underlying subsurface material is unsaturated seepage will also be large since the water depth increases with volume of water in storage or pool surface area water losses from the pool will increase with water depth 3 1 2 probability of the pool drying out based on the observed losses and time to empty the river flow and rainfall data from 1990 to 2020 were used to establish the chances of the pool drying out there is only a 10 chance of finding the pool dry as the pool was likely to have dried out 11 times in 30 years or it could have potentially dried out for 1115 days out of 11322 days 30 years table 2 this is based on the no flow and no rain days exceeding 258 days rainfall reduces the number of potential pool dry days for instance 52 mm during the no flow period can delay the drying of the pool by eight days these estimates suggest that the most prolonged period with no water was 411 days during the 2015 2017 drought assuming that it did not receive any water from the groundwater 3 2 the water balance model based on the understanding of the pool the water balance approach was used to simulate its water levels the water balance satisfactorily predicted the water levels me 0 03 m mae 0 05 m r 0 96 and there was no significant difference between the means t 4 5 over the assessed period 2019 08 25 2021 08 10 fig 7 besides the inputs rainfall and evaporation the model was supplied with a maximum water level of 1 7 m which is also the cease to flow level and the initial water level moreover the model was able to predict the water levels during the period where no observed data were available february july 2020 the model shows that when the pool has more water the water is rapidly lost via seepage into the subsurface strata or aquifer and that the seepage ranged from 0 to 0 005 m day and was defined as 0 003 of the water level of the pool fig a2 in supplementary materials rainfall delays the drying of the pool the pool is sensitive to the flow occurrence and the assumption that every flow will fill the pool to capacity is correct and drives the model after the river flow has ceased evaporation dominates the water losses the model suggests that seepage into the subsurface material occurs when the water depth exceeds 1 1 m seepage out of the pool does not occur below this water depth instead groundwater discharge into the pool occurs when the water depth is less than 1 1 m this proposed behavior could be that the water level in the pool will be greater than the local water table around the pool when the water depth exceeds 1 1 m hence groundwater recharge occurs from the pool with a water depth below 1 1 m the local water table will be above the pool water level hence groundwater discharges into the pool fig a3 in the supplementary material shows the model that does not take the above behavior into consideration 3 2 1 transferability of the water balance to the surrounding pools the simulated water levels of the ww1 pool were in good agreement with the observed water level me 0 02 m mae 0 04 m r 0 96 fig 8 the only changes made from the original water balance model from the ww2 pool was the maximum water level which was adjusted by trial and error to be 0 95 m for the ww1 pool and the initial observed water level water lost to groundwater was estimated in the same way as for the ww2 pool 0 003 of the water depth however the model overestimated the water lost by the pool between december 2020 and november 2021 which resulted in the lowest predicted level of 0 2 m compared to the observed level of 0 35 m for the twb pool which is 450 m downstream of the ww2 pool the model did not perform as well as the ww1 pool me 0 02 m mae 0 06 m r 0 86 which suggests that the pool varies significantly from the focus pool ww2 during a field visit seepage into the pool was observed the constant water level of the pool between june and august 2021 suggests that it probably receives substantial sub surface inflows in order to maintain such water levels the paired t test t 8 3 showed that at a 5 significance level there is no significant difference between the observed daily mean water level 0 64 m and the simulated mean 0 62 m for the ww1 pool there was however a significant difference t 1 9 between the modelled mean water level 0 89 m and the observed mean 0 91 m of the twb pool 3 3 water balance analysis using remote sensing data 3 3 1 comparison of the remote sensing and observed inputs of the model in terms of comparing the inputs the chirps rainfall estimates compared well with the observed rainfall data r 0 6 however it has errors during some periods such as july to august 2020 fig 9 although the remotely sensed evaporation rates from modis 16 pet are closely related to the observed evaporation rates that were derived by using the penman equation r 0 98 they overestimated the months with lower evaporation april to sept fig 10 a general assessment of the climatic water balance shows that the remotely sensed climatic water balance is strongly associated with observed climatic water balance r 0 87 fig 11 which suggests that a monthly based water balance can have errors caused by rainfall and evaporation but these are likely to be small the negative climate water balance indicates that the catchment is potentially in a water deficit the comparison between the observed water level and the remotely sensed surface area of the pool is in good agreement me 0 04 m mae 0 2 m r 0 72 see fig 12 there seem to be more discrepancies but they are minor when the pool is almost full water level 1 2 m overall the remote sensing estimated surface water of pool is promising therefore freely accessible remote sensing data were incorporated into the water balance particularly the chirps and modis16 pet data the initial and maximum water level and flow occurrence were the only inputs used this also assumes that no information exists about water losses due to subsurface groundwater the results show an underestimation of the water losses as expected fig 13 as losses into the sub surface are not incorporated the surface area of the pool obtained from remote sensing was converted to the water level equation 12 the remote sensing based estimation showed an increase in the water level in response to the flow occurrence fig 14 the remote sensing based water balance suggests that 65 of the water is lost through evaporation therefore 35 is lost to the sub surface negative residual which is higher than the outcomes from the in situ based estimation 4 discussion the study focused on improving the understanding of pool dynamics along non perennial rivers by assessing the water fluxes using the water balance approach the results showed that one flow event can sustain the pool for 258 days without any inflows although the probability of such a prolonged no flow is low 10 this suggests that the ww2 pool that was focused upon is semi permanent to permanent pools in south australia showed a similar persistency i e 286 days for the pool with a maximum water level of greater than 1 6 m marshall et al 2016 the water balance model also supports the fact that the pool is very sensitive to the flow occurrence as indicated by maswanganye et al 2022 the persistence of the pool might change over time as evaporation increases and as the rainfall declines over the region due to climate change department of environmental affairs 2018 these findings also suggest that if there is dam construction upstream which reduces the frequency of the river flows the pools will be impacted and this could lead to the drying out of the pools which has further implications for the biodiversity found in these pools bonada et al 2020 larned et al 2010 therefore this information should be considered when proposing any new development such as the construction of a dam the water balance models indicate that there might be groundwater inflow into the pools will occur during the period of low water depth this might be seasonal as observed by bestland et al 2017 in this case of the current study this was observed when the pool reached a certain level as it has been stated that the study catchment has no clear wet and dry season maswanganye et al 2022 found that surface flow and rainfall did not cause a fluctuation in the groundwater levels hence it was suggested that the groundwater does not feed the pool the water balance analyses revealed that water losses from the pool into the subsurface is insignificant to cause groundwater level fluctuations the substrate and the underlying geology of the pool also suggest that there is limited or no interaction low conductivity hwang et al 2017 mohuba et al 2020 the interaction might also depend on the gradient between the pool and the water table as illustrated in fig 15 this observation is further supported by the elevation plot using dgps measurements which shows that groundwater usually fluctuates at around 1 1 m of the pool s water level fig 16 bourke et al 2020 referred to this kind of pool as a through flow pool although the water balance models performed well by using just the flow occurrence having information about the discharge into and out of the pool could have provided more insight for instance how the relationship between the discharge and pool water level affects the water losses furthermore in order to determine whether the pool water losses from upstream are detected downstream interaction between the pools some studies have suggested that pools can remain hydrologically connected through shallow groundwater paths while being disconnected on the surface larned et al 2010 the water balance model displayed robustness and transferability to the ww1 pool albeit with minor adjustments to the maximum and initial water level however it did not perform as well when evaluated at the twb pool this might be due to the pool having a strong subsurface flow impact which influences the dynamics of the pool it is also possible that the properties of the twb pool may differ for example the presence of algae and shade over the water which might significantly reduce evaporation trimmel et al 2018 furthermore seaman et al 2016 indicated that neighbouring pools along the same reach can differ significantly the ww1 pool upstream was shown to have the same pattern as the ww2 pool however it will dry out before the ww2 pool because it is smaller in size the twb pool downstream showed a very distinct pattern in terms of losses as itsustained its size or water level for longer periods which suggests that this could be a permanent pool remote sensing detects the pools and provides a general overview of the pool dynamics as suggested by maswanganye et al 2022 as it was able to detect major changes correctly however it does not provide detailed information or an understanding of the pool dynamics at the water balance level this might be due to errors emanating from each of the model input variables furthermore errors may also be caused by the resolution of the remote sensing data when compared to the size and the temporal dynamics of the pool when the water balance approach is applied in larger surface bodies such as large dams and lakes these errors might be negligible chen et al 2022 dues et al 2018 the water balance can also provide a better insight when applied on a long term basis however to improve the remote sensing based water balance model there is a need to acquire more information on the flow occurrence this could be done by detecting flows from satellite images or it can be predicted through rainfall a runoff rainfall model furthermore the groundwater information that is required for predicting pool water losses to subsurface stores is still a mystery in the remote sensing field this could be predicted by using the climatic variable s for instance groundwater losses could be expressed as function of evaporation this estimation should take into account the substrate and underlying geology of the area and the fact that the relationship is not linear as it depends on the size of the pool and the season predicting the gwin flow will still be a challenge as it was shown that it could be function of the groundwater table the grace satellite showed to be useful in larger water bodies deus et al 2013 however the incorporation of remote sensing based climatic variables was shown to be limited by the unknown groundwater pool interaction this suggests that remote sensing can used to understand the pool dynamics of pools that are not influenced by groundwater processes overall the results provided a better understanding of the pool dynamics and they imply that the water balance approach could be useful for understanding pools along non perennial rivers the information derived from the water balance should be incorporated in the water resource management of nprs and catchments water resource managers can determine the water that is available in the pools by knowing the last day of the flow 5 conclusion there are limited studies on the hydrology of pools along non perennial rivers using pools along the touws river in the karoo region of south africa this study assessed the pool dynamics by using the water balance approach the study established that wolverfontein 2 pool is a semi permanent pool that has little chance of completely drying out the water balance of the pools was established and modelled with limited data and the simulated water levels showed a satisfactory performance the model was transferable to the neighbouring pools although it required an adjustment of the maximum and initial water levels the water balance approach that was applied to the pool provided a better insight into the pool dynamics the models suggest that there is groundwater pool interaction at the assessed site however the magnitude of the losses seems to be minor when compared to the losses into the atmosphere via evaporation the pool has a point where the rate of the loss is less than the evaporation which indicates that there is a potential gain from the groundwater these gains and rainfall into the pools delay the drying out of the pools we assume that the errors in the estimation of water levels are due to the uncertainty related to a full understanding of the pool groundwater interactions the use of remotely sensed climatic variables with a maximum water level can provide temporal dynamics for pools with no groundwater influence when the flow occurrence is known if the size of the pool is known remote sensing can provide an overview of the general behaviour of the pool but it cannot provide the detailed information that an in situ observation can provide however with all the rapid advancements in the remote sensing field this gap will soon be closed this study successfully used the water balance approach to understand the pool dynamics and the information derived from the water balance models is of significant importance for the management of pools and pool dynamics in semi arid environments credit authorship contribution statement sagwati e maswanganye conceptualization writing original draft data curation timothy dube conceptualization writing review editing supervision project administration funding acquisition nebo jovanovic writing review editing supervision project administration funding acquisition evison kapangaziwiri writing review editing supervision project administration funding acquisition dominic mazvimavi conceptualization writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to appreciate the water research commission of south africa for funding this project k5 2936 and the university of the western cape for providing us with the opportunity to do this work appendix a supporting information supplementary data associated with this article can be found in the online version at doi 10 1016 j ejrh 2022 101244 appendix a supplementary material supplementary material 
7821,study region the touws river in the klein karoo region of south africa study focus this study sought to improve the understanding of pool dynamics along non perennial rivers nprs by utilising the water balance approach to assess the water fluxes that influence pool dynamics in the touws river the water balance model made use of various in situ and satellite derived data new hydrological insights the analysis of the water losses from the pool showed that most of the water was lost through evaporation the interaction between the pool and groundwater is dependent on the water levels as the pool loses water to the subsurface up to a certain depth then it starts gaining when the wolverfontein 2 pool is full it can retained water for approximately 258 days without having a surface water inflow a water balance model was established and it simulated the water levels with a high correlation of 0 9 this model was also evaluated in the neighbouring pools and while it simulated the water levels of the upstream pool well this was not the case for the downstream pool when remote sensing derived rainfall and evaporation data were used in the model the simulated water levels had a slightly lower correlation of 0 7 with the observed water levels overall the remotely sensing based monthly fluxes estimates could not provide the detailed pool information that was required for the water balance errors may have arisen or they may have been inherited from any of the three remotely sensed parameters namely the surface area the rainfall or the evaporation although remote sensing did not provide detailed information it is worth noting that it provides baseline information on the pool dynamics overall this work underscores the relevance of multisource data and the water balance it helps to better understand the pool dynamics and it will help with the better management of nprs keywords dryland areas pool hydrodynamics hydrological water balance river ponds temporary rivers water budget data availability the authors do not have permission to share data 1 introduction non perennial rivers nprs comprise all rivers that cease to flow for certain periods of the year these occur globally and across all climatic zones and biomes messager et al 2021 and their occurrence is increasing due to climate change social economic uses and land use effects for some of the nprs when flows cease water occurs in pools along these rivers these pools are of importance for aquatic life as refugia and surrounding communities as a source of water for livestock garden watering and domestic use maswanganye et al 2022 pools occurring along nprs have been recognised for their ecological importance ilhéu et al 2020 marshall et al 2016 sheldon et al 2010 there is a relationship between the ecological state of pools and their hydrology for example bonada et al 2020 found that larger pools tend to have a higher species richness and abundance because of this pools are often considered when determining environmental flows there is however limited information about the nature and causes of spatiotemporal variations of water storage in pools bonada et al 2020 bourke et al 2020 shanafield et al 2021 this knowledge gap constrains the formulation of appropriate management measures consequently management decisions are made by extrapolating knowledge based on the spatiotemporal variations of water storage in lakes bonada et al 2020 maswanganye et al 2021 shanafield et al 2021 recommended the need to improve the understanding of the persistence of pools and how they are impacted by climatic shifts and groundwater abstractions furthermore the communities that utilise these pools need information for allocation and planning purposes for example how long will it take for them to dry up ali et al 2015 routine monitoring of water storage in pools along nprs has not been included in most national hydrological monitoring systems partly because these systems are often considered to have low value rodríguez lozano et al 2020 and due to the absence of adequate financial resources there are also physical limitations as some of these pools are not easily accessible and some may disappear after flow events depending on river bed material hattingh 2020 maswanganye et al 2022 however very few studies have shown that remote sensing can provide useful information about these pools including their spatial distribution and size maswanganye et al 2021 seaton et al 2020 maswanganye et al 2022 found that river flows are the major controlling factor of pool dynamics and suggested that rainfall is important for delaying the drying out of pools in the semi arid and arid environments of south africa however the study also expressed that there is a need to assess pools in detail in order to gain a better understanding of their hydrodynamics several methods can be applied for assessing the pool water fluxes these methods include direct measurements labaugh et al 2016 as well as linear and multiple regression stasik et al 2020 although direct measurements are accurate the limited availability of data on some components remains challenging due to the complexity associated with field measurements and monitoring for instance it is challenging to quantify the interaction between groundwater and pools the linear and multiple regression methods also require data and are easy to use but difficulties are experienced with non linear and non stationary systems li et al 2016 seo et al 2015 to overcome this issue more complex process based models are used such as deterministic and stochastic models while artificial intelligence ai and machine learning have also been used to assist with the complexity of the water systems seo et al 2015 these models may have difficulty estimating beyond the data ranges that are used for training and they may be difficult to interpret due to hidden processes layers talebizadeh and moridnejad 2011 while environmental tracers can also be used to qualify the water sources in a pool as in hamilton et al 2020 in some cases water types cannot be separated by a hydrochemical analysis bourke et al 2020 the water balance approach has been widely used to represent and predict changes in water storage of water bodies ali et al 2015 this approach is based on the law of conservation and has been used to understand water fluxes that influence water body dynamics and simulate the water availability in hydrological systems such as lakes and wetlands gronewold et al 2020 mbanguka et al 2016 the water balance like other methods requires data or estimates of each of the hydrological components evaporation precipitation surface water in and outflows and groundwater in and outflows however the advantage of the water balance is that it can be used to estimate an unknown component of the water balance equation this component is usually the groundwater in and outflows which are difficult to measure directly e g xiao et al 2018 for instance parsons and vermeulen 2017 found that 16 9 and 83 1 of the water lost by the groenvlei lake were due to groundwater outflows and evaporation respectively in addition the water balance method can be used to predict the responses of pools to changes in inputs or outflows this information can also be used to predict how development for example building a dam will alter the hydrology of a water body although the water balance has been applied to understand the dynamics of water bodies across the globe it has not been used to understand pools along nprs maswanganye et al 2022 and bourke et al 2020 suggest that the water balance approach can assist in improving the understanding of pool dynamics which could be useful in the management of nprs and their contributing catchments therefore this study aims to improve the understanding of pool dynamics or water storage changes in pools along non perennial rivers nprs in the semi arid environments of the karoo region of south africa the study uses the water balance method to assess water fluxes that influence the pool dynamics in addition because most areas with these pools may not have the required data for the water balance approach this study also explores the potential of using open access remotely sensed data in the water balance model 2 material and methods 2 1 study area description the study was conducted in the pools occurring along the touws river which is located in the karoo region in south africa fig 1 the touws river is 140 km long and with channel widths of about 200 m the channel has a sandy gravel substrate above the adolpaspoort shale formation the entire catchment is 6 280 km2 but this study is confined to a site where the catchment area is 5750 km2 the catchment is covered mainly in natural vegetation predominantly shrubland and fynbos with some parts of the floodplain being used for agricultural purposes the mean annual rainfall is 244 mm year grenfell et al 2021 however the study site received rainfall amounting to 112 mm year in 2018 91 mm year in 2019 and 182 mm year in 2020 there is no distinct wet dry seasons maswanganye et al 2022 according to petersen et al 2017 the catchment has a mean annual runoff mar of 16 32 mm3 year or 2 5 mm year the available observed flow data for the touws river shows that the river can go for years without having a flow fig 2 2 2 pool description the study investigated three pools located along a 1 2 km stretch of the touws river in the plathuis area at wolverfontein fig 1 the pool on the upstream end referred to as wolverfontein 1 ww1 is located at 33 641726 s and 20 965985 e and had a maximum area of 10 045 m2 the second pool wolverfontein 2 ww2 is 700 m downstream of ww1 located at 33 639076 s and 20 975719 e and with a maximum area of 17 742 m2 the third pool is 450 m downstream of ww2 at 33 642918 s and 20 982405 e and is referred to as touwsberg twb this third pool had a maximum area of 15 722 m2 the study focuses mainly on wolverfontein 2 ww2 pool which is situated along the left bank that is hilly with exposed bedrock while the right bank has a sparsely vegetated floodplain fig 1c and d according to hattingh 2020 the ww2 pool has a substrate of predominantly fine sand this pool has a maximum depth of 1 7 m the ww2 pool nearly dried up during the 2016 2019 drought during flow events these pools connect they are accessible and they persist for long enough to sustain some form of life aquatic vegetation and animal community as described in zacharias and zamparas 2010 furthermore these pools are located close to the flow occurrence rainfall and groundwater level observation points 2 3 data collection and analyses 2 3 1 in situ data a water balance analysis of water storage in pools requires data on rainfall evaporation rates pool storage inflows and outflows of both river water and groundwater the surface area data were obtained from the global positioning system gps measurements collected along the edges of the pools using a hand held gps and a staff gauge was used to measure the water levels during the field visits table 1 a solinst water level logger m3001 m5 and logging at hourly intervals was installed in each pool to measure water levels water levels in ww2 were measured for two years 2019 2021 while this was done for a year 2020 2021 in ww1 and twb pools data from this rain gauge were used for water balance analysis two boreholes for monitoring changes in the depth to the water table were drilled on the left bank 200 m upstream of ww2 this site was the closest to ww2 that a drilling rig could access because of the hilly terrain adjacent to ww2 the two boreholes had depths of 25 m and 60 m a water level data logger logging at hourly intervals was installed in each borehole weather data were required for estimating evaporation rates using the penman method data from the closest weather station owned by the agricultural research council were used this station is located 27 km south east of the study pools rainfall and flow occurrence obtained from the citizen science monitoring programme whereby farmers neighboring the pools collected these data the rainfall data were collected using non recording rain gauges manual notes on flow occurrence absence and presence were recorded by event one farmer is located within the study site 600 m from the ww2 pool fig 1 and the other is located one km upstream of the study site the relationship rating curves between the surface area depth and volume were determined in order to be able to convert between these measurements the volume of the pool was estimated based on the following equations which were derived using 3d analyst on arcgis and by using the differential global positioning system dgps points and continuous water level measurements eq 1 and 2 the following relationships were specifically derived and used for the ww2 pool 1 h 0 00009 a r 0 99 2 v 0 00005a2 0 1415 a 18 83 r 0 99 where h is the depth of water in a pool in metres v is the volume of water stored in m3 and a is the area of the pool in m2 2 3 2 remote sensing data since in situ data on water balance components of non perennial pools are often unavailable the study explored the use of readily and freely available remotely sensed data for water balance analysis of water storage in pools along touws river evaporation data were obtained from the moderate resolution imaging spectroradiometer modis 16 pet product as described by mu et al 2011 2007 the data were downloaded from the appears website https lpdaacsvc cr usgs gov appeears modis 16 evapotranspiration data are widely and commonly used and jovanovic et al 2015 found that modis 16 evapotranspiration and potential evapotranspiration estimates over south africa s landscape were satisfactory however they cautioned that the spatial resolution can limit its potential for small scale use some studies have argued that modis 16 derived pet is suited for small scale application astuti et al 2022 bugan et al 2020 utilised the modis 16 dataset in a hydrological model at a catchment level and concluded that the dataset has the potential to be used in data scarce regions based on the findings of these previous studies the study explored the use of this it was assumed that modis 16 pet would provide the closest satellite derived and freely accessible estimates satellite derived rainfall estimates were obtained from the climate hazards infrared precipitation with stations chirps product as described by funk et al 2015 which was downloaded from the climate engine website https app climateengine com climateengine many studies such as those of maswanganye 2018 plessis and kibii 2021 and pitman and bailey 2021 have suggested that chirps can be used in the absence of in situ data to obtain the surface area of the pool from remote sensing data sentinel 2 images were downloaded and the modified normalized difference water index mndwi was computed to distinguish the water areas pixels from the non water areas shadows in the imagery were classified using the random forest technique and were used to mask out their effect on the derived mndwi water pixels maswanganye et al 2022 twenty four monthly sentinel 2 images close to the end of each month from august 2019 to august 2021 were used the relationship between the surface area and the water depth obtained from the bathymetric survey presented in the in situ data analysis section was used to convert the remotely sensing derived surface area to water depth and then compared with the observed water levels 2 3 3 pool dynamics data analyses this study first examined the water depths of the focal pool ww2 in relation to the water fluxes in order to gain an insight into the water gains and losses the time to empty and the probability of the pool drying out were then determined the water balance model was established by using in situ data which will be explained in the next section the water balance model calibrated using ww2 data was tested on the other two pools ww1 and twb satellite derived rainfall and evaporation estimates were incorporated into the model by substituting the observed rainfall and evaporation which resulted in an in situ and remote sensing hybrid water balance this model does not consider the groundwater in and outflows fig 3 the fully remote sensing based analysis used the changes from the surface area that were obtained from the sentinel 2 images and the satellite derived rainfall and evaporation the performance of all the models was evaluated by using the actual water levels measured of the pools fig 3 illustrates the methodological flow of the analyses in this study the time to empty te was defined as the time it takes for a pool to completely drain out the water from being full this is based on the water loss rate of the pool and assumes that there are no surface water inflows into the pool 1 te s max l wl where smax is the maximum water level in meters and lwl is the average water loss per day in meters which is obtained from assessing the observed water levels the probability of the pool drying out is the chance of finding the pool dry which is calculated based on the dry period no streamflow duration exceeding the time to empty while considering that the rainfall over the pool can reduce the number of days that the pool will be dry in this study this was calculated by using the 30 year flow occurrence and rainfall data because there is no long term data on the other water balance components 2 3 4 water balance analysis water storage in the pool is described by the following water balance eq 2 and illustration 4 fig 4 2 s t s t 1 p t e t q i n t q o u t t g i n t g o u t t where s t is storage at the end of time period t t being a daily interval p t is volume of rainfall over the pool e t is volume of water evaporated from the pool during the day t qin t is river inflows into the pool qout t is surface outflow from the pool gin t is groundwater discharge into the pool gout t is groundwater recharge from the pool daily rainfall p t data obtained from the nearby homestead was used to estimate volume of rainfall over the pool using the following relation 3 p t p t a t where a t is the surface area of the pool obtained using the relationship between surface area and water storage evaporation from the pool e t was estimated similarly to p t with evaporation rates derived using the penman 1948 method based on weather station data as it is a commonly used method for estimating open water evaporation mbanguka et al 2016 yihdego and webb 2018 based on empirical observations the study assumed that when river inflows are occurring continuously then the pool fills and during that period inflows will equal outflows from the pool thus qin t qout t during this period although the pool remains full some of the inflowing water will contribute to subsurface water around and beneath the pool the influence of the pool recharging subsurface water will materialise when no surface inflows occur after the inflows have ceased the amount of water flowing from the pool into the subsurface material will depend on the area of the pool or volume of water in storage thus gout t was assumed to be described by the following relationship 4 g o u t t a s t s 1 b where s1 is the volume of water in the pool below which there will be no positive hydraulic gradient into the subsurface material the volume of water in the pool can also be represented by the depth of water in the pool we assumed that gout t will be a function of the depth of water in the pool 5 g i n t c h where c is a coefficient the model was built specifically for the ww2 pool by using the above water balance approach and its equation and assumptions were transferred to the ww1 and twb pools only two adjustments were made the initial water level starting point and the maximum water level as these pools were not of equal size these pools are very close to the ww2 pool therefore it was assumed that they have the same hydroclimatic conditions 2 4 statistical analysis in order to evaluate the performance of the water balance analysis the following statistics were used the mean error me the mean absolute error mae the correlation coefficient r and the paired t test were used the mean error me which is also called bias measures the average of the estimation error this considers the direction of the errors eq 6 the me ranges from negative infinity to positive infinity and has a perfect score of 0 a positive score indicates that the model is over estimating while a negative score indicates that it is under estimating on average however with the me a perfect score can be achieved when the over and under estimation compensate each other hence the mean absolute error mae was used to provide a true estimation error eq 7 and the me was used to derive the direction of the error 6 me 1 n i 1 n h obs i h sim i 7 mae 1 n i 1 n h obs i h sim i where h obs i is the observed water level h sim i is the simulated water level and n is total number of data points a t test eq 8 was used to determine whether there is a significant difference between means of the observed and simulated water levels 8 t x y n x 2 y 2 x y 2 n 1 where t is the t statistic x is the observed water level mean y is the modelled water level and n is the total number of data points a paired t test assumes that the data sets are continuous that they follow a normal distribution that the mean is a good measure of the central tendency and that the two samples are paired helsel et al 2020 to assess the relationship between the simulated and observed water levels at different time steps daily monthly a correlation coefficient eq 9 was used the correlation ranges from 1 to 1 with 1 being a perfect relationship and 0 meaning that there is no relationship between the observed and the simulated values 9 r n oe o e n o 2 o 2 n e 2 e 2 where o is the observed water level measured by a logger e is the simulated water level and n is the number of score pairs of scores the mean error mean absolute error t test and correlation coefficient were also used to assess the transferability of the model to a pool that is upstream and downstream of ww2 3 results 3 1 water level assessment the water balance analysis shows that the major gains in water level were due to river flow occurrences and that the minor gains were due to the rainfall received over the pool fig 5 high losses always followed the high gain episodes which suggests that water losses might be a function of the water level the depth to water of the shallow and deep boreholes shows no significant changes in relation to the pool water levels nor to the occurrence of flows however the water level data between 2020 02 07 2020 07 31 was missing due to a stolen logger during the covid 19 hard lockdown period 3 1 1 assessment of the water losses from the pool the observed pool water level data from august 2019 to august 2021 suggests that the pool loses approximately 0 2 m per month or 2 4 m per year the losses are high during the southern hemisphere summer 0 29 m month and low during the winter months 0 09 m month fig 6 this indicates that when the pool is full it can last on average for 258 days 8 5 months without any inflows this pool loses 0 7 m year more than the estimated penman evaporation rates the difference may be attributed to water lost through seepage into the subsurface material fig a2 in supplementary material when the volume of water in storage or the water surface area of the pool is large evaporation losses will be large similarly with a large pool bed covered with water and if the underlying subsurface material is unsaturated seepage will also be large since the water depth increases with volume of water in storage or pool surface area water losses from the pool will increase with water depth 3 1 2 probability of the pool drying out based on the observed losses and time to empty the river flow and rainfall data from 1990 to 2020 were used to establish the chances of the pool drying out there is only a 10 chance of finding the pool dry as the pool was likely to have dried out 11 times in 30 years or it could have potentially dried out for 1115 days out of 11322 days 30 years table 2 this is based on the no flow and no rain days exceeding 258 days rainfall reduces the number of potential pool dry days for instance 52 mm during the no flow period can delay the drying of the pool by eight days these estimates suggest that the most prolonged period with no water was 411 days during the 2015 2017 drought assuming that it did not receive any water from the groundwater 3 2 the water balance model based on the understanding of the pool the water balance approach was used to simulate its water levels the water balance satisfactorily predicted the water levels me 0 03 m mae 0 05 m r 0 96 and there was no significant difference between the means t 4 5 over the assessed period 2019 08 25 2021 08 10 fig 7 besides the inputs rainfall and evaporation the model was supplied with a maximum water level of 1 7 m which is also the cease to flow level and the initial water level moreover the model was able to predict the water levels during the period where no observed data were available february july 2020 the model shows that when the pool has more water the water is rapidly lost via seepage into the subsurface strata or aquifer and that the seepage ranged from 0 to 0 005 m day and was defined as 0 003 of the water level of the pool fig a2 in supplementary materials rainfall delays the drying of the pool the pool is sensitive to the flow occurrence and the assumption that every flow will fill the pool to capacity is correct and drives the model after the river flow has ceased evaporation dominates the water losses the model suggests that seepage into the subsurface material occurs when the water depth exceeds 1 1 m seepage out of the pool does not occur below this water depth instead groundwater discharge into the pool occurs when the water depth is less than 1 1 m this proposed behavior could be that the water level in the pool will be greater than the local water table around the pool when the water depth exceeds 1 1 m hence groundwater recharge occurs from the pool with a water depth below 1 1 m the local water table will be above the pool water level hence groundwater discharges into the pool fig a3 in the supplementary material shows the model that does not take the above behavior into consideration 3 2 1 transferability of the water balance to the surrounding pools the simulated water levels of the ww1 pool were in good agreement with the observed water level me 0 02 m mae 0 04 m r 0 96 fig 8 the only changes made from the original water balance model from the ww2 pool was the maximum water level which was adjusted by trial and error to be 0 95 m for the ww1 pool and the initial observed water level water lost to groundwater was estimated in the same way as for the ww2 pool 0 003 of the water depth however the model overestimated the water lost by the pool between december 2020 and november 2021 which resulted in the lowest predicted level of 0 2 m compared to the observed level of 0 35 m for the twb pool which is 450 m downstream of the ww2 pool the model did not perform as well as the ww1 pool me 0 02 m mae 0 06 m r 0 86 which suggests that the pool varies significantly from the focus pool ww2 during a field visit seepage into the pool was observed the constant water level of the pool between june and august 2021 suggests that it probably receives substantial sub surface inflows in order to maintain such water levels the paired t test t 8 3 showed that at a 5 significance level there is no significant difference between the observed daily mean water level 0 64 m and the simulated mean 0 62 m for the ww1 pool there was however a significant difference t 1 9 between the modelled mean water level 0 89 m and the observed mean 0 91 m of the twb pool 3 3 water balance analysis using remote sensing data 3 3 1 comparison of the remote sensing and observed inputs of the model in terms of comparing the inputs the chirps rainfall estimates compared well with the observed rainfall data r 0 6 however it has errors during some periods such as july to august 2020 fig 9 although the remotely sensed evaporation rates from modis 16 pet are closely related to the observed evaporation rates that were derived by using the penman equation r 0 98 they overestimated the months with lower evaporation april to sept fig 10 a general assessment of the climatic water balance shows that the remotely sensed climatic water balance is strongly associated with observed climatic water balance r 0 87 fig 11 which suggests that a monthly based water balance can have errors caused by rainfall and evaporation but these are likely to be small the negative climate water balance indicates that the catchment is potentially in a water deficit the comparison between the observed water level and the remotely sensed surface area of the pool is in good agreement me 0 04 m mae 0 2 m r 0 72 see fig 12 there seem to be more discrepancies but they are minor when the pool is almost full water level 1 2 m overall the remote sensing estimated surface water of pool is promising therefore freely accessible remote sensing data were incorporated into the water balance particularly the chirps and modis16 pet data the initial and maximum water level and flow occurrence were the only inputs used this also assumes that no information exists about water losses due to subsurface groundwater the results show an underestimation of the water losses as expected fig 13 as losses into the sub surface are not incorporated the surface area of the pool obtained from remote sensing was converted to the water level equation 12 the remote sensing based estimation showed an increase in the water level in response to the flow occurrence fig 14 the remote sensing based water balance suggests that 65 of the water is lost through evaporation therefore 35 is lost to the sub surface negative residual which is higher than the outcomes from the in situ based estimation 4 discussion the study focused on improving the understanding of pool dynamics along non perennial rivers by assessing the water fluxes using the water balance approach the results showed that one flow event can sustain the pool for 258 days without any inflows although the probability of such a prolonged no flow is low 10 this suggests that the ww2 pool that was focused upon is semi permanent to permanent pools in south australia showed a similar persistency i e 286 days for the pool with a maximum water level of greater than 1 6 m marshall et al 2016 the water balance model also supports the fact that the pool is very sensitive to the flow occurrence as indicated by maswanganye et al 2022 the persistence of the pool might change over time as evaporation increases and as the rainfall declines over the region due to climate change department of environmental affairs 2018 these findings also suggest that if there is dam construction upstream which reduces the frequency of the river flows the pools will be impacted and this could lead to the drying out of the pools which has further implications for the biodiversity found in these pools bonada et al 2020 larned et al 2010 therefore this information should be considered when proposing any new development such as the construction of a dam the water balance models indicate that there might be groundwater inflow into the pools will occur during the period of low water depth this might be seasonal as observed by bestland et al 2017 in this case of the current study this was observed when the pool reached a certain level as it has been stated that the study catchment has no clear wet and dry season maswanganye et al 2022 found that surface flow and rainfall did not cause a fluctuation in the groundwater levels hence it was suggested that the groundwater does not feed the pool the water balance analyses revealed that water losses from the pool into the subsurface is insignificant to cause groundwater level fluctuations the substrate and the underlying geology of the pool also suggest that there is limited or no interaction low conductivity hwang et al 2017 mohuba et al 2020 the interaction might also depend on the gradient between the pool and the water table as illustrated in fig 15 this observation is further supported by the elevation plot using dgps measurements which shows that groundwater usually fluctuates at around 1 1 m of the pool s water level fig 16 bourke et al 2020 referred to this kind of pool as a through flow pool although the water balance models performed well by using just the flow occurrence having information about the discharge into and out of the pool could have provided more insight for instance how the relationship between the discharge and pool water level affects the water losses furthermore in order to determine whether the pool water losses from upstream are detected downstream interaction between the pools some studies have suggested that pools can remain hydrologically connected through shallow groundwater paths while being disconnected on the surface larned et al 2010 the water balance model displayed robustness and transferability to the ww1 pool albeit with minor adjustments to the maximum and initial water level however it did not perform as well when evaluated at the twb pool this might be due to the pool having a strong subsurface flow impact which influences the dynamics of the pool it is also possible that the properties of the twb pool may differ for example the presence of algae and shade over the water which might significantly reduce evaporation trimmel et al 2018 furthermore seaman et al 2016 indicated that neighbouring pools along the same reach can differ significantly the ww1 pool upstream was shown to have the same pattern as the ww2 pool however it will dry out before the ww2 pool because it is smaller in size the twb pool downstream showed a very distinct pattern in terms of losses as itsustained its size or water level for longer periods which suggests that this could be a permanent pool remote sensing detects the pools and provides a general overview of the pool dynamics as suggested by maswanganye et al 2022 as it was able to detect major changes correctly however it does not provide detailed information or an understanding of the pool dynamics at the water balance level this might be due to errors emanating from each of the model input variables furthermore errors may also be caused by the resolution of the remote sensing data when compared to the size and the temporal dynamics of the pool when the water balance approach is applied in larger surface bodies such as large dams and lakes these errors might be negligible chen et al 2022 dues et al 2018 the water balance can also provide a better insight when applied on a long term basis however to improve the remote sensing based water balance model there is a need to acquire more information on the flow occurrence this could be done by detecting flows from satellite images or it can be predicted through rainfall a runoff rainfall model furthermore the groundwater information that is required for predicting pool water losses to subsurface stores is still a mystery in the remote sensing field this could be predicted by using the climatic variable s for instance groundwater losses could be expressed as function of evaporation this estimation should take into account the substrate and underlying geology of the area and the fact that the relationship is not linear as it depends on the size of the pool and the season predicting the gwin flow will still be a challenge as it was shown that it could be function of the groundwater table the grace satellite showed to be useful in larger water bodies deus et al 2013 however the incorporation of remote sensing based climatic variables was shown to be limited by the unknown groundwater pool interaction this suggests that remote sensing can used to understand the pool dynamics of pools that are not influenced by groundwater processes overall the results provided a better understanding of the pool dynamics and they imply that the water balance approach could be useful for understanding pools along non perennial rivers the information derived from the water balance should be incorporated in the water resource management of nprs and catchments water resource managers can determine the water that is available in the pools by knowing the last day of the flow 5 conclusion there are limited studies on the hydrology of pools along non perennial rivers using pools along the touws river in the karoo region of south africa this study assessed the pool dynamics by using the water balance approach the study established that wolverfontein 2 pool is a semi permanent pool that has little chance of completely drying out the water balance of the pools was established and modelled with limited data and the simulated water levels showed a satisfactory performance the model was transferable to the neighbouring pools although it required an adjustment of the maximum and initial water levels the water balance approach that was applied to the pool provided a better insight into the pool dynamics the models suggest that there is groundwater pool interaction at the assessed site however the magnitude of the losses seems to be minor when compared to the losses into the atmosphere via evaporation the pool has a point where the rate of the loss is less than the evaporation which indicates that there is a potential gain from the groundwater these gains and rainfall into the pools delay the drying out of the pools we assume that the errors in the estimation of water levels are due to the uncertainty related to a full understanding of the pool groundwater interactions the use of remotely sensed climatic variables with a maximum water level can provide temporal dynamics for pools with no groundwater influence when the flow occurrence is known if the size of the pool is known remote sensing can provide an overview of the general behaviour of the pool but it cannot provide the detailed information that an in situ observation can provide however with all the rapid advancements in the remote sensing field this gap will soon be closed this study successfully used the water balance approach to understand the pool dynamics and the information derived from the water balance models is of significant importance for the management of pools and pool dynamics in semi arid environments credit authorship contribution statement sagwati e maswanganye conceptualization writing original draft data curation timothy dube conceptualization writing review editing supervision project administration funding acquisition nebo jovanovic writing review editing supervision project administration funding acquisition evison kapangaziwiri writing review editing supervision project administration funding acquisition dominic mazvimavi conceptualization writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to appreciate the water research commission of south africa for funding this project k5 2936 and the university of the western cape for providing us with the opportunity to do this work appendix a supporting information supplementary data associated with this article can be found in the online version at doi 10 1016 j ejrh 2022 101244 appendix a supplementary material supplementary material 
7822,study region regions of bamako kati and kangaba southwestern mali study focus machine learning based mapping of borehole yield three algorithms were trained on an imbalanced multiclass database of boreholes while twenty variables were used as predictors for borehole yield all models returned balanced and geometric scores in the order of 0 80 with area under the receiver operating characteristic curve up to 0 87 three main methodological conclusions are drawn a the evaluation of different machine learning classifiers and various resampling strategies and the subsequent selection of the best performing ones is shown to be a good strategy in this type of studies b ad hoc calibration tools such as data on borehole success rates provide an apt complement to standard machine learning metrics and c a multiclass approach with an unbalanced database represents a greater challenge than predicting a bivariate outcome but potentially results in a finer depiction of field conditions new hydrological insights for the region alluvial sediments were found to be the most productive areas while the mandingue plateau has the lowest groundwater potential the piedmont areas showcase an intermediate groundwater prospect elevation basement depth slope and geology rank among the most important variables lower values of clay content slopes and elevations and higher values of basement depth and saturated thickness were linked to the most productive class graphical abstract ga1 keywords machine learning groundwater exploration yield prediction gis mali data availability the authors do not have permission to share data 1 introduction groundwater plays a vital role in drinking water supplies food security and ecosystem services an estimated 2 5 billion people worldwide rely exclusively on groundwater to meet their daily needs while hundreds of millions of farmers depend on groundwater to sustain their livelihoods unesco 2015 adelana and macdonald 2008 explain that the increasing prominence of groundwater as a drinking water source in africa is motivated by three main reasons namely the large natural storage capacity of aquifers the often good water quality and that groundwater based infrastructures are generally more affordable for disadvantaged communities while critical for the survival of human beings and ecosystems groundwater also suffers from the hidden treasure syndrome because groundwater is out of sight aquifers are seldom well known even in industrialized countries this leads to widespread contamination problems of unregistered use and dropping water tables worldwide which all hamper attempts to manage the resource sustainably improving hydrogeological knowledge is thus essential particularly given the current context of climate change in which groundwater resources are expected to be increasingly relied upon groundwater potential mapping gpm can be used to underpin groundwater resources planning and exploration elbeih 2015 díaz alcaide and martínez santos 2019 carried out a review of gpm approaches analyzing the definitions of gpm the variables that are typically involved in gpm studies and the data integration procedures available in the literature these authors found that most of the existing studies rely on expert decision approaches like analytical hierarchy processes muavhi et al 2021 weight of evidence approaches ahmed et al 2021 fuzzy analytical hierarchy processes boughariou et al 2021 multi criteria decision making forootan and seyedi 2021 frequency ratio trabelsi et al 2018 and shannon s entropy methods khoshtinat et al 2019 gómez escalonilla et al 2022 show that these expert based techniques often require the reclassification of explanatory variables in intervals which is likely to originate bias from the outset because the intervals are based almost exclusively on the criteria of the operator the advent of machine learning approaches ml opens up a whole new methodological dimension to gpm as ml approaches can overcome this disadvantage to a large extent the recent literature showcases several examples of ml algorithms in groundwater potential mapping studies these include support vector machines al fugara et al 2022 panahi et al 2020 decision trees al abadi et al 2021 arabameri et al 2021 braham et al 2022 artificial neural networks chen et al 2021 nguyen et al 2020 hakim et al 2022 and ensemble methods like boosting random forests and extra trees classification among others bai et al 2022 choudhary et al 2022 gómez escalonilla et al 2021 2022 martinsen et al 2022 sachdeva and kumar 2021 ml based predictions of groundwater potential are typically carried out within a geographic information database the underlying logic is that the presence of groundwater that is the target variable can be inferred from a series of explanatory variables the latter typically include landforms rainfall lineaments lithology slope weathering and drainage density among others díaz alcaide and martínez santos 2019 the ground truth sample that is used to train the algorithms consists of an existing borehole database with information on positive and negative boreholes and in some cases characteristics such as flow rate or water table depth among others if the borehole database presents a sufficiently large and diverse number of records machine learning classifiers can find those patterns of explanatory variables that lead to a positive or a negative borehole the validated algorithm can then be extrapolated spatially to obtain a groundwater potential score for every pixel in the geographic database most ml gpm studies focus on predicting a positive or negative outcome that is in identifying zones of high or low groundwater prospect in this context our paper presents two major innovations the first one contributes to clarify the manifold concept of groundwater potential díaz alcaide and martínez santos 2019 by providing groundwater potential predictions in terms of borehole yield the second one consists of going one step beyond conventional predictive capabilities by replacing bivariate outcomes with multiclass outcomes to the authors knowledge there is only one methodological precedent in this regard in which kumar et al 2021 evaluated machine learning and fuzzy ahp methods for mapping groundwater potential in data scarce areas this study used a data augmentation technique to increase the database available to train the algorithms because our database is roughly twice as large augmentation is not needed and thus the present work represents a unique example in the literature based on an entirely independent set of field records we evaluated the performance of four different resampling methods synthetic minority oversampling technique smote adaptive synthetic sampling approach adasyn random oversampling ros and random undersampling rus resampling was performed after the training testing split and was only applied to the training dataset which contributes to avoid the problems of data leakage and overfitting from a geographical standpoint ml gpm studies have been carried out mainly in asia while there are very few in the african continent braham et al 2022 gómez escalonilla et al 2021 2022 martínez santos and renard 2020 namous et al 2021 moreover ours is one of the first studies in the literature addressing groundwater potential based on a multiclass approach a crucial component in ensuing analyses is the need to approach the problem of predicting yield based on an unbalanced ground truth dataset where there is a much larger number of positive than negative points since in many cases drilling companies do not report negative drillings to the competent authorities in this context two main objectives have been established 1 to develop a multiclass predictive borehole yield map of the study region and 2 to evaluate different machine learning models and resampling techniques to address the imbalanced dataset approach in order to establish a strategy for potential future studies of the same nature 2 materials and methods 2 1 study area the study area spans 21 000 km2 including the administrative region of bamako and the municipalities of kati and kangaba of the koulikoro region southern mali fig 1 the maximum altitude in the study area is 800 m a s l in the mandingue plateau while the minimum elevation is 300 m a s l in the northernmost end of the niger river the region is characterized by a tropical savanna climate traore et al 2018 with average temperatures of 27 c and an average rainfall around 1000 mm year the dry season lasts from october to may and accounts for just 25 of the total rainfall while the west african monsoon months provide the rest diancoumba et al 2020 estimated aquifer recharge rates to be around 3 26 of annual precipitation from a geological standpoint the area consists of four geological units fig 2 namely precambrian materials including sandstones intrusive igneous rocks recent alluvial sediments and weathering mantles developed over the precambrian materials the alluvial sediments take place mostly along river niger and its tributaries the map of the main geological units was obtained from landsat 8 bands and was constructed with qgis 3 s semiautomatic classification plugin congedo 2021 training points were obtained from the 1 200 000 geological map of mali dngm and lgsjm 1988 precambrian rocks exhibit fissured porosity and fractures are often interconnected this provides relatively high permeability on a regional scale and favors precipitation infiltration and groundwater flow to lower elevations traore 1985 shows that the transmissivity of these units ranges around 130 m² day this study also indicates that the thickness of fractured horizons varies between 30 and 50 m although deeper fractures can increase the thickness of the aquifer up to 80 100 m intrusive igneous rocks more specifically dolerites may behave either as a barrier or as preferential pathways for groundwater flow due to fracturing associated with the intrusive process under certain conditions these units may constitute aquifers of local importance however the average transmissivity of the system fluctuates around 4 m² day traore 1985 weathered mantles extend across most of the study region pnud 1982 these materials present an average transmissivity of 7 m2 day with a maximum of 350 m2 day and a minimum of 0 1 m2 day depending on the lithology traore et al 2018 these authors indicate that the thickness of the productive zone varies from 10 to 50 m and the depth of the static water levels ranges between 8 and 20 m below the surface these aquifers are drained by the niger river system recent alluvial sediments of the niger river show relatively good hydrogeology properties with an estimated transmissivity that varies from 0 34 to 21 6 m2 d alpha et al 1991 thickness is highly variable but generally ranges between 2 and 20 m mihe 1990 static levels typically vary between 2 and 8 m below the surface 2 2 target variable the database provided by the national hydraulic direction of mali dnh 2010 contains 483 points with borehole yield data these are widely distributed in space fig 3 the dataset is however imbalanced in terms of yield levels 36 points 7 4 were classified as negative 323 points 66 9 present borehole yields of between 0 5 and 5 m3 h and 124 points 25 4 exceed 5 m3 h in the experience of the authors this is common in regional borehole databases gómez escalonilla et al 2021 and is likely attributable to the fact that negative boreholes are reported to the authorities less frequently than positive boreholes because data imbalances cause difficulties to the predictive potential of the algorithms a series of trial runs were carried out to determine the optimal threshold between borehole yield classes these will be described later on joint interpretation of figs 1 and 3 suggests that most of the high yield boreholes are located along the alluvial materials of river niger whereas the mandingue plateau accounts for most of the negative ones the remainder of the study area presents a combination of low and high yield boreholes 2 3 conditioning factors díaz alcaide and martínez santos 2019 show several variables that are commonly used in gpm studies these include lithology geology geomorphology soil land use land cover topography lineaments drainage and slope related variables rainfall and groundwater recharge in turn kumar 1997 and jyrkama et al 2002 contend that crucial elements of groundwater potential such as aquifer recharge are influenced by several factors namely climatic conditions soil types and characteristics land cover geomorphological factors and hydrological features besides the available data allows for the development of other spatially distributed layers of information including the expected saturated thickness of the basement depth thus in this case we took into consideration twenty explanatory variables or conditioning factors as partial predictors of borehole yield table 1 the borehole database contains information about borehole depth as well as static level measurements boreholes throughout the study area are frequently drilled to the impervious basement consequently borehole depth may be interpreted as a proxy for the thickness of the overlying materials water table depth fig 4 a and expected saturated thickness layer fig 4 b may also be inferred from this information soil types of the study area were obtained from the european soil data centre dewitte et al 2013 fig 4c petric plinthosols take up almost 50 of the study area followed the haplic lixisols and lithic leptosols which cover about 24 and 18 respectively petric plinthosols are defined as a type of soil arranged in continuous or fractured sheets rich in fe or formed by concretions or nodules also rich in fe connected and strongly cemented haplic lixisols exhibit a higher clay content in the deeper part than in the shallower one due to pedogenetic processes lithic leptosols are very fine soils developed on continuous rock with an extremely rich content of coarse fragments the parent rock appears less than 10 cm from the soil surface iuss working group wrb 2015 land cover information stems from the european space agency climate change initiative esa 2010 close and open shrublands account for more than 52 of the study area while mosaic vegetation represents about 36 fig 4d a combination of rainfed croplands and broadleaved evergreen or semi evergreen forest add up to the remaining 9 the digital elevation model dem was obtained from the radar based shuttle radar topography mission available at https earthexplorer usgs gov the dem was used to produce several layers including slope fig 4e and slope curvature fig 4f while landforms fig 5a were derived with qgis 3 0 s geomorphon plugin jasiewicz and stepinski 2013 the surface water channels extracted from the dem analysis were used to develop the drainage density fig 5b and distance from channels layers topographic wetness index twi and stream power index spi maps were also derived from the dem twi fig 5c represents the facility for water to accumulate on the surface beven and kirkby 1979 in turn spi fig 5d provides a proxy of the erosive potential of runoff water moore et al 1991 satellite imagery can provide valuable information despite not penetrating deep into the ground in those areas associated with shallow groundwater levels díaz alcaide and martínez santos 2019 in this research landsat 8 bands were used to elaborate four different layers these include a surface lithological map constructed by means semi automatic classification plugin congedo 2021 fig 2 a normalized difference vegetation index ndvi layer fig 5e which is an estimation of the vigor of vegetation and is calculated from vegetation response to red and visible infrared wavelengths xie et al 2008 the normalized difference water index ndwi a measure of the quantity of water in the plants or soil humidity xu 2006 fig 5f and the alteration band ratio or surface clay content fig 5g the latter was developed by combining landsat 8 bands 6 shortwave infrared 1 and 7 shortwave infrared 2 ourhzif et al 2019 this layer provides information on the clay content in the surface which can be expected to control infiltration potential to some extent to complement the surface clay information a clay content layer g kg in the first two meters of the subsoil was also used fig 5h water infiltration into the underlying materials is not only controlled by the surface clay content the first few meters of the subsoil may allow less clay content or impede more clay content infiltration down to the water table therefore this layer provides additional information to that obtained with landsat images this conditioning factor obtained from soilgrids 250 m 2 0 was elaborated using state of the art machine learning methods by poggio and de sousa 2020 2 4 machine learning models ml based spatial predictions need three main elements for being conducted 1 a spatially distributed set of explanatory variables described in the previous section section 2 3 conditioning factors 2 a database of points where the outcome is known section 2 2 target variable and finally 3 a machine learning model or models capable of learning the patterns of explanatory variables that lead to a given outcome in reference to the last element ml models the mlmapper 2 0 code gómez escalonilla et al 2021 2022 has been used in this research to develop spatial predictions of borehole yield the code is an evolution of the original mlmapper tool developed by martinez santos and renard 2020 this new version developed in python 3 7 routinely implements multicollinearity checks random search hyperparameter optimization cross validation and recursive feature elimination fig 6 in addition it allows users to choose the target score metric to be optimized in the aforementioned processes finally it provides an agreement map of the predictions of the best performing algorithms to estimate uncertainty the complexity of the architecture and operation of machine learning algorithms is combined with the associations also complex that may exist between the explanatory variables leading to a given outcome all this leads to the unfeasibility to forecast at the starting point which of the machine learning models will perform better than the others on a particular dataset to tackle this challenge a series of runs is performed with all the algorithms included in mlmapper mlmapper incorporates 19 supervised classification algorithms from the scikit learn toolbox pedregosa et al 2011 subsequently the best performing algorithms based on user defined performance thresholds are selected and the rest are discarded the results and performance of the models were appraised on the basis of standard machine learning metrics namely the geometric mean the area under the receiver operating characteristic curve auc score and the balanced accuracy score ling and li 1998 sun et al 2006 pedregosa et al 2011 mlmapper works with different types of support vector machines and statistical learners such as linear discriminant analysis gaussian naïve bayes classification ridge classifier or logistic regression it also includes ensemble methods random forest classifier gradient boosting classifier ada boost classifier and extra trees classifier instance learners k neighbor classification neural networks multilayer perceptron neural network and a simple decision tree classifier among others following the methodology described above a series of tests were performed to identify the algorithms that consistently outperformed the others on this particular dataset this led to choosing extra trees etc logistic regression lrg and the gradient boosting classifier gbc as the most suitable more information on discarded algorithms can be found in previous studies gómez escalonilla et al 2021 2022 martínez santos and renard 2020 pedregosa et al 2011 etc uses an ensemble application of the simple decision tree algorithm it builds an ensemble of unpruned decision trees according to a classical top down process geurts et al 2006 etc uses the entire training sample to grow the trees as opposed to the widely used random forest which uses the bootstrapping technique furthermore during the generation of a tree etc splits the nodes by choosing the cut points completely randomly gbc is based on gradient boosting technique that initially uses a decision tree classifier as a weak model gbc builds an additive model in a forward stage wise fashion and allows the optimization of arbitrary differentiable loss functions friedman 2001 pedregosa et al 2011 the basic function of this method is to predict a new classification membership after each iteration in an additive way predictions are created from weak learners that continuously evolve on the errors of the previous learners misclassified samples receive higher weights in the next step which forces the classifier to focus on their performance in the following iterations georganos et al 2018 lrg is a widely used linear model the probabilities of each potential outcome are modeled by means of a logistic function the multi class setting is similar to the binary case except the label y is now an integer in 1 c where c is the number of classes 2 5 supervised classification procedure an imbalanced multiclass dataset approach before applying all the models explained above a multicollinearity analysis of the conditioning factors was performed multicollinearity arises when some of the conditioning factors used for predictions are highly correlated with each other the presence of multicollinearity among the conditioning factors can be a potential problem and affect the performance of the algorithms issues can arise from attributing extra weight to an input factor or from incorporating noise into the final results multicollinearity among all conditioning factors was identified using the variance inflation factor and tolerance indices as commonly used to estimate multicollinearity of predictors in geospatial modeling bui et al 2016 the 70 of the water point database was used to train the models and the remaining 30 was employed to validate the predictive ability of the classifiers machine learning studies are sometimes performed on ground truth datasets where one of the outcome classes is under represented in relation to the others lemaître et al 2017 this may lead to the problem of class imbalance prati et al 2009 which essentially means that the potential to predict the under represented class is limited in such circumstances performance metrics will present high accuracy in the majority classes and poor performance in the minority classes singh and purohit 2015 this will typically result in discarding some valuable information from the latter as mentioned earlier dataset imbalance was perceived as a potential issue from the outset hence a strategy based on comparing sklearn s imbalanced learn tools was developed sklearn provides two main different strategies to tackle imbalance lemaître et al 2017 namely undersampling and oversampling based on an imbalanced dataset in which x min and x maj are the subset of samples belonging to the minority and majority class respectively undersampling refers to the process of reducing the number of samples of the x maj on the contrary oversampling performs the data balancing by generating new samples in x min a series of tests were performed to identify the strategy to address the unbalanced dataset problem that performed best these strategies include synthetic minority oversampling technique smote adaptive synthetic sampling approach adasyn random oversampling ros and random undersampling rus importantly these oversampling or undersampling techniques were applied only on the training dataset points otherwise the use of these resampling strategies on the entire data set could lead to overoptimism or data leakage problems lemaître et al 2017 santos et al 2018 chawla et al 2002 developed smote as an oversampling approach in which the minority class is over sampled by creating synthetic instances rather than oversampling with replacement in this technique synthetic examples are generated by operating on the feature space the minority class is oversampled by taking each sample of the minority class and introducing synthetic examples along the line segments linking any all nearest neighbors of the minority class adasyn is an extension of smote adasyn can decide the number of synthetic instances that need to be produced for the minority class he et al 2008 note that this technique not only provides a balanced data distribution but also forces the learning algorithm to focus on complex samples in the dataset finally ros increases the number of minority class points in the training set by randomly replicating existing minority class instances while rus decreases the number of majority class data points by randomly eliminating majority class data points currently in the training set chawla et al 2002 liu 2004 reflects that one of the main problems with rus is that it is not possible to control what information about the majority class is removed so very important information about the decision boundary between the minority and majority class may be eliminated 3 results 3 1 multicollinearity analysis table 2 presents the results of the multicollinearity analysis variance inflation factor vif and tolerance tol values were calculated for all conditioning factors vif values range from 1 to 6 2 while tol fluctuates between 0 16 and 0 93 this suggests that there are no multicollinearity issues among the explanatory variables dormann et al 2013 3 2 borehole yield threshold selection fig 7a and fig 7b shows the results of the borehole yield threshold analysis mlmapper was run once for each unit threshold from 1 m3 h to 10 m3 h for the original dataset without resampling and for each of the different resampling strategies smote adasyn ros and rus the optimal thresholds in terms of the metrics of choice were 1 m3 h and 5 m3 h when both the original dataset without resampling and smote adasyn and ros resampling techniques were used the most discriminating thresholds when the rus technique was used were 1 m3 h and 6 m3 h the borehole database was subsequently split in three classes class 0 refers to negative boreholes while the 5 m3 h threshold was used to separate class 1 less than 5 m3 h and class 2 over 5 m3 h the 5 m3 h threshold was picked over the 1 m3 h for three main reasons 1 the data would not allow for a multiclass representation if the 1 m3 h were picked 2 the dataset imbalance would be accentuated if the 1 m3 h threshold were used with 13 7 and 78 9 corresponding to class 1 and class 2 respectively instead of 66 9 and 25 7 and 3 the 5 m3 h threshold conveys more valuable information for prospective uses with greater water needs table 3 shows the results of the three best performing algorithms for each scenario in addition the means of each of the four metrics were calculated to analyze the most accurate resampling or no resampling technique the least effective resampling technique in terms of scores was rus with a mean for the balanced accuracy score bas test score ts geometric mean score gms and auc score of 0 760 0 636 0 736 and 0 813 respectively the no resampling scenario obtains the best results in terms of ts 0 743 but the scores on the different ml metrics focused on imbalanced datasets were worse than those obtained with two of the resampling techniques ros and adasyn the adasyn resampling technique obtained the best results in three of the four metrics with the exception of ts where it obtained the second best result the mean of the metrics obtained with the three best performing algorithms were 0 803 0 790 and 0 856 for bas mgb and ras respectively this prior analysis allows the selection of adasyn as the most efficient approach in this case therefore all ensuing analyses including the development of the borehole yield prediction maps refer to the results obtained with this technique 3 3 validation of the ml models table 4 shows the results of the three best performing algorithms with the adasyn resampling technique the results of the discarded models can be shown in the table s1 of the supporting information all models returned a balanced score greater than 0 80 a geometric mean score above 0 78 and an auc score exceeding 0 85 this implies that the predictions are sufficiently accurate for all classifiers and thus that the problems of data imbalance were successfully tackled the extra trees classifier obtained the best results for two of the imbalance metrics with 0 81 0 80 and 0 85 for the balanced score geometric mean score and auc score respectively logistic regression and gradient boosting obtained similar results for the three metrics used fig 8 shows the confusion matrix for all three algorithms f1 score for class 0 were 0 90 0 95 and 0 95 for etc gbc and lrg respectively this implies that all algorithms predict the class labelled as groundwater absence highly accurately class 1 borehole yield 5 m3 h rendered f1 scores of 0 79 0 81 and 0 72 for etc gbc and lrg respectively while the f1 score for class 2 borehole yield 5 m3 h were 0 61 0 58 and 0 59 for etc gbc and lrg respectively this all suggests that the algorithms generally found it more difficult to distinguish between class 1 and class 2 3 4 conditioning factors selection and importance the recursive feature elimination procedure identified a different optimal number of explanatory variables depending on the algorithm of choice table 5 shows the conditioning factors used by each of the three classifiers gradient boosting and logistic regression relied on several conditioning factors with 18 and 19 respectively while extra trees used the fewest 12 conditioning factors clay content geology geomorphology land use rainfall slope curvature drainage density saturated thickness elevation and basement depth were used by all three algorithms other explanatory variables that were used by at least two classifiers were alteration band ratio water table depth distance from channels soil spi twi ndvi and ndwi fig 9 shows the results of the feature importance calculated for the best performing tree based algorithms etc and gbc both agreed in attributing importance to elevation 0 30 and 0 25 respectively etc also attributed a high degree of importance to saturated thickness and land cover this algorithm selected slope basement depth geology drainage density and clay content as the next most important variables in turn gbc relied on basement depth saturated thickness land cover clay content slope geology and rain among the most important factors both algorithms agreed in assigning the least importance to slope curvature partial dependence plots pdp for some of the most important conditioning factors used by extra trees were generated in order to analyze the relationship between the values of the explanatory variables and the target variable fig 10 clay content pdp shows that higher values of clay content were associated with larger partial dependencies for class 0 and to a lesser extent for class 1 class 2 exhibits mainly higher partial dependence at lower values of clay content however it shows a slight increase in partial dependence at higher values of clay content slope factor pdp shows that lower values of slope were associated with higher class 2 partial dependencies and larger values of slope were linked to higher class 0 partial dependencies class 1 exhibits a high variability for this variable elevation pdp shows that greater values of elevation were associated with class 0 in contrast class 1 and class 2 present greater values of partial dependence for lower altitudes this is consistent with the fact that most negative boreholes in the database are located on the heights of the mandingue plateau while the productive ones tend to be located on the plain the pdp of the basement depth shows that the lowest values of this conditioning factor are mainly associated with the highest class 0 partial dependencies the partial dependence of class 1 presents two peaks with low medium and high values of the basement depth factor class 2 on the other hand presents a high partial dependence on the intermediate values the saturated thickness pdp also renders intuitive results when appraised from a hydrogeological perspective lower saturated thickness values are associated with a higher partial dependence on class 0 on the other hand although there is no linear relation the trend presents higher values of this conditioning factor associated with higher partial dependencies for class 1 and class 2 the high partial dependence of the lower values of saturated thickness could be associated with the areas near the mandingue plateau where there are no high saturated thicknesses but percolation from the mountains could increase the groundwater potential 3 5 borehole yield potential mapping fig 11 shows the predictive borehole yield maps obtained with the three algorithms green areas are those where the algorithms have found a combination of explanatory variables leading to class 2 borehole yield 5 m3 h and yellow zones are those in which the algorithms have found a combination of conditioning factors that lead to class 1 borehole yield 5 m3 h red pixels represent those areas where the algorithms found a combination of explanatory variables leading to class 0 i e absence of groundwater the three maps resemble each other especially in two regions the first one is the western area of the mandingue plateau where the algorithms generally agreed in predicting class 0 the second major area of agreement is the south sector of the niger riverbanks all algorithms predicted class 2 borehole yield 5 m3 h for this sector the shores of river niger along the bamako sector were also predicted as class 2 but to different extents depending on the classifier the remainder of the study area shows different degrees of discrepancy depending on the algorithms of choice an ensemble map was specifically developed to analyze these discrepancies colors in fig 12 represent the arithmetic mean of all three algorithms for each pixel thus the ensemble in fig 12 is best described as an agreement map where all three algorithms have equal weight the darker shades of green represent the points that were labelled as highly productive class 2 by all algorithms and red areas represent those that were labelled as no groundwater class 0 by all algorithms intermediate colors represent different levels of agreement among the three algorithms the results are largely consistent with those observed in fig 11 in addition there were at least two algorithms that predicted the highest yield class in some areas both south and north of the mandingue plateau this was reflected in the less intense green colors of the agreement map a total of 9 6 57 7 and 2 4 of the study area were predicted as class 0 class 1 and class 2 respectively in addition 4 0 of the total surface was predicted to be between class 0 and class 1 the remaining 26 3 was predicted as between class 1 and class 2 4 discussion 4 1 model selection and importance of conditioning factors the literature shows that different classifiers rely on different subsets of explanatory variables and that no subset of conditioning factors can be readily extrapolated from one hydrogeological context to another martínez santos and renard 2020 this is partially due to the internal logic of each one of the classifiers and partially due to the fact that the factors governing groundwater infiltration flow and discharge may differ from case to case considering that the internal complexity of the algorithms and the size of the databases make it impossible to predict which algorithm will perform best on a given dataset a first conclusion that springs to mind is that using a large number of algorithms and then picking the best performing ones is a suitable course of action in machine learning studies dealing with groundwater potential gómez escalonilla et al 2021 from a site specific perspective the feature importance analysis for the tree based algorithms showed that elevation is the variable that correlates best with borehole yield fig 9 this is consistent with the analysis of the partial dependence plots fig 10 as well as with the spatial analysis of the predictive yield maps the fact that three classifiers used the clay content geology geomorphology land use rainfall slope curvature drainage density saturated thickness elevation and basement depth layers suggests that these conditioning factors are all strongly correlated with borehole yield basement depth saturated thickness geology and to some extent elevation could be expected to be solid predictors of borehole yield however the fact that some of the most important variables are non hydrogeological deserves additional comment this is the case of land use which seems to correlate well with borehole yield the relationship between the explanatory variable and the target variable seems to be one of consequence rather than of cause in some cases for example the prevalence of irrigated croplands along river niger obeys to the fact that the underlying aquifer is productive not all the way around much like there are no croplands throughout the mandingue plateau because there is no groundwater to irrigate them despite this the land use has been used as an explanatory variable in many studies that suggest that the different land use characteristics will modify the groundwater potential or groundwater resources i e more cause than consequence ifediegwu 2022 lerner and harris 2009 pham et al 2019 selvam et al 2016 among even the most favorable land uses there are large differences related to the magnitude of groundwater recharge scanlon et al 2007 shows that recharge is two orders of magnitude greater through crops than through the original native forests it is very important to understand that the outcomes of machine learning studies require careful interpretation to avoid spurious conclusions important information can be inferred from the pdp analysis take for instance saturated thickness which was also used to some extent by all three algorithms the saturated thickness pdp shows that the lowest values of saturated thickness mandingue plateau relate closely to class 0 which was seen as a likely outcome from the outset however the pdps for classes 1 and 2 require a finer analysis intuitively one would expect a greater saturated thickness to be better correlated with class 2 higher borehole yields but this is not the case class 2 relates to an intermediate saturated thickness which is more commonly found along the alluvial sediments of river niger whereas the peak in the class 1 pdp occurs where the highest values of saturated thickness take place i e weathering mantles a plausible explanation for all this is that borehole yield in this case could be more related to hydrodynamic parameters such as hydraulic conductivity and porosity than to saturated thickness but further data is needed to confirm this hypothesis other pdps also convey valuable information for instance the clay content pdps suggest that a higher clay content leads to lower recharge of the underlying layers and consequently to less productive boreholes conversely zones with lower clay content have a higher partial dependence on the most productive class similarly the slope pdp shows that the steeper slopes favoring surface runoff are associated with the less productive classes while flat areas and gentle slopes correlate spatially with the more productive boreholes another important methodological issue is that of imbalanced classes imbalanced multiclass problems present added difficulties when compared to the prediction of bivariate outcomes based on balanced datasets this is because increasing the number of classes to be predicted naturally reduces the number of examples for the algorithms to learn from furthermore the more limited the number of ground truth points is the more difficult it will be to fine tune the predictive ability of the classifiers the aforementioned study by kumar et al 2021 used a data augmentation technique consisting of assigning the same yield class to each pixel 30 30 m within a given radius from the actual borehole other mapping studies based on machine learning techniques have applied different strategies to balance the number of samples in each class in a study about soil mapping taghizadeh mehrjardi et al 2020 found that predictive maps obtained from imbalanced datasets tend to render results that are biased towards the most represented class these authors evaluated the use of eight different resampling strategies including random under and over sampling synthetic minority oversampling and adaptive synthetic sampling among others outcomes show that classifiers trained on the smote achieved the best overall performance followed by adasyn this study as well as the one conducted in our paper reveals that the best resampling technique will vary depending on the initial dataset in other words the use of several strategies and the subsequent selection of the one that works best is revealed as a suitable approach when working with imbalanced multiclass datasets 4 2 borehole yield potential maps and limitations fig 12 presents the ensemble borehole yield map the results have been discussed to some extent in the previous section where the feature importance and partial dependence plots of some of the conditioning factors have been shown aside from the niger riverbanks especially in the southern and northern sectors which are clearly associated with the most productive class the western sector presented a higher potential borehole yield than the eastern sector as mentioned in section 2 1 groundwater percolation from the highly fractured mandingue plateau into the plains where the weathered mantle is thickest explains the predictions observed in the western sector of the agreement map furthermore high yield predictions along the niger riverbanks were associated with the fact that the river carries high flows throughout the year this in turn implies a continuous supply and a hydraulic connection with the alluvial sediments the remaining area corresponding essentially to the weathered mantles was predicted as the medium potential class class 1 which is consistent with the results outlined in table 6 martínez santos et al 2021 argue that the results obtained by machine learning methods should be verified beyond standard machine learning indicators whenever possible in this case the results of the agreement map were cross checked with variables such as the number of boreholes the average yield of the boreholes the percentage of wells exceeding 10 m3 h and the average success rate of boreholes located in the different yield domains table 6 shows a comparison between the agreement map and actual yield data boreholes located in areas predicted as class 1 n 260 present an average yield of 2 83 m3 h this accounts for 20 6 of the high yield boreholes 10 m3 h and villages in these sectors present an average borehole success rate of 80 7 water points located in areas labelled with intermediate values between class 1 and 2 n 152 present an average yield of 5 85 m3 h account for 57 7 of the high yield boreholes and present an average drilling success rate of 90 2 finally boreholes located within class 2 areas had an average yield of 8 50 m3 h comprised 21 6 of high yielding boreholes despite accounting for only 7 5 of the villages and rendered an average borehole success rate of 99 6 this all suggests that despite the difficulties encountered by the algorithms in terms of telling apart classes 1 and 2 the map in fig 12 represents a sufficiently accurate depiction of field conditions while this all seems coherent with field observations it is important to note that there are certain limitations associated with our ground truth dataset a recurrent problem with regional borehole databases in west africa pertains to the widespread use of hand pumps for rural water supply indeed many boreholes in the region have traditionally been fitted with hand pumps regardless their productivity thus determining whether yields in the order of 1 m3 h represent the productivity or the aquifer or that of the pump is impossible without extensive fieldwork furthermore the national borehole database contains the information at a village scale resolution this implies that all boreholes within a given village have the same coordinates which can be problematic if the village overlies a heterogeneous hydrogeological environment while this is perceived as a minor shortcoming in view of the scale of work a more detailed knowledge of the borehole coordinates would likely render a finer depiction of field conditions finally it must be acknowledged that borehole productivity is not only a function of aquifer productivity but also of borehole construction this means that deficient borehole implementation and or insufficient drilling depth may result in poor production in areas where the groundwater potential is high enhancing the ground truth database with pumping test data would be a welcome addition for future studies 5 conclusions groundwater is a strategic resource in arid and semiarid regions machine learning techniques present enormous upside in terms of interpreting large amounts of spatially distributed groundwater data which may be beneficial to improve water access in low income countries while groundwater potential mapping is becoming widespread in the academic literature predicting specific variables such as borehole yield is perceived as a major challenge ahead our paper represents one of the first attempts to deal with this particular issue a key finding is that machine learning based maps may actually depict field conditions better than standard machine learning metrics and therefore that combining standard metrics with ad hoc indicators is a suitable course of action in this type of study from a site specific perspective the mandingue plateau was delineated as a low borehole yield area while the banks of the niger river have been found to be the most productive zones in the region piedmonts located north and south of the mountains represent intermediate productivity these finding are all consistent with the available borehole yield data and could be used to inform water supply policy at the regional scale this research has also explored and demonstrated the possibility of dealing with imbalanced datasets in groundwater potential mapping studies the adasyn strategy proved to be the most appropriate in preventing the difficulties associated with class imbalance in our ground truth sample furthermore this research shows that the use of a large number of classifiers different forms of preprocessing and in the case of unbalanced databases different strategies such as oversampling and undersampling provide adequate courses of action in machine learning applications to the field of groundwater from a methodological viewpoint the updated version of mlmapper presents the added capability of dealing with multi class targets which attests to its suitability to map groundwater related variables beyond a mere positive negative outcome credit authorship contribution statement víctor gómez escalonilla conceptualization formal analysis methodology software and writing original draft oumou diancoumba validation and writing review editing dasso yollande traoré validation and writing review editing esperanza montero validation and writing review editing miguel martín loeches conceptualization validation and writing review editing pedro martínez santos conceptualization methodology software validation writing review editing and funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work has been funded under research grant pid2021 124018ob i00 of spain s ministry of science and innovation this work is also related to the research grant rti2018 099394 b i00 of spain s ministry of science innovation and universities the first author received an fpi grant from the ministry of science and innovation to develop his phd within this project pre2019 090026 the last author received a salvador de madariaga grant prx18 00235 from spain s ministry of education culture and sport to carry out a 3 month research stay at the université de neuchâtel switzerland where the original version of the software used in this paper was developed the authors thank the direction generale de l hydraulique of mali for making its borehole database available this work has also been funded and is part of the research grant 101059372 within the framework of horizon europe 2021 of the european commission appendix a supporting information supplementary data associated with this article can be found in the online version at doi 10 1016 j ejrh 2022 101245 appendix a supplementary material supplementary material the following kml files contain the google maps of the most important areas described in this article map kml file containing the google map map kml file containing the google map 
7822,study region regions of bamako kati and kangaba southwestern mali study focus machine learning based mapping of borehole yield three algorithms were trained on an imbalanced multiclass database of boreholes while twenty variables were used as predictors for borehole yield all models returned balanced and geometric scores in the order of 0 80 with area under the receiver operating characteristic curve up to 0 87 three main methodological conclusions are drawn a the evaluation of different machine learning classifiers and various resampling strategies and the subsequent selection of the best performing ones is shown to be a good strategy in this type of studies b ad hoc calibration tools such as data on borehole success rates provide an apt complement to standard machine learning metrics and c a multiclass approach with an unbalanced database represents a greater challenge than predicting a bivariate outcome but potentially results in a finer depiction of field conditions new hydrological insights for the region alluvial sediments were found to be the most productive areas while the mandingue plateau has the lowest groundwater potential the piedmont areas showcase an intermediate groundwater prospect elevation basement depth slope and geology rank among the most important variables lower values of clay content slopes and elevations and higher values of basement depth and saturated thickness were linked to the most productive class graphical abstract ga1 keywords machine learning groundwater exploration yield prediction gis mali data availability the authors do not have permission to share data 1 introduction groundwater plays a vital role in drinking water supplies food security and ecosystem services an estimated 2 5 billion people worldwide rely exclusively on groundwater to meet their daily needs while hundreds of millions of farmers depend on groundwater to sustain their livelihoods unesco 2015 adelana and macdonald 2008 explain that the increasing prominence of groundwater as a drinking water source in africa is motivated by three main reasons namely the large natural storage capacity of aquifers the often good water quality and that groundwater based infrastructures are generally more affordable for disadvantaged communities while critical for the survival of human beings and ecosystems groundwater also suffers from the hidden treasure syndrome because groundwater is out of sight aquifers are seldom well known even in industrialized countries this leads to widespread contamination problems of unregistered use and dropping water tables worldwide which all hamper attempts to manage the resource sustainably improving hydrogeological knowledge is thus essential particularly given the current context of climate change in which groundwater resources are expected to be increasingly relied upon groundwater potential mapping gpm can be used to underpin groundwater resources planning and exploration elbeih 2015 díaz alcaide and martínez santos 2019 carried out a review of gpm approaches analyzing the definitions of gpm the variables that are typically involved in gpm studies and the data integration procedures available in the literature these authors found that most of the existing studies rely on expert decision approaches like analytical hierarchy processes muavhi et al 2021 weight of evidence approaches ahmed et al 2021 fuzzy analytical hierarchy processes boughariou et al 2021 multi criteria decision making forootan and seyedi 2021 frequency ratio trabelsi et al 2018 and shannon s entropy methods khoshtinat et al 2019 gómez escalonilla et al 2022 show that these expert based techniques often require the reclassification of explanatory variables in intervals which is likely to originate bias from the outset because the intervals are based almost exclusively on the criteria of the operator the advent of machine learning approaches ml opens up a whole new methodological dimension to gpm as ml approaches can overcome this disadvantage to a large extent the recent literature showcases several examples of ml algorithms in groundwater potential mapping studies these include support vector machines al fugara et al 2022 panahi et al 2020 decision trees al abadi et al 2021 arabameri et al 2021 braham et al 2022 artificial neural networks chen et al 2021 nguyen et al 2020 hakim et al 2022 and ensemble methods like boosting random forests and extra trees classification among others bai et al 2022 choudhary et al 2022 gómez escalonilla et al 2021 2022 martinsen et al 2022 sachdeva and kumar 2021 ml based predictions of groundwater potential are typically carried out within a geographic information database the underlying logic is that the presence of groundwater that is the target variable can be inferred from a series of explanatory variables the latter typically include landforms rainfall lineaments lithology slope weathering and drainage density among others díaz alcaide and martínez santos 2019 the ground truth sample that is used to train the algorithms consists of an existing borehole database with information on positive and negative boreholes and in some cases characteristics such as flow rate or water table depth among others if the borehole database presents a sufficiently large and diverse number of records machine learning classifiers can find those patterns of explanatory variables that lead to a positive or a negative borehole the validated algorithm can then be extrapolated spatially to obtain a groundwater potential score for every pixel in the geographic database most ml gpm studies focus on predicting a positive or negative outcome that is in identifying zones of high or low groundwater prospect in this context our paper presents two major innovations the first one contributes to clarify the manifold concept of groundwater potential díaz alcaide and martínez santos 2019 by providing groundwater potential predictions in terms of borehole yield the second one consists of going one step beyond conventional predictive capabilities by replacing bivariate outcomes with multiclass outcomes to the authors knowledge there is only one methodological precedent in this regard in which kumar et al 2021 evaluated machine learning and fuzzy ahp methods for mapping groundwater potential in data scarce areas this study used a data augmentation technique to increase the database available to train the algorithms because our database is roughly twice as large augmentation is not needed and thus the present work represents a unique example in the literature based on an entirely independent set of field records we evaluated the performance of four different resampling methods synthetic minority oversampling technique smote adaptive synthetic sampling approach adasyn random oversampling ros and random undersampling rus resampling was performed after the training testing split and was only applied to the training dataset which contributes to avoid the problems of data leakage and overfitting from a geographical standpoint ml gpm studies have been carried out mainly in asia while there are very few in the african continent braham et al 2022 gómez escalonilla et al 2021 2022 martínez santos and renard 2020 namous et al 2021 moreover ours is one of the first studies in the literature addressing groundwater potential based on a multiclass approach a crucial component in ensuing analyses is the need to approach the problem of predicting yield based on an unbalanced ground truth dataset where there is a much larger number of positive than negative points since in many cases drilling companies do not report negative drillings to the competent authorities in this context two main objectives have been established 1 to develop a multiclass predictive borehole yield map of the study region and 2 to evaluate different machine learning models and resampling techniques to address the imbalanced dataset approach in order to establish a strategy for potential future studies of the same nature 2 materials and methods 2 1 study area the study area spans 21 000 km2 including the administrative region of bamako and the municipalities of kati and kangaba of the koulikoro region southern mali fig 1 the maximum altitude in the study area is 800 m a s l in the mandingue plateau while the minimum elevation is 300 m a s l in the northernmost end of the niger river the region is characterized by a tropical savanna climate traore et al 2018 with average temperatures of 27 c and an average rainfall around 1000 mm year the dry season lasts from october to may and accounts for just 25 of the total rainfall while the west african monsoon months provide the rest diancoumba et al 2020 estimated aquifer recharge rates to be around 3 26 of annual precipitation from a geological standpoint the area consists of four geological units fig 2 namely precambrian materials including sandstones intrusive igneous rocks recent alluvial sediments and weathering mantles developed over the precambrian materials the alluvial sediments take place mostly along river niger and its tributaries the map of the main geological units was obtained from landsat 8 bands and was constructed with qgis 3 s semiautomatic classification plugin congedo 2021 training points were obtained from the 1 200 000 geological map of mali dngm and lgsjm 1988 precambrian rocks exhibit fissured porosity and fractures are often interconnected this provides relatively high permeability on a regional scale and favors precipitation infiltration and groundwater flow to lower elevations traore 1985 shows that the transmissivity of these units ranges around 130 m² day this study also indicates that the thickness of fractured horizons varies between 30 and 50 m although deeper fractures can increase the thickness of the aquifer up to 80 100 m intrusive igneous rocks more specifically dolerites may behave either as a barrier or as preferential pathways for groundwater flow due to fracturing associated with the intrusive process under certain conditions these units may constitute aquifers of local importance however the average transmissivity of the system fluctuates around 4 m² day traore 1985 weathered mantles extend across most of the study region pnud 1982 these materials present an average transmissivity of 7 m2 day with a maximum of 350 m2 day and a minimum of 0 1 m2 day depending on the lithology traore et al 2018 these authors indicate that the thickness of the productive zone varies from 10 to 50 m and the depth of the static water levels ranges between 8 and 20 m below the surface these aquifers are drained by the niger river system recent alluvial sediments of the niger river show relatively good hydrogeology properties with an estimated transmissivity that varies from 0 34 to 21 6 m2 d alpha et al 1991 thickness is highly variable but generally ranges between 2 and 20 m mihe 1990 static levels typically vary between 2 and 8 m below the surface 2 2 target variable the database provided by the national hydraulic direction of mali dnh 2010 contains 483 points with borehole yield data these are widely distributed in space fig 3 the dataset is however imbalanced in terms of yield levels 36 points 7 4 were classified as negative 323 points 66 9 present borehole yields of between 0 5 and 5 m3 h and 124 points 25 4 exceed 5 m3 h in the experience of the authors this is common in regional borehole databases gómez escalonilla et al 2021 and is likely attributable to the fact that negative boreholes are reported to the authorities less frequently than positive boreholes because data imbalances cause difficulties to the predictive potential of the algorithms a series of trial runs were carried out to determine the optimal threshold between borehole yield classes these will be described later on joint interpretation of figs 1 and 3 suggests that most of the high yield boreholes are located along the alluvial materials of river niger whereas the mandingue plateau accounts for most of the negative ones the remainder of the study area presents a combination of low and high yield boreholes 2 3 conditioning factors díaz alcaide and martínez santos 2019 show several variables that are commonly used in gpm studies these include lithology geology geomorphology soil land use land cover topography lineaments drainage and slope related variables rainfall and groundwater recharge in turn kumar 1997 and jyrkama et al 2002 contend that crucial elements of groundwater potential such as aquifer recharge are influenced by several factors namely climatic conditions soil types and characteristics land cover geomorphological factors and hydrological features besides the available data allows for the development of other spatially distributed layers of information including the expected saturated thickness of the basement depth thus in this case we took into consideration twenty explanatory variables or conditioning factors as partial predictors of borehole yield table 1 the borehole database contains information about borehole depth as well as static level measurements boreholes throughout the study area are frequently drilled to the impervious basement consequently borehole depth may be interpreted as a proxy for the thickness of the overlying materials water table depth fig 4 a and expected saturated thickness layer fig 4 b may also be inferred from this information soil types of the study area were obtained from the european soil data centre dewitte et al 2013 fig 4c petric plinthosols take up almost 50 of the study area followed the haplic lixisols and lithic leptosols which cover about 24 and 18 respectively petric plinthosols are defined as a type of soil arranged in continuous or fractured sheets rich in fe or formed by concretions or nodules also rich in fe connected and strongly cemented haplic lixisols exhibit a higher clay content in the deeper part than in the shallower one due to pedogenetic processes lithic leptosols are very fine soils developed on continuous rock with an extremely rich content of coarse fragments the parent rock appears less than 10 cm from the soil surface iuss working group wrb 2015 land cover information stems from the european space agency climate change initiative esa 2010 close and open shrublands account for more than 52 of the study area while mosaic vegetation represents about 36 fig 4d a combination of rainfed croplands and broadleaved evergreen or semi evergreen forest add up to the remaining 9 the digital elevation model dem was obtained from the radar based shuttle radar topography mission available at https earthexplorer usgs gov the dem was used to produce several layers including slope fig 4e and slope curvature fig 4f while landforms fig 5a were derived with qgis 3 0 s geomorphon plugin jasiewicz and stepinski 2013 the surface water channels extracted from the dem analysis were used to develop the drainage density fig 5b and distance from channels layers topographic wetness index twi and stream power index spi maps were also derived from the dem twi fig 5c represents the facility for water to accumulate on the surface beven and kirkby 1979 in turn spi fig 5d provides a proxy of the erosive potential of runoff water moore et al 1991 satellite imagery can provide valuable information despite not penetrating deep into the ground in those areas associated with shallow groundwater levels díaz alcaide and martínez santos 2019 in this research landsat 8 bands were used to elaborate four different layers these include a surface lithological map constructed by means semi automatic classification plugin congedo 2021 fig 2 a normalized difference vegetation index ndvi layer fig 5e which is an estimation of the vigor of vegetation and is calculated from vegetation response to red and visible infrared wavelengths xie et al 2008 the normalized difference water index ndwi a measure of the quantity of water in the plants or soil humidity xu 2006 fig 5f and the alteration band ratio or surface clay content fig 5g the latter was developed by combining landsat 8 bands 6 shortwave infrared 1 and 7 shortwave infrared 2 ourhzif et al 2019 this layer provides information on the clay content in the surface which can be expected to control infiltration potential to some extent to complement the surface clay information a clay content layer g kg in the first two meters of the subsoil was also used fig 5h water infiltration into the underlying materials is not only controlled by the surface clay content the first few meters of the subsoil may allow less clay content or impede more clay content infiltration down to the water table therefore this layer provides additional information to that obtained with landsat images this conditioning factor obtained from soilgrids 250 m 2 0 was elaborated using state of the art machine learning methods by poggio and de sousa 2020 2 4 machine learning models ml based spatial predictions need three main elements for being conducted 1 a spatially distributed set of explanatory variables described in the previous section section 2 3 conditioning factors 2 a database of points where the outcome is known section 2 2 target variable and finally 3 a machine learning model or models capable of learning the patterns of explanatory variables that lead to a given outcome in reference to the last element ml models the mlmapper 2 0 code gómez escalonilla et al 2021 2022 has been used in this research to develop spatial predictions of borehole yield the code is an evolution of the original mlmapper tool developed by martinez santos and renard 2020 this new version developed in python 3 7 routinely implements multicollinearity checks random search hyperparameter optimization cross validation and recursive feature elimination fig 6 in addition it allows users to choose the target score metric to be optimized in the aforementioned processes finally it provides an agreement map of the predictions of the best performing algorithms to estimate uncertainty the complexity of the architecture and operation of machine learning algorithms is combined with the associations also complex that may exist between the explanatory variables leading to a given outcome all this leads to the unfeasibility to forecast at the starting point which of the machine learning models will perform better than the others on a particular dataset to tackle this challenge a series of runs is performed with all the algorithms included in mlmapper mlmapper incorporates 19 supervised classification algorithms from the scikit learn toolbox pedregosa et al 2011 subsequently the best performing algorithms based on user defined performance thresholds are selected and the rest are discarded the results and performance of the models were appraised on the basis of standard machine learning metrics namely the geometric mean the area under the receiver operating characteristic curve auc score and the balanced accuracy score ling and li 1998 sun et al 2006 pedregosa et al 2011 mlmapper works with different types of support vector machines and statistical learners such as linear discriminant analysis gaussian naïve bayes classification ridge classifier or logistic regression it also includes ensemble methods random forest classifier gradient boosting classifier ada boost classifier and extra trees classifier instance learners k neighbor classification neural networks multilayer perceptron neural network and a simple decision tree classifier among others following the methodology described above a series of tests were performed to identify the algorithms that consistently outperformed the others on this particular dataset this led to choosing extra trees etc logistic regression lrg and the gradient boosting classifier gbc as the most suitable more information on discarded algorithms can be found in previous studies gómez escalonilla et al 2021 2022 martínez santos and renard 2020 pedregosa et al 2011 etc uses an ensemble application of the simple decision tree algorithm it builds an ensemble of unpruned decision trees according to a classical top down process geurts et al 2006 etc uses the entire training sample to grow the trees as opposed to the widely used random forest which uses the bootstrapping technique furthermore during the generation of a tree etc splits the nodes by choosing the cut points completely randomly gbc is based on gradient boosting technique that initially uses a decision tree classifier as a weak model gbc builds an additive model in a forward stage wise fashion and allows the optimization of arbitrary differentiable loss functions friedman 2001 pedregosa et al 2011 the basic function of this method is to predict a new classification membership after each iteration in an additive way predictions are created from weak learners that continuously evolve on the errors of the previous learners misclassified samples receive higher weights in the next step which forces the classifier to focus on their performance in the following iterations georganos et al 2018 lrg is a widely used linear model the probabilities of each potential outcome are modeled by means of a logistic function the multi class setting is similar to the binary case except the label y is now an integer in 1 c where c is the number of classes 2 5 supervised classification procedure an imbalanced multiclass dataset approach before applying all the models explained above a multicollinearity analysis of the conditioning factors was performed multicollinearity arises when some of the conditioning factors used for predictions are highly correlated with each other the presence of multicollinearity among the conditioning factors can be a potential problem and affect the performance of the algorithms issues can arise from attributing extra weight to an input factor or from incorporating noise into the final results multicollinearity among all conditioning factors was identified using the variance inflation factor and tolerance indices as commonly used to estimate multicollinearity of predictors in geospatial modeling bui et al 2016 the 70 of the water point database was used to train the models and the remaining 30 was employed to validate the predictive ability of the classifiers machine learning studies are sometimes performed on ground truth datasets where one of the outcome classes is under represented in relation to the others lemaître et al 2017 this may lead to the problem of class imbalance prati et al 2009 which essentially means that the potential to predict the under represented class is limited in such circumstances performance metrics will present high accuracy in the majority classes and poor performance in the minority classes singh and purohit 2015 this will typically result in discarding some valuable information from the latter as mentioned earlier dataset imbalance was perceived as a potential issue from the outset hence a strategy based on comparing sklearn s imbalanced learn tools was developed sklearn provides two main different strategies to tackle imbalance lemaître et al 2017 namely undersampling and oversampling based on an imbalanced dataset in which x min and x maj are the subset of samples belonging to the minority and majority class respectively undersampling refers to the process of reducing the number of samples of the x maj on the contrary oversampling performs the data balancing by generating new samples in x min a series of tests were performed to identify the strategy to address the unbalanced dataset problem that performed best these strategies include synthetic minority oversampling technique smote adaptive synthetic sampling approach adasyn random oversampling ros and random undersampling rus importantly these oversampling or undersampling techniques were applied only on the training dataset points otherwise the use of these resampling strategies on the entire data set could lead to overoptimism or data leakage problems lemaître et al 2017 santos et al 2018 chawla et al 2002 developed smote as an oversampling approach in which the minority class is over sampled by creating synthetic instances rather than oversampling with replacement in this technique synthetic examples are generated by operating on the feature space the minority class is oversampled by taking each sample of the minority class and introducing synthetic examples along the line segments linking any all nearest neighbors of the minority class adasyn is an extension of smote adasyn can decide the number of synthetic instances that need to be produced for the minority class he et al 2008 note that this technique not only provides a balanced data distribution but also forces the learning algorithm to focus on complex samples in the dataset finally ros increases the number of minority class points in the training set by randomly replicating existing minority class instances while rus decreases the number of majority class data points by randomly eliminating majority class data points currently in the training set chawla et al 2002 liu 2004 reflects that one of the main problems with rus is that it is not possible to control what information about the majority class is removed so very important information about the decision boundary between the minority and majority class may be eliminated 3 results 3 1 multicollinearity analysis table 2 presents the results of the multicollinearity analysis variance inflation factor vif and tolerance tol values were calculated for all conditioning factors vif values range from 1 to 6 2 while tol fluctuates between 0 16 and 0 93 this suggests that there are no multicollinearity issues among the explanatory variables dormann et al 2013 3 2 borehole yield threshold selection fig 7a and fig 7b shows the results of the borehole yield threshold analysis mlmapper was run once for each unit threshold from 1 m3 h to 10 m3 h for the original dataset without resampling and for each of the different resampling strategies smote adasyn ros and rus the optimal thresholds in terms of the metrics of choice were 1 m3 h and 5 m3 h when both the original dataset without resampling and smote adasyn and ros resampling techniques were used the most discriminating thresholds when the rus technique was used were 1 m3 h and 6 m3 h the borehole database was subsequently split in three classes class 0 refers to negative boreholes while the 5 m3 h threshold was used to separate class 1 less than 5 m3 h and class 2 over 5 m3 h the 5 m3 h threshold was picked over the 1 m3 h for three main reasons 1 the data would not allow for a multiclass representation if the 1 m3 h were picked 2 the dataset imbalance would be accentuated if the 1 m3 h threshold were used with 13 7 and 78 9 corresponding to class 1 and class 2 respectively instead of 66 9 and 25 7 and 3 the 5 m3 h threshold conveys more valuable information for prospective uses with greater water needs table 3 shows the results of the three best performing algorithms for each scenario in addition the means of each of the four metrics were calculated to analyze the most accurate resampling or no resampling technique the least effective resampling technique in terms of scores was rus with a mean for the balanced accuracy score bas test score ts geometric mean score gms and auc score of 0 760 0 636 0 736 and 0 813 respectively the no resampling scenario obtains the best results in terms of ts 0 743 but the scores on the different ml metrics focused on imbalanced datasets were worse than those obtained with two of the resampling techniques ros and adasyn the adasyn resampling technique obtained the best results in three of the four metrics with the exception of ts where it obtained the second best result the mean of the metrics obtained with the three best performing algorithms were 0 803 0 790 and 0 856 for bas mgb and ras respectively this prior analysis allows the selection of adasyn as the most efficient approach in this case therefore all ensuing analyses including the development of the borehole yield prediction maps refer to the results obtained with this technique 3 3 validation of the ml models table 4 shows the results of the three best performing algorithms with the adasyn resampling technique the results of the discarded models can be shown in the table s1 of the supporting information all models returned a balanced score greater than 0 80 a geometric mean score above 0 78 and an auc score exceeding 0 85 this implies that the predictions are sufficiently accurate for all classifiers and thus that the problems of data imbalance were successfully tackled the extra trees classifier obtained the best results for two of the imbalance metrics with 0 81 0 80 and 0 85 for the balanced score geometric mean score and auc score respectively logistic regression and gradient boosting obtained similar results for the three metrics used fig 8 shows the confusion matrix for all three algorithms f1 score for class 0 were 0 90 0 95 and 0 95 for etc gbc and lrg respectively this implies that all algorithms predict the class labelled as groundwater absence highly accurately class 1 borehole yield 5 m3 h rendered f1 scores of 0 79 0 81 and 0 72 for etc gbc and lrg respectively while the f1 score for class 2 borehole yield 5 m3 h were 0 61 0 58 and 0 59 for etc gbc and lrg respectively this all suggests that the algorithms generally found it more difficult to distinguish between class 1 and class 2 3 4 conditioning factors selection and importance the recursive feature elimination procedure identified a different optimal number of explanatory variables depending on the algorithm of choice table 5 shows the conditioning factors used by each of the three classifiers gradient boosting and logistic regression relied on several conditioning factors with 18 and 19 respectively while extra trees used the fewest 12 conditioning factors clay content geology geomorphology land use rainfall slope curvature drainage density saturated thickness elevation and basement depth were used by all three algorithms other explanatory variables that were used by at least two classifiers were alteration band ratio water table depth distance from channels soil spi twi ndvi and ndwi fig 9 shows the results of the feature importance calculated for the best performing tree based algorithms etc and gbc both agreed in attributing importance to elevation 0 30 and 0 25 respectively etc also attributed a high degree of importance to saturated thickness and land cover this algorithm selected slope basement depth geology drainage density and clay content as the next most important variables in turn gbc relied on basement depth saturated thickness land cover clay content slope geology and rain among the most important factors both algorithms agreed in assigning the least importance to slope curvature partial dependence plots pdp for some of the most important conditioning factors used by extra trees were generated in order to analyze the relationship between the values of the explanatory variables and the target variable fig 10 clay content pdp shows that higher values of clay content were associated with larger partial dependencies for class 0 and to a lesser extent for class 1 class 2 exhibits mainly higher partial dependence at lower values of clay content however it shows a slight increase in partial dependence at higher values of clay content slope factor pdp shows that lower values of slope were associated with higher class 2 partial dependencies and larger values of slope were linked to higher class 0 partial dependencies class 1 exhibits a high variability for this variable elevation pdp shows that greater values of elevation were associated with class 0 in contrast class 1 and class 2 present greater values of partial dependence for lower altitudes this is consistent with the fact that most negative boreholes in the database are located on the heights of the mandingue plateau while the productive ones tend to be located on the plain the pdp of the basement depth shows that the lowest values of this conditioning factor are mainly associated with the highest class 0 partial dependencies the partial dependence of class 1 presents two peaks with low medium and high values of the basement depth factor class 2 on the other hand presents a high partial dependence on the intermediate values the saturated thickness pdp also renders intuitive results when appraised from a hydrogeological perspective lower saturated thickness values are associated with a higher partial dependence on class 0 on the other hand although there is no linear relation the trend presents higher values of this conditioning factor associated with higher partial dependencies for class 1 and class 2 the high partial dependence of the lower values of saturated thickness could be associated with the areas near the mandingue plateau where there are no high saturated thicknesses but percolation from the mountains could increase the groundwater potential 3 5 borehole yield potential mapping fig 11 shows the predictive borehole yield maps obtained with the three algorithms green areas are those where the algorithms have found a combination of explanatory variables leading to class 2 borehole yield 5 m3 h and yellow zones are those in which the algorithms have found a combination of conditioning factors that lead to class 1 borehole yield 5 m3 h red pixels represent those areas where the algorithms found a combination of explanatory variables leading to class 0 i e absence of groundwater the three maps resemble each other especially in two regions the first one is the western area of the mandingue plateau where the algorithms generally agreed in predicting class 0 the second major area of agreement is the south sector of the niger riverbanks all algorithms predicted class 2 borehole yield 5 m3 h for this sector the shores of river niger along the bamako sector were also predicted as class 2 but to different extents depending on the classifier the remainder of the study area shows different degrees of discrepancy depending on the algorithms of choice an ensemble map was specifically developed to analyze these discrepancies colors in fig 12 represent the arithmetic mean of all three algorithms for each pixel thus the ensemble in fig 12 is best described as an agreement map where all three algorithms have equal weight the darker shades of green represent the points that were labelled as highly productive class 2 by all algorithms and red areas represent those that were labelled as no groundwater class 0 by all algorithms intermediate colors represent different levels of agreement among the three algorithms the results are largely consistent with those observed in fig 11 in addition there were at least two algorithms that predicted the highest yield class in some areas both south and north of the mandingue plateau this was reflected in the less intense green colors of the agreement map a total of 9 6 57 7 and 2 4 of the study area were predicted as class 0 class 1 and class 2 respectively in addition 4 0 of the total surface was predicted to be between class 0 and class 1 the remaining 26 3 was predicted as between class 1 and class 2 4 discussion 4 1 model selection and importance of conditioning factors the literature shows that different classifiers rely on different subsets of explanatory variables and that no subset of conditioning factors can be readily extrapolated from one hydrogeological context to another martínez santos and renard 2020 this is partially due to the internal logic of each one of the classifiers and partially due to the fact that the factors governing groundwater infiltration flow and discharge may differ from case to case considering that the internal complexity of the algorithms and the size of the databases make it impossible to predict which algorithm will perform best on a given dataset a first conclusion that springs to mind is that using a large number of algorithms and then picking the best performing ones is a suitable course of action in machine learning studies dealing with groundwater potential gómez escalonilla et al 2021 from a site specific perspective the feature importance analysis for the tree based algorithms showed that elevation is the variable that correlates best with borehole yield fig 9 this is consistent with the analysis of the partial dependence plots fig 10 as well as with the spatial analysis of the predictive yield maps the fact that three classifiers used the clay content geology geomorphology land use rainfall slope curvature drainage density saturated thickness elevation and basement depth layers suggests that these conditioning factors are all strongly correlated with borehole yield basement depth saturated thickness geology and to some extent elevation could be expected to be solid predictors of borehole yield however the fact that some of the most important variables are non hydrogeological deserves additional comment this is the case of land use which seems to correlate well with borehole yield the relationship between the explanatory variable and the target variable seems to be one of consequence rather than of cause in some cases for example the prevalence of irrigated croplands along river niger obeys to the fact that the underlying aquifer is productive not all the way around much like there are no croplands throughout the mandingue plateau because there is no groundwater to irrigate them despite this the land use has been used as an explanatory variable in many studies that suggest that the different land use characteristics will modify the groundwater potential or groundwater resources i e more cause than consequence ifediegwu 2022 lerner and harris 2009 pham et al 2019 selvam et al 2016 among even the most favorable land uses there are large differences related to the magnitude of groundwater recharge scanlon et al 2007 shows that recharge is two orders of magnitude greater through crops than through the original native forests it is very important to understand that the outcomes of machine learning studies require careful interpretation to avoid spurious conclusions important information can be inferred from the pdp analysis take for instance saturated thickness which was also used to some extent by all three algorithms the saturated thickness pdp shows that the lowest values of saturated thickness mandingue plateau relate closely to class 0 which was seen as a likely outcome from the outset however the pdps for classes 1 and 2 require a finer analysis intuitively one would expect a greater saturated thickness to be better correlated with class 2 higher borehole yields but this is not the case class 2 relates to an intermediate saturated thickness which is more commonly found along the alluvial sediments of river niger whereas the peak in the class 1 pdp occurs where the highest values of saturated thickness take place i e weathering mantles a plausible explanation for all this is that borehole yield in this case could be more related to hydrodynamic parameters such as hydraulic conductivity and porosity than to saturated thickness but further data is needed to confirm this hypothesis other pdps also convey valuable information for instance the clay content pdps suggest that a higher clay content leads to lower recharge of the underlying layers and consequently to less productive boreholes conversely zones with lower clay content have a higher partial dependence on the most productive class similarly the slope pdp shows that the steeper slopes favoring surface runoff are associated with the less productive classes while flat areas and gentle slopes correlate spatially with the more productive boreholes another important methodological issue is that of imbalanced classes imbalanced multiclass problems present added difficulties when compared to the prediction of bivariate outcomes based on balanced datasets this is because increasing the number of classes to be predicted naturally reduces the number of examples for the algorithms to learn from furthermore the more limited the number of ground truth points is the more difficult it will be to fine tune the predictive ability of the classifiers the aforementioned study by kumar et al 2021 used a data augmentation technique consisting of assigning the same yield class to each pixel 30 30 m within a given radius from the actual borehole other mapping studies based on machine learning techniques have applied different strategies to balance the number of samples in each class in a study about soil mapping taghizadeh mehrjardi et al 2020 found that predictive maps obtained from imbalanced datasets tend to render results that are biased towards the most represented class these authors evaluated the use of eight different resampling strategies including random under and over sampling synthetic minority oversampling and adaptive synthetic sampling among others outcomes show that classifiers trained on the smote achieved the best overall performance followed by adasyn this study as well as the one conducted in our paper reveals that the best resampling technique will vary depending on the initial dataset in other words the use of several strategies and the subsequent selection of the one that works best is revealed as a suitable approach when working with imbalanced multiclass datasets 4 2 borehole yield potential maps and limitations fig 12 presents the ensemble borehole yield map the results have been discussed to some extent in the previous section where the feature importance and partial dependence plots of some of the conditioning factors have been shown aside from the niger riverbanks especially in the southern and northern sectors which are clearly associated with the most productive class the western sector presented a higher potential borehole yield than the eastern sector as mentioned in section 2 1 groundwater percolation from the highly fractured mandingue plateau into the plains where the weathered mantle is thickest explains the predictions observed in the western sector of the agreement map furthermore high yield predictions along the niger riverbanks were associated with the fact that the river carries high flows throughout the year this in turn implies a continuous supply and a hydraulic connection with the alluvial sediments the remaining area corresponding essentially to the weathered mantles was predicted as the medium potential class class 1 which is consistent with the results outlined in table 6 martínez santos et al 2021 argue that the results obtained by machine learning methods should be verified beyond standard machine learning indicators whenever possible in this case the results of the agreement map were cross checked with variables such as the number of boreholes the average yield of the boreholes the percentage of wells exceeding 10 m3 h and the average success rate of boreholes located in the different yield domains table 6 shows a comparison between the agreement map and actual yield data boreholes located in areas predicted as class 1 n 260 present an average yield of 2 83 m3 h this accounts for 20 6 of the high yield boreholes 10 m3 h and villages in these sectors present an average borehole success rate of 80 7 water points located in areas labelled with intermediate values between class 1 and 2 n 152 present an average yield of 5 85 m3 h account for 57 7 of the high yield boreholes and present an average drilling success rate of 90 2 finally boreholes located within class 2 areas had an average yield of 8 50 m3 h comprised 21 6 of high yielding boreholes despite accounting for only 7 5 of the villages and rendered an average borehole success rate of 99 6 this all suggests that despite the difficulties encountered by the algorithms in terms of telling apart classes 1 and 2 the map in fig 12 represents a sufficiently accurate depiction of field conditions while this all seems coherent with field observations it is important to note that there are certain limitations associated with our ground truth dataset a recurrent problem with regional borehole databases in west africa pertains to the widespread use of hand pumps for rural water supply indeed many boreholes in the region have traditionally been fitted with hand pumps regardless their productivity thus determining whether yields in the order of 1 m3 h represent the productivity or the aquifer or that of the pump is impossible without extensive fieldwork furthermore the national borehole database contains the information at a village scale resolution this implies that all boreholes within a given village have the same coordinates which can be problematic if the village overlies a heterogeneous hydrogeological environment while this is perceived as a minor shortcoming in view of the scale of work a more detailed knowledge of the borehole coordinates would likely render a finer depiction of field conditions finally it must be acknowledged that borehole productivity is not only a function of aquifer productivity but also of borehole construction this means that deficient borehole implementation and or insufficient drilling depth may result in poor production in areas where the groundwater potential is high enhancing the ground truth database with pumping test data would be a welcome addition for future studies 5 conclusions groundwater is a strategic resource in arid and semiarid regions machine learning techniques present enormous upside in terms of interpreting large amounts of spatially distributed groundwater data which may be beneficial to improve water access in low income countries while groundwater potential mapping is becoming widespread in the academic literature predicting specific variables such as borehole yield is perceived as a major challenge ahead our paper represents one of the first attempts to deal with this particular issue a key finding is that machine learning based maps may actually depict field conditions better than standard machine learning metrics and therefore that combining standard metrics with ad hoc indicators is a suitable course of action in this type of study from a site specific perspective the mandingue plateau was delineated as a low borehole yield area while the banks of the niger river have been found to be the most productive zones in the region piedmonts located north and south of the mountains represent intermediate productivity these finding are all consistent with the available borehole yield data and could be used to inform water supply policy at the regional scale this research has also explored and demonstrated the possibility of dealing with imbalanced datasets in groundwater potential mapping studies the adasyn strategy proved to be the most appropriate in preventing the difficulties associated with class imbalance in our ground truth sample furthermore this research shows that the use of a large number of classifiers different forms of preprocessing and in the case of unbalanced databases different strategies such as oversampling and undersampling provide adequate courses of action in machine learning applications to the field of groundwater from a methodological viewpoint the updated version of mlmapper presents the added capability of dealing with multi class targets which attests to its suitability to map groundwater related variables beyond a mere positive negative outcome credit authorship contribution statement víctor gómez escalonilla conceptualization formal analysis methodology software and writing original draft oumou diancoumba validation and writing review editing dasso yollande traoré validation and writing review editing esperanza montero validation and writing review editing miguel martín loeches conceptualization validation and writing review editing pedro martínez santos conceptualization methodology software validation writing review editing and funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work has been funded under research grant pid2021 124018ob i00 of spain s ministry of science and innovation this work is also related to the research grant rti2018 099394 b i00 of spain s ministry of science innovation and universities the first author received an fpi grant from the ministry of science and innovation to develop his phd within this project pre2019 090026 the last author received a salvador de madariaga grant prx18 00235 from spain s ministry of education culture and sport to carry out a 3 month research stay at the université de neuchâtel switzerland where the original version of the software used in this paper was developed the authors thank the direction generale de l hydraulique of mali for making its borehole database available this work has also been funded and is part of the research grant 101059372 within the framework of horizon europe 2021 of the european commission appendix a supporting information supplementary data associated with this article can be found in the online version at doi 10 1016 j ejrh 2022 101245 appendix a supplementary material supplementary material the following kml files contain the google maps of the most important areas described in this article map kml file containing the google map map kml file containing the google map 
7823,study region dongqing reservoir located in guizhou china study focus zoning the rwtf reservoir water temperature field is of great significance and is an effective way to analyze rwtf s nature in practice the conditions of rwtf fluctuate greatly as time goes on which leads to the existing rwtf zoning methods can t give a steady zoning result in consequence this paper creates a kind of zoning method to study the properties of rwtf in dongqing reservoir which has two main steps firstly numerical simulation is used to obtain the whole data of rwtf and then the k means clustering algorithm is executed based on the numerical simulation results new hydrological insights for the region this paper proved that the zoning method developed in this paper which combined numerical simulation and unsupervised machine learning can be effectively applied and divided the rwtf into four zones without using experimental parameters in dongqing reservoir moreover on the base of the four zones spatial borders the influencing factors of water temperature in each zone of dongqing reservoir were able to be found which would be of value in further research of rwtf graphical abstract ga1 keywords reservoir water temperature field unsupervised machine learning k means clustering algorithm numerical simulation zoning method data availability the authors do not have permission to share data 1 introduction artificial reservoirs formed by building dams on rivers bring multifaceted benefits to human society however these reservoirs have some potential adverse impacts on the domestic ecological system morgan 1985 poole and berman 2001 particularly the variations of rwtf reservoir water temperature field take a long duration and lead to varieties of negative impacts on a large scale which has been a primary concern gu et al 1998 caissie 2006 ducharne 2008 liu et al 2018 tao et al 2020b 2020a the research of the rwtf generally focuses on the vertical stratification phenomenon of water temperature which is the typical three layer structure epilimnion thermocline and hysterocline yan et al 2014 song et al 2011 zhang and peng 2009 in addition a transition zone also exists and its nature is similar to a natural river gao et al 2021 which extends a certain distance from the end of the reservoir toward the dam consequently at the same time there are one or more of the four types of zones in a reservoir mixed zone epilimnion zone thermocline zone and hysteresis zone the rwtf is mainly divided by a kind of physical parameter the cwtg critical water temperature gradient wang and pu 1982 zheng et al 2017 he et al 2019 through this method the zone will be regarded as the thermocline if its vertical water temperature gradient is higher than or equal to the cwtg nevertheless the cwtg is given in different studies at varying values 0 05 0 2 c m wang and pu 1982 he et al 2019 but the water temperature field characteristics of different reservoirs differ significantly and there isn t a clear approach to determine the value of cwtg at a specific reservoir for instance water temperature stratification occurred from april to september in longyangxia reservoir according to the simulation of rwtf in 2014 and the thermocline had a water temperature gradient of 0 049 0 160 c m gao et al 2021 the greatest water temperature gradient was roughly 0 72 c m in 2015 according to simulation and measurement data from the xiluodu reservoir and the thickness of the thermocline varied from about 20 m in april to may to about 10 m at the end of june long et al 2018 therefore developing a reliable precise and objective zoning approach is crucial to studying rwtf machine learning a kind of data processing technology has progressed rapidly in recent decades because of its advantages in terms of huge data analysis and autonomous learning capabilities based on this trend the application of machine learning models for a deeper analysis of massive data that is calculated by numerical simulations has a good effect as well callaham et al 2021 the field of rwtf also has a great development in the combination of numerical simulation and machine learning in the jinping i hydropower plant for instance numerical simulations were used to obtain detailed reservoir water temperature hydrodynamic data based on this the lstm model can work out excellent predictions of the outflow temperature in various stratified water intake operating conditions zhang et al 2022 the k means clustering algorithm was first developed by hartigan in 1975 and presented a full algorithm and implementation route in 1979 which is an essential part of unsupervised machine learning hartigan and wong 1979 this algorithm is efficient and widely used in unlabeled datasets for clustering tasks jain 2010 pandovedivya et al 2018 mahdi et al 2021 therefore it is valuable to explore whether the zoning of rwtf can be accomplished by studying the inherent properties and rules of the data generated by numerical simulation using the k means clustering algorithm the followings are the paper s research aims 1 to come up with a zoning method of rwtf that integrates numerical simulation with unsupervised machine learning 2 to confirm the k means clustering algorithm s applicability to the task of rwtf zoning through the clustering evaluation index 3 to observe the accurate spatial range of each zone as well as the temperature and velocity properties of each zone in dongqing reservoir 4 to conduct correlation analysis between the water temperature and its influencing factors in each zone and then rank the importance of them in each zone 2 method 2 1 procedure of rwtf zoning method the rwtf zoning method put forward in this work was using the k means clustering algorithm based on the dataset generated by the hydrodynamic module the procedure of this method was given in fig 1 step a we gathered the hydrological meteorological and topographical data of the reservoir the spatial coordinates of each discrete point were extracted after using the pre processing approach of numerical simulation to get a series of discrete points from the reservoir space then a hydrodynamic module was established and executed a simulation for a year if the simulation results were reasonable the value of water temperature and flow velocity would be extracted separately else the parameters of the hydrodynamic module would be calibrated and validated again until the simulation results became reasonable step b in step b the spatial coordinates water temperature and flow velocity were combined as the initial dataset the initial dataset should be transformed into the standard dataset after invalid data was removed to increase the training accuracy and effectiveness of the k means clustering algorithm in the rwtf zoning task the k means clustering algorithm was required to analyze the standard dataset and divide it into four clusters and the specific steps were as follows 1 four samples were selected randomly as initial clustering centers a a1 a2 a3 a4 2 for each sample xi in the standard dataset the distances between it and the initial clustering centers were compared then it would be regarded as the same cluster as one of the initial clustering centers with the smallest distance 3 for each cluster aj j 1 2 3 4 its cluster center was recalculated as follows a j 1 c i x c i x 4 repeating steps 2 and 3 until an abort condition number of iterations or minimum error was satisfied after these steps the zoning of the rwtf would be finished 2 2 evaluation of k means clustering algorithm s effect it was significant to evaluate the clustering outcomes of different clustering numbers to confirm whether four was a fair selection in different clustering numbers although the four clusters were necessary for the rwtf zoning task meanwhile as the initial clustering centers were chosen at random the k means clustering algorithm s computation procedure would have some randomness which might affect the final clustering results if the clustering results showed considerable fluctuations when the computation was repeated at the same condition it meant that the dataset was not appropriate for applying the k means clustering algorithm consequently three metrics calinski harabasz ch silhouette coefficient sc and davies bouldin db were used to assess the performance of the k means clustering algorithm in this paper ch is defined as the ratio of the within cluster covariance to the between cluster covariance the higher the value of ch the better the clustering result the formula is as follows 1 c h t r b k t r w k m k k 1 the value of sc is between 1 1 the closer it is to 1 the higher the matching relationship between the sample and its cluster and the lower the matching relationship with other clusters the formula is as follows 2 s c b i a i max a i b i db is defined as the ratio of within cluster distances to between cluster distances therefore the lower the value of ch the better the clustering result the formula is as follows 3 d b 1 n i 1 n max j i σ i σ j d c i c j in eq 1 m is the number of samples in the dataset k is the number of clusters b k is the covariance matrix between clusters w k is the covariance matrix of the dataset within each cluster and tr is the trace of the matrix in eq 2 a i represents the mean of the degree of dissimilarity of sample i to other samples within the same cluster b i represents the minimum of the average dissimilarity of sample i to other clusters in eq 3 n is the number of clusters σ i is the distance of each point to the center in the i i 1 2 3 4 cluster c i is the centroid of cluster i and represents the distance between the centers of the two clusters 2 3 study case the embodiment of this paper was the dongqing reservoir located in guizhou province china the meteorological inflow and outflow conditions of the dongqing reservoir are shown in fig 2 a the variation of vertical water temperature structure with time is shown in fig 2 b and the geomorphological map of the dongqing reservoir is shown in fig 2 c the dongqing reservoir lies in southwest china which is in the subtropical monsoon climate zone it is hot and rainy here in summer and cold as well as dry in winter the average annual runoff is 12 272 billion m³ here the normal storage level of the dongqing reservoir is 490 m and the dead water level is 483 m based on the huanghai sea elevation coordinate system the total storage capacity is 955 million m³ the regulating storage capacity is 143 8 million m³ the corresponding storage capacity is 882 million m³ at the normal storage level and the dead storage capacity is 739 million m³ the dongqing reservoir has an obvious water temperature stratification phenomenon in spring and summer the surface water temperature is significantly higher than the bottom water temperature and the maximum temperature difference can reach 10 2 c 3 generation of the dataset 3 1 hydrodynamic module for dongqing reservoir the study area of dongqing reservoir was about 26 km along the river which was discretized and shown in fig 3 the number of grids was 330 vertical 15 horizontal 16 vertical the longitudinal mesh size was 14 84 m the transverse mesh size was 58 80 m and the vertical mesh size was 0 8 28 0 m the figures of water temperature distribution at three time points were selected to display the outcomes of the hydrodynamic module which were shown in fig 4 as it could be seen that there were primarily two different water temperature stratification states and the difference was mainly in the epilimnion zone the first was the typical stratified state fig 4 b where the water temperature at the same elevation varied slightly another was the segmentation of surface water temperature fig 4 a c which showed that there were noticeable fluctuations in surface water temperature along the river flow the evolutionary process was generally from a to b to c at the beginning of january the air temperature inflow water temperature and water temperature of the whole reservoir were at a low level so the rwtf was uniformly distributed from the end of february to the beginning of march the air temperature started to increase and the water temperature of the inflow from the upper reaches also began to rise but the inflow was small and the action time was short so it could only affect the mixed zone in the upper section of the reservoir with shallow water and the phenomenon was shown in fig 4 a in early summer inflow inflow water temperature and air temperature kept rising so the water temperature of the epilimnion zone became hotter under the continuous influence of upstream inflow but the water of the hysteresis zone remained cold as a result the thermocline zone with a large vertical temperature gradient connected the upper and lower parts of the reservoir when the typical three layer distribution state as shown in fig 4 b appeared during autumn and winter air temperature and water temperature of inflow decreased therefore water temperature in the mixing zone was influenced and began cold simultaneously due to the large storage capacity of the epilimnion zone it would remain hot at this time and the phenomenon as shown in fig 4 c appeared according to the results of numerical simulation the evolution mechanism of the rwtf was well represented which meant the generated data was qualified 3 2 dataset generation to make the k means clustering algorithm comprehensively learn the spatial temporal characteristics of the rwtf and automatically divide it the dataset should contain the geometry water temperature hydrodynamic and meteorological conditions of the reservoir the geometric parameters were represented by the x y and z coordinate values of each discrete point water temperature parameters were represented by the water temperature of all discrete points at noon each day within a year for hydrodynamic parameters the velocity at noon every day of each discrete point within a year was used the vector characteristics of the velocity were not considered and only the magnitude of the velocity value was used in this study from the simulation results the required data were extracted cleaned and organized into a dataset to execute the k means clustering algorithm the dataset s structure was shown in table1 which included 36 975 rows and 733 columns containing a total of 27 102 675 pieces of data 4 result 4 1 evaluation of zoning results first to evaluate the clustering outcomes of different clustering numbers the integer of 2 9 were chosen as clustering numbers and the values of the three parameters ch sc and db were independently calculated for different clustering results then the case of dividing the dataset into four clusters would be repeated nine times to assess the stability of the k means clustering algorithm in which case the algorithm would use different clustering centers randomly the parameters of ch sc and db were calculated for each of the nine clustering results as well as the percentages of the number of points in each cluster would be counted to check if the initial center selection would affect the final clustering results the evaluation results were shown in fig 5 1 the ch and sc for different clustering numbers showed that dividing into three clusters was optimal and dividing into four clusters also had good parameter values meanwhile the db showed that dividing into four clusters was optimal 2 when the k means clustering algorithm was repeatedly used to divide the dataset into four clusters the three parameters did not fluctuate and the percentage of each cluster was stable these results proved that dividing into four clusters was a fair selection and the k means clustering algorithm was suitable for the rwtf zoning task 4 2 zoning result display in order to make us better understand the result visualized expression was necessary to be done after the rwtf zoning result was worked out through the k means clustering algorithm the zoning results were shown in fig 6 and the four zones were a hysteresis zone b epilimnion zone c thermocline zone and d mixed zone it was observed that the hysteresis zone a the epilimnion zone b and the thermocline zone c were distributed with typical vertical stratification specifically the hysteresis zone a was located from the bottom of the reservoir to elevation 430 m the epilimnion zone b was located from elevation 470 m to the reservoir surface and the thermocline zone c was located from the elevation 430 470 m the mixed zone d was located upstream which flowed about 12 km along the river and was mixed with epilimnion zone b and thermocline zone c to some extent once the spatial border of each zone was obtained it could make much deeper research about the rwtf for instance the properties of temperature and velocity in the reservoir the water temperature and velocity of all discrete points in one of the zones at different times could be extracted which could be used to get the average water temperature and velocity of a zone varying with time combined with theoretical analysis as shown in e and f of fig 6 the reservoir s water was cold at the start of the year then the mixed zone d was first to be affected by the increase of inflow water temperature in early spring and the water in other zones remained cold as time went by in late spring water with a high temperature in the mixed zone d gradually migrated to the epilimnion zone b under the action of buoyancy as a result the water temperatures in the epilimnion zone b and the mixed zone d tended to be consistent which led to the phenomenon of water temperature stratification in the reservoir due to the slow flow in the epilimnion zone b and the strong solar radiation in summer the water temperature in the epilimnion zone b began to rise above that in the mixed zone d the change of water temperature in the hysteresis zone a depended on the heat transfer in the thermocline zone c but this action was weak because of the low vertical velocity so the hysteresis zone a had a distinct lag in the temperature change beyond that the flow velocity was significantly faster in the mixed zone d than in the other zones because of its shallower water and smaller reservoir volume 4 3 analysis based on correlation on the base of the four zones spatial borders the most important influencing factors of water temperature in each zone of dongqing reservoir were able to be found which would be of value in further research of rwtf hence the pearson correlation coefficients between water temperature and its influencing factors were calculated for each zone and the calculation results were shown in fig 7 the correlation between water temperature and its influencing factors in the hysteretic zone a in descending order were inflow 0 71 inflow temperature 0 70 relative humidity 0 58 air temperature 0 35 cloud cover 0 23 wind velocity 0 82 it could be seen that although the water temperature in the hysteretic zone a remained relatively stable throughout the year the variation of water temperature in the hysteretic zone a was still closely related to meteorological and hydrological factors such as inflow and inflow temperature the wind velocity was significantly negatively correlated with the water temperature in this zone which meant that the higher the wind velocity the colder the water temperature of the zone the correlation between water temperature and its influencing factors in the epilimnion zone b in descending order were inflow temperature 0 97 air temperature 0 84 inflow 0 75 relative humidity 0 45 cloud cover 0 56 wind velocity 0 67 the epilimnion zone b was more sensitive to meteorological factors and more likely to change with seasonal climate variation specifically inflow temperature had very strong influences on water temperature in this zone the correlation between water temperature and its influencing factors in the thermocline zone c in descending order were inflow temperature 0 94 inflow 0 75 air temperature 0 71 relative humidity 0 57 cloud cover 0 44 wind velocity 0 76 the thermocline zone c served as the zone bridging the epilimnion zone b and hysteretic zone a where the water temperature had high correlations with inflow temperature inflow and air temperature the correlation between water temperature and its influencing factors in the mixed zone d in descending order were inflow temperature 0 99 air temperature 0 88 inflow 0 74 relative humidity 0 43 cloud cover 0 58 and wind velocity 0 64 the mixed zone d was in the tail of the reservoir and its physical property was similar to the natural river therefore inflow temperature and air temperature were the main factors affecting the water temperature in this zone and the water temperature had negative correlations with cloud cover and wind velocity 5 discussions 5 1 analysis of applicability in 4 result this paper has zoned the rwtf of dongqing reservoir based on the dataset of a whole year but it doesn t mean that the dataset is not able to be changed in contrast the dataset structure should be flexibly adjusted to different shapes to meet the diverse needs of specified engineering or academic problems hence this chapter will zone the rwtf of dongqing reservoir with the dataset of spring and summer to check the applicability of the method developed by this paper when the dataset structure is changed it can be seen in fig 8 that the distribution of the four zones in dongqing reservoir is different to some degree but the overall structure is similar which could make sense in theory in spring the inflow temperature begins to rise leading to the thermocline zone c coming into being and the water flowing from the mixed zone d to the epilimnion zone b is mostly on the surface of the reservoir under the action of buoyancy so that the top of the mixed zone d is longer than the bottom in summer the water temperature in the epilimnion zone b tends to be higher than that in the mixed zone d owing to the high air temperature thus the water of the mixed zone d starts to dive into the thermocline zone c with the action of buoyancy which contributes to the thermocline zone c thicker the zoning results are still reasonable and can reflect the different characteristics of different periods which shows that the method in this paper can be applied to different dataset structures 5 2 mechanism discussion considering the previous analysis appropriate spatial zoning results can be obtained through the k means clustering algorithm which is based on the results of numerical simulations by analyzing the patterns and characteristics of the data itself the k means clustering algorithm splits the rwtf spatially without relying on empirical parameters nevertheless it is currently impossible to explain machine learning s internal functioning the k means clustering algorithm s training criteria determine how far a point is from each zone s center then chooses the zone to which the point belongs based on the minimum distance the dataset used in this study has 733 dimensions but it is impossible to understand the meaning of the distances in a high dimensional space although the dataset s structure cannot be shown or plotted in a high dimensional environment the water temperature and the velocity can be observed by downscaling the dataset to three dimensions separately as shown in fig 9 they are extremely complex even when dropped to three dimensions respectively let alone the combination of them in higher dimensions therefore the mechanism of the machine for zoning the rwtf needs to be further investigated 6 conclusions this study implements a successful method for zoning the rwtf based on numerical simulation results using the k means clustering algorithm the main conclusions are as follows 1 these evaluation results prove that dividing into four clusters was a fair selection and the k means clustering algorithm is suitable for the rwtf zoning task 2 the mixed zone of the dongqing reservoir is approximately 12 km along the river the epilimnion zone lies roughly from an elevation of 470 m to the surface of the reservoir the thermocline zone lies roughly between an elevation of 430 m and 470 m and the hysteresis zone lies between the bottom of the reservoir and elevation 430 m 3 based on the zoning results it is found that inflow conditions and air temperature were the main meteorological factors affecting rwtf inflow conditions have the greatest effect on the mixed zone and air temperature has the greatest effect on the epilimnion zone the method presented in this research might be used as a useful zoning technique for investigating the mechanism of rwtf which is independent of the physical parameters and can integrate multiple time points to give a reliable zoning result in addition the mechanism of it needs further research it will be difficult but worthwhile credit authorship contribution statement wei liu methodology investigation formal analysis supervision writing original draft data curation funding acquisition peng zou conceptualization methodology software formal analysis writing original draft visualization dingguo jiang conceptualization investigation methodology formal analysis resources writing review editing xiufeng quan writing review editing software visualization huichao dai resources supervision conceptualization writing review editing declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests dingguo jiang has patent pending to state intellectual property office of china acknowledgements this work is supported by the national natural science foundation of china no u2040220 and special thanks are given to the anonymous reviewers and editors for their constructive comments appendix a supporting information supplementary data associated with this article can be found in the online version at doi 10 1016 j ejrh 2022 101239 appendix a supplementary material supplementary material 
7823,study region dongqing reservoir located in guizhou china study focus zoning the rwtf reservoir water temperature field is of great significance and is an effective way to analyze rwtf s nature in practice the conditions of rwtf fluctuate greatly as time goes on which leads to the existing rwtf zoning methods can t give a steady zoning result in consequence this paper creates a kind of zoning method to study the properties of rwtf in dongqing reservoir which has two main steps firstly numerical simulation is used to obtain the whole data of rwtf and then the k means clustering algorithm is executed based on the numerical simulation results new hydrological insights for the region this paper proved that the zoning method developed in this paper which combined numerical simulation and unsupervised machine learning can be effectively applied and divided the rwtf into four zones without using experimental parameters in dongqing reservoir moreover on the base of the four zones spatial borders the influencing factors of water temperature in each zone of dongqing reservoir were able to be found which would be of value in further research of rwtf graphical abstract ga1 keywords reservoir water temperature field unsupervised machine learning k means clustering algorithm numerical simulation zoning method data availability the authors do not have permission to share data 1 introduction artificial reservoirs formed by building dams on rivers bring multifaceted benefits to human society however these reservoirs have some potential adverse impacts on the domestic ecological system morgan 1985 poole and berman 2001 particularly the variations of rwtf reservoir water temperature field take a long duration and lead to varieties of negative impacts on a large scale which has been a primary concern gu et al 1998 caissie 2006 ducharne 2008 liu et al 2018 tao et al 2020b 2020a the research of the rwtf generally focuses on the vertical stratification phenomenon of water temperature which is the typical three layer structure epilimnion thermocline and hysterocline yan et al 2014 song et al 2011 zhang and peng 2009 in addition a transition zone also exists and its nature is similar to a natural river gao et al 2021 which extends a certain distance from the end of the reservoir toward the dam consequently at the same time there are one or more of the four types of zones in a reservoir mixed zone epilimnion zone thermocline zone and hysteresis zone the rwtf is mainly divided by a kind of physical parameter the cwtg critical water temperature gradient wang and pu 1982 zheng et al 2017 he et al 2019 through this method the zone will be regarded as the thermocline if its vertical water temperature gradient is higher than or equal to the cwtg nevertheless the cwtg is given in different studies at varying values 0 05 0 2 c m wang and pu 1982 he et al 2019 but the water temperature field characteristics of different reservoirs differ significantly and there isn t a clear approach to determine the value of cwtg at a specific reservoir for instance water temperature stratification occurred from april to september in longyangxia reservoir according to the simulation of rwtf in 2014 and the thermocline had a water temperature gradient of 0 049 0 160 c m gao et al 2021 the greatest water temperature gradient was roughly 0 72 c m in 2015 according to simulation and measurement data from the xiluodu reservoir and the thickness of the thermocline varied from about 20 m in april to may to about 10 m at the end of june long et al 2018 therefore developing a reliable precise and objective zoning approach is crucial to studying rwtf machine learning a kind of data processing technology has progressed rapidly in recent decades because of its advantages in terms of huge data analysis and autonomous learning capabilities based on this trend the application of machine learning models for a deeper analysis of massive data that is calculated by numerical simulations has a good effect as well callaham et al 2021 the field of rwtf also has a great development in the combination of numerical simulation and machine learning in the jinping i hydropower plant for instance numerical simulations were used to obtain detailed reservoir water temperature hydrodynamic data based on this the lstm model can work out excellent predictions of the outflow temperature in various stratified water intake operating conditions zhang et al 2022 the k means clustering algorithm was first developed by hartigan in 1975 and presented a full algorithm and implementation route in 1979 which is an essential part of unsupervised machine learning hartigan and wong 1979 this algorithm is efficient and widely used in unlabeled datasets for clustering tasks jain 2010 pandovedivya et al 2018 mahdi et al 2021 therefore it is valuable to explore whether the zoning of rwtf can be accomplished by studying the inherent properties and rules of the data generated by numerical simulation using the k means clustering algorithm the followings are the paper s research aims 1 to come up with a zoning method of rwtf that integrates numerical simulation with unsupervised machine learning 2 to confirm the k means clustering algorithm s applicability to the task of rwtf zoning through the clustering evaluation index 3 to observe the accurate spatial range of each zone as well as the temperature and velocity properties of each zone in dongqing reservoir 4 to conduct correlation analysis between the water temperature and its influencing factors in each zone and then rank the importance of them in each zone 2 method 2 1 procedure of rwtf zoning method the rwtf zoning method put forward in this work was using the k means clustering algorithm based on the dataset generated by the hydrodynamic module the procedure of this method was given in fig 1 step a we gathered the hydrological meteorological and topographical data of the reservoir the spatial coordinates of each discrete point were extracted after using the pre processing approach of numerical simulation to get a series of discrete points from the reservoir space then a hydrodynamic module was established and executed a simulation for a year if the simulation results were reasonable the value of water temperature and flow velocity would be extracted separately else the parameters of the hydrodynamic module would be calibrated and validated again until the simulation results became reasonable step b in step b the spatial coordinates water temperature and flow velocity were combined as the initial dataset the initial dataset should be transformed into the standard dataset after invalid data was removed to increase the training accuracy and effectiveness of the k means clustering algorithm in the rwtf zoning task the k means clustering algorithm was required to analyze the standard dataset and divide it into four clusters and the specific steps were as follows 1 four samples were selected randomly as initial clustering centers a a1 a2 a3 a4 2 for each sample xi in the standard dataset the distances between it and the initial clustering centers were compared then it would be regarded as the same cluster as one of the initial clustering centers with the smallest distance 3 for each cluster aj j 1 2 3 4 its cluster center was recalculated as follows a j 1 c i x c i x 4 repeating steps 2 and 3 until an abort condition number of iterations or minimum error was satisfied after these steps the zoning of the rwtf would be finished 2 2 evaluation of k means clustering algorithm s effect it was significant to evaluate the clustering outcomes of different clustering numbers to confirm whether four was a fair selection in different clustering numbers although the four clusters were necessary for the rwtf zoning task meanwhile as the initial clustering centers were chosen at random the k means clustering algorithm s computation procedure would have some randomness which might affect the final clustering results if the clustering results showed considerable fluctuations when the computation was repeated at the same condition it meant that the dataset was not appropriate for applying the k means clustering algorithm consequently three metrics calinski harabasz ch silhouette coefficient sc and davies bouldin db were used to assess the performance of the k means clustering algorithm in this paper ch is defined as the ratio of the within cluster covariance to the between cluster covariance the higher the value of ch the better the clustering result the formula is as follows 1 c h t r b k t r w k m k k 1 the value of sc is between 1 1 the closer it is to 1 the higher the matching relationship between the sample and its cluster and the lower the matching relationship with other clusters the formula is as follows 2 s c b i a i max a i b i db is defined as the ratio of within cluster distances to between cluster distances therefore the lower the value of ch the better the clustering result the formula is as follows 3 d b 1 n i 1 n max j i σ i σ j d c i c j in eq 1 m is the number of samples in the dataset k is the number of clusters b k is the covariance matrix between clusters w k is the covariance matrix of the dataset within each cluster and tr is the trace of the matrix in eq 2 a i represents the mean of the degree of dissimilarity of sample i to other samples within the same cluster b i represents the minimum of the average dissimilarity of sample i to other clusters in eq 3 n is the number of clusters σ i is the distance of each point to the center in the i i 1 2 3 4 cluster c i is the centroid of cluster i and represents the distance between the centers of the two clusters 2 3 study case the embodiment of this paper was the dongqing reservoir located in guizhou province china the meteorological inflow and outflow conditions of the dongqing reservoir are shown in fig 2 a the variation of vertical water temperature structure with time is shown in fig 2 b and the geomorphological map of the dongqing reservoir is shown in fig 2 c the dongqing reservoir lies in southwest china which is in the subtropical monsoon climate zone it is hot and rainy here in summer and cold as well as dry in winter the average annual runoff is 12 272 billion m³ here the normal storage level of the dongqing reservoir is 490 m and the dead water level is 483 m based on the huanghai sea elevation coordinate system the total storage capacity is 955 million m³ the regulating storage capacity is 143 8 million m³ the corresponding storage capacity is 882 million m³ at the normal storage level and the dead storage capacity is 739 million m³ the dongqing reservoir has an obvious water temperature stratification phenomenon in spring and summer the surface water temperature is significantly higher than the bottom water temperature and the maximum temperature difference can reach 10 2 c 3 generation of the dataset 3 1 hydrodynamic module for dongqing reservoir the study area of dongqing reservoir was about 26 km along the river which was discretized and shown in fig 3 the number of grids was 330 vertical 15 horizontal 16 vertical the longitudinal mesh size was 14 84 m the transverse mesh size was 58 80 m and the vertical mesh size was 0 8 28 0 m the figures of water temperature distribution at three time points were selected to display the outcomes of the hydrodynamic module which were shown in fig 4 as it could be seen that there were primarily two different water temperature stratification states and the difference was mainly in the epilimnion zone the first was the typical stratified state fig 4 b where the water temperature at the same elevation varied slightly another was the segmentation of surface water temperature fig 4 a c which showed that there were noticeable fluctuations in surface water temperature along the river flow the evolutionary process was generally from a to b to c at the beginning of january the air temperature inflow water temperature and water temperature of the whole reservoir were at a low level so the rwtf was uniformly distributed from the end of february to the beginning of march the air temperature started to increase and the water temperature of the inflow from the upper reaches also began to rise but the inflow was small and the action time was short so it could only affect the mixed zone in the upper section of the reservoir with shallow water and the phenomenon was shown in fig 4 a in early summer inflow inflow water temperature and air temperature kept rising so the water temperature of the epilimnion zone became hotter under the continuous influence of upstream inflow but the water of the hysteresis zone remained cold as a result the thermocline zone with a large vertical temperature gradient connected the upper and lower parts of the reservoir when the typical three layer distribution state as shown in fig 4 b appeared during autumn and winter air temperature and water temperature of inflow decreased therefore water temperature in the mixing zone was influenced and began cold simultaneously due to the large storage capacity of the epilimnion zone it would remain hot at this time and the phenomenon as shown in fig 4 c appeared according to the results of numerical simulation the evolution mechanism of the rwtf was well represented which meant the generated data was qualified 3 2 dataset generation to make the k means clustering algorithm comprehensively learn the spatial temporal characteristics of the rwtf and automatically divide it the dataset should contain the geometry water temperature hydrodynamic and meteorological conditions of the reservoir the geometric parameters were represented by the x y and z coordinate values of each discrete point water temperature parameters were represented by the water temperature of all discrete points at noon each day within a year for hydrodynamic parameters the velocity at noon every day of each discrete point within a year was used the vector characteristics of the velocity were not considered and only the magnitude of the velocity value was used in this study from the simulation results the required data were extracted cleaned and organized into a dataset to execute the k means clustering algorithm the dataset s structure was shown in table1 which included 36 975 rows and 733 columns containing a total of 27 102 675 pieces of data 4 result 4 1 evaluation of zoning results first to evaluate the clustering outcomes of different clustering numbers the integer of 2 9 were chosen as clustering numbers and the values of the three parameters ch sc and db were independently calculated for different clustering results then the case of dividing the dataset into four clusters would be repeated nine times to assess the stability of the k means clustering algorithm in which case the algorithm would use different clustering centers randomly the parameters of ch sc and db were calculated for each of the nine clustering results as well as the percentages of the number of points in each cluster would be counted to check if the initial center selection would affect the final clustering results the evaluation results were shown in fig 5 1 the ch and sc for different clustering numbers showed that dividing into three clusters was optimal and dividing into four clusters also had good parameter values meanwhile the db showed that dividing into four clusters was optimal 2 when the k means clustering algorithm was repeatedly used to divide the dataset into four clusters the three parameters did not fluctuate and the percentage of each cluster was stable these results proved that dividing into four clusters was a fair selection and the k means clustering algorithm was suitable for the rwtf zoning task 4 2 zoning result display in order to make us better understand the result visualized expression was necessary to be done after the rwtf zoning result was worked out through the k means clustering algorithm the zoning results were shown in fig 6 and the four zones were a hysteresis zone b epilimnion zone c thermocline zone and d mixed zone it was observed that the hysteresis zone a the epilimnion zone b and the thermocline zone c were distributed with typical vertical stratification specifically the hysteresis zone a was located from the bottom of the reservoir to elevation 430 m the epilimnion zone b was located from elevation 470 m to the reservoir surface and the thermocline zone c was located from the elevation 430 470 m the mixed zone d was located upstream which flowed about 12 km along the river and was mixed with epilimnion zone b and thermocline zone c to some extent once the spatial border of each zone was obtained it could make much deeper research about the rwtf for instance the properties of temperature and velocity in the reservoir the water temperature and velocity of all discrete points in one of the zones at different times could be extracted which could be used to get the average water temperature and velocity of a zone varying with time combined with theoretical analysis as shown in e and f of fig 6 the reservoir s water was cold at the start of the year then the mixed zone d was first to be affected by the increase of inflow water temperature in early spring and the water in other zones remained cold as time went by in late spring water with a high temperature in the mixed zone d gradually migrated to the epilimnion zone b under the action of buoyancy as a result the water temperatures in the epilimnion zone b and the mixed zone d tended to be consistent which led to the phenomenon of water temperature stratification in the reservoir due to the slow flow in the epilimnion zone b and the strong solar radiation in summer the water temperature in the epilimnion zone b began to rise above that in the mixed zone d the change of water temperature in the hysteresis zone a depended on the heat transfer in the thermocline zone c but this action was weak because of the low vertical velocity so the hysteresis zone a had a distinct lag in the temperature change beyond that the flow velocity was significantly faster in the mixed zone d than in the other zones because of its shallower water and smaller reservoir volume 4 3 analysis based on correlation on the base of the four zones spatial borders the most important influencing factors of water temperature in each zone of dongqing reservoir were able to be found which would be of value in further research of rwtf hence the pearson correlation coefficients between water temperature and its influencing factors were calculated for each zone and the calculation results were shown in fig 7 the correlation between water temperature and its influencing factors in the hysteretic zone a in descending order were inflow 0 71 inflow temperature 0 70 relative humidity 0 58 air temperature 0 35 cloud cover 0 23 wind velocity 0 82 it could be seen that although the water temperature in the hysteretic zone a remained relatively stable throughout the year the variation of water temperature in the hysteretic zone a was still closely related to meteorological and hydrological factors such as inflow and inflow temperature the wind velocity was significantly negatively correlated with the water temperature in this zone which meant that the higher the wind velocity the colder the water temperature of the zone the correlation between water temperature and its influencing factors in the epilimnion zone b in descending order were inflow temperature 0 97 air temperature 0 84 inflow 0 75 relative humidity 0 45 cloud cover 0 56 wind velocity 0 67 the epilimnion zone b was more sensitive to meteorological factors and more likely to change with seasonal climate variation specifically inflow temperature had very strong influences on water temperature in this zone the correlation between water temperature and its influencing factors in the thermocline zone c in descending order were inflow temperature 0 94 inflow 0 75 air temperature 0 71 relative humidity 0 57 cloud cover 0 44 wind velocity 0 76 the thermocline zone c served as the zone bridging the epilimnion zone b and hysteretic zone a where the water temperature had high correlations with inflow temperature inflow and air temperature the correlation between water temperature and its influencing factors in the mixed zone d in descending order were inflow temperature 0 99 air temperature 0 88 inflow 0 74 relative humidity 0 43 cloud cover 0 58 and wind velocity 0 64 the mixed zone d was in the tail of the reservoir and its physical property was similar to the natural river therefore inflow temperature and air temperature were the main factors affecting the water temperature in this zone and the water temperature had negative correlations with cloud cover and wind velocity 5 discussions 5 1 analysis of applicability in 4 result this paper has zoned the rwtf of dongqing reservoir based on the dataset of a whole year but it doesn t mean that the dataset is not able to be changed in contrast the dataset structure should be flexibly adjusted to different shapes to meet the diverse needs of specified engineering or academic problems hence this chapter will zone the rwtf of dongqing reservoir with the dataset of spring and summer to check the applicability of the method developed by this paper when the dataset structure is changed it can be seen in fig 8 that the distribution of the four zones in dongqing reservoir is different to some degree but the overall structure is similar which could make sense in theory in spring the inflow temperature begins to rise leading to the thermocline zone c coming into being and the water flowing from the mixed zone d to the epilimnion zone b is mostly on the surface of the reservoir under the action of buoyancy so that the top of the mixed zone d is longer than the bottom in summer the water temperature in the epilimnion zone b tends to be higher than that in the mixed zone d owing to the high air temperature thus the water of the mixed zone d starts to dive into the thermocline zone c with the action of buoyancy which contributes to the thermocline zone c thicker the zoning results are still reasonable and can reflect the different characteristics of different periods which shows that the method in this paper can be applied to different dataset structures 5 2 mechanism discussion considering the previous analysis appropriate spatial zoning results can be obtained through the k means clustering algorithm which is based on the results of numerical simulations by analyzing the patterns and characteristics of the data itself the k means clustering algorithm splits the rwtf spatially without relying on empirical parameters nevertheless it is currently impossible to explain machine learning s internal functioning the k means clustering algorithm s training criteria determine how far a point is from each zone s center then chooses the zone to which the point belongs based on the minimum distance the dataset used in this study has 733 dimensions but it is impossible to understand the meaning of the distances in a high dimensional space although the dataset s structure cannot be shown or plotted in a high dimensional environment the water temperature and the velocity can be observed by downscaling the dataset to three dimensions separately as shown in fig 9 they are extremely complex even when dropped to three dimensions respectively let alone the combination of them in higher dimensions therefore the mechanism of the machine for zoning the rwtf needs to be further investigated 6 conclusions this study implements a successful method for zoning the rwtf based on numerical simulation results using the k means clustering algorithm the main conclusions are as follows 1 these evaluation results prove that dividing into four clusters was a fair selection and the k means clustering algorithm is suitable for the rwtf zoning task 2 the mixed zone of the dongqing reservoir is approximately 12 km along the river the epilimnion zone lies roughly from an elevation of 470 m to the surface of the reservoir the thermocline zone lies roughly between an elevation of 430 m and 470 m and the hysteresis zone lies between the bottom of the reservoir and elevation 430 m 3 based on the zoning results it is found that inflow conditions and air temperature were the main meteorological factors affecting rwtf inflow conditions have the greatest effect on the mixed zone and air temperature has the greatest effect on the epilimnion zone the method presented in this research might be used as a useful zoning technique for investigating the mechanism of rwtf which is independent of the physical parameters and can integrate multiple time points to give a reliable zoning result in addition the mechanism of it needs further research it will be difficult but worthwhile credit authorship contribution statement wei liu methodology investigation formal analysis supervision writing original draft data curation funding acquisition peng zou conceptualization methodology software formal analysis writing original draft visualization dingguo jiang conceptualization investigation methodology formal analysis resources writing review editing xiufeng quan writing review editing software visualization huichao dai resources supervision conceptualization writing review editing declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests dingguo jiang has patent pending to state intellectual property office of china acknowledgements this work is supported by the national natural science foundation of china no u2040220 and special thanks are given to the anonymous reviewers and editors for their constructive comments appendix a supporting information supplementary data associated with this article can be found in the online version at doi 10 1016 j ejrh 2022 101239 appendix a supplementary material supplementary material 
7824,study region built up area of zhengzhou china study focus an urban flood forecasting model based on multi model integrated forecasting is proposed based on the data of rainfall process and inundation process a multi model ensemble prediction model was established for 27 typical waterlogging points in zhengzhou by using bma bayesian model average coupling different data driven methods combined with the rainfall forecast data the bma was driven to predict and warn the inundation process of urban flood new hydrological insights for the region using the bma to predict urban flood can improve the accuracy and stability of using single model to predict urban flood the results indicate that the prediction accuracy of bma model is 36 46 higher than that of single model which demonstrate that bma makes effective use of the advantages of each model and can provide higher accuracy in prediction and early warning additionally bma significantly reduces the uncertainty of single model prediction in the prediction of inundation process the analysis of early warning in two different urban flood events indicates that bma is more suitable for the prediction of severe waterlogging and illustrates the great potential and prospects of bma in urban flood early warning graphical abstract ga1 keywords urban flood waterlogging point bayesian model average prediction data availability the data that has been used is confidential 1 introduction globally the occurrence of urban floods has been unprecedented resulting in huge economic and social losses sundaram et al 2021 in china a country with rapid urbanization construction and varying topography and climatic conditions more than 100 cities suffered from urban floods every year from 2008 to 2019 wu et al 2021 causing direct economic losses of more than 100 billion yuan every year in july 2021 the 7 20 rainstorm in zhengzhou china caused 380 deaths and a direct economic loss of 40 9 billion yuan urban flood disaster has become a crucial problem restricting the healthy development of china s economy and society zheng et al 2016 therefore it is necessary to explore the rapid accurate and stable prediction and early warning methods of urban flood in order to reduce the loss of urban flood disasters in recent years hydrologic experts are devoted to study a variety of one dimensional models two dimensional 2d hydrodynamic models and data driven models to simulate and predict urban flood disaster alfieri et al 2014 bates et al 2010 dimitriadis et al 2016 hou et al 2021 however some recent studies have shown that the prediction performance of a single method or model is always limited liu and merwade 2018 yan and moradkhani 2016 due to the uncertainty of model structure parameters and inputs a specific model that works well in one flood event might not perform well in another liu and merwade 2019 therefore relying on a single model for the flood prediction usually increases the statistical deviation of prediction liu and merwade 2018 in order to deal with this problem recent studies used multi model combination methods to deal with model uncertainty and improve model performance among the current existing multi model combination approaches including simple model average sma weighted model average wma and multi model super ensemble mmse ajami et al 2006 chowdhury and sharma 2009 najafi and moradkhani 2015 2016 bma is one of the most common and widely used methods to generate a reliable deterministic model prediction by synthesizing several models li et al 2017 liu and merwade 2019 madadgar and moradkhani 2014 in recent years the bma technique has been widely used in hydrology basher et al 2020 yan and moradkhani 2015 meteorology baran and möller 2015 ecology yang et al 2021 and economy fernández et al 2001 huo et al 2019 simulated the flood process of three semi humid regions by using seven hydrological models and the bma method they found that bma method effectively makes use of the advantages of each model and provides more accurate prediction qu et al 2017 applied the bma method to predict runoff in fuhe river basin china the results show that bma post processing approach can improve the performance of runoff prediction and obtain a more accurate prediction probability density function over a range of lead times from 24 to 120 h yan and moradkhani 2016 proposed an extreme flood information estimation method considering the uncertainty of distribution and model structure using the bma method they found that the main uncertainty in extreme flood forecasting is determined by the model structure liu and merwade 2018 established a robust deterministic water level prediction and prediction distribution by combined the different hydraulic models the results show that the bma method provides consistent and reliable basin flood level prediction although it does not always outperform the best model as indicated earlier the bma provides a more robust method for the flood prediction the bma technique can not only quantify the uncertainty of the model through a priori probability and a posteriori probability but also weighted average the model according to a posteriori probability its core idea is to assign a weight to each model according to the performance of the model during training liu and merwade 2018 the advantage of this approach is that bma provides a model weighted average prediction of the interested variable rings et al 2012 however to the authors best knowledge literature about the bma method to study the urban flood simulation and prediction is quite limited and no study has applied the bma and data driven approach to the prediction of local urban flooding in summary this study attempts to establish a comprehensive urban flood prediction model using bma and data driven methods and explore the application of data driven model using bma as a fusion based method to predict urban flood depth at different waterlogging points the main objectives that are addressed in this study are to i construct a comprehensive prediction model by bma and three machine learning methods support vector machines svm back propagation neural network bpnn and adaptive boosting adaboost ii analyze the performance of bma and single model in urban flood prediction from the perspectives of model efficiency accuracy and stability and iii use rainfall forecast data to drive bma model for fine early warning of urban flood the innovation of this study is to establish an accurate and stable prediction model of urban flood by using bma and data driven model the research results can provide new ideas for urban flood prediction as well as provide a scientific basis for urban flood control and early warning 2 materials and methods 2 1 study area zhengzhou city the capital of henan province in china was selected for this study fig 1 as of the end of 2020 the permanent population of zhengzhou exceeded 12 million and the total gross domestic product gdp reached 1 20 trillion yuan the annual average precipitation in this region is 639 5 mm however the distribution of precipitation is very uneven and the interannual variation is large the precipitation in the flood season june to september accounts for about 60 of the annual precipitation in the year of 2018 2019 and 2021 serious urban floods occurred successively in this region especially the urban flood event in july 20 2021 which caused 380 deaths and a direct economic loss of 40 9 billion yuan the urban flood disaster has become a prominent problem affecting the sustainable and healthy development of zhengzhou 2 2 data in this paper the rainfall and inundation data of 18 historical rainfall events in zhengzhou from 2016 to 2018 were selected for inundation process simulation to evaluate the performance of bma model among them three randomly selected rainfall events with different rainfall intensity and duration were used as test data and the other 15 rainfall events were used as training data and the rainfall forecast data of two different types of rainfall events from 2019 to 2020 were used for early warning the detailed data were described as follows rainfall data rainfall data refer to the rainfall process data with 10 min time resolution observed and recorded by 16 rainfall stations in zhengzhou which are june 4 june 23 august 4 august 25 and september 12 in 2016 may 22 june 22 july 18 august 12 august 25 and august 30 in 2017 may 15 july 4 july 13 july 27 august 10 september 15 and september 25 in 2018 the rainfall process data of each waterlogging point were obtained by interpolating the rainfall process data of the rainfall station by using gis and kriging interpolation method the rainfall data of the rainfall station were collected from the meteorological department of zhengzhou city waterlogging data waterlogging data refer to the water depth of waterlogging points in historical waterlogging events which mainly from the monitoring equipment of waterlogging at each road intersection the water depth data of 27 typical waterlogging points with 1 min time resolution were collected from the urban management department of zhengzhou in order to be consistent with rainfall data the time resolution of waterlogging data was set to 10 min the waterlogging data of 18 rainfall events from 2016 to 2018 correspond to the above rainfall data are used as the training and verification data of the model the water depth data of rainfall events on august 1 2019 and august 7 2020 were used as the verification data of early warning results rainfall forecast data rainfall forecast data is basis of urban flood early warning the rainfall forecast data of two rainfall events in august 1 2019 and august 7 2020 were obtained by calling the api of the meteorological technology company caiyun technology the rainfall forecast data have a temporal resolution of 1 min and a spatial resolution of 1 km in order to unify the time resolution the time resolution of rainfall forecast data was changed to 10 min and the forecast period of rainfall forecast data is from 0 to 120 min 2 3 data driven approach for the purpose of modeling inundation process prediction model for each waterlogging point three data driven models with different structures and complexities including svm bpnn and adaboost were selected svm is a supervised machine learning algorithm the core idea is to use the kernel function to find the optimal hyperplane by mapping data to a high dimensional feature space yu et al 2008 bpnn is a feedforward neural network which is widely used in data mining it repeatedly modifies the weight of each layer through two processes of forward propagation and back propagation to train the model chen et al 2010 adaboost is an ensemble learning algorithm which was usually selected the decision tree as the basic learner the core idea is to update the sample weight value by using the error of the previous iteration during each iteration taherkhani et al 2020 in order to ensure the consistency of model comparison the parameters of the above three data driven models were calibrated by monte carlo sampling mean absolute error mae and nash sutcliffe efficiency coefficient nse were selected to evaluate their accuracy and performance the results using single model are the basis of the bma model therefore the above data driven models were used to construct the bma model for inundation process prediction in this study however the modeling process of using data driven model will not be introduced in detail in this study since the purpose and core of this paper is to evaluate the performance of bma model in the local urban flooding prediction a complete mathematical description and modeling process using data driven approach can be found in the previous research wu et al 2020a 2020b 2 4 basic principle of the bma algorithm assuming that y is the simulation variable of multi mode d y 1 y 2 y t is the measured inundation process data required to calibrate the model f f 1 f 2 f k represents the model space composed of svm bpnn and adaboost p k y f k d is the posterior distribution of simulation variable y given measured data d and model according to the law of total probability the posterior probability of simulation variable y in bma can be expressed as 1 p y d k 1 k p f k d p k y f k d where p f k d is the posterior probability of the k th model f k given the measured data w k in other words w k is the weight of each model in bma k 1 k w k 1 the predicted value of bma was obtained by the weighted average of each data driven model assuming that the predicted value and measured value of each model obey the normal distribution the expectation and variance of the bma model can be expressed as 2 e y d k 1 k p f k d e p k y f k d k 1 k w k f k 3 v a r y d k 1 k f k k 1 k w i f i 2 k 1 k w k σ k 2 em expectation maximization algorithm is an effective method to calculate bma which requires that the prediction results of data driven models obey normal distribution raftery et al 2005 in this study the measured and model simulation sequences were normalized by the box cox function in matlab based on this the probability distribution parameters w k and σ k 2 of simulation variable y in bma were calculated using the em algorithm assuming that θ w k σ k 2 k 1 2 k is the parameter to be solved of the bma model the log likelihood function can be expressed as 4 l θ ln p y d ln k 1 k w k p k y f k d the em algorithm is iterative and alternates between two the e step and the m step in step e latent variable is estimated according to the current guess of the parameters in step m θ is estimated according to the current value of the latent variable the em algorithm iterates repeatedly through expectation and maximization the iteration stops when the error of likelihood obtained by the iteration is less than the allowable error 10 6 in this study the log likelihood function will converge to the local maximum of the likelihood since the log likelihood function increased at each em iteration wu 1983 a complete mathematical and technical description of bma and em algorithm can be found in raftery et al 2005 liu and merwade 2018 2 5 the uncertainty analysis of bma model uncertainty of the model reflects the stability of the prediction performance after the weight ω k and variance σ k 2 of the bma model were obtained using the em algorithm the uncertainty interval of the prediction value at any time twas deduced using the monte carlo method hammersley and handscomb 1964 a large number of sampling and gradual approximation are the core idea of monte carlo method when the sampling times are large enough the deduced results gradually approach the real results stably therefore the sampling time was determined to 10 000 at every time node of inundation process in this study and 95 confidence interval of the bma model was deduced as shown in fig 2 the estimated value y ˆ at any time t was calculated by sampling method and arrange all the estimated values y ˆ in size the 95 confidence interval of bma model is the middle part of the quantile of 2 5 and 97 5 in addition the monte carlo method was also used for single models and the sampling times were also set to 10 000 times the 95 confidence interval of the predicted value was deduced according to the probability distribution of each model 2 6 the evaluation statistics of bma three types of indicators are considered to evaluate the performance of bma algorithm in this study in the first type the mae and nse were selected to evaluate the performance of the model in validation periods eqs 5 6 since the bma approach produces both deterministic prediction and probabilistic prediction the smaller the mae and the closer the nse is to 1 indicating that the model accuracy and performance is better 5 m a e i 1 n x i y i n 6 n s e 1 i 1 n x i y i 2 i 1 n x i y i 2 where x i is the predicted value y i is the measured value and y is the average of the measured value in the second type overall accuracy oa overestimation rate or and underestimation rate ur were used to evaluate the overall accuracy of early warning results eqs 7 9 specifically oa is the proportion of correct samples in all samples of early warning results ur refers to the proportion of samples with early warning level lower than the observed waterlogging level in contrast or refers to the proportion of samples with early warning level higher than the observed waterlogging level oa or ur 1 7 o a n 1 4 m 1 4 x m n n m n 8 u r n 1 4 m 1 4 x m n n m n 9 o r n 1 4 m 1 4 x m n n m n where m is the predicted waterlogging level n is the measured waterlogging level n refers to the total number of samples and x m n refers to the number of samples that the measured m level is predicted as level n in addition precision and recall are selected to evaluate the accuracy of early warning results at each level eqs 10 11 precision is the proportion of true positive samples in the prediction samples recall refers to the proportion of true positive samples in all positive samples faceli 2011 10 pr e c i s i o n t p t p f p 11 re c a l l t p t p f n where tp is the number of samples correctly classified as positive fp is the number of samples incorrectly classified as positive because the right category is negative and fn is the number of samples incorrectly classified as negative because the right class is positive faceli et al 2011 when calculating the precision and recall of early warning results tp fp and fn need to be determined first on this basis the precision and recall of early warning results at each level were calculated by using eq 10 and eq 11 finally the overall precision and recall of early warning results were obtained by weighted averaging the weight was determined according to the number of waterlogging points under each level the precision and recall of each early warning level 3 results and discussion 3 1 the characteristics of rainfall and waterlogging data 3 1 1 the characteristics of rainfall data in this study the rainfall data of 18 rainfall events from 2016 to 2018 were selected as the training data of the ponding prediction model from the perspective of precipitation fig 3 the precipitation of the 18 urban flood events was distributed between 38 mm and 105 mm of which the rainfall events over 70 mm account for 83 3 of the total rainfall events the rainfall duration was distributed between 170 min and 290 min and the location coefficient was distributed between 0 19 and 0 77 among them the rainfall events with the location coefficient lower than 0 5 account for 61 1 of the total rainfall events therefore although the rainfall events with small location coefficient less than 0 5 and large precipitation more than 70 mm account for the majority of the total rainfall events the selected rainfall events basically cover different types of rainfall which can provide more comprehensive sample data for the construction of ponding prediction model in addition the average precipitation of 18 urban flood events reached 79 4 mm indicating that large precipitation was one of the main reasons for urban flood furthermore it can be easily found that the precipitation of the 1st 14th and 16th urban flood events did not exceed 45 mm fig 3 and the rainfall duration of three urban flood events were no more than 190 min indicating that the short term heavy rainfall with a small total precipitation was also one of the reasons for urban flood 3 1 2 the characteristics of waterlogging data fig 4 a reflects the water accumulation characteristics of 27 waterlogging points in 18 rainfall events the maximum depth and average depth of water accumulation in the 27 waterlogging points fluctuate greatly the maximum depth is between 0 15 m and 0 68 m and the average depth is distributed between 0 11 m and 0 55 m among them the water accumulation at the waterlogging point of number 10 is the smallest the water accumulation at the waterlogging points of number 1 16 and 26 is very serious and the maximum depth is more than 0 5 m urban flood control should pay more attention to these waterlogging points with serious water accumulation in contrast the minimum depth of water accumulation at these waterlogging points fluctuates less ranging from 0 05 m to 0 19 m the main reason may be that it is difficult to form serious water accumulation at these waterlogging points under relatively small precipitation which makes the water accumulation depth show a small difference in general the water accumulation depth of the waterlogging points in the 18 rainfall events is between 0 05 m and 0 68 m covering different degrees of waterlogging events which can provide more comprehensive waterlogging sample data for the construction of urban flood prediction models in order to analyze the water accumulation characteristics of 27 waterlogging points under similar rainfall conditions the water depth of each waterlogging point under the 2nd 7th and 18th rainfall events with similar rainfall rainfall duration and location coefficient was analyzed as shown in fig 4 b under similar rainfall conditions the average water depth of each waterlogging point was between 0 08 m and 0 44 m and the number of waterlogging points with more than 0 2 m water depth exceeded 88 it shows that although most waterlogging points have accumulated water there are significant differences in the severity of the accumulated water the main reason is that the topography land use and drainage conditions of each waterlogging point were quite different the waterlogging points in sunken terrain such as tunnels and overpasses are more likely to form serious waterlogging this spatial difference also provides some reference for urban flood control which should pay more attention to these waterlogging points with serious waterlogging in addition these spatial differences of waterlogging data also can provide more comprehensive waterlogging sample data for the construction of urban flood prediction models 3 2 validation of bma prediction performance 3 2 1 the overall performance of bma three randomly selected rainfall events with different location coefficients and rainfall duration from 2016 to 2018 were selected as verification events to evaluate the performance of bma model which were august 25 in 2016 with 0 39 location coefficient and 250 min rainfall duration august 30 in 2017 with 0 70 location coefficient and 220 min rainfall duration july 27 in 2018 with 0 16 location coefficient and 280 min rainfall duration the root mean square error rmse was selected as the objective function the prediction sequence results of bma model were obtained by em algorithm in addition the performance of bma model was analyzed from the running time and accuracy of the model in terms of model running time the average running time of bma and three single models were 5 6 s bma 1 6 s bpnn 4 2 s svm and 0 6 s adaboost it can be seen that bma model has the longest running time the main reason is that bma needs to couple multiple models for prediction so that the running time of bma model contains the running time of single model however although the calculation efficiency of bma is lower than single model the running time of 5 6 s will hardly affect the prediction period of flood warning which is also a significant advantage of data driven model in calculation efficiency fig 5 shows the overall accuracy and difference between the bma model and the single model the prediction results of svm bpnn and adaboost were greater than 0 94 indicating that these models have good prediction performance in local urban flooding prediction among them svm has the highest nse and the lowest mae showing the best prediction effect in contrast adaboost has a lower nse and a higher mae showing the worst performance the main reason was that the limited sample data make it difficult for adaboost to give full play to its advantages and svm also performs well in the limited sample making the performance of svm in local urban flooding prediction slightly higher than adaboost in addition although both the single model and bma model have achieved good results in local urban flooding prediction the bma model has further improved the prediction accuracy the mae of the local urban flooding prediction in bma is 36 46 lower than that of svm bpnn and adaboost specifically the mae in bma is less than 0 02 m 0 019 m but the mae in svm bpnn and adaboost are more than 0 03 m in addition the nse of the bma model is also significantly higher than svm bpnn and adaboost therefore the above results indicate the overall accuracy of the bma model is significantly better than the single model svm bpnn and adaboost in the local urban flooding prediction fig 6 demonstrates the performance between bma model and single model in different types of rainfall events bpnn has the highest prediction accuracy in the verification event 1 and 2 but adaboost shows higher accuracy in the verification event 3 which indicates that it is difficult for the single model to maintain the highest prediction accuracy in all rainfall event in contrast the bma model always has the lowest mae and the highest nse in three different types of rainfall events which shows that the stability of bma model is significantly higher than svm bpnn and adaboost in addition fig 6 showed that the prediction performance of adaboost in different verification events were quite different although it has better prediction performance than svm and bp in the verification event 3 the prediction performance in the first verification event was significantly lower than other models on the contrary the prediction performance of svm and bp was relatively stable the main reason may still be that the amount of rainfall and waterlogging data was not large enough making it difficult for adaboost to fully explore the potential relationship between rainfall and waterlogging data 3 2 2 the performance of bma in each waterlogging point in order to verify the robustness of bma model the prediction performance of the bma and single model in each waterlogging point was analyzed it should be pointed out that the characteristics and number of waterlogging points under different rainfall events are uncertain therefore the waterlogging points in the three verification events may have certain differences which may cause interference to the model performance analysis therefore this study selects 27 identical waterlogging points in three validation events to analyze the prediction performance of the model in order to eliminate the impact of changes in waterlogging points on the performance analysis as shown in fig 7 the mae fluctuation range of bma model is between 0 and 0 030 m which is 34 8 68 1 lower than that of svm 0 009 0 055 m bpnn 0 010 0 056 m and adaboost 0 012 0 106 m and the lowest nse value of bma model 0 945 is 26 0 33 1 higher than that of svm 0 74 bpnn 0 75 and adaboost 0 71 these results show that the prediction accuracy of bma model is significantly better than that of single model svm bpnn and adaboost in the first validation event similarly in the second and third validation events the lowest nse values of bma model were 0 87 and 0 922 which were 14 4 72 7 and 15 0 61 8 higher than that of single model and compared with the single model the mae fluctuation range of bma model also decreased by 29 1 48 4 these results indicate that the bma model not only has higher prediction accuracy in different waterlogging points but also has stable prediction performance however the prediction performance of bma is not the highest in all cases for example the nse of bma 0 95 is lower than that of bpnn model 0 98 in the waterlogging point of number 21 the main reason is that the results of bma were obtained by weighted averaging the results of single models therefore when the local error of single model is large the prediction accuracy of the bma model may be lower than that of the single model with the highest accuracy nevertheless the prediction performance of the bma model is significantly better than that of the single model in most cases which can provide more stable urban flood prediction in order to deeply explore the potential relationship between the model performance and different types of waterlogging points three different types of waterlogging points 16 23 and 26 were selected from 27 waterlogging points to compare the prediction performance by reflecting the characteristics of the waterlogging points among them the waterlogging point of number 26 was located near the drainage outlet at the bottom of the tunnel the waterlogging point of number 16 was located near the road intersection and the waterlogging point of number 23 was located at the edge of the auxiliary road under the elevated road as shown in fig 8 from the perspective of water accumulation characteristics the water depth at the waterlogging point of number 26 is the largest and the water accumulation process line shows the characteristics of steep increase gentle and then slow decrease the main reason is that the waterlogging point of number 26 is located at the underpass tunnel due to the influence of the microtopography of the underpass tunnel the rainfall converges rapidly after reaching the surface resulting in the rapid increase of the water depth and the water depth at the waterlogging point of number 16 is the smallest the reason may be that the terrain at the road intersection is relatively flat the lower surface confluence speed and the flat terrain make the water depth smaller in addition it is precisely because of this terrain that the water accumulation curve is relatively gentle from the perspective of prediction performance the prediction performance of single model shows great uncertainty for example the nse of adaboost model at 16 and 23 waterlogging point is greater than 0 96 but at 23 waterlogging point is only 0 943 similarly the nse of bpnn at 26 and 23 waterlogging point is greater than 0 96 but at no 16 waterlogging point is only 0 924 in contrast the prediction results of bma model in three different types of waterlogging points are 0 984 26 0 971 16 and 0 982 32 showing high accuracy and stable performance therefore it can be concluded that bma model has stable prediction performance in the prediction of different types of waterlogging points and can effectively improve the prediction effect of single model 3 3 the inundation characteristics based on the bma model the rainfall data under different rainfall return periods with 180 min rainfall duration once in half a year once a year once in two years once in five years once in ten years once in twenty years and once in fifty years wu et al 2020a were obtained using the classic chicago rainstorm method and the rainstorm intensity formula in zhengzhou city shao and liu 2018 eq 12 then the local urban flooding prediction results of bma model under different rainfall return periods were obtained by inputting rainfall data into the model the polynomial fitting and cluster method were used to analyze the trend and characteristics of water depth under different rainfall return periods specifically the increasing trend of water depth under different rainfall return periods was determined by using the polynomial fitting method and the trend of 27 waterlogging points was clustered into four categories by cluster analysis 12 i 40 1 1 0 794 lg p t 25 8 0 948 where i is the intensity of rain p refers to the rainfall return period t is the duration of rain as shown in table 1 and fig 9 including 12 waterlogging points in category 2 accounting for 44 4 of the waterlogging points in this study the inundation characteristics of these waterlogging points are that the water depth is approximately linear with rainfall and there are 7 waterlogging points with the water depth of more than 0 3 m and 11 waterlogging points with the water depth of more than 0 2 m under the rainfall return period of 10a in category 3 including 5 waterlogging points showing growth trend of gradually decrease in the water depth the water depth exceeds 0 3 m in 10 years rainfall return period and 0 4 m in 50 years rainfall return period however the growth trend of water depth in category 4 including 4 waterlogging points is gradually increasing which is opposite to the inundation characteristics of category 3 under the rainfall return period of 50 years the water depth is more than 0 4 m in all waterlogging points of category 4 and the water depth is more than 0 6 m in the waterlogging point of no 1 and no 26 which is higher than other waterlogging points on the contrary the water depth of the waterlogging points in category 1 is generally low to be specific there is almost no waterlogging in these waterlogging points under the rainfall return period of 0 5year 1year and 2years the water depth increases gradually when the rainfall return period over 5years however water depth still does not exceed 0 2 m under the rainfall return period of 10 years and there is also only one waterlogging point with the water depth of over 0 3 m under the rainfall return period of 50 a on the whole there are 74 waterlogging points have a water depth of more than 0 25 m when the rainfall return period over or equal to 10 years however there are only 37 waterlogging points have a water depth of more than 0 25 m when the rainfall return period did not exceed 5 years in addition no matter in waterlogging points of category 2 category 3 or category 4 the water depth generally exceeds 0 25 m when the rainfall return period over 10 years especially in the waterlogging points of category 4 the growth trend of water depth is increasing with the increase of rainfall which will threaten the traffic safety of vehicles and pedestrians therefore in urban flood control urban management departments should pay more attention to the rainfall events with a return period of more than or equal to 10 years and the increasing trend of water depth these waterlogging points specifically many effective measures including dredge rainwater wells before the flood season using pumping pumps to improve drainage capacity and cutting off the road in case of serious waterlogging should be taken to ensure the traffic safety of vehicles the safety of pedestrians and reduce the losses caused by flood disasters 3 4 uncertainty analysis of results coverage rate cr and bandwidth b were used to evaluate the uncertainty of bma and single model the cr represents the reliability of the model and it is the proportion of observed results falling within 95 confidence interval the b space between upper interval and lower interval reflects uncertainty of the prediction results and it is the mean width across the inundation process liu and merwade 2018 the narrower the b the higher the cr indicating that the uncertainty of the model is lower in addition the uncertainty of each stage may be significantly different in the prediction of inundation process therefore in order to deeply analyze the uncertainty and accurately identify this difference four typical waterlogging points were selected by stratified sampling method which were 7 c1 16 c2 11 c3 and 1 c4 as shown in table 2 on the premise that the cr of bma is not less than 96 the b of bma is reduced by 41 9 48 1 compared with the single model indicating that bma model has lower uncertainty in addition in the three different validation events bma model always has the highest cr and the narrowest b and the cr of bma is also more than 96 these results demonstrate that bma significantly reduced the uncertainty of the single model which is consistent with the analysis results in section 3 1 figs 10 12 shows uncertainty of the svm bpnn adaboost and bma under different validation events as shown in fig 10 the b of bma model is not more than 0 05 m in the first validation event which is 9 8 61 6 less than that of svm bpnn and adaboost indicating that bma reduces the uncertainty of the prediction results of single model however in the middle period of inundation process although the cr of bma is 100 the b of bma is higher than that in the beginning and end stages of inundation process indicating that the bma shows high uncertainty in the middle period of inundation process in addition it can be found that the b of bma model 0 046 m at the waterlogging point of category 4 is 7 0 24 3 higher than that of other types of waterlogging points indicating that the prediction results of bma model show higher uncertainty at the waterlogging point of category 4 in the second validation event fig 11 the b of bma is between 0 042 m and 0 058 m which is 25 9 56 7 less than that of single model indicating that bma show lower uncertainty than svm bpnn and adaboost under the 95 confidence interval to be specific the prediction results of bma always have the lowest b in all types of waterlogging points and the b of bma across the inundation process is also lower than that of svm bpnn and adaboost in addition the uncertainty of bma has the same characteristics as the verification event 1 in different stages of inundation process in the beginning and end stages of inundation process bma has a narrow interval width while in the middle period of inundation process bma has a wide interval width indicating that bma also shows higher uncertainty in the middle period of inundation process similarly the b of bma model between 0 038 m and 0 059 m is 20 3 61 8 less than that of single model fig 12 indicating that bma significantly reduces the uncertainty of the single model prediction from the characteristics of the interval width under the 95 confidence interval bma has a higher interval width in the middle period of inundation process showing higher uncertainty in addition the prediction results of bma at the waterlogging point of category 4 1 have the highest average bandwidth and higher uncertainty which is similar to the first two verification events therefore uncertainty of the bma shows the similar characteristic in different validation events first of all the interval width under 95 confidence interval of the bma prediction results is 9 8 61 8 less than svm bpnn and adaboost showing that bma can significantly reduce the uncertainty of single model prediction in different types of rainfall events secondly the interval width of bma at the waterlogging points of category 4 is 7 0 55 3 higher than that of the other types of waterlogging points which demonstrate the prediction results of bma have higher uncertainty at the waterlogging points of category 4 in addition the bma forecast model has lower uncertainty at the beginning and end periods of inundation process but it has higher uncertainty at the middle period of inundation process the reason may be that the runoff generation and confluence conditions are clear in the initial inundation stage and the catchment area is relatively stable similarly in the end stage of inundation the confluence at waterlogging point gradually weakens and it is mainly the process of receding water however the confluence characteristics in the middle stage of inundation may be complex for example there may be hydraulic connection with the surrounding pipe network and waterlogging points resulting in the change of confluence conditions these may lead to higher uncertainty in the prediction of middle stage of inundation nevertheless bma has significantly reduced the uncertainty of single model prediction 3 5 urban flood forecasting and early warning 3 5 1 analysis of rainfall forecast data the rainfall forecast data of two rainfall events on august 1 2019 and august 7 2020 were selected as input samples which have a forecast period of 0 120 min and a temporal resolution of 10 min it is well known that the accuracy of rainfall forecast data has great uncertainty which may bring great uncertainty to the output results of the model therefore it is necessary to evaluate the accuracy of rainfall forecast data before using rainfall forecast data to drive the urban flood prediction model as shown in table 3 although there are large uncertainties in the accuracy of rainfall forecast data the accuracy of rainfall forecast data gradually improves with the shortening of the forecast period when the forecast period is shortened to 90 min the mean relative error mre of rainfall forecast data is reduced to 29 2 which can basically meet the accuracy requirements of input data of bma model 3 5 2 the early warning results based on bma by inputting the rainfall forecast data with different lead forecast time into the bma model using python 3 7 the inundation process results of each waterlogging point were obtained the water depth was used to evaluate the urban flood disaster risk of each waterlogging point which is divided into four classes 0 3 cm 3 10 cm 10 25 cm 25 cm on this basis the early warning results of two rainfall events were obtained using gis for example by inputting the rainfall forecast data with 100 min lead forecast time into the model combined with the classification standard and gis the early warning results with 100 min lead forecast time were obtained similarly with the shortening of lead forecast time the rainfall forecast data with 90 min 80 min 10 min lead forecast time were continuously input into the model the early warning results under different lead forecast time were obtained it should be noted that because the running time of bma model 5 6 s hardly affects the forecast period of early warning this study didn t consider the running time in the discussion of forecast time as shown in table 4 with the shortening of lead forecast time the accuracy of early warning results is gradually improved the accuracy of early warning results has increased from 74 0 100 min lead forecast time to 90 7 10 min lead forecast time the main reason for this phenomenon is that the accuracy of rainfall forecast data gradually improves with the shortening of lead forecast time and the improvement of the accuracy of input data will improve the accuracy of the output result then the accuracy of early warning results will be improved it is worth noting that when the lead forecast time is less than or equal to 80 min the precision and recall of the bma model are both exceeding 80 which basically meets the accuracy requirements of early warning what more the lead forecast time of 80 min is basically enough for relevant managers to make full response therefore these demonstrate that this method has great application potential in the early warning and response of waterlogging in addition table 4 and fig 13 also shows the accuracy of bma model at each early warning level the performance of the early warning results gradually improved with the increase of early warning level to be specific the bma model has low precision in the early warning of level 1 68 6 but the precision in the early warning of other levels is exceeding 88 similarly the recall of the bma is from 80 1 to 91 4 with the improvement of early warning level in addition it is obvious that the prediction performance of the bma model for the early warning level of level 4 is higher than the early warning level of other levels indicating that bma model is more suitable for the prediction of serious waterlogging fig 14 reflects the spatial characteristics of waterlogging it is obvious that the areas with serious waterlogging were mainly concentrated in the middle and south of zhengzhou city in the middle area of zhengzhou city the waterlogging degree is obviously high since these areas are old urban areas with low pipe network design standards and aging pipe networks the south area of zhengzhou city is in the stage of rapid development and construction the drainage system of these areas is still under construction in addition the new project will change the layout of the original drainage pipe network therefore these areas are easier to produce serious waterlogging in heavy rainfall events in addition in the rainfall event on august 1 2019 there were 15 waterlogging points with water accumulation degree of more than grade 3 including grade 3 however 11 of the 15 waterlogging points in the rainfall event on august 7 2020 still have a water accumulation degree of more than grade 3 which indicates that although the distribution and inundation of the waterlogging points may be affected by the rainfall event the underlying surface characteristics determine that the risk of waterlogging at these waterlogging points is greater therefore urban flood control should pay more attention to these waterlogging points 3 5 3 comparison of early warning results between the bma and single model the oa ur and or were used to compare the early warning performance of svm bpnn adaboost and the bma model as shown in fig 15 the oa of bma was 90 7 oa but the oa of three single models were no more than 82 indicating that the bma has the higher accuracy than the single model in early warning of local urban flooding in addition it can be found that the early warning results in svm and bpnn have high or 16 7 in svm and 18 5 in bpnn and low ur 1 9 in svm and 5 6 in bpnn indicating that the early warning results were mainly overestimated in contrast the early warning results in adaboost have high ur 13 0 and low or 9 3 which shows that the early warning results were mainly underestimated however the ur 3 7 and or 5 6 of the bma model was not only close but also relatively low therefore the above results and analysis demonstrate that the bma method can be used to improve the accuracy of urban flood prediction and early warning although the early warning results may cause slight overestimation 4 discussion 4 1 influence of rainfall forecast data on early warning accuracy it is generally accepted that the accuracy of rainfall forecast data decreases gradually with the extension of lead forecast time which directly affect the early warning results of the bma model under different lead forecast time as shown in table 4 and fig 13 under different lead forecast time the accuracy of early warning gradually decreases with the extension of the forecast time which is mainly due to the influence of the accuracy of rainfall forecast data nevertheless the influence of rainfall forecast data on early warning accuracy is not clear therefore in order to analyze the impact of rainfall forecast data on early warning accuracy the quantitative impact of rainfall forecast data in different lead forecast time on early warning accuracy was analyzed by using the control variable method specifically by inputting the measured rainfall data into the bma model the prediction results of inundation process were obtained and combined with the early warning classification standard the early warning results under actual rainfall data were obtained these results were used as a control group to analyze the impact of the rainfall forecast data in different lead forecast time on the accuracy of early warning as shown in fig 16 on the premise that the measured rainfall data was used as the input data control group the error of bma model is 6 9 and due to the influence of rainfall forecast data the error of bma model increased by 9 3 26 in addition the error caused by rainfall forecast data increased significantly after the lead forecast time exceeds 50 min therefore although the accuracy of the early warning results within 80 min lead forecast time can be accepted precision 80 the early warning within 50 min lead forecast time is more reliable from the changing trend of early warning results 4 2 comparative analysis with physical model based on three classical machine learning model this study uses bma approach to couple these machine learning models to construct an integrated prediction model in terms of prediction accuracy the nse of the prediction results of the bma and machine learning models are more than 0 94 in terms of calculation efficiency bma model needs the average running time of 5 6 s and in the early warning under different lead forecast time the precision of bma model is more than 80 when the lead forecast time is no more than 80 min similarly the physical model can also predict the inundation process our previous research has used swmm model to simulate the flood events in zhengzhou wang et al 2022 the results demonstrate that the prediction results of swmm model have an error of 16 7 in addition qi et al 2022 established an urban flood simulation model in haikou city by using pcswmm the results show that the nse of the model is 0 844 therefore the physical model can also achieve accurate urban flood prediction and there is no significant difference in accuracy compared with the data driven model in terms of the calculation efficiency the average calculation time of swmm model under three rainfall events 20170706 20160605 and 20160805 was 1 8 min and the calculation time under the rainfall events on july 20 2021 was 10 1 min recent approaches show that the one dimensional surface and drainage pipe network model usually needs computation time ranging from 5 min to 1 h bulti and abebe 2020 and accurate two dimensional hydraulic model usually needs the running time of more than one hour bulti and abebe 2020 chang et al 2015 the main reason for this difference in calculation efficiency is the complexity of physical model and the duration of rainfall event therefore it can be concluded that the approach proposed in this study is superior to the physical model in terms of calculation efficiency however it should be pointed out that the approach proposed in this study is only applicable to the area with sufficient historical urban flood data in other words the data driven method is difficult to be directly applied in the area lacking data in the area lacking urban flood data this approach can be used as a proxy model of physical model to improve the calculation efficiency recent researches kwon and kim 2021 mignot and dewals 2022 demonstrate that data driven method can significantly improve the computational efficiency of physical model on the premise of ensuring the accuracy in addition the physical model can also be used as the benchmark model of bma introducing physical model into bma model for inundation process prediction and early warning needs to meet two basic conditions one condition is that the physical model can accurately identify the location of waterlogging points and accurately simulate the inundation process of urban flood another condition is that the calculation time of physical model should not be too long therefore future research can focus on how to introduce physical model into bma model on the premise of ensuring accuracy and computational efficiency 4 3 the generalization ability of bma model on july 20 2021 an extreme rainstorm occurred in zhengzhou with a total rainfall of 624 1 mm within 24 h by inputting the rainfall data into the bma model the inundation process was obtained which aims to verify the applicability of the bma model under the extreme rainfall event as shown in fig 17 the trend of inundation process predicted by bma model is consistent with the measured process however the maximum depth of inundation process is underestimated which is 26 1 lower than the actual value and the predicted inundation process is 1 h earlier than the measured inundation process the peak time is 1 h later than the actual inundation process the end time is 1 h earlier than the actual inundation process the one reason for this error is that the bma model does not contain sufficient sample data specifically although the return period of rainfall events in the sample data set has reached once in a hundred years the return period of the rainfall event on july 20 2021 has reached once in a thousand years which seriously exceeds the intensity of all rainfall events in the sample data set therefore due to the limitation of sample data set the accuracy of bma model in predicting extreme rainfall events has decreased an important way to solve this problem is to continuously enrich the sample data set to improve the generalization ability of the model based on this this study collected the data of the extreme rainfall event in zhengzhou on july 20 2021 and imported it into the sample data set which aims to improve the generalization ability of bma model under extreme rainfall events in addition another reason for the decrease in the model prediction accuracy is that the hydraulic connection at the waterlogging point has changed significantly to be specific the increase of rainfall may increase the catchment area greatly and the basin flood will also effects on urban flood therefore in the extreme rainfall event basin flood and urban flood should be considered comprehensively which is also one of the important development directions of urban flood research in the future 5 conclusion bma is a multi model weighted average simulation method integrating the results of various models in this paper the bma was applied to the prediction and early warning of urban flood the main conclusions are as follows 1 in the prediction of local urban flooding bma can improve the accuracy and stability of the prediction results compare with three single models the bma algorithm has the lowest mae 0 019 and the highest nse 0 984 in the prediction of local urban flooding in addition although bma does not always have the highest accuracy in all rainfall events and all waterlogging points the overall predictive performance and stability of the bma model is better than the single model in different rainfall events and waterlogging points which demonstrates that bma can provide a more accurate and stable method for urban flood prediction 2 analysis indicates that urban management departments should pay more attention to the rainfall event with the return period of more than or equal to 10 years and the waterlogging points of category 2 category 3 or category 4 there are 74 waterlogging points have a depth of more than 0 25 m when the rainfall return period over or equal to 10 years especially in the waterlogging points of category 4 the average depth is 0 34 m under the rainfall return period of 10 years and the water depth shows an exponential growth trend with the increase of rainfall 3 among the three verification events of different types bma always has the highest cr and the narrowest b in different stages of local urban flooding bma has lower uncertainty in the beginning and end stages but in the middle period bma has higher uncertainty nevertheless the bma method has reduced the uncertainty of local urban flooding prediction to a certain extent 4 in the fine early warning of local urban flooding using rainfall forecast data the early warning result of bma model has the overall accuracy of 90 7 which is significantly higher than that of svm 81 5 bpnn 75 9 and adaboost 77 8 it shows that bma can provide a more accurate and stable method for fine early warning of urban flood however there are still some directions for further exploration in this study for instance although bma provides a more stable and accurate local urban flooding prediction and early warning method this approach can only predict urban flood in the waterlogging points with detailed observation data therefore future research can combine hydrological model data driven methods and the bma to explore comprehensive and refined urban flood prediction and early warning methods credit authorship contribution statement yihong zhou data curation methodology validation writing original draft writing review editing zening wu methodology validation writing review editing supervision funding acquisition hongshi xu methodology huiliang wang methodology supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research was funded by the key project of national natural science foundation of china no 51739009 excellent youth fund of henan province of china 212300410088 science and technology innovation talents project of henan education department of china 21hastit011 young backbone teachers training fund of henan education department of china 2020ggjs005 national natural science foundation of china 52109040 and china postdoctoral science foundation 2021m702950 we thank the meteorological department and administration bureau of zhengzhou city for providing rainfall and waterlogging data we also thank the anonymous reviewers for their valuable comments 
7824,study region built up area of zhengzhou china study focus an urban flood forecasting model based on multi model integrated forecasting is proposed based on the data of rainfall process and inundation process a multi model ensemble prediction model was established for 27 typical waterlogging points in zhengzhou by using bma bayesian model average coupling different data driven methods combined with the rainfall forecast data the bma was driven to predict and warn the inundation process of urban flood new hydrological insights for the region using the bma to predict urban flood can improve the accuracy and stability of using single model to predict urban flood the results indicate that the prediction accuracy of bma model is 36 46 higher than that of single model which demonstrate that bma makes effective use of the advantages of each model and can provide higher accuracy in prediction and early warning additionally bma significantly reduces the uncertainty of single model prediction in the prediction of inundation process the analysis of early warning in two different urban flood events indicates that bma is more suitable for the prediction of severe waterlogging and illustrates the great potential and prospects of bma in urban flood early warning graphical abstract ga1 keywords urban flood waterlogging point bayesian model average prediction data availability the data that has been used is confidential 1 introduction globally the occurrence of urban floods has been unprecedented resulting in huge economic and social losses sundaram et al 2021 in china a country with rapid urbanization construction and varying topography and climatic conditions more than 100 cities suffered from urban floods every year from 2008 to 2019 wu et al 2021 causing direct economic losses of more than 100 billion yuan every year in july 2021 the 7 20 rainstorm in zhengzhou china caused 380 deaths and a direct economic loss of 40 9 billion yuan urban flood disaster has become a crucial problem restricting the healthy development of china s economy and society zheng et al 2016 therefore it is necessary to explore the rapid accurate and stable prediction and early warning methods of urban flood in order to reduce the loss of urban flood disasters in recent years hydrologic experts are devoted to study a variety of one dimensional models two dimensional 2d hydrodynamic models and data driven models to simulate and predict urban flood disaster alfieri et al 2014 bates et al 2010 dimitriadis et al 2016 hou et al 2021 however some recent studies have shown that the prediction performance of a single method or model is always limited liu and merwade 2018 yan and moradkhani 2016 due to the uncertainty of model structure parameters and inputs a specific model that works well in one flood event might not perform well in another liu and merwade 2019 therefore relying on a single model for the flood prediction usually increases the statistical deviation of prediction liu and merwade 2018 in order to deal with this problem recent studies used multi model combination methods to deal with model uncertainty and improve model performance among the current existing multi model combination approaches including simple model average sma weighted model average wma and multi model super ensemble mmse ajami et al 2006 chowdhury and sharma 2009 najafi and moradkhani 2015 2016 bma is one of the most common and widely used methods to generate a reliable deterministic model prediction by synthesizing several models li et al 2017 liu and merwade 2019 madadgar and moradkhani 2014 in recent years the bma technique has been widely used in hydrology basher et al 2020 yan and moradkhani 2015 meteorology baran and möller 2015 ecology yang et al 2021 and economy fernández et al 2001 huo et al 2019 simulated the flood process of three semi humid regions by using seven hydrological models and the bma method they found that bma method effectively makes use of the advantages of each model and provides more accurate prediction qu et al 2017 applied the bma method to predict runoff in fuhe river basin china the results show that bma post processing approach can improve the performance of runoff prediction and obtain a more accurate prediction probability density function over a range of lead times from 24 to 120 h yan and moradkhani 2016 proposed an extreme flood information estimation method considering the uncertainty of distribution and model structure using the bma method they found that the main uncertainty in extreme flood forecasting is determined by the model structure liu and merwade 2018 established a robust deterministic water level prediction and prediction distribution by combined the different hydraulic models the results show that the bma method provides consistent and reliable basin flood level prediction although it does not always outperform the best model as indicated earlier the bma provides a more robust method for the flood prediction the bma technique can not only quantify the uncertainty of the model through a priori probability and a posteriori probability but also weighted average the model according to a posteriori probability its core idea is to assign a weight to each model according to the performance of the model during training liu and merwade 2018 the advantage of this approach is that bma provides a model weighted average prediction of the interested variable rings et al 2012 however to the authors best knowledge literature about the bma method to study the urban flood simulation and prediction is quite limited and no study has applied the bma and data driven approach to the prediction of local urban flooding in summary this study attempts to establish a comprehensive urban flood prediction model using bma and data driven methods and explore the application of data driven model using bma as a fusion based method to predict urban flood depth at different waterlogging points the main objectives that are addressed in this study are to i construct a comprehensive prediction model by bma and three machine learning methods support vector machines svm back propagation neural network bpnn and adaptive boosting adaboost ii analyze the performance of bma and single model in urban flood prediction from the perspectives of model efficiency accuracy and stability and iii use rainfall forecast data to drive bma model for fine early warning of urban flood the innovation of this study is to establish an accurate and stable prediction model of urban flood by using bma and data driven model the research results can provide new ideas for urban flood prediction as well as provide a scientific basis for urban flood control and early warning 2 materials and methods 2 1 study area zhengzhou city the capital of henan province in china was selected for this study fig 1 as of the end of 2020 the permanent population of zhengzhou exceeded 12 million and the total gross domestic product gdp reached 1 20 trillion yuan the annual average precipitation in this region is 639 5 mm however the distribution of precipitation is very uneven and the interannual variation is large the precipitation in the flood season june to september accounts for about 60 of the annual precipitation in the year of 2018 2019 and 2021 serious urban floods occurred successively in this region especially the urban flood event in july 20 2021 which caused 380 deaths and a direct economic loss of 40 9 billion yuan the urban flood disaster has become a prominent problem affecting the sustainable and healthy development of zhengzhou 2 2 data in this paper the rainfall and inundation data of 18 historical rainfall events in zhengzhou from 2016 to 2018 were selected for inundation process simulation to evaluate the performance of bma model among them three randomly selected rainfall events with different rainfall intensity and duration were used as test data and the other 15 rainfall events were used as training data and the rainfall forecast data of two different types of rainfall events from 2019 to 2020 were used for early warning the detailed data were described as follows rainfall data rainfall data refer to the rainfall process data with 10 min time resolution observed and recorded by 16 rainfall stations in zhengzhou which are june 4 june 23 august 4 august 25 and september 12 in 2016 may 22 june 22 july 18 august 12 august 25 and august 30 in 2017 may 15 july 4 july 13 july 27 august 10 september 15 and september 25 in 2018 the rainfall process data of each waterlogging point were obtained by interpolating the rainfall process data of the rainfall station by using gis and kriging interpolation method the rainfall data of the rainfall station were collected from the meteorological department of zhengzhou city waterlogging data waterlogging data refer to the water depth of waterlogging points in historical waterlogging events which mainly from the monitoring equipment of waterlogging at each road intersection the water depth data of 27 typical waterlogging points with 1 min time resolution were collected from the urban management department of zhengzhou in order to be consistent with rainfall data the time resolution of waterlogging data was set to 10 min the waterlogging data of 18 rainfall events from 2016 to 2018 correspond to the above rainfall data are used as the training and verification data of the model the water depth data of rainfall events on august 1 2019 and august 7 2020 were used as the verification data of early warning results rainfall forecast data rainfall forecast data is basis of urban flood early warning the rainfall forecast data of two rainfall events in august 1 2019 and august 7 2020 were obtained by calling the api of the meteorological technology company caiyun technology the rainfall forecast data have a temporal resolution of 1 min and a spatial resolution of 1 km in order to unify the time resolution the time resolution of rainfall forecast data was changed to 10 min and the forecast period of rainfall forecast data is from 0 to 120 min 2 3 data driven approach for the purpose of modeling inundation process prediction model for each waterlogging point three data driven models with different structures and complexities including svm bpnn and adaboost were selected svm is a supervised machine learning algorithm the core idea is to use the kernel function to find the optimal hyperplane by mapping data to a high dimensional feature space yu et al 2008 bpnn is a feedforward neural network which is widely used in data mining it repeatedly modifies the weight of each layer through two processes of forward propagation and back propagation to train the model chen et al 2010 adaboost is an ensemble learning algorithm which was usually selected the decision tree as the basic learner the core idea is to update the sample weight value by using the error of the previous iteration during each iteration taherkhani et al 2020 in order to ensure the consistency of model comparison the parameters of the above three data driven models were calibrated by monte carlo sampling mean absolute error mae and nash sutcliffe efficiency coefficient nse were selected to evaluate their accuracy and performance the results using single model are the basis of the bma model therefore the above data driven models were used to construct the bma model for inundation process prediction in this study however the modeling process of using data driven model will not be introduced in detail in this study since the purpose and core of this paper is to evaluate the performance of bma model in the local urban flooding prediction a complete mathematical description and modeling process using data driven approach can be found in the previous research wu et al 2020a 2020b 2 4 basic principle of the bma algorithm assuming that y is the simulation variable of multi mode d y 1 y 2 y t is the measured inundation process data required to calibrate the model f f 1 f 2 f k represents the model space composed of svm bpnn and adaboost p k y f k d is the posterior distribution of simulation variable y given measured data d and model according to the law of total probability the posterior probability of simulation variable y in bma can be expressed as 1 p y d k 1 k p f k d p k y f k d where p f k d is the posterior probability of the k th model f k given the measured data w k in other words w k is the weight of each model in bma k 1 k w k 1 the predicted value of bma was obtained by the weighted average of each data driven model assuming that the predicted value and measured value of each model obey the normal distribution the expectation and variance of the bma model can be expressed as 2 e y d k 1 k p f k d e p k y f k d k 1 k w k f k 3 v a r y d k 1 k f k k 1 k w i f i 2 k 1 k w k σ k 2 em expectation maximization algorithm is an effective method to calculate bma which requires that the prediction results of data driven models obey normal distribution raftery et al 2005 in this study the measured and model simulation sequences were normalized by the box cox function in matlab based on this the probability distribution parameters w k and σ k 2 of simulation variable y in bma were calculated using the em algorithm assuming that θ w k σ k 2 k 1 2 k is the parameter to be solved of the bma model the log likelihood function can be expressed as 4 l θ ln p y d ln k 1 k w k p k y f k d the em algorithm is iterative and alternates between two the e step and the m step in step e latent variable is estimated according to the current guess of the parameters in step m θ is estimated according to the current value of the latent variable the em algorithm iterates repeatedly through expectation and maximization the iteration stops when the error of likelihood obtained by the iteration is less than the allowable error 10 6 in this study the log likelihood function will converge to the local maximum of the likelihood since the log likelihood function increased at each em iteration wu 1983 a complete mathematical and technical description of bma and em algorithm can be found in raftery et al 2005 liu and merwade 2018 2 5 the uncertainty analysis of bma model uncertainty of the model reflects the stability of the prediction performance after the weight ω k and variance σ k 2 of the bma model were obtained using the em algorithm the uncertainty interval of the prediction value at any time twas deduced using the monte carlo method hammersley and handscomb 1964 a large number of sampling and gradual approximation are the core idea of monte carlo method when the sampling times are large enough the deduced results gradually approach the real results stably therefore the sampling time was determined to 10 000 at every time node of inundation process in this study and 95 confidence interval of the bma model was deduced as shown in fig 2 the estimated value y ˆ at any time t was calculated by sampling method and arrange all the estimated values y ˆ in size the 95 confidence interval of bma model is the middle part of the quantile of 2 5 and 97 5 in addition the monte carlo method was also used for single models and the sampling times were also set to 10 000 times the 95 confidence interval of the predicted value was deduced according to the probability distribution of each model 2 6 the evaluation statistics of bma three types of indicators are considered to evaluate the performance of bma algorithm in this study in the first type the mae and nse were selected to evaluate the performance of the model in validation periods eqs 5 6 since the bma approach produces both deterministic prediction and probabilistic prediction the smaller the mae and the closer the nse is to 1 indicating that the model accuracy and performance is better 5 m a e i 1 n x i y i n 6 n s e 1 i 1 n x i y i 2 i 1 n x i y i 2 where x i is the predicted value y i is the measured value and y is the average of the measured value in the second type overall accuracy oa overestimation rate or and underestimation rate ur were used to evaluate the overall accuracy of early warning results eqs 7 9 specifically oa is the proportion of correct samples in all samples of early warning results ur refers to the proportion of samples with early warning level lower than the observed waterlogging level in contrast or refers to the proportion of samples with early warning level higher than the observed waterlogging level oa or ur 1 7 o a n 1 4 m 1 4 x m n n m n 8 u r n 1 4 m 1 4 x m n n m n 9 o r n 1 4 m 1 4 x m n n m n where m is the predicted waterlogging level n is the measured waterlogging level n refers to the total number of samples and x m n refers to the number of samples that the measured m level is predicted as level n in addition precision and recall are selected to evaluate the accuracy of early warning results at each level eqs 10 11 precision is the proportion of true positive samples in the prediction samples recall refers to the proportion of true positive samples in all positive samples faceli 2011 10 pr e c i s i o n t p t p f p 11 re c a l l t p t p f n where tp is the number of samples correctly classified as positive fp is the number of samples incorrectly classified as positive because the right category is negative and fn is the number of samples incorrectly classified as negative because the right class is positive faceli et al 2011 when calculating the precision and recall of early warning results tp fp and fn need to be determined first on this basis the precision and recall of early warning results at each level were calculated by using eq 10 and eq 11 finally the overall precision and recall of early warning results were obtained by weighted averaging the weight was determined according to the number of waterlogging points under each level the precision and recall of each early warning level 3 results and discussion 3 1 the characteristics of rainfall and waterlogging data 3 1 1 the characteristics of rainfall data in this study the rainfall data of 18 rainfall events from 2016 to 2018 were selected as the training data of the ponding prediction model from the perspective of precipitation fig 3 the precipitation of the 18 urban flood events was distributed between 38 mm and 105 mm of which the rainfall events over 70 mm account for 83 3 of the total rainfall events the rainfall duration was distributed between 170 min and 290 min and the location coefficient was distributed between 0 19 and 0 77 among them the rainfall events with the location coefficient lower than 0 5 account for 61 1 of the total rainfall events therefore although the rainfall events with small location coefficient less than 0 5 and large precipitation more than 70 mm account for the majority of the total rainfall events the selected rainfall events basically cover different types of rainfall which can provide more comprehensive sample data for the construction of ponding prediction model in addition the average precipitation of 18 urban flood events reached 79 4 mm indicating that large precipitation was one of the main reasons for urban flood furthermore it can be easily found that the precipitation of the 1st 14th and 16th urban flood events did not exceed 45 mm fig 3 and the rainfall duration of three urban flood events were no more than 190 min indicating that the short term heavy rainfall with a small total precipitation was also one of the reasons for urban flood 3 1 2 the characteristics of waterlogging data fig 4 a reflects the water accumulation characteristics of 27 waterlogging points in 18 rainfall events the maximum depth and average depth of water accumulation in the 27 waterlogging points fluctuate greatly the maximum depth is between 0 15 m and 0 68 m and the average depth is distributed between 0 11 m and 0 55 m among them the water accumulation at the waterlogging point of number 10 is the smallest the water accumulation at the waterlogging points of number 1 16 and 26 is very serious and the maximum depth is more than 0 5 m urban flood control should pay more attention to these waterlogging points with serious water accumulation in contrast the minimum depth of water accumulation at these waterlogging points fluctuates less ranging from 0 05 m to 0 19 m the main reason may be that it is difficult to form serious water accumulation at these waterlogging points under relatively small precipitation which makes the water accumulation depth show a small difference in general the water accumulation depth of the waterlogging points in the 18 rainfall events is between 0 05 m and 0 68 m covering different degrees of waterlogging events which can provide more comprehensive waterlogging sample data for the construction of urban flood prediction models in order to analyze the water accumulation characteristics of 27 waterlogging points under similar rainfall conditions the water depth of each waterlogging point under the 2nd 7th and 18th rainfall events with similar rainfall rainfall duration and location coefficient was analyzed as shown in fig 4 b under similar rainfall conditions the average water depth of each waterlogging point was between 0 08 m and 0 44 m and the number of waterlogging points with more than 0 2 m water depth exceeded 88 it shows that although most waterlogging points have accumulated water there are significant differences in the severity of the accumulated water the main reason is that the topography land use and drainage conditions of each waterlogging point were quite different the waterlogging points in sunken terrain such as tunnels and overpasses are more likely to form serious waterlogging this spatial difference also provides some reference for urban flood control which should pay more attention to these waterlogging points with serious waterlogging in addition these spatial differences of waterlogging data also can provide more comprehensive waterlogging sample data for the construction of urban flood prediction models 3 2 validation of bma prediction performance 3 2 1 the overall performance of bma three randomly selected rainfall events with different location coefficients and rainfall duration from 2016 to 2018 were selected as verification events to evaluate the performance of bma model which were august 25 in 2016 with 0 39 location coefficient and 250 min rainfall duration august 30 in 2017 with 0 70 location coefficient and 220 min rainfall duration july 27 in 2018 with 0 16 location coefficient and 280 min rainfall duration the root mean square error rmse was selected as the objective function the prediction sequence results of bma model were obtained by em algorithm in addition the performance of bma model was analyzed from the running time and accuracy of the model in terms of model running time the average running time of bma and three single models were 5 6 s bma 1 6 s bpnn 4 2 s svm and 0 6 s adaboost it can be seen that bma model has the longest running time the main reason is that bma needs to couple multiple models for prediction so that the running time of bma model contains the running time of single model however although the calculation efficiency of bma is lower than single model the running time of 5 6 s will hardly affect the prediction period of flood warning which is also a significant advantage of data driven model in calculation efficiency fig 5 shows the overall accuracy and difference between the bma model and the single model the prediction results of svm bpnn and adaboost were greater than 0 94 indicating that these models have good prediction performance in local urban flooding prediction among them svm has the highest nse and the lowest mae showing the best prediction effect in contrast adaboost has a lower nse and a higher mae showing the worst performance the main reason was that the limited sample data make it difficult for adaboost to give full play to its advantages and svm also performs well in the limited sample making the performance of svm in local urban flooding prediction slightly higher than adaboost in addition although both the single model and bma model have achieved good results in local urban flooding prediction the bma model has further improved the prediction accuracy the mae of the local urban flooding prediction in bma is 36 46 lower than that of svm bpnn and adaboost specifically the mae in bma is less than 0 02 m 0 019 m but the mae in svm bpnn and adaboost are more than 0 03 m in addition the nse of the bma model is also significantly higher than svm bpnn and adaboost therefore the above results indicate the overall accuracy of the bma model is significantly better than the single model svm bpnn and adaboost in the local urban flooding prediction fig 6 demonstrates the performance between bma model and single model in different types of rainfall events bpnn has the highest prediction accuracy in the verification event 1 and 2 but adaboost shows higher accuracy in the verification event 3 which indicates that it is difficult for the single model to maintain the highest prediction accuracy in all rainfall event in contrast the bma model always has the lowest mae and the highest nse in three different types of rainfall events which shows that the stability of bma model is significantly higher than svm bpnn and adaboost in addition fig 6 showed that the prediction performance of adaboost in different verification events were quite different although it has better prediction performance than svm and bp in the verification event 3 the prediction performance in the first verification event was significantly lower than other models on the contrary the prediction performance of svm and bp was relatively stable the main reason may still be that the amount of rainfall and waterlogging data was not large enough making it difficult for adaboost to fully explore the potential relationship between rainfall and waterlogging data 3 2 2 the performance of bma in each waterlogging point in order to verify the robustness of bma model the prediction performance of the bma and single model in each waterlogging point was analyzed it should be pointed out that the characteristics and number of waterlogging points under different rainfall events are uncertain therefore the waterlogging points in the three verification events may have certain differences which may cause interference to the model performance analysis therefore this study selects 27 identical waterlogging points in three validation events to analyze the prediction performance of the model in order to eliminate the impact of changes in waterlogging points on the performance analysis as shown in fig 7 the mae fluctuation range of bma model is between 0 and 0 030 m which is 34 8 68 1 lower than that of svm 0 009 0 055 m bpnn 0 010 0 056 m and adaboost 0 012 0 106 m and the lowest nse value of bma model 0 945 is 26 0 33 1 higher than that of svm 0 74 bpnn 0 75 and adaboost 0 71 these results show that the prediction accuracy of bma model is significantly better than that of single model svm bpnn and adaboost in the first validation event similarly in the second and third validation events the lowest nse values of bma model were 0 87 and 0 922 which were 14 4 72 7 and 15 0 61 8 higher than that of single model and compared with the single model the mae fluctuation range of bma model also decreased by 29 1 48 4 these results indicate that the bma model not only has higher prediction accuracy in different waterlogging points but also has stable prediction performance however the prediction performance of bma is not the highest in all cases for example the nse of bma 0 95 is lower than that of bpnn model 0 98 in the waterlogging point of number 21 the main reason is that the results of bma were obtained by weighted averaging the results of single models therefore when the local error of single model is large the prediction accuracy of the bma model may be lower than that of the single model with the highest accuracy nevertheless the prediction performance of the bma model is significantly better than that of the single model in most cases which can provide more stable urban flood prediction in order to deeply explore the potential relationship between the model performance and different types of waterlogging points three different types of waterlogging points 16 23 and 26 were selected from 27 waterlogging points to compare the prediction performance by reflecting the characteristics of the waterlogging points among them the waterlogging point of number 26 was located near the drainage outlet at the bottom of the tunnel the waterlogging point of number 16 was located near the road intersection and the waterlogging point of number 23 was located at the edge of the auxiliary road under the elevated road as shown in fig 8 from the perspective of water accumulation characteristics the water depth at the waterlogging point of number 26 is the largest and the water accumulation process line shows the characteristics of steep increase gentle and then slow decrease the main reason is that the waterlogging point of number 26 is located at the underpass tunnel due to the influence of the microtopography of the underpass tunnel the rainfall converges rapidly after reaching the surface resulting in the rapid increase of the water depth and the water depth at the waterlogging point of number 16 is the smallest the reason may be that the terrain at the road intersection is relatively flat the lower surface confluence speed and the flat terrain make the water depth smaller in addition it is precisely because of this terrain that the water accumulation curve is relatively gentle from the perspective of prediction performance the prediction performance of single model shows great uncertainty for example the nse of adaboost model at 16 and 23 waterlogging point is greater than 0 96 but at 23 waterlogging point is only 0 943 similarly the nse of bpnn at 26 and 23 waterlogging point is greater than 0 96 but at no 16 waterlogging point is only 0 924 in contrast the prediction results of bma model in three different types of waterlogging points are 0 984 26 0 971 16 and 0 982 32 showing high accuracy and stable performance therefore it can be concluded that bma model has stable prediction performance in the prediction of different types of waterlogging points and can effectively improve the prediction effect of single model 3 3 the inundation characteristics based on the bma model the rainfall data under different rainfall return periods with 180 min rainfall duration once in half a year once a year once in two years once in five years once in ten years once in twenty years and once in fifty years wu et al 2020a were obtained using the classic chicago rainstorm method and the rainstorm intensity formula in zhengzhou city shao and liu 2018 eq 12 then the local urban flooding prediction results of bma model under different rainfall return periods were obtained by inputting rainfall data into the model the polynomial fitting and cluster method were used to analyze the trend and characteristics of water depth under different rainfall return periods specifically the increasing trend of water depth under different rainfall return periods was determined by using the polynomial fitting method and the trend of 27 waterlogging points was clustered into four categories by cluster analysis 12 i 40 1 1 0 794 lg p t 25 8 0 948 where i is the intensity of rain p refers to the rainfall return period t is the duration of rain as shown in table 1 and fig 9 including 12 waterlogging points in category 2 accounting for 44 4 of the waterlogging points in this study the inundation characteristics of these waterlogging points are that the water depth is approximately linear with rainfall and there are 7 waterlogging points with the water depth of more than 0 3 m and 11 waterlogging points with the water depth of more than 0 2 m under the rainfall return period of 10a in category 3 including 5 waterlogging points showing growth trend of gradually decrease in the water depth the water depth exceeds 0 3 m in 10 years rainfall return period and 0 4 m in 50 years rainfall return period however the growth trend of water depth in category 4 including 4 waterlogging points is gradually increasing which is opposite to the inundation characteristics of category 3 under the rainfall return period of 50 years the water depth is more than 0 4 m in all waterlogging points of category 4 and the water depth is more than 0 6 m in the waterlogging point of no 1 and no 26 which is higher than other waterlogging points on the contrary the water depth of the waterlogging points in category 1 is generally low to be specific there is almost no waterlogging in these waterlogging points under the rainfall return period of 0 5year 1year and 2years the water depth increases gradually when the rainfall return period over 5years however water depth still does not exceed 0 2 m under the rainfall return period of 10 years and there is also only one waterlogging point with the water depth of over 0 3 m under the rainfall return period of 50 a on the whole there are 74 waterlogging points have a water depth of more than 0 25 m when the rainfall return period over or equal to 10 years however there are only 37 waterlogging points have a water depth of more than 0 25 m when the rainfall return period did not exceed 5 years in addition no matter in waterlogging points of category 2 category 3 or category 4 the water depth generally exceeds 0 25 m when the rainfall return period over 10 years especially in the waterlogging points of category 4 the growth trend of water depth is increasing with the increase of rainfall which will threaten the traffic safety of vehicles and pedestrians therefore in urban flood control urban management departments should pay more attention to the rainfall events with a return period of more than or equal to 10 years and the increasing trend of water depth these waterlogging points specifically many effective measures including dredge rainwater wells before the flood season using pumping pumps to improve drainage capacity and cutting off the road in case of serious waterlogging should be taken to ensure the traffic safety of vehicles the safety of pedestrians and reduce the losses caused by flood disasters 3 4 uncertainty analysis of results coverage rate cr and bandwidth b were used to evaluate the uncertainty of bma and single model the cr represents the reliability of the model and it is the proportion of observed results falling within 95 confidence interval the b space between upper interval and lower interval reflects uncertainty of the prediction results and it is the mean width across the inundation process liu and merwade 2018 the narrower the b the higher the cr indicating that the uncertainty of the model is lower in addition the uncertainty of each stage may be significantly different in the prediction of inundation process therefore in order to deeply analyze the uncertainty and accurately identify this difference four typical waterlogging points were selected by stratified sampling method which were 7 c1 16 c2 11 c3 and 1 c4 as shown in table 2 on the premise that the cr of bma is not less than 96 the b of bma is reduced by 41 9 48 1 compared with the single model indicating that bma model has lower uncertainty in addition in the three different validation events bma model always has the highest cr and the narrowest b and the cr of bma is also more than 96 these results demonstrate that bma significantly reduced the uncertainty of the single model which is consistent with the analysis results in section 3 1 figs 10 12 shows uncertainty of the svm bpnn adaboost and bma under different validation events as shown in fig 10 the b of bma model is not more than 0 05 m in the first validation event which is 9 8 61 6 less than that of svm bpnn and adaboost indicating that bma reduces the uncertainty of the prediction results of single model however in the middle period of inundation process although the cr of bma is 100 the b of bma is higher than that in the beginning and end stages of inundation process indicating that the bma shows high uncertainty in the middle period of inundation process in addition it can be found that the b of bma model 0 046 m at the waterlogging point of category 4 is 7 0 24 3 higher than that of other types of waterlogging points indicating that the prediction results of bma model show higher uncertainty at the waterlogging point of category 4 in the second validation event fig 11 the b of bma is between 0 042 m and 0 058 m which is 25 9 56 7 less than that of single model indicating that bma show lower uncertainty than svm bpnn and adaboost under the 95 confidence interval to be specific the prediction results of bma always have the lowest b in all types of waterlogging points and the b of bma across the inundation process is also lower than that of svm bpnn and adaboost in addition the uncertainty of bma has the same characteristics as the verification event 1 in different stages of inundation process in the beginning and end stages of inundation process bma has a narrow interval width while in the middle period of inundation process bma has a wide interval width indicating that bma also shows higher uncertainty in the middle period of inundation process similarly the b of bma model between 0 038 m and 0 059 m is 20 3 61 8 less than that of single model fig 12 indicating that bma significantly reduces the uncertainty of the single model prediction from the characteristics of the interval width under the 95 confidence interval bma has a higher interval width in the middle period of inundation process showing higher uncertainty in addition the prediction results of bma at the waterlogging point of category 4 1 have the highest average bandwidth and higher uncertainty which is similar to the first two verification events therefore uncertainty of the bma shows the similar characteristic in different validation events first of all the interval width under 95 confidence interval of the bma prediction results is 9 8 61 8 less than svm bpnn and adaboost showing that bma can significantly reduce the uncertainty of single model prediction in different types of rainfall events secondly the interval width of bma at the waterlogging points of category 4 is 7 0 55 3 higher than that of the other types of waterlogging points which demonstrate the prediction results of bma have higher uncertainty at the waterlogging points of category 4 in addition the bma forecast model has lower uncertainty at the beginning and end periods of inundation process but it has higher uncertainty at the middle period of inundation process the reason may be that the runoff generation and confluence conditions are clear in the initial inundation stage and the catchment area is relatively stable similarly in the end stage of inundation the confluence at waterlogging point gradually weakens and it is mainly the process of receding water however the confluence characteristics in the middle stage of inundation may be complex for example there may be hydraulic connection with the surrounding pipe network and waterlogging points resulting in the change of confluence conditions these may lead to higher uncertainty in the prediction of middle stage of inundation nevertheless bma has significantly reduced the uncertainty of single model prediction 3 5 urban flood forecasting and early warning 3 5 1 analysis of rainfall forecast data the rainfall forecast data of two rainfall events on august 1 2019 and august 7 2020 were selected as input samples which have a forecast period of 0 120 min and a temporal resolution of 10 min it is well known that the accuracy of rainfall forecast data has great uncertainty which may bring great uncertainty to the output results of the model therefore it is necessary to evaluate the accuracy of rainfall forecast data before using rainfall forecast data to drive the urban flood prediction model as shown in table 3 although there are large uncertainties in the accuracy of rainfall forecast data the accuracy of rainfall forecast data gradually improves with the shortening of the forecast period when the forecast period is shortened to 90 min the mean relative error mre of rainfall forecast data is reduced to 29 2 which can basically meet the accuracy requirements of input data of bma model 3 5 2 the early warning results based on bma by inputting the rainfall forecast data with different lead forecast time into the bma model using python 3 7 the inundation process results of each waterlogging point were obtained the water depth was used to evaluate the urban flood disaster risk of each waterlogging point which is divided into four classes 0 3 cm 3 10 cm 10 25 cm 25 cm on this basis the early warning results of two rainfall events were obtained using gis for example by inputting the rainfall forecast data with 100 min lead forecast time into the model combined with the classification standard and gis the early warning results with 100 min lead forecast time were obtained similarly with the shortening of lead forecast time the rainfall forecast data with 90 min 80 min 10 min lead forecast time were continuously input into the model the early warning results under different lead forecast time were obtained it should be noted that because the running time of bma model 5 6 s hardly affects the forecast period of early warning this study didn t consider the running time in the discussion of forecast time as shown in table 4 with the shortening of lead forecast time the accuracy of early warning results is gradually improved the accuracy of early warning results has increased from 74 0 100 min lead forecast time to 90 7 10 min lead forecast time the main reason for this phenomenon is that the accuracy of rainfall forecast data gradually improves with the shortening of lead forecast time and the improvement of the accuracy of input data will improve the accuracy of the output result then the accuracy of early warning results will be improved it is worth noting that when the lead forecast time is less than or equal to 80 min the precision and recall of the bma model are both exceeding 80 which basically meets the accuracy requirements of early warning what more the lead forecast time of 80 min is basically enough for relevant managers to make full response therefore these demonstrate that this method has great application potential in the early warning and response of waterlogging in addition table 4 and fig 13 also shows the accuracy of bma model at each early warning level the performance of the early warning results gradually improved with the increase of early warning level to be specific the bma model has low precision in the early warning of level 1 68 6 but the precision in the early warning of other levels is exceeding 88 similarly the recall of the bma is from 80 1 to 91 4 with the improvement of early warning level in addition it is obvious that the prediction performance of the bma model for the early warning level of level 4 is higher than the early warning level of other levels indicating that bma model is more suitable for the prediction of serious waterlogging fig 14 reflects the spatial characteristics of waterlogging it is obvious that the areas with serious waterlogging were mainly concentrated in the middle and south of zhengzhou city in the middle area of zhengzhou city the waterlogging degree is obviously high since these areas are old urban areas with low pipe network design standards and aging pipe networks the south area of zhengzhou city is in the stage of rapid development and construction the drainage system of these areas is still under construction in addition the new project will change the layout of the original drainage pipe network therefore these areas are easier to produce serious waterlogging in heavy rainfall events in addition in the rainfall event on august 1 2019 there were 15 waterlogging points with water accumulation degree of more than grade 3 including grade 3 however 11 of the 15 waterlogging points in the rainfall event on august 7 2020 still have a water accumulation degree of more than grade 3 which indicates that although the distribution and inundation of the waterlogging points may be affected by the rainfall event the underlying surface characteristics determine that the risk of waterlogging at these waterlogging points is greater therefore urban flood control should pay more attention to these waterlogging points 3 5 3 comparison of early warning results between the bma and single model the oa ur and or were used to compare the early warning performance of svm bpnn adaboost and the bma model as shown in fig 15 the oa of bma was 90 7 oa but the oa of three single models were no more than 82 indicating that the bma has the higher accuracy than the single model in early warning of local urban flooding in addition it can be found that the early warning results in svm and bpnn have high or 16 7 in svm and 18 5 in bpnn and low ur 1 9 in svm and 5 6 in bpnn indicating that the early warning results were mainly overestimated in contrast the early warning results in adaboost have high ur 13 0 and low or 9 3 which shows that the early warning results were mainly underestimated however the ur 3 7 and or 5 6 of the bma model was not only close but also relatively low therefore the above results and analysis demonstrate that the bma method can be used to improve the accuracy of urban flood prediction and early warning although the early warning results may cause slight overestimation 4 discussion 4 1 influence of rainfall forecast data on early warning accuracy it is generally accepted that the accuracy of rainfall forecast data decreases gradually with the extension of lead forecast time which directly affect the early warning results of the bma model under different lead forecast time as shown in table 4 and fig 13 under different lead forecast time the accuracy of early warning gradually decreases with the extension of the forecast time which is mainly due to the influence of the accuracy of rainfall forecast data nevertheless the influence of rainfall forecast data on early warning accuracy is not clear therefore in order to analyze the impact of rainfall forecast data on early warning accuracy the quantitative impact of rainfall forecast data in different lead forecast time on early warning accuracy was analyzed by using the control variable method specifically by inputting the measured rainfall data into the bma model the prediction results of inundation process were obtained and combined with the early warning classification standard the early warning results under actual rainfall data were obtained these results were used as a control group to analyze the impact of the rainfall forecast data in different lead forecast time on the accuracy of early warning as shown in fig 16 on the premise that the measured rainfall data was used as the input data control group the error of bma model is 6 9 and due to the influence of rainfall forecast data the error of bma model increased by 9 3 26 in addition the error caused by rainfall forecast data increased significantly after the lead forecast time exceeds 50 min therefore although the accuracy of the early warning results within 80 min lead forecast time can be accepted precision 80 the early warning within 50 min lead forecast time is more reliable from the changing trend of early warning results 4 2 comparative analysis with physical model based on three classical machine learning model this study uses bma approach to couple these machine learning models to construct an integrated prediction model in terms of prediction accuracy the nse of the prediction results of the bma and machine learning models are more than 0 94 in terms of calculation efficiency bma model needs the average running time of 5 6 s and in the early warning under different lead forecast time the precision of bma model is more than 80 when the lead forecast time is no more than 80 min similarly the physical model can also predict the inundation process our previous research has used swmm model to simulate the flood events in zhengzhou wang et al 2022 the results demonstrate that the prediction results of swmm model have an error of 16 7 in addition qi et al 2022 established an urban flood simulation model in haikou city by using pcswmm the results show that the nse of the model is 0 844 therefore the physical model can also achieve accurate urban flood prediction and there is no significant difference in accuracy compared with the data driven model in terms of the calculation efficiency the average calculation time of swmm model under three rainfall events 20170706 20160605 and 20160805 was 1 8 min and the calculation time under the rainfall events on july 20 2021 was 10 1 min recent approaches show that the one dimensional surface and drainage pipe network model usually needs computation time ranging from 5 min to 1 h bulti and abebe 2020 and accurate two dimensional hydraulic model usually needs the running time of more than one hour bulti and abebe 2020 chang et al 2015 the main reason for this difference in calculation efficiency is the complexity of physical model and the duration of rainfall event therefore it can be concluded that the approach proposed in this study is superior to the physical model in terms of calculation efficiency however it should be pointed out that the approach proposed in this study is only applicable to the area with sufficient historical urban flood data in other words the data driven method is difficult to be directly applied in the area lacking data in the area lacking urban flood data this approach can be used as a proxy model of physical model to improve the calculation efficiency recent researches kwon and kim 2021 mignot and dewals 2022 demonstrate that data driven method can significantly improve the computational efficiency of physical model on the premise of ensuring the accuracy in addition the physical model can also be used as the benchmark model of bma introducing physical model into bma model for inundation process prediction and early warning needs to meet two basic conditions one condition is that the physical model can accurately identify the location of waterlogging points and accurately simulate the inundation process of urban flood another condition is that the calculation time of physical model should not be too long therefore future research can focus on how to introduce physical model into bma model on the premise of ensuring accuracy and computational efficiency 4 3 the generalization ability of bma model on july 20 2021 an extreme rainstorm occurred in zhengzhou with a total rainfall of 624 1 mm within 24 h by inputting the rainfall data into the bma model the inundation process was obtained which aims to verify the applicability of the bma model under the extreme rainfall event as shown in fig 17 the trend of inundation process predicted by bma model is consistent with the measured process however the maximum depth of inundation process is underestimated which is 26 1 lower than the actual value and the predicted inundation process is 1 h earlier than the measured inundation process the peak time is 1 h later than the actual inundation process the end time is 1 h earlier than the actual inundation process the one reason for this error is that the bma model does not contain sufficient sample data specifically although the return period of rainfall events in the sample data set has reached once in a hundred years the return period of the rainfall event on july 20 2021 has reached once in a thousand years which seriously exceeds the intensity of all rainfall events in the sample data set therefore due to the limitation of sample data set the accuracy of bma model in predicting extreme rainfall events has decreased an important way to solve this problem is to continuously enrich the sample data set to improve the generalization ability of the model based on this this study collected the data of the extreme rainfall event in zhengzhou on july 20 2021 and imported it into the sample data set which aims to improve the generalization ability of bma model under extreme rainfall events in addition another reason for the decrease in the model prediction accuracy is that the hydraulic connection at the waterlogging point has changed significantly to be specific the increase of rainfall may increase the catchment area greatly and the basin flood will also effects on urban flood therefore in the extreme rainfall event basin flood and urban flood should be considered comprehensively which is also one of the important development directions of urban flood research in the future 5 conclusion bma is a multi model weighted average simulation method integrating the results of various models in this paper the bma was applied to the prediction and early warning of urban flood the main conclusions are as follows 1 in the prediction of local urban flooding bma can improve the accuracy and stability of the prediction results compare with three single models the bma algorithm has the lowest mae 0 019 and the highest nse 0 984 in the prediction of local urban flooding in addition although bma does not always have the highest accuracy in all rainfall events and all waterlogging points the overall predictive performance and stability of the bma model is better than the single model in different rainfall events and waterlogging points which demonstrates that bma can provide a more accurate and stable method for urban flood prediction 2 analysis indicates that urban management departments should pay more attention to the rainfall event with the return period of more than or equal to 10 years and the waterlogging points of category 2 category 3 or category 4 there are 74 waterlogging points have a depth of more than 0 25 m when the rainfall return period over or equal to 10 years especially in the waterlogging points of category 4 the average depth is 0 34 m under the rainfall return period of 10 years and the water depth shows an exponential growth trend with the increase of rainfall 3 among the three verification events of different types bma always has the highest cr and the narrowest b in different stages of local urban flooding bma has lower uncertainty in the beginning and end stages but in the middle period bma has higher uncertainty nevertheless the bma method has reduced the uncertainty of local urban flooding prediction to a certain extent 4 in the fine early warning of local urban flooding using rainfall forecast data the early warning result of bma model has the overall accuracy of 90 7 which is significantly higher than that of svm 81 5 bpnn 75 9 and adaboost 77 8 it shows that bma can provide a more accurate and stable method for fine early warning of urban flood however there are still some directions for further exploration in this study for instance although bma provides a more stable and accurate local urban flooding prediction and early warning method this approach can only predict urban flood in the waterlogging points with detailed observation data therefore future research can combine hydrological model data driven methods and the bma to explore comprehensive and refined urban flood prediction and early warning methods credit authorship contribution statement yihong zhou data curation methodology validation writing original draft writing review editing zening wu methodology validation writing review editing supervision funding acquisition hongshi xu methodology huiliang wang methodology supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research was funded by the key project of national natural science foundation of china no 51739009 excellent youth fund of henan province of china 212300410088 science and technology innovation talents project of henan education department of china 21hastit011 young backbone teachers training fund of henan education department of china 2020ggjs005 national natural science foundation of china 52109040 and china postdoctoral science foundation 2021m702950 we thank the meteorological department and administration bureau of zhengzhou city for providing rainfall and waterlogging data we also thank the anonymous reviewers for their valuable comments 
