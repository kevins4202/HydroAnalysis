index,text
25710,the universal soil loss equation usle has been the de facto standard for soil erosion management studies since its seminal publication in the 1970 s widespread use of the model is due in part to its simple empirical modelling structure and parsimonious parameterization however these benefits have also led to criticism of its relevance as a tool for estimating soil erosion while the usle has a strong empirical basis it is regularly used beyond its intended design space i e predicting soil loss from planar hillslopes to predict distributed soil erosion rates at large spatial extents which introduces uncertainty in model outcomes in this paper we use a case study for up scaling the usle to a large spatial extent to assess the variability in model outcomes from different model user s design choices our analysis demonstrates that a standardized and accredited methodology for up scaling the usle is needed to reduce uncertainty in model outcomes keywords universal soil loss equation erosion watershed uncertainty soil degradation gis 1 introduction the roots of modern soil erosion modelling originated in the american midwest during the early 1900 s e g 1917 missouri agricultural experiment station the detrimental impacts of mechanized agriculture on soil erosion and agricultural productivity were brought to the attention of the american congress by hugh hammond bennett who secured funding in 1929 for establishing ten experimental erosion plots meyer and moldenhauer 1985 the advocacy of bennett to the american congress the 1930 s dust bowl in the great plains of north america and the subsequent widespread crop failure collectively influenced the american congress to pass the soil conservation act of 1935 public law 74 46 the act states that it is hereby recognized that the wastage of soil and moisture resources on farm grazing and forest lands of the nation resulting from soil erosion is a menace to the national welfare and that it is hereby declared to be the policy of congress to provide permanently for the control and prevention of soil erosion public law 74 46 this act provided national funding for soil erosion research resulting in the first conceptualizations of erosion models e g zingg 1940 smith 1941 musgrave 1947 and most prominently culminated with the development of the universal soil loss equation usle wischmeier and smith 1965 the usle is a lumped empirically based soil erosion model that was developed by the united states department of agriculture and soil conservation services first published in agricultural handbook no 282 wischmeier and smith 1965 and widely adopted based on the superseding publication in 1978 no 537 wischmeier and smith 1978 the usle is the culmination of over 10 000 plot years of erosion measurements spanning several decades the basis of the usle is a unit plot represented by a small fallow agricultural plot 22 1 m long x 1 8 m wide with a 9 slope gradient and an up down slope tillage pattern the soil loss of other experimental erosion plots e g slopes 3 18 slope lengths 9 91 m were described relative to these reference conditions the usle encapsulates a representation of the erodibility of an agricultural hillslope relative to the conditions of the unit plot using six empirically derived factors wischmeier and smith 1978 a r k l s c p where a is the annual soil loss per unit area t ha 1 yr 1 r is a rainfall and runoff factor mj mm ha 1 h 1 yr 1 k is a soil erodibility factor t ha h ha 1 mj 1 mm 1 l is a slope length factor unitless s is a slope steepness factor unitless c is a cover and management factor unitless and p is a supporting practice factor unitless wischmeier and smith 1978 soil loss in the usle is conceptualized as soil loss from an agricultural hillslope resulting from rill and interrill erosion since the seminal publication of 1978 wischmeier and smith 1978 the usle has been revised rusle renard et al 1991 and succeeded by rusle2 and the process based water erosion prediction project wepp laflen et al 1991 the successors of the usle and new models developed in the late 20th century sought to overcome the inherent empirical limitations of the usle and extend the applicability of soil erosion models by representing sediment conveyance and depositional processes e g laflen et al 1991 these modelling developments can be classified into two classes of erosion models 1 hybrid models that couple the usle factors with a sediment transport model e g soil and water assessment tool arnold et al 1998 areal nonpoint source watershed environment response simulation beasley et al 1980 or 2 process based models that are independent of the empiricisms of the usle e g lisem de roo et al 1996 wepp despite scientific efforts to derive new process based models or improve upon the usle the usle and revised usle still remain the de facto standards for management oriented soil erosion studies alewell et al 2019 both as a standalone tool and via incorporation into hybrid models the usle has been operationalized for ease of use with geographic information systems gis to up scale the model from predicting soil loss for individual hillslopes to predicting soil erosion at national e g italy grimm et al 2003 switzerland prasuhn et al 2013 continental e g europe panagos et al 2015a and global e g borrelli et al 2017 scales among these large scale applications the usle has been used as a decision support tool for soil erosion prevention e g prasuhn et al 2013 a technical support tool for sustainable development e g van der knijff et al 1999 grimm et al 2003 and for quantifying the severity of soil degradation from soil erosion processes globally e g fao 2015 models applied at large spatial extents typically forgo complicated process descriptions which result in higher intrinsic model error but low model input error rompaey and govers 2002 the simple structure and parsimonious parameterization of the usle meets this criterion and has driven its widespread use from small i e plot field to large watershed national global spatial extents however if model applications across large spatial extents are the result of extending a model beyond its designed application space then additional uncertainty is introduced into modelling outcomes despite the usle being designed for predicting soil loss from planar hillslopes its implementation in combination with a gis has taken vastly different methodological approaches e g fistikoglu and harmancioglu 2002 amore et al 2004 erdogan et al 2007 pandey et al 2007 dabral et al 2008 hui et al 2010 devatha et al 2015 belasri and lakhouili 2016 rizeei et al 2016 singh and panda 2017 for up scaling the usle to large spatial extents with little to no acknowledgement of the different types of error and uncertainty or their quantification in the up scaling process the predictive accuracy of an erosion model can be conceptualized as comprising four components 1 intrinsic model error 2 model input error 3 model user error and 4 stochastic error intrinsic model error is the error inherent to the model as a result of a simplified modelling structure which manifests itself when a model developer chooses to forgo a parameter that affects soil erodibility e g hydraulic conductivity in favor of a more parsimonious modelling structure e g soil texture model input error is the error derived from poorly measured or estimated model inputs which typically manifests itself when remote sensing products are used in lieu of detailed in situ measurements model user error is a result of an incorrect application or parameterization of a model lastly the stochasticity of coupled human natural systems e g variance in replicate plots wendt et al 1986 can result in variation among erosion estimates that cannot practically be modelled i e stochastic error the total error of a model s prediction is the sum of these four error sources all usle predictions of soil erosion rates will carry some amount and combination of intrinsic model error model input error model user error and stochastic error however when up scaling the usle from the hillslope scale to larger spatial extents model input and user error are of particular concern since data constraints design choices and spatial conceptualizations of a system will invariably influence model outcomes error in erosion predictions can be evaluated for studies conducted at larger spatial extents by comparing modelled and measured soil erosion rates on a subset of farm fields within a watershed e g favis mortlock 1998 or when there is comprehensive erosion data collected for the watershed e g jetten et al 1999 since comprehensive erosion data are typically not available and validating the outcomes of spatially distributed erosion rates is complex we focus our discussion on the variability in usle model outcomes rather than error or uncertainty to exemplify the variability in usle model outcomes we conduct a case study for up scaling the usle in the upper nith watershed ontario canada we compare our recommended modelling approach with other common modelling approaches in literature to answer the following question what is the variability in model outcomes from different model user s design choices for up scaling the usle to large spatial extents 2 materials and methods 2 1 study site our case study is situated in the upper reaches of the nith watershed in ontario canada the upper nith watershed is an agricultural watershed characterized by row cropping of corn soybean winter wheat and alfalfa in 2015 farm fields covered 39 708 ha 73 of the watershed with an average farm field size of 8 46 ha an average farm field slope of 1 8 and a standard crop rotation of corn soybean and winter wheat an extensive tile drainage system covers the upper nith watershed appendix b which drains 22 660 ha of farm fields 57 of farmland soils in the northern half of the upper nith watershed are composed of clay loam and loam contrasted by a mosaic of soils in the southern half of the watershed which are primarily loam and silty loam the annual precipitation for the upper nith watershed is 958 3 mm yr 1 2005 2014 wellesley dam meteorological station with snow melt occurring intermittently throughout the winter months and during the early spring the intermittent snow melts and spring freshet result in a high rate of discharge in the upper nith river throughout the winter and early spring months with 76 82 of annual discharge occurring from november to april and the remaining 23 18 of annual discharge occurring during the warmer growing season may to october 2005 2014 environment canada hydrometric station 02ga018 the sediment yield of the upper nith river correlates well with discharge a linear model relating sediment yield samples n 41 environment canada station 16018403202 appendix b with discharge r2 0 47 estimated that 87 14 of sediment export occurred during the non growing season november to april and the remaining 12 86 of sediment export occurred during the growing season may to october the 10 year average sediment yield of the upper nith watershed was calculated at 22 131 35 t yr 1 2005 2014 0 41 t ha 1 yr 1 with a more recent 5 year average of 19 943 07 t yr 1 2010 2014 0 37 t ha 1 yr 1 the upper nith watershed has consistently experienced elevated levels of suspended sediments and nutrients n and p relative to the other sub watersheds of the grand river loomer and cooke 2011 nutrient concentrations measured at the outlet of the upper nith watershed of unfiltered p environment canada station 16018403202 have exceeded water quality guidelines guideline value is 0 03 mg l 1 to prevent eutrophication environment canada 2004 in 98 of samples n 47 between 2005 and 2014 elevated levels of suspended sediments and p can be associated with eroded agricultural sediments making the upper nith watershed ideal for a case study on agricultural soil erosion and soil loss 2 2 erosion modelling 2 2 1 soil erosion usle our recommended modelling approach implements the usle as outlined in agricultural handbook no 537 to estimate a 5 year average erosion rate 2010 2014 for the upper nith watershed while most usle studies use a raster based model implementation our recommended modelling approach uses a polygon based discretization for calculating and summarizing results whereby each polygon is representative of an individual farm field and was manually digitized using 2015 southwestern ontario orthoimagery project swoop airborne imagery with a fixed scale of 1 4000 fig 1 each farm field was assigned a static k and ls factor for our study period an r factor for each year and a c factor for each cropstage the six usle factors in our recommended modelling approach were calculated as follows rainfall and runoff r the r factor reflects the impacts that rainfall and runoff have on water erosion the r factor is a summation of the total kinetic storm energy times the maximum 30 min rainfall intensity for all rainstorms in a given year p 5 wischmeier and smith 1978 r i 1 m e i 30 i where r is the annual rainfall and runoff factor mj mm ha 1 h 1 yr 1 m is the total number of storms in a year ei30 is the rainfall erosivity of a single storm event i mj mm ha 1 hr 1 calculated as the total kinetic energy of rainfall e mj ha 1 times the maximum 30 min rainfall intensity i30 mm hr 1 storms are only included in r calculations that exceed 12 7 mm of precipitation while events smaller than 12 7 mm in size can produce a runoff response if they occur on wet antecedent conditions most erosion and nutrient loss events are associated with high magnitude rainfall events e g macrae et al 2007a we calculated the rainfall erosivity of each storm event using hourly rainfall data collected from the wellesley dam meteorological station the meteorological station is located in the central portion of the watershed and we assumed that the spatial distribution of rainfall was uniform over the entire watershed since the minimum requirement for calculating the i30 of each storm is 30 min rainfall data and we only had hourly rainfall data we used a relationship developed by panagos et al 2015b to convert from 60 min intensities to 30 min intensities r30 min 1 5597 x r60 min this conversion was found to be a reasonable approximation for annual predictions by meinen and robinson 2021 for a field scale usle case study in the watershed albeit with a poor characterization of the most intense rainfall events we only recommend using this empirical scaling approach in the absence of 30 min rainfall data since it may be a substantial source of uncertainty in modelling outcomes our 5 year average r factor 1923 mj mm ha 1 h 1 yr 1 related very well to the long term average r factor calculated by the ontario ministry of agriculture and rural affairs 1864 mj mm ha 1 h 1 yr 1 omafra 2012 soil erodibility k the k factor represents the susceptibility of different soil types to water erosion p 10 wischmeier and smith 1978 100 k 2 1 m 1 14 10 4 12 a 3 25 b 2 2 5 c 3 0 1317 where k is the soil erodibility factor t ha h ha 1 mj 1 mm 1 m is the soil particle size parameter based on soil texture a is percent organic matter b is the soil structure code and c is the profile permeability class to calculate the k factor the spatial location of soil textural classes m and other edaphic characteristics a b c were selected from the ontario soil survey complex polygon 1 50 000 scale omafra 2019 the ontario soil survey complex polygon was rasterized and the k value for each farm field polygon was calculated as the average of the k factor raster within the polygon slope length factor l the l factor represents the effect that the length of a hillslope has on water erosion the usle l factor was developed to be applicable to planar hillslopes i e the shape of the experimental erosion plots and does not have any provision for complex topography p 12 wischmeier and smith 1978 due to this simplification of topography a common methodological challenge in up scaling the usle stems from the difficulty and subjectivity with calculating the l factor in a gis morgan and nearing 2011 the most common approach for up scaling the l factor involves using the concept of specific catchment areas i e upslope area of a unit contour divided by the contour width in lieu of planar hillslopes e g moore and burch 1986 griffin et al 1988 moore and wilson 1992 desmet and govers 1996 mitasova et al 1996 when using the concept of specific catchment areas two challenges are immediately presented that have the potential to introduce model input and user error 1 the choice of spatial elevation dataset i e grid cell size accuracy and 2 the methodology used for discretization of specific catchment areas the standard approach for calculating specific catchment areas in a gis is using the dataset with the finest spatial resolution hydrologically conditioning the dem and using a gis flow accumulation tool to delineate upslope areas e g d8 algorithm jenson and domingue 1988 algorithms like the d8 flow direction and d8 flow accumulation typically used in a gis for calculating specific catchment areas are sensitive to topographic variation and require depression filling i e hydrologic conditioning to derive meaningful estimates of hydrological connectivity however when a dem is fully hydrologically conditioned all flow is assumed to be connected from any point in the landscape to the lowest point of elevation there is no consideration of natural or anthropogenic barriers that inhibit flow or impede sediment movement fryirs et al 2007 while hydrologic conditioning is prudent for many hydrological analyses it removes the depositional cavities in the landscape which increases the size of specific catchment areas by artificially increasing landscape connectivity and results in an overestimation of the l factor for calculating the l factor in our recommended modelling approach we use the concept of specific catchment areas for up scaling the l factor and followed a three step approach for discretizing specific catchment areas 1 we clipped our dem to our farm field polygon layer before calculating contributing areas to ensure that natural e g wind breaks and artificial e g roadways barriers between fields correctly inhibited flow paths 2 we partially hydrologically conditioned our dem z fill limit 0 4 m to remove spurious artefacts and small microtopographic depressions in the dem while preserving larger topographic depressions where flow would be disconnected and 3 calculated specific catchment areas using a d8 flow algorithm and d8 flow accumulation tool for calculating the l factor we use the methodology of griffin et al 1988 l a s 22 13 m m 1 where l is the slope length factor unitless as is the specific catchment area upslope contributing area divided by the grid resolution and m is a slope exponent typically m 0 4 the last part of the equation i e m 1 was introduced by griffin et al 1988 for predicting erosion at a point e g for each grid cell on a dem specific catchment areas were calculated using a 2 m lidar dem for all agricultural land in the watershed the 2 m lidar dem was the highest resolution dem available for our watershed the slope exponent m represents the impacts that slope gradient has on surface runoff and is constrained to a range of values between 0 4 and 0 6 we adjusted the value within this range from 0 4 which underestimated the l factor by 35 45 relative to the usle methodology in agricultural handbook no 537 tested on 20 farm fields p 12 15 wischmeier and smith 1978 to 0 6 which provided the closest alignment with the original usle methodology 15 once we calibrated the slope exponent each farm field polygon was assigned an l factor based on the average of the l factor raster for each specific catchment area within the farm field polygon slope steepness factor s the s factor represents the impact that slope gradient has on water erosion since the usle was developed on planar one dimensional erosion plots the s factor needs to be reconceptualized for up scaling to larger spatial extents similar to the l factor the choice of spatial elevation dataset e g cell size accuracy and methodological implementation of the s factor will have an impact on soil erosion estimates decreasing the spatial resolution of topographic data i e using coarser data will decrease slope steepness estimates fine spatial resolutions e g lidar dems model the micro topography of the landscape while coarse dems e g satellite dems model an averaged macro topography to calculate the s factor we selectively chose to use a 4 m dem up sampled 2 m lidar dem as the most appropriate resolution for calculating the average slope steepness of specific catchment areas coarse dems e g 10 m poorly represented slopes on small agricultural fields while finer dems e g 2 m captured the slope of microtopography rather than the predominant slope of the macrotopography we calculated the s factor in our recommended modelling approach based on the average slope of each catchment area using the methodology of moore and burch 1986 s sinθ 0 0896 n where θ is the slope of the catchment area and n is a slope exponent n 1 3 each farm field polygon was assigned an s factor based on the average of the s factor raster within the polygon cover and management c the effects of farm management on soil erodibility varies greatly throughout a year and soil is more susceptible to water erosion during seedbed preparation than when a mature crop is present wischmeier and smith 1978 the temporal variability in soil loss during each management cycle is partly a function of rainfall erosivity crop cover and farm management e g tillage the representation of crop cover and management in the usle i e the c factor is equal to the soil loss ratio for a given cropping and tillage system multiplied by the percentage of annual ei e kinetic energy of rainfall i rainfall intensity for a specific sub annual timestep the usle operationalizes the c factor for individual cropstages i e six distinct periods of crop growth that are represented in agricultural handbook no 537 as 1 rough fallow 2 seedbed preparation 3 crop establishment 4 crop development 5 maturing crop and 6 residue or stubble a common approach for calculating the c factor is selecting a lumped annual value using for example annual c factor tables e g table c 3a in the ruslefac handbook p 91 wall et al 2002 however since the c factor is a time integrated factor a lumped annual value fails to represent the varying relationships between rainfall intensity land management and crop cover throughout the year while the usle can be parameterized and yield accurate results using an annual c factor selected from supplementary resources the annual c factor tables created by the soil conservation service and other agencies are applicable to a specific climate time period and field management regime e g primary tillage secondary tillage planting and harvest dates etc recalculating a time integrated c factor value specific to a specific study area should always yield more accurate results since farm fields can have different planting and harvest dates more than one crop within a year e g winter wheat following soybeans and different land management practices the choice of how these processes are captured will constrain the accuracy of the c factor when the c factor is calculated using a gis land cover data derived from remotely sensed imagery are often used while the spatial coverage and resolution of these data are typically sufficient for most applications land cover data are typically annual and automated image classification techniques cannot identify land management activities e g tillage types therefore most studies calculate an annual c factor e g mlc singh and panda 2017 ndvi grimm et al 2003 unsupervised classification hui et al 2010 and ignore variation in tillage planting and harvest dates or crop rotations to represent the human and natural component of the c factor in our recommended modelling approach we calculated the c factor for each cropstage as a function of crop type crop rotation planting and harvest dates tillage type the respective soil loss ratios for each management cycle p 22 26 wischmeier and smith 1978 and the percentage of annual ei30 during each cropstage in lieu of using annual land cover data we used a monte carlo approach to generate plausible realizations of each agricultural system to generate these realizations we inferred land management practices using the 2016 census of agriculture in canada based on the wilmot wellesley and perth east counties the census data were summarized by four dominant crop types corn soybean alfalfa wheat three types of tillage no till conservation till conventional till and two types of agricultural land use pasture cropland table 1 the four crops comprised 88 5 of all cropland in the watershed with the remaining 11 5 of cropland being either ill defined e g mixed grains or not have corresponding soil loss ratios in the agricultural handbook e g sunflowers we reclassified the remaining 11 5 crops to one of the four dominant crop types based on similarity e g mixed grains assigned to wheat to accommodate for this shift in crop practices land area under no till seeding was decreased to only winter wheat applications which is the most common practice in the watershed for winter wheat the decrease in no till was balanced by an increase in conservation tillage across the other three dominant crop types the similarity in crop types minimizes the effects of these amendments to accommodate data limitations on our soil erosion estimates supporting practices p the p factor represents the effects that different cropland practices e g contouring or terracing have on water erosion no additional supporting practices were present in the watershed so the p factor was left at the default of 1 which represents no additional supporting practices soil erosion rkclsp our recommended modelling approach uses a polygon based discretization for calculating the average soil erosion rate of the upper nith watershed where the watershed is conceptualized as a collection of farm fields since the data in the canadian census of agriculture is summarized for each county rather than spatially for each field a monte carlo simulation was used to randomly assign each farm field polygon a cover and management practice based on the proportion of occurrences for each in the watershed while this approach does not allow us to have a deterministic outcome it has the advantage of being able to identify the most erosive farm fields in the watershed and isolate what specific management activities are associated with unsustainable rates of soil erosion for each farm field a prescriptive approach for land management activities is largely prohibitive at the watershed scale the monte carlo simulation was run 100 000 times per study year to generate a comprehensive distribution of potential soil erosion estimates for the watershed by multiplying the usle factors of each farm field together using each possible arrangement of crop type tillage and land use for each simulated year the estimated soil erosion value for each farm field was stochastically modified by 20 to coincide with the natural variation in empirical soil erosion measurements e g 20 variation in replicate erosion plots wendt et al 1986 2 2 2 soil loss usle and sediment delivery ratios conservation efforts are often focussed on ameliorating the off site impacts of agricultural soil loss e g pollution of adjacent surface waters rather than on site sediment redistribution from soil erosion while sediment yield from a watershed is impacted by the amount of arable land and cropland within the watershed vanmaercke et al 2015 and it is well established that agricultural activities accelerate the rate of soil erosion montgomery 2007 extending usle results outside of the models intended design space to predict soil loss from fields to waterways with a sediment delivery ratio sdr is a dubious task relating gross erosion to sediment yield has been classically done using a single sdr whereby the fraction of gross erosion that is transported out of a watershed is expressed as s d r y e where y is the sediment yield at the watershed outlet and e is the gross erosion of the watershed e is typically represented by usle soil erosion estimates e g fistikoglu and harmancioglu 2002 amore et al 2004 pandey et al 2007 hui et al 2010 rizeei et al 2016 singh and panda 2017 and the sdr indicates what percentage of eroded agricultural sediments are leaving the watershed usda 1971 the first problem with using the usle to represent the gross erosion of a watershed e is that agricultural sediments are not the only source of sediments contributing to sediment yield and measurements of sediment yield are only reflective of the fine material transported in the waterway waterways have a natural baseline sediment yield that is augmented not only by eroded agricultural sediments but also sediment from subsurface tile drainage in stream erosion and changes of sediment storage within the system itself furthermore agricultural sediments are a highly variable component of watershed sediment budgets it has been estimated that in tile drained agricultural systems sediment from field drains can account for 28 29 walling et al 2002 to 51 55 walling et al 2002 of the catchment sediment yield and can be a significant source of particulate p macrae et al 2007b king et al 2015 while in other landscapes in stream erosion can be the dominant supplier of sediment and nutrients e g 90 94 kronvang et al 2013 the second problem with using the usle with an sdr to represent the delivery of eroded agricultural sediments to waterways is that not all eroded sediments have an equal probability of being conveyed to a catchment outlet a distributed approach is required to capture the field to field variation in landscape conditions that drive sediment delivery to waterways each agricultural field has a unique delivery potential based on its relative position to a waterway and the filtering efficiency of the adjacent riparian zone we can express the soil loss from fields to waterways of each agricultural field f as a function of each catchment within the field s d r f j 1 n e j f j j 1 n e j i 1 m e i where sdrf is the sediment delivery ratio of a farm field to a waterway e is the soil erosion rate t of an agricultural catchment f is the filtering efficiency of the riparian zone j is an index for the number of catchments on a farm field that have a potential to produce soil loss to waterways and i is an index for the number of catchments on a farm field that are unlikely to produce soil loss to waterways i e the catchment does not direct runoff towards a waterway to calculate the sdrf of each field our recommended modelling approach uses a distributed approach whereby e is the usle soil erosion rate of an agricultural catchment within a field the average filtering efficiency of riparian zones is defined by yuan et al 2009 and j and i are calculated in arcgis v10 6 1 based on the direction of overland flow and proximity to waterways appendix a when using the usle to represent the soil erosion rate of a farm field it is important to note that the usle does not account for depositional processes at the bottom of a hillslope so there will be a disconnect between the soil erosion rate and rate of soil loss from the field to account for this we re ran our monte carlo simulation to calculate soil loss from farm fields to waterways using a stochastic estimate of in field depositional processes 25 to 75 and additionally included a stochastic estimate of the filtering efficiency of riparian zones 20 based on yuan et al 2009 the range of these stochastic estimates illustrate considerable uncertainty associated with predicting soil loss to waterways with usle which is beyond its intended design however the stochasticity is necessary to derive a range of quantitative outcomes that capture the behaviour of the agricultural system and do not incorrectly emphasize a single estimate 2 3 variability in model outcomes since there is no standardized and accredited setup for up scaling the usle different methodological implementations for up scaling usle factors will contribute to uncertainty in model outcomes while certain up scaling approaches are conceptually poor e g calculating the c factor using a single year of airborne imagery it is difficult to provide guidance on what constitutes a correctly up scaled implementation of each usle factor we focus on quantifying the variability in model outcomes rather than error or uncertainty because the size of our study area precludes a validation of erosion rates to quantify the variability in model outcomes we compare the recommended modelling approach from our case study with 1 different design choices for implementing individual usle factors 2 a model implementation synonymous with the most common approach in a sampling of literature and 3 national soileri clearwater et al 2016 and global studies on soil erosion borrelli et al 2017 to select different design choices for implementing individual usle factors we searched google scholar with the keywords usle watershed and erosion to understand the most commonly used usle methodologies among the first ten relevant papers fistikoglu and harmancioglu 2002 amore et al 2004 erdogan et al 2007 pandey et al 2007 dabral et al 2008 hui et al 2010 devatha et al 2015 belasri and lakhouili 2016 rizeei et al 2016 singh and panda 2017 and supplemented this with several hand selected papers van der knijff et al 1999 grimm et al 2003 prasuhn et al 2013 soileri clearwater et al 2016 borrelli et al 2017 these publications have been cited on google scholar a total of 2269 times and use the usle in a gis environment to evaluate distributed soil erosion rates at large spatial extents 3 results 3 1 soil erosion usle for contextualizing our results the rate of natural soil regeneration is less than 1 t ha 1 yr 1 montgomery 2007 with most conservation efforts focusing on keeping soil erosion rates under 5 to 11 t ha 1 yr 1 e g ontario guidelines from omafra are 6 7 t ha 1 yr 1 our recommended modelling approach summarized across 100 000 monte carlo simulations calculated a 5 year average soil erosion rate of 6 66 t ha 1 yr 1 table 2 for the upper nith watershed with a lower bound of 5 22 t ha 1 yr 1 and an upper bound of 8 18 t ha 1 yr 1 in this context soil erosion represents soil loss from hillslopes to anywhere in the landscape including both on site sediment redistribution and off site soil loss investigating the effects of a conventional tillage versus a conservation tillage demonstrated a substantial reduction in erosion rates when using a conventional tillage system for the watershed i e moldboard corn moldboard soybean no till winter wheat e g meinen and robinson 2021 we identified that 1421 farm fields 29 18 of agricultural land would have an unsustainable rate of soil erosion i e 11 2 t ha 1 yr 1 while under a conservation tillage system i e chisel corn chisel soybean no till winter wheat only 377 farm fields 6 57 of agricultural land would have a soil erosion rate greater than 11 2 t ha 1 yr 1 table 3 the moldboard plow inverts the topsoil burying the majority of crop residue and leaving a rough soil surface while the chisel plow breaks up the soil surface but does not invert the topsoil like the moldboard plow moldboard plows are typically used at a greater depth than chisel plows and do not leave sufficient biomass on the field surface to protect against water erosion the average of all our monte carlo simulations calculated that 805 farm fields 15 55 of agricultural land are likely eroding at a rate greater than 11 2 t ha 1 yr 1 the northern half of the watershed is characterized by very low erosion rates whereas fields in the south western portion of the watershed have high soil erosion rates resulting from steeper slopes and soils more susceptible to water erosion i e silty loams fig 2 a conservation efforts focussed on the fields that have an unsustainable rate of soil erosion 11 2 t ha 1 yr 1 fig 2a would ameliorate the economic and environmental impact of soil erosion while conservation efforts focussed on the northern half of the watershed would likely have no significance 3 2 soil loss usle and sediment delivery ratios soil erosion has on site impacts e g tillage erosion affecting crop yield lobb et al 1995 van oost et al 2006 while soil loss has both on site e g lost topsoil affecting crop yield den biggelaar et al 2001 and off site impacts e g eutrophication of local water bodies michalak et al 2013 to estimate soil loss from fields to waterways we applied a field scale sdrf to our usle results and summarized the results across 100 000 monte carlo simulations our modelling results estimated that soil loss from fields to waterways ranged from 6083 to 12 256 t yr 1 mean one standard deviation sdrf 2 30 4 63 with a 5 year average soil loss of 9170 t yr 1 sdr 3 47 0 23 t ha 1 yr 1 table 4 field scale results showcased that the 20 of fields with the highest rates of soil loss were responsible for 77 of the total soil loss with the tail end of this distribution contributing the majority of sediments fig 2b a large portion of farm fields with high erosion rates had low estimates of soil loss since they were disconnected from waterways e g fig 3 b and c for estimating the environmental impact of soil loss from agricultural fields to waterways relative to other sources of sediment we compared the estimated soil loss from farm fields to the sediment yield measured at the outlet of the upper nith watershed if we assume that all the sediment that entered the nith river is transient eroded agricultural sediments entrained and transported via overland flow would represent a maximum of 30 50 61 45 of the watershed s sediment yield sdrf 2 30 4 63 from 2010 to 2014 based on a qualitative visual analysis of fields in the watershed we hypothesize that in field depositional processes are substantial and that the soil loss is likely closer to the lower bound 3 3 variability in model outcomes 3 3 1 different design choices for individual usle factors to quantify the variability in model outcomes for different implementations of usle factors we recalculated the average soil erosion rate for the upper nith watershed using different design choices for individual usle factors commensurate with the most common approaches in literature the outcome of using different design choices in a usle implementation led to a range in soil erosion estimates of 3 04 11 02 t ha 1 yr 1 for the upper nith watershed with significant spatial discrepancies between model outcomes table 5 this high level of variability can be constrained and challenges with equifinality ameliorated by making improved up scaling design choices for example c factor a correct implementation of the c factor should include land use crop type tillage type and crop rotations a more detailed time integrated c factor should also include planting harvest and crop stage dates when using simple look up tables that did not include a human management component i e they only represented crop types there was a range in model outcomes of 9 44 9 81 t ha 1 yr 1 which over predicted soil erosion rates relative to our recommended modelling approach i e 6 66 t ha 1 yr 1 the uncertainty of using look up tables is best exemplified when you consider that land management practices are what drive soil erosion in agricultural systems not including the human dimension indicates that the model user has no understanding of the impacts of the human component of agriculture the polygon based discretization of the landscape used in our recommended modelling approach rather than a raster based discretization is a pragmatic approach for spatially assigning the human dimension of the c factor on a field by field basis l factor in lieu of a topographic based delineation e g using a gis of specific catchment areas a constant hillslope length can be used to avoid propagating model uncertainty if the model user only has access to coarse resolution dems using constant hillslope lengths of 50 m and 122 m led to model outcomes of 5 76 t ha 1 yr 1 and 8 22 t ha 1 yr 1 respectively with both approaches having a high spatial correlation with our recommended modelling approach both the range in model outcomes i e 6 18 11 02 t ha 1 yr 1 and spatial correlations were much poorer when a distributed gis approach was used with coarse dems relative to our recommended modelling approach dem resolution since dem resolution has a significant impact on model uncertainty and is used to calculate both the l factor and s factor we recommend using a dem resolution of no less than 10 m for an accurate discretization of specific catchment areas and calculation of slope gradients coarse dem resolutions e g 30 m are unable to model the slope of small topographically complex farm fields e g 5 ha and will lead to a poor discretization of specific catchment areas implementing the l factor and s factor with 90 m dems led to model outcomes of 11 02 t ha 1 yr 1 and 3 04 t ha 1 yr 1 respectively and was the largest source of uncertainty in modelling outcomes compared to our recommended modelling approach coarse dems may be appropriate for regions that are less topographically complex with larger farm field sizes but they were a poor choice for modelling the upper nith watershed 3 3 2 literature based model implementation using the most commonly represented methodology in our sampling of literature i e a literature based model implementation we calculated a 30 m raster of annual long term rates of soil erosion per pixel in the upper nith watershed for the literature based model implementation the r factor was selected from an annual isoerodent map for ontario figure r 1 p 46 wall et al 2002 annual c factors were selected from a look up table in the ruslefac handbook table c 3a p 91 wall et al 2002 and spatially assigned using the 2015 30 m aafc crop inventory raster fisette et al 2013 crop types and land cover classes were derived from landsat 8 optical imagery and radarsat 2 radar imagery k factors were selected from a table provided by omafra and spatially assigned using the ontario soil survey complex polygon the l factor was calculated on a 30 m fully hydrologically conditioned dem provincial dem using the approach of moore and burch 1986 m 0 4 and the s factor was calculated on a 30 m dem provincial dem using the approach of moore and burch 1986 n 1 3 the literature based implementation of the usle calculated an average erosion rate of 4 05 t ha 1 yr 1 for the upper nith watershed the similarity in the estimated average soil erosion rates of the literature based outcome 4 05 t ha 1 yr 1 relative to our recommended modelling approach 6 66 t ha 1 yr 1 table 5 demonstrates that vastly different model implementation of the usle can have similar results the literature based outcome had a compensatory effect whereby two factors s factor r factor were underestimated and two factors l factor c factor were overestimated relative to our recommended modelling approach this compensatory effect can lead to poor spatial outcomes and challenges with equifinality for example our recommended modelling approach estimated that on average 805 farm fields would be eroding at an unsustainable rate i e 11 2 t ha 1 yr 1 the literature based approach when summarized at the field level identified that 419 farm fields would be eroding at an unsustainable rate with only 60 4 of those fields being the same as our recommended modelling approach a simple comparison between the literature based l factor and the l factor from our recommended modelling approach shows little spatial correspondence i e spatial correlation of 0 34 at the farm field scale and a visual comparison shows significant discrepancies between model outcomes fig 4 the uncertainty associated with the literature based approach relative to our recommended modelling approach can be further exemplified by the c factor in our recommended modelling approach we used a time integrated c factor where each field was assigned a detailed description of crop types crop rotations planting and harvest dates based on crop type tillage types and land use for each cropstage the literature based approach calculated an annual c factor from a single year of airborne imagery i e each cell in the raster was assigned a crop type and the remaining management practices were estimated there is no understanding of the land management practices that drive soil erosion in the literature based approach both the human and natural components to be fully modelled for a meaningful model outcome 3 3 3 national and global studies to further emphasize the disparity among different usle applications and model implementations we compared the results from our recommended modelling approach to the most rigorous national estimate of soil loss in canada the soileri created by aafc canada clearwater et al 2016 the soileri represents soil loss from both water and tillage erosion and is calculated using soil landscape of canada slc polygons and a combination of the usle and rusle2 when we aggregated the soileri to the same spatial aggregation as our recommend modelling approach i e summarized the values for each farm field polygon the soileri over predicted the rate of soil erosion by 9 07 t ha 1 yr 1 136 2 difference relative to our recommended modelling approach soileri estimates 15 73 t ha 1 yr 1 recommended modelling approach 6 66 t ha 1 yr 1 this is sharply contrasted by the global study of borrelli et al 2017 that underpredicted soil loss by 5 94 t ha 1 yr 1 89 2 difference in the upper nith watershed relative to our recommended modelling approach borrelli et al 2017 estimate of 0 72 t ha 1 yr 1 while it is important to acknowledge the considerable data constraints of national and global erosion estimates and that the soileri estimates of soil erosion will be marginally higher since they include a tillage erosion component there was a significant disparity between all three usle applications despite the same model being applied 4 discussion 4 1 why the usle for an environmental problem to be a prominent issue on the world stage and to garner the interest of conservation groups rigorous measurements are needed to quantify the severity of the problem socio economic drivers and associated costs soil erosion is widely recognized as a global problem e g status of the world s soil resources report fao 2015 whereby soil erosion has been identified as one of the main threats to agricultural sustainability but somewhat paradoxically these conclusions are drawn from a paucity of data the sentiments of trimble and crosson 2000 aptly reflect why this is a problem it is questionable whether there has ever been another perceived public problem for which so much time effort and money were spent in light of so little scientific evidence p 248 these sentiments are echoed by boardman 2006 we have difficulty in the recognition description and quantification of erosion and limited information on the magnitude and frequency of events that cause erosion the inadequacy and frequent misuse of existing data leaves us open to the charge of exaggeration of the erosion problem a la lomborg p 73 the conclusions drawn from the paucity of data on soil erosion may be perceived as pragmatic to garner public interest albeit rightly open to criticism criticisms stem primarily from the generalization of plot based erosion measurements that are extremely difficult to source and not suitable for generalization for example pimentel 2006 is well cited 1001 citations on google scholar and identifies a worldwide erosion rate of 30 t ha 1 yr 1 sourcing pimentel et al 1995 for the erosion estimate pimentel et al 1995 cites barrow 1991 for an erosion rate for asia africa and south america of 30 40 tons ha 1 yr 1 and an erosion rate of 17 tons ha 1 yr 1 for the united states and europe however barrow 1991 estimate is derived from lal et al 1989 table iii using a synthesis of research from 24 countries that includes the following disclaimer the data used in this table comes from a wide range of sources and is derived through a wide range of sampling methodologies it is therefore not standardized and serves as only a general indication table 10 6 barrow 1991 while this synthesis of research is useful and informative the soil erosion rates found in table iii of lal et al 1989 are sourced from eight different documents barber 1983 fournier 1967 humphreys 1984 lal 1976a 1976b ngatunga et al 1984 roose 1977 world resources institute 1986 which are mostly inaccessible and the few that are available were based on small plot based studies e g tanzania ngatunga et al 1984 lal et al 1989 warn that the data obtained from small plots are often not comparable misinterpretation and erroneous conclusions are major worries when using such data p 58 while the nesting of sources and lack of citing primary literature is a concern the issue lies in the use of data that has explicitly cautioned its use and lack of comparability and scalability a similar example is offered when searching for a primary source from the world resources institute 1986 citation in table iii of lal et al 1989 the world resources institute 1986 table 5 5 p 270 sources a compiled list of cropland soil erosion rates from the world resources institute and international institute for environment and development which has an additional 16 references one of these references is the estimate of soil erosion in central belgium sourced from richter 1983 where richter 1983 is not the primary source but cites bollinne 1982 in text eight year measurements carried out in central belgium showed soil losses of 10 25 tonnes per hectare per year p 11 bollinne 1982 however bollinne 1982 did not calculate an average erosion rate for central belgium but calculated the erosion rates of 12 small experimental erosion plots in central belgium while the scaling of small plot based studies to large spatial extents may be the only approach at a particular moment in time it embeds substantial errors in the estimate due to the disconnect in space and time between plots and their broad areas of application boardman 1998 and crosson et al 1995 have a more detailed discussion on the perpetuation of the poorly sourced estimate of pimentel et al 1995 due to a paucity of data on soil erosion measurements and the frequent erroneous up scaling of plot based based studies in both space and time modelling is required to gain an understanding of the extent of erosion at large spatial extents for example the most rigorous global application of the usle estimates a global soil erosion rate of 2 8 t ha 1 yr 1 borrelli et al 2017 all land uses which is in stark contrast with the widely cited global erosion rate of 30 t ha 1 yr 1 estimated by pimentel 2006 cropland and pastures while the predictive accuracy of erosion models can be evaluated at the farm field scale meinen and robinson 2021 model evaluation is much more challenging at large spatial scales due to a lack of spatially distributed erosion data and therefore models should be up scaled in a rigorous manner that does not introduce significant uncertainty or error in their outcomes without proper model execution and rigorous analysis we are in danger of discrediting our results which can erode public trust and sustainability efforts while the usle has the ideal characteristics of a large scale model i e a simple parsimonious modelling structure our modelling results highlight the uncertainty implicit to up scaling the usle to the watershed scale our recommended modelling approach calculated an average erosion rate of 6 66 t ha 1 yr 1 the outcome of using different design choices led to a range in model outcomes from 3 04 to 11 02 t ha 1 yr 1 changing the implementation of all the usle factors with a literature based methodology led to a modelling outcome of 4 05 t ha 1 yr 1 a comparison with literature showed a range in model outcomes from 0 72 t ha 1 yr 1 to 15 73 t ha 1 yr 1 while the usle is a simple empirical model the variability in model outcomes demonstrates the challenges in up scaling the usle to a large spatial extent a standardized and accredited methodology for up scaling the usle is needed to reduce uncertainty in modelling results our analysis provides a first step toward discussing this standardization by quantifying the impacts of different design choices on erosion estimates and when specific design choices should be made 4 2 challenges with using the usle and sediment delivery ratios while we present a quantitative model outcome for coupling the usle with a field scale sdr i e sdrf to predict soil loss from fields to waterways by defining a wide range of stochastic uncertainty in our monte carlo simulation i e 25 to 75 in field deposition 20 soil erosion rate 20 filtering efficiency of riparian zones the complexity of human natural systems often precludes a quantitative interpretation of results this complexity and the challenges of modelling sediment delivery in agricultural systems are best exemplified when looking at overland flow paths on satellite imagery for example in the upper nith watershed overland flow frequently routed to artificial drainage structures at field edges accompanied by large depositional plumes and drained directly to the nith river via surface inlets and subsurface tiles or to an adjacent field or ditch via a culvert when we examined historical airborne imagery we found that the majority of fields with visible signs of water erosion that also had connectivity to the nith river were associated with artificial drainage issues from culvert and field tiles further complicating this drainage patterns and flow connectivity changed considerably over a short period of time due to new land management practices e g fig 5 the implementation of a more complex environmental model in lieu of our sdrf methodology will not provide more meaningful results unless field scale features and their connectivity e g location of drainage structures are empirically captured and their effects on overland flow explicitly modelled studies that couple the usle with an sdr should generally be treated as qualitative due to the complexity of agricultural systems the inability to validate model results and since the application of an sdr is so far outside of the models intended design space 5 conclusions it is imperative that a standardized and accredited usle setup is firmly established in the literature for model up scaling that has results synonymous with agricultural handbook no 537 wischmeier and smith 1978 or agricultural handbook no 703 renard et al 1991 for revised usle applications in a gis cross comparisons of usle modelling results cannot be conducted without a standardized modelling approach while we cannot provide guidelines for what constitutes an appropriate remote sensing dataset for estimating model input parameters or a methodology for up scaling from the field to watershed scale we provided a first step in this direction by demonstrating the impacts of different design choices and up scaling methodologies on model outcomes pragmatism alone is not enough to justify a data source or modelling endeavour the ideal model for large scale environmental assessments of soil erosion will have a high intrinsic model error but low model input error unfortunately even with the simple parsimonious modelling structure of the usle poor results can be driven by model input error and user error if the usle continues to be used for large scale environmental assessments of soil erosion more rigor needs to be employed by model users to ensure that modelling results are not invalidated by poor design choices declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to acknowledge with gratitude infrastructure funding provided by the canadian foundation for innovation 32247 the ontario research fund flir systems the natural sciences and engineering research council nserc of canada 06252 2014 547496 2020 as well as the ministry for research and innovation for support through the early researcher award er15 11 198 and the faculty of environment at the university of waterloo for student funding in addition we would like to thank omar dzinic for his assistance with modelling appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105229 software availability the implementation of the usle in this study is based off of agricultural handbook no 537 wischmeier and smith 1978 and was conducted in arcgis 10 6 1 using freely available municipal provincial and federal datasets a tutorial for each usle factor is included in the attached appendices r factor wellesley dam meteorological station grand river conservation authority 2010 2014 k factor ontario soil survey complex ontario ministry of agriculture food and rural affairs 2019 l factor lidar dem ontario ministry of natural resources and forestry 2021 s factor lidar dem ontario ministry of natural resources and forestry 2021 c factor census of agriculture statistics canada 2016 imagery southwestern ontario orthoimagery project land information ontario 2015 waterways watercourse and waterbody grand river conservation authority 2021 
25710,the universal soil loss equation usle has been the de facto standard for soil erosion management studies since its seminal publication in the 1970 s widespread use of the model is due in part to its simple empirical modelling structure and parsimonious parameterization however these benefits have also led to criticism of its relevance as a tool for estimating soil erosion while the usle has a strong empirical basis it is regularly used beyond its intended design space i e predicting soil loss from planar hillslopes to predict distributed soil erosion rates at large spatial extents which introduces uncertainty in model outcomes in this paper we use a case study for up scaling the usle to a large spatial extent to assess the variability in model outcomes from different model user s design choices our analysis demonstrates that a standardized and accredited methodology for up scaling the usle is needed to reduce uncertainty in model outcomes keywords universal soil loss equation erosion watershed uncertainty soil degradation gis 1 introduction the roots of modern soil erosion modelling originated in the american midwest during the early 1900 s e g 1917 missouri agricultural experiment station the detrimental impacts of mechanized agriculture on soil erosion and agricultural productivity were brought to the attention of the american congress by hugh hammond bennett who secured funding in 1929 for establishing ten experimental erosion plots meyer and moldenhauer 1985 the advocacy of bennett to the american congress the 1930 s dust bowl in the great plains of north america and the subsequent widespread crop failure collectively influenced the american congress to pass the soil conservation act of 1935 public law 74 46 the act states that it is hereby recognized that the wastage of soil and moisture resources on farm grazing and forest lands of the nation resulting from soil erosion is a menace to the national welfare and that it is hereby declared to be the policy of congress to provide permanently for the control and prevention of soil erosion public law 74 46 this act provided national funding for soil erosion research resulting in the first conceptualizations of erosion models e g zingg 1940 smith 1941 musgrave 1947 and most prominently culminated with the development of the universal soil loss equation usle wischmeier and smith 1965 the usle is a lumped empirically based soil erosion model that was developed by the united states department of agriculture and soil conservation services first published in agricultural handbook no 282 wischmeier and smith 1965 and widely adopted based on the superseding publication in 1978 no 537 wischmeier and smith 1978 the usle is the culmination of over 10 000 plot years of erosion measurements spanning several decades the basis of the usle is a unit plot represented by a small fallow agricultural plot 22 1 m long x 1 8 m wide with a 9 slope gradient and an up down slope tillage pattern the soil loss of other experimental erosion plots e g slopes 3 18 slope lengths 9 91 m were described relative to these reference conditions the usle encapsulates a representation of the erodibility of an agricultural hillslope relative to the conditions of the unit plot using six empirically derived factors wischmeier and smith 1978 a r k l s c p where a is the annual soil loss per unit area t ha 1 yr 1 r is a rainfall and runoff factor mj mm ha 1 h 1 yr 1 k is a soil erodibility factor t ha h ha 1 mj 1 mm 1 l is a slope length factor unitless s is a slope steepness factor unitless c is a cover and management factor unitless and p is a supporting practice factor unitless wischmeier and smith 1978 soil loss in the usle is conceptualized as soil loss from an agricultural hillslope resulting from rill and interrill erosion since the seminal publication of 1978 wischmeier and smith 1978 the usle has been revised rusle renard et al 1991 and succeeded by rusle2 and the process based water erosion prediction project wepp laflen et al 1991 the successors of the usle and new models developed in the late 20th century sought to overcome the inherent empirical limitations of the usle and extend the applicability of soil erosion models by representing sediment conveyance and depositional processes e g laflen et al 1991 these modelling developments can be classified into two classes of erosion models 1 hybrid models that couple the usle factors with a sediment transport model e g soil and water assessment tool arnold et al 1998 areal nonpoint source watershed environment response simulation beasley et al 1980 or 2 process based models that are independent of the empiricisms of the usle e g lisem de roo et al 1996 wepp despite scientific efforts to derive new process based models or improve upon the usle the usle and revised usle still remain the de facto standards for management oriented soil erosion studies alewell et al 2019 both as a standalone tool and via incorporation into hybrid models the usle has been operationalized for ease of use with geographic information systems gis to up scale the model from predicting soil loss for individual hillslopes to predicting soil erosion at national e g italy grimm et al 2003 switzerland prasuhn et al 2013 continental e g europe panagos et al 2015a and global e g borrelli et al 2017 scales among these large scale applications the usle has been used as a decision support tool for soil erosion prevention e g prasuhn et al 2013 a technical support tool for sustainable development e g van der knijff et al 1999 grimm et al 2003 and for quantifying the severity of soil degradation from soil erosion processes globally e g fao 2015 models applied at large spatial extents typically forgo complicated process descriptions which result in higher intrinsic model error but low model input error rompaey and govers 2002 the simple structure and parsimonious parameterization of the usle meets this criterion and has driven its widespread use from small i e plot field to large watershed national global spatial extents however if model applications across large spatial extents are the result of extending a model beyond its designed application space then additional uncertainty is introduced into modelling outcomes despite the usle being designed for predicting soil loss from planar hillslopes its implementation in combination with a gis has taken vastly different methodological approaches e g fistikoglu and harmancioglu 2002 amore et al 2004 erdogan et al 2007 pandey et al 2007 dabral et al 2008 hui et al 2010 devatha et al 2015 belasri and lakhouili 2016 rizeei et al 2016 singh and panda 2017 for up scaling the usle to large spatial extents with little to no acknowledgement of the different types of error and uncertainty or their quantification in the up scaling process the predictive accuracy of an erosion model can be conceptualized as comprising four components 1 intrinsic model error 2 model input error 3 model user error and 4 stochastic error intrinsic model error is the error inherent to the model as a result of a simplified modelling structure which manifests itself when a model developer chooses to forgo a parameter that affects soil erodibility e g hydraulic conductivity in favor of a more parsimonious modelling structure e g soil texture model input error is the error derived from poorly measured or estimated model inputs which typically manifests itself when remote sensing products are used in lieu of detailed in situ measurements model user error is a result of an incorrect application or parameterization of a model lastly the stochasticity of coupled human natural systems e g variance in replicate plots wendt et al 1986 can result in variation among erosion estimates that cannot practically be modelled i e stochastic error the total error of a model s prediction is the sum of these four error sources all usle predictions of soil erosion rates will carry some amount and combination of intrinsic model error model input error model user error and stochastic error however when up scaling the usle from the hillslope scale to larger spatial extents model input and user error are of particular concern since data constraints design choices and spatial conceptualizations of a system will invariably influence model outcomes error in erosion predictions can be evaluated for studies conducted at larger spatial extents by comparing modelled and measured soil erosion rates on a subset of farm fields within a watershed e g favis mortlock 1998 or when there is comprehensive erosion data collected for the watershed e g jetten et al 1999 since comprehensive erosion data are typically not available and validating the outcomes of spatially distributed erosion rates is complex we focus our discussion on the variability in usle model outcomes rather than error or uncertainty to exemplify the variability in usle model outcomes we conduct a case study for up scaling the usle in the upper nith watershed ontario canada we compare our recommended modelling approach with other common modelling approaches in literature to answer the following question what is the variability in model outcomes from different model user s design choices for up scaling the usle to large spatial extents 2 materials and methods 2 1 study site our case study is situated in the upper reaches of the nith watershed in ontario canada the upper nith watershed is an agricultural watershed characterized by row cropping of corn soybean winter wheat and alfalfa in 2015 farm fields covered 39 708 ha 73 of the watershed with an average farm field size of 8 46 ha an average farm field slope of 1 8 and a standard crop rotation of corn soybean and winter wheat an extensive tile drainage system covers the upper nith watershed appendix b which drains 22 660 ha of farm fields 57 of farmland soils in the northern half of the upper nith watershed are composed of clay loam and loam contrasted by a mosaic of soils in the southern half of the watershed which are primarily loam and silty loam the annual precipitation for the upper nith watershed is 958 3 mm yr 1 2005 2014 wellesley dam meteorological station with snow melt occurring intermittently throughout the winter months and during the early spring the intermittent snow melts and spring freshet result in a high rate of discharge in the upper nith river throughout the winter and early spring months with 76 82 of annual discharge occurring from november to april and the remaining 23 18 of annual discharge occurring during the warmer growing season may to october 2005 2014 environment canada hydrometric station 02ga018 the sediment yield of the upper nith river correlates well with discharge a linear model relating sediment yield samples n 41 environment canada station 16018403202 appendix b with discharge r2 0 47 estimated that 87 14 of sediment export occurred during the non growing season november to april and the remaining 12 86 of sediment export occurred during the growing season may to october the 10 year average sediment yield of the upper nith watershed was calculated at 22 131 35 t yr 1 2005 2014 0 41 t ha 1 yr 1 with a more recent 5 year average of 19 943 07 t yr 1 2010 2014 0 37 t ha 1 yr 1 the upper nith watershed has consistently experienced elevated levels of suspended sediments and nutrients n and p relative to the other sub watersheds of the grand river loomer and cooke 2011 nutrient concentrations measured at the outlet of the upper nith watershed of unfiltered p environment canada station 16018403202 have exceeded water quality guidelines guideline value is 0 03 mg l 1 to prevent eutrophication environment canada 2004 in 98 of samples n 47 between 2005 and 2014 elevated levels of suspended sediments and p can be associated with eroded agricultural sediments making the upper nith watershed ideal for a case study on agricultural soil erosion and soil loss 2 2 erosion modelling 2 2 1 soil erosion usle our recommended modelling approach implements the usle as outlined in agricultural handbook no 537 to estimate a 5 year average erosion rate 2010 2014 for the upper nith watershed while most usle studies use a raster based model implementation our recommended modelling approach uses a polygon based discretization for calculating and summarizing results whereby each polygon is representative of an individual farm field and was manually digitized using 2015 southwestern ontario orthoimagery project swoop airborne imagery with a fixed scale of 1 4000 fig 1 each farm field was assigned a static k and ls factor for our study period an r factor for each year and a c factor for each cropstage the six usle factors in our recommended modelling approach were calculated as follows rainfall and runoff r the r factor reflects the impacts that rainfall and runoff have on water erosion the r factor is a summation of the total kinetic storm energy times the maximum 30 min rainfall intensity for all rainstorms in a given year p 5 wischmeier and smith 1978 r i 1 m e i 30 i where r is the annual rainfall and runoff factor mj mm ha 1 h 1 yr 1 m is the total number of storms in a year ei30 is the rainfall erosivity of a single storm event i mj mm ha 1 hr 1 calculated as the total kinetic energy of rainfall e mj ha 1 times the maximum 30 min rainfall intensity i30 mm hr 1 storms are only included in r calculations that exceed 12 7 mm of precipitation while events smaller than 12 7 mm in size can produce a runoff response if they occur on wet antecedent conditions most erosion and nutrient loss events are associated with high magnitude rainfall events e g macrae et al 2007a we calculated the rainfall erosivity of each storm event using hourly rainfall data collected from the wellesley dam meteorological station the meteorological station is located in the central portion of the watershed and we assumed that the spatial distribution of rainfall was uniform over the entire watershed since the minimum requirement for calculating the i30 of each storm is 30 min rainfall data and we only had hourly rainfall data we used a relationship developed by panagos et al 2015b to convert from 60 min intensities to 30 min intensities r30 min 1 5597 x r60 min this conversion was found to be a reasonable approximation for annual predictions by meinen and robinson 2021 for a field scale usle case study in the watershed albeit with a poor characterization of the most intense rainfall events we only recommend using this empirical scaling approach in the absence of 30 min rainfall data since it may be a substantial source of uncertainty in modelling outcomes our 5 year average r factor 1923 mj mm ha 1 h 1 yr 1 related very well to the long term average r factor calculated by the ontario ministry of agriculture and rural affairs 1864 mj mm ha 1 h 1 yr 1 omafra 2012 soil erodibility k the k factor represents the susceptibility of different soil types to water erosion p 10 wischmeier and smith 1978 100 k 2 1 m 1 14 10 4 12 a 3 25 b 2 2 5 c 3 0 1317 where k is the soil erodibility factor t ha h ha 1 mj 1 mm 1 m is the soil particle size parameter based on soil texture a is percent organic matter b is the soil structure code and c is the profile permeability class to calculate the k factor the spatial location of soil textural classes m and other edaphic characteristics a b c were selected from the ontario soil survey complex polygon 1 50 000 scale omafra 2019 the ontario soil survey complex polygon was rasterized and the k value for each farm field polygon was calculated as the average of the k factor raster within the polygon slope length factor l the l factor represents the effect that the length of a hillslope has on water erosion the usle l factor was developed to be applicable to planar hillslopes i e the shape of the experimental erosion plots and does not have any provision for complex topography p 12 wischmeier and smith 1978 due to this simplification of topography a common methodological challenge in up scaling the usle stems from the difficulty and subjectivity with calculating the l factor in a gis morgan and nearing 2011 the most common approach for up scaling the l factor involves using the concept of specific catchment areas i e upslope area of a unit contour divided by the contour width in lieu of planar hillslopes e g moore and burch 1986 griffin et al 1988 moore and wilson 1992 desmet and govers 1996 mitasova et al 1996 when using the concept of specific catchment areas two challenges are immediately presented that have the potential to introduce model input and user error 1 the choice of spatial elevation dataset i e grid cell size accuracy and 2 the methodology used for discretization of specific catchment areas the standard approach for calculating specific catchment areas in a gis is using the dataset with the finest spatial resolution hydrologically conditioning the dem and using a gis flow accumulation tool to delineate upslope areas e g d8 algorithm jenson and domingue 1988 algorithms like the d8 flow direction and d8 flow accumulation typically used in a gis for calculating specific catchment areas are sensitive to topographic variation and require depression filling i e hydrologic conditioning to derive meaningful estimates of hydrological connectivity however when a dem is fully hydrologically conditioned all flow is assumed to be connected from any point in the landscape to the lowest point of elevation there is no consideration of natural or anthropogenic barriers that inhibit flow or impede sediment movement fryirs et al 2007 while hydrologic conditioning is prudent for many hydrological analyses it removes the depositional cavities in the landscape which increases the size of specific catchment areas by artificially increasing landscape connectivity and results in an overestimation of the l factor for calculating the l factor in our recommended modelling approach we use the concept of specific catchment areas for up scaling the l factor and followed a three step approach for discretizing specific catchment areas 1 we clipped our dem to our farm field polygon layer before calculating contributing areas to ensure that natural e g wind breaks and artificial e g roadways barriers between fields correctly inhibited flow paths 2 we partially hydrologically conditioned our dem z fill limit 0 4 m to remove spurious artefacts and small microtopographic depressions in the dem while preserving larger topographic depressions where flow would be disconnected and 3 calculated specific catchment areas using a d8 flow algorithm and d8 flow accumulation tool for calculating the l factor we use the methodology of griffin et al 1988 l a s 22 13 m m 1 where l is the slope length factor unitless as is the specific catchment area upslope contributing area divided by the grid resolution and m is a slope exponent typically m 0 4 the last part of the equation i e m 1 was introduced by griffin et al 1988 for predicting erosion at a point e g for each grid cell on a dem specific catchment areas were calculated using a 2 m lidar dem for all agricultural land in the watershed the 2 m lidar dem was the highest resolution dem available for our watershed the slope exponent m represents the impacts that slope gradient has on surface runoff and is constrained to a range of values between 0 4 and 0 6 we adjusted the value within this range from 0 4 which underestimated the l factor by 35 45 relative to the usle methodology in agricultural handbook no 537 tested on 20 farm fields p 12 15 wischmeier and smith 1978 to 0 6 which provided the closest alignment with the original usle methodology 15 once we calibrated the slope exponent each farm field polygon was assigned an l factor based on the average of the l factor raster for each specific catchment area within the farm field polygon slope steepness factor s the s factor represents the impact that slope gradient has on water erosion since the usle was developed on planar one dimensional erosion plots the s factor needs to be reconceptualized for up scaling to larger spatial extents similar to the l factor the choice of spatial elevation dataset e g cell size accuracy and methodological implementation of the s factor will have an impact on soil erosion estimates decreasing the spatial resolution of topographic data i e using coarser data will decrease slope steepness estimates fine spatial resolutions e g lidar dems model the micro topography of the landscape while coarse dems e g satellite dems model an averaged macro topography to calculate the s factor we selectively chose to use a 4 m dem up sampled 2 m lidar dem as the most appropriate resolution for calculating the average slope steepness of specific catchment areas coarse dems e g 10 m poorly represented slopes on small agricultural fields while finer dems e g 2 m captured the slope of microtopography rather than the predominant slope of the macrotopography we calculated the s factor in our recommended modelling approach based on the average slope of each catchment area using the methodology of moore and burch 1986 s sinθ 0 0896 n where θ is the slope of the catchment area and n is a slope exponent n 1 3 each farm field polygon was assigned an s factor based on the average of the s factor raster within the polygon cover and management c the effects of farm management on soil erodibility varies greatly throughout a year and soil is more susceptible to water erosion during seedbed preparation than when a mature crop is present wischmeier and smith 1978 the temporal variability in soil loss during each management cycle is partly a function of rainfall erosivity crop cover and farm management e g tillage the representation of crop cover and management in the usle i e the c factor is equal to the soil loss ratio for a given cropping and tillage system multiplied by the percentage of annual ei e kinetic energy of rainfall i rainfall intensity for a specific sub annual timestep the usle operationalizes the c factor for individual cropstages i e six distinct periods of crop growth that are represented in agricultural handbook no 537 as 1 rough fallow 2 seedbed preparation 3 crop establishment 4 crop development 5 maturing crop and 6 residue or stubble a common approach for calculating the c factor is selecting a lumped annual value using for example annual c factor tables e g table c 3a in the ruslefac handbook p 91 wall et al 2002 however since the c factor is a time integrated factor a lumped annual value fails to represent the varying relationships between rainfall intensity land management and crop cover throughout the year while the usle can be parameterized and yield accurate results using an annual c factor selected from supplementary resources the annual c factor tables created by the soil conservation service and other agencies are applicable to a specific climate time period and field management regime e g primary tillage secondary tillage planting and harvest dates etc recalculating a time integrated c factor value specific to a specific study area should always yield more accurate results since farm fields can have different planting and harvest dates more than one crop within a year e g winter wheat following soybeans and different land management practices the choice of how these processes are captured will constrain the accuracy of the c factor when the c factor is calculated using a gis land cover data derived from remotely sensed imagery are often used while the spatial coverage and resolution of these data are typically sufficient for most applications land cover data are typically annual and automated image classification techniques cannot identify land management activities e g tillage types therefore most studies calculate an annual c factor e g mlc singh and panda 2017 ndvi grimm et al 2003 unsupervised classification hui et al 2010 and ignore variation in tillage planting and harvest dates or crop rotations to represent the human and natural component of the c factor in our recommended modelling approach we calculated the c factor for each cropstage as a function of crop type crop rotation planting and harvest dates tillage type the respective soil loss ratios for each management cycle p 22 26 wischmeier and smith 1978 and the percentage of annual ei30 during each cropstage in lieu of using annual land cover data we used a monte carlo approach to generate plausible realizations of each agricultural system to generate these realizations we inferred land management practices using the 2016 census of agriculture in canada based on the wilmot wellesley and perth east counties the census data were summarized by four dominant crop types corn soybean alfalfa wheat three types of tillage no till conservation till conventional till and two types of agricultural land use pasture cropland table 1 the four crops comprised 88 5 of all cropland in the watershed with the remaining 11 5 of cropland being either ill defined e g mixed grains or not have corresponding soil loss ratios in the agricultural handbook e g sunflowers we reclassified the remaining 11 5 crops to one of the four dominant crop types based on similarity e g mixed grains assigned to wheat to accommodate for this shift in crop practices land area under no till seeding was decreased to only winter wheat applications which is the most common practice in the watershed for winter wheat the decrease in no till was balanced by an increase in conservation tillage across the other three dominant crop types the similarity in crop types minimizes the effects of these amendments to accommodate data limitations on our soil erosion estimates supporting practices p the p factor represents the effects that different cropland practices e g contouring or terracing have on water erosion no additional supporting practices were present in the watershed so the p factor was left at the default of 1 which represents no additional supporting practices soil erosion rkclsp our recommended modelling approach uses a polygon based discretization for calculating the average soil erosion rate of the upper nith watershed where the watershed is conceptualized as a collection of farm fields since the data in the canadian census of agriculture is summarized for each county rather than spatially for each field a monte carlo simulation was used to randomly assign each farm field polygon a cover and management practice based on the proportion of occurrences for each in the watershed while this approach does not allow us to have a deterministic outcome it has the advantage of being able to identify the most erosive farm fields in the watershed and isolate what specific management activities are associated with unsustainable rates of soil erosion for each farm field a prescriptive approach for land management activities is largely prohibitive at the watershed scale the monte carlo simulation was run 100 000 times per study year to generate a comprehensive distribution of potential soil erosion estimates for the watershed by multiplying the usle factors of each farm field together using each possible arrangement of crop type tillage and land use for each simulated year the estimated soil erosion value for each farm field was stochastically modified by 20 to coincide with the natural variation in empirical soil erosion measurements e g 20 variation in replicate erosion plots wendt et al 1986 2 2 2 soil loss usle and sediment delivery ratios conservation efforts are often focussed on ameliorating the off site impacts of agricultural soil loss e g pollution of adjacent surface waters rather than on site sediment redistribution from soil erosion while sediment yield from a watershed is impacted by the amount of arable land and cropland within the watershed vanmaercke et al 2015 and it is well established that agricultural activities accelerate the rate of soil erosion montgomery 2007 extending usle results outside of the models intended design space to predict soil loss from fields to waterways with a sediment delivery ratio sdr is a dubious task relating gross erosion to sediment yield has been classically done using a single sdr whereby the fraction of gross erosion that is transported out of a watershed is expressed as s d r y e where y is the sediment yield at the watershed outlet and e is the gross erosion of the watershed e is typically represented by usle soil erosion estimates e g fistikoglu and harmancioglu 2002 amore et al 2004 pandey et al 2007 hui et al 2010 rizeei et al 2016 singh and panda 2017 and the sdr indicates what percentage of eroded agricultural sediments are leaving the watershed usda 1971 the first problem with using the usle to represent the gross erosion of a watershed e is that agricultural sediments are not the only source of sediments contributing to sediment yield and measurements of sediment yield are only reflective of the fine material transported in the waterway waterways have a natural baseline sediment yield that is augmented not only by eroded agricultural sediments but also sediment from subsurface tile drainage in stream erosion and changes of sediment storage within the system itself furthermore agricultural sediments are a highly variable component of watershed sediment budgets it has been estimated that in tile drained agricultural systems sediment from field drains can account for 28 29 walling et al 2002 to 51 55 walling et al 2002 of the catchment sediment yield and can be a significant source of particulate p macrae et al 2007b king et al 2015 while in other landscapes in stream erosion can be the dominant supplier of sediment and nutrients e g 90 94 kronvang et al 2013 the second problem with using the usle with an sdr to represent the delivery of eroded agricultural sediments to waterways is that not all eroded sediments have an equal probability of being conveyed to a catchment outlet a distributed approach is required to capture the field to field variation in landscape conditions that drive sediment delivery to waterways each agricultural field has a unique delivery potential based on its relative position to a waterway and the filtering efficiency of the adjacent riparian zone we can express the soil loss from fields to waterways of each agricultural field f as a function of each catchment within the field s d r f j 1 n e j f j j 1 n e j i 1 m e i where sdrf is the sediment delivery ratio of a farm field to a waterway e is the soil erosion rate t of an agricultural catchment f is the filtering efficiency of the riparian zone j is an index for the number of catchments on a farm field that have a potential to produce soil loss to waterways and i is an index for the number of catchments on a farm field that are unlikely to produce soil loss to waterways i e the catchment does not direct runoff towards a waterway to calculate the sdrf of each field our recommended modelling approach uses a distributed approach whereby e is the usle soil erosion rate of an agricultural catchment within a field the average filtering efficiency of riparian zones is defined by yuan et al 2009 and j and i are calculated in arcgis v10 6 1 based on the direction of overland flow and proximity to waterways appendix a when using the usle to represent the soil erosion rate of a farm field it is important to note that the usle does not account for depositional processes at the bottom of a hillslope so there will be a disconnect between the soil erosion rate and rate of soil loss from the field to account for this we re ran our monte carlo simulation to calculate soil loss from farm fields to waterways using a stochastic estimate of in field depositional processes 25 to 75 and additionally included a stochastic estimate of the filtering efficiency of riparian zones 20 based on yuan et al 2009 the range of these stochastic estimates illustrate considerable uncertainty associated with predicting soil loss to waterways with usle which is beyond its intended design however the stochasticity is necessary to derive a range of quantitative outcomes that capture the behaviour of the agricultural system and do not incorrectly emphasize a single estimate 2 3 variability in model outcomes since there is no standardized and accredited setup for up scaling the usle different methodological implementations for up scaling usle factors will contribute to uncertainty in model outcomes while certain up scaling approaches are conceptually poor e g calculating the c factor using a single year of airborne imagery it is difficult to provide guidance on what constitutes a correctly up scaled implementation of each usle factor we focus on quantifying the variability in model outcomes rather than error or uncertainty because the size of our study area precludes a validation of erosion rates to quantify the variability in model outcomes we compare the recommended modelling approach from our case study with 1 different design choices for implementing individual usle factors 2 a model implementation synonymous with the most common approach in a sampling of literature and 3 national soileri clearwater et al 2016 and global studies on soil erosion borrelli et al 2017 to select different design choices for implementing individual usle factors we searched google scholar with the keywords usle watershed and erosion to understand the most commonly used usle methodologies among the first ten relevant papers fistikoglu and harmancioglu 2002 amore et al 2004 erdogan et al 2007 pandey et al 2007 dabral et al 2008 hui et al 2010 devatha et al 2015 belasri and lakhouili 2016 rizeei et al 2016 singh and panda 2017 and supplemented this with several hand selected papers van der knijff et al 1999 grimm et al 2003 prasuhn et al 2013 soileri clearwater et al 2016 borrelli et al 2017 these publications have been cited on google scholar a total of 2269 times and use the usle in a gis environment to evaluate distributed soil erosion rates at large spatial extents 3 results 3 1 soil erosion usle for contextualizing our results the rate of natural soil regeneration is less than 1 t ha 1 yr 1 montgomery 2007 with most conservation efforts focusing on keeping soil erosion rates under 5 to 11 t ha 1 yr 1 e g ontario guidelines from omafra are 6 7 t ha 1 yr 1 our recommended modelling approach summarized across 100 000 monte carlo simulations calculated a 5 year average soil erosion rate of 6 66 t ha 1 yr 1 table 2 for the upper nith watershed with a lower bound of 5 22 t ha 1 yr 1 and an upper bound of 8 18 t ha 1 yr 1 in this context soil erosion represents soil loss from hillslopes to anywhere in the landscape including both on site sediment redistribution and off site soil loss investigating the effects of a conventional tillage versus a conservation tillage demonstrated a substantial reduction in erosion rates when using a conventional tillage system for the watershed i e moldboard corn moldboard soybean no till winter wheat e g meinen and robinson 2021 we identified that 1421 farm fields 29 18 of agricultural land would have an unsustainable rate of soil erosion i e 11 2 t ha 1 yr 1 while under a conservation tillage system i e chisel corn chisel soybean no till winter wheat only 377 farm fields 6 57 of agricultural land would have a soil erosion rate greater than 11 2 t ha 1 yr 1 table 3 the moldboard plow inverts the topsoil burying the majority of crop residue and leaving a rough soil surface while the chisel plow breaks up the soil surface but does not invert the topsoil like the moldboard plow moldboard plows are typically used at a greater depth than chisel plows and do not leave sufficient biomass on the field surface to protect against water erosion the average of all our monte carlo simulations calculated that 805 farm fields 15 55 of agricultural land are likely eroding at a rate greater than 11 2 t ha 1 yr 1 the northern half of the watershed is characterized by very low erosion rates whereas fields in the south western portion of the watershed have high soil erosion rates resulting from steeper slopes and soils more susceptible to water erosion i e silty loams fig 2 a conservation efforts focussed on the fields that have an unsustainable rate of soil erosion 11 2 t ha 1 yr 1 fig 2a would ameliorate the economic and environmental impact of soil erosion while conservation efforts focussed on the northern half of the watershed would likely have no significance 3 2 soil loss usle and sediment delivery ratios soil erosion has on site impacts e g tillage erosion affecting crop yield lobb et al 1995 van oost et al 2006 while soil loss has both on site e g lost topsoil affecting crop yield den biggelaar et al 2001 and off site impacts e g eutrophication of local water bodies michalak et al 2013 to estimate soil loss from fields to waterways we applied a field scale sdrf to our usle results and summarized the results across 100 000 monte carlo simulations our modelling results estimated that soil loss from fields to waterways ranged from 6083 to 12 256 t yr 1 mean one standard deviation sdrf 2 30 4 63 with a 5 year average soil loss of 9170 t yr 1 sdr 3 47 0 23 t ha 1 yr 1 table 4 field scale results showcased that the 20 of fields with the highest rates of soil loss were responsible for 77 of the total soil loss with the tail end of this distribution contributing the majority of sediments fig 2b a large portion of farm fields with high erosion rates had low estimates of soil loss since they were disconnected from waterways e g fig 3 b and c for estimating the environmental impact of soil loss from agricultural fields to waterways relative to other sources of sediment we compared the estimated soil loss from farm fields to the sediment yield measured at the outlet of the upper nith watershed if we assume that all the sediment that entered the nith river is transient eroded agricultural sediments entrained and transported via overland flow would represent a maximum of 30 50 61 45 of the watershed s sediment yield sdrf 2 30 4 63 from 2010 to 2014 based on a qualitative visual analysis of fields in the watershed we hypothesize that in field depositional processes are substantial and that the soil loss is likely closer to the lower bound 3 3 variability in model outcomes 3 3 1 different design choices for individual usle factors to quantify the variability in model outcomes for different implementations of usle factors we recalculated the average soil erosion rate for the upper nith watershed using different design choices for individual usle factors commensurate with the most common approaches in literature the outcome of using different design choices in a usle implementation led to a range in soil erosion estimates of 3 04 11 02 t ha 1 yr 1 for the upper nith watershed with significant spatial discrepancies between model outcomes table 5 this high level of variability can be constrained and challenges with equifinality ameliorated by making improved up scaling design choices for example c factor a correct implementation of the c factor should include land use crop type tillage type and crop rotations a more detailed time integrated c factor should also include planting harvest and crop stage dates when using simple look up tables that did not include a human management component i e they only represented crop types there was a range in model outcomes of 9 44 9 81 t ha 1 yr 1 which over predicted soil erosion rates relative to our recommended modelling approach i e 6 66 t ha 1 yr 1 the uncertainty of using look up tables is best exemplified when you consider that land management practices are what drive soil erosion in agricultural systems not including the human dimension indicates that the model user has no understanding of the impacts of the human component of agriculture the polygon based discretization of the landscape used in our recommended modelling approach rather than a raster based discretization is a pragmatic approach for spatially assigning the human dimension of the c factor on a field by field basis l factor in lieu of a topographic based delineation e g using a gis of specific catchment areas a constant hillslope length can be used to avoid propagating model uncertainty if the model user only has access to coarse resolution dems using constant hillslope lengths of 50 m and 122 m led to model outcomes of 5 76 t ha 1 yr 1 and 8 22 t ha 1 yr 1 respectively with both approaches having a high spatial correlation with our recommended modelling approach both the range in model outcomes i e 6 18 11 02 t ha 1 yr 1 and spatial correlations were much poorer when a distributed gis approach was used with coarse dems relative to our recommended modelling approach dem resolution since dem resolution has a significant impact on model uncertainty and is used to calculate both the l factor and s factor we recommend using a dem resolution of no less than 10 m for an accurate discretization of specific catchment areas and calculation of slope gradients coarse dem resolutions e g 30 m are unable to model the slope of small topographically complex farm fields e g 5 ha and will lead to a poor discretization of specific catchment areas implementing the l factor and s factor with 90 m dems led to model outcomes of 11 02 t ha 1 yr 1 and 3 04 t ha 1 yr 1 respectively and was the largest source of uncertainty in modelling outcomes compared to our recommended modelling approach coarse dems may be appropriate for regions that are less topographically complex with larger farm field sizes but they were a poor choice for modelling the upper nith watershed 3 3 2 literature based model implementation using the most commonly represented methodology in our sampling of literature i e a literature based model implementation we calculated a 30 m raster of annual long term rates of soil erosion per pixel in the upper nith watershed for the literature based model implementation the r factor was selected from an annual isoerodent map for ontario figure r 1 p 46 wall et al 2002 annual c factors were selected from a look up table in the ruslefac handbook table c 3a p 91 wall et al 2002 and spatially assigned using the 2015 30 m aafc crop inventory raster fisette et al 2013 crop types and land cover classes were derived from landsat 8 optical imagery and radarsat 2 radar imagery k factors were selected from a table provided by omafra and spatially assigned using the ontario soil survey complex polygon the l factor was calculated on a 30 m fully hydrologically conditioned dem provincial dem using the approach of moore and burch 1986 m 0 4 and the s factor was calculated on a 30 m dem provincial dem using the approach of moore and burch 1986 n 1 3 the literature based implementation of the usle calculated an average erosion rate of 4 05 t ha 1 yr 1 for the upper nith watershed the similarity in the estimated average soil erosion rates of the literature based outcome 4 05 t ha 1 yr 1 relative to our recommended modelling approach 6 66 t ha 1 yr 1 table 5 demonstrates that vastly different model implementation of the usle can have similar results the literature based outcome had a compensatory effect whereby two factors s factor r factor were underestimated and two factors l factor c factor were overestimated relative to our recommended modelling approach this compensatory effect can lead to poor spatial outcomes and challenges with equifinality for example our recommended modelling approach estimated that on average 805 farm fields would be eroding at an unsustainable rate i e 11 2 t ha 1 yr 1 the literature based approach when summarized at the field level identified that 419 farm fields would be eroding at an unsustainable rate with only 60 4 of those fields being the same as our recommended modelling approach a simple comparison between the literature based l factor and the l factor from our recommended modelling approach shows little spatial correspondence i e spatial correlation of 0 34 at the farm field scale and a visual comparison shows significant discrepancies between model outcomes fig 4 the uncertainty associated with the literature based approach relative to our recommended modelling approach can be further exemplified by the c factor in our recommended modelling approach we used a time integrated c factor where each field was assigned a detailed description of crop types crop rotations planting and harvest dates based on crop type tillage types and land use for each cropstage the literature based approach calculated an annual c factor from a single year of airborne imagery i e each cell in the raster was assigned a crop type and the remaining management practices were estimated there is no understanding of the land management practices that drive soil erosion in the literature based approach both the human and natural components to be fully modelled for a meaningful model outcome 3 3 3 national and global studies to further emphasize the disparity among different usle applications and model implementations we compared the results from our recommended modelling approach to the most rigorous national estimate of soil loss in canada the soileri created by aafc canada clearwater et al 2016 the soileri represents soil loss from both water and tillage erosion and is calculated using soil landscape of canada slc polygons and a combination of the usle and rusle2 when we aggregated the soileri to the same spatial aggregation as our recommend modelling approach i e summarized the values for each farm field polygon the soileri over predicted the rate of soil erosion by 9 07 t ha 1 yr 1 136 2 difference relative to our recommended modelling approach soileri estimates 15 73 t ha 1 yr 1 recommended modelling approach 6 66 t ha 1 yr 1 this is sharply contrasted by the global study of borrelli et al 2017 that underpredicted soil loss by 5 94 t ha 1 yr 1 89 2 difference in the upper nith watershed relative to our recommended modelling approach borrelli et al 2017 estimate of 0 72 t ha 1 yr 1 while it is important to acknowledge the considerable data constraints of national and global erosion estimates and that the soileri estimates of soil erosion will be marginally higher since they include a tillage erosion component there was a significant disparity between all three usle applications despite the same model being applied 4 discussion 4 1 why the usle for an environmental problem to be a prominent issue on the world stage and to garner the interest of conservation groups rigorous measurements are needed to quantify the severity of the problem socio economic drivers and associated costs soil erosion is widely recognized as a global problem e g status of the world s soil resources report fao 2015 whereby soil erosion has been identified as one of the main threats to agricultural sustainability but somewhat paradoxically these conclusions are drawn from a paucity of data the sentiments of trimble and crosson 2000 aptly reflect why this is a problem it is questionable whether there has ever been another perceived public problem for which so much time effort and money were spent in light of so little scientific evidence p 248 these sentiments are echoed by boardman 2006 we have difficulty in the recognition description and quantification of erosion and limited information on the magnitude and frequency of events that cause erosion the inadequacy and frequent misuse of existing data leaves us open to the charge of exaggeration of the erosion problem a la lomborg p 73 the conclusions drawn from the paucity of data on soil erosion may be perceived as pragmatic to garner public interest albeit rightly open to criticism criticisms stem primarily from the generalization of plot based erosion measurements that are extremely difficult to source and not suitable for generalization for example pimentel 2006 is well cited 1001 citations on google scholar and identifies a worldwide erosion rate of 30 t ha 1 yr 1 sourcing pimentel et al 1995 for the erosion estimate pimentel et al 1995 cites barrow 1991 for an erosion rate for asia africa and south america of 30 40 tons ha 1 yr 1 and an erosion rate of 17 tons ha 1 yr 1 for the united states and europe however barrow 1991 estimate is derived from lal et al 1989 table iii using a synthesis of research from 24 countries that includes the following disclaimer the data used in this table comes from a wide range of sources and is derived through a wide range of sampling methodologies it is therefore not standardized and serves as only a general indication table 10 6 barrow 1991 while this synthesis of research is useful and informative the soil erosion rates found in table iii of lal et al 1989 are sourced from eight different documents barber 1983 fournier 1967 humphreys 1984 lal 1976a 1976b ngatunga et al 1984 roose 1977 world resources institute 1986 which are mostly inaccessible and the few that are available were based on small plot based studies e g tanzania ngatunga et al 1984 lal et al 1989 warn that the data obtained from small plots are often not comparable misinterpretation and erroneous conclusions are major worries when using such data p 58 while the nesting of sources and lack of citing primary literature is a concern the issue lies in the use of data that has explicitly cautioned its use and lack of comparability and scalability a similar example is offered when searching for a primary source from the world resources institute 1986 citation in table iii of lal et al 1989 the world resources institute 1986 table 5 5 p 270 sources a compiled list of cropland soil erosion rates from the world resources institute and international institute for environment and development which has an additional 16 references one of these references is the estimate of soil erosion in central belgium sourced from richter 1983 where richter 1983 is not the primary source but cites bollinne 1982 in text eight year measurements carried out in central belgium showed soil losses of 10 25 tonnes per hectare per year p 11 bollinne 1982 however bollinne 1982 did not calculate an average erosion rate for central belgium but calculated the erosion rates of 12 small experimental erosion plots in central belgium while the scaling of small plot based studies to large spatial extents may be the only approach at a particular moment in time it embeds substantial errors in the estimate due to the disconnect in space and time between plots and their broad areas of application boardman 1998 and crosson et al 1995 have a more detailed discussion on the perpetuation of the poorly sourced estimate of pimentel et al 1995 due to a paucity of data on soil erosion measurements and the frequent erroneous up scaling of plot based based studies in both space and time modelling is required to gain an understanding of the extent of erosion at large spatial extents for example the most rigorous global application of the usle estimates a global soil erosion rate of 2 8 t ha 1 yr 1 borrelli et al 2017 all land uses which is in stark contrast with the widely cited global erosion rate of 30 t ha 1 yr 1 estimated by pimentel 2006 cropland and pastures while the predictive accuracy of erosion models can be evaluated at the farm field scale meinen and robinson 2021 model evaluation is much more challenging at large spatial scales due to a lack of spatially distributed erosion data and therefore models should be up scaled in a rigorous manner that does not introduce significant uncertainty or error in their outcomes without proper model execution and rigorous analysis we are in danger of discrediting our results which can erode public trust and sustainability efforts while the usle has the ideal characteristics of a large scale model i e a simple parsimonious modelling structure our modelling results highlight the uncertainty implicit to up scaling the usle to the watershed scale our recommended modelling approach calculated an average erosion rate of 6 66 t ha 1 yr 1 the outcome of using different design choices led to a range in model outcomes from 3 04 to 11 02 t ha 1 yr 1 changing the implementation of all the usle factors with a literature based methodology led to a modelling outcome of 4 05 t ha 1 yr 1 a comparison with literature showed a range in model outcomes from 0 72 t ha 1 yr 1 to 15 73 t ha 1 yr 1 while the usle is a simple empirical model the variability in model outcomes demonstrates the challenges in up scaling the usle to a large spatial extent a standardized and accredited methodology for up scaling the usle is needed to reduce uncertainty in modelling results our analysis provides a first step toward discussing this standardization by quantifying the impacts of different design choices on erosion estimates and when specific design choices should be made 4 2 challenges with using the usle and sediment delivery ratios while we present a quantitative model outcome for coupling the usle with a field scale sdr i e sdrf to predict soil loss from fields to waterways by defining a wide range of stochastic uncertainty in our monte carlo simulation i e 25 to 75 in field deposition 20 soil erosion rate 20 filtering efficiency of riparian zones the complexity of human natural systems often precludes a quantitative interpretation of results this complexity and the challenges of modelling sediment delivery in agricultural systems are best exemplified when looking at overland flow paths on satellite imagery for example in the upper nith watershed overland flow frequently routed to artificial drainage structures at field edges accompanied by large depositional plumes and drained directly to the nith river via surface inlets and subsurface tiles or to an adjacent field or ditch via a culvert when we examined historical airborne imagery we found that the majority of fields with visible signs of water erosion that also had connectivity to the nith river were associated with artificial drainage issues from culvert and field tiles further complicating this drainage patterns and flow connectivity changed considerably over a short period of time due to new land management practices e g fig 5 the implementation of a more complex environmental model in lieu of our sdrf methodology will not provide more meaningful results unless field scale features and their connectivity e g location of drainage structures are empirically captured and their effects on overland flow explicitly modelled studies that couple the usle with an sdr should generally be treated as qualitative due to the complexity of agricultural systems the inability to validate model results and since the application of an sdr is so far outside of the models intended design space 5 conclusions it is imperative that a standardized and accredited usle setup is firmly established in the literature for model up scaling that has results synonymous with agricultural handbook no 537 wischmeier and smith 1978 or agricultural handbook no 703 renard et al 1991 for revised usle applications in a gis cross comparisons of usle modelling results cannot be conducted without a standardized modelling approach while we cannot provide guidelines for what constitutes an appropriate remote sensing dataset for estimating model input parameters or a methodology for up scaling from the field to watershed scale we provided a first step in this direction by demonstrating the impacts of different design choices and up scaling methodologies on model outcomes pragmatism alone is not enough to justify a data source or modelling endeavour the ideal model for large scale environmental assessments of soil erosion will have a high intrinsic model error but low model input error unfortunately even with the simple parsimonious modelling structure of the usle poor results can be driven by model input error and user error if the usle continues to be used for large scale environmental assessments of soil erosion more rigor needs to be employed by model users to ensure that modelling results are not invalidated by poor design choices declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to acknowledge with gratitude infrastructure funding provided by the canadian foundation for innovation 32247 the ontario research fund flir systems the natural sciences and engineering research council nserc of canada 06252 2014 547496 2020 as well as the ministry for research and innovation for support through the early researcher award er15 11 198 and the faculty of environment at the university of waterloo for student funding in addition we would like to thank omar dzinic for his assistance with modelling appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105229 software availability the implementation of the usle in this study is based off of agricultural handbook no 537 wischmeier and smith 1978 and was conducted in arcgis 10 6 1 using freely available municipal provincial and federal datasets a tutorial for each usle factor is included in the attached appendices r factor wellesley dam meteorological station grand river conservation authority 2010 2014 k factor ontario soil survey complex ontario ministry of agriculture food and rural affairs 2019 l factor lidar dem ontario ministry of natural resources and forestry 2021 s factor lidar dem ontario ministry of natural resources and forestry 2021 c factor census of agriculture statistics canada 2016 imagery southwestern ontario orthoimagery project land information ontario 2015 waterways watercourse and waterbody grand river conservation authority 2021 
25711,environmental models are often essential to implement projects in planning consulting and regulatory institutions research models are often poorly suited to such applications due to their complexity data requirements operational boundaries and factors such as institutional capacities this contribution enhances a modeling framework to help mitigate research model complexity streamline data and parameter setup reduce runtime and improve model infrastructure efficiency using a surrogate modeling approach we capture the intrinsic knowledge of a conceptual or process based model into an ensemble of artificial neural networks the enhanced modeling framework interacts with machine learning libraries to derive surrogate models for each model service this process is secured using blockchain technology after describing the methods and implementation we present an example wherein hydrologic peak discharge provided by the curve number model is emulated with a surrogate model ensemble the ensemble median values outperformed any individual surrogate model fit to the curve number model keywords surrogate modeling cloud services framework integration framework architecture blockchain service delivery 1 introduction many environmental problems are studied and expressed through the application of conceptual or process based models which are simplified representations of physical systems wheater et al 2012 they result from idealized understandings of real world phenomena described through mathematical concepts and translated into computer code razavi et al 2012 alizadeh et al 2020 advances and development efforts in model research most often focus on improving simulation accuracy increasing spatial and temporal resolutions and adding processes and their interactions serafin 2019 improved and refined equations and numerical methods are continuously derived and implemented e g casulli 2017 tubini et al 2017 dumbser et al 2018 advancements in sensor technologies support the collection of observed data and provide an increasing number of input or calibration variables akinmolayan et al 2018 anees et al 2018 bayramov et al 2019 additionally the combination of complex methods for calibration as well as sensitivity analysis maximize model prediction accuracy green et al 2015 as a result conceptual and process based models have become complex software efforts which strive to more accurately represent real world processes within a given scope and domain serafin 2019 in this regard the evolution of swat soil water assessment tool arnold et al 2012 is a representative example swat is a watershed model that simulates water quality and quantity of surface and subsurface waters to assess the environmental effects of different land use and management practices as well as climate change from its early predecessors and development in the 1990 s to its current state the addition of new features such as management of multiple hydrological response units hrus and more detailed hydrological and water quality processes increased modeling capabilities and complexity williams et al 2008 arnold et al 2020 these innovations may provide more accurate simulations though increased complexity does not ensure greater accuracy gilroy and mccuen 2011 but require higher user proficiency and experience to master them which result in a steeper learning curve and more tedious model setup for example serafin 2019 compared four consecutive swat releases v2000 v2005 v2009 and v2012 to highlight how the overall total lines of code increased from 16 308 to 34 113 and input variables and model parameters for simulation set up increased from 540 up to 612 model documentation likewise grew to allow swat users to set up new modeling solutions and increased from 450 to 650 pages a modeling solution results from connecting modeling components through a modeling framework to create an application here every component could be a model service a data service or any self contained computational sub module that can be reused in different contexts to create a different modeling solution overall this demonstrates the progressive advancement of model flexibility and capability as well as intricacy and complexity long known challenges exist to fully comprehend correctly parameterize efficiently execute and flexibly deploy process based models research institutions usually conduct the development of the models within science performance criteria but do not usually focus on their integration into external organizations and workflows such organizations referred to here as service delivery organizations sdos could be regulatory entities action agencies consultancies or for profit and non profit institutions integrate such models into their processes and workflows but may find this process challenging resource intensive and time consuming these projects often fail because the research model has become unwieldy and too burdensome to use in a practical environment mcphee and yeh 2008 ismail et al 2008 cai et al 2004 thiessen and loucks 1992 therefore sdos need an approach for integration of modeling technology that is technically straightforward but also scientifically sound models are expected to produce results within reasonable operational expectation they should require limited set up and possibly reduced data entry while taking advantage of existing organization wide data resources and infrastructure wheater et al 2012 devia et al 2015 the use of environmental modeling frameworks emfs and cloud computing platforms ccps like the object modeling system and the cloud services integration platform oms csip david et al 2013 2014 lloyd et al 2012 have alleviated some of the complexity implications for model users david et al 2012 lloyd et al 2011 by using concepts such as software encapsulation process based models can be turned into model components and web services as model as a service constructs zou et al 2012 the introduction of emfs and ccps in environmental modeling addresses the run time issue data preparation and reduces expenditures for software development and maintenance david et al 2013 emfs and ccps simplify a user s approach to a simulation model by providing an access protocol with a payload format for example json might be the preferred format for a model request response interaction via a restful http api fielding and taylor 2000 additionally the architectural design of emfs and ccps allows for leveraging innovative hardware infrastructure like super computing environments and computer clusters david et al 2013 2014 lloyd et al 2012 2018 zhu et al 2019 however such an approach does not lessen a user s burden of collecting data setting up a modeling solution by connecting framework compliant modeling components or handling calibration and sensitivity analysis processes this contribution bridges the multi faceted gap between research and service delivery by deriving an enhanced framework that links process models with surrogate models fig 1 in this context we propose a machine learning ml based surrogate modeling approach that aims to capture the intrinsic knowledge of a process model as an ensemble of artificial neural networks anns this contribution circumvents issues of directly applying research models due to their inherent complexity in data and parameter setup infrastructure and runtime requirements the framework accommodates the application needs to achieve quick and sufficiently accurate model results using a set of limited inputs and limited a priori knowledge of the internal processes here the data driven methodology is used as an emulator of any deterministic simulation model this contribution takes the model integration concept a step further by using a framework integrated model to derive a surrogate for a process model thus providing an enhanced modeling framework to help mitigate research model complexity streamline data and parameter setup reduce runtime and improve model infrastructure efficiency the oms csip framework was extended to capture and store relevant model simulation data and to derive the surrogate model at the modeling framework level it interacts with machine learning libraries nosql databases and a set of csip services to create model surrogates here neuroevolution of augmenting topology neat stanley and miikkulainen 2002 whiteson et al 2005 techniques in an ensemble application are combined with ann uncertainty quantification mongodb for data storage and blockchain technology for validating and tracing surrogate model origins this paper is organized as follows a summary of related work is presented in section 2 section 3 introduces research methodologies that support the integration of ml libraries into framework architecture to expand framework capabilities section 4 illustrates the initial experiment conducted on a synthetic dataset to test and validate the technology finally section 5 summarizes conclusions challenges and future work 2 background surrogate modeling concepts date back to blanning 1974 where the author conceptualized the need for surrogate models or meta models for sensitivity analysis purposes kleijnen 1975 provided statistical tools to operationalize blanning s theory since then this idea has advanced and has been utilized widely in applied science and engineering studies requiring large volume simulation runs alizadeh et al 2020 viana and haftka 2008 such as optimization uncertainty analysis operational management and prediction beh et al 2017 asher et al 2015 razavi et al 2012 surrogate models attempt to mitigate long simulation times by eliminating real time application of their complex parent models alizadeh et al 2020 there are several mathematical techniques available for approximating the behavior of original models alizadeh et al 2020 reviewed over 200 papers on surrogate modeling in engineering applications mainly focusing on data driven methods fernández godino et al 2016 analysed more than 150 papers to give an overview of multi fidelity surrogate models razavi et al 2012 and asher et al 2015 provided the most comprehensive taxonomy for surrogate models identifying three main categories as follows 1 projection based surrogate models make use of the projection of actual model equations into a basis of orthogonal vectors to reduce the size of the initial vector space the most popular methods applied to engineering related topics are frequency weighted balanced truncation e g sahlan et al 2013 proper orthogonal decomposition e g mcphee and yeh 2008 volkwein 2013 and dynamic mode decomposition e g ghommem et al 2013 bistrian and navon 2015 mahmoodian et al 2018 pointed out that these techniques are computationally efficient and may provide an associated error bound willcox and megretski 2005 however these methodologies require ad hoc mathematical analysis of each equation implemented in the original model which is possible by source code analysis and source descriptions of modeling algorithms these methodologies reduce both simulation run time and input space dimensionality while preserving basic physics of the model itself nonetheless they result from dedicated mathematical study of the original models and are not suitable for emulating highly complex systems asher et al 2015 2 hierarchical or multi fidelity surrogate models are generated from the original models by 1 reducing numerical accuracy using coarser spatial temporal grid size e g chung et al 2016 chung et al 2018 or 2 ignoring approximating some physical processes e g hou and wu 1997 sangalli 2003 shi et al 2012 in the first case reducing numerical accuracy only slightly addresses the run time issue this type of complexity reduction allows for obtaining accurate results because physical components are identical to the original model however a simulation setup still requires the entire input dataset and developer level understanding of computational model structure the second option partially takes on the issues of 1 speeding up simulation run time 2 requiring a smaller input dataset and 3 simplifying the inner modeling complexity nonetheless these kinds of models might still have long computational times require calibration procedures and large input datasets furthermore they are not able to return accurate results because it is not possible to compensate for simplified physical processes of a system with corrective parameters 3 data driven surrogate models or response surface models are statistical or empirical models capable of capturing and approximating the original model behaviour the response surface typically a nonlinear hyperplane by learning the existing nonlinear relationship between a set of original input output snapshots there is no direct emulation of any inner conceptual physical process described in the original simulation model kianifar et al 2020 the literature reports a variety of approximation methodologies employed in surrogate modeling such as kriging e g babaei et al 2015 support vector machine e g azamathulla et al 2016 generalized polynomial chaos expansion e g meng and li 2018 and dynamic mode analysis e g young and ratto 2011 in the most recent literature anns are among the most commonly used techniques e g nguyen et al 2019 lima et al 2018 shaw et al 2017 anns are flexible function approximators they allow for emulating specific aspects of the original model behaviour by using only the most relevant input parameters of the original model decreasing simulation run time and requiring no user knowledge of the original model however standard methods such as multilayer perceptrons mlps require several subjective decisions to develop and apply a proper ann these decisions involve selection of 1 the optimal number of neurons for input and output layers 2 the number of hidden layers 3 the number of neurons in each hidden layer and 4 the type of transfer function as a result surrogate model setup entails an iterative trial and error approach in summary different types of surrogate models and their utilization to applied science and engineering topics have been studied extensively and constitute an active field of research projection based and multi fidelity surrogate models originate from actual simplifications of conceptual physical processes of original simulation models and are tightly coupled with model internals in contrast data driven surrogate models completely decouple from original simulation models because they are statistically or empirically based therefore there is no connection between the structure of data driven surrogates and any emulated conceptual physical process maier 2013 provided a useful resource to guide this literature review identify scientific gaps and formulate key objectives the literature reports algorithms for automatic calibration of several of the previously mentioned data driven methods usually including the use of fixed optimal structures e g practical water supply forecasting specific autonomous machine learning solutions implemented by fleming and goodbody 2019 neat integrates recent advancements in evolutionary intelligence to couple ann creation and training with neuroevolutionary algorithms to overcome previous mlps manual adjustments this accommodates the requirement of automatic generation of the surrogate model and streamlines its integration into a framework workflow to our knowledge however there are no applications of neat in environmental surrogate modeling additionally none of the papers we reviewed highlights an automated process that connects each and every step going from original model data set collection to surrogate model creation and training followed by running the surrogate model as a result our research intends to generalize a surrogate modeling approach by addressing the following objectives 1 automate the process of surrogate model creation by integrating neuro evolutionary algorithms neat within the workflow of an environmental modeling framework 2 implement neat as a useful method for surrogate modeling of environmental topics 3 guarantee consistency in the process of developing the surrogate model sm and track the sm back to the original model once it is deployed 3 research methods our methodological approach is comprised of the following pillars 1 neuroevolution of augmenting topologies neat provides the evolutionary algorithm capable of creating the structure of the ann while adjusting connection weights during the training phase stanley and miikkulainen 2002 the feature selective neat fs neat extension introduces an implicit dimensionality reduction mechanism to select only input parameters that yield the best ann performance whiteson et al 2005 2 uncertainty quantification of surrogate model ensemble sme results handles neat inherent stochasticity to create a more robust and flexible original model emulator with uncertainty quantification uq 3 integration of neat and sme within a modeling framework streamlines the automatic generation of the sme at a framework level abstracted to the fens framework enabled neuroevolutionary built surrogate modeling architectural style 4 fens blockchain integrates blockchain technology yli huumo 2016 in the fens architecture to consistently validate the process of sme generation and track the sme from the original process model and associated input data to its deployment and final results our technical approach and implementation integrates software libraries apis platform extension and web services to apply the concepts described in the methodological approach 1 software libraries apis databases and platforms include mongodb chodorow 2013 for data storage encog3 heaton 2015 machine learning framework for the neat algorithm csip and csip api david et al 2013 2014 lloyd et al 2012 model as a service maas platform and its open api providing access to simulations as cloud based web services 2 fens compliant csip surrogate modeling services pipeline automatically emerges the sme at a framework level while the integration of blockchain technology provides validation and tracking of the process from original model runs to deployed sme 3 1 methodological approach 3 1 1 neuroevolution of augmenting topologies neat and feature selective neat fs neat the neat evolutionary algorithm with its feature selective extension creates artificial neural networks anns starting from a minimal combination of input output nodes during supervised learning this artificial genetic evolution automatically builds a population of anns and selects the best one with little or no human interaction the integration of this algorithm into a modeling framework workflow enables it to automatically emerge sms from any framework compliant modeling solution neat stanley and miikkulainen 2002 simultaneously evolves topology and weights of a population of anns during supervised learning neat generates the structure of each neural network starting from minimal topology progressively adding complexity and optimizing it over generations the topology of the initial population has input nodes directly connected to output nodes with no hidden nodes streamlining the learning process and keeping the size of the search space of connection weights to its minimum fs neat whiteson et al 2005 is an improvement of the standard neat algorithm it adds a feature selection process which automatically identifies inputs that yield the best ann performance this process recognizes redundant and insignificant parameters pruning them out of the ann connection structure fs neat alleviates the chore of setting up ml systems by enabling the search algorithm to select only a subset of the input parameters that increase learning performance by imitating natural evolution and selection in biological populations neat and fs neat automate the creation and identification of the best ann this avoids the trial and error procedures typical of other mlp methods and minimizes the need for human intervention once integrated into a modeling framework workflow since fs neat extends neat functionalities neat represents both in the rest of the paper 3 1 2 ensemble of surrogate models the ensemble of surrogate models approach serafin et al 2018 serafin 2019 results from combining the stochasticity inherent to the evolutionary genetic algorithm implemented in neat with a specifically designed cross validation technique supervised learning methodologies typically feature splitting the available dataset d comprising n data into three disjoint subsets training t validation v and testing χ govindaraju and rao 2000 friedman et al 2001 may et al 2010 james et al 2013 several data splitting methods exist in the literature including simple random sampling srs knuth 2014 kennard stone sampling kennard and stone 1969 sampling based on minimization of the difference in statistics such as the mean μ and standard deviation σ shahin et al 2004 here we assume that d is a collection of model runs designed to sufficiently describe the behavior of the original model within a user identified problem domain we want to split d such that distributions of resulting subsets are similar mcgaughey et al 2016 consequently we use the kolmogorov smirnov ks statistic to verify that the distributions of the split subsets for each variable sufficiently match bowden et al 2005 ks is a nonparametric test that compares two empirical data distributions by estimating their cumulative distribution functions cdfs we can compute the cdf ℱ of n data points of a variable x such that 1 f n x 1 n i i x i x where i is the indicator function that returns 1 when x i x and 0 otherwise we want to verify the hypotheses 2 h 0 f g v s h 1 f g where the first subset x 1 x m of size m has distribution with cdf f x and the second subset y 1 y n of size n has distribution with cdf g x the ks statistic compares the two cdfs f x and g x to identify the maximum discrepancy d m n 3 d m n s u p x f m x g n x where s u p is the supremum function by knowing d m n and for large m n we reject the null hypothesis if 4 d m n c α m n m n where α is the level of significance and c α is 5 c α l n α 2 1 2 accordingly the split algorithm initially divides d into two subsets training validation t v and testing χ subsets by using identical t and v sets neat s stochasticity generates dissimilar anns which differ in their structure and connection weights as a consequence training an ensemble of anns results in several sms capable of properly emulating the model behaviour instead of selecting one sm out of a trained ensemble we design a cross validation method that leverages and amplifies the behavioural stochasticity of neat before the beginning of the training of each ann the algorithm iteratively and randomly splits t v into disjoint t and v until it verifies the ks statistic for each variable in the sets as a result of this splitting procedure we train and initially validate each ann against slightly different datasets the ensemble of surrogate models sme results from concurrently training several anns which overall use the vast majority of the samples in t v and selecting the most performant ones finally the sme collectively runs against χ providing a collection of uncertainty quantified results the comparison between original model results and the median of each sme estimate certifies the goodness of the original model emulation 3 1 3 integration of neat and sme within a modeling framework the integration of neat and sme within an emf automates the creation of surrogate models from research models by extending the modeling framework workflow serafin et al 2018 serafin 2019 emfs manage and preprocess model parameter input facilitate model execution and postprocess and analyze outputs here we tailor the emf to derive an sme from a pm by streamlining all sm generation steps fig 2 illustrates the conceptual design highlighting four phases that we discuss consequently the first phase involves research modeling simulation runs where the emf provides consistency and traceability of model results in addition to seamless access to model inputs and outputs these are the information the data driven training algorithm recursively learns from thus success depends on the selection of the most sensitive parameters to describe the behavior of the physical process to emulate as well as the design of the experiment enabling sampled simulation runs to create a homogeneous input space serafin et al 2020 as the model runs the emf harvests and stores input output snapshots of each simulation we identify this first step as generation of dataset d gen in fig 2 the training validation of the sm phase train validate in fig 2 involves the creation of the response surface model box and wilson 1951 by feeding the supervised training algorithm with t and validating the resulting sm against v the stochasticity of the neuroevolutionary algorithm and the specifically designed cross validation procedure allow for generating several different sms which are stored in the database after validation together with prediction estimators the latter are necessary in order to select the most performant sms discarding low performing or incompletely trained sms this step is the uncertainty quantification phase uq in fig 2 selected anns form the sme characterized as a whole by quantifying their uncertainty in emulating original model behaviour the final phase is the run of the sme where the sme is packaged and deployed as a more compatible option for integration with environmental assessment program applications run in fig 2 the foregoing demonstrates how the concept of integrating neat and sme within emf has the potential for automatically emerging surrogate models from research models we identify and connect four fundamental phases into a sequential and ordered pipeline where the previous phase feeds the following one in a systematic way however the translation of this abstract concept into an actual implementation requires a further effort to highlight basic elements and constraints that regulate the relationships among each element of the entire system the resulting architectural style should in turn allow widespread implementation of this abstract concept on any modeling framework consequently we introduce framework enabled neuroevolutionary built surrogate modeling fens as an abstract model of the extended modeling framework workflow fens attempts to bridge the gap between research environments which develop conceptual and process based models and sdos which need easier access to simplified models by streamlining their communication and facilitating the transfer of knowledge fig 1 this knowledge initially resides within framework compliant models therefore we need to expose it translate it and bundle it into easier to use tools this is achieved by combining different software architectural styles to extend the current state of art emf workflow and derive smes from original models at the framework level the deployment of the smes to sdos is the last step to finalize the gap bridging process fens is a sequence or pipeline of individual service calls which can be captured and added into a workflow engine thus being reproducible if a new version of the pm is available a corresponding new version of the sme can be obtained by re running the workflow and evaluating the statistical qa qc metrics in every step of the pipeline such a process might be computational involved but can take advantage of previous fens parameter settings it has to be determined if qualitative process improvements in the pm justify an unsupervised fens re run alternatively a change in training data and parameters may be required due to extensive changes in the pm process behavior the literature review guided us to select data driven sms as the most appropriate method for our goal as a result fens starts as a data centered repository architectural style its functionalities are expanded by integrating a service oriented emf which orchestrates and organizes the data accessors in a pipeline architecture following we identify fundamental structural elements and their communication placing architectural constraints to make fens a portable system by minimizing its impact on existing emf workflow and leveraging advanced emf architecture we do not describe the details of each integrated architecture since we do not intend to alter their standard protocols 3 1 3 1 requirements fens architecture relies on three major requirements 1 an emf designed on microservice architecture 2 availability of storage capacity and 3 the algorithm that builds the sm emfs manage original model execution and seamlessly access model inputs and outputs an emf microservice architecture carries key software design principles such as modifiability extensibility simplicity and scalability fielding and taylor 2000 that allow for keeping up with evolution of requirements from research environments and sdos original mathematical models and statistical approximators evolve and improve over time consequently the fens architecture must be simple and flexible enough to allow for quickly modifying and extending its underlying system capabilities to adapt to new modeling necessities for example neat selects only input parameters that speed up and facilitate the learning process some may want to add further checks on input elements before the training phase by using methodologies such as principal component analysis pca fleming and goodbody 2019 in this example fens allows for easily interposing another phase between phase 1 generation of dataset d and phase 2 training of the sm augmenting the pipeline additionally an emf designed on microservice architecture reduces coupling between software components by introducing separation of concern and software encapsulation this makes the system scalable facilitating partial and fragmented updates since data driven methodologies simplify the automation of surrogate model creation storage capacity must be part of the architecture for seamlessly storing the permutations of input output parameters out of each original model run as well as sm hyperparameters efficient storage increases system reliability by tracking and storing partial and full results along each phase of the system allowing recovery from abrupt or expected interruptions we strongly recommend a nosql database because collections and documents enhance system flexibility since they are open to extension over fixed schemas and tables typical of a relational database the last requirement is the methodology that supports building and training the sm by only learning the relationships linear or nonlinear between original model inputs and outputs for example in case of ann based sms several mathematical algorithms can calibrate the connection weights of a fully connected ann and if we are willing to use methodologies to train the ann by evolving the weights of the fixed topology through a genetic algorithm as oppose to minimization of a loss function e g backpropagation conjugate gradient etc traditional neuroevolutionary ne systems are widely available and well tested stanley and mikkulainen 2002 stanley et al 2019 here a trial and error approach usually identifies the most suitable fixed topology ann neat is a ne algorithm but it efficiently evolves ann structure along with connection weights this feature provides additional flexibility by building and calibrating each ann ad hoc it enables further automating the building and training process because it avoids the trial and error approach to identify the most suitable structure of the ann 3 1 3 2 architectural style fens is a data centered repository architectural style with data accessors organized in a pipeline architecture and orchestrated by a service oriented emf fig 3 this definition highlights the three main constraints of the overall architecture 1 data accessors are part of the pipeline 2 emf incorporates the data accessor pipeline and becomes part of the architecture itself 3 data elements comprise of data and metadata stored in the central data store at each phase the first constraint limits the independence of the data accessors since the phases of fens follow a specifically designed pipeline data accessors cannot operate independently on the data store but must follow the pipeline order otherwise the pipeline would fail to provide valid consistent and traceable output the second constraint integrates the emf within the architecture since it is responsible for handling connections between the data accessors between the data accessors and the data store and between the data accessors and the user agents the data accessors must be framework compliant for widening potential interactions with future framework extensions and more importantly for achieving higher levels of modeling flexibility emf integration in the architecture demonstrates how the process of simplifying original models into surrogates is a modeling solution itself and provides a different perspective on the definition of surrogate models as models of models the last constraint regulates the communication between the data accessors and the data store in order to keep the pipeline alive metadata must accompany the data during each transfer when possible metadata should be leveraged to execute operations on the database side limiting slower transfer of data between the data accessors and the data store the high level schema to extend the emf workflow to automatically create sme from research models fig 2 served as a conceptual design from which we derived the fens architectural style with its requirements and constraints that provides a blueprint for integrating the automated surrogate model development workflow in any emf 3 1 4 fens blockchain the first constraint of the fens architecture identifies the need for consistency and traceability of the sme creation before deploying it for integration in program delivery environments for regulatory and consulting objectives the sme results from an ordered list of operations starting from data generation ending with final deployment to validate every generation step and prove the integrity of the pipeline we integrate blockchain technology within the fens system fig 3 a blockchain represents the glue providing the sme s provenance capturing processes attributes parameters metadata throughout all intermittent fens steps to the originating pm the pm that the sme is the surrogate for a blockchain in general is an open distributed and immutable ledger that records a sequence of operations of tangible or intangible goods lakhani and iansiti 2017 underwood 2016 each block of the chain is immutable by design since it contains its own hash as well as the hash of the previous block we achieve the consistency and traceability of the fens pipeline by creating a new block at each phase using metadata and hyperparameters of the current phase of the pipeline to create the current hash while inheriting the previous hash from the previous block this results in an immutable and tamper resistant process of sme creation and the smes deployed within the blockchain change of a parameter in one phase of the pipeline would affect the entire set of hashes and invalidate subsequent pipeline steps and finally the sme accordingly moreover the blockchain allows for tracking the deployed sme back to the original model the dataset and the hyperparameters that generated it however this poses an interesting question does the deployment of the sme represent the end of the pipeline and blockchain consequently the sme could be the first block of a more comprehensive blockchain followed by sme application blocks with untested data for example consider an agricultural producer using a web application for computing ecosystem credits or benefits sustainability claims soil carbon markets cost share program applications etc a fens blockchain integration with service delivery may be carried out as follows 1 a research environment builds and deploys the consistent and validated sme to sdos as a fens blockchain 2 sdos test and approve the sme block adding it to their program delivery chains and making the approved block available to third party users 3 other certification organizations may approve the sme block 4 third party users ecosystem credit providers in this specific context add the certified sme block to their blockchains 5 the consultant computes ecosystem credits and transacts a claim for a farmer adding to the relevant blockchain the claim is buttressed by the results from the certified sme in the chain 6 additionally these blockchains might span the entire supply chain carrying relevant information of the farming history of a product this information can potentially influence the end consumer e g at a market or grocery store who might opt for a sustainably produced product and validate its authenticity trienekens et al 2012 fens with added blockchain integration asserts and underlines the bridge between research environments and sdos supported by research models therefore the addition of blockchain as a means to validate the sme opens additional opportunities for the sme as a building block of a comprehensive enterprise integration tripoli 2018 pan et al 2019 the authors suggest and favor blockchain implementation that extracts relevant metadata about the origin of an sm solution through the fens service api here for given sm metadata such as statistical measures of the ua uq ann training and performance characteristics training data hashes input data statistics pm model version hash etc combined with location and time stamps can be presented to create a sm lineage the implementation method of such data extraction may be achieved through chaincode 3 2 technical approach and implementation the rationale for a web service centric implementation of fens is to allow for remote access highly scalable operation and institutional management of all tasks encompassing the sme generation in general terms we are utilizing a service design that can be characterized as a resource api web service api style daigneau 2011 utilizing the cloud services integration platform csip by managing resources in a nosql database mongodb with the ml framework encog we decided on encog because of its seamless integration into the csip modeling architecture and on mongodb since it is already an integral part of csip in this introduction we briefly describe csip encog and mongodb in the following subsections we analyze the web services that compose fens in detail the object modeling system cloud service integration platform oms csip was originally developed to facilitate the integration of software components models and model services david et al 2012 2013 lloyd et al 2011 2012 while first emphasizing on the creation of modular modeling solutions exhibiting implicit multi threading annotation driven component integration and multi language support the framework was extended into a model as a service offering to allow cloud based modeling solutions david et al 2014 it was used to implement 250 model and supporting data services for a variety of applications deployed at colorado state university united states department of agriculture usda other federal agencies and commercial data centers currently oms csip services integrate with modeling applications such as the usda nrcs integrated erosion tool iet resource stewardship evaluation tool rset conservation assessment ranking tool cart and water supply forecasting system ewsf as well as the usda ars wepp online webrhem and ages agroecosystem green et al 2015 applications the experiences gained in integrating the models and data services at scale while implementing modeling workflows led to the obvious choice of realizing the surrogate modeling solution pipeline as a set of oms csip services here the csip service api integrates with the encog 3 ml library for ann management and mongodb nosql libraries for data management into a service package encog3 is an open source ml framework available for java and c that has been designed with scalability in mind by exploiting multicore processors heaton 2015 this framework provides a variety of ml technologies including neat mongodb is a nosql database attempting to overcome limitations of sql databases when managing big data it is built on the notion of json documents and collections to overcome lack of flexibility due to the use of fixed schemas tables and rows typical of relational sql databases chodorow 2013 mongodb is schema less allowing for data management flexibility but creates challenges for data integrity collections contain sets of documents which can potentially have different field structures data in documents are self contained which allows for easily splitting up documents across different machines data sharding and setting up active active cluster configuration plugge et al 2015 3 2 1 surrogate modeling services pipeline and blockchain technology the fens is implemented as a set of five microservices that form a service pipeline the pipeline represents the conceptual workflow steps of fens the services integrate encog for ml related processing such as training and validation mongodb for data metadata storage and blockchain technology to create store and validate the sme the five web services are responsible for 1 collecting model input output snapshots t v dataset into an application dedicated nosql database 2 normalizing the t v dataset 3 training several sms on t subset and validating them against v subset 4 selecting the utmost performant sms to create an sme and 5 deploying and running the sme against new data such as χ subset as a consequence of stepping through the pipeline each service creates and adds a new block to the blockchain with service data and metadata which makes sm modeler decisions immutable each web service digests a json payload which consists of two sections to comply with csip api metainfo and parameter see listing 1 in appendix for csip payload example the metainfo section lists service context such as information required to set up the service run e g mode async to request asynchronous run the parameter section contains an array of json objects with two mandatory tuples with predefined keys name service variable name and value value to assign each json payload sets up service specific input parameters and contains a different set of objects consequently users can retrieve a template of each payload by invoking an http get against the corresponding service every service obtains required data and metadata from and pushes processed data and metadata to mongodb collections the design of each collection facilitates service database interaction and stores information useful to keep the execution of the pipeline valid starting from metadata of input output variables created by fens collect service each stage of the pipeline enriches the metadata section with additional information which are used to create a new block of the blockchain to ensure maximum flexibility modellers can manually run each service as described in the next sections one at a time or set up a modeling solution as an entire pipeline from start to end 3 2 1 1 fens collect service the fens collect service stores the raw t v dataset into a dedicated raw mongodb collection for later ann training it manages the upload of data as single input output model data records or in bulk as a csv file containing a list of records in case of single parameter permutation fens collect digests a json payload where the parameter section lists the json objects the mandatory name of the database to push data to and name value and description of each sm input output variable see listing 1 in appendix here name is the name of the model parameter while value is the actual model input or output description is a comma separated list of properties a in or out is mandatory to identify if that parameter is part of the input or output layer of the ann b normalized is required only if the value of that parameter is already normalized to skip the normalization process c normalization range e g 0 1 0 9 is required if the parameter is not normalized and user needs to set up a custom normalization range additionally fens collect allows for processing a csv file of input output snapshots of the original model runs by attaching it to the json payload the fens collect service has been designed to efficiently read in parse and push long csv tables by processing chunks of data the t dataset is stored in a dedicated collection called raw every variable is stored in a different mongodb document each document contains a values array with raw model data and a metadata document the latter contains table 1 1 variable name 2 variable normalization range normalization minimum and maximum 3 minimum and maximum values contained in the collected array 4 a type that specifies if the variable is part of the input or output layer of the ann structure 3 2 1 2 fens normalize service the fens normalize service normalizes the t v dataset stored into a raw mongodb collection and pushes processed data and additional metadata into a dedicated collection called normalized the fens normalize service builds the normalization algorithm and pushes it to the database and executes it as a database function it leverages mongdb aggregation operators which perform arithmetic expressions on grouped data records database side csip normalize currently implements the feature scaling normalization algorithm only as a result the json request to activate the service contains only the name of the database that stores the raw dataset a dedicated collection called normalized stores the processed dataset a mongodb document stores the array of normalized data and their metadata the latter is an enriched version of the metadata from the raw collection that contains additional information such as location of minimum and maximum values within the normalized dataset array table 2 this complies with fens requirements of augmenting metadata information throughout each phase of the pipeline 3 2 1 3 fens train service the fens train service creates the sm evaluates its emulation capabilities against the unseen validation subset and stores the structure of the ann and the augmented metadata into the database it also allows for a periodic recovery and constantly monitors sm learning performance as well as the growing structure and the mutating topology of the ann fens train service may run in parallel for a given data set each creating an ann instance of an ensemble every fens train instance creates one sm in three phases 1 t ν dataset splitting 2 sm creation 3 sm validation and storing the json payload contains the following json objects database name to identify the project scale mechanism and training perc to set the splitting mechanism population connection density training error and max epochs to set the sm creation recovery epochs to set how often sm validation and storing happens during the training process for monitoring purposes recovery 1 t v dataset splitting fens train service retrieves normalized t v dataset from the normalized collection and splits it into training t and validation v data sets three split mechanisms are currently available 1 simple 2 random and 3 same distribution simple method splits t v in t and v based on user defined percentage random method shuffles t v and randomly splits it in t and v based on user defined percentage same distribution method shuffles t v splits it into t and based on user defined percentage verifying the kolmogorov smirnov test between t and v on each variable 2 sm creation after in memory t v dataset split t feeds the evolutionary algorithm and initializes the sm creation neat requires a population of anns and their initial connection density between input and output nodes which activates the feature selection implicit in neat the creation of the sm terminates when 1 the training mean squared error mse is lower than target mse training error 2 the number of training iterations is higher than max epochs 3 the mse computed on the v subset increases for more than 10 training iterations since last decrement regularization as early stopping 4 the contraction of the mse is less than 10 6 for 99 iterations prechelt 1998 3 sm validation and storing recovery fens train executes this step during the previous phase sm creation as a recovery step and when the sm is fully trained during this phase the partially or fully trained ann runs against v the service assesses 18 model efficiency coefficients for each ann output variable see table a in appendix and stores them in the database along with additional metadata such as hyper parameters used to set up t v dataset splitting and sm creation steps evolution of training mse and mutation history of the network structure of the best genome number of input output and hidden nodes as well as nodes connections this allows users to seamlessly monitor sm learning performance and ann structural state at each recovery epoch table 3 at the end of the sm creation fens train leverages mongodb gridfs api to store the serialized ann as sm object gridfs automatically stores the binary file of the sm in trained chunk collection and metadata in trained files collection the fens train service creates the sm in an automated fashion by leveraging the neat genetic evolutionary algorithm and its stochasticity it also allows for constantly monitoring the improvement of the sm performance and ann structure during sm generation this service runs multiple times to create several different sms because of the thoroughly designed dataset splitting once fens train reaches either threshold mse error or maximum number of epochs the service runs the validation phase for the last time and stores sm performance and ann serialized objects in the database 3 2 1 4 fens select service the fens select service identifies the most performant sms stored in the trained collection and saves their uids into a new collection named selected the latter is the sme fens select provides three selection algorithms to select the utmost performant ones and create the sme in three steps 1 loop through the prediction estimators stored in the performance meta data of each trained and validated sm table 3 2 select the utmost performant sms based on a specific criteria and 3 save sm uid into a new collection named selected to form the sme see table 4 the three algorithms currently implemented are 1 error the service retrieves sorted sm metadata from the most performant to the least performant from the trained collection and selects the sms that show performance greater than threshold 2 percentile the service retrieves the performance errors of the sms generates their probability distribution and selects the sms which performance are greater than user specified percentile 3 number the service retrieves sorted sm metadata from the most performant to the least performant from the trained collection and selects n most performant sms without checking the actual performance errors the fens select json result payload contains the three objects required to set up the selection mechanism the mechanism type the threshold value either error value distribution percentile number of sms and the statistical error name once the fens select service has performed the selection operation the uids of the most performant sms are stored into the selected collection and will compose the application specific sme fens select also creates the final block of the internal blockchain that tracks the sme generation this block contains the application specific sme 3 2 1 5 fens run service the fens run is the final step in the fens pipeline it runs the sme against provided new input data such as χ it feeds the input layer of the sme after parsing the json payload with χ and returns denormalized uncertainty quantified results an example json response of fens tr20 is available in listing 2 in appendix in addition the fens run service allows the retrieval of the sme anns as a compressed archive containing data and metadata for their potential offline use external to csip finally every new instance of fens run adds a new block to the blockchain containing the sme package user s inputs and sme outputs to capture and trace the sme application 3 2 1 6 fens blockchain for a blockchain supporting fens we selected hyperledger fabric androulaki et al 2018 to serve as a prototype and reference implementation we choose a docker based solution to ensure a quick setup and a well curated environment each phase in the fens pipeline represents a state to be captured as a block in the blockchain the state of each phase is expressed in mongo as a distinct collection due to the nature of fens managing a sizable amount of data we opted for an off chain data storage and on chain storage of collection meta data we also utilized a chaincode approach for compiling and emitting relevant meta data parameter ranges statistics and data ann ensemble blobs to provision the run service a multi site implementation was prototyped to exercise a configuration to tag on sme applications at sdo locations 4 testing tr 20 with a synthetic dataset the goal of this experiment is to test the methods and the implementation technology introduced in the previous section we created a synthetic dataset to generate an sme producing runoff peak discharge and evaluate 1 neat and the sme as generic approximators of nonlinear functions 2 the automated generation of the sme at the framework level by leveraging the fens architectural style and the set of fens compliant web services implemented in csip 3 the blockchain technology to make the sme creation tamper resistant and connect the original dataset to the final sme for demonstrating the fens approach and workflow we used tr 20 usda 1983 a simple yet nonlinear model that fits this purpose tr 20 is an event based hydrologic model developed by the usda national resource conservation service nrcs to estimate runoff and peak discharge of stormwater events at watershed scales fennessey et al 2001 merkel 2002 tr 20 implements the curve number cn method for calculating the volume of runoff and the unit hydrograph to evaluate the peak discharge fennessey et al 2001 coleman et al 2016 the basic relationship for storm runoff is 6 q p i a 2 p i a s where q is the daily runoff as an average depth i n over the watershed area p is the 24 h rainfall depth i n i a is the initial abstraction i e interception surface storage and infiltration prior to runoff estimated as i a λ s λ 0 2 here but values between 0 0 and 0 3 are feasible singh et al 2010 moglen et al 2018 and s is the maximum potential retention storage i n 7 s 1000 c n 10 c n is the dimensionless curve number which varies between 0 no runoff generated from rainfall and 100 all rainfall becomes runoff tr 20 uses english units the peak discharge q p f t 3 s 1 is calculated as 8 q p 484 a q 0 5 d 0 6 t c where a is the watershed area m i 2 d is the duration of rainfall excess h and t c is the time of concentration h estimated by 9 t c l 0 8 s 1 0 7 1140 s l 0 5 where l is the longest flow path f t and s l is the average overland slope f t f t 1 woodward 2010 tr 20 is primarily applied to watershed planning and evaluation studies of alternative structures such as flood retarding structure channels miller and woodward 1994 reservoir construction merkel 2002 and irrigated paddy runoff jang et al 2010 it has been applied to storm runoff prediction in agricultural and urban watersheds jang et al 2010 in spite of its limitations tr 20 is still widely used by nrcs hydraulic and field engineers as well as regulators such as us federal agencies state and local governments mainly because of its ease of use fennessey et al 2001 coleman et al 2016 4 1 experimental design we implemented tr20 also as a csip service using the same platform and service api as the fens services both service packages were deployed as containers using kubernetes cephfs on a 400 core cluster and a sharded 87 tb mongodb 4 x for fens storage the tr 20 service was deployed using 4 containers fens itself on 8 containers with no cpu memory resource limits for docker we created the synthetic dataset d as a uniformly distributed set of points and ran csip tr 20 with each input sample to evaluate the corresponding peak discharge q p the halton sequence a low discrepancy quasi random number generator was used to generate uniformly distributed samples over the integration domain de rainville et al 2012 this deterministic method outperforms pseudo random number generators which usually create non uniformly distributed samples the simulation model ran at each input sample to evaluate corresponding model output eason and cremaschi 2014 this creates the dataset d which is then split into t v and χ t v constructs and validates each global sm part of the sme χ allows for testing the sme performance against an unseen dataset a python package implementing the generalized halton sequence generator braaten and weller 1979 faure and lemieux 2009 de rainville et al 2012 and the csip python client api concurrently invokes the csip tr 20 web service 40 000 times permuting input values between the provided ranges drainage area a 0 01 3 125 mi2 curve number cn 30 100 watershed slope sl 0 5 64 watershed length l 200 26 000 ft rain depth p 0 01 26 in to compute each value of peak discharge q p f t 3 s 1 the same python software split the generated dataset in 90 t v and 10 χ and verifies the ks test on each variable 4 2 results and discussion we utilized the fens system by invoking the fens collect web service to store the t v dataset and initiate the fens pipeline after normalizing the dataset with fens normalize 15 instances of fens train deployed on the csip cluster concurrently generated 15 anns we set up each fens train instance with a population of 500 anns targeting training mse of 1 5 10 4 of normalized values and 20 000 maximum number of epochs 6 instances of fens train terminated because they reached the mse limit while the remaining 9 because they reached the maximum number of epochs the final rmse for the 15 sms varied between 733 and 1014 cfs fens select created the sme by selecting the 10 sms with root mean squared error rmse lower than 900 cfs see appendix table b every sm has its own unique ann internal structure and connection weights with a total number of internal connections varying between 39 and 100 and hidden nodes between 10 and 23 summary of the ann structures are available in table b the delineation of the tr20 surrogate model was captured in a hyperledger fabric blockchain prototype we ran fens run using inputs from the 4 000 samples of the dataset we then compared the sme estimates with the original tr 20 model results to assess sme accuracy the sme generates a distribution of responses for each sample in χ dataset the median of each distribution compares with the tr 20 model result and the comparison on the entire χ dataset allows for evaluating prediction estimators that define the goodness of the sme fig 4 illustrates a subset of 50 samples of the full dataset comparing ensemble responses box plots and original tr 20 model runs red crosses for visual inspection sm prediction estimates are computed using the full 4 000 samples the prediction estimators show overall very good emulation capabilities of the sme generated through the fens system nash sutcliffe efficiency nse and kling gupta efficiency kge are both above 0 98 which demonstrate an excellent goodness of fit between original model results and the median of the sme results percentage bias pbias of 0 5 indicates that overall the sme has a very slight tendency to overestimate the peak discharge but it s very close to 0 and proves to be a very good sme approximation overall the sme accuracy is very good because relative root mean squared error rrmse is 11 6 li et al 2013 fig 5 compares the median of the sme with original model results on the identity line we use the blue color to identify the subset of 50 samples selected for fig 4 while red color for the full dataset additionally the plot shows regression lines and prediction estimators with the respective colors for the two sets the blue dotted regression line in fig 5 shows a very good proximity with the equality line and has the following form y 198 2 0 97 x where the positive intercept 198 2 cfs and the slope slightly smaller than 1 highlight the small tendency of the sme to overestimate low values of peak discharge and underestimate high values of peak discharge nevertheless the prediction estimators computed on this specific subset confirm the good emulation capabilities of the sme nse and kge are above 0 97 and rrmse is 10 6 the red regression line in fig 5 compares very well with the blue one and has the following form y 97 37 0 98 x there is a minimal difference in slope 0 97 vs 0 98 and intercept 198 2 cfs vs 97 37 cfs the first being just the 0 4 of the 0 50 000 cfs range while the second the 0 2 as well as prediction estimators however comparing these results we can state that the subset we selected for visual inspection in fig 4 is representative of the full 4 000 samples and spans through the entire range of peak discharge values there is a tendency to underestimate high values of peak discharge in both cases likely because of the low density of points in the t v dataset that generate high peak discharge to validate the overall sme response and demonstrate the goodness of the ensemble approach we selected the sms in the sme with the best and worst rmse and compared their performance metrics of fit to the process model on the 4 000 samples of the dataset with the median of the ensemble response the results in table 5 demonstrate how the ensemble median outperforms single best and single worst sms similarly nse of the median of the ensemble is greater than the nse values of the single worst sm and of the single best sm the small positive pbias of the ensemble median indicates a slight tendency to overestimate peak discharge values while single best and single worst sms have a slight tendency to underestimate peak discharge kge is very good overall with values falling in a very narrow range of 0 986 0 989 these results support using an ensemble median rather than selecting the best individual sm from the ensemble we did not perform any comparisons with other ann methods because the goal of this test was to 1 evaluate the goodness of neat for surrogate modeling 2 illustrate an application of the fens system and demonstrate the workflow of the fens pipeline and 3 test the implementation of the local fens blockchain additionally the unique features of neat make it particularly well suited for emf integration and automated generation of the sme at the framework level a long training time is an important weakness of the neat algorithm it took about two days to train 15 anns using several nodes of the csip cluster we originally decided to use encog3 java library to facilitate the integration and widespread use of the fens system within existing modeling frameworks such as csip the latter runs without any graphic cards and any research laboratory can easily integrate this system as a result since encog3 is just multithreaded it constrains the training scalability to a single multi core computer or server consequently we are considering the improvement of horizontal scaling by implementing a distributed execution framework that leverages a multi node computer cluster additionally wang et al 2013 identified an efficiency limitation in the standard neat algorithm and improved it by introducing node location and recurrent connection of node gene and increased population size over time thus optimizing complex learning tasks encog3 could benefit from the integration of these changes since it implements the original neat algorithm nevertheless we are also considering additional java libraries e g deeplearning4j to improve fens flexibility overall neat works very well as a ml methodology for data driven surrogate modeling tasks accurately emulating the non linear set of equations that estimates the peak discharge in tr 20 it perfectly integrates within the fens system altogether accomplishing the goal of automatically emerging robust and flexible sme at the framework level each web service of the fens pipeline creates a new block of the blockchain that is internally stored in the database fens select creates the last block of the internal blockchain which can be deployed for operational use we also verified that standard neat is a slow algorithm so we identified two possible ways to overcome this limitation in future applications 5 conclusions this work addressed a long known discrepancy for managing conceptual and process based models to fully comprehend correctly parameterize efficiently execute and flexibly deploy them within service delivery organizations for widespread and frequent use these organizations need a model to compute results quickly with limited set up and reduced data entry taking advantage of existing organization wide data resources to bridge this technology transfer gap this contribution aimed to alleviate research model application complexity with respect to data and parameter setup runtime requirements traceability and proper model infrastructure setup we proposed the extension of current state of the art environmental modeling framework workflows to automatically emerge data driven sms which capture the intrinsic knowledge of a conceptual or process based model into an ensemble system of sms this methodology accommodates application needs to get quick and accurate enough model results with limited input entries and limited a priori knowledge of internal processes involved in conceptual and process models this method allows for more consistent deployment on server desktop and mobile hardware while being platform and operating system independent furthermore it can potentially run offline in remote locations we started by leveraging neat which has proven to be appropriate for creating sms of process based models we enhanced the stochasticity of neat with a specifically designed cross validation technique which improves robustness and flexibility by creating an ensemble of surrogate models we conceptualized the extended modeling framework workflow to emerge the sme at the framework level the derived software architectural style should in turn facilitate widespread integration on any emf based on its characteristics we named this architecture framework enabled neuroevolutionary built surrogate model fens the fens design allows us to reshape the definition of surrogate model since the integration of an emf and a pipeline approach within a service architecture characterizes the sme itself as a modeling solution of the original model fens requirements and constraints stressed the need for blockchain technology as a means to consistently validate the pipeline and trace the deployed sme back to the source data and model here we realize that the fens blockchain not only bridges the gap between research environments and sdos but brings research efforts into everyday use as a matter of fact we showed in a high level example how this technology connects every step from the sme deployment all the way through to the end product deployment of sme results this aspect of the fens system opens important opportunities to simplify the integration of research modeling efforts into operational modeling and decision support we implemented the fens architecture within csip to expand its functionalities with a pipeline of csip and fens compliant web services the jupyter notebook link provided under software availability exercises the system and demonstrates how the pipeline of csip services automatically creates the sme we tested and demonstrated the methods using tr 20 peak discharge results to show that 1 neat is a good method for function approximation since this doe produced nse and kge accuracy greater than 0 98 and rrmse less than 12 2 fens automatically generates an ensemble of surrogate models sme for specific modeling solutions in a controlled environment and 3 the tamper resistant design of blockchain guarantees consistency in the process of sme creation and traceability of the tool once it is deployed outside the research environment of particular relevance the fens design uses maximal computation resources for training while requiring minimal computational resources and infrastructure setup for sme application this allows for delivery and application of sme within sdos e g a consultancy organization since an sme is extremely lightweight an sme with 10 anns of tr 20 sm is about 13 kb in size and provides the estimated rate of peak discharge in less than 100 ms one can deploy it within a block of the fens blockchain the tr 20 case study demonstrated that the median sme performed better than any single sm in conclusion fens has considerable potential for improving workflows for rapid resource assessment by using models with acceptable accuracy known uncertainty and verifiable research origin thus we believe that our contribution may help make results of complex conceptual or process models more available and accessible software availability csip cloud computing platform name csip cloud services integration platform developer dr olaf david contact odavid colostate edu availability license mit language java website https alm engr colostate edu cb project csip source code and documentation https alm engr colostate edu jenkins job csip mercurial repository https alm engr colostate edu cb hg csip core fens surrogate modeling services pipeline name fens framework enabled neuroevolutionary built surrogate modeling developer dr francesco serafin dr olaf david contact francesco serafin colostate edu availability license mit language java website https alm engr colostate edu cb project 45 repositories mercurial repository https alm engr colostate edu cb hg fens pipeline jupyter notebook that exercises the fens system including dataset https colab research google com drive 10sj7nau5o4zdpbz9hp nduqzr7zrstvq authuser 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank dr sean fleming for his assistance and feedback before the submission of the manuscript and the anonymous reviewers for their comments which have improved the quality of this paper the financial support from the university of trento https www dicam unitn it en is gratefully acknowledged the usda is an equal opportunity provider and employer mention of trade names or commercial products is solely for the purpose of providing specific information and does not imply recommendation of endorsement by the usda list of abbreviations ann artificial neural network api application programming interface ccp cloud computing platform cdf cumulative distribution functions cfs cubic square feet cn curve number csip cloud services integration platform doe design of experiment emf environmental modeling framework sme surrogate model ensemble fens framework enabled neuroevolutionary built surrogate modeling fs neat feature selective neat see below hru hydrological response unit json javascript object notation kge kling gupta efficiency ks kolmogorov smirnov maas model as a service ml machine learning mlp multilayer perceptron mse mean squared error neat neuroevolution of augmenting topology ne neuroevolutionary nse nash sutcliffe efficiency oms object modeling system pbias percentage bias pm process model rmse root mean squared error rrmse relative root mean squared error swat soil water assessment tool sm surrogate model uid unique identifier uq uncertainty quantification usda united states department of agriculture appendix table a model efficiency coefficients implemented in csip train service to validate partially of fully trained anns in the table o means observed s means simulated table a name abbreviation equation absolute difference absdiff i 1 n q i o q i s absolute volume error absvolumeerror i 1 n q i s q i o slope of linear regression of observed cumulative vs simulated cumulative dsgrad i 1 n c i o c o c i s c s i 1 n c i o c o 2 w h e r e c j 1 i q i i 1 2 n sum of residuals err sum i 0 n q i s q i o fenicia high flow fhf i 1 n q i s q i o 2 n fenicia low flow flf i 1 n ln q i s ln q i o 2 n index of agreement ioa 1 i 1 n q i o q i s 2 i 1 n q i s q o q i o q o 2 kling gupta efficiency kge 1 r 1 2 σ s i m σ o b s 1 2 μ s i m μ o b s 1 2 model deviation modeldev i 1 n q i s i 1 n q i o nash sutcliffe efficiency nashsutcliffe 1 i 1 n q i o q i s 2 i 1 n q i o q o 2 bias nbias i 1 n q i o q i s i 1 n q i s normalized rmse norm rmse i 1 n q i o q i s 2 i 1 n q i o q o percent bias pbias i 1 n q i o q i s i 1 n q i s 100 pearson correlation coefficient personcorrelation c o v q o q s σ q o σ q s peak weighted rmse pwrmse 1 n i 1 n q i o q i s 2 q i o q o 2 q o coefficient of determination r2 1 i 1 n q i o q i s 2 i 1 n q i o q o 2 root mean square error rmse 1 n i 1 n q s q o 2 transformed root mean squared error transformedrmse 1 n i 1 n 1 q i s 0 3 1 0 3 1 q i o 0 3 1 0 3 2 table b summary of structural elements training exit final rmse and selection of the 15 trained sms ns non selected s selected table b links hidden nodes training exit final rmse cfs sme selection 66 17 max epochs 941 44 ns 60 22 max epochs 928 63 ns 83 23 max epochs 834 42 s 56 15 max epochs 812 44 s 66 21 max epochs 913 12 ns 45 10 max epochs 838 50 s 39 10 max epochs 1013 89 ns 71 23 mse 781 09 s 54 17 max epochs 995 42 ns 56 19 mse 807 69 s 55 14 max epochs 880 22 s 100 17 mse 780 83 s 53 11 mse 756 24 s 66 15 mse 849 94 s 51 15 mse 733 01 s listing 1 template of csip collect service request listing 1 listing 2 example of sme json response emulation of tr 20 peak discharge listing 2 
25711,environmental models are often essential to implement projects in planning consulting and regulatory institutions research models are often poorly suited to such applications due to their complexity data requirements operational boundaries and factors such as institutional capacities this contribution enhances a modeling framework to help mitigate research model complexity streamline data and parameter setup reduce runtime and improve model infrastructure efficiency using a surrogate modeling approach we capture the intrinsic knowledge of a conceptual or process based model into an ensemble of artificial neural networks the enhanced modeling framework interacts with machine learning libraries to derive surrogate models for each model service this process is secured using blockchain technology after describing the methods and implementation we present an example wherein hydrologic peak discharge provided by the curve number model is emulated with a surrogate model ensemble the ensemble median values outperformed any individual surrogate model fit to the curve number model keywords surrogate modeling cloud services framework integration framework architecture blockchain service delivery 1 introduction many environmental problems are studied and expressed through the application of conceptual or process based models which are simplified representations of physical systems wheater et al 2012 they result from idealized understandings of real world phenomena described through mathematical concepts and translated into computer code razavi et al 2012 alizadeh et al 2020 advances and development efforts in model research most often focus on improving simulation accuracy increasing spatial and temporal resolutions and adding processes and their interactions serafin 2019 improved and refined equations and numerical methods are continuously derived and implemented e g casulli 2017 tubini et al 2017 dumbser et al 2018 advancements in sensor technologies support the collection of observed data and provide an increasing number of input or calibration variables akinmolayan et al 2018 anees et al 2018 bayramov et al 2019 additionally the combination of complex methods for calibration as well as sensitivity analysis maximize model prediction accuracy green et al 2015 as a result conceptual and process based models have become complex software efforts which strive to more accurately represent real world processes within a given scope and domain serafin 2019 in this regard the evolution of swat soil water assessment tool arnold et al 2012 is a representative example swat is a watershed model that simulates water quality and quantity of surface and subsurface waters to assess the environmental effects of different land use and management practices as well as climate change from its early predecessors and development in the 1990 s to its current state the addition of new features such as management of multiple hydrological response units hrus and more detailed hydrological and water quality processes increased modeling capabilities and complexity williams et al 2008 arnold et al 2020 these innovations may provide more accurate simulations though increased complexity does not ensure greater accuracy gilroy and mccuen 2011 but require higher user proficiency and experience to master them which result in a steeper learning curve and more tedious model setup for example serafin 2019 compared four consecutive swat releases v2000 v2005 v2009 and v2012 to highlight how the overall total lines of code increased from 16 308 to 34 113 and input variables and model parameters for simulation set up increased from 540 up to 612 model documentation likewise grew to allow swat users to set up new modeling solutions and increased from 450 to 650 pages a modeling solution results from connecting modeling components through a modeling framework to create an application here every component could be a model service a data service or any self contained computational sub module that can be reused in different contexts to create a different modeling solution overall this demonstrates the progressive advancement of model flexibility and capability as well as intricacy and complexity long known challenges exist to fully comprehend correctly parameterize efficiently execute and flexibly deploy process based models research institutions usually conduct the development of the models within science performance criteria but do not usually focus on their integration into external organizations and workflows such organizations referred to here as service delivery organizations sdos could be regulatory entities action agencies consultancies or for profit and non profit institutions integrate such models into their processes and workflows but may find this process challenging resource intensive and time consuming these projects often fail because the research model has become unwieldy and too burdensome to use in a practical environment mcphee and yeh 2008 ismail et al 2008 cai et al 2004 thiessen and loucks 1992 therefore sdos need an approach for integration of modeling technology that is technically straightforward but also scientifically sound models are expected to produce results within reasonable operational expectation they should require limited set up and possibly reduced data entry while taking advantage of existing organization wide data resources and infrastructure wheater et al 2012 devia et al 2015 the use of environmental modeling frameworks emfs and cloud computing platforms ccps like the object modeling system and the cloud services integration platform oms csip david et al 2013 2014 lloyd et al 2012 have alleviated some of the complexity implications for model users david et al 2012 lloyd et al 2011 by using concepts such as software encapsulation process based models can be turned into model components and web services as model as a service constructs zou et al 2012 the introduction of emfs and ccps in environmental modeling addresses the run time issue data preparation and reduces expenditures for software development and maintenance david et al 2013 emfs and ccps simplify a user s approach to a simulation model by providing an access protocol with a payload format for example json might be the preferred format for a model request response interaction via a restful http api fielding and taylor 2000 additionally the architectural design of emfs and ccps allows for leveraging innovative hardware infrastructure like super computing environments and computer clusters david et al 2013 2014 lloyd et al 2012 2018 zhu et al 2019 however such an approach does not lessen a user s burden of collecting data setting up a modeling solution by connecting framework compliant modeling components or handling calibration and sensitivity analysis processes this contribution bridges the multi faceted gap between research and service delivery by deriving an enhanced framework that links process models with surrogate models fig 1 in this context we propose a machine learning ml based surrogate modeling approach that aims to capture the intrinsic knowledge of a process model as an ensemble of artificial neural networks anns this contribution circumvents issues of directly applying research models due to their inherent complexity in data and parameter setup infrastructure and runtime requirements the framework accommodates the application needs to achieve quick and sufficiently accurate model results using a set of limited inputs and limited a priori knowledge of the internal processes here the data driven methodology is used as an emulator of any deterministic simulation model this contribution takes the model integration concept a step further by using a framework integrated model to derive a surrogate for a process model thus providing an enhanced modeling framework to help mitigate research model complexity streamline data and parameter setup reduce runtime and improve model infrastructure efficiency the oms csip framework was extended to capture and store relevant model simulation data and to derive the surrogate model at the modeling framework level it interacts with machine learning libraries nosql databases and a set of csip services to create model surrogates here neuroevolution of augmenting topology neat stanley and miikkulainen 2002 whiteson et al 2005 techniques in an ensemble application are combined with ann uncertainty quantification mongodb for data storage and blockchain technology for validating and tracing surrogate model origins this paper is organized as follows a summary of related work is presented in section 2 section 3 introduces research methodologies that support the integration of ml libraries into framework architecture to expand framework capabilities section 4 illustrates the initial experiment conducted on a synthetic dataset to test and validate the technology finally section 5 summarizes conclusions challenges and future work 2 background surrogate modeling concepts date back to blanning 1974 where the author conceptualized the need for surrogate models or meta models for sensitivity analysis purposes kleijnen 1975 provided statistical tools to operationalize blanning s theory since then this idea has advanced and has been utilized widely in applied science and engineering studies requiring large volume simulation runs alizadeh et al 2020 viana and haftka 2008 such as optimization uncertainty analysis operational management and prediction beh et al 2017 asher et al 2015 razavi et al 2012 surrogate models attempt to mitigate long simulation times by eliminating real time application of their complex parent models alizadeh et al 2020 there are several mathematical techniques available for approximating the behavior of original models alizadeh et al 2020 reviewed over 200 papers on surrogate modeling in engineering applications mainly focusing on data driven methods fernández godino et al 2016 analysed more than 150 papers to give an overview of multi fidelity surrogate models razavi et al 2012 and asher et al 2015 provided the most comprehensive taxonomy for surrogate models identifying three main categories as follows 1 projection based surrogate models make use of the projection of actual model equations into a basis of orthogonal vectors to reduce the size of the initial vector space the most popular methods applied to engineering related topics are frequency weighted balanced truncation e g sahlan et al 2013 proper orthogonal decomposition e g mcphee and yeh 2008 volkwein 2013 and dynamic mode decomposition e g ghommem et al 2013 bistrian and navon 2015 mahmoodian et al 2018 pointed out that these techniques are computationally efficient and may provide an associated error bound willcox and megretski 2005 however these methodologies require ad hoc mathematical analysis of each equation implemented in the original model which is possible by source code analysis and source descriptions of modeling algorithms these methodologies reduce both simulation run time and input space dimensionality while preserving basic physics of the model itself nonetheless they result from dedicated mathematical study of the original models and are not suitable for emulating highly complex systems asher et al 2015 2 hierarchical or multi fidelity surrogate models are generated from the original models by 1 reducing numerical accuracy using coarser spatial temporal grid size e g chung et al 2016 chung et al 2018 or 2 ignoring approximating some physical processes e g hou and wu 1997 sangalli 2003 shi et al 2012 in the first case reducing numerical accuracy only slightly addresses the run time issue this type of complexity reduction allows for obtaining accurate results because physical components are identical to the original model however a simulation setup still requires the entire input dataset and developer level understanding of computational model structure the second option partially takes on the issues of 1 speeding up simulation run time 2 requiring a smaller input dataset and 3 simplifying the inner modeling complexity nonetheless these kinds of models might still have long computational times require calibration procedures and large input datasets furthermore they are not able to return accurate results because it is not possible to compensate for simplified physical processes of a system with corrective parameters 3 data driven surrogate models or response surface models are statistical or empirical models capable of capturing and approximating the original model behaviour the response surface typically a nonlinear hyperplane by learning the existing nonlinear relationship between a set of original input output snapshots there is no direct emulation of any inner conceptual physical process described in the original simulation model kianifar et al 2020 the literature reports a variety of approximation methodologies employed in surrogate modeling such as kriging e g babaei et al 2015 support vector machine e g azamathulla et al 2016 generalized polynomial chaos expansion e g meng and li 2018 and dynamic mode analysis e g young and ratto 2011 in the most recent literature anns are among the most commonly used techniques e g nguyen et al 2019 lima et al 2018 shaw et al 2017 anns are flexible function approximators they allow for emulating specific aspects of the original model behaviour by using only the most relevant input parameters of the original model decreasing simulation run time and requiring no user knowledge of the original model however standard methods such as multilayer perceptrons mlps require several subjective decisions to develop and apply a proper ann these decisions involve selection of 1 the optimal number of neurons for input and output layers 2 the number of hidden layers 3 the number of neurons in each hidden layer and 4 the type of transfer function as a result surrogate model setup entails an iterative trial and error approach in summary different types of surrogate models and their utilization to applied science and engineering topics have been studied extensively and constitute an active field of research projection based and multi fidelity surrogate models originate from actual simplifications of conceptual physical processes of original simulation models and are tightly coupled with model internals in contrast data driven surrogate models completely decouple from original simulation models because they are statistically or empirically based therefore there is no connection between the structure of data driven surrogates and any emulated conceptual physical process maier 2013 provided a useful resource to guide this literature review identify scientific gaps and formulate key objectives the literature reports algorithms for automatic calibration of several of the previously mentioned data driven methods usually including the use of fixed optimal structures e g practical water supply forecasting specific autonomous machine learning solutions implemented by fleming and goodbody 2019 neat integrates recent advancements in evolutionary intelligence to couple ann creation and training with neuroevolutionary algorithms to overcome previous mlps manual adjustments this accommodates the requirement of automatic generation of the surrogate model and streamlines its integration into a framework workflow to our knowledge however there are no applications of neat in environmental surrogate modeling additionally none of the papers we reviewed highlights an automated process that connects each and every step going from original model data set collection to surrogate model creation and training followed by running the surrogate model as a result our research intends to generalize a surrogate modeling approach by addressing the following objectives 1 automate the process of surrogate model creation by integrating neuro evolutionary algorithms neat within the workflow of an environmental modeling framework 2 implement neat as a useful method for surrogate modeling of environmental topics 3 guarantee consistency in the process of developing the surrogate model sm and track the sm back to the original model once it is deployed 3 research methods our methodological approach is comprised of the following pillars 1 neuroevolution of augmenting topologies neat provides the evolutionary algorithm capable of creating the structure of the ann while adjusting connection weights during the training phase stanley and miikkulainen 2002 the feature selective neat fs neat extension introduces an implicit dimensionality reduction mechanism to select only input parameters that yield the best ann performance whiteson et al 2005 2 uncertainty quantification of surrogate model ensemble sme results handles neat inherent stochasticity to create a more robust and flexible original model emulator with uncertainty quantification uq 3 integration of neat and sme within a modeling framework streamlines the automatic generation of the sme at a framework level abstracted to the fens framework enabled neuroevolutionary built surrogate modeling architectural style 4 fens blockchain integrates blockchain technology yli huumo 2016 in the fens architecture to consistently validate the process of sme generation and track the sme from the original process model and associated input data to its deployment and final results our technical approach and implementation integrates software libraries apis platform extension and web services to apply the concepts described in the methodological approach 1 software libraries apis databases and platforms include mongodb chodorow 2013 for data storage encog3 heaton 2015 machine learning framework for the neat algorithm csip and csip api david et al 2013 2014 lloyd et al 2012 model as a service maas platform and its open api providing access to simulations as cloud based web services 2 fens compliant csip surrogate modeling services pipeline automatically emerges the sme at a framework level while the integration of blockchain technology provides validation and tracking of the process from original model runs to deployed sme 3 1 methodological approach 3 1 1 neuroevolution of augmenting topologies neat and feature selective neat fs neat the neat evolutionary algorithm with its feature selective extension creates artificial neural networks anns starting from a minimal combination of input output nodes during supervised learning this artificial genetic evolution automatically builds a population of anns and selects the best one with little or no human interaction the integration of this algorithm into a modeling framework workflow enables it to automatically emerge sms from any framework compliant modeling solution neat stanley and miikkulainen 2002 simultaneously evolves topology and weights of a population of anns during supervised learning neat generates the structure of each neural network starting from minimal topology progressively adding complexity and optimizing it over generations the topology of the initial population has input nodes directly connected to output nodes with no hidden nodes streamlining the learning process and keeping the size of the search space of connection weights to its minimum fs neat whiteson et al 2005 is an improvement of the standard neat algorithm it adds a feature selection process which automatically identifies inputs that yield the best ann performance this process recognizes redundant and insignificant parameters pruning them out of the ann connection structure fs neat alleviates the chore of setting up ml systems by enabling the search algorithm to select only a subset of the input parameters that increase learning performance by imitating natural evolution and selection in biological populations neat and fs neat automate the creation and identification of the best ann this avoids the trial and error procedures typical of other mlp methods and minimizes the need for human intervention once integrated into a modeling framework workflow since fs neat extends neat functionalities neat represents both in the rest of the paper 3 1 2 ensemble of surrogate models the ensemble of surrogate models approach serafin et al 2018 serafin 2019 results from combining the stochasticity inherent to the evolutionary genetic algorithm implemented in neat with a specifically designed cross validation technique supervised learning methodologies typically feature splitting the available dataset d comprising n data into three disjoint subsets training t validation v and testing χ govindaraju and rao 2000 friedman et al 2001 may et al 2010 james et al 2013 several data splitting methods exist in the literature including simple random sampling srs knuth 2014 kennard stone sampling kennard and stone 1969 sampling based on minimization of the difference in statistics such as the mean μ and standard deviation σ shahin et al 2004 here we assume that d is a collection of model runs designed to sufficiently describe the behavior of the original model within a user identified problem domain we want to split d such that distributions of resulting subsets are similar mcgaughey et al 2016 consequently we use the kolmogorov smirnov ks statistic to verify that the distributions of the split subsets for each variable sufficiently match bowden et al 2005 ks is a nonparametric test that compares two empirical data distributions by estimating their cumulative distribution functions cdfs we can compute the cdf ℱ of n data points of a variable x such that 1 f n x 1 n i i x i x where i is the indicator function that returns 1 when x i x and 0 otherwise we want to verify the hypotheses 2 h 0 f g v s h 1 f g where the first subset x 1 x m of size m has distribution with cdf f x and the second subset y 1 y n of size n has distribution with cdf g x the ks statistic compares the two cdfs f x and g x to identify the maximum discrepancy d m n 3 d m n s u p x f m x g n x where s u p is the supremum function by knowing d m n and for large m n we reject the null hypothesis if 4 d m n c α m n m n where α is the level of significance and c α is 5 c α l n α 2 1 2 accordingly the split algorithm initially divides d into two subsets training validation t v and testing χ subsets by using identical t and v sets neat s stochasticity generates dissimilar anns which differ in their structure and connection weights as a consequence training an ensemble of anns results in several sms capable of properly emulating the model behaviour instead of selecting one sm out of a trained ensemble we design a cross validation method that leverages and amplifies the behavioural stochasticity of neat before the beginning of the training of each ann the algorithm iteratively and randomly splits t v into disjoint t and v until it verifies the ks statistic for each variable in the sets as a result of this splitting procedure we train and initially validate each ann against slightly different datasets the ensemble of surrogate models sme results from concurrently training several anns which overall use the vast majority of the samples in t v and selecting the most performant ones finally the sme collectively runs against χ providing a collection of uncertainty quantified results the comparison between original model results and the median of each sme estimate certifies the goodness of the original model emulation 3 1 3 integration of neat and sme within a modeling framework the integration of neat and sme within an emf automates the creation of surrogate models from research models by extending the modeling framework workflow serafin et al 2018 serafin 2019 emfs manage and preprocess model parameter input facilitate model execution and postprocess and analyze outputs here we tailor the emf to derive an sme from a pm by streamlining all sm generation steps fig 2 illustrates the conceptual design highlighting four phases that we discuss consequently the first phase involves research modeling simulation runs where the emf provides consistency and traceability of model results in addition to seamless access to model inputs and outputs these are the information the data driven training algorithm recursively learns from thus success depends on the selection of the most sensitive parameters to describe the behavior of the physical process to emulate as well as the design of the experiment enabling sampled simulation runs to create a homogeneous input space serafin et al 2020 as the model runs the emf harvests and stores input output snapshots of each simulation we identify this first step as generation of dataset d gen in fig 2 the training validation of the sm phase train validate in fig 2 involves the creation of the response surface model box and wilson 1951 by feeding the supervised training algorithm with t and validating the resulting sm against v the stochasticity of the neuroevolutionary algorithm and the specifically designed cross validation procedure allow for generating several different sms which are stored in the database after validation together with prediction estimators the latter are necessary in order to select the most performant sms discarding low performing or incompletely trained sms this step is the uncertainty quantification phase uq in fig 2 selected anns form the sme characterized as a whole by quantifying their uncertainty in emulating original model behaviour the final phase is the run of the sme where the sme is packaged and deployed as a more compatible option for integration with environmental assessment program applications run in fig 2 the foregoing demonstrates how the concept of integrating neat and sme within emf has the potential for automatically emerging surrogate models from research models we identify and connect four fundamental phases into a sequential and ordered pipeline where the previous phase feeds the following one in a systematic way however the translation of this abstract concept into an actual implementation requires a further effort to highlight basic elements and constraints that regulate the relationships among each element of the entire system the resulting architectural style should in turn allow widespread implementation of this abstract concept on any modeling framework consequently we introduce framework enabled neuroevolutionary built surrogate modeling fens as an abstract model of the extended modeling framework workflow fens attempts to bridge the gap between research environments which develop conceptual and process based models and sdos which need easier access to simplified models by streamlining their communication and facilitating the transfer of knowledge fig 1 this knowledge initially resides within framework compliant models therefore we need to expose it translate it and bundle it into easier to use tools this is achieved by combining different software architectural styles to extend the current state of art emf workflow and derive smes from original models at the framework level the deployment of the smes to sdos is the last step to finalize the gap bridging process fens is a sequence or pipeline of individual service calls which can be captured and added into a workflow engine thus being reproducible if a new version of the pm is available a corresponding new version of the sme can be obtained by re running the workflow and evaluating the statistical qa qc metrics in every step of the pipeline such a process might be computational involved but can take advantage of previous fens parameter settings it has to be determined if qualitative process improvements in the pm justify an unsupervised fens re run alternatively a change in training data and parameters may be required due to extensive changes in the pm process behavior the literature review guided us to select data driven sms as the most appropriate method for our goal as a result fens starts as a data centered repository architectural style its functionalities are expanded by integrating a service oriented emf which orchestrates and organizes the data accessors in a pipeline architecture following we identify fundamental structural elements and their communication placing architectural constraints to make fens a portable system by minimizing its impact on existing emf workflow and leveraging advanced emf architecture we do not describe the details of each integrated architecture since we do not intend to alter their standard protocols 3 1 3 1 requirements fens architecture relies on three major requirements 1 an emf designed on microservice architecture 2 availability of storage capacity and 3 the algorithm that builds the sm emfs manage original model execution and seamlessly access model inputs and outputs an emf microservice architecture carries key software design principles such as modifiability extensibility simplicity and scalability fielding and taylor 2000 that allow for keeping up with evolution of requirements from research environments and sdos original mathematical models and statistical approximators evolve and improve over time consequently the fens architecture must be simple and flexible enough to allow for quickly modifying and extending its underlying system capabilities to adapt to new modeling necessities for example neat selects only input parameters that speed up and facilitate the learning process some may want to add further checks on input elements before the training phase by using methodologies such as principal component analysis pca fleming and goodbody 2019 in this example fens allows for easily interposing another phase between phase 1 generation of dataset d and phase 2 training of the sm augmenting the pipeline additionally an emf designed on microservice architecture reduces coupling between software components by introducing separation of concern and software encapsulation this makes the system scalable facilitating partial and fragmented updates since data driven methodologies simplify the automation of surrogate model creation storage capacity must be part of the architecture for seamlessly storing the permutations of input output parameters out of each original model run as well as sm hyperparameters efficient storage increases system reliability by tracking and storing partial and full results along each phase of the system allowing recovery from abrupt or expected interruptions we strongly recommend a nosql database because collections and documents enhance system flexibility since they are open to extension over fixed schemas and tables typical of a relational database the last requirement is the methodology that supports building and training the sm by only learning the relationships linear or nonlinear between original model inputs and outputs for example in case of ann based sms several mathematical algorithms can calibrate the connection weights of a fully connected ann and if we are willing to use methodologies to train the ann by evolving the weights of the fixed topology through a genetic algorithm as oppose to minimization of a loss function e g backpropagation conjugate gradient etc traditional neuroevolutionary ne systems are widely available and well tested stanley and mikkulainen 2002 stanley et al 2019 here a trial and error approach usually identifies the most suitable fixed topology ann neat is a ne algorithm but it efficiently evolves ann structure along with connection weights this feature provides additional flexibility by building and calibrating each ann ad hoc it enables further automating the building and training process because it avoids the trial and error approach to identify the most suitable structure of the ann 3 1 3 2 architectural style fens is a data centered repository architectural style with data accessors organized in a pipeline architecture and orchestrated by a service oriented emf fig 3 this definition highlights the three main constraints of the overall architecture 1 data accessors are part of the pipeline 2 emf incorporates the data accessor pipeline and becomes part of the architecture itself 3 data elements comprise of data and metadata stored in the central data store at each phase the first constraint limits the independence of the data accessors since the phases of fens follow a specifically designed pipeline data accessors cannot operate independently on the data store but must follow the pipeline order otherwise the pipeline would fail to provide valid consistent and traceable output the second constraint integrates the emf within the architecture since it is responsible for handling connections between the data accessors between the data accessors and the data store and between the data accessors and the user agents the data accessors must be framework compliant for widening potential interactions with future framework extensions and more importantly for achieving higher levels of modeling flexibility emf integration in the architecture demonstrates how the process of simplifying original models into surrogates is a modeling solution itself and provides a different perspective on the definition of surrogate models as models of models the last constraint regulates the communication between the data accessors and the data store in order to keep the pipeline alive metadata must accompany the data during each transfer when possible metadata should be leveraged to execute operations on the database side limiting slower transfer of data between the data accessors and the data store the high level schema to extend the emf workflow to automatically create sme from research models fig 2 served as a conceptual design from which we derived the fens architectural style with its requirements and constraints that provides a blueprint for integrating the automated surrogate model development workflow in any emf 3 1 4 fens blockchain the first constraint of the fens architecture identifies the need for consistency and traceability of the sme creation before deploying it for integration in program delivery environments for regulatory and consulting objectives the sme results from an ordered list of operations starting from data generation ending with final deployment to validate every generation step and prove the integrity of the pipeline we integrate blockchain technology within the fens system fig 3 a blockchain represents the glue providing the sme s provenance capturing processes attributes parameters metadata throughout all intermittent fens steps to the originating pm the pm that the sme is the surrogate for a blockchain in general is an open distributed and immutable ledger that records a sequence of operations of tangible or intangible goods lakhani and iansiti 2017 underwood 2016 each block of the chain is immutable by design since it contains its own hash as well as the hash of the previous block we achieve the consistency and traceability of the fens pipeline by creating a new block at each phase using metadata and hyperparameters of the current phase of the pipeline to create the current hash while inheriting the previous hash from the previous block this results in an immutable and tamper resistant process of sme creation and the smes deployed within the blockchain change of a parameter in one phase of the pipeline would affect the entire set of hashes and invalidate subsequent pipeline steps and finally the sme accordingly moreover the blockchain allows for tracking the deployed sme back to the original model the dataset and the hyperparameters that generated it however this poses an interesting question does the deployment of the sme represent the end of the pipeline and blockchain consequently the sme could be the first block of a more comprehensive blockchain followed by sme application blocks with untested data for example consider an agricultural producer using a web application for computing ecosystem credits or benefits sustainability claims soil carbon markets cost share program applications etc a fens blockchain integration with service delivery may be carried out as follows 1 a research environment builds and deploys the consistent and validated sme to sdos as a fens blockchain 2 sdos test and approve the sme block adding it to their program delivery chains and making the approved block available to third party users 3 other certification organizations may approve the sme block 4 third party users ecosystem credit providers in this specific context add the certified sme block to their blockchains 5 the consultant computes ecosystem credits and transacts a claim for a farmer adding to the relevant blockchain the claim is buttressed by the results from the certified sme in the chain 6 additionally these blockchains might span the entire supply chain carrying relevant information of the farming history of a product this information can potentially influence the end consumer e g at a market or grocery store who might opt for a sustainably produced product and validate its authenticity trienekens et al 2012 fens with added blockchain integration asserts and underlines the bridge between research environments and sdos supported by research models therefore the addition of blockchain as a means to validate the sme opens additional opportunities for the sme as a building block of a comprehensive enterprise integration tripoli 2018 pan et al 2019 the authors suggest and favor blockchain implementation that extracts relevant metadata about the origin of an sm solution through the fens service api here for given sm metadata such as statistical measures of the ua uq ann training and performance characteristics training data hashes input data statistics pm model version hash etc combined with location and time stamps can be presented to create a sm lineage the implementation method of such data extraction may be achieved through chaincode 3 2 technical approach and implementation the rationale for a web service centric implementation of fens is to allow for remote access highly scalable operation and institutional management of all tasks encompassing the sme generation in general terms we are utilizing a service design that can be characterized as a resource api web service api style daigneau 2011 utilizing the cloud services integration platform csip by managing resources in a nosql database mongodb with the ml framework encog we decided on encog because of its seamless integration into the csip modeling architecture and on mongodb since it is already an integral part of csip in this introduction we briefly describe csip encog and mongodb in the following subsections we analyze the web services that compose fens in detail the object modeling system cloud service integration platform oms csip was originally developed to facilitate the integration of software components models and model services david et al 2012 2013 lloyd et al 2011 2012 while first emphasizing on the creation of modular modeling solutions exhibiting implicit multi threading annotation driven component integration and multi language support the framework was extended into a model as a service offering to allow cloud based modeling solutions david et al 2014 it was used to implement 250 model and supporting data services for a variety of applications deployed at colorado state university united states department of agriculture usda other federal agencies and commercial data centers currently oms csip services integrate with modeling applications such as the usda nrcs integrated erosion tool iet resource stewardship evaluation tool rset conservation assessment ranking tool cart and water supply forecasting system ewsf as well as the usda ars wepp online webrhem and ages agroecosystem green et al 2015 applications the experiences gained in integrating the models and data services at scale while implementing modeling workflows led to the obvious choice of realizing the surrogate modeling solution pipeline as a set of oms csip services here the csip service api integrates with the encog 3 ml library for ann management and mongodb nosql libraries for data management into a service package encog3 is an open source ml framework available for java and c that has been designed with scalability in mind by exploiting multicore processors heaton 2015 this framework provides a variety of ml technologies including neat mongodb is a nosql database attempting to overcome limitations of sql databases when managing big data it is built on the notion of json documents and collections to overcome lack of flexibility due to the use of fixed schemas tables and rows typical of relational sql databases chodorow 2013 mongodb is schema less allowing for data management flexibility but creates challenges for data integrity collections contain sets of documents which can potentially have different field structures data in documents are self contained which allows for easily splitting up documents across different machines data sharding and setting up active active cluster configuration plugge et al 2015 3 2 1 surrogate modeling services pipeline and blockchain technology the fens is implemented as a set of five microservices that form a service pipeline the pipeline represents the conceptual workflow steps of fens the services integrate encog for ml related processing such as training and validation mongodb for data metadata storage and blockchain technology to create store and validate the sme the five web services are responsible for 1 collecting model input output snapshots t v dataset into an application dedicated nosql database 2 normalizing the t v dataset 3 training several sms on t subset and validating them against v subset 4 selecting the utmost performant sms to create an sme and 5 deploying and running the sme against new data such as χ subset as a consequence of stepping through the pipeline each service creates and adds a new block to the blockchain with service data and metadata which makes sm modeler decisions immutable each web service digests a json payload which consists of two sections to comply with csip api metainfo and parameter see listing 1 in appendix for csip payload example the metainfo section lists service context such as information required to set up the service run e g mode async to request asynchronous run the parameter section contains an array of json objects with two mandatory tuples with predefined keys name service variable name and value value to assign each json payload sets up service specific input parameters and contains a different set of objects consequently users can retrieve a template of each payload by invoking an http get against the corresponding service every service obtains required data and metadata from and pushes processed data and metadata to mongodb collections the design of each collection facilitates service database interaction and stores information useful to keep the execution of the pipeline valid starting from metadata of input output variables created by fens collect service each stage of the pipeline enriches the metadata section with additional information which are used to create a new block of the blockchain to ensure maximum flexibility modellers can manually run each service as described in the next sections one at a time or set up a modeling solution as an entire pipeline from start to end 3 2 1 1 fens collect service the fens collect service stores the raw t v dataset into a dedicated raw mongodb collection for later ann training it manages the upload of data as single input output model data records or in bulk as a csv file containing a list of records in case of single parameter permutation fens collect digests a json payload where the parameter section lists the json objects the mandatory name of the database to push data to and name value and description of each sm input output variable see listing 1 in appendix here name is the name of the model parameter while value is the actual model input or output description is a comma separated list of properties a in or out is mandatory to identify if that parameter is part of the input or output layer of the ann b normalized is required only if the value of that parameter is already normalized to skip the normalization process c normalization range e g 0 1 0 9 is required if the parameter is not normalized and user needs to set up a custom normalization range additionally fens collect allows for processing a csv file of input output snapshots of the original model runs by attaching it to the json payload the fens collect service has been designed to efficiently read in parse and push long csv tables by processing chunks of data the t dataset is stored in a dedicated collection called raw every variable is stored in a different mongodb document each document contains a values array with raw model data and a metadata document the latter contains table 1 1 variable name 2 variable normalization range normalization minimum and maximum 3 minimum and maximum values contained in the collected array 4 a type that specifies if the variable is part of the input or output layer of the ann structure 3 2 1 2 fens normalize service the fens normalize service normalizes the t v dataset stored into a raw mongodb collection and pushes processed data and additional metadata into a dedicated collection called normalized the fens normalize service builds the normalization algorithm and pushes it to the database and executes it as a database function it leverages mongdb aggregation operators which perform arithmetic expressions on grouped data records database side csip normalize currently implements the feature scaling normalization algorithm only as a result the json request to activate the service contains only the name of the database that stores the raw dataset a dedicated collection called normalized stores the processed dataset a mongodb document stores the array of normalized data and their metadata the latter is an enriched version of the metadata from the raw collection that contains additional information such as location of minimum and maximum values within the normalized dataset array table 2 this complies with fens requirements of augmenting metadata information throughout each phase of the pipeline 3 2 1 3 fens train service the fens train service creates the sm evaluates its emulation capabilities against the unseen validation subset and stores the structure of the ann and the augmented metadata into the database it also allows for a periodic recovery and constantly monitors sm learning performance as well as the growing structure and the mutating topology of the ann fens train service may run in parallel for a given data set each creating an ann instance of an ensemble every fens train instance creates one sm in three phases 1 t ν dataset splitting 2 sm creation 3 sm validation and storing the json payload contains the following json objects database name to identify the project scale mechanism and training perc to set the splitting mechanism population connection density training error and max epochs to set the sm creation recovery epochs to set how often sm validation and storing happens during the training process for monitoring purposes recovery 1 t v dataset splitting fens train service retrieves normalized t v dataset from the normalized collection and splits it into training t and validation v data sets three split mechanisms are currently available 1 simple 2 random and 3 same distribution simple method splits t v in t and v based on user defined percentage random method shuffles t v and randomly splits it in t and v based on user defined percentage same distribution method shuffles t v splits it into t and based on user defined percentage verifying the kolmogorov smirnov test between t and v on each variable 2 sm creation after in memory t v dataset split t feeds the evolutionary algorithm and initializes the sm creation neat requires a population of anns and their initial connection density between input and output nodes which activates the feature selection implicit in neat the creation of the sm terminates when 1 the training mean squared error mse is lower than target mse training error 2 the number of training iterations is higher than max epochs 3 the mse computed on the v subset increases for more than 10 training iterations since last decrement regularization as early stopping 4 the contraction of the mse is less than 10 6 for 99 iterations prechelt 1998 3 sm validation and storing recovery fens train executes this step during the previous phase sm creation as a recovery step and when the sm is fully trained during this phase the partially or fully trained ann runs against v the service assesses 18 model efficiency coefficients for each ann output variable see table a in appendix and stores them in the database along with additional metadata such as hyper parameters used to set up t v dataset splitting and sm creation steps evolution of training mse and mutation history of the network structure of the best genome number of input output and hidden nodes as well as nodes connections this allows users to seamlessly monitor sm learning performance and ann structural state at each recovery epoch table 3 at the end of the sm creation fens train leverages mongodb gridfs api to store the serialized ann as sm object gridfs automatically stores the binary file of the sm in trained chunk collection and metadata in trained files collection the fens train service creates the sm in an automated fashion by leveraging the neat genetic evolutionary algorithm and its stochasticity it also allows for constantly monitoring the improvement of the sm performance and ann structure during sm generation this service runs multiple times to create several different sms because of the thoroughly designed dataset splitting once fens train reaches either threshold mse error or maximum number of epochs the service runs the validation phase for the last time and stores sm performance and ann serialized objects in the database 3 2 1 4 fens select service the fens select service identifies the most performant sms stored in the trained collection and saves their uids into a new collection named selected the latter is the sme fens select provides three selection algorithms to select the utmost performant ones and create the sme in three steps 1 loop through the prediction estimators stored in the performance meta data of each trained and validated sm table 3 2 select the utmost performant sms based on a specific criteria and 3 save sm uid into a new collection named selected to form the sme see table 4 the three algorithms currently implemented are 1 error the service retrieves sorted sm metadata from the most performant to the least performant from the trained collection and selects the sms that show performance greater than threshold 2 percentile the service retrieves the performance errors of the sms generates their probability distribution and selects the sms which performance are greater than user specified percentile 3 number the service retrieves sorted sm metadata from the most performant to the least performant from the trained collection and selects n most performant sms without checking the actual performance errors the fens select json result payload contains the three objects required to set up the selection mechanism the mechanism type the threshold value either error value distribution percentile number of sms and the statistical error name once the fens select service has performed the selection operation the uids of the most performant sms are stored into the selected collection and will compose the application specific sme fens select also creates the final block of the internal blockchain that tracks the sme generation this block contains the application specific sme 3 2 1 5 fens run service the fens run is the final step in the fens pipeline it runs the sme against provided new input data such as χ it feeds the input layer of the sme after parsing the json payload with χ and returns denormalized uncertainty quantified results an example json response of fens tr20 is available in listing 2 in appendix in addition the fens run service allows the retrieval of the sme anns as a compressed archive containing data and metadata for their potential offline use external to csip finally every new instance of fens run adds a new block to the blockchain containing the sme package user s inputs and sme outputs to capture and trace the sme application 3 2 1 6 fens blockchain for a blockchain supporting fens we selected hyperledger fabric androulaki et al 2018 to serve as a prototype and reference implementation we choose a docker based solution to ensure a quick setup and a well curated environment each phase in the fens pipeline represents a state to be captured as a block in the blockchain the state of each phase is expressed in mongo as a distinct collection due to the nature of fens managing a sizable amount of data we opted for an off chain data storage and on chain storage of collection meta data we also utilized a chaincode approach for compiling and emitting relevant meta data parameter ranges statistics and data ann ensemble blobs to provision the run service a multi site implementation was prototyped to exercise a configuration to tag on sme applications at sdo locations 4 testing tr 20 with a synthetic dataset the goal of this experiment is to test the methods and the implementation technology introduced in the previous section we created a synthetic dataset to generate an sme producing runoff peak discharge and evaluate 1 neat and the sme as generic approximators of nonlinear functions 2 the automated generation of the sme at the framework level by leveraging the fens architectural style and the set of fens compliant web services implemented in csip 3 the blockchain technology to make the sme creation tamper resistant and connect the original dataset to the final sme for demonstrating the fens approach and workflow we used tr 20 usda 1983 a simple yet nonlinear model that fits this purpose tr 20 is an event based hydrologic model developed by the usda national resource conservation service nrcs to estimate runoff and peak discharge of stormwater events at watershed scales fennessey et al 2001 merkel 2002 tr 20 implements the curve number cn method for calculating the volume of runoff and the unit hydrograph to evaluate the peak discharge fennessey et al 2001 coleman et al 2016 the basic relationship for storm runoff is 6 q p i a 2 p i a s where q is the daily runoff as an average depth i n over the watershed area p is the 24 h rainfall depth i n i a is the initial abstraction i e interception surface storage and infiltration prior to runoff estimated as i a λ s λ 0 2 here but values between 0 0 and 0 3 are feasible singh et al 2010 moglen et al 2018 and s is the maximum potential retention storage i n 7 s 1000 c n 10 c n is the dimensionless curve number which varies between 0 no runoff generated from rainfall and 100 all rainfall becomes runoff tr 20 uses english units the peak discharge q p f t 3 s 1 is calculated as 8 q p 484 a q 0 5 d 0 6 t c where a is the watershed area m i 2 d is the duration of rainfall excess h and t c is the time of concentration h estimated by 9 t c l 0 8 s 1 0 7 1140 s l 0 5 where l is the longest flow path f t and s l is the average overland slope f t f t 1 woodward 2010 tr 20 is primarily applied to watershed planning and evaluation studies of alternative structures such as flood retarding structure channels miller and woodward 1994 reservoir construction merkel 2002 and irrigated paddy runoff jang et al 2010 it has been applied to storm runoff prediction in agricultural and urban watersheds jang et al 2010 in spite of its limitations tr 20 is still widely used by nrcs hydraulic and field engineers as well as regulators such as us federal agencies state and local governments mainly because of its ease of use fennessey et al 2001 coleman et al 2016 4 1 experimental design we implemented tr20 also as a csip service using the same platform and service api as the fens services both service packages were deployed as containers using kubernetes cephfs on a 400 core cluster and a sharded 87 tb mongodb 4 x for fens storage the tr 20 service was deployed using 4 containers fens itself on 8 containers with no cpu memory resource limits for docker we created the synthetic dataset d as a uniformly distributed set of points and ran csip tr 20 with each input sample to evaluate the corresponding peak discharge q p the halton sequence a low discrepancy quasi random number generator was used to generate uniformly distributed samples over the integration domain de rainville et al 2012 this deterministic method outperforms pseudo random number generators which usually create non uniformly distributed samples the simulation model ran at each input sample to evaluate corresponding model output eason and cremaschi 2014 this creates the dataset d which is then split into t v and χ t v constructs and validates each global sm part of the sme χ allows for testing the sme performance against an unseen dataset a python package implementing the generalized halton sequence generator braaten and weller 1979 faure and lemieux 2009 de rainville et al 2012 and the csip python client api concurrently invokes the csip tr 20 web service 40 000 times permuting input values between the provided ranges drainage area a 0 01 3 125 mi2 curve number cn 30 100 watershed slope sl 0 5 64 watershed length l 200 26 000 ft rain depth p 0 01 26 in to compute each value of peak discharge q p f t 3 s 1 the same python software split the generated dataset in 90 t v and 10 χ and verifies the ks test on each variable 4 2 results and discussion we utilized the fens system by invoking the fens collect web service to store the t v dataset and initiate the fens pipeline after normalizing the dataset with fens normalize 15 instances of fens train deployed on the csip cluster concurrently generated 15 anns we set up each fens train instance with a population of 500 anns targeting training mse of 1 5 10 4 of normalized values and 20 000 maximum number of epochs 6 instances of fens train terminated because they reached the mse limit while the remaining 9 because they reached the maximum number of epochs the final rmse for the 15 sms varied between 733 and 1014 cfs fens select created the sme by selecting the 10 sms with root mean squared error rmse lower than 900 cfs see appendix table b every sm has its own unique ann internal structure and connection weights with a total number of internal connections varying between 39 and 100 and hidden nodes between 10 and 23 summary of the ann structures are available in table b the delineation of the tr20 surrogate model was captured in a hyperledger fabric blockchain prototype we ran fens run using inputs from the 4 000 samples of the dataset we then compared the sme estimates with the original tr 20 model results to assess sme accuracy the sme generates a distribution of responses for each sample in χ dataset the median of each distribution compares with the tr 20 model result and the comparison on the entire χ dataset allows for evaluating prediction estimators that define the goodness of the sme fig 4 illustrates a subset of 50 samples of the full dataset comparing ensemble responses box plots and original tr 20 model runs red crosses for visual inspection sm prediction estimates are computed using the full 4 000 samples the prediction estimators show overall very good emulation capabilities of the sme generated through the fens system nash sutcliffe efficiency nse and kling gupta efficiency kge are both above 0 98 which demonstrate an excellent goodness of fit between original model results and the median of the sme results percentage bias pbias of 0 5 indicates that overall the sme has a very slight tendency to overestimate the peak discharge but it s very close to 0 and proves to be a very good sme approximation overall the sme accuracy is very good because relative root mean squared error rrmse is 11 6 li et al 2013 fig 5 compares the median of the sme with original model results on the identity line we use the blue color to identify the subset of 50 samples selected for fig 4 while red color for the full dataset additionally the plot shows regression lines and prediction estimators with the respective colors for the two sets the blue dotted regression line in fig 5 shows a very good proximity with the equality line and has the following form y 198 2 0 97 x where the positive intercept 198 2 cfs and the slope slightly smaller than 1 highlight the small tendency of the sme to overestimate low values of peak discharge and underestimate high values of peak discharge nevertheless the prediction estimators computed on this specific subset confirm the good emulation capabilities of the sme nse and kge are above 0 97 and rrmse is 10 6 the red regression line in fig 5 compares very well with the blue one and has the following form y 97 37 0 98 x there is a minimal difference in slope 0 97 vs 0 98 and intercept 198 2 cfs vs 97 37 cfs the first being just the 0 4 of the 0 50 000 cfs range while the second the 0 2 as well as prediction estimators however comparing these results we can state that the subset we selected for visual inspection in fig 4 is representative of the full 4 000 samples and spans through the entire range of peak discharge values there is a tendency to underestimate high values of peak discharge in both cases likely because of the low density of points in the t v dataset that generate high peak discharge to validate the overall sme response and demonstrate the goodness of the ensemble approach we selected the sms in the sme with the best and worst rmse and compared their performance metrics of fit to the process model on the 4 000 samples of the dataset with the median of the ensemble response the results in table 5 demonstrate how the ensemble median outperforms single best and single worst sms similarly nse of the median of the ensemble is greater than the nse values of the single worst sm and of the single best sm the small positive pbias of the ensemble median indicates a slight tendency to overestimate peak discharge values while single best and single worst sms have a slight tendency to underestimate peak discharge kge is very good overall with values falling in a very narrow range of 0 986 0 989 these results support using an ensemble median rather than selecting the best individual sm from the ensemble we did not perform any comparisons with other ann methods because the goal of this test was to 1 evaluate the goodness of neat for surrogate modeling 2 illustrate an application of the fens system and demonstrate the workflow of the fens pipeline and 3 test the implementation of the local fens blockchain additionally the unique features of neat make it particularly well suited for emf integration and automated generation of the sme at the framework level a long training time is an important weakness of the neat algorithm it took about two days to train 15 anns using several nodes of the csip cluster we originally decided to use encog3 java library to facilitate the integration and widespread use of the fens system within existing modeling frameworks such as csip the latter runs without any graphic cards and any research laboratory can easily integrate this system as a result since encog3 is just multithreaded it constrains the training scalability to a single multi core computer or server consequently we are considering the improvement of horizontal scaling by implementing a distributed execution framework that leverages a multi node computer cluster additionally wang et al 2013 identified an efficiency limitation in the standard neat algorithm and improved it by introducing node location and recurrent connection of node gene and increased population size over time thus optimizing complex learning tasks encog3 could benefit from the integration of these changes since it implements the original neat algorithm nevertheless we are also considering additional java libraries e g deeplearning4j to improve fens flexibility overall neat works very well as a ml methodology for data driven surrogate modeling tasks accurately emulating the non linear set of equations that estimates the peak discharge in tr 20 it perfectly integrates within the fens system altogether accomplishing the goal of automatically emerging robust and flexible sme at the framework level each web service of the fens pipeline creates a new block of the blockchain that is internally stored in the database fens select creates the last block of the internal blockchain which can be deployed for operational use we also verified that standard neat is a slow algorithm so we identified two possible ways to overcome this limitation in future applications 5 conclusions this work addressed a long known discrepancy for managing conceptual and process based models to fully comprehend correctly parameterize efficiently execute and flexibly deploy them within service delivery organizations for widespread and frequent use these organizations need a model to compute results quickly with limited set up and reduced data entry taking advantage of existing organization wide data resources to bridge this technology transfer gap this contribution aimed to alleviate research model application complexity with respect to data and parameter setup runtime requirements traceability and proper model infrastructure setup we proposed the extension of current state of the art environmental modeling framework workflows to automatically emerge data driven sms which capture the intrinsic knowledge of a conceptual or process based model into an ensemble system of sms this methodology accommodates application needs to get quick and accurate enough model results with limited input entries and limited a priori knowledge of internal processes involved in conceptual and process models this method allows for more consistent deployment on server desktop and mobile hardware while being platform and operating system independent furthermore it can potentially run offline in remote locations we started by leveraging neat which has proven to be appropriate for creating sms of process based models we enhanced the stochasticity of neat with a specifically designed cross validation technique which improves robustness and flexibility by creating an ensemble of surrogate models we conceptualized the extended modeling framework workflow to emerge the sme at the framework level the derived software architectural style should in turn facilitate widespread integration on any emf based on its characteristics we named this architecture framework enabled neuroevolutionary built surrogate model fens the fens design allows us to reshape the definition of surrogate model since the integration of an emf and a pipeline approach within a service architecture characterizes the sme itself as a modeling solution of the original model fens requirements and constraints stressed the need for blockchain technology as a means to consistently validate the pipeline and trace the deployed sme back to the source data and model here we realize that the fens blockchain not only bridges the gap between research environments and sdos but brings research efforts into everyday use as a matter of fact we showed in a high level example how this technology connects every step from the sme deployment all the way through to the end product deployment of sme results this aspect of the fens system opens important opportunities to simplify the integration of research modeling efforts into operational modeling and decision support we implemented the fens architecture within csip to expand its functionalities with a pipeline of csip and fens compliant web services the jupyter notebook link provided under software availability exercises the system and demonstrates how the pipeline of csip services automatically creates the sme we tested and demonstrated the methods using tr 20 peak discharge results to show that 1 neat is a good method for function approximation since this doe produced nse and kge accuracy greater than 0 98 and rrmse less than 12 2 fens automatically generates an ensemble of surrogate models sme for specific modeling solutions in a controlled environment and 3 the tamper resistant design of blockchain guarantees consistency in the process of sme creation and traceability of the tool once it is deployed outside the research environment of particular relevance the fens design uses maximal computation resources for training while requiring minimal computational resources and infrastructure setup for sme application this allows for delivery and application of sme within sdos e g a consultancy organization since an sme is extremely lightweight an sme with 10 anns of tr 20 sm is about 13 kb in size and provides the estimated rate of peak discharge in less than 100 ms one can deploy it within a block of the fens blockchain the tr 20 case study demonstrated that the median sme performed better than any single sm in conclusion fens has considerable potential for improving workflows for rapid resource assessment by using models with acceptable accuracy known uncertainty and verifiable research origin thus we believe that our contribution may help make results of complex conceptual or process models more available and accessible software availability csip cloud computing platform name csip cloud services integration platform developer dr olaf david contact odavid colostate edu availability license mit language java website https alm engr colostate edu cb project csip source code and documentation https alm engr colostate edu jenkins job csip mercurial repository https alm engr colostate edu cb hg csip core fens surrogate modeling services pipeline name fens framework enabled neuroevolutionary built surrogate modeling developer dr francesco serafin dr olaf david contact francesco serafin colostate edu availability license mit language java website https alm engr colostate edu cb project 45 repositories mercurial repository https alm engr colostate edu cb hg fens pipeline jupyter notebook that exercises the fens system including dataset https colab research google com drive 10sj7nau5o4zdpbz9hp nduqzr7zrstvq authuser 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank dr sean fleming for his assistance and feedback before the submission of the manuscript and the anonymous reviewers for their comments which have improved the quality of this paper the financial support from the university of trento https www dicam unitn it en is gratefully acknowledged the usda is an equal opportunity provider and employer mention of trade names or commercial products is solely for the purpose of providing specific information and does not imply recommendation of endorsement by the usda list of abbreviations ann artificial neural network api application programming interface ccp cloud computing platform cdf cumulative distribution functions cfs cubic square feet cn curve number csip cloud services integration platform doe design of experiment emf environmental modeling framework sme surrogate model ensemble fens framework enabled neuroevolutionary built surrogate modeling fs neat feature selective neat see below hru hydrological response unit json javascript object notation kge kling gupta efficiency ks kolmogorov smirnov maas model as a service ml machine learning mlp multilayer perceptron mse mean squared error neat neuroevolution of augmenting topology ne neuroevolutionary nse nash sutcliffe efficiency oms object modeling system pbias percentage bias pm process model rmse root mean squared error rrmse relative root mean squared error swat soil water assessment tool sm surrogate model uid unique identifier uq uncertainty quantification usda united states department of agriculture appendix table a model efficiency coefficients implemented in csip train service to validate partially of fully trained anns in the table o means observed s means simulated table a name abbreviation equation absolute difference absdiff i 1 n q i o q i s absolute volume error absvolumeerror i 1 n q i s q i o slope of linear regression of observed cumulative vs simulated cumulative dsgrad i 1 n c i o c o c i s c s i 1 n c i o c o 2 w h e r e c j 1 i q i i 1 2 n sum of residuals err sum i 0 n q i s q i o fenicia high flow fhf i 1 n q i s q i o 2 n fenicia low flow flf i 1 n ln q i s ln q i o 2 n index of agreement ioa 1 i 1 n q i o q i s 2 i 1 n q i s q o q i o q o 2 kling gupta efficiency kge 1 r 1 2 σ s i m σ o b s 1 2 μ s i m μ o b s 1 2 model deviation modeldev i 1 n q i s i 1 n q i o nash sutcliffe efficiency nashsutcliffe 1 i 1 n q i o q i s 2 i 1 n q i o q o 2 bias nbias i 1 n q i o q i s i 1 n q i s normalized rmse norm rmse i 1 n q i o q i s 2 i 1 n q i o q o percent bias pbias i 1 n q i o q i s i 1 n q i s 100 pearson correlation coefficient personcorrelation c o v q o q s σ q o σ q s peak weighted rmse pwrmse 1 n i 1 n q i o q i s 2 q i o q o 2 q o coefficient of determination r2 1 i 1 n q i o q i s 2 i 1 n q i o q o 2 root mean square error rmse 1 n i 1 n q s q o 2 transformed root mean squared error transformedrmse 1 n i 1 n 1 q i s 0 3 1 0 3 1 q i o 0 3 1 0 3 2 table b summary of structural elements training exit final rmse and selection of the 15 trained sms ns non selected s selected table b links hidden nodes training exit final rmse cfs sme selection 66 17 max epochs 941 44 ns 60 22 max epochs 928 63 ns 83 23 max epochs 834 42 s 56 15 max epochs 812 44 s 66 21 max epochs 913 12 ns 45 10 max epochs 838 50 s 39 10 max epochs 1013 89 ns 71 23 mse 781 09 s 54 17 max epochs 995 42 ns 56 19 mse 807 69 s 55 14 max epochs 880 22 s 100 17 mse 780 83 s 53 11 mse 756 24 s 66 15 mse 849 94 s 51 15 mse 733 01 s listing 1 template of csip collect service request listing 1 listing 2 example of sme json response emulation of tr 20 peak discharge listing 2 
25712,sensitivity analysis sa as a formal and standard component of scientific development and policy support is relatively young many researchers and practitioners from a wide range of disciplines have contributed to sa over the last three decades and the samo sensitivity analysis of model output conferences since 1995 have been the primary driver of breeding a community culture in this heterogeneous population now sa is evolving into a mature and independent field of science indeed a discipline with emerging applications extending well into new areas such as data science and machine learning at this growth stage the present editorial leads a special issue consisting of one position paper on the future of sensitivity analysis and 11 research papers on sensitivity analysis for environmental modelling published in environmental modelling software in 2020 21 keywords sensitivity analysis uncertainty analysis evidence based policy machine learning validation and verification of mathematical models 1 the topic sensitivity analysis sa is the tool to gauge how the inference originating from a model is dependent upon the assumptions and parameters feeding into it sa tackles the trade off between model completeness and model interpretability i e when the complexity of a model is justified by the quality of the data feeding into it and for many other applications linked to the quality of models sensitivity analysis could thus be seen as the hermeneutics of mathematical modelling to discern the meaning carried by the model under its mathematical and algorithmic formalism sa has been historically but informally a fundamental underpinning of scientific discovery and human decision making consider for example the classic laws of sliding friction whose discovery is commonly attributed to the experiments by leonardo da vinci in the 15th century hutchings 2016 these laws state that the friction force acting between two sliding surfaces is proportional i e linearly sensitive to the load pressing the surfaces together but is independent of i e insensitive to the apparent contact area between the two surfaces these laws were discovered by a series of physical experiments designed informally based on basic principles of sa changing one factor at a time in a system and assessing the impact of that change in the early 20th century the need for the efficient design of physical and chemical experiments to acquire representative information about the existence or strength of effects of one or multiple variables on another variable in a system was the motivation for the development of a new paradigm called design of experiments doe fisher 1953 doe was a first step towards formalization of sensitivity analysis later in the century the birth and growth of computational models of real world systems demanded newer paradigms to enable answering sa type questions in the context of complex and high dimensional but cheap to run computer experiments in response to this demand in the 1980s and 90s sa as a formal way of thinking started to materialize sobol 1993 in the last few years the sa community has gained visibility and assertiveness thanks to the efforts of the community and to the journal environmental modelling software ems which has seen sa as an essential discipline and set of tools for studying environmental system models of particular note is a manifesto for good modelling practices published by nature saltelli et al 2020 that also acknowledged the fundamental role of sensitivity analysis another recent recognition of sa s service role is a paper on major challenges in socio environmental system modelling elsawah et al 2020 appearing in the same period the challenges coincide with several of the topics flagged by the sa community now sa has started finding applications beyond conventional computational models in areas such as machine learning and data science in the context of machine learning different recently developed heuristics to facilitate feature and structure selection and to address issues around explainability and interpretability are rooted in principles of sa see bach et al 2015 galelli et al 2014 samek and müller 2019 toms et al 2020 or razavi et al 2020b for a review further formalization and standardization of sa approaches and tools in the field of machine learning can be instrumental in addressing the grand and emerging challenges that machine learning is facing in terms of explainability and interpretability and therefore falsifiability razavi 2021 rudin 2019 finally sa is now emerging as a paradigm that can directly deal with data in the absence of any model describing the underlying system that the data is collected from pianosi and wagener 2018 plischke et al 2013 sheikholeslami and 2020 the given data sa can provide unprecedented opportunities for scientists and practitioners across a wide range of disciplines to interrogate available datasets of any size small or large and on a range of in situ and remotely sensed variables to learn about their correlational or possibly causal relationships 2 the samo conferences started in 1995 in belgirate italy and held since then every three years the samo sensitivity analysis of model output conference series has been instrumental in creating a community of practitioners in what was once an archipelago of teams and disciplines 1 1 all samo proceedings 1995 2019 are stored at https bit ly 3zjpmce unlike many other disciplines the field of sa does not owe relevance to a specific discipline only and many researchers typically with vastly different research and educational backgrounds have contributed to it over the years samo has been central to bringing a community feeling to this field because of all these efforts sa is now beyond its original settings such as factor ranking or prioritization and its potential extends well into evidence based policy data science and machine learning in accord with the increasing prominence and relevance of sa a web site is being constructed to link all conference pages 2 2 see also https www gdr mascotnum fr samo html 3 the special issue and contributions the papers which make up this special issue for sa have been appearing in the journal of environmental modelling software ems between 2020 and 2021 these two years bracket the 9th international samo conference held at the open university of barcelona in october 2019 open university of catalonia 2019 and the next 10th conference to be held in march 2022 in the usa at florida state university florida state university 2022 the issue includes a major effort from the samo community an ems position paper signed by as many as twenty six practitioners of the discipline razavi et al 2021 entitled the future of sensitivity analysis an essential discipline for systems modeling and policy support and 11 research papers many originating from samo 2019 open university of catalonia 2019 all papers can be reached via the journal s homepage razavi et al 2020a more papers from the same samo 2019 event are collected in a twin special issue in the journal reliability engineering and system safety ress iooss and sudret 2021 the position paper maps the open challenges which need to be tackled to fully transform sa into a recognized discipline from the need to identify best practice and the attendant teaching material to the tackling of applications in new domains fig 1 from razavi et al 2021 the position paper also discusses how to engage more with modelling in the social sciences which is one of the great challenges identified in toms et al 2020 and with the decision sciences incorporating input from the social sciences for the quality of models is also an important topic as discussed in bammer et al 2020 another theme discussed in the position paper is how to tackle deep uncertainty problems following the interesting discussion on the topic in relation to the present pandemic saltelli et al 2020 steinmann et al 2020 a major challenge is also undertaking uncertainty analysis on models with high runtimes efficient sampling methods and model emulation techniques are required to address it as well as discussing with modellers when these analysis are worth the effort and the prioritization of computational resources that they demand one field of potential development identified in the position paper is machine learning almost all approaches toward interpretability and explainability are informally and sometimes formally based on sa the recent use of sensitivity analysis in the context of variable selection in regression becker et al 2021 by practitioners acquainted with sensitivity analysis literature confirms that similar developments are also possible for machine learning likely to be a hot application for sa in the coming years in addition most often emulators cheap surrogate models developed from the full model representation typically rooted in machine learning are used to generate sensitivity measures such as sobol the other papers in the present special issue represent an interesting compilation of ongoing sa research topics comparing the efficiency of existing sa methods puy et al 2021a azzini et al rosati sa for spatially and temporally distributed outputs roux et al 2021 sa for problems with dependent variables il idrissi et al 2021 development of new software tools for sa kimet al 2021 development of efficient visualization approaches to understand sa results şalap ayça et al 2021 application of sa to statistical modelling problems such as propensity score matching woo et al 2021 combining methods such as variance based and distribution based baroni and francke 2020 and new applications of sa to large models korgaonkar et al 2020 susini and todd 2021 4 the road to samo 2022 conclusions and challenges ahead historically various heuristics based on principles of sa but not named so have been the fundamental underpinnings of a variety of analyses in modelling and decision making such a process has a long history of application perhaps in all areas of science examples include assessment of the effectiveness of a decision option in a policy making problem the impact of a problem constraint on the optimality of a cost or benefit function via shadow prices or the role and function of a model parameter in generating a model output such analyses are generally referred to as local sensitivity analysis often not having gone through the learning curve of the samo community these applications leave scope for improvement in terms of multidimensional exploration of the input space via a specific design of experiment now that sa is gradually being recognized as an independent discipline with its own community an available handbook with different language versions douglas smith et al 2020 saltelli et al 2008 wu et al 2018 and various software tools adams 2020 baudin et al 2017 herman and usher 2017 iooss et al 2018 kucherenko and zaccheus 2018 marelli and sudret 2014 noacco et al 2019 puy et al 2022 razavi et al 2019 tong 2015 samo s effort to disseminate what samo does best must be redoubled samo should also continue its tradition to go beyond an analysis of just model parameter or structural uncertainty opening up to normative or framing dimensions in the analysis of the quality of a model as discussed in both the manifesto and the position paper saltelli et al 2020 razavi et al 2021 sa is now well positioned to guide the process of scenario generation about the possible future states of the world to address societal needs by identifying dominant controls of human natural systems razavi et al 2020c ghoreishi et al 2021 puy et al 2021b see also the growing literature around sensitivity auditing an extension of sa to policy relevant modelling studies saltelli et al 2013 a challenge for the samo community is how to reach out to modellers in all disciplines to support not only good practices and ongoing method development for their problem contexts but also to avoid rediscovery and relabeling of established sa methods in different applications one area where there is likely scope for cooperation is between the sa community and modellers engaged in ensemble modelling e g in climatic studies parker 2013 the application of uncertainty and sensitivity analysis may lead to a reconsideration of the severity of environmental threats based on point estimates puy et al 2020 and their use is hence to be advised in the making of ecological policies the coming of age mentioned in the title of our editorial will not be without devoted efforts and the occasional conflict but the payoff in terms of societal acceptance of mathematical models justifies it declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this special issue was made possible by the generous efforts of many reviewers among those who brought the heaviest load were samuele lo piano razi sheikholeslami arnald puy mohamed abdelhamed takuya iwanaga nhu do sergei kucherenko elmar plischke and matieyendou lamboni 
25712,sensitivity analysis sa as a formal and standard component of scientific development and policy support is relatively young many researchers and practitioners from a wide range of disciplines have contributed to sa over the last three decades and the samo sensitivity analysis of model output conferences since 1995 have been the primary driver of breeding a community culture in this heterogeneous population now sa is evolving into a mature and independent field of science indeed a discipline with emerging applications extending well into new areas such as data science and machine learning at this growth stage the present editorial leads a special issue consisting of one position paper on the future of sensitivity analysis and 11 research papers on sensitivity analysis for environmental modelling published in environmental modelling software in 2020 21 keywords sensitivity analysis uncertainty analysis evidence based policy machine learning validation and verification of mathematical models 1 the topic sensitivity analysis sa is the tool to gauge how the inference originating from a model is dependent upon the assumptions and parameters feeding into it sa tackles the trade off between model completeness and model interpretability i e when the complexity of a model is justified by the quality of the data feeding into it and for many other applications linked to the quality of models sensitivity analysis could thus be seen as the hermeneutics of mathematical modelling to discern the meaning carried by the model under its mathematical and algorithmic formalism sa has been historically but informally a fundamental underpinning of scientific discovery and human decision making consider for example the classic laws of sliding friction whose discovery is commonly attributed to the experiments by leonardo da vinci in the 15th century hutchings 2016 these laws state that the friction force acting between two sliding surfaces is proportional i e linearly sensitive to the load pressing the surfaces together but is independent of i e insensitive to the apparent contact area between the two surfaces these laws were discovered by a series of physical experiments designed informally based on basic principles of sa changing one factor at a time in a system and assessing the impact of that change in the early 20th century the need for the efficient design of physical and chemical experiments to acquire representative information about the existence or strength of effects of one or multiple variables on another variable in a system was the motivation for the development of a new paradigm called design of experiments doe fisher 1953 doe was a first step towards formalization of sensitivity analysis later in the century the birth and growth of computational models of real world systems demanded newer paradigms to enable answering sa type questions in the context of complex and high dimensional but cheap to run computer experiments in response to this demand in the 1980s and 90s sa as a formal way of thinking started to materialize sobol 1993 in the last few years the sa community has gained visibility and assertiveness thanks to the efforts of the community and to the journal environmental modelling software ems which has seen sa as an essential discipline and set of tools for studying environmental system models of particular note is a manifesto for good modelling practices published by nature saltelli et al 2020 that also acknowledged the fundamental role of sensitivity analysis another recent recognition of sa s service role is a paper on major challenges in socio environmental system modelling elsawah et al 2020 appearing in the same period the challenges coincide with several of the topics flagged by the sa community now sa has started finding applications beyond conventional computational models in areas such as machine learning and data science in the context of machine learning different recently developed heuristics to facilitate feature and structure selection and to address issues around explainability and interpretability are rooted in principles of sa see bach et al 2015 galelli et al 2014 samek and müller 2019 toms et al 2020 or razavi et al 2020b for a review further formalization and standardization of sa approaches and tools in the field of machine learning can be instrumental in addressing the grand and emerging challenges that machine learning is facing in terms of explainability and interpretability and therefore falsifiability razavi 2021 rudin 2019 finally sa is now emerging as a paradigm that can directly deal with data in the absence of any model describing the underlying system that the data is collected from pianosi and wagener 2018 plischke et al 2013 sheikholeslami and 2020 the given data sa can provide unprecedented opportunities for scientists and practitioners across a wide range of disciplines to interrogate available datasets of any size small or large and on a range of in situ and remotely sensed variables to learn about their correlational or possibly causal relationships 2 the samo conferences started in 1995 in belgirate italy and held since then every three years the samo sensitivity analysis of model output conference series has been instrumental in creating a community of practitioners in what was once an archipelago of teams and disciplines 1 1 all samo proceedings 1995 2019 are stored at https bit ly 3zjpmce unlike many other disciplines the field of sa does not owe relevance to a specific discipline only and many researchers typically with vastly different research and educational backgrounds have contributed to it over the years samo has been central to bringing a community feeling to this field because of all these efforts sa is now beyond its original settings such as factor ranking or prioritization and its potential extends well into evidence based policy data science and machine learning in accord with the increasing prominence and relevance of sa a web site is being constructed to link all conference pages 2 2 see also https www gdr mascotnum fr samo html 3 the special issue and contributions the papers which make up this special issue for sa have been appearing in the journal of environmental modelling software ems between 2020 and 2021 these two years bracket the 9th international samo conference held at the open university of barcelona in october 2019 open university of catalonia 2019 and the next 10th conference to be held in march 2022 in the usa at florida state university florida state university 2022 the issue includes a major effort from the samo community an ems position paper signed by as many as twenty six practitioners of the discipline razavi et al 2021 entitled the future of sensitivity analysis an essential discipline for systems modeling and policy support and 11 research papers many originating from samo 2019 open university of catalonia 2019 all papers can be reached via the journal s homepage razavi et al 2020a more papers from the same samo 2019 event are collected in a twin special issue in the journal reliability engineering and system safety ress iooss and sudret 2021 the position paper maps the open challenges which need to be tackled to fully transform sa into a recognized discipline from the need to identify best practice and the attendant teaching material to the tackling of applications in new domains fig 1 from razavi et al 2021 the position paper also discusses how to engage more with modelling in the social sciences which is one of the great challenges identified in toms et al 2020 and with the decision sciences incorporating input from the social sciences for the quality of models is also an important topic as discussed in bammer et al 2020 another theme discussed in the position paper is how to tackle deep uncertainty problems following the interesting discussion on the topic in relation to the present pandemic saltelli et al 2020 steinmann et al 2020 a major challenge is also undertaking uncertainty analysis on models with high runtimes efficient sampling methods and model emulation techniques are required to address it as well as discussing with modellers when these analysis are worth the effort and the prioritization of computational resources that they demand one field of potential development identified in the position paper is machine learning almost all approaches toward interpretability and explainability are informally and sometimes formally based on sa the recent use of sensitivity analysis in the context of variable selection in regression becker et al 2021 by practitioners acquainted with sensitivity analysis literature confirms that similar developments are also possible for machine learning likely to be a hot application for sa in the coming years in addition most often emulators cheap surrogate models developed from the full model representation typically rooted in machine learning are used to generate sensitivity measures such as sobol the other papers in the present special issue represent an interesting compilation of ongoing sa research topics comparing the efficiency of existing sa methods puy et al 2021a azzini et al rosati sa for spatially and temporally distributed outputs roux et al 2021 sa for problems with dependent variables il idrissi et al 2021 development of new software tools for sa kimet al 2021 development of efficient visualization approaches to understand sa results şalap ayça et al 2021 application of sa to statistical modelling problems such as propensity score matching woo et al 2021 combining methods such as variance based and distribution based baroni and francke 2020 and new applications of sa to large models korgaonkar et al 2020 susini and todd 2021 4 the road to samo 2022 conclusions and challenges ahead historically various heuristics based on principles of sa but not named so have been the fundamental underpinnings of a variety of analyses in modelling and decision making such a process has a long history of application perhaps in all areas of science examples include assessment of the effectiveness of a decision option in a policy making problem the impact of a problem constraint on the optimality of a cost or benefit function via shadow prices or the role and function of a model parameter in generating a model output such analyses are generally referred to as local sensitivity analysis often not having gone through the learning curve of the samo community these applications leave scope for improvement in terms of multidimensional exploration of the input space via a specific design of experiment now that sa is gradually being recognized as an independent discipline with its own community an available handbook with different language versions douglas smith et al 2020 saltelli et al 2008 wu et al 2018 and various software tools adams 2020 baudin et al 2017 herman and usher 2017 iooss et al 2018 kucherenko and zaccheus 2018 marelli and sudret 2014 noacco et al 2019 puy et al 2022 razavi et al 2019 tong 2015 samo s effort to disseminate what samo does best must be redoubled samo should also continue its tradition to go beyond an analysis of just model parameter or structural uncertainty opening up to normative or framing dimensions in the analysis of the quality of a model as discussed in both the manifesto and the position paper saltelli et al 2020 razavi et al 2021 sa is now well positioned to guide the process of scenario generation about the possible future states of the world to address societal needs by identifying dominant controls of human natural systems razavi et al 2020c ghoreishi et al 2021 puy et al 2021b see also the growing literature around sensitivity auditing an extension of sa to policy relevant modelling studies saltelli et al 2013 a challenge for the samo community is how to reach out to modellers in all disciplines to support not only good practices and ongoing method development for their problem contexts but also to avoid rediscovery and relabeling of established sa methods in different applications one area where there is likely scope for cooperation is between the sa community and modellers engaged in ensemble modelling e g in climatic studies parker 2013 the application of uncertainty and sensitivity analysis may lead to a reconsideration of the severity of environmental threats based on point estimates puy et al 2020 and their use is hence to be advised in the making of ecological policies the coming of age mentioned in the title of our editorial will not be without devoted efforts and the occasional conflict but the payoff in terms of societal acceptance of mathematical models justifies it declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this special issue was made possible by the generous efforts of many reviewers among those who brought the heaviest load were samuele lo piano razi sheikholeslami arnald puy mohamed abdelhamed takuya iwanaga nhu do sergei kucherenko elmar plischke and matieyendou lamboni 
25713,watershed models are widely used to study hydrological processes here we investigated how the temporal resolution of river water routing within the soil and water assessment tool swat model can affect simulated riverine hydrological processes we conducted numerical experiments in two watersheds in the northeastern u s to execute the swat model s variable storage coefficient river routing method with six different time steps ranging from 1 min to 1 day for the period between 2001 and 2018 we found that simulated stream discharge largely stabilizes with different time steps whereas the water storage and depth vary greatly those simulation results lead to a dramatic difference in the calculated dendritic connectivity index dci which is a widely used indicator for assessing aquatic ecosystem health we recommend taking a time step that is shorter than 1 h while recognizing that the appropriate time steps depend on actual watershed size and configuration graphical abstract image 1 keywords variable storage routing water storage water depth temporal resolution 1 introduction distributed physically based hydrologic models dpbhms are widely used for understanding the interconnections and dependencies among human water and energy nexus beven 2002 mccallum et al 2020 wagener et al 2010 intrinsic time marching attributes enabled dpbhms to continuously simulate the evolving hydrologic processes and address the cause and effect relationship between model inputs and predictions beven 2002 however there exists unavoidable uncertainties as we discretize the continuous natural processes into small time steps when driving forward the dpbhms the first source of those uncertainties could be coming from the temporal resolutions of input data or calibration data for example the temporal resolutions of precipitation can greatly influence the hydrologic simulations of streamflow ficchì et al 2016 huang et al 2019 higher temporal resolutions of rainfall can improve the hydrologic simulations of runoff especially the peak runoff events huang et al 2019 lyu et al 2018 schaller et al 2020 temporal resolutions of calibration data such as stream discharge could also significantly influence the model parameter estimations and predictions kavetski et al 2011 the second source of uncertainties could be attributed to the numerical errors which are typically stemming from using numerical discretization to approximate the differential equations anderson 1995 additional uncertainties may be caused by approximations or simplifications of the model structure and or governing equations which often ignore some physical processes while producing generic model errors götzinger and bárdossy 2008 streamflow simulation is one of the key model components of dpbhms the dynamic wave equation or the one dimensional saint venant equation can be used to describe the flow movement in rivers saint venant and valérie 1871 yet in real world applications of physically based hydrologic models the dynamic wave equation was often simplified to diffusive wave equation by assuming that the inertial terms are negligible relative to the gravity friction and pressure to accommodate computational tractability singh 1996 in a more simplified form such as the variable storage coefficient vsc method which belongs to the kinematic wave methods the streamflow equation is further simplified by assuming that the friction slope is approximately equal to the slope of the channel novak et al 2010 singh 1996 williams 1969 a classic dpbhm adopting the vsc method is the soil water assessment tool swat model which has been widely used in hydrologic studies arnold et al 1998 gassman et al 2007 the swat model has been extensively applied and validated for assessing water availability and quality such as stream discharge geza and mccray 2008 githui et al 2009 noori and kalin 2016 qi et al 2018 ramesh et al 2020 spruill et al 2000 and nutrient and sediment transport processes chu et al 2004 gassman et al 2014 malagó et al 2017 neupane et al 2020 ouyang et al 2020 qi et al 2020a 2020c santhi et al 2001 wu and chen 2012 zhang 2018 the typical temporal resolution for swat to solve the streamflow is 1 day while sub daily simulations are also applied boithias et al 2017 brighenti et al 2019 duan et al 2019 jodar abellan et al 2019 li et al 2018 maharjan et al 2013 q yang et al 2016 yu et al 2018 a few studies evaluated the effects of temporal resolution on the streamflow simulations with swat and showed that using a hourly time step helps improve hydrologic simulations as compared with a daily time step arnold and fohrer 2005 kim and lee 2010 krysanova and arnold 2008 nguyen et al 2018 however up to date the effects of temporal resolution on other hydrologic variables such as the stream water depth and storage remain to be assessed stream water depth and storage are important variables in evaluating stream health woznicki et al 2016 and reliably simulating other important hydrologic processes such as the sediment transport luque and beek 1976 and stream temperature dynamics macdonald et al 2014 qiu et al 2019 2020 and assessing water quality for example concentration of dissolved oxygen is one of the most important indicators of aquatic ecosystem health kannel et al 2007 accurate estimation of concentration of dissolved oxygen and gas aeration on air water surface is largely determined by water depth given the importance of stream water depth and storage there is an urgent need to understand the responses of stream water storage and depth to river routing temporal resolution in this study we aim to 1 investigate the sensitivity of swat simulated riverine hydrologic variables including the stream discharge stream water storage and stream water depth to different river routing time steps and 2 understand potential uncertainties associated with ecological assessment stemming from different river routing temporal resolutions specifically we conducted numerical experiments in two small watersheds in the northeastern u s over an 18 year 2001 2018 to evaluate the swat riverine simulations at six different time steps ranging from 1 min to 1 day previous research has evaluated the potential water quality impacts result from different land development scenarios sharifi et al 2017 and assessed transferability of parameter calibrations using different land use data yen et al 2015 in this region however no research has explored the potential impacts of temporal resolutions on the model simulations of detailed stream water variables and the potential influences on the stream health evaluations in this area we analyze model results regarding stream discharge and reach water storage and depth at different time steps and further compare dendritic connectivity index dci cote et al 2009 derived with hydrologic simulations at different time steps to understand implications of time resolution uncertainties for assessing hydrologic connectivity and aquatic ecosystem health finally discussion on how to choose appropriate time steps for river routing it is anticipated the results presented here will benefit future watershed model development in addition results and guidance provided by the present study can help modelers and decision makers to choose effective and appropriate modeling strategies and model configuration when making management policies 2 materials and methods 2 1 study sites and data sources we used two watersheds that respectively drain to the u s geological survey usgs hydrologic stations at the tuckahoe creek near ruthsburg usgs 01491500 and the choptank river near greensboro usgs 01491000 hereafter those watersheds are referred to as the tuckahoe creek watershed tcw 220 7 km2 and greensboro watershed gbw 290 1 km2 fig 1 a respectively these two watersheds are neighboring to each other and are both located at the headwaters of the choptank river watershed crw that overlies on the coastal plain of the chesapeake bay watershed the landscape in this region is of low relief with most of the land surface slopes smaller than 2 this region has a temperate humid climate with an annual precipitation of approximately 1200 mm and annual mean temperature of roughly 15 4 c the mainstem of tcw extends a total length of around 26 km from its northeast headwaters with an elevation around 20 m to its southwest outlet with an elevation around 4 m fig 1b the main channel of gbw also falls from ca 22 m at its source water to ca 4 m at its outlet and stretches a total of 30 km in length fig 1c although tcw and gbw have similar catchment sizes reach lengths and land surface slopes they exhibit varied land use and physiographic characteristics tcw is dominated by agriculture land use 54 0 and followed by forest 32 8 while the primary land use of gbw is forest 48 3 and agriculture land only accounts for 36 1 major soil types in tcw are well drained 56 1 hydrologic soil group hsg a b of which 69 5 of the area is occupied by croplands lee et al 2016 in contrast poorly drained soils 74 5 dominate in gbw hsg c d and 67 2 of those soils are on cropland 67 2 ator and denver 2015 these contrasts in land use and soil types between the two experimental watersheds serve well for our purpose to assess the impacts of temporal resolutions on the simulations of riverine hydrologic processes in different watersheds 2 2 description of the swat model the swat model is a dpbhm designed to simulate hydrological processes and predict water quantity and quality in ungauged basins as affected by management practices and changes in land use and climate arnold et al 1998 it is developed by the united states department of agriculture agricultural research service usda ars and has been widely used to simulate the hydrological cycle plant growth sediment loading and agricultural chemical leaching in numerous watersheds worldwide ficklin et al 2009 gassman et al 2007 li et al 2014 liang et al 2019 r srinivasan et al 2010 zhang et al 2017 swat divides a watershed into subbasins that are connected through a river network delineated based on a digital elevation model dem for each subbasin swat further defines hydrologic response units hrus that represent unique combinations of land use soils and elevation in swat simulation of the hydrology of a watershed is separated into two major divisions i e the land phase hydrologic processes occur on hrus and the routing of water through the channel network neitsch et al 2011 for the land phase swat simulates precipitation surface runoff vadose zone processes i e infiltration evaporation plant uptake lateral flows and percolation baseflow in this study based on previous model configuration and evaluation lee et al 2016 qi et al 2020b we used the modified cn method neitsch et al 2011 to estimate runoff generation and soil infiltration evapotranspiration is simulated using the penman monteith method monteith 1965 a bucket soil water model is used to simulate soil moisture distribution in the soil profile narasimhan et al 2005 the kinematic storage model developed by sloan and moore 1984 is used to estimate lateral flow from soils to river networks swat represents groundwater dynamics in a shallow aquifer which are regulated by base flow to the main channel and recharge received via percolation from soil bottom base flow is estimated by assuming a linear relationship to the rate of change in water table height and can be estimated by a baseflow recession constant neitsch et al 2011 for the routing phase swat uses manning s equation to define the rate and velocity of flow water is routed through the channel network using either the variable storage coefficient vsc routing method williams 1969 or the muskingum cunge routing method cunge 1969 overton 1966 williams 1969 developed the vsc method as an alternative to the muskingum method to allow for variable travel time within a reach here we use the vsc routing method because it is often used in previous sub daily studies with swat maharjan et al 2013 x yang et al 2016 and nguyen et al 2018 showed comparable performance between vsc and muskingum cunge the vsc routing method was developed based on the continuity equation 1 v i n v o u t δ v s t o r e d where v in m3 is the volume of inflow during the time step δ t v out m3 is the volume of outflow during the time step δ t and δ v s t o r e d m3 is the change in volume of storage during the time step δ t the continuity equation is linearly discretized between the beginning and end of the time step leading to the following form 2 δ t i 1 i 2 2 δ t o 1 o 2 2 v s t o r e d 2 v s t o r e d 1 where i 1 and i 2 are the inflow rate at the beginning and the end of the routing time interval respectively o 1 and o 2 are the outflow rate at the beginning and the end of the routing interval respectively v stored 1 and v stored 2 are the storage volumes at the beginning and the end of the routing interval respectively moving all unknown variables to the left equation 2 can be rearranged as 3 i a v e v s t o r e d 1 δ t o 1 2 v s t o r e d 2 δ t o 2 2 where i ave is the average inflow rate during δ t i a v e i 1 i 2 2 assuming inflow and outflow rates are approximately equal travel time tt can be calculated by dividing the volume of water in the channel by the flow rate using 4 t t v s t o r e d o v s t o r e d 1 o 1 v s t o r e d 2 o 2 the relationship between travel time out flow and storage coefficient can be obtained by substituting equation 4 into equation 3 5 o 2 2 δ t 2 t t δ t i a v e 1 2 δ t 2 t t δ t o 1 if we assign the storage coefficient sc as s c 2 δ t 2 t t δ t equation 5 can be simplified as 6 o 2 s c i a v e 1 s c o 1 combined with equations 3 and 4 the following equations can be obtained 7 o 2 s c i a v e v s t o r e d 1 δ t multiply both sides of equation 7 with the time step δ t the following equation is attained 8 v o u t s c v i n v s t o r e d 1 from equation 8 sc must satisfy s c 1 to ensure the v s t o r e d 1 to be physically meaningful the derivation of the vsc method clearly shows that the sc depends on δt or the routing time step in addition vsc assumes inflow and outflow rates are approximately equal in the calculation of travel time tt which is also time dependent notably despite the wide use of the vsc method in previous swat applications the effects of routing time step on its performance have not been examined 2 3 model setup and evaluation the swat model requires spatially explicit information on the climate soils and land use for the study watersheds the soil map was from the usda natural resources conservation service nrcs soil survey geographic database ssurgo topography data was obtained by resampling a 1 m resolution light detection and ranging lidar based digital elevation model to 10 m using nearest neighbor interpolation the land use map and the scheduling of crop rotations were generated using 2008 2012 data from the usda national agricultural statistics service nass cropland data layer cdl daily precipitation and temperature were downloaded from the national oceanic atmospheric administration noaa national climate data center ncdc at chestertown and royal oak usc00181750 and usc00187806 respectively daily solar radiation relative humidity and wind speed were generated by the weather generator wxgen within the swat model for more information regarding swat model setup for these two watersheds please refer to lee et al 2016 2 4 model comparison and evaluation between different time steps we modified the original vsc method to be able to run at flexible time steps δt from 1 min to 1 day and assessed model outputs of stream discharge water storage and water depth for those time steps 1 day we aggregated the model outputs to a daily time step to evaluate and compare the simulation results for the land phase hydrologic processes we discretized the daily water yield into different time steps assuming a uniform distribution as accurately observed precipitation and other climate data are not available at a sub daily time step in our study region this treatment also helps eliminate the differences in terrestrial hydrologic processes which may affect simulation of riverine hydrologic processes we evaluated the model performance of daily stream discharge against usgs observations using nash sutcliffe efficiency nse nash and sutcliffe 1970 and the percent bias pbias nse ranges from to 1 with higher values indicating better performance pbias assesses overall model bias over the entire simulation period with small values indicating low bias note that our model comparison and evaluation were performed without calibrating swat parameters for two major reasons first the swat model was originally developed to assess hydrology and water quality in large scale ungauged watersheds arnold et al 1998 recently zhang et al 2013a 2013b and arnold et al 2015 both highlighted the importance of improving representation of physical processes given that parameter calibration in many cases cannot guarantee high fidelity of modeling results second optimization algorithms have difficulty identifying globally optimal parameter solutions for complex hydrologic models like swat zhang et al 2009 therefore parameter calibration will likely introduce additional uncertainties to model comparison between different time steps we also calculated the dci to quantitatively assess the diversification of ecologic indications under different river routing temporal resolutions the dci is widely used to assess continuity and connectivity of a river network and estimate the probability that fish can move between any two points in a river network cote et al 2009 jaeger et al 2014 for example jaeger et al 2014 found that climate change will alter hydrologic connectivity and damage endemic fishes in the verde river basin united states based on swat simulated hydrographs at the daily time step specifically the dci is calculated based on the connectivity of any two discrete segments of a river network which is determined by the passability of fish movement between the two river segments 9 d c i i 1 n j 1 n l i l l j l c i j 100 where n is the total number of river segments within the network l is the total length of all the river segments c ij is the connectivity between segments i and j with respective lengths of l i l j the dci has a value range between 0 and 100 with 0 denotes extremely low habitat connectivity and 100 represents the highest habitat connectivity and fish movement passability in the whole dendritic river network the calculation of c ij is based on the number and length of zero flow segments or the density of the barriers which physically denote spatially discontinuous wet reaches in this study each river reach is considered as a segment and the barriers or the zero flow conditions are defined as stream water storage less than 0 1 m3 assuming the passabilities of upstream and downstream reaches are equal for a given pair of reaches c ij can be calculated as 10 c i j k 1 k p k u p k d k 1 k p k 2 where k is the total number of dry barriers between a given pair of reaches i and j with individual barriers marked with k p k u and p k d are the upstream and downstream passabilities through barrier k and has an equal value of p k in this analysis we simplify the calculation of p k using a binary threshold method which assuming reaches with zero daily water storage storage 0 1 m3 has a zero chance passability p k 0 any wet reaches storage 0 1 m3 have a passability of 1 p k 1 3 results and discussions 3 1 stream discharge simulated at different time steps the average daily stream discharges simulated for the cross section of each reach along the mainstems fig 1 of the two experimental watersheds are plotted for each month and different time steps in fig 2 visual inspection shows that spatial and temporal patterns of simulated stream discharge are similar between different river routing time steps as expected downstream reaches that drain a larger area exhibit higher stream discharge relative to upstream reaches for the tcw the peak discharges occurred during march june with the highest averaged daily discharge of 5 85 0 01 m3 s 1 mean standard deviation in june at the catchment outlet similarly the peak stream discharges in the gbw also occur in june with a daily average of 7 92 0 01 m3 s 1 mean standard deviation at the catchment outlet for both watersheds different swat simulations show nearly identical daily stream discharge under different temporal resolutions the nse values obtained in the tcw show minimal variations over different time steps with an average value of 0 287 and a standard deviation value of 0 002 as shown in fig 3 for the gbw smaller temporal resolutions lead to slightly better model performances or higher nse values the mean of nse values over different time steps is 1 635 with a standard deviation value of 0 166 the pbias values obtained with different river touring time steps also are not much different from each other in the two watersheds for example the mean pbias is 43 62 0 01 mean standard deviation and 32 00 0 01 respectively for tcw and gbw note that nse values obtained in both tcw and gbw are negative and pbias indicates relatively large bias this is because we did not calibrate parameters of the swat model to match observed daily streamflow thus ensuring a fair comparison among model simulations at different temporal resolutions 3 2 stream water storage estimated at different river routing time steps the monthly average daily stream water storage over the period 2001 2018 for reaches along the mainstem fig 1 of the two watersheds are plotted in fig 4 in contrast to the stream discharge the stream water storages simulated with different river routing time steps are dramatically different from each other in both watersheds fig 4 in general higher temporal resolutions attained greater water storage in reaches notably the water storage in the river networks was nearly zero with river routing time steps of 4 hr 12 hr and 1 day indicating that the water entering a reach drain away instantly in addition seasonal variation and longitudinal distribution of water storage was almost undetectable for these larger time steps when the time step became smaller the seasonal and spatial distribution of water storage for the mainstem became clearer it is worth noting that with the change from 1 h to 1 min swat simulated an increase of mean water storage from 9500 to 18900 m3 for tcw and from 15000 m3 to 27000 m3 for gbw in the reach adjacent to the outlet respectively these results indicate that model simulated water storage and depth are sensitive to changes in river routing time steps when the model is executed at finer time resolutions e g 1 min to 1 hr 3 3 stream water depth estimated at different river routing time steps the daily stream water depth averaged for each month over the period 2001 2018 for reaches along the mainstem fig 1 of the two watersheds are plotted in fig 5 like stream water storage stream water depth also varied greatly over different river routing time steps in both tcw and gbw fig 5 similar to water storage seasonal variation and longitudinal distribution of water depth was almost undetectable for these larger time steps of 4 hr 12 hr and 1 day and when the time step became smaller the seasonal and spatial distribution of water depth for the mainstem became clearer this is not a surprise as the volume of water storage in reach determines water depth stream depth determines stream heat capacity which directly influences the stream temperature dynamics macdonald et al 2014 qiu et al 2019 2020 meanwhile stream depth is also closely related to water quality modeling for example stream depth is an important variable influencing the nitrate uptake rate mulholland et al 2008 2009 and gas aeration sorrell and tanner 2000 moreover stream depth and water storage determine the hydrologic connectivity which is an important indicator of stream health cote et al 2009 woznicki et al 2016 the strong variations of simulated stream depth results indicate that uncertainties could derive from using coarse river routing temporal resolutions when swat is applied to simulate water temperature and water quality processes 3 4 spatial assessment of different hydrologic variables simulated with different river routing time steps here we present spatial maps showing the average hydrologic variables to further illustrate the implications of choosing different river routing time steps for hydrologic modeling and assessment the spatial maps of daily stream discharge averaged over 2001 2018 are shown in fig 6 the stream discharge maps show nearly identical distributions under different time steps for both watersheds which is consistent with the previous temporal comparisons the spatial maps of averaged daily stream water storage fig 7 and stream water depth fig 8 show large differences between different river routing time steps in general when routing time steps are equal to or less than 1 hr stream water storage begins to accumulate while all reaches are nearly dry for routing time steps larger than 4 h we went on to calculate zero flow days per year for each reach fig 9 based on the criterion that if water storage in a reach is less than 0 1 m3 it is determined as dry we observed more zero flow days with larger time steps for example the annual zero flow days for the two watersheds are more than 200 days across the whole stream network when the time steps are 12 hr and 1 day when routing time steps are equal to or less than 1 h the number of simulated zero flow days is minimal and spatial patterns are like each other 3 5 implications of river routing time steps for hydrologic connectivity assessment as shown in fig 10 the simulated monthly dci decreases as the river routing temporal resolution increases in both watersheds which is expected since the annual zero flow days increase with the temporal resolution as discussed above for both experimental watersheds the monthly dci ranges from nearly 0 under the time step of 1 day to 100 under the time step of 1 min during the entire simulation period this indicates incompatible connectivity of the river network from results under different temporal resolutions overall dci time series derived for the time steps from 30 min to 12 h missed the year round high hydrologic connectivity in the two watersheds this may be because shorter time steps e g 30 min can simulate the subtle changes that cannot not be captured at a larger time step e g a day rapidly changing climate is continuously impacting the surface water and groundwater storage which are poised to threaten the habitats of aquatic species gober and kirkwood 2010 lake 2011 in addition increasing water demand due to population growth boretti and rosa 2019 accelerated human disturbances such as deforestation peña arancibia et al 2019 are simultaneously exerting pressure on the resilience of aquatic ecosystems hydrologic models including the swat model are often used to project hydrologic connectivity of freshwater ecosystems and understand associated impacts on species persistence ecosystem integrity and human well being jaeger et al 2014 the dramatic differences between the zero flows and dci derived with different time steps figs 9 and 10 clearly illustrates the importance of selecting appropriate river routing time steps for reliable assessment of ecological impacts of future changes to regional hydrology 3 6 analysis of storage coefficient derived with different routing time steps the constraint of sc s c 1 in eq 8 δ t 2 t t must be satisfied to ensure the outflow does not deplete the volume of inflow plus the storage in other words if δ t is larger than 2 tt the simulated storage volume may be negative in real world applications especially when simulating short reaches with relatively short travel times this constraint may not be satisfied to prevent this swat constrained sc to be equal to 1 when its calculated value is larger than 1 the sc values are close or equal to 1 for both experimental watersheds when δ t reaches 4 h and larger fig 11 however the simulated storage volume can still approach zero in this setup as can be seen from eq 8 especially in short reaches with relatively small tt for most reaches of the two experimental watersheds with the longest reach length of 7875 m for tcw and 10328 m for gbw using time steps longer than 4 h could not guarantee δ t 2 t t or s c 1 as a result the simulated water storage volume and water depth may approach zero as discussed in section 3 2 3 4 overall the vsc method within the swat model can provide consistent stream discharge results regardless of temporal resolutions however when hydrological variables such as stream water storage and depth are important in swat applications we recommend using at least 1 hr time step for streamflow routing when using the vsc method while recognizing that the appropriate time steps depend on actual watershed size and configuration 3 7 recommendations for more comprehensive evaluation of hydrologic variables reconciling the balance between model efficiency and fidelity remains a big challenge in developing dpbhms beven 1993 2000 to pursue computational efficiency model simplifications usually focus on the dominant physical processes while overlooking some relevant coevolving processes as detailed river channel geometry and field observations of river water storage and depth data are not available in the two experimental watersheds we could not directly assess and compare the model performance with different routing time steps based on streamflow simulation results fig 3 we contend that finer routing time steps help improve model performance particularly for water storage and depth simulations note the model performance of riverine processes could be influenced by errors and uncertainties associated with model inputs e g climate forcing land use and soil types and other hydrologic processes e g evapotranspiration soil moisture and groundwater therefore we recommend conducting numerical experiments to examine the model simulations of stream discharge water storage and depth with different routing time steps the general model evaluation procedures widely used for the swat model moriasi et al 2007 santhi et al 2001 consider only streamflow without explicit assessment of water storage and depth in streams not only are hydrologic connectivity and aquatic ecosystem health assessment sensitive to river routing time steps section 3 5 but also are sediment transport and deposition stream thermal dynamics and nutrient cycles luque and beek 1976 macdonald et al 2014 qiu et al 2019 2020 given the high sensitivity of river water storage and depth to river routing time steps we contend that those two hydrologic variables should be included in the model assessment procedures fig 12 if reliable data of channel geometry and observations of the three riverine hydrologic variables are available quantitative assessment using widely accepted statistical metrics such as nse and pbias moriasi et al 2007 can be applied to choose the appropriate time steps by balancing efficiency and model accuracy when those data are not available we suggest either choose the finest time steps that are affordable with available computational resources or compare the spatial and temporal patterns of simulated variables as discussed in sections 3 1 3 2 and 3 3 and ensure the selected times steps do not miss important characteristics of the hydrologic systems for example in the tcw and gbw although we do not have reliable water storage and depth data our field experience in the two watersheds does show that the rivers are perennial instead of ephemeral therefore we need to select a time step 1 hr or less that at least shows the rivers have water for most days of the year 4 conclusions in this study we examined the effects of routing time steps on streamflow water storage and depth simulations with the widely used vsc method within the swat model the results show that there exist nonignorable uncertainties in the simulated riverine hydrologic variables stemming from variations in routing temporal resolution although strong agreement was reached among stream discharge simulations with different temporal resolutions the simulated stream water storage and depth are irreconcilable swat with coarser time steps i e 4 hr 12 hr and 1 day drains nearly all flows entering a reach leading to large number of zero flow days minimal water storage and depth during a year and low hydrologic connectivity as indicated by the low dci values in the river networks in contrast with finer river routing time steps e g 1 min 30 min and 1 hr swat simulates persistent water storage in the river networks which is consistent with perception of the watershed via many years of field experimental experience in addition finer routing time steps in general slightly improve stream discharge simulations as evaluated against long term observed daily streamflow data in the two experimental watersheds overall our numerical analysis shows high sensitivity of simulations of stream water storage and depth to river routing time steps and the consequent significant implications for hydrologic connectivity and aquatic health assessment therefore we recommend in addition to streamflow including stream water storage and depth into the general procedures that are widely used for watershed model evaluation particularly when the application purposes are beyond streamflow prediction although our experiments suggest a routing time step less than 1 hr should be adopted we recognize that model performance is dependent on both routing time steps and reach length i e actual watershed size and configuration that vary greatly across study regions therefore we could not specify a standard threshold for appropriate routing time steps instead general guidelines are recommended when detailed channel geometry and reliable observations of water storage and depth data are available we suggest conducting quantitative assessment using statistical metrics when those data are not available we suggest examining the spatial and temporal patterns of simulated river hydrologic variables and choose a time step that match the experiential perception of the hydrologic conditions in the watersheds in addition as using finer time steps e g 1 min could greatly increase the computation cost both storage space and computational time the balance between model efficiency and accuracy should also be factored in we anticipate the findings derived from this study help future watershed model development and application to support watershed management and assessment software revised swat 2012 operating systems windows linux dependent software fortran 90 availability free of charge the revised swat 2012 with flexible river routing time steps is available by contacting the authors the code will also be published in future releases of swat 2012 through the swat website editorial conflict of interest statement given his role as environmental modelling software editorial board member xuesong zhang was not involved in the peer review of this article and has no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to journal editor daniel p ames declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was supported in part by the u s department of agriculture agricultural research service as a contribution from the long term agroecosystem research ltar network the funding support for this project was provided by national aeronautics and space administration nnx17ae66gand 18 cms18 0052 and the u s department of agriculture 2017 67003 26484 funding was also provided in part by the u s department of agriculture natural resources conservation service conservation effects assessment project nrcs ceap drs min chen and han qiu were also partly supported by national aeronautics and space administration 80hqtr19t0055 
25713,watershed models are widely used to study hydrological processes here we investigated how the temporal resolution of river water routing within the soil and water assessment tool swat model can affect simulated riverine hydrological processes we conducted numerical experiments in two watersheds in the northeastern u s to execute the swat model s variable storage coefficient river routing method with six different time steps ranging from 1 min to 1 day for the period between 2001 and 2018 we found that simulated stream discharge largely stabilizes with different time steps whereas the water storage and depth vary greatly those simulation results lead to a dramatic difference in the calculated dendritic connectivity index dci which is a widely used indicator for assessing aquatic ecosystem health we recommend taking a time step that is shorter than 1 h while recognizing that the appropriate time steps depend on actual watershed size and configuration graphical abstract image 1 keywords variable storage routing water storage water depth temporal resolution 1 introduction distributed physically based hydrologic models dpbhms are widely used for understanding the interconnections and dependencies among human water and energy nexus beven 2002 mccallum et al 2020 wagener et al 2010 intrinsic time marching attributes enabled dpbhms to continuously simulate the evolving hydrologic processes and address the cause and effect relationship between model inputs and predictions beven 2002 however there exists unavoidable uncertainties as we discretize the continuous natural processes into small time steps when driving forward the dpbhms the first source of those uncertainties could be coming from the temporal resolutions of input data or calibration data for example the temporal resolutions of precipitation can greatly influence the hydrologic simulations of streamflow ficchì et al 2016 huang et al 2019 higher temporal resolutions of rainfall can improve the hydrologic simulations of runoff especially the peak runoff events huang et al 2019 lyu et al 2018 schaller et al 2020 temporal resolutions of calibration data such as stream discharge could also significantly influence the model parameter estimations and predictions kavetski et al 2011 the second source of uncertainties could be attributed to the numerical errors which are typically stemming from using numerical discretization to approximate the differential equations anderson 1995 additional uncertainties may be caused by approximations or simplifications of the model structure and or governing equations which often ignore some physical processes while producing generic model errors götzinger and bárdossy 2008 streamflow simulation is one of the key model components of dpbhms the dynamic wave equation or the one dimensional saint venant equation can be used to describe the flow movement in rivers saint venant and valérie 1871 yet in real world applications of physically based hydrologic models the dynamic wave equation was often simplified to diffusive wave equation by assuming that the inertial terms are negligible relative to the gravity friction and pressure to accommodate computational tractability singh 1996 in a more simplified form such as the variable storage coefficient vsc method which belongs to the kinematic wave methods the streamflow equation is further simplified by assuming that the friction slope is approximately equal to the slope of the channel novak et al 2010 singh 1996 williams 1969 a classic dpbhm adopting the vsc method is the soil water assessment tool swat model which has been widely used in hydrologic studies arnold et al 1998 gassman et al 2007 the swat model has been extensively applied and validated for assessing water availability and quality such as stream discharge geza and mccray 2008 githui et al 2009 noori and kalin 2016 qi et al 2018 ramesh et al 2020 spruill et al 2000 and nutrient and sediment transport processes chu et al 2004 gassman et al 2014 malagó et al 2017 neupane et al 2020 ouyang et al 2020 qi et al 2020a 2020c santhi et al 2001 wu and chen 2012 zhang 2018 the typical temporal resolution for swat to solve the streamflow is 1 day while sub daily simulations are also applied boithias et al 2017 brighenti et al 2019 duan et al 2019 jodar abellan et al 2019 li et al 2018 maharjan et al 2013 q yang et al 2016 yu et al 2018 a few studies evaluated the effects of temporal resolution on the streamflow simulations with swat and showed that using a hourly time step helps improve hydrologic simulations as compared with a daily time step arnold and fohrer 2005 kim and lee 2010 krysanova and arnold 2008 nguyen et al 2018 however up to date the effects of temporal resolution on other hydrologic variables such as the stream water depth and storage remain to be assessed stream water depth and storage are important variables in evaluating stream health woznicki et al 2016 and reliably simulating other important hydrologic processes such as the sediment transport luque and beek 1976 and stream temperature dynamics macdonald et al 2014 qiu et al 2019 2020 and assessing water quality for example concentration of dissolved oxygen is one of the most important indicators of aquatic ecosystem health kannel et al 2007 accurate estimation of concentration of dissolved oxygen and gas aeration on air water surface is largely determined by water depth given the importance of stream water depth and storage there is an urgent need to understand the responses of stream water storage and depth to river routing temporal resolution in this study we aim to 1 investigate the sensitivity of swat simulated riverine hydrologic variables including the stream discharge stream water storage and stream water depth to different river routing time steps and 2 understand potential uncertainties associated with ecological assessment stemming from different river routing temporal resolutions specifically we conducted numerical experiments in two small watersheds in the northeastern u s over an 18 year 2001 2018 to evaluate the swat riverine simulations at six different time steps ranging from 1 min to 1 day previous research has evaluated the potential water quality impacts result from different land development scenarios sharifi et al 2017 and assessed transferability of parameter calibrations using different land use data yen et al 2015 in this region however no research has explored the potential impacts of temporal resolutions on the model simulations of detailed stream water variables and the potential influences on the stream health evaluations in this area we analyze model results regarding stream discharge and reach water storage and depth at different time steps and further compare dendritic connectivity index dci cote et al 2009 derived with hydrologic simulations at different time steps to understand implications of time resolution uncertainties for assessing hydrologic connectivity and aquatic ecosystem health finally discussion on how to choose appropriate time steps for river routing it is anticipated the results presented here will benefit future watershed model development in addition results and guidance provided by the present study can help modelers and decision makers to choose effective and appropriate modeling strategies and model configuration when making management policies 2 materials and methods 2 1 study sites and data sources we used two watersheds that respectively drain to the u s geological survey usgs hydrologic stations at the tuckahoe creek near ruthsburg usgs 01491500 and the choptank river near greensboro usgs 01491000 hereafter those watersheds are referred to as the tuckahoe creek watershed tcw 220 7 km2 and greensboro watershed gbw 290 1 km2 fig 1 a respectively these two watersheds are neighboring to each other and are both located at the headwaters of the choptank river watershed crw that overlies on the coastal plain of the chesapeake bay watershed the landscape in this region is of low relief with most of the land surface slopes smaller than 2 this region has a temperate humid climate with an annual precipitation of approximately 1200 mm and annual mean temperature of roughly 15 4 c the mainstem of tcw extends a total length of around 26 km from its northeast headwaters with an elevation around 20 m to its southwest outlet with an elevation around 4 m fig 1b the main channel of gbw also falls from ca 22 m at its source water to ca 4 m at its outlet and stretches a total of 30 km in length fig 1c although tcw and gbw have similar catchment sizes reach lengths and land surface slopes they exhibit varied land use and physiographic characteristics tcw is dominated by agriculture land use 54 0 and followed by forest 32 8 while the primary land use of gbw is forest 48 3 and agriculture land only accounts for 36 1 major soil types in tcw are well drained 56 1 hydrologic soil group hsg a b of which 69 5 of the area is occupied by croplands lee et al 2016 in contrast poorly drained soils 74 5 dominate in gbw hsg c d and 67 2 of those soils are on cropland 67 2 ator and denver 2015 these contrasts in land use and soil types between the two experimental watersheds serve well for our purpose to assess the impacts of temporal resolutions on the simulations of riverine hydrologic processes in different watersheds 2 2 description of the swat model the swat model is a dpbhm designed to simulate hydrological processes and predict water quantity and quality in ungauged basins as affected by management practices and changes in land use and climate arnold et al 1998 it is developed by the united states department of agriculture agricultural research service usda ars and has been widely used to simulate the hydrological cycle plant growth sediment loading and agricultural chemical leaching in numerous watersheds worldwide ficklin et al 2009 gassman et al 2007 li et al 2014 liang et al 2019 r srinivasan et al 2010 zhang et al 2017 swat divides a watershed into subbasins that are connected through a river network delineated based on a digital elevation model dem for each subbasin swat further defines hydrologic response units hrus that represent unique combinations of land use soils and elevation in swat simulation of the hydrology of a watershed is separated into two major divisions i e the land phase hydrologic processes occur on hrus and the routing of water through the channel network neitsch et al 2011 for the land phase swat simulates precipitation surface runoff vadose zone processes i e infiltration evaporation plant uptake lateral flows and percolation baseflow in this study based on previous model configuration and evaluation lee et al 2016 qi et al 2020b we used the modified cn method neitsch et al 2011 to estimate runoff generation and soil infiltration evapotranspiration is simulated using the penman monteith method monteith 1965 a bucket soil water model is used to simulate soil moisture distribution in the soil profile narasimhan et al 2005 the kinematic storage model developed by sloan and moore 1984 is used to estimate lateral flow from soils to river networks swat represents groundwater dynamics in a shallow aquifer which are regulated by base flow to the main channel and recharge received via percolation from soil bottom base flow is estimated by assuming a linear relationship to the rate of change in water table height and can be estimated by a baseflow recession constant neitsch et al 2011 for the routing phase swat uses manning s equation to define the rate and velocity of flow water is routed through the channel network using either the variable storage coefficient vsc routing method williams 1969 or the muskingum cunge routing method cunge 1969 overton 1966 williams 1969 developed the vsc method as an alternative to the muskingum method to allow for variable travel time within a reach here we use the vsc routing method because it is often used in previous sub daily studies with swat maharjan et al 2013 x yang et al 2016 and nguyen et al 2018 showed comparable performance between vsc and muskingum cunge the vsc routing method was developed based on the continuity equation 1 v i n v o u t δ v s t o r e d where v in m3 is the volume of inflow during the time step δ t v out m3 is the volume of outflow during the time step δ t and δ v s t o r e d m3 is the change in volume of storage during the time step δ t the continuity equation is linearly discretized between the beginning and end of the time step leading to the following form 2 δ t i 1 i 2 2 δ t o 1 o 2 2 v s t o r e d 2 v s t o r e d 1 where i 1 and i 2 are the inflow rate at the beginning and the end of the routing time interval respectively o 1 and o 2 are the outflow rate at the beginning and the end of the routing interval respectively v stored 1 and v stored 2 are the storage volumes at the beginning and the end of the routing interval respectively moving all unknown variables to the left equation 2 can be rearranged as 3 i a v e v s t o r e d 1 δ t o 1 2 v s t o r e d 2 δ t o 2 2 where i ave is the average inflow rate during δ t i a v e i 1 i 2 2 assuming inflow and outflow rates are approximately equal travel time tt can be calculated by dividing the volume of water in the channel by the flow rate using 4 t t v s t o r e d o v s t o r e d 1 o 1 v s t o r e d 2 o 2 the relationship between travel time out flow and storage coefficient can be obtained by substituting equation 4 into equation 3 5 o 2 2 δ t 2 t t δ t i a v e 1 2 δ t 2 t t δ t o 1 if we assign the storage coefficient sc as s c 2 δ t 2 t t δ t equation 5 can be simplified as 6 o 2 s c i a v e 1 s c o 1 combined with equations 3 and 4 the following equations can be obtained 7 o 2 s c i a v e v s t o r e d 1 δ t multiply both sides of equation 7 with the time step δ t the following equation is attained 8 v o u t s c v i n v s t o r e d 1 from equation 8 sc must satisfy s c 1 to ensure the v s t o r e d 1 to be physically meaningful the derivation of the vsc method clearly shows that the sc depends on δt or the routing time step in addition vsc assumes inflow and outflow rates are approximately equal in the calculation of travel time tt which is also time dependent notably despite the wide use of the vsc method in previous swat applications the effects of routing time step on its performance have not been examined 2 3 model setup and evaluation the swat model requires spatially explicit information on the climate soils and land use for the study watersheds the soil map was from the usda natural resources conservation service nrcs soil survey geographic database ssurgo topography data was obtained by resampling a 1 m resolution light detection and ranging lidar based digital elevation model to 10 m using nearest neighbor interpolation the land use map and the scheduling of crop rotations were generated using 2008 2012 data from the usda national agricultural statistics service nass cropland data layer cdl daily precipitation and temperature were downloaded from the national oceanic atmospheric administration noaa national climate data center ncdc at chestertown and royal oak usc00181750 and usc00187806 respectively daily solar radiation relative humidity and wind speed were generated by the weather generator wxgen within the swat model for more information regarding swat model setup for these two watersheds please refer to lee et al 2016 2 4 model comparison and evaluation between different time steps we modified the original vsc method to be able to run at flexible time steps δt from 1 min to 1 day and assessed model outputs of stream discharge water storage and water depth for those time steps 1 day we aggregated the model outputs to a daily time step to evaluate and compare the simulation results for the land phase hydrologic processes we discretized the daily water yield into different time steps assuming a uniform distribution as accurately observed precipitation and other climate data are not available at a sub daily time step in our study region this treatment also helps eliminate the differences in terrestrial hydrologic processes which may affect simulation of riverine hydrologic processes we evaluated the model performance of daily stream discharge against usgs observations using nash sutcliffe efficiency nse nash and sutcliffe 1970 and the percent bias pbias nse ranges from to 1 with higher values indicating better performance pbias assesses overall model bias over the entire simulation period with small values indicating low bias note that our model comparison and evaluation were performed without calibrating swat parameters for two major reasons first the swat model was originally developed to assess hydrology and water quality in large scale ungauged watersheds arnold et al 1998 recently zhang et al 2013a 2013b and arnold et al 2015 both highlighted the importance of improving representation of physical processes given that parameter calibration in many cases cannot guarantee high fidelity of modeling results second optimization algorithms have difficulty identifying globally optimal parameter solutions for complex hydrologic models like swat zhang et al 2009 therefore parameter calibration will likely introduce additional uncertainties to model comparison between different time steps we also calculated the dci to quantitatively assess the diversification of ecologic indications under different river routing temporal resolutions the dci is widely used to assess continuity and connectivity of a river network and estimate the probability that fish can move between any two points in a river network cote et al 2009 jaeger et al 2014 for example jaeger et al 2014 found that climate change will alter hydrologic connectivity and damage endemic fishes in the verde river basin united states based on swat simulated hydrographs at the daily time step specifically the dci is calculated based on the connectivity of any two discrete segments of a river network which is determined by the passability of fish movement between the two river segments 9 d c i i 1 n j 1 n l i l l j l c i j 100 where n is the total number of river segments within the network l is the total length of all the river segments c ij is the connectivity between segments i and j with respective lengths of l i l j the dci has a value range between 0 and 100 with 0 denotes extremely low habitat connectivity and 100 represents the highest habitat connectivity and fish movement passability in the whole dendritic river network the calculation of c ij is based on the number and length of zero flow segments or the density of the barriers which physically denote spatially discontinuous wet reaches in this study each river reach is considered as a segment and the barriers or the zero flow conditions are defined as stream water storage less than 0 1 m3 assuming the passabilities of upstream and downstream reaches are equal for a given pair of reaches c ij can be calculated as 10 c i j k 1 k p k u p k d k 1 k p k 2 where k is the total number of dry barriers between a given pair of reaches i and j with individual barriers marked with k p k u and p k d are the upstream and downstream passabilities through barrier k and has an equal value of p k in this analysis we simplify the calculation of p k using a binary threshold method which assuming reaches with zero daily water storage storage 0 1 m3 has a zero chance passability p k 0 any wet reaches storage 0 1 m3 have a passability of 1 p k 1 3 results and discussions 3 1 stream discharge simulated at different time steps the average daily stream discharges simulated for the cross section of each reach along the mainstems fig 1 of the two experimental watersheds are plotted for each month and different time steps in fig 2 visual inspection shows that spatial and temporal patterns of simulated stream discharge are similar between different river routing time steps as expected downstream reaches that drain a larger area exhibit higher stream discharge relative to upstream reaches for the tcw the peak discharges occurred during march june with the highest averaged daily discharge of 5 85 0 01 m3 s 1 mean standard deviation in june at the catchment outlet similarly the peak stream discharges in the gbw also occur in june with a daily average of 7 92 0 01 m3 s 1 mean standard deviation at the catchment outlet for both watersheds different swat simulations show nearly identical daily stream discharge under different temporal resolutions the nse values obtained in the tcw show minimal variations over different time steps with an average value of 0 287 and a standard deviation value of 0 002 as shown in fig 3 for the gbw smaller temporal resolutions lead to slightly better model performances or higher nse values the mean of nse values over different time steps is 1 635 with a standard deviation value of 0 166 the pbias values obtained with different river touring time steps also are not much different from each other in the two watersheds for example the mean pbias is 43 62 0 01 mean standard deviation and 32 00 0 01 respectively for tcw and gbw note that nse values obtained in both tcw and gbw are negative and pbias indicates relatively large bias this is because we did not calibrate parameters of the swat model to match observed daily streamflow thus ensuring a fair comparison among model simulations at different temporal resolutions 3 2 stream water storage estimated at different river routing time steps the monthly average daily stream water storage over the period 2001 2018 for reaches along the mainstem fig 1 of the two watersheds are plotted in fig 4 in contrast to the stream discharge the stream water storages simulated with different river routing time steps are dramatically different from each other in both watersheds fig 4 in general higher temporal resolutions attained greater water storage in reaches notably the water storage in the river networks was nearly zero with river routing time steps of 4 hr 12 hr and 1 day indicating that the water entering a reach drain away instantly in addition seasonal variation and longitudinal distribution of water storage was almost undetectable for these larger time steps when the time step became smaller the seasonal and spatial distribution of water storage for the mainstem became clearer it is worth noting that with the change from 1 h to 1 min swat simulated an increase of mean water storage from 9500 to 18900 m3 for tcw and from 15000 m3 to 27000 m3 for gbw in the reach adjacent to the outlet respectively these results indicate that model simulated water storage and depth are sensitive to changes in river routing time steps when the model is executed at finer time resolutions e g 1 min to 1 hr 3 3 stream water depth estimated at different river routing time steps the daily stream water depth averaged for each month over the period 2001 2018 for reaches along the mainstem fig 1 of the two watersheds are plotted in fig 5 like stream water storage stream water depth also varied greatly over different river routing time steps in both tcw and gbw fig 5 similar to water storage seasonal variation and longitudinal distribution of water depth was almost undetectable for these larger time steps of 4 hr 12 hr and 1 day and when the time step became smaller the seasonal and spatial distribution of water depth for the mainstem became clearer this is not a surprise as the volume of water storage in reach determines water depth stream depth determines stream heat capacity which directly influences the stream temperature dynamics macdonald et al 2014 qiu et al 2019 2020 meanwhile stream depth is also closely related to water quality modeling for example stream depth is an important variable influencing the nitrate uptake rate mulholland et al 2008 2009 and gas aeration sorrell and tanner 2000 moreover stream depth and water storage determine the hydrologic connectivity which is an important indicator of stream health cote et al 2009 woznicki et al 2016 the strong variations of simulated stream depth results indicate that uncertainties could derive from using coarse river routing temporal resolutions when swat is applied to simulate water temperature and water quality processes 3 4 spatial assessment of different hydrologic variables simulated with different river routing time steps here we present spatial maps showing the average hydrologic variables to further illustrate the implications of choosing different river routing time steps for hydrologic modeling and assessment the spatial maps of daily stream discharge averaged over 2001 2018 are shown in fig 6 the stream discharge maps show nearly identical distributions under different time steps for both watersheds which is consistent with the previous temporal comparisons the spatial maps of averaged daily stream water storage fig 7 and stream water depth fig 8 show large differences between different river routing time steps in general when routing time steps are equal to or less than 1 hr stream water storage begins to accumulate while all reaches are nearly dry for routing time steps larger than 4 h we went on to calculate zero flow days per year for each reach fig 9 based on the criterion that if water storage in a reach is less than 0 1 m3 it is determined as dry we observed more zero flow days with larger time steps for example the annual zero flow days for the two watersheds are more than 200 days across the whole stream network when the time steps are 12 hr and 1 day when routing time steps are equal to or less than 1 h the number of simulated zero flow days is minimal and spatial patterns are like each other 3 5 implications of river routing time steps for hydrologic connectivity assessment as shown in fig 10 the simulated monthly dci decreases as the river routing temporal resolution increases in both watersheds which is expected since the annual zero flow days increase with the temporal resolution as discussed above for both experimental watersheds the monthly dci ranges from nearly 0 under the time step of 1 day to 100 under the time step of 1 min during the entire simulation period this indicates incompatible connectivity of the river network from results under different temporal resolutions overall dci time series derived for the time steps from 30 min to 12 h missed the year round high hydrologic connectivity in the two watersheds this may be because shorter time steps e g 30 min can simulate the subtle changes that cannot not be captured at a larger time step e g a day rapidly changing climate is continuously impacting the surface water and groundwater storage which are poised to threaten the habitats of aquatic species gober and kirkwood 2010 lake 2011 in addition increasing water demand due to population growth boretti and rosa 2019 accelerated human disturbances such as deforestation peña arancibia et al 2019 are simultaneously exerting pressure on the resilience of aquatic ecosystems hydrologic models including the swat model are often used to project hydrologic connectivity of freshwater ecosystems and understand associated impacts on species persistence ecosystem integrity and human well being jaeger et al 2014 the dramatic differences between the zero flows and dci derived with different time steps figs 9 and 10 clearly illustrates the importance of selecting appropriate river routing time steps for reliable assessment of ecological impacts of future changes to regional hydrology 3 6 analysis of storage coefficient derived with different routing time steps the constraint of sc s c 1 in eq 8 δ t 2 t t must be satisfied to ensure the outflow does not deplete the volume of inflow plus the storage in other words if δ t is larger than 2 tt the simulated storage volume may be negative in real world applications especially when simulating short reaches with relatively short travel times this constraint may not be satisfied to prevent this swat constrained sc to be equal to 1 when its calculated value is larger than 1 the sc values are close or equal to 1 for both experimental watersheds when δ t reaches 4 h and larger fig 11 however the simulated storage volume can still approach zero in this setup as can be seen from eq 8 especially in short reaches with relatively small tt for most reaches of the two experimental watersheds with the longest reach length of 7875 m for tcw and 10328 m for gbw using time steps longer than 4 h could not guarantee δ t 2 t t or s c 1 as a result the simulated water storage volume and water depth may approach zero as discussed in section 3 2 3 4 overall the vsc method within the swat model can provide consistent stream discharge results regardless of temporal resolutions however when hydrological variables such as stream water storage and depth are important in swat applications we recommend using at least 1 hr time step for streamflow routing when using the vsc method while recognizing that the appropriate time steps depend on actual watershed size and configuration 3 7 recommendations for more comprehensive evaluation of hydrologic variables reconciling the balance between model efficiency and fidelity remains a big challenge in developing dpbhms beven 1993 2000 to pursue computational efficiency model simplifications usually focus on the dominant physical processes while overlooking some relevant coevolving processes as detailed river channel geometry and field observations of river water storage and depth data are not available in the two experimental watersheds we could not directly assess and compare the model performance with different routing time steps based on streamflow simulation results fig 3 we contend that finer routing time steps help improve model performance particularly for water storage and depth simulations note the model performance of riverine processes could be influenced by errors and uncertainties associated with model inputs e g climate forcing land use and soil types and other hydrologic processes e g evapotranspiration soil moisture and groundwater therefore we recommend conducting numerical experiments to examine the model simulations of stream discharge water storage and depth with different routing time steps the general model evaluation procedures widely used for the swat model moriasi et al 2007 santhi et al 2001 consider only streamflow without explicit assessment of water storage and depth in streams not only are hydrologic connectivity and aquatic ecosystem health assessment sensitive to river routing time steps section 3 5 but also are sediment transport and deposition stream thermal dynamics and nutrient cycles luque and beek 1976 macdonald et al 2014 qiu et al 2019 2020 given the high sensitivity of river water storage and depth to river routing time steps we contend that those two hydrologic variables should be included in the model assessment procedures fig 12 if reliable data of channel geometry and observations of the three riverine hydrologic variables are available quantitative assessment using widely accepted statistical metrics such as nse and pbias moriasi et al 2007 can be applied to choose the appropriate time steps by balancing efficiency and model accuracy when those data are not available we suggest either choose the finest time steps that are affordable with available computational resources or compare the spatial and temporal patterns of simulated variables as discussed in sections 3 1 3 2 and 3 3 and ensure the selected times steps do not miss important characteristics of the hydrologic systems for example in the tcw and gbw although we do not have reliable water storage and depth data our field experience in the two watersheds does show that the rivers are perennial instead of ephemeral therefore we need to select a time step 1 hr or less that at least shows the rivers have water for most days of the year 4 conclusions in this study we examined the effects of routing time steps on streamflow water storage and depth simulations with the widely used vsc method within the swat model the results show that there exist nonignorable uncertainties in the simulated riverine hydrologic variables stemming from variations in routing temporal resolution although strong agreement was reached among stream discharge simulations with different temporal resolutions the simulated stream water storage and depth are irreconcilable swat with coarser time steps i e 4 hr 12 hr and 1 day drains nearly all flows entering a reach leading to large number of zero flow days minimal water storage and depth during a year and low hydrologic connectivity as indicated by the low dci values in the river networks in contrast with finer river routing time steps e g 1 min 30 min and 1 hr swat simulates persistent water storage in the river networks which is consistent with perception of the watershed via many years of field experimental experience in addition finer routing time steps in general slightly improve stream discharge simulations as evaluated against long term observed daily streamflow data in the two experimental watersheds overall our numerical analysis shows high sensitivity of simulations of stream water storage and depth to river routing time steps and the consequent significant implications for hydrologic connectivity and aquatic health assessment therefore we recommend in addition to streamflow including stream water storage and depth into the general procedures that are widely used for watershed model evaluation particularly when the application purposes are beyond streamflow prediction although our experiments suggest a routing time step less than 1 hr should be adopted we recognize that model performance is dependent on both routing time steps and reach length i e actual watershed size and configuration that vary greatly across study regions therefore we could not specify a standard threshold for appropriate routing time steps instead general guidelines are recommended when detailed channel geometry and reliable observations of water storage and depth data are available we suggest conducting quantitative assessment using statistical metrics when those data are not available we suggest examining the spatial and temporal patterns of simulated river hydrologic variables and choose a time step that match the experiential perception of the hydrologic conditions in the watersheds in addition as using finer time steps e g 1 min could greatly increase the computation cost both storage space and computational time the balance between model efficiency and accuracy should also be factored in we anticipate the findings derived from this study help future watershed model development and application to support watershed management and assessment software revised swat 2012 operating systems windows linux dependent software fortran 90 availability free of charge the revised swat 2012 with flexible river routing time steps is available by contacting the authors the code will also be published in future releases of swat 2012 through the swat website editorial conflict of interest statement given his role as environmental modelling software editorial board member xuesong zhang was not involved in the peer review of this article and has no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to journal editor daniel p ames declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was supported in part by the u s department of agriculture agricultural research service as a contribution from the long term agroecosystem research ltar network the funding support for this project was provided by national aeronautics and space administration nnx17ae66gand 18 cms18 0052 and the u s department of agriculture 2017 67003 26484 funding was also provided in part by the u s department of agriculture natural resources conservation service conservation effects assessment project nrcs ceap drs min chen and han qiu were also partly supported by national aeronautics and space administration 80hqtr19t0055 
25714,the paper analyses how the water energy food nexus is treated in computable general equilibrium cge models discussing their design importance and possible ways of improvement the analysis of their structure is critical for evaluating their potential efficiency in understanding the nexus which will be particularly effective for gauging the importance of the topic the reciprocal dependency of its elements and the expected macroeconomic demographic and climatic pressures that will act on its components general equilibrium models can be useful devices to this end as they are specifically built to track interdependencies and transmission effects across sectors and countries nevertheless the review showed that most cges in the literature struggle to represent the competing water uses across sectors and in particular those concerning the energy sector therefore it highlights the need to resolve this issue as a necessary step toward improving future research keywords water energy food nexus computable general equilibrium model economic modelling 1 introduction the water energy food wef 1 1 abbreviations wef water energy food cge computable general equilibrium nexus is a topic that has recently gained unprecedented attention in the academic environment since 2011 the year in which hoff and the world economic forum introduced the wef concept it has become increasingly influential for both research and policy its importance derives from the fact that water energy and food are three fundamental human needs and therefore are critical for social development and well being raworth 2017 moreover understanding the interconnections between the three elements of the nexus will be fundamental in light of the increasing macroeconomic demographic and climatic pressures expected in the short and mid term future aboelnga et al 2018 alexandratos and bruinsma 2012 allouche et al 2014 beddington 2010 nelson et al 2010 wiebe et al 2015 the wef nexus is multifaced but primarily marked by two concepts the first is the idea of constraint and limitations the items of the nexus are nature based and so connected to the risks of resource exhaustion and overexploitation al saidi and elagib 2017 beck and villarroel walker 2013 lotze campen et al 2008 lundqvist and unver 2018 zhang et al 2018 as well as to consequent security issues bakker 2012 giupponi and gain 2017 zhang et al 2019 the second fundamental aspect of the nexus is the conundrum relative to interconnections and linkages these ideas are the very basis of the nexus as it is a concept mainly built upon the finding that historically the single sectoral perspectives on resource management adopted by the policy makers have turned out to be ineffective and inefficient bazilian et al 2011 bieber et al 2018 el gafy et al 2017 rasul 2016 as the recent literature points out this approach has often led to a decline in well being and to unsustainable development strategies bazilian et al 2011 bizikova et al 2013 both could be avoided by adopting an integrated perspective systemic thinking makes possible a better understanding of trends pressures on the use of resources and how to better assess the direct and cascading effects of different policies flammini et al 2013 the pervasive nature of interconnections becomes particularly evident by looking at the schematic structure of the nexus as shown in fig 1 first there are direct interconnections water is linked to food through agriculture production food processing livestock management and cooking water is also used for energy production through electricity and fuel extraction food can be used as an energy source in the form of biofuels energy is consumed by the food sector in the form of fertilizers food processing transport cooking and storage and can be used to extract distribute and process water desalinization water convection water treatment garcia and you 2016 smajgl et al 2016 among direct connections it is worth noting that all of the nexus components also entail some degree of self consumption e g fodder for livestock or energy for energy production then there are external elements or indirect drivers of the nexus zhang et al 2018 these can be classified according to type and temporal dimension and can be divided into four main categories chronic social factors user behaviours population growth economic conditions urbanization acute social factors sabotage riots wars politics technical innovation chronic physical factors climate change resource depletion infrastructures land use and acute physical factors pollution incidents extreme weather events natural hazards all these factors are important for the nexus and should be incorporated in any model aiming to realistically represent the processes trends and feedbacks involved therefore this review will focus on analysing the strategies and methods for integrating the nexus into computable general equilibrium models which are considered particularly promising tools for this aim the paper is structured as follows section 2 presents the rationale of the review section 3 introduces the criteria guiding the paper collection section 4 discusses the development of nexus modelling in cge section 5 discusses possible ways forward to overcome the main limitations that have emerged section 6 draws conclusions 2 rationale of the review cge models and their role in the literature wef relationships are intrinsically complex and have been modelled in several ways aboelnga et al 2018 al riffai et al 2017 albrecht et al 2018 dai et al 2018 khan et al 2017 li et al 2019 namany et al 2019 in general the choice of the modelling approach type depends upon research scale priorities and level of the interdependency that the study wants to consider zhang et al 2018 while there are some methodologies that could be applied with some flexibility to different spatial scales e g integrated indexes statistics or system dynamics the general rule of thumb is to address wider dimensions with aggregated methods such as econometric analysis or macro economic approaches conversely more micro oriented methods such as agent based models can be more effectively applied to small scale questions also in the light of their significant data requirement for example if the aim is to observe the impact of a policy developed at a national level or the effects on trade patterns a macro approach like such as general equilibrium ge macro econometric me or system dynamic sd 2 2 computable general equilibrium identifies models that are a solved numerically computable b concern the whole economy general and c are based on the idea of equilibrium which is the optimisation of the agent s behavior equating demand and supply in a macroeconomically balanced framework burfisher 2016 system dynamic models are complexity oriented tools that aim to address nonlinear behavior of complex systems utilizing concepts such as stocks flows and feedbacks randers 1980 can be more appropriate if the objective is to analyse the emergence of consumption patterns a more micro approach such as agent based modelling or micro econometric analysis could be a better choice among the modelling techniques the interest of this review falls on cge models they present many features that are potentially useful for a nexus analysis such as its ability to capture input output linkages between sectors and countries its effectiveness in representing impacts of technical changes e g changing the productivity of a factor or policy interventions and its ability to account for macro economic feedbacks carrera et al 2015 y zhou et al 2018b cges can therefore provide an aggregated or top down representation of the economy while maintaining a relative simplicity of interpretation and to some extent avoiding the pitfalls of complexity frequently connected to the use of other approaches such as system dynamic models in particular as tools that are able to assess the propagation of the magnitude and direction of disturbances in complex circumstances e g global scale economic issues allan et al 2007 zisopoulou et al 2018 they are effective in linking the different elements of the nexus and in discussing resource competition across different sectors and countries dudu et al 2018 nechifor and winning 2017 however it is important to recognise that cges also have some limitations as several authors have extensively described farmer et al 2015 mercure et al 2016 pindyck 2017 2013 first of all they are normative and optimisation based tools their theoretical underpinning which strives toward for optimisation can lead in facilitating mathematical tractability to the exclusion of increasing returns and self reinforcing phenomena secondly they consider actors as rational and homogeneous representative agents but agent heterogeneity is an important feature that needs to be addressed further while their country sectoral level of aggregation can provide a full economy overview of the consequence of various shocks it is not able to account for local structures or intra sectoral intricacies al riffai et al 2017 willenbockel et al 2016 cge models are also heavily dependent on calibration data their parameterization is based upon observation of the economic system at a given time accordingly they can lose predictive power as their simulations are pushed too far from the starting point they are rarely tested against their predictive power to check for the consistency of their projections allan et al 2007 another common problem is related to the ability of cge to treat economic issues with the appropriate spatial and temporal scale economic phenomena often take place at the sub national and interannual scale while cge work at a generally country yearly attributed scale shannak et al 2018 finally a challenge for cge is the modelling of technological changes farmer et al 2015 hertel and liu 2019 the common representation of technology shifts in cge models is developed by altering the constant elasticity of substitution ces functions baum et al 2016 blignaut and van heerden 2009 and or the exogenous productivity factors on the one hand this simplifies the mathematical and computational structure of the model on the other with this structure technology becomes static or at best exogenous which is quite unrealistic to sum up the main criticism against cges is that they are based on rigid and idealized economic assumptions allan et al 2007 bataille and melton 2017 howarth and monasterolo 2017 this can be a problem for nexus analysis given the important role played especially by technological factors in energy water and food production and efficiency nevertheless computable general equilibrium cge models can still be considered useful tools to address the nexus schlör et al 2021 they are suitable for detecting impacts between sectors and on the whole economy which perfectly fits in with the characteristic of a nexus investigation willenbockel et al 2016 furthermore they are still central in the economic literature blanchard 2016 hence the aim of this review is to present the main technical features of nexus modelling by focusing on cge models evaluating their strengths weaknesses and blind spots and to highlight a possible line of research to overcome the major shortcomings identified 3 methodology of the review process the review started with a general research around the concept of nexus nexus modelling and specifically the role of cges the collection of the publications regarding the application of cge modelling of wef nexus has been developed in a systematic way by following the steps illustrated in fig 2 first of all a search in the elsevier scopus elsevier sciencedirect and web of science database has been performed the keywords used were water and energy and food and nexus and cge the results were 4 for scopus and 103 articles for sciencedirect and 5 for web of science as a complement to this first screening removing the papers not directly involving cge modelling a google scholar citation tracing forward and backward has been implemented afterwards a further search has been conducted that focuses on cge modelling in relation with the three sub components of the nexus this review on scopus was run with the keywords food and energy and cge food and water and cge and water and energy and cge originating respectively in 43 37 and 71 papers on scopus title abstract and keyword restriction 17 11 40 on sciencedirect title abstract and keyword restriction and 78 67 114 on web of science filtered for topic after the google scholar citation tracing forward and backward the screening of the papers and the cleaning of the double counted references included the articles from the previous step a total of 583 papers were identified reviews of nexus modelling conceptual discussion of the framework and historical development and resource modelling with cges were also collected adding a total of 314 papers to the screening after abstract and keyword screening and the removal of the papers not relevant for a contextualization of the concept of nexus and or not relevant at list partially to cge modelling a total of 407 were collected for textual analysis with the irrelevant papers discarded the review proceeded to a qualitative analysis of the wef in cges for a total of 127 studies of these 90 of them were used for quantitative analysis 4 results wef nexus and cge modelling 4 1 first link water energy and cges around 98 of the world s electricity production uses water van vliet et al 2016 in a different share according to the type of generation energy production can employ water directly e g in the case of hydro power or indirectly e g in thermo electric production where water is used for cooling van vliet et al 2016 q zhou et al 2018a both uses need to be considered for a proper representation of the energy water link table 1 highlights that only four studies found in this review directly integrate water as an energy factor of production in the cge structure sun et al 2021b 2020 taheripour et al 2020 teotónio et al 2020 most of the studies account for it externally e g by establishing links with hydrological models but without back feeding quantifying variation of water demand as a proportion of energy production changes or using the fictitious instrument of pricing through taxation su et al 2019 q zhou et al 2018a y zhou et al 2018b zhou et al 2016 the latter in particular is a method that sets water fees proportional to the sectorial water withdrawal in order to account for the effects of different water costs fan et al 2018 from the review what also emerges is that water uses generally refer to freshwater withdrawal with just two studies explicitly addressing water uses for cooling q zhou et al 2018a zhou et al 2016 none of the studies accounts for the overall nexus or water competition implications finally with the sole exception of q zhou et al 2018a they are country level focusing mostly on china and showing the lack of global analyses their main conclusion is that regional energy production can indeed be affected negatively by climate change through water scarcity extreme events and the relative seasonal variations some hot spots e g north africa and middle east emerge as high risk regions and this topic arose as a most urgent research field for the near future 4 2 second link energy food and cges the relationship between energy and food in the literature is complex and multifaceted nevertheless it is easy to identify a preponderant cluster developed around the biofuel issue 3 3 indeed the connection between food and energy is much broader than the biofuel issue the agri food sector consumes around 30 of global energy of which 70 is absorbed directly by crop production without considering transport or processing fao 2012 or the role of fertilizers vlek et al 2004 which are highly energy intensive inputs but the representation of energy demand originated by agriculture towards energy producing sectors is part of the standard representation of sectoral exchanges in cge models typically cges focus on biofuels for two main reasons their potential though controversial contribution to emission reduction timilsina et al 2010 and their effects on agricultural price volatility and food security the latter has attracted increasing attention after the 2008 food price crisis in which biofuels played a critical role mitchell 2008 on the contrary other important impacts of biofuel use such as those on biodiversity and water are much less frequently investigated with cges table 2 in these analyses although important the role of water as the third element of the nexus has been explicitly treated by very few contributions the first one is ge lei and tokunaga ge et al 2014 who introduce water costs in their input output table of the non grain fuel ethanol sector their main aim however is not to investigate water consumption but to understand the effects of a possible expansion of ethanol production on food security and land use in a subsequent paper ge and lei ge and lei 2017 model water as an economic sector this is an improvement with respect to ge et al 2014 as it enriches the description of water interactions with all sectors but it still cannot represent the role of water in the production process similarly kaenchan et al kaenchan et al 2019 use water requirement statistics and define a water demand function to analyse scenarios for the sustainability of bioethanol production in thailand but in none of these studies is the water input embedded directly in the firm s production function this is done in the gtap w bio model haqiqi 2016 taheripour et al 2020 2013a 2007 developed on the basis of the global trade analysis project gtap model gtap initiated in 1997 hertel 1997 made several improvements that increased its ability to tackle nexus connections these included an improved energy sector representation in its gtap e burniaux and truong 2002 mcdougall and golub 2007 and gtap p peters 2016 versions which accounted for biofuels in gtap bio birur et al 2008 taheripour et al 2007 biofuels are introduced by disaggregating the energy capital bundle and the crop sector to capture crop wide competition for food and crop fuelled energy as well as the relative price and trade effects in agricultural markets birur et al 2008 further developments included the introduction of agro ecological zones aez disaggregate land types to better represent the trade offs between different land uses in the gtap bio aez model version birur et al 2008 taheripour et al 2013b and the full linking of ghg emissions data from gtap e to the aezs in the gtap bio adv model version tyner et al 2013 finally particularly relevant for this survey gtap introduced water among the endowments primary production factors in its gtap w bio version taheripour et al 2013a the next section deals with water modelling in gtap w bio tin relation to the water food nexus although developed in a model addressing biofuel use it refers to the use of water for agricultural production and thus fits more consistently there gtap bio w has been used to address complex phenomena such as irrigation induced land use changes stemming from biofuel production taheripour et al 2013b the economic and resource impacts of crop switching or the effects of technological changes e g irrigation implementation haqiqi 2016 nevertheless the only paper that focuses explicitly on the overall economic impacts and trade offs between energy food and water is qu et al 2021 which while it openly addresses the issue with a nexus perspective still lacks the explicit representation of the third element of the wef to achieve a broader perspective 4 3 third link water food and cges the connections between water and food are obvious and highly significant 4 4 water influences food production both directly i e input with proportional output and indirectly i e ineffective water drainage infrastructures can lead to a decrease in the salinization of the land or to waterlogging which could lower crop yields foley et al 2011 gleick 2003 their relationship is also quantitatively significant as agriculture is responsible for roughly 70 of water withdrawals fao 2003 and accounts for around 86 of the world freshwater resource consumption foley et al 2011 shiklomanov 1998 the water food link has also had a complex relationship with climate change with multiplier dynamics acting on both sides bell et al 2014 temperature variations and extreme events are expected to influence future crop productivity nelson et al 2018 2010 wheeler and von braun 2013 and water availability ipcc 2020 2019 2014 2008 with water scarcity increasingly pressuring countries to rely on trade in order to ensure food security fao 2003 moreover climate change will be particularly important for rainfed crops which nowadays account for 80 of the cultivated land and 60 of the world s food supply fao 2015 rockstrom et al 2007 even though irrigation practices are expanding rosegrant et al 2002 climate change will therefore be a critical driver of food production and water management issues fao 2003 the cge analyses linking water and food are concentrated around the topics of socio economic impacts on the agricultural sector al riffai et al 2017 the effects of water policies such as water pricing or quantity restrictions on irrigation calzadilla et al 2011 johansson et al 2002 and agricultural price determination this research field has sometimes been expanded to investigate technological shifts decaluwé et al 1998 there are mainly two approaches to introducing water into cge models see table 3 one is to introduce it indirectly or externally while the other includes water as an explicit production factor a typical indirect approach to modelling water is to use changes in land productivity as a proxy of water supply variations which would then influence the final demand supply and price of agricultural commodities recently dudu ferrari and sartori dudu et al 2018 tried to add more breadth to this type of water modelling they suggest using a ces shifter i e sectorial specifically calibrated total factor productivity to adjust the factor productivity of different sectors in the production function this would make it possible to capture water availability impacts on productivity also in non agricultural sectors enabling a more comprehensive assessment of production constraints and inefficiencies modifications in crop productivity in response to water supply changes that entail changes in production costs have also been used to set a fictional cost for water in an attempt to overcome one of the typical issues in water modelling with cge that of the zero price of the resource see horridge et al 2005 and below alternatively alterations in land productivity have been used for the same purpose letsoalo et al 2007 the models that directly embed water as a factor in the production function of the firms assume no substitution leontief functions as in berrittella et al 2007 van heerden et al 2008 or a partial substitution ces function as in calzadilla et al 2011 darwin 2004 across water and the other primary inputs cge models represent the values of exchanges accordingly defining the value of the water input as essential this is also a particularly thorny issue as water is often a free or at best under priced resource one method used is to disentangle water value from the land value by using as a criterion the price differential across irrigated and non irrigated crops and attributing the difference to the contribution of water since 2013 shortly after the introduction of the nexus concept some studies appeared that included water as a production factor also for non agricultural sectors nevertheless their presence in the literature is still limited dermody et al 2018 li et al 2018 nielsen et al 2016 and they tackle non agricultural water uses only in an aggregated way koopman et al 2017 liu et al 2019 luckmann et al 2016 roson and damania 2017 taheripour et al 2020 2018 a partial exception is the rescu water model by nechifor and winning nechifor and winning 2018 which starting from the gtap e structure burniaux and truong 2002 and gtap p peters 2016 specifies and disentangles water consumption for the energy sector it is finally worth stressing that independently upon the direct or indirect introduction of water into the models cges study of water issues often entails coupling economic models with physical hydrologic models e g al riffai et al 2017 this practice is meant to exploit the best available information of physical changes in resource availability and to increase precision and reliability of the simulations nevertheless it still presents some limits such as the difficulty to test convergence across different model outputs parrado et al 2019 and to match the different spatial temporal scales bell et al 2014 across different model typologies accordingly the development and improvement of multiple scale modelling remains an important research practice and objective al riffai et al 2017 dermody et al 2018 li et al 2018 5 discussion challenges and paths forward for cge nexus modelling the review highlighted that the integrated nexus analysis is challenging there are objective difficulties in modelling the complex interdependencies involved and in gathering good quality data indeed among all the studies surveyed only one taheripour et al 2020 addresses the nexus nodes explicitly and in their entirety it also emerged that the most problematic issue in addressing the wef nexus with cge models is the realistic representation of the water dimension fig 3 in fact only one third of the studies in this review implemented water as an explicit produc 5 taheripour et al 2020 is reported in both the energy food and water food as this framework is built explicitly to address both issues i e gtap w bio tion factor with most of them concentrating on the analysis of the water food implications for agriculture while neglecting the other sectors an explicit representation of water as an input for energy production is provided only by four studies table 3 moreover the survey emphasized that the water energy nexus is underrepresented compared to other nexus nodes thus a first conclusion is already possible there is the need and room for improvement in the explicit introduction of water in cges in general and particularly in the energy sector the review process also highlighted the following modelling issues water pricing usually water is an almost free public good with virtually no price in the standard cge structure with ces demand functions this would translate into infinite consumption dudu et al 2018 moreover even when a water pricing structure can be identified it is usually sector specific and does not respond to competitive market mechanisms since water is ultimately allocated by institution and policy considerations outside the market forces olmstead 2014 this is an evident problem for models like cges founded as they are on market transactions data availability drews et al 2019 there is a lack of clear information on the different water uses and availability of water types i e surface or groundwater distribution through irrigation or rain across sectors this unavoidably leads to weak model calibration which translates into weak model prescriptions and reliability spatial and time scale problems raised by water modelling de fraiture 2007 harou et al 2009 li et al 2018 maneta et al 2009 water resources are often trans boundary while the typical investigation scale of cge models is the country level crops vary seasonally and water availability and flows change hourly while economic analyses with cge refer to yearly time frames shannak et al 2018 baum et al baum et al 2016 for example highlight this issue by studying the water use salinity changes in food production in israel in this study the factors change according to different temporal and spatial scales e g field national level creating a mismatch in the profitability of the crops at the regional level and sub optimal allocation of land and water among crops and sub regions thus the first task for achieving a better nexus representation and understanding is to address these shortcomings the literature also proposes further suggestions for increasing the realism of water modelling hertel and liu 2019 the first is to account for the potential losses from evapotranspiration variation 6 6 evapotranspiration is the quantity of water that passes from land and plants to the atmosphere therefore not going to contribute directly to groundwater recharge it is an important factor contributing to water availability reduction this can be done in two ways a econometrically on the basis of existing data on how water availability is affected by different facilities technologies or b as in mekonnen and hoekstra mekonnen and hoekstra 2011 where it is computed endogenously by using the penman monteith equation 7 7 the penman monteith equation is the standard method used by the united nations food and agriculture organization to model evapotranspiration it is based on daily mean temperature wind speed humidity and solar radiation a specific issue that also deserves more attention is the buffering time for groundwater this factor which affects the ability of the environment to recharge its water supply could greatly decrease water availability and should be considered in order to achieve a more realistic representation of water dynamics diao et al 2008 in nexus terms it would be important to explicitly introduce the consumption feedbacks between water and energy hertel and liu hertel and liu 2019 suggest switching from bundling water only with land in production functions to a nest with capital atkeson and kehoe 1999 doing so and estimating the cost of capital and water as well as their substitution would make it possible to endogenously address phenomena such as switches to higher water efficient technologies as water become scarcer as concerns biofuel modelling cges should systematically endogenize irrigation water calzadilla et al 2011 taheripour et al 2013a in order to capture competition between food crops and biofuels based also on water uses this procedure however is not mainstream practice in the literature an additional important and under investigated topic are dynamics in residential demand for water this is often disregarded because of the relatively low share of water involved but it actually has profound implications for well being conducting this investigation will be particularly difficult in developing countries that are affected by the joint presence of formal and informal water uses that mystify water quantities and pricing another aspect to consider in household water consumption is agent heterogeneity different classes of households whether rich poor or rural urban have very distinct and disparate water consumption patterns which could be exasperated with the development of demographic socio economic and environmental changes accordingly considering this heterogeneity is fundamental for addressing the nexus perspective a final advancement could be incorporating the trade off between environmental quality and consumption in household utility which in the specific nexus context translates into placing environmental water services in the household utility function tsigas et al 2001 6 conclusions cges models are used to describe the behaviours of rational economic agents markets institutions and the relationships across them their structure allows for the synchronous identification of endogenous interdependencies between sectors and for the detection of the impacts of exogenous factors on the full economy therefore they can be appropriate instruments for addressing a complex topic such as the wef nexus which inherently concerns multiple sectors and fluxes this said it appears that there are not many cges able to address the nexus in its entirety which limits the realism and robustness of both the modelling results and the related policy advice one of the biggest hurdles relates to the explicit modelling of the water input and its competitive uses across sectors and agents this is in turn determined by the difficulty of representing economically a zero price resource like water and by reliable data on water uses should we suggest some priority for action we would recommend improving data quality on water uses and prices in different countries and sectors and including water as a specific production factor for the different sectors in the economy starting from the energy one to then include household water consumption and expand the analysis at the multi country level author contributions conceptualization e b f b methodology e b writing original draft preparation e b supervision f b writing reviewing and editing f b funding this research did not receive any specific grant from funding agencies in the public commercial or non profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this paper has benefited from comments and suggestions of enrica de cian ca foscari university the ecip division of the euro mediterranean center on climate change venice italy and from karl steininger birgit bednar friedl and the whole econclim research group of the wegener center for climate and global change of graz austria 
25714,the paper analyses how the water energy food nexus is treated in computable general equilibrium cge models discussing their design importance and possible ways of improvement the analysis of their structure is critical for evaluating their potential efficiency in understanding the nexus which will be particularly effective for gauging the importance of the topic the reciprocal dependency of its elements and the expected macroeconomic demographic and climatic pressures that will act on its components general equilibrium models can be useful devices to this end as they are specifically built to track interdependencies and transmission effects across sectors and countries nevertheless the review showed that most cges in the literature struggle to represent the competing water uses across sectors and in particular those concerning the energy sector therefore it highlights the need to resolve this issue as a necessary step toward improving future research keywords water energy food nexus computable general equilibrium model economic modelling 1 introduction the water energy food wef 1 1 abbreviations wef water energy food cge computable general equilibrium nexus is a topic that has recently gained unprecedented attention in the academic environment since 2011 the year in which hoff and the world economic forum introduced the wef concept it has become increasingly influential for both research and policy its importance derives from the fact that water energy and food are three fundamental human needs and therefore are critical for social development and well being raworth 2017 moreover understanding the interconnections between the three elements of the nexus will be fundamental in light of the increasing macroeconomic demographic and climatic pressures expected in the short and mid term future aboelnga et al 2018 alexandratos and bruinsma 2012 allouche et al 2014 beddington 2010 nelson et al 2010 wiebe et al 2015 the wef nexus is multifaced but primarily marked by two concepts the first is the idea of constraint and limitations the items of the nexus are nature based and so connected to the risks of resource exhaustion and overexploitation al saidi and elagib 2017 beck and villarroel walker 2013 lotze campen et al 2008 lundqvist and unver 2018 zhang et al 2018 as well as to consequent security issues bakker 2012 giupponi and gain 2017 zhang et al 2019 the second fundamental aspect of the nexus is the conundrum relative to interconnections and linkages these ideas are the very basis of the nexus as it is a concept mainly built upon the finding that historically the single sectoral perspectives on resource management adopted by the policy makers have turned out to be ineffective and inefficient bazilian et al 2011 bieber et al 2018 el gafy et al 2017 rasul 2016 as the recent literature points out this approach has often led to a decline in well being and to unsustainable development strategies bazilian et al 2011 bizikova et al 2013 both could be avoided by adopting an integrated perspective systemic thinking makes possible a better understanding of trends pressures on the use of resources and how to better assess the direct and cascading effects of different policies flammini et al 2013 the pervasive nature of interconnections becomes particularly evident by looking at the schematic structure of the nexus as shown in fig 1 first there are direct interconnections water is linked to food through agriculture production food processing livestock management and cooking water is also used for energy production through electricity and fuel extraction food can be used as an energy source in the form of biofuels energy is consumed by the food sector in the form of fertilizers food processing transport cooking and storage and can be used to extract distribute and process water desalinization water convection water treatment garcia and you 2016 smajgl et al 2016 among direct connections it is worth noting that all of the nexus components also entail some degree of self consumption e g fodder for livestock or energy for energy production then there are external elements or indirect drivers of the nexus zhang et al 2018 these can be classified according to type and temporal dimension and can be divided into four main categories chronic social factors user behaviours population growth economic conditions urbanization acute social factors sabotage riots wars politics technical innovation chronic physical factors climate change resource depletion infrastructures land use and acute physical factors pollution incidents extreme weather events natural hazards all these factors are important for the nexus and should be incorporated in any model aiming to realistically represent the processes trends and feedbacks involved therefore this review will focus on analysing the strategies and methods for integrating the nexus into computable general equilibrium models which are considered particularly promising tools for this aim the paper is structured as follows section 2 presents the rationale of the review section 3 introduces the criteria guiding the paper collection section 4 discusses the development of nexus modelling in cge section 5 discusses possible ways forward to overcome the main limitations that have emerged section 6 draws conclusions 2 rationale of the review cge models and their role in the literature wef relationships are intrinsically complex and have been modelled in several ways aboelnga et al 2018 al riffai et al 2017 albrecht et al 2018 dai et al 2018 khan et al 2017 li et al 2019 namany et al 2019 in general the choice of the modelling approach type depends upon research scale priorities and level of the interdependency that the study wants to consider zhang et al 2018 while there are some methodologies that could be applied with some flexibility to different spatial scales e g integrated indexes statistics or system dynamics the general rule of thumb is to address wider dimensions with aggregated methods such as econometric analysis or macro economic approaches conversely more micro oriented methods such as agent based models can be more effectively applied to small scale questions also in the light of their significant data requirement for example if the aim is to observe the impact of a policy developed at a national level or the effects on trade patterns a macro approach like such as general equilibrium ge macro econometric me or system dynamic sd 2 2 computable general equilibrium identifies models that are a solved numerically computable b concern the whole economy general and c are based on the idea of equilibrium which is the optimisation of the agent s behavior equating demand and supply in a macroeconomically balanced framework burfisher 2016 system dynamic models are complexity oriented tools that aim to address nonlinear behavior of complex systems utilizing concepts such as stocks flows and feedbacks randers 1980 can be more appropriate if the objective is to analyse the emergence of consumption patterns a more micro approach such as agent based modelling or micro econometric analysis could be a better choice among the modelling techniques the interest of this review falls on cge models they present many features that are potentially useful for a nexus analysis such as its ability to capture input output linkages between sectors and countries its effectiveness in representing impacts of technical changes e g changing the productivity of a factor or policy interventions and its ability to account for macro economic feedbacks carrera et al 2015 y zhou et al 2018b cges can therefore provide an aggregated or top down representation of the economy while maintaining a relative simplicity of interpretation and to some extent avoiding the pitfalls of complexity frequently connected to the use of other approaches such as system dynamic models in particular as tools that are able to assess the propagation of the magnitude and direction of disturbances in complex circumstances e g global scale economic issues allan et al 2007 zisopoulou et al 2018 they are effective in linking the different elements of the nexus and in discussing resource competition across different sectors and countries dudu et al 2018 nechifor and winning 2017 however it is important to recognise that cges also have some limitations as several authors have extensively described farmer et al 2015 mercure et al 2016 pindyck 2017 2013 first of all they are normative and optimisation based tools their theoretical underpinning which strives toward for optimisation can lead in facilitating mathematical tractability to the exclusion of increasing returns and self reinforcing phenomena secondly they consider actors as rational and homogeneous representative agents but agent heterogeneity is an important feature that needs to be addressed further while their country sectoral level of aggregation can provide a full economy overview of the consequence of various shocks it is not able to account for local structures or intra sectoral intricacies al riffai et al 2017 willenbockel et al 2016 cge models are also heavily dependent on calibration data their parameterization is based upon observation of the economic system at a given time accordingly they can lose predictive power as their simulations are pushed too far from the starting point they are rarely tested against their predictive power to check for the consistency of their projections allan et al 2007 another common problem is related to the ability of cge to treat economic issues with the appropriate spatial and temporal scale economic phenomena often take place at the sub national and interannual scale while cge work at a generally country yearly attributed scale shannak et al 2018 finally a challenge for cge is the modelling of technological changes farmer et al 2015 hertel and liu 2019 the common representation of technology shifts in cge models is developed by altering the constant elasticity of substitution ces functions baum et al 2016 blignaut and van heerden 2009 and or the exogenous productivity factors on the one hand this simplifies the mathematical and computational structure of the model on the other with this structure technology becomes static or at best exogenous which is quite unrealistic to sum up the main criticism against cges is that they are based on rigid and idealized economic assumptions allan et al 2007 bataille and melton 2017 howarth and monasterolo 2017 this can be a problem for nexus analysis given the important role played especially by technological factors in energy water and food production and efficiency nevertheless computable general equilibrium cge models can still be considered useful tools to address the nexus schlör et al 2021 they are suitable for detecting impacts between sectors and on the whole economy which perfectly fits in with the characteristic of a nexus investigation willenbockel et al 2016 furthermore they are still central in the economic literature blanchard 2016 hence the aim of this review is to present the main technical features of nexus modelling by focusing on cge models evaluating their strengths weaknesses and blind spots and to highlight a possible line of research to overcome the major shortcomings identified 3 methodology of the review process the review started with a general research around the concept of nexus nexus modelling and specifically the role of cges the collection of the publications regarding the application of cge modelling of wef nexus has been developed in a systematic way by following the steps illustrated in fig 2 first of all a search in the elsevier scopus elsevier sciencedirect and web of science database has been performed the keywords used were water and energy and food and nexus and cge the results were 4 for scopus and 103 articles for sciencedirect and 5 for web of science as a complement to this first screening removing the papers not directly involving cge modelling a google scholar citation tracing forward and backward has been implemented afterwards a further search has been conducted that focuses on cge modelling in relation with the three sub components of the nexus this review on scopus was run with the keywords food and energy and cge food and water and cge and water and energy and cge originating respectively in 43 37 and 71 papers on scopus title abstract and keyword restriction 17 11 40 on sciencedirect title abstract and keyword restriction and 78 67 114 on web of science filtered for topic after the google scholar citation tracing forward and backward the screening of the papers and the cleaning of the double counted references included the articles from the previous step a total of 583 papers were identified reviews of nexus modelling conceptual discussion of the framework and historical development and resource modelling with cges were also collected adding a total of 314 papers to the screening after abstract and keyword screening and the removal of the papers not relevant for a contextualization of the concept of nexus and or not relevant at list partially to cge modelling a total of 407 were collected for textual analysis with the irrelevant papers discarded the review proceeded to a qualitative analysis of the wef in cges for a total of 127 studies of these 90 of them were used for quantitative analysis 4 results wef nexus and cge modelling 4 1 first link water energy and cges around 98 of the world s electricity production uses water van vliet et al 2016 in a different share according to the type of generation energy production can employ water directly e g in the case of hydro power or indirectly e g in thermo electric production where water is used for cooling van vliet et al 2016 q zhou et al 2018a both uses need to be considered for a proper representation of the energy water link table 1 highlights that only four studies found in this review directly integrate water as an energy factor of production in the cge structure sun et al 2021b 2020 taheripour et al 2020 teotónio et al 2020 most of the studies account for it externally e g by establishing links with hydrological models but without back feeding quantifying variation of water demand as a proportion of energy production changes or using the fictitious instrument of pricing through taxation su et al 2019 q zhou et al 2018a y zhou et al 2018b zhou et al 2016 the latter in particular is a method that sets water fees proportional to the sectorial water withdrawal in order to account for the effects of different water costs fan et al 2018 from the review what also emerges is that water uses generally refer to freshwater withdrawal with just two studies explicitly addressing water uses for cooling q zhou et al 2018a zhou et al 2016 none of the studies accounts for the overall nexus or water competition implications finally with the sole exception of q zhou et al 2018a they are country level focusing mostly on china and showing the lack of global analyses their main conclusion is that regional energy production can indeed be affected negatively by climate change through water scarcity extreme events and the relative seasonal variations some hot spots e g north africa and middle east emerge as high risk regions and this topic arose as a most urgent research field for the near future 4 2 second link energy food and cges the relationship between energy and food in the literature is complex and multifaceted nevertheless it is easy to identify a preponderant cluster developed around the biofuel issue 3 3 indeed the connection between food and energy is much broader than the biofuel issue the agri food sector consumes around 30 of global energy of which 70 is absorbed directly by crop production without considering transport or processing fao 2012 or the role of fertilizers vlek et al 2004 which are highly energy intensive inputs but the representation of energy demand originated by agriculture towards energy producing sectors is part of the standard representation of sectoral exchanges in cge models typically cges focus on biofuels for two main reasons their potential though controversial contribution to emission reduction timilsina et al 2010 and their effects on agricultural price volatility and food security the latter has attracted increasing attention after the 2008 food price crisis in which biofuels played a critical role mitchell 2008 on the contrary other important impacts of biofuel use such as those on biodiversity and water are much less frequently investigated with cges table 2 in these analyses although important the role of water as the third element of the nexus has been explicitly treated by very few contributions the first one is ge lei and tokunaga ge et al 2014 who introduce water costs in their input output table of the non grain fuel ethanol sector their main aim however is not to investigate water consumption but to understand the effects of a possible expansion of ethanol production on food security and land use in a subsequent paper ge and lei ge and lei 2017 model water as an economic sector this is an improvement with respect to ge et al 2014 as it enriches the description of water interactions with all sectors but it still cannot represent the role of water in the production process similarly kaenchan et al kaenchan et al 2019 use water requirement statistics and define a water demand function to analyse scenarios for the sustainability of bioethanol production in thailand but in none of these studies is the water input embedded directly in the firm s production function this is done in the gtap w bio model haqiqi 2016 taheripour et al 2020 2013a 2007 developed on the basis of the global trade analysis project gtap model gtap initiated in 1997 hertel 1997 made several improvements that increased its ability to tackle nexus connections these included an improved energy sector representation in its gtap e burniaux and truong 2002 mcdougall and golub 2007 and gtap p peters 2016 versions which accounted for biofuels in gtap bio birur et al 2008 taheripour et al 2007 biofuels are introduced by disaggregating the energy capital bundle and the crop sector to capture crop wide competition for food and crop fuelled energy as well as the relative price and trade effects in agricultural markets birur et al 2008 further developments included the introduction of agro ecological zones aez disaggregate land types to better represent the trade offs between different land uses in the gtap bio aez model version birur et al 2008 taheripour et al 2013b and the full linking of ghg emissions data from gtap e to the aezs in the gtap bio adv model version tyner et al 2013 finally particularly relevant for this survey gtap introduced water among the endowments primary production factors in its gtap w bio version taheripour et al 2013a the next section deals with water modelling in gtap w bio tin relation to the water food nexus although developed in a model addressing biofuel use it refers to the use of water for agricultural production and thus fits more consistently there gtap bio w has been used to address complex phenomena such as irrigation induced land use changes stemming from biofuel production taheripour et al 2013b the economic and resource impacts of crop switching or the effects of technological changes e g irrigation implementation haqiqi 2016 nevertheless the only paper that focuses explicitly on the overall economic impacts and trade offs between energy food and water is qu et al 2021 which while it openly addresses the issue with a nexus perspective still lacks the explicit representation of the third element of the wef to achieve a broader perspective 4 3 third link water food and cges the connections between water and food are obvious and highly significant 4 4 water influences food production both directly i e input with proportional output and indirectly i e ineffective water drainage infrastructures can lead to a decrease in the salinization of the land or to waterlogging which could lower crop yields foley et al 2011 gleick 2003 their relationship is also quantitatively significant as agriculture is responsible for roughly 70 of water withdrawals fao 2003 and accounts for around 86 of the world freshwater resource consumption foley et al 2011 shiklomanov 1998 the water food link has also had a complex relationship with climate change with multiplier dynamics acting on both sides bell et al 2014 temperature variations and extreme events are expected to influence future crop productivity nelson et al 2018 2010 wheeler and von braun 2013 and water availability ipcc 2020 2019 2014 2008 with water scarcity increasingly pressuring countries to rely on trade in order to ensure food security fao 2003 moreover climate change will be particularly important for rainfed crops which nowadays account for 80 of the cultivated land and 60 of the world s food supply fao 2015 rockstrom et al 2007 even though irrigation practices are expanding rosegrant et al 2002 climate change will therefore be a critical driver of food production and water management issues fao 2003 the cge analyses linking water and food are concentrated around the topics of socio economic impacts on the agricultural sector al riffai et al 2017 the effects of water policies such as water pricing or quantity restrictions on irrigation calzadilla et al 2011 johansson et al 2002 and agricultural price determination this research field has sometimes been expanded to investigate technological shifts decaluwé et al 1998 there are mainly two approaches to introducing water into cge models see table 3 one is to introduce it indirectly or externally while the other includes water as an explicit production factor a typical indirect approach to modelling water is to use changes in land productivity as a proxy of water supply variations which would then influence the final demand supply and price of agricultural commodities recently dudu ferrari and sartori dudu et al 2018 tried to add more breadth to this type of water modelling they suggest using a ces shifter i e sectorial specifically calibrated total factor productivity to adjust the factor productivity of different sectors in the production function this would make it possible to capture water availability impacts on productivity also in non agricultural sectors enabling a more comprehensive assessment of production constraints and inefficiencies modifications in crop productivity in response to water supply changes that entail changes in production costs have also been used to set a fictional cost for water in an attempt to overcome one of the typical issues in water modelling with cge that of the zero price of the resource see horridge et al 2005 and below alternatively alterations in land productivity have been used for the same purpose letsoalo et al 2007 the models that directly embed water as a factor in the production function of the firms assume no substitution leontief functions as in berrittella et al 2007 van heerden et al 2008 or a partial substitution ces function as in calzadilla et al 2011 darwin 2004 across water and the other primary inputs cge models represent the values of exchanges accordingly defining the value of the water input as essential this is also a particularly thorny issue as water is often a free or at best under priced resource one method used is to disentangle water value from the land value by using as a criterion the price differential across irrigated and non irrigated crops and attributing the difference to the contribution of water since 2013 shortly after the introduction of the nexus concept some studies appeared that included water as a production factor also for non agricultural sectors nevertheless their presence in the literature is still limited dermody et al 2018 li et al 2018 nielsen et al 2016 and they tackle non agricultural water uses only in an aggregated way koopman et al 2017 liu et al 2019 luckmann et al 2016 roson and damania 2017 taheripour et al 2020 2018 a partial exception is the rescu water model by nechifor and winning nechifor and winning 2018 which starting from the gtap e structure burniaux and truong 2002 and gtap p peters 2016 specifies and disentangles water consumption for the energy sector it is finally worth stressing that independently upon the direct or indirect introduction of water into the models cges study of water issues often entails coupling economic models with physical hydrologic models e g al riffai et al 2017 this practice is meant to exploit the best available information of physical changes in resource availability and to increase precision and reliability of the simulations nevertheless it still presents some limits such as the difficulty to test convergence across different model outputs parrado et al 2019 and to match the different spatial temporal scales bell et al 2014 across different model typologies accordingly the development and improvement of multiple scale modelling remains an important research practice and objective al riffai et al 2017 dermody et al 2018 li et al 2018 5 discussion challenges and paths forward for cge nexus modelling the review highlighted that the integrated nexus analysis is challenging there are objective difficulties in modelling the complex interdependencies involved and in gathering good quality data indeed among all the studies surveyed only one taheripour et al 2020 addresses the nexus nodes explicitly and in their entirety it also emerged that the most problematic issue in addressing the wef nexus with cge models is the realistic representation of the water dimension fig 3 in fact only one third of the studies in this review implemented water as an explicit produc 5 taheripour et al 2020 is reported in both the energy food and water food as this framework is built explicitly to address both issues i e gtap w bio tion factor with most of them concentrating on the analysis of the water food implications for agriculture while neglecting the other sectors an explicit representation of water as an input for energy production is provided only by four studies table 3 moreover the survey emphasized that the water energy nexus is underrepresented compared to other nexus nodes thus a first conclusion is already possible there is the need and room for improvement in the explicit introduction of water in cges in general and particularly in the energy sector the review process also highlighted the following modelling issues water pricing usually water is an almost free public good with virtually no price in the standard cge structure with ces demand functions this would translate into infinite consumption dudu et al 2018 moreover even when a water pricing structure can be identified it is usually sector specific and does not respond to competitive market mechanisms since water is ultimately allocated by institution and policy considerations outside the market forces olmstead 2014 this is an evident problem for models like cges founded as they are on market transactions data availability drews et al 2019 there is a lack of clear information on the different water uses and availability of water types i e surface or groundwater distribution through irrigation or rain across sectors this unavoidably leads to weak model calibration which translates into weak model prescriptions and reliability spatial and time scale problems raised by water modelling de fraiture 2007 harou et al 2009 li et al 2018 maneta et al 2009 water resources are often trans boundary while the typical investigation scale of cge models is the country level crops vary seasonally and water availability and flows change hourly while economic analyses with cge refer to yearly time frames shannak et al 2018 baum et al baum et al 2016 for example highlight this issue by studying the water use salinity changes in food production in israel in this study the factors change according to different temporal and spatial scales e g field national level creating a mismatch in the profitability of the crops at the regional level and sub optimal allocation of land and water among crops and sub regions thus the first task for achieving a better nexus representation and understanding is to address these shortcomings the literature also proposes further suggestions for increasing the realism of water modelling hertel and liu 2019 the first is to account for the potential losses from evapotranspiration variation 6 6 evapotranspiration is the quantity of water that passes from land and plants to the atmosphere therefore not going to contribute directly to groundwater recharge it is an important factor contributing to water availability reduction this can be done in two ways a econometrically on the basis of existing data on how water availability is affected by different facilities technologies or b as in mekonnen and hoekstra mekonnen and hoekstra 2011 where it is computed endogenously by using the penman monteith equation 7 7 the penman monteith equation is the standard method used by the united nations food and agriculture organization to model evapotranspiration it is based on daily mean temperature wind speed humidity and solar radiation a specific issue that also deserves more attention is the buffering time for groundwater this factor which affects the ability of the environment to recharge its water supply could greatly decrease water availability and should be considered in order to achieve a more realistic representation of water dynamics diao et al 2008 in nexus terms it would be important to explicitly introduce the consumption feedbacks between water and energy hertel and liu hertel and liu 2019 suggest switching from bundling water only with land in production functions to a nest with capital atkeson and kehoe 1999 doing so and estimating the cost of capital and water as well as their substitution would make it possible to endogenously address phenomena such as switches to higher water efficient technologies as water become scarcer as concerns biofuel modelling cges should systematically endogenize irrigation water calzadilla et al 2011 taheripour et al 2013a in order to capture competition between food crops and biofuels based also on water uses this procedure however is not mainstream practice in the literature an additional important and under investigated topic are dynamics in residential demand for water this is often disregarded because of the relatively low share of water involved but it actually has profound implications for well being conducting this investigation will be particularly difficult in developing countries that are affected by the joint presence of formal and informal water uses that mystify water quantities and pricing another aspect to consider in household water consumption is agent heterogeneity different classes of households whether rich poor or rural urban have very distinct and disparate water consumption patterns which could be exasperated with the development of demographic socio economic and environmental changes accordingly considering this heterogeneity is fundamental for addressing the nexus perspective a final advancement could be incorporating the trade off between environmental quality and consumption in household utility which in the specific nexus context translates into placing environmental water services in the household utility function tsigas et al 2001 6 conclusions cges models are used to describe the behaviours of rational economic agents markets institutions and the relationships across them their structure allows for the synchronous identification of endogenous interdependencies between sectors and for the detection of the impacts of exogenous factors on the full economy therefore they can be appropriate instruments for addressing a complex topic such as the wef nexus which inherently concerns multiple sectors and fluxes this said it appears that there are not many cges able to address the nexus in its entirety which limits the realism and robustness of both the modelling results and the related policy advice one of the biggest hurdles relates to the explicit modelling of the water input and its competitive uses across sectors and agents this is in turn determined by the difficulty of representing economically a zero price resource like water and by reliable data on water uses should we suggest some priority for action we would recommend improving data quality on water uses and prices in different countries and sectors and including water as a specific production factor for the different sectors in the economy starting from the energy one to then include household water consumption and expand the analysis at the multi country level author contributions conceptualization e b f b methodology e b writing original draft preparation e b supervision f b writing reviewing and editing f b funding this research did not receive any specific grant from funding agencies in the public commercial or non profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this paper has benefited from comments and suggestions of enrica de cian ca foscari university the ecip division of the euro mediterranean center on climate change venice italy and from karl steininger birgit bednar friedl and the whole econclim research group of the wegener center for climate and global change of graz austria 
