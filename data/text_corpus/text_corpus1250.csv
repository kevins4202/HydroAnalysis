index,text
6250,the robust optimization of groundwater quality monitoring network is subject to many conflicting objectives and high level of uncertainty in hydraulic conductivity this study develops a two stage stochastic optimization framework including the uncertainty quantification using a cheap to evaluate surrogate model and an improved epsilon multi objective noisy memetic algorithm ε monma for monitoring network design the surrogate model based on sparse polynomial chaos expansion pce is constructed to replace expensive simulation model in the uncertainty quantification of concentrations at the pre defined monitoring locations for reducing huge computational cost additionally the scenario discovery strategy using sparse pce model is applied to filter a typical scenario set and the centroid of contaminant plume is used as the diversity metric which avoids enumerating all possible contamination plumes caused by the uncertain k field in the optimization the proposed algorithm is then employed to solve stochastic management model to achieve robust monitoring design indicating the insensitivity of monitoring design to plume uncertainty no matter which of the many possible scenarios becomes the true distribution of contamination under the true k field a synthetic aquifer considering uncertainty in hydraulic conductivity is designed to optimize monitoring network design the pareto optimal solutions to the synthetic example are achieved under three of plume scenario sets defined at deterministic scenario scenario a0 monte carlo based scenario discovery scenario a1 and surrogate assisted scenario discovery scenario a2 respectively comprehensive analysis demonstrates that the monitoring design based on scenario a2 outperforms either of the two designs based on scenarios a0 and a1 in terms of the improvement of robustness of designs evaluated against the typical scenario set meanwhile the performance of monitoring network deteriorates as the uncertainty of plume noisy strength increases indicating the significance of reducing parameter uncertainty in groundwater monitoring design the research findings show that the developed stochastic optimization framework is a computationally efficient and promising tool for multi objective design of groundwater monitoring network under uncertainty keywords multi objective optimization robustness uncertainty quantification surrogate model monitoring network design 1 introduction optimal design of groundwater monitoring network that is capable of providing accurate and informative data is crucial to improve our understanding of complex groundwater systems while reducing huge and unnecessary capital expenditure in general an optimal sampling design needs to consider multiple contradictory objectives for minimizing the costs of data acquisition and maximizing the amount of information in the linked simulation optimization framework kollat and reed 2006 reed and kollat 2013 luo et al 2016 however as a prerequisite for the simulation optimization management model the groundwater flow and transport simulation model inevitably involves the uncertainty associated with hydrogeological parameters which is essentially used to describe and update the state variables in the management model therefore the management models such as one used for optimal design of groundwater monitoring network have to consider model uncertainty derived from model parameters or model structures otherwise they might result in erroneous management decisions wu et al 2006 bayer et al 2010 kollat et al 2011 alzraiee et al 2013 luo et al 2016 the heterogeneity of spatially distributed parameters e g hydraulic conductivity k in the subsurface systems can be characterized using sparse measurement points combined with a geostatistical model in the field studies deutsch and journel 1997 diggle and ribeiro 2007 the traditional geostatistical model utilizes the monte carlo mc simulation to generate a large number of stochastic realizations of parameter fields based on unbiased estimations at the unmeasured locations the uncertainty quantification uq of model outputs can be implemented based on numerous realizations by the simulation model which needs a huge computational expense furthermore stochastic optimization framework generally requires thousands of individual evaluations for each parameter realization in order to search for robust solutions that are insensitive to parameter uncertainty it is computationally prohibitive in the practical application that millions of model evaluations in total need to be implemented when several thousand realizations are generated during evolutionary search motivated by the above mentioned difficulties wu et al 2006 exploited a noisy genetic algorithm nga to evaluate per objective function with the much smaller realization set the nga can be used to find highly reliable and robust solutions using a dynamic random resample strategy in which a set of realizations are randomly resampled at each generation and the prior realizations will be augmented at the later stage of search then singh and minsker 2008 developed the multi objective stochastic optimization coupled with nga after that bayer et al 2010 exploited a stack ordering technique to update realizations in the evaluation subset in order to achieve highly reliable solutions with lower computational burden compared to the random resample scheme however the previous studies only considered several thousand realizations using mc sampling and was unable to implement accurate uq associated with random parameters which should be an essential task for stochastic optimization this challenge in the popular mc method is highly time consuming for uq in the stochastic partial differential equation governing groundwater flow and solute transport the computationally efficient surrogate models have attracted more intention in recent years asher et al 2015 razavi et al 2012 surrogate assisted uq exploits much smaller sample set to train approximate model and fully replaces expensive simulation model for quantifying uncertainty of model outputs many surrogate models have been successfully applied to uq and obtained credible approximate results compared to cpu intensive mc simulation with the groundwater models müller et al 2011 zhang et al 2013 crevillen garcia et al 2017 meng and li 2017 mo et al 2017 polynomial chaos expansion pce is an effective alternative surrogate method which has been extensively used in the uq rajabi et al 2015 bazargan et al 2015 meng and li 2017 the pce method constructs a series of orthogonal polynomial basis functions to represent the quantities of interest based on independent random inputs with different probability distributions xiu and karniadakis 2002a ghanem and spanos 2003 furthermore non intrusive pce is more attractive due to the permission of using a well validated simulator that can be treated as a black box to train the surrogate model in this study we utilize stepwise regression and least angle regression lar proposed by blatman and sudret 2010 2011 to reduce the number of regression points in the face of higher random dimensionality or higher degree of polynomial equations as long as the pce coefficients are achieved by the stepwise regression the statistical moment of quantities of interest can be easily obtained spatial correlated k field is a critical physical property that always dominates the process of groundwater flow and solute transport also karhunen loève k l expansion can be applied to transform k field into low dimensional random subspace and was used to construct highly efficient surrogate model integrated with the pce li and zhang 2007 2013 meng and li 2017 indicating that the k l technique is promising and effective due to its ability to significantly reduce dimension of random parameters accordingly in this study we employ the sparse pce model constructed by lar algorithm to implement uq at the potential monitoring locations under k field uncertainty surrogate assisted uq is the first stage in the stochastic optimization framework for groundwater monitoring network design to quantify uncertainty in the optimization scenario based multi objective evolutionary algorithm moea has been exploited to implement robust optimization e g singh and minsker 2008 sreekanth and datta 2011 luo et al 2014 2016 beh et al 2015 sreekanth et al 2016 yang et al 2017 sankary and ostfeld 2018 however evaluation of evolutionary individuals against all possible operational scenarios may be computationally impractical therefore a random resample strategy that selects much smaller ones from a set of typical scenarios capable of approximating the feature of all possible scenarios is developed in the optimization framework sankary and ostfeld 2018 developed a random resampling scheme that used the advantage of statistical bootstrapping to achieve reliable estimates of the uncertainty on the unseen samples in the scenario based moea their results indicated that the moea coupled with the dynamic resample strategy could possess the ability to create robust solutions under unknown operational environments the study generated a typical scenario set by setting resolution of centroid coordinates of a large number of plume scenarios from surrogate assisted scenario discovery and utilized statistical bootstrapping technique to resample in every generation the groundwater monitoring network design often involves many objectives typically more than three objectives which may result in the domination resistance phenomenon purshouse and fleming 2007 hadka and reed 2013 the phenomenon shows that the capacity of pareto sorting with distinguishing optimal solutions weakens and leads to a large number of non dominated solutions i e none of the objective values could be improved without degrading one or more of the other objective values existed in population hadka and reed 2013 developed an auto adaptive moea framework named borg which combines several promising techniques to improve the performance of the algorithm in addressing multi objective optimization in order to enhance the local optimality of solutions memetic algorithms composed by natural evolution based on darwinian principles and cultural evolution capable of local refinements were applied to speed up the convergence of moea sindhya et al 2011 2013 in this study we employ the ε dominance concept and a modified auto adaptive multi operators recombination to alleviate domination resistance and a bidirectional neighborhood mutation operator to enhance the local optimality of archived solutions then we integrate these techniques and the fast non dominated sort process of nsga ii deb et al 2002 to the framework of nga wu et al 2006 therefore the epsilon multi objective noisy memetic algorithm ε monma possesses the ability of highly effective global search in addressing many objective optimization under noisy environment moreover ε monma integrates the surrogate assisted uq into a unified optimization framework so as to implement accurate uq at the cost of acceptable computation burden finally we demonstrate the availability and reliability of the framework in the uncertainty based groundwater monitoring network design through a synthetic reactive transport aquifer system 2 methodology fig 1 illustrates the general flowchart to implement robust optimization first we have to construct a groundwater flow and reactive transport model and implement uq with sparse pce for the contaminant concentration at the monitoring wells then we exploit a scenario discovery strategy to generate a typical scenario set and perform stochastic multi objective optimization by ε monma in this section we only present the details of aforementioned modules as for the post optimization analysis of the pareto optimal solution set it can be found in section 4 2 2 1 groundwater flow and reactive transport model the reactive transport model was constructed using a finite difference code pht3d v2 10 prommer and post 2010 the program is designed to simulate three dimensional advective dispersive multicomponent reactive transport process and had been benchmarked and applied to complex reactive networks prommer et al 2006 2009 rodriguez escales et al 2017 pht3d integrates the multicomponent transport model mt3dms zheng and wang 1999 with geochemical model phreeqc 2 parkhurst and appelp 1999 using a sequential operator splitting technique therefore the numerous reactive networks can be coupled with groundwater flow and transport model in the aquifer system considering spatial distributed physical heterogeneity and reactive mineral at field scale is essential for improving fidelity and reliability of numerical model and providing understanding of the influence of aquifer characteristics on contaminant transport which is significant for the optimal design of groundwater monitoring network 2 2 sparse polynomial chaos expansion 2 2 1 polynomial chaos expansion pce is a widely used metamodeling method that provides a functional approximation of a computationally expansive model for propagating uncertainty ξ ξ 1 ξ 2 ξn is a set of independent random variables that follows the specific joint probability density function pdf where n is the dimension of random variables let m ξ represent simulation model pce model can be defined as blatman and sudret 2011 1 y m ξ α n n λ α ψ α ξ where ψ α ξ is the basis function expressed as the multivariate orthogonal polynomials in the stochastic space λ α is the corresponding expansion coefficient and α is a multi index that identifies the components of the polynomials particularly for α 0 λ 0 denotes the first expansion coefficient the multivariate orthogonal polynomial can be combined with the univariate polynomials owing to the independence of ξ 2 ψ α ξ i 1 n ω α i ξ i where ω α i denotes the univariate polynomial of degree αi in ith variable dimension the type of ω α i is selected according to the probability distributions of ξ which adopts the scheme proposed by askey and wilson 1985 to map the distribution of ξ e g gaussian gamma or beta to the optimal polynomial e g hermite laguerre or jacobi polynomial in case that ξ follows arbitrary distribution the univariate polynomial can be constructed by generalized pce xiu and karniadakis 2002a b for realistic application eq 1 is often truncated to a limited number of terms by assigning the expansion order d and modified as 3 m ξ m t ξ α a λ α ψ α ξ where a is a candidate subset satisfying the constraint of α i 1 n α i d in the truncated pce there are p p n d n d orthogonal polynomials and regression coefficients to be solved once the pce models are constructed the mean μ and variance σ 2 of quantities of interest can be respectively calculated by the following equations 4 μ y λ 0 5 σ 2 y α a α 0 λ α 2 2 2 2 adaptive sparse pce the evaluation of pce coefficients which can be solved by intrusive or nonintrusive methods is the key issue to build exact and reliable surrogate model the intrusive approach must insert pces into the governing equation of numerical model and significantly modify the well validated model codes which is less attractive for industry abraham et al 2017 however the non intrusive approaches such as the projection and regression methods pettersson et al 2015 require no modification to the existing deterministic codes eldred and burkardt 2009 hosder 2012 in this study we focus on the regression approach that establishes an over determined linear system of equations by minimizing the least square criterion for estimating polynomials expansion coefficients however the main drawback of the regression method is that the computational burden caused by the evaluation of numerical model drastically increases due to the increase of random variable dimension and polynomial expansion degree to mitigate the above mentioned problem we employ the sparse regression method blatman and sudret 2008 2010 2011 which sequentially determines the most important basis functions from few training samples to build computationally efficient surrogate model lar efron et al 2004 is a heuristic regression method and designed to search the most influential basis functions on the model responses over a large number of candidates after detecting the relevant basis polynomials using lar the coefficients corresponding to the retained basis polynomials are calculated using ordinary least square regression finally the sparse pce is constructed and the number of optimal basis functions is much lower than the classical full representation to avoid the improper construction of a prior truncation scheme the basis adaptive algorithm is proposed to enhance sparse pce approximation by increasing the maximum polynomial degree blatman and sudret 2011 integrated sparse pce with the full degree adaptive algorithm to develop a powerful and effective surrogate model as implemented in the uqlab marelli and sudret 2014 the present study employed uqlab to construct sparse pce model for uq of concentration after that the surrogate model was coupled with ε monma for robust optimization of monitoring network design 2 3 multi objective robust optimization of groundwater monitoring design 2 3 1 stochastic representation of k field the k field can be represented by y x θ ln k x θ where x d θ θ and assumed to be the second order stationary gaussian random field we can write y x θ y x y x θ where y x and y x θ are mean and fluctuation respectively the spatial structure of random field is expressed by covariance c y x y y x θ y y θ which is bounded symmetric and positive definite and can be decomposed as ghanem and spanos 2003 6 c y x y i 1 λ i f i x f i y where λi and fi x are the eigenvalue and eigenfunction respectively they can be solved from the following fredholm equation 7 d c y x y f x d x λ f y in most instances the eigenpairs are solved numerically while analytic and semi analytic solutions exist under certain condition zhang and lu 2004 then the random field can be stated as 8 y x θ y x i 1 λ i f i x ξ i θ where ξ i θ are the orthogonal random variables eq 8 is known as k l expansion the variance of output variables can be calculated with σ y 2 i 1 λ i f i 2 x which can be transferred into d σ y 2 i 1 λ i where d is a measure of physical domain size for practical applications eq 8 needs to be truncated into finite terms the number of retained terms depends on the convergence rate of eigenvalue summation that is to say i 1 n λ i d σ y 2 has to guarantee relatively higher proportion in the field scale study the known k values are obtained at sampling locations the conditional simulation can be implemented in the handing of k field uncertainty assume n known k values y 1 y 2 y n located at x 1 x 2 x n the conditional mean and covariance of y can be stated as follows lu and zhang 2004 9 y c x y x i 1 n μ i x y x i y x i 10 c y c x y c y x y i j 1 n μ i x μ j y c y x i x j 11 c y x x j i 1 n μ i x c y x i x j j 1 2 n where the mean function μi x needs to be solved using the kriging equation eq 11 the conditional covariance function is nonstationary and the corresponding eigenvalues and eigenfunctions have to be solved numerically using the fredholm function lu and zhang 2004 12 d c y c x y f c x d x λ c f c y the conditional realization of k field can be implemented using k l expansion similar to the unconditional simulation and stated as 13 y c x θ y c x i 1 λ i c f i c x ξ i θ likewise we can truncate the k l expansion to reach the desired accuracy using the largest n eigenvalues which significantly reduces the dimensionality of the parameter space meanwhile we assume the log transformed hydraulic conductivity has a separable exponential covariance function at two different location xx xy and yx yy 14 c y x x x y y x y y σ y 2 exp x x y x η x x y y y η y where σ y 2 is the variance ηx and ηy are the correlation lengths along x and y directions respectively 2 3 2 management model the goals of groundwater monitoring network design aim at minimizing monitoring costs and maximizing the accuracy of estimated contamination plume in this study the four objectives are employed to minimize i monitoring cost f cost ii mass estimation error f mass iii first moment estimation error f 1st and iv concentration estimation error f conc the ordinary kriging method is used to interpolate concentration estimation at no sampling location throughout the plume wu et al 2005 2006 the mathematical formulation of management model can be stated as luo et al 2016 15 minimize f x ω f cost x ω f mass x ω f 1 s t x ω f conc x ω 16 subject to u x ω ω 0 where xω xω 1 xω 2 xω s xω s is sth sample location of ωth monitoring scheme if sth well is sampled xω s 1 otherwise xω s 0 to avoid too few sample data in the search radius of kriging to perform interpolate the number of unestimated point u xω in the specified domain is 0 the objective of monitoring cost can be described as 17 f cost x ω s 1 n α s x ω s where αs is the cost to the sth sampling location n is the number of pre defined potential monitoring wells in this study the cost coefficient αs is uniformly set to 1 the error objective functions in terms of percentage can be stated as kollat and reed 2006 18 f mass x ω m 0 a l l m 0 ω m 0 a l l 100 19 f 1 st x ω j δ 1 a l l j δ 1 ω j δ 1 a l l j 100 20 f conc x ω i 1 g c all u i c ω u i i 1 g c all u i 100 where m 0 all is the total mass of contaminant plume i e zeroth spatial moment interpolated with all pre defined monitoring wells m 0 ω is the total mass calculated under the ωth sampling scheme δ 1 a l l j and δ 1 ω j are the estimated center of contaminant plume i e first spatial moment based on all potential monitoring wells and the ωth sampling design j denotes the direction along which the moment is computed call ui and cω ui are contaminant concentrations at ith grid cell with sampling all monitoring locations and the ωth sampling plan respectively g is the number of active cell in the model domain this study is aimed at developing multi objective robust optimization framework for the monitoring network design in the presence of k field uncertainty thus the objective functions have to be modified across multiple stochastic realizations in the evolutionary optimization the min max formulation deb et al 2015 sankary and ostfeld 2018 showing the best objective performance in the worst evaluation realization can be used to identify the robust optimal monitoring scheme as follows 21 minimize f θ x ω k f cost x ω max i r s f mass x ω k i max i r s f 1 st x ω k i max i r s f conc x ω k i where ki is ith k field realization rs is the realization set being evaluated f θ xω k is the maximum objective value over entire realization set corresponding to design xω the estimated plume is interpolated using ordinary kriging under specified k field realization and then the error objectives are achieved in this study we use sparse pce to output logarithmic concentration of all sampling locations in the evaluation realization set as follows 22 ln c i f pce k i i r s 2 3 3 multi objective robust optimization framework the complex or field scale groundwater monitoring design requires the consideration of uncertain characteristics e g hydraulic conductivity in the aquifer however enumerating all possible stochastic realizations or scenarios is computationally infeasible especially for transport problem coupled with complex reactive processes moreover the optimal solutions under noisy environments have to be evaluated against many scenarios to guarantee the robustness and reliability which leads to tremendous computational burden in the optimization thus this study proposes a surrogate assisted ε monma to optimize the monitoring network under parameter uncertainty as shown in fig 2 the algorithm includes two main modules i e the surrogate assisted stochastic scenario discovery and the epsilon multi objective noisy memetic algorithm the details of the algorithmic implementation process are described below module i surrogate assisted scenario discovery the purpose of module i is to provide the evaluation scenarios which can represent all possible plume scenarios caused by the uncertain k field as far as possible first a set of sparse pce models are trained to predict concentration in the candidate monitoring locations then mc simulation is implemented to generate a large number of evaluation scenarios p o which are very close to all possible scenarios next the centroid coordinate of contamination plume is considered as diversity index to filter a smaller set of evaluation scenarios pe finally pe is used as a typical scenario set in the noisy evolutionary algorithm if an individual is evaluated against each realization in the typical scenario set pe the total computational burden is still unacceptable owing to tens of millions of kriging interpolation running thus we exploit bootstrapping technique to stochastically resample from pe to achieve smaller scenario set ps evaluating for each individual and quantify the effect of uncertainty on the optimal solutions during evolutionary search the statistical bootstrapping resamples with replacement from the typical scenario set and can estimate the typical scenarios overall statistical measures with a smaller sample set which has been widely used in the genetic programming fitzgerald et al 2013 module ii integrated multi objective noisy memetic algorithm the proposed ε monma includes several design elements from the existing borg moea hadka and reed 2013 and a newly added neighborhood mutation operator to implement local search which coupled with the framework of the noisy genetic algorithm nga wu et al 2006 and the fast non dominated sort process in the nsga ii deb et al 2002 the implementation processes of the modules are presented as follows step i initialization of population and input parameters the initial population is generated using latin hypercube sample lhs and the key parameters include population size npop maximum function evaluation number neval crossover probability pc mutation probability pm the epsilon resolution of objectives ε cost ε mass ε 1st ε conc the number of evaluation scenario for each candidate solution ns step ii sampling of a set of plume scenarios and computation of objective function the ns sized scenario set ps is resampled by bootstrapping technique for each generation the min max formulation is applied to compute maximum error objective across multiple scenarios in the robust optimization step iii multi operator recombination and polynomial mutation the auto adaptive multi operator recombination proposed by hadka and reed 2013 is a promising technique to select optimal operator for various optimization problem the crossover probability of each operator is updated periodically based on the proportion of the solutions generated by each operator in the ε dominance archive the recombination strategy is essential for the complex multi objective and real world optimization due to the inability to known a prior the optimal recombination operator this study attempts to integrate the multiple recombination operators i e simulated binary crossover differential evolution simplex crossover parent centric crossover laplace crossover uniform mutation into ε monma to enhance search ability step iv ε dominance archive the multi objective optimization exists a domination resistance phenomenon which results in a widespread dispersal of local non dominated solutions in the population purshouse and fleming 2007 hadka and reed 2013 the defect may slow down the convergence speed of evolutionary search and then the diversity metric becomes a primary selection operator to obtain an approximate pareto set away from the true global tradeoff surface the ε dominance archive developed by laumanns et al 2002 attempts to ensure convergence and diversity in the multi objective optimization moreover the decision makers can define the minimum resolution of objective vector f f 1 f 2 f n with epsilon vector ε ε 1 ε 2 ε n to satisfy their acceptable precision target and restrict the archive size the several advanced multi objective algorithms e g ε moea ε nsga ii borg moea have used the ε dominance archive strategy to upgrade evolutionary search and obtained better convergence and diversity performance than traditional moeas deb et al 2003 kollat and reed 2007a hadka and reed 2013 this study implemented the ε dominance archive process after the fast non dominated sorting of offspring individuals and alleviated the difficulties derived from the domination resistance step v bidirectional neighborhood mutation the retained individuals in the archive will be mutated based on gaussian perturbation in the neighborhood of solution given an archived individual ia s 1 s 2 s 3 s n the mutated solutions can be stated as 23 i a s 1 s 2 s i c u i r i s n 24 i a s 1 s 2 s i c u i r i s n where s s 1 s 2 s n is a n dimensional decision variable vector u u 1 u 2 un and r r 1 r 2 rn are the two solutions stochastically selected from the archive c is a perturbation factor and follows a standard gaussian distribution this study revived the mutation operator in every several generations of evolutionary search the cheap to evaluate surrogate model is constructed to perform uncertainty analysis of monitoring concentration with the brute force mc simulation then a set of contamination plumes are generated to represent all possible scenarios caused by the uncertainty in k field as much as possible the plume centroid is also adopted as the diversity metric to filter a typical scenario set for robust optimization afterward ε monma is applied to optimize monitoring network design under the typical scenario set therefore the methodology can provide the most robust monitoring network design in terms of preferred objectives while fully investigate the effect of uncertain k field on the decision making behaviors 3 application to robust optimal design of groundwater monitoring network 3 1 description of the synthetic aquifer system the model domain of hypothetical unconfined aquifer is 600 m in longitudinal extent 400 m in transverse extent and a model thickness of 10 m the aquifer modeled in this study is discretized into a uniform model grid of 40 rows and 60 columns as shown in fig 3 the contaminant source comprises immobile lnapl hydrocarbon compounds benzene in this study to account for the geochemical reaction processes driven by biodegradation a site specific reaction network was developed to explain the relevant conceptual model and then coupled with the groundwater flow model to simulate contaminant transport the non equilibrium dissolution of lnapl is controlled including heterogeneous porous media and morphology of source zone which is empirically formulated as a kinetic reaction model mayer and miller 1996 25 m dis β c org sat c org 26 c org sat c org sol γ org n org where β is the mass transfer coefficient c org is the concentration of the organic compound in the groundwater c org sat is the equilibrium aqueous concentration for each organic compound and calculated with eq 26 c org sol is the pure organic compound solubility γ org is the activity coefficient and typically assumed to be unity n org is the molar fraction in the lnapl the reactive transport process of contaminant considered the complete oxidation reduction sequence i e aerobic degradation nitrate reducing manganese reducing iron reducing sulfate reducing and methanogenesis which was simulated by a two step partial equilibrium reaction approach brun and engesgaard 2002 prommer et al 2006 colombani et al 2009 thus the reduction reaction in response to the aqueous and mineral form electron acceptors can be simply simulated using the reactive simulator pht3d prommer and post 2010 the distribution of k in the aquifer is assumed to be the log normally distributed isotropic and spatially correlated random field that follows an exponential covariance function the k field based on the 30 measured data is conditionally simulated using k l expansion solid squares in fig 3 the management period is set to three years for groundwater monitoring network design the plume uniformly distributes over the pre defined locations of potential monitoring wells at the end of the period for the majority of realizations the primary parameters used in the flow and reactive transport model are listed in table 1 the initial concentration used in the reactive transport model can be found in colombani et al 2009 3 2 implementation of stochastic optimization the optimization objectives are minimization of the cost of monitoring network design and plume estimation error in terms of total mass centroid and concentration estimation error under k field uncertainty the log k field is represented with the k l expansion conditioning to the 30 measured k values the infinite terms in the k l expansion are truncated for the dimensionality reduction of random variables at the certain energy level the higher number of retained terms preserves more detailed heterogeneous feature while the smaller scale characteristic in the k field has less influence on the solute transport in the present example we select first the 50 terms to preserve 73 energy and the random field can be expressed by a 50 dimensional standard gaussian random vector ξ ξ 1 ξ 2 ξ 50 then the sparse pce models are constructed using ξ as inputs and concentrations as output at pre defined monitoring locations lhs is used to generate 400 original samples to run pht3d and constantly augment sample set with 100 new samples until the desired accuracy is reached totally the 1 500 samples are selected to train sparse pce models the proposed algorithm utilizes sparse pce models to filter the representative plume scenario set to consider the influence of uncertainty in the k field we perform 250 000 times mc simulations based on sparse pce which is just calculation of polynomial to output concentration at the pre defined locations with high computational efficiency then the total plume scenario set p o interpolated with ordinary kriging is achieved to represent all possible plume scenarios the plume centroid is used as the diversity indicator and users can specify its coordinates precision ζ to split centroid space into many course or fine grids which depends on ζ value grid blocks containing multiple centroid coordinates prefer the centroid closet to the center of the block therefore we can select a smaller typical scenario set p e from p o while the plume scenarios diversity can be preserved as feasibly as possible the centroid coordinates resolution ζ along the x and y directions is set to 2 0 m which finally filters 2 663 plume scenarios from p o into p e for each individual evaluation an evaluation suite p s of 100 plume scenarios are randomly resampled from the scenario set p e using bootstrapping method at each generation the epsilon resolutions for the four objectives i e ε cost ε mass ε 1st ε conc are set to 1 0 0 001 0 001 and 0 001 respectively the bidirectional mutation operator is applied to the epsilon archived individuals in every ten generations the parameters used in the ε monma are listed in table 2 4 results and discussion 4 1 uncertainty quantification 4 1 1 assessment of surrogate accuracy to achieve the desired approximate accuracy of sparse pce models the 1 500 samples were generated to train surrogate model and additional 100 samples were randomly selected to test the performance the root mean square error rmse and correlation coefficient r are used to evaluate the prediction accuracy on the test dataset fig 4 shows the predicted concentration at 112 monitoring locations are in good agreement with results from the forward model predictions the plumes interpolated with monitoring concentration based on sparse pce are compared with those using pht3d from the three randomly selected test samples as shown in fig 5 the sparse pce models can accurately estimate concentration field from the simulation model the maximum value in the absolute error field primarily arises in the region with greater concentration gradient where the complex nonlinear response feature exists however after enriching training sample size from 400 to 1 500 the rmse value is only about 1 of the maximum value in the contamination field and r value increases to 0 998 therefore sparse pce is capable of estimating the morphology of contamination plume and the computational burden associated with uq e g mc simulation can be alleviated significantly 4 1 2 uncertainty analysis the statistical moment e g mean and variance of the log concentration can be analytically obtained by sparse pce due to the orthogonality of the basis functions to validate the effectiveness of uq using surrogate models we randomly generate 30 000 mc realizations by running pht3d model to compute the reference statistical moment fig 6 compares the mean and variance calculated by sparse pce with the pht3d based reference values in the pre defined monitoring locations it can be observed that both means and variances of log concentration of monitoring wells are very close to the reference values from the brute force mc simulation by running mc sampling on the surrogate model and simulation model the probability density function pdf at monitoring location can be achieved the estimated pdf at the selected monitoring wells is shown in fig 7 the estimations with sparse pce trained by 1 500 samples are quite consistent with the reference pdf with 30 000 mc experiments by running pht3d simulator while the results with 1 500 mc samplings exist a large bias the cpu time of surrogate model running with mc experiments is negligible and the runtime of a pht3d run for each k field realization is similar therefore the proposed surrogate assisted algorithm can significantly improve the computational efficiency up to approximately 95 1 1 500 30 000 of time saving at the stage of uq the above analysis shows that the sparse pce can efficiently provide accurate approximation of statistical moments and be integrated into robust optimization framework for groundwater monitoring network design 4 2 assessment of the optimization results 4 2 1 pareto optimal front the typical scenario set p e with 2 663 samples is selected by the surrogate assisted uq and the diversity metric with plume centroid to compare the effect of k field uncertainty on the quality of solutions the deterministic scenario i e scenario a0 and randomly generated 2 663 scenarios i e scenario a1 also are considered as the evaluation scenario suite during the optimization in comparison to the typical scenario set i e scenario a2 the plume centroid coordinates of all scenarios from scenarios a0 a1 and a2 are illustrated in fig 8 the figure shows that surrogate assisted scenario discovery can obtain a maximally diverse set of plume scenarios which means the accurate uncertainty propagation derived from k field uncertainty in optimization phase this study performed three optimization with three scenario discovery processes to search optimal monitoring network design to visualize and analyze the multi objective tradeoff of pareto optimal monitoring designs we utilized the discoverydv software package developed by kollat and reed 2007b fig 9 shows that the multi objective tradeoffs under scenarios a0 a1 and a2 respectively in this figure each sphere represents a pareto optimal monitoring network design sphere size indicates the concentration estimation error and the green sphere is an ideal solution as shown in fig 9 the small decrease in monitoring cost can result in large increase in three of error objective especially when the monitoring cost is less than 50 the objective of relative concentration error reach to 50 exists more sensitive tradeoff response to monitoring cost in comparison to the objectives of relative moment error and mass error it can be noted that the pareto optimal front in scenario a0 i e deterministic scenario exhibits the lowest estimation error and monitoring cost while the solutions under k field uncertainty i e scenarios a1 and a2 appear to be worse performance in which scenario a2 based designs perform the worst the reason is that the probability of these solutions improving monitoring design decreases as the uncertainty of plume increases however the better solutions with insufficient uncertainty propagation may be unfeasible or low performance in the future to further explain the discrepancy of pareto solutions from scenarios a0 a1 and a2 we use the parallel coordinates plot inselberg 2009 to illustrate the trade surface in fig 10 the objective values of all pareto optimal solutions are illustrated in line segments and the concentration estimation error is represented by the color the solutions with lower monitoring cost appear to be higher values of relative errors especially for the concentration error as shown in fig 10 it can be observed that the range of f cost values is similar while those of objective values of estimation errors exist larger difference the normalized error objective values in scenario a0 have maximum of 0 34 0 38 and 0 57 for mass first moment and concentration respectively which are lower than those computed by scenarios a1 and a2 the variation of error metrics based on scenario a2 is relatively larger than those based on scenario a1 for example more dark red line segments larger concentration error joining in the parallel axes the results show that the noise strength caused by k field uncertainty has adverse effect on the performance of monitoring network in the optimization even through the same monitoring expense is invested therefore reducing model parameter uncertainty is essential for stakeholders to implement robust design of monitoring network 4 2 2 robustness analysis robustness indicator measures how far the performance i e relative error objectives of monitoring network deviates from the expectation value in the noisy environment the relative error objectives of pareto optimal solutions in scenarios a0 a1 and a2 are calculated against the typical scenario set p e which also called post optimization process the relative deviation rd of error objectives is utilized as robustness metric and can be stated as 27 r d mass j f mass p r e x w k f mass p o s t x w k f mass p o s t x w k 28 r d 1 s t j f 1 s t p r e x w k f 1 s t p o s t x w k f 1 s t p o s t x w k 29 r d conc j f conc p r e x w k f conc p o s t x w k f conc p o s t x w k where fmass pre xw k represents the maximum relative mass error objective of solution j corresponding to scenarios a0 a1 or a2 fmass post xw k is the maximum of relative mass error objective evaluated across the typical scenario set after post optimization xw is a solution for monitoring network design k is a hydraulic conductivity realization set the definition of other symbols is similar to relative mass error and represents relative deviation of first moment error and concentration error rd metric measures a solution s deviation from its performance in the baseline or expected state the typical scenario set is considered as reference noisy environment lower rd value indicates lesser effect of k field uncertainty on the performance of monitoring network fig 11 illustrated the results in the robustness analysis using rd metric subfigures a c show rd of mass estimation error f mass first moment estimation error f 1st and concentration estimation error f conc based on scenarios a0 a1 and a2 respectively it is observed that the solutions achieved using scenario a0 obviously yield substantially larger rd which means worst performance in the robustness while the solutions obtained using scenarios a1 and a2 exhibits lower value overall the distribution of rd in terms of error objectives in scenario a2 is relatively lower than that in scenario a1 the average values of rd metric are listed in table 3 it is obvious that mean of rd in the scenario a2 is smaller than scenarios a0 and a1 and average value in scenario a0 is substantially larger consequently the optimal designs in scenario a2 are more robust than those of scenarios a0 and a1 when evaluated against typical scenario set importantly decision maker need consider accurate uq of contaminant transport under k field uncertainty fig 12 shows the distribution of pareto optimal solutions after post optimization as illustrated in fig 12 the solutions in scenario a0 show largest relative error objectives and cost while the monitoring designs based on scenario a2 exhibit lowest objectives the results contradict with those illustrated in fig 9 which shows the performance of solutions from scenarios a0 and a1 exists obvious deterioration owing to uncertainty propagation to compare pareto optimality of the tradeoff solutions objectives of all the optimal designs using min max formulation are rearranged using fast non domination sorting given in table 4 is the comparison of the non dominated solutions from scenarios a0 a1 and a2 and those after post optimization as shown in table 4 before post optimization there are 96 4 of non dominated solutions from scenario a0 only 2 4 and 1 2 of those from scenarios a1 and a2 respectively however after post optimization using typical scenario set there are 35 7 and 59 1 of non dominated solutions from scenarios a1 and a2 only 5 2 of those from scenario a0 table 4 shows that surrogate assisted scenario a2 can achieve relatively larger pareto optimal solution set than randomly selected scenario a1 and much larger solution set than scenario a0 under k field uncertainty it is obvious that surrogate assisted optimization based pareto optimal designs can obtain robust performance while those achieved by the stochastic optimization with mc sampling exhibit worse monitoring performance and the designs in the deterministic scenario demonstrate dramatic performance degradation 4 2 3 monitoring schemes analysis to explore the performance difference of monitoring design under scenarios a0 a1 and a2 we selected five typical solutions across a range of monitoring costs after post optimization table 5 shows the maximum relative error objective values using the min max formulation comparison of the objective values associating with scenarios a1 and a2 indicates that increasing monitoring costs of solutions 1 5 would lead to the decline of relative error objectives the relative error objectives in scenario a0 also decreases as the increase of the monitoring costs except that the concentration error objectives of solutions 4 and 5 have a slight rise the relative error objectives in scenario a2 are smaller than those in scenarios a1 and a0 under the same monitoring investment the result demonstrates the performance metrics of the monitoring designs with accurate uq based on surrogate model dominate those without full consideration of uncertainty fig 13 shows the absolute error distributions between the plumes interpolated by the data from monitoring schemes in the solutions 1 5 and the reference plumes interpolated using all the potential monitoring concentrations under scenarios a0 a1 and a2 comparing the absolute error distributions in the different optimization scenarios demonstrates the failure of scenario a0 based designs due to the absolute error exhibits a slight decrease even through the monitoring cost dramatically increases the absolute errors in the scenarios a1 and a2 show their declines due to the superiority of scenario based stochastic optimization the scenario a2 based solutions indicate the best performance improvement because the absolute errors show dramatic decrease especially for the cost constrained by 60 and 70 this indicates that the significant benefit of surrogate assisted multi objective robust optimization is to achieve the robust monitoring design even in the presence of uncertainty it is noteworthy that the maximum absolute error appears in the zone where there is no monitoring well while the purpose of robust monitoring design is to minimize the absolute error to capture key characteristics of plume under the prescribed monitoring cost in the face of plume uncertainty 5 conclusions in this study we have developed a two stage stochastic optimization framework including uq coupled with sparse pce and the epsilon multi objective noisy memetic algorithm ε monma for optimal design of groundwater monitoring network under k field uncertainty the effectiveness of the proposed stochastic optimization framework was evaluated through a two dimensional hypothetical monitoring design application in the first stage the sparse pce models are trained to replace expensive reactive transport model pht3d to implement uq of concentration at the pre defined monitoring locations the results indicate that sparse pce can successfully approximate the concentration field under conditional k field uncertainty then the surrogate model applied to uq can achieve accurate statistical moments compared with those obtained using numerical model therefore we exploited sparse pce to implement mc simulation and filtered a set of typical plume realizations using the diversity metric in terms of the centroid coordinate of plume in the stochastic optimization it is still computational infeasible to exploit the typical scenarios to evaluate each individual thus the statistical bootstrapping resamples from typical scenario set to generate smaller evaluation scenario suite at the every generation to elucidate the effect of uncertainty on the optimization results we selected the deterministic scenario scenario a0 randomly generated scenario set scenario a1 and surrogate assisted typical scenario set scenario a2 to optimize monitoring network design then ε monma integrates several promising techniques into nga framework to implement multi objective optimization in the face of noisy environment the optimization results indicate that the pareto optimal front in scenario a0 exhibits lowest estimation error and monitoring cost while the solutions in scenario a2 perform the worst the analysis shows that the increase of uncertainty noisy strength deteriorates the performance of monitoring network design then for all pareto optimal solutions from scenarios a0 a1 and a2 the objective values compared to the results against post optimization evaluation with typical scenario set the post optimization analysis shows the contradictory results in which scenario a2 based solutions exhibit optimal performance as shown in figs 9 and 12 in order to further illustrate the performance difference of monitoring design across a range of costs five solutions are selected after post optimization the comparison indicates the surrogate assisted stochastic optimization can achieve the minimum absolute errors of concentration field in the worst evaluation realization therefore the proposed stochastic optimization framework can achieve the largest improvement of robustness of groundwater monitoring design and provide diverse pareto optimal solutions in the high order objective space additionally surrogate assisted uq has been a potential and promising technique for stochastic optimization in the complex and noisy environment system it is noteworthy that although k l expansion can be applied to reduce dimensionality of spatial correlated k field the field scale groundwater reactive transport model always involves multiple three dimensional random filed e g k field porosity field reactive mineral field reactive parameter field which probably results in much more random parameters and inapplicability of uq based on sparse pce future research needs to improve accuracy of surrogate model in addressing high dimensional uq and stochastic multi objective optimization for groundwater monitoring network design meanwhile the improved multi objective noisy evolutionary algorithm not only is useful for groundwater monitoring network design but also can be extended to groundwater remediation or integrated surface groundwater management declaration of competing interest the authors declare that they have no conflict of interest acknowledgements this study was financially supported by the national key research and development plan of china 2016yfc0402800 and the national natural science foundation of china 41730856 and 41772254 the numerical calculations in this paper have been implemented on the ibm blade cluster system in the high performance computing center of nanjing university china also the authors are grateful to the managing guest editor dr behzad ataie ashtiani the reviewer dr j sreekanth and another anonymous reviewer whose constructive suggestions have led to significant improvement of this manuscript 
6250,the robust optimization of groundwater quality monitoring network is subject to many conflicting objectives and high level of uncertainty in hydraulic conductivity this study develops a two stage stochastic optimization framework including the uncertainty quantification using a cheap to evaluate surrogate model and an improved epsilon multi objective noisy memetic algorithm ε monma for monitoring network design the surrogate model based on sparse polynomial chaos expansion pce is constructed to replace expensive simulation model in the uncertainty quantification of concentrations at the pre defined monitoring locations for reducing huge computational cost additionally the scenario discovery strategy using sparse pce model is applied to filter a typical scenario set and the centroid of contaminant plume is used as the diversity metric which avoids enumerating all possible contamination plumes caused by the uncertain k field in the optimization the proposed algorithm is then employed to solve stochastic management model to achieve robust monitoring design indicating the insensitivity of monitoring design to plume uncertainty no matter which of the many possible scenarios becomes the true distribution of contamination under the true k field a synthetic aquifer considering uncertainty in hydraulic conductivity is designed to optimize monitoring network design the pareto optimal solutions to the synthetic example are achieved under three of plume scenario sets defined at deterministic scenario scenario a0 monte carlo based scenario discovery scenario a1 and surrogate assisted scenario discovery scenario a2 respectively comprehensive analysis demonstrates that the monitoring design based on scenario a2 outperforms either of the two designs based on scenarios a0 and a1 in terms of the improvement of robustness of designs evaluated against the typical scenario set meanwhile the performance of monitoring network deteriorates as the uncertainty of plume noisy strength increases indicating the significance of reducing parameter uncertainty in groundwater monitoring design the research findings show that the developed stochastic optimization framework is a computationally efficient and promising tool for multi objective design of groundwater monitoring network under uncertainty keywords multi objective optimization robustness uncertainty quantification surrogate model monitoring network design 1 introduction optimal design of groundwater monitoring network that is capable of providing accurate and informative data is crucial to improve our understanding of complex groundwater systems while reducing huge and unnecessary capital expenditure in general an optimal sampling design needs to consider multiple contradictory objectives for minimizing the costs of data acquisition and maximizing the amount of information in the linked simulation optimization framework kollat and reed 2006 reed and kollat 2013 luo et al 2016 however as a prerequisite for the simulation optimization management model the groundwater flow and transport simulation model inevitably involves the uncertainty associated with hydrogeological parameters which is essentially used to describe and update the state variables in the management model therefore the management models such as one used for optimal design of groundwater monitoring network have to consider model uncertainty derived from model parameters or model structures otherwise they might result in erroneous management decisions wu et al 2006 bayer et al 2010 kollat et al 2011 alzraiee et al 2013 luo et al 2016 the heterogeneity of spatially distributed parameters e g hydraulic conductivity k in the subsurface systems can be characterized using sparse measurement points combined with a geostatistical model in the field studies deutsch and journel 1997 diggle and ribeiro 2007 the traditional geostatistical model utilizes the monte carlo mc simulation to generate a large number of stochastic realizations of parameter fields based on unbiased estimations at the unmeasured locations the uncertainty quantification uq of model outputs can be implemented based on numerous realizations by the simulation model which needs a huge computational expense furthermore stochastic optimization framework generally requires thousands of individual evaluations for each parameter realization in order to search for robust solutions that are insensitive to parameter uncertainty it is computationally prohibitive in the practical application that millions of model evaluations in total need to be implemented when several thousand realizations are generated during evolutionary search motivated by the above mentioned difficulties wu et al 2006 exploited a noisy genetic algorithm nga to evaluate per objective function with the much smaller realization set the nga can be used to find highly reliable and robust solutions using a dynamic random resample strategy in which a set of realizations are randomly resampled at each generation and the prior realizations will be augmented at the later stage of search then singh and minsker 2008 developed the multi objective stochastic optimization coupled with nga after that bayer et al 2010 exploited a stack ordering technique to update realizations in the evaluation subset in order to achieve highly reliable solutions with lower computational burden compared to the random resample scheme however the previous studies only considered several thousand realizations using mc sampling and was unable to implement accurate uq associated with random parameters which should be an essential task for stochastic optimization this challenge in the popular mc method is highly time consuming for uq in the stochastic partial differential equation governing groundwater flow and solute transport the computationally efficient surrogate models have attracted more intention in recent years asher et al 2015 razavi et al 2012 surrogate assisted uq exploits much smaller sample set to train approximate model and fully replaces expensive simulation model for quantifying uncertainty of model outputs many surrogate models have been successfully applied to uq and obtained credible approximate results compared to cpu intensive mc simulation with the groundwater models müller et al 2011 zhang et al 2013 crevillen garcia et al 2017 meng and li 2017 mo et al 2017 polynomial chaos expansion pce is an effective alternative surrogate method which has been extensively used in the uq rajabi et al 2015 bazargan et al 2015 meng and li 2017 the pce method constructs a series of orthogonal polynomial basis functions to represent the quantities of interest based on independent random inputs with different probability distributions xiu and karniadakis 2002a ghanem and spanos 2003 furthermore non intrusive pce is more attractive due to the permission of using a well validated simulator that can be treated as a black box to train the surrogate model in this study we utilize stepwise regression and least angle regression lar proposed by blatman and sudret 2010 2011 to reduce the number of regression points in the face of higher random dimensionality or higher degree of polynomial equations as long as the pce coefficients are achieved by the stepwise regression the statistical moment of quantities of interest can be easily obtained spatial correlated k field is a critical physical property that always dominates the process of groundwater flow and solute transport also karhunen loève k l expansion can be applied to transform k field into low dimensional random subspace and was used to construct highly efficient surrogate model integrated with the pce li and zhang 2007 2013 meng and li 2017 indicating that the k l technique is promising and effective due to its ability to significantly reduce dimension of random parameters accordingly in this study we employ the sparse pce model constructed by lar algorithm to implement uq at the potential monitoring locations under k field uncertainty surrogate assisted uq is the first stage in the stochastic optimization framework for groundwater monitoring network design to quantify uncertainty in the optimization scenario based multi objective evolutionary algorithm moea has been exploited to implement robust optimization e g singh and minsker 2008 sreekanth and datta 2011 luo et al 2014 2016 beh et al 2015 sreekanth et al 2016 yang et al 2017 sankary and ostfeld 2018 however evaluation of evolutionary individuals against all possible operational scenarios may be computationally impractical therefore a random resample strategy that selects much smaller ones from a set of typical scenarios capable of approximating the feature of all possible scenarios is developed in the optimization framework sankary and ostfeld 2018 developed a random resampling scheme that used the advantage of statistical bootstrapping to achieve reliable estimates of the uncertainty on the unseen samples in the scenario based moea their results indicated that the moea coupled with the dynamic resample strategy could possess the ability to create robust solutions under unknown operational environments the study generated a typical scenario set by setting resolution of centroid coordinates of a large number of plume scenarios from surrogate assisted scenario discovery and utilized statistical bootstrapping technique to resample in every generation the groundwater monitoring network design often involves many objectives typically more than three objectives which may result in the domination resistance phenomenon purshouse and fleming 2007 hadka and reed 2013 the phenomenon shows that the capacity of pareto sorting with distinguishing optimal solutions weakens and leads to a large number of non dominated solutions i e none of the objective values could be improved without degrading one or more of the other objective values existed in population hadka and reed 2013 developed an auto adaptive moea framework named borg which combines several promising techniques to improve the performance of the algorithm in addressing multi objective optimization in order to enhance the local optimality of solutions memetic algorithms composed by natural evolution based on darwinian principles and cultural evolution capable of local refinements were applied to speed up the convergence of moea sindhya et al 2011 2013 in this study we employ the ε dominance concept and a modified auto adaptive multi operators recombination to alleviate domination resistance and a bidirectional neighborhood mutation operator to enhance the local optimality of archived solutions then we integrate these techniques and the fast non dominated sort process of nsga ii deb et al 2002 to the framework of nga wu et al 2006 therefore the epsilon multi objective noisy memetic algorithm ε monma possesses the ability of highly effective global search in addressing many objective optimization under noisy environment moreover ε monma integrates the surrogate assisted uq into a unified optimization framework so as to implement accurate uq at the cost of acceptable computation burden finally we demonstrate the availability and reliability of the framework in the uncertainty based groundwater monitoring network design through a synthetic reactive transport aquifer system 2 methodology fig 1 illustrates the general flowchart to implement robust optimization first we have to construct a groundwater flow and reactive transport model and implement uq with sparse pce for the contaminant concentration at the monitoring wells then we exploit a scenario discovery strategy to generate a typical scenario set and perform stochastic multi objective optimization by ε monma in this section we only present the details of aforementioned modules as for the post optimization analysis of the pareto optimal solution set it can be found in section 4 2 2 1 groundwater flow and reactive transport model the reactive transport model was constructed using a finite difference code pht3d v2 10 prommer and post 2010 the program is designed to simulate three dimensional advective dispersive multicomponent reactive transport process and had been benchmarked and applied to complex reactive networks prommer et al 2006 2009 rodriguez escales et al 2017 pht3d integrates the multicomponent transport model mt3dms zheng and wang 1999 with geochemical model phreeqc 2 parkhurst and appelp 1999 using a sequential operator splitting technique therefore the numerous reactive networks can be coupled with groundwater flow and transport model in the aquifer system considering spatial distributed physical heterogeneity and reactive mineral at field scale is essential for improving fidelity and reliability of numerical model and providing understanding of the influence of aquifer characteristics on contaminant transport which is significant for the optimal design of groundwater monitoring network 2 2 sparse polynomial chaos expansion 2 2 1 polynomial chaos expansion pce is a widely used metamodeling method that provides a functional approximation of a computationally expansive model for propagating uncertainty ξ ξ 1 ξ 2 ξn is a set of independent random variables that follows the specific joint probability density function pdf where n is the dimension of random variables let m ξ represent simulation model pce model can be defined as blatman and sudret 2011 1 y m ξ α n n λ α ψ α ξ where ψ α ξ is the basis function expressed as the multivariate orthogonal polynomials in the stochastic space λ α is the corresponding expansion coefficient and α is a multi index that identifies the components of the polynomials particularly for α 0 λ 0 denotes the first expansion coefficient the multivariate orthogonal polynomial can be combined with the univariate polynomials owing to the independence of ξ 2 ψ α ξ i 1 n ω α i ξ i where ω α i denotes the univariate polynomial of degree αi in ith variable dimension the type of ω α i is selected according to the probability distributions of ξ which adopts the scheme proposed by askey and wilson 1985 to map the distribution of ξ e g gaussian gamma or beta to the optimal polynomial e g hermite laguerre or jacobi polynomial in case that ξ follows arbitrary distribution the univariate polynomial can be constructed by generalized pce xiu and karniadakis 2002a b for realistic application eq 1 is often truncated to a limited number of terms by assigning the expansion order d and modified as 3 m ξ m t ξ α a λ α ψ α ξ where a is a candidate subset satisfying the constraint of α i 1 n α i d in the truncated pce there are p p n d n d orthogonal polynomials and regression coefficients to be solved once the pce models are constructed the mean μ and variance σ 2 of quantities of interest can be respectively calculated by the following equations 4 μ y λ 0 5 σ 2 y α a α 0 λ α 2 2 2 2 adaptive sparse pce the evaluation of pce coefficients which can be solved by intrusive or nonintrusive methods is the key issue to build exact and reliable surrogate model the intrusive approach must insert pces into the governing equation of numerical model and significantly modify the well validated model codes which is less attractive for industry abraham et al 2017 however the non intrusive approaches such as the projection and regression methods pettersson et al 2015 require no modification to the existing deterministic codes eldred and burkardt 2009 hosder 2012 in this study we focus on the regression approach that establishes an over determined linear system of equations by minimizing the least square criterion for estimating polynomials expansion coefficients however the main drawback of the regression method is that the computational burden caused by the evaluation of numerical model drastically increases due to the increase of random variable dimension and polynomial expansion degree to mitigate the above mentioned problem we employ the sparse regression method blatman and sudret 2008 2010 2011 which sequentially determines the most important basis functions from few training samples to build computationally efficient surrogate model lar efron et al 2004 is a heuristic regression method and designed to search the most influential basis functions on the model responses over a large number of candidates after detecting the relevant basis polynomials using lar the coefficients corresponding to the retained basis polynomials are calculated using ordinary least square regression finally the sparse pce is constructed and the number of optimal basis functions is much lower than the classical full representation to avoid the improper construction of a prior truncation scheme the basis adaptive algorithm is proposed to enhance sparse pce approximation by increasing the maximum polynomial degree blatman and sudret 2011 integrated sparse pce with the full degree adaptive algorithm to develop a powerful and effective surrogate model as implemented in the uqlab marelli and sudret 2014 the present study employed uqlab to construct sparse pce model for uq of concentration after that the surrogate model was coupled with ε monma for robust optimization of monitoring network design 2 3 multi objective robust optimization of groundwater monitoring design 2 3 1 stochastic representation of k field the k field can be represented by y x θ ln k x θ where x d θ θ and assumed to be the second order stationary gaussian random field we can write y x θ y x y x θ where y x and y x θ are mean and fluctuation respectively the spatial structure of random field is expressed by covariance c y x y y x θ y y θ which is bounded symmetric and positive definite and can be decomposed as ghanem and spanos 2003 6 c y x y i 1 λ i f i x f i y where λi and fi x are the eigenvalue and eigenfunction respectively they can be solved from the following fredholm equation 7 d c y x y f x d x λ f y in most instances the eigenpairs are solved numerically while analytic and semi analytic solutions exist under certain condition zhang and lu 2004 then the random field can be stated as 8 y x θ y x i 1 λ i f i x ξ i θ where ξ i θ are the orthogonal random variables eq 8 is known as k l expansion the variance of output variables can be calculated with σ y 2 i 1 λ i f i 2 x which can be transferred into d σ y 2 i 1 λ i where d is a measure of physical domain size for practical applications eq 8 needs to be truncated into finite terms the number of retained terms depends on the convergence rate of eigenvalue summation that is to say i 1 n λ i d σ y 2 has to guarantee relatively higher proportion in the field scale study the known k values are obtained at sampling locations the conditional simulation can be implemented in the handing of k field uncertainty assume n known k values y 1 y 2 y n located at x 1 x 2 x n the conditional mean and covariance of y can be stated as follows lu and zhang 2004 9 y c x y x i 1 n μ i x y x i y x i 10 c y c x y c y x y i j 1 n μ i x μ j y c y x i x j 11 c y x x j i 1 n μ i x c y x i x j j 1 2 n where the mean function μi x needs to be solved using the kriging equation eq 11 the conditional covariance function is nonstationary and the corresponding eigenvalues and eigenfunctions have to be solved numerically using the fredholm function lu and zhang 2004 12 d c y c x y f c x d x λ c f c y the conditional realization of k field can be implemented using k l expansion similar to the unconditional simulation and stated as 13 y c x θ y c x i 1 λ i c f i c x ξ i θ likewise we can truncate the k l expansion to reach the desired accuracy using the largest n eigenvalues which significantly reduces the dimensionality of the parameter space meanwhile we assume the log transformed hydraulic conductivity has a separable exponential covariance function at two different location xx xy and yx yy 14 c y x x x y y x y y σ y 2 exp x x y x η x x y y y η y where σ y 2 is the variance ηx and ηy are the correlation lengths along x and y directions respectively 2 3 2 management model the goals of groundwater monitoring network design aim at minimizing monitoring costs and maximizing the accuracy of estimated contamination plume in this study the four objectives are employed to minimize i monitoring cost f cost ii mass estimation error f mass iii first moment estimation error f 1st and iv concentration estimation error f conc the ordinary kriging method is used to interpolate concentration estimation at no sampling location throughout the plume wu et al 2005 2006 the mathematical formulation of management model can be stated as luo et al 2016 15 minimize f x ω f cost x ω f mass x ω f 1 s t x ω f conc x ω 16 subject to u x ω ω 0 where xω xω 1 xω 2 xω s xω s is sth sample location of ωth monitoring scheme if sth well is sampled xω s 1 otherwise xω s 0 to avoid too few sample data in the search radius of kriging to perform interpolate the number of unestimated point u xω in the specified domain is 0 the objective of monitoring cost can be described as 17 f cost x ω s 1 n α s x ω s where αs is the cost to the sth sampling location n is the number of pre defined potential monitoring wells in this study the cost coefficient αs is uniformly set to 1 the error objective functions in terms of percentage can be stated as kollat and reed 2006 18 f mass x ω m 0 a l l m 0 ω m 0 a l l 100 19 f 1 st x ω j δ 1 a l l j δ 1 ω j δ 1 a l l j 100 20 f conc x ω i 1 g c all u i c ω u i i 1 g c all u i 100 where m 0 all is the total mass of contaminant plume i e zeroth spatial moment interpolated with all pre defined monitoring wells m 0 ω is the total mass calculated under the ωth sampling scheme δ 1 a l l j and δ 1 ω j are the estimated center of contaminant plume i e first spatial moment based on all potential monitoring wells and the ωth sampling design j denotes the direction along which the moment is computed call ui and cω ui are contaminant concentrations at ith grid cell with sampling all monitoring locations and the ωth sampling plan respectively g is the number of active cell in the model domain this study is aimed at developing multi objective robust optimization framework for the monitoring network design in the presence of k field uncertainty thus the objective functions have to be modified across multiple stochastic realizations in the evolutionary optimization the min max formulation deb et al 2015 sankary and ostfeld 2018 showing the best objective performance in the worst evaluation realization can be used to identify the robust optimal monitoring scheme as follows 21 minimize f θ x ω k f cost x ω max i r s f mass x ω k i max i r s f 1 st x ω k i max i r s f conc x ω k i where ki is ith k field realization rs is the realization set being evaluated f θ xω k is the maximum objective value over entire realization set corresponding to design xω the estimated plume is interpolated using ordinary kriging under specified k field realization and then the error objectives are achieved in this study we use sparse pce to output logarithmic concentration of all sampling locations in the evaluation realization set as follows 22 ln c i f pce k i i r s 2 3 3 multi objective robust optimization framework the complex or field scale groundwater monitoring design requires the consideration of uncertain characteristics e g hydraulic conductivity in the aquifer however enumerating all possible stochastic realizations or scenarios is computationally infeasible especially for transport problem coupled with complex reactive processes moreover the optimal solutions under noisy environments have to be evaluated against many scenarios to guarantee the robustness and reliability which leads to tremendous computational burden in the optimization thus this study proposes a surrogate assisted ε monma to optimize the monitoring network under parameter uncertainty as shown in fig 2 the algorithm includes two main modules i e the surrogate assisted stochastic scenario discovery and the epsilon multi objective noisy memetic algorithm the details of the algorithmic implementation process are described below module i surrogate assisted scenario discovery the purpose of module i is to provide the evaluation scenarios which can represent all possible plume scenarios caused by the uncertain k field as far as possible first a set of sparse pce models are trained to predict concentration in the candidate monitoring locations then mc simulation is implemented to generate a large number of evaluation scenarios p o which are very close to all possible scenarios next the centroid coordinate of contamination plume is considered as diversity index to filter a smaller set of evaluation scenarios pe finally pe is used as a typical scenario set in the noisy evolutionary algorithm if an individual is evaluated against each realization in the typical scenario set pe the total computational burden is still unacceptable owing to tens of millions of kriging interpolation running thus we exploit bootstrapping technique to stochastically resample from pe to achieve smaller scenario set ps evaluating for each individual and quantify the effect of uncertainty on the optimal solutions during evolutionary search the statistical bootstrapping resamples with replacement from the typical scenario set and can estimate the typical scenarios overall statistical measures with a smaller sample set which has been widely used in the genetic programming fitzgerald et al 2013 module ii integrated multi objective noisy memetic algorithm the proposed ε monma includes several design elements from the existing borg moea hadka and reed 2013 and a newly added neighborhood mutation operator to implement local search which coupled with the framework of the noisy genetic algorithm nga wu et al 2006 and the fast non dominated sort process in the nsga ii deb et al 2002 the implementation processes of the modules are presented as follows step i initialization of population and input parameters the initial population is generated using latin hypercube sample lhs and the key parameters include population size npop maximum function evaluation number neval crossover probability pc mutation probability pm the epsilon resolution of objectives ε cost ε mass ε 1st ε conc the number of evaluation scenario for each candidate solution ns step ii sampling of a set of plume scenarios and computation of objective function the ns sized scenario set ps is resampled by bootstrapping technique for each generation the min max formulation is applied to compute maximum error objective across multiple scenarios in the robust optimization step iii multi operator recombination and polynomial mutation the auto adaptive multi operator recombination proposed by hadka and reed 2013 is a promising technique to select optimal operator for various optimization problem the crossover probability of each operator is updated periodically based on the proportion of the solutions generated by each operator in the ε dominance archive the recombination strategy is essential for the complex multi objective and real world optimization due to the inability to known a prior the optimal recombination operator this study attempts to integrate the multiple recombination operators i e simulated binary crossover differential evolution simplex crossover parent centric crossover laplace crossover uniform mutation into ε monma to enhance search ability step iv ε dominance archive the multi objective optimization exists a domination resistance phenomenon which results in a widespread dispersal of local non dominated solutions in the population purshouse and fleming 2007 hadka and reed 2013 the defect may slow down the convergence speed of evolutionary search and then the diversity metric becomes a primary selection operator to obtain an approximate pareto set away from the true global tradeoff surface the ε dominance archive developed by laumanns et al 2002 attempts to ensure convergence and diversity in the multi objective optimization moreover the decision makers can define the minimum resolution of objective vector f f 1 f 2 f n with epsilon vector ε ε 1 ε 2 ε n to satisfy their acceptable precision target and restrict the archive size the several advanced multi objective algorithms e g ε moea ε nsga ii borg moea have used the ε dominance archive strategy to upgrade evolutionary search and obtained better convergence and diversity performance than traditional moeas deb et al 2003 kollat and reed 2007a hadka and reed 2013 this study implemented the ε dominance archive process after the fast non dominated sorting of offspring individuals and alleviated the difficulties derived from the domination resistance step v bidirectional neighborhood mutation the retained individuals in the archive will be mutated based on gaussian perturbation in the neighborhood of solution given an archived individual ia s 1 s 2 s 3 s n the mutated solutions can be stated as 23 i a s 1 s 2 s i c u i r i s n 24 i a s 1 s 2 s i c u i r i s n where s s 1 s 2 s n is a n dimensional decision variable vector u u 1 u 2 un and r r 1 r 2 rn are the two solutions stochastically selected from the archive c is a perturbation factor and follows a standard gaussian distribution this study revived the mutation operator in every several generations of evolutionary search the cheap to evaluate surrogate model is constructed to perform uncertainty analysis of monitoring concentration with the brute force mc simulation then a set of contamination plumes are generated to represent all possible scenarios caused by the uncertainty in k field as much as possible the plume centroid is also adopted as the diversity metric to filter a typical scenario set for robust optimization afterward ε monma is applied to optimize monitoring network design under the typical scenario set therefore the methodology can provide the most robust monitoring network design in terms of preferred objectives while fully investigate the effect of uncertain k field on the decision making behaviors 3 application to robust optimal design of groundwater monitoring network 3 1 description of the synthetic aquifer system the model domain of hypothetical unconfined aquifer is 600 m in longitudinal extent 400 m in transverse extent and a model thickness of 10 m the aquifer modeled in this study is discretized into a uniform model grid of 40 rows and 60 columns as shown in fig 3 the contaminant source comprises immobile lnapl hydrocarbon compounds benzene in this study to account for the geochemical reaction processes driven by biodegradation a site specific reaction network was developed to explain the relevant conceptual model and then coupled with the groundwater flow model to simulate contaminant transport the non equilibrium dissolution of lnapl is controlled including heterogeneous porous media and morphology of source zone which is empirically formulated as a kinetic reaction model mayer and miller 1996 25 m dis β c org sat c org 26 c org sat c org sol γ org n org where β is the mass transfer coefficient c org is the concentration of the organic compound in the groundwater c org sat is the equilibrium aqueous concentration for each organic compound and calculated with eq 26 c org sol is the pure organic compound solubility γ org is the activity coefficient and typically assumed to be unity n org is the molar fraction in the lnapl the reactive transport process of contaminant considered the complete oxidation reduction sequence i e aerobic degradation nitrate reducing manganese reducing iron reducing sulfate reducing and methanogenesis which was simulated by a two step partial equilibrium reaction approach brun and engesgaard 2002 prommer et al 2006 colombani et al 2009 thus the reduction reaction in response to the aqueous and mineral form electron acceptors can be simply simulated using the reactive simulator pht3d prommer and post 2010 the distribution of k in the aquifer is assumed to be the log normally distributed isotropic and spatially correlated random field that follows an exponential covariance function the k field based on the 30 measured data is conditionally simulated using k l expansion solid squares in fig 3 the management period is set to three years for groundwater monitoring network design the plume uniformly distributes over the pre defined locations of potential monitoring wells at the end of the period for the majority of realizations the primary parameters used in the flow and reactive transport model are listed in table 1 the initial concentration used in the reactive transport model can be found in colombani et al 2009 3 2 implementation of stochastic optimization the optimization objectives are minimization of the cost of monitoring network design and plume estimation error in terms of total mass centroid and concentration estimation error under k field uncertainty the log k field is represented with the k l expansion conditioning to the 30 measured k values the infinite terms in the k l expansion are truncated for the dimensionality reduction of random variables at the certain energy level the higher number of retained terms preserves more detailed heterogeneous feature while the smaller scale characteristic in the k field has less influence on the solute transport in the present example we select first the 50 terms to preserve 73 energy and the random field can be expressed by a 50 dimensional standard gaussian random vector ξ ξ 1 ξ 2 ξ 50 then the sparse pce models are constructed using ξ as inputs and concentrations as output at pre defined monitoring locations lhs is used to generate 400 original samples to run pht3d and constantly augment sample set with 100 new samples until the desired accuracy is reached totally the 1 500 samples are selected to train sparse pce models the proposed algorithm utilizes sparse pce models to filter the representative plume scenario set to consider the influence of uncertainty in the k field we perform 250 000 times mc simulations based on sparse pce which is just calculation of polynomial to output concentration at the pre defined locations with high computational efficiency then the total plume scenario set p o interpolated with ordinary kriging is achieved to represent all possible plume scenarios the plume centroid is used as the diversity indicator and users can specify its coordinates precision ζ to split centroid space into many course or fine grids which depends on ζ value grid blocks containing multiple centroid coordinates prefer the centroid closet to the center of the block therefore we can select a smaller typical scenario set p e from p o while the plume scenarios diversity can be preserved as feasibly as possible the centroid coordinates resolution ζ along the x and y directions is set to 2 0 m which finally filters 2 663 plume scenarios from p o into p e for each individual evaluation an evaluation suite p s of 100 plume scenarios are randomly resampled from the scenario set p e using bootstrapping method at each generation the epsilon resolutions for the four objectives i e ε cost ε mass ε 1st ε conc are set to 1 0 0 001 0 001 and 0 001 respectively the bidirectional mutation operator is applied to the epsilon archived individuals in every ten generations the parameters used in the ε monma are listed in table 2 4 results and discussion 4 1 uncertainty quantification 4 1 1 assessment of surrogate accuracy to achieve the desired approximate accuracy of sparse pce models the 1 500 samples were generated to train surrogate model and additional 100 samples were randomly selected to test the performance the root mean square error rmse and correlation coefficient r are used to evaluate the prediction accuracy on the test dataset fig 4 shows the predicted concentration at 112 monitoring locations are in good agreement with results from the forward model predictions the plumes interpolated with monitoring concentration based on sparse pce are compared with those using pht3d from the three randomly selected test samples as shown in fig 5 the sparse pce models can accurately estimate concentration field from the simulation model the maximum value in the absolute error field primarily arises in the region with greater concentration gradient where the complex nonlinear response feature exists however after enriching training sample size from 400 to 1 500 the rmse value is only about 1 of the maximum value in the contamination field and r value increases to 0 998 therefore sparse pce is capable of estimating the morphology of contamination plume and the computational burden associated with uq e g mc simulation can be alleviated significantly 4 1 2 uncertainty analysis the statistical moment e g mean and variance of the log concentration can be analytically obtained by sparse pce due to the orthogonality of the basis functions to validate the effectiveness of uq using surrogate models we randomly generate 30 000 mc realizations by running pht3d model to compute the reference statistical moment fig 6 compares the mean and variance calculated by sparse pce with the pht3d based reference values in the pre defined monitoring locations it can be observed that both means and variances of log concentration of monitoring wells are very close to the reference values from the brute force mc simulation by running mc sampling on the surrogate model and simulation model the probability density function pdf at monitoring location can be achieved the estimated pdf at the selected monitoring wells is shown in fig 7 the estimations with sparse pce trained by 1 500 samples are quite consistent with the reference pdf with 30 000 mc experiments by running pht3d simulator while the results with 1 500 mc samplings exist a large bias the cpu time of surrogate model running with mc experiments is negligible and the runtime of a pht3d run for each k field realization is similar therefore the proposed surrogate assisted algorithm can significantly improve the computational efficiency up to approximately 95 1 1 500 30 000 of time saving at the stage of uq the above analysis shows that the sparse pce can efficiently provide accurate approximation of statistical moments and be integrated into robust optimization framework for groundwater monitoring network design 4 2 assessment of the optimization results 4 2 1 pareto optimal front the typical scenario set p e with 2 663 samples is selected by the surrogate assisted uq and the diversity metric with plume centroid to compare the effect of k field uncertainty on the quality of solutions the deterministic scenario i e scenario a0 and randomly generated 2 663 scenarios i e scenario a1 also are considered as the evaluation scenario suite during the optimization in comparison to the typical scenario set i e scenario a2 the plume centroid coordinates of all scenarios from scenarios a0 a1 and a2 are illustrated in fig 8 the figure shows that surrogate assisted scenario discovery can obtain a maximally diverse set of plume scenarios which means the accurate uncertainty propagation derived from k field uncertainty in optimization phase this study performed three optimization with three scenario discovery processes to search optimal monitoring network design to visualize and analyze the multi objective tradeoff of pareto optimal monitoring designs we utilized the discoverydv software package developed by kollat and reed 2007b fig 9 shows that the multi objective tradeoffs under scenarios a0 a1 and a2 respectively in this figure each sphere represents a pareto optimal monitoring network design sphere size indicates the concentration estimation error and the green sphere is an ideal solution as shown in fig 9 the small decrease in monitoring cost can result in large increase in three of error objective especially when the monitoring cost is less than 50 the objective of relative concentration error reach to 50 exists more sensitive tradeoff response to monitoring cost in comparison to the objectives of relative moment error and mass error it can be noted that the pareto optimal front in scenario a0 i e deterministic scenario exhibits the lowest estimation error and monitoring cost while the solutions under k field uncertainty i e scenarios a1 and a2 appear to be worse performance in which scenario a2 based designs perform the worst the reason is that the probability of these solutions improving monitoring design decreases as the uncertainty of plume increases however the better solutions with insufficient uncertainty propagation may be unfeasible or low performance in the future to further explain the discrepancy of pareto solutions from scenarios a0 a1 and a2 we use the parallel coordinates plot inselberg 2009 to illustrate the trade surface in fig 10 the objective values of all pareto optimal solutions are illustrated in line segments and the concentration estimation error is represented by the color the solutions with lower monitoring cost appear to be higher values of relative errors especially for the concentration error as shown in fig 10 it can be observed that the range of f cost values is similar while those of objective values of estimation errors exist larger difference the normalized error objective values in scenario a0 have maximum of 0 34 0 38 and 0 57 for mass first moment and concentration respectively which are lower than those computed by scenarios a1 and a2 the variation of error metrics based on scenario a2 is relatively larger than those based on scenario a1 for example more dark red line segments larger concentration error joining in the parallel axes the results show that the noise strength caused by k field uncertainty has adverse effect on the performance of monitoring network in the optimization even through the same monitoring expense is invested therefore reducing model parameter uncertainty is essential for stakeholders to implement robust design of monitoring network 4 2 2 robustness analysis robustness indicator measures how far the performance i e relative error objectives of monitoring network deviates from the expectation value in the noisy environment the relative error objectives of pareto optimal solutions in scenarios a0 a1 and a2 are calculated against the typical scenario set p e which also called post optimization process the relative deviation rd of error objectives is utilized as robustness metric and can be stated as 27 r d mass j f mass p r e x w k f mass p o s t x w k f mass p o s t x w k 28 r d 1 s t j f 1 s t p r e x w k f 1 s t p o s t x w k f 1 s t p o s t x w k 29 r d conc j f conc p r e x w k f conc p o s t x w k f conc p o s t x w k where fmass pre xw k represents the maximum relative mass error objective of solution j corresponding to scenarios a0 a1 or a2 fmass post xw k is the maximum of relative mass error objective evaluated across the typical scenario set after post optimization xw is a solution for monitoring network design k is a hydraulic conductivity realization set the definition of other symbols is similar to relative mass error and represents relative deviation of first moment error and concentration error rd metric measures a solution s deviation from its performance in the baseline or expected state the typical scenario set is considered as reference noisy environment lower rd value indicates lesser effect of k field uncertainty on the performance of monitoring network fig 11 illustrated the results in the robustness analysis using rd metric subfigures a c show rd of mass estimation error f mass first moment estimation error f 1st and concentration estimation error f conc based on scenarios a0 a1 and a2 respectively it is observed that the solutions achieved using scenario a0 obviously yield substantially larger rd which means worst performance in the robustness while the solutions obtained using scenarios a1 and a2 exhibits lower value overall the distribution of rd in terms of error objectives in scenario a2 is relatively lower than that in scenario a1 the average values of rd metric are listed in table 3 it is obvious that mean of rd in the scenario a2 is smaller than scenarios a0 and a1 and average value in scenario a0 is substantially larger consequently the optimal designs in scenario a2 are more robust than those of scenarios a0 and a1 when evaluated against typical scenario set importantly decision maker need consider accurate uq of contaminant transport under k field uncertainty fig 12 shows the distribution of pareto optimal solutions after post optimization as illustrated in fig 12 the solutions in scenario a0 show largest relative error objectives and cost while the monitoring designs based on scenario a2 exhibit lowest objectives the results contradict with those illustrated in fig 9 which shows the performance of solutions from scenarios a0 and a1 exists obvious deterioration owing to uncertainty propagation to compare pareto optimality of the tradeoff solutions objectives of all the optimal designs using min max formulation are rearranged using fast non domination sorting given in table 4 is the comparison of the non dominated solutions from scenarios a0 a1 and a2 and those after post optimization as shown in table 4 before post optimization there are 96 4 of non dominated solutions from scenario a0 only 2 4 and 1 2 of those from scenarios a1 and a2 respectively however after post optimization using typical scenario set there are 35 7 and 59 1 of non dominated solutions from scenarios a1 and a2 only 5 2 of those from scenario a0 table 4 shows that surrogate assisted scenario a2 can achieve relatively larger pareto optimal solution set than randomly selected scenario a1 and much larger solution set than scenario a0 under k field uncertainty it is obvious that surrogate assisted optimization based pareto optimal designs can obtain robust performance while those achieved by the stochastic optimization with mc sampling exhibit worse monitoring performance and the designs in the deterministic scenario demonstrate dramatic performance degradation 4 2 3 monitoring schemes analysis to explore the performance difference of monitoring design under scenarios a0 a1 and a2 we selected five typical solutions across a range of monitoring costs after post optimization table 5 shows the maximum relative error objective values using the min max formulation comparison of the objective values associating with scenarios a1 and a2 indicates that increasing monitoring costs of solutions 1 5 would lead to the decline of relative error objectives the relative error objectives in scenario a0 also decreases as the increase of the monitoring costs except that the concentration error objectives of solutions 4 and 5 have a slight rise the relative error objectives in scenario a2 are smaller than those in scenarios a1 and a0 under the same monitoring investment the result demonstrates the performance metrics of the monitoring designs with accurate uq based on surrogate model dominate those without full consideration of uncertainty fig 13 shows the absolute error distributions between the plumes interpolated by the data from monitoring schemes in the solutions 1 5 and the reference plumes interpolated using all the potential monitoring concentrations under scenarios a0 a1 and a2 comparing the absolute error distributions in the different optimization scenarios demonstrates the failure of scenario a0 based designs due to the absolute error exhibits a slight decrease even through the monitoring cost dramatically increases the absolute errors in the scenarios a1 and a2 show their declines due to the superiority of scenario based stochastic optimization the scenario a2 based solutions indicate the best performance improvement because the absolute errors show dramatic decrease especially for the cost constrained by 60 and 70 this indicates that the significant benefit of surrogate assisted multi objective robust optimization is to achieve the robust monitoring design even in the presence of uncertainty it is noteworthy that the maximum absolute error appears in the zone where there is no monitoring well while the purpose of robust monitoring design is to minimize the absolute error to capture key characteristics of plume under the prescribed monitoring cost in the face of plume uncertainty 5 conclusions in this study we have developed a two stage stochastic optimization framework including uq coupled with sparse pce and the epsilon multi objective noisy memetic algorithm ε monma for optimal design of groundwater monitoring network under k field uncertainty the effectiveness of the proposed stochastic optimization framework was evaluated through a two dimensional hypothetical monitoring design application in the first stage the sparse pce models are trained to replace expensive reactive transport model pht3d to implement uq of concentration at the pre defined monitoring locations the results indicate that sparse pce can successfully approximate the concentration field under conditional k field uncertainty then the surrogate model applied to uq can achieve accurate statistical moments compared with those obtained using numerical model therefore we exploited sparse pce to implement mc simulation and filtered a set of typical plume realizations using the diversity metric in terms of the centroid coordinate of plume in the stochastic optimization it is still computational infeasible to exploit the typical scenarios to evaluate each individual thus the statistical bootstrapping resamples from typical scenario set to generate smaller evaluation scenario suite at the every generation to elucidate the effect of uncertainty on the optimization results we selected the deterministic scenario scenario a0 randomly generated scenario set scenario a1 and surrogate assisted typical scenario set scenario a2 to optimize monitoring network design then ε monma integrates several promising techniques into nga framework to implement multi objective optimization in the face of noisy environment the optimization results indicate that the pareto optimal front in scenario a0 exhibits lowest estimation error and monitoring cost while the solutions in scenario a2 perform the worst the analysis shows that the increase of uncertainty noisy strength deteriorates the performance of monitoring network design then for all pareto optimal solutions from scenarios a0 a1 and a2 the objective values compared to the results against post optimization evaluation with typical scenario set the post optimization analysis shows the contradictory results in which scenario a2 based solutions exhibit optimal performance as shown in figs 9 and 12 in order to further illustrate the performance difference of monitoring design across a range of costs five solutions are selected after post optimization the comparison indicates the surrogate assisted stochastic optimization can achieve the minimum absolute errors of concentration field in the worst evaluation realization therefore the proposed stochastic optimization framework can achieve the largest improvement of robustness of groundwater monitoring design and provide diverse pareto optimal solutions in the high order objective space additionally surrogate assisted uq has been a potential and promising technique for stochastic optimization in the complex and noisy environment system it is noteworthy that although k l expansion can be applied to reduce dimensionality of spatial correlated k field the field scale groundwater reactive transport model always involves multiple three dimensional random filed e g k field porosity field reactive mineral field reactive parameter field which probably results in much more random parameters and inapplicability of uq based on sparse pce future research needs to improve accuracy of surrogate model in addressing high dimensional uq and stochastic multi objective optimization for groundwater monitoring network design meanwhile the improved multi objective noisy evolutionary algorithm not only is useful for groundwater monitoring network design but also can be extended to groundwater remediation or integrated surface groundwater management declaration of competing interest the authors declare that they have no conflict of interest acknowledgements this study was financially supported by the national key research and development plan of china 2016yfc0402800 and the national natural science foundation of china 41730856 and 41772254 the numerical calculations in this paper have been implemented on the ibm blade cluster system in the high performance computing center of nanjing university china also the authors are grateful to the managing guest editor dr behzad ataie ashtiani the reviewer dr j sreekanth and another anonymous reviewer whose constructive suggestions have led to significant improvement of this manuscript 
6251,although previous studies have estimated the effects of human regulations on hydrological drought few studies have examined the impacts from the perspective of the development and recovery processes of hydrological drought this study uses the dongjiang river basin in south china as an example and focuses on the influence of reservoir operation on hydrological drought the standardized streamflow index ssi and run theory were applied to determine the development and recovery processes of hydrological drought a semi distributed hydrological model the soil and water assessment tool swat was used to simulate long time series 30 years monthly streamflow data the swat model was calibrated and validated using the monthly streamflow data from periods with less human regulations streamflow was then simulated and compared by using the same model parameters as well as the sub surface features to simulate the streamflow during the human regulation periods the simulated observed comparative approach i e a comparison of the characteristics of the hydrological drought obtained from the simulation based ssi and observation based ssi was used to assess the impacts of human regulations on hydrological drought development and recovery the results showed that the simulated observed comparison approach based on the swat model exhibited a good ability to ascertain the impacts of human regulations on hydrological drought development and recovery the reservoir operation was the dominant factor affecting the hydrological drought propagation in the lower reaches of the dongjiang river basin and had mostly a short term effect by decreasing the duration and severity of drought development or recovery however the reservoirs tended to release water during the development process and to store water during the recovery process during a long term hydrological drought in the study area leading to an increase in average recovery duration and severity the results of this study may further optimize water resources management system during drought for effective drought prevention and mitigation keywords hydrological drought development and recovery reservoir operation swat dongjiang river basin 1 introduction droughts are recurrent natural hazards and may occur in any climatic region mishra and singh 2010 recent studies have reported on the trends in the frequency of severe droughts over the past decades huang et al 2016 and several studies have shown that an increase in the drought frequency has significantly affected the food supply water security and the environment wernberg et al 2013 the investigation of drought propagation is of great importance for regional eco environmental safety and water related disaster management against the background of a changing environment droughts are commonly divided into different types based on the phase of the water shortage in the hydrological cycle i e meteorological drought agricultural drought hydrological drought and socioeconomic drought american meteorological society 1997 world meteorological organization 2006 the first phase of drought is the meteorological drought which refers to a deficiency of precipitation in a certain period the continuation of the meteorological drought may result in a deficiency in soil moisture and is then called the agricultural drought further precipitation shortage leads to a decrease in water in rivers lakes and reservoirs this phase is referred to as hydrological drought and is regarded as a thorough drought dracup et al 1980 the change from a meteorological drought to a hydrological drought is commonly regarded as drought propagation apurv et al 2017 however drought propagation not only occurs among different drought types e g from meteorological to hydrological drought but also within a certain drought type which is called internal propagation parry et al 2016a wu et al 2018a the focus of the present study is on the internal propagation of hydrological drought from the beginning to the termination of the hydrological drought and including the development and recovery processes it is common to extract the propagation characteristics of hydrological drought by using a specific drought threshold to determine the start development recovery and termination of a hydrological drought parry et al 2016a the standardized hydrological drought indices such as the standardized runoff index sri shukla and wood 2008 and standardized streamflow index ssi vicente serrano et al 2011 are two common indices for quantifying the propagation characteristics of hydrological drought and they provide useful information on hydrological drought e g the beginning and termination of the drought for users and the public the sri focuses on the surface runoff of catchments whereas the ssi is obtained by using the streamflow of river channels in addition hydrological drought is characterized by multiple attributes such as duration d severity s and peak intensity pi which can be extracted from a continuous or discrete drought index time series by taking into account a certain truncation level tl of the run theory yevjevich 1967 this approach has been widely used to analyze the evolution of hydrological drought against the backdrop of a changing environment van loon et al 2012 wu et al 2017 parry et al 2016a defined the development and recovery processes of hydrological drought by using the threshold truncation method in which the former process focuses on the period from the beginning of the hydrological drought to the maximum water shortage i e the time of the pi and the latter process focuses on the period from the time of the pi to the termination time of hydrological drought the climate properties van loon et al 2014 catchment characteristics van loon et al 2016 land use land cover change cook et al 2009 groundwater storage parry et al 2018 reservoir operation wu et al 2018b and various human regulations firoz et al 2018 tijdeman et al 2018 have a significant influence on hydrological drought propagation and have been the topic of many studies because these characteristics are related to the accuracy of hydrological drought prediction and early warning hao et al 2018 especially the reservoir operation tijdeman et al 2018 the reason is that the reservoirs provide a reliable water supply and are crucial for a variety of human regulations and needs during droughts although several previous studies have analyzed the linear and nonlinear relationship of hydrological drought responding to meteorological drought under the impact of human regulations tijdeman et al 2018 these studies only focused on the phase of the meteorological drought during the formation of hydrological drought that is the critical condition of the propagation from the meteorological drought to the hydrological drought wu et al 2017 other studies have focused on the changes in the drought duration and severity during the hydrological drought period and the impacts of general human regulations but they used only the overall propagation characteristics e g the d s and pi firoz et al 2018 tijdeman et al 2018 whereas the development and recovery processes of hydrological drought were often overlooked a comparative analysis method is often used to quantify the impacts of human regulations on hydrological drought characteristics i e d s and pi and four types of comparisons are common van loon et al 2019 i in the upstream downstream comparison e g rangecroft et al 2019 researchers have used only observation data in the upstream portion that is not affected by certain human regulations e g reservoir operation whereas the downstream portion is affected by it the uncertainty depends on the possible non linear relationship between the streamflow of the upstream hydrometric stations and that of the downstream station ii in the paired catchment approach e g van loon et al 2019 two watersheds with similar characteristics e g catchment area climatic properties geology and so on except for special human regulations are compared namely one watershed is an undisturbed and the other is a disturbed catchment however it is very difficult to find two watersheds that have similar natural characteristics other than special human regulations iii the pre post disturbance period comparative approach has been commonly used to assess the impact of reservoir operations on the hydrological drought e g lopez moreno et al 2009 wu et al 2018b that is the hydrological drought characteristics of the same hydrometric station between pre reservoir operation and the post reservoir operation period are compared however this method is restricted by the timing of the reservoir operation point in the time series for example if the time series of the observation data before or after the reservoir operation point is 30 years it is not valid to conduct a comparison of the statistical results before and after because of the time required for analyzing a hydrometeorological time series iv in the simulated observed comparative approach e g van loon et al 2012 firoz et al 2018 the hydrological drought characteristics of the simulation data affected by less human regulations regarded as an approximate natural hydrological process and without human regulation impact are compared with the observation data affected by human regulations regarded as the hydrological process after human regulations however the simulated streamflow may have large deviations if the model calibration and validation during the less regulated period are not performed appropriately van loon et al 2012 although each comparative analysis method has its own advantages and disadvantages van loon et al 2019 the simulated observed comparative approach was used in our study because it can simulate the near natural hydrological processes of long time series data and this method has also been used in other earlier studies van loon et al 2012 moreover an increasing number of hydrological models have been used for hydrological drought monitoring and forecasting such as the variable infiltration capacity vic rajsekhar et al 2015 xin anjiang model xaj lai et al 2018 water evaluation and planning model weap rangecroft et al 2019 land surface hydrological models lsms mishra et al 2018 and the soil water and assessment tool swat kang and sridhar 2017 in recent years these hydrological models also represent a potential reference for hydrological drought monitoring and early warning in regions where ungauged or sparsely gauged stations exist but real time hydrometeorological resources are lacking van loon et al 2019 lai et al 2018 compared with other hydrological models the semi distributed hydrological model swat is more widely accepted because it is an open source model is well documented and has advantages regarding water quantity and quality issues in the different watersheds around the globe e g small medium and large sized watersheds kang and sridhar 2017 the main goal of this study is therefore to estimate the effects of human regulations on the hydrological drought from the perspective of the development and recovery processes using the swat model and the simulated observed comparative approach the results of this study are expected to provide a new perspective for understanding the influencing mechanism of human regulations on hydrological drought from the perspective of the development and recovery and help water conservation managers further optimize the allocation of water resources during a hydrological drought in this study the ssi a statistic based hydrological drought index focusing on the streamflow of river channels is used to describe the simulated and observed hydrological drought propagation called the simulation based ssi and observation based ssi respectively the dongjiang river basin located in the humid zone in south china is chosen as a case study because the hydrological process of the mainstream in this basin is affected by reservoir operation and it often suffers from water shortages zhang et al 2015b tu et al 2018 2 study area and data set 2 1 study area 2 1 1 the dongjiang river basin the dongjiang river basin fig 1 is mainly located in guangdong province in south china 114 47 11 5 52 e 24 20 25 12 n fig 1 it covers an area of 35 340 km2 and has a length of 562 km as a result of a typical subtropical monsoon climate rainfall is abundant with around 1800 mm of annual precipitation however the intra annual distribution is uneven 80 of the annual rainfall occurs during the flood season that is from may to october zhang et al 2015b additionally the dongjiang river is a major water supply source for the mega urban agglomeration of the pearl river delta in south china e g the big cities of guangzhou shenzhen and hongkong thus an investigation of the hydrological drought propagation is crucial for the optimal allocation of regional water resources especially for the pearl river delta area 2 1 2 the xinfengjiang reservoir and its impact on downstream flow 896 hydraulic projects with a total capacity of 19 billion m3 have been constructed in the dongjiang river basin among them the xinfengjiang reservoir built in 1961 is the largest reservoir in the basin it has a storage capacity of 13 9 billion m3 with a water surface of 1600 m3 the xinfengjiang reservoir is located on a tributary of the dongjiang river in its middle reach fig 1 the functions of the xinfengjiang reservoir are mainly flood control water supply power generation shipping and irrigation according to the relevant regulations of the guangdong provincial government several previous studies have shown that the hydrological process in the downstream portion of the basin has been changed significantly because of the xinfengjiang reservoir operation in addition it should be noted that although the xinfengjiang reservoir dam was completed in 1961 it had less influence on the downstream hydrological processes prior to the construction of other reservoirs in the basin during 1961 1972 as reported by shi et al 2005 the main reason is that in the initial stage of the xinfengjiang reservoir the water storage nearly free released for the main function of power generation although climate change land use cover and large reservoir operations are the main factors influencing the variation of hydrological conditions in the dongjiang river basin the effects of climate change averagely 1 and land use cover change averagely 9 have been minor in the past few years i e from 1956 to 2009 tu et al 2015 especially their effects on the streamflow in the dry season is very limited lin et al 2014 other studies have shown that the reservoir operation including the xinfengjiang reservoir was the main reason for the change in hydrological conditions in the dongjiang river basin zhang et al 2015b in these studies the mann kendall scanning t and rank sum test approaches were used to detect that there was only one common significant and abrupt point in 1973 at the boluo lingxia and heyuan hydrometric stations to ascertain the impact of the reservoir operation in the mainstream of the dongjiang river basin 2 2 dataset the data consist of two parts i the spatial data including a digital elevation model dem and soil type and land use data and ii hydrometeorological data the details and sources of the data are as follows a 90 m resolution dem was obtained from the u s geological survey usgs website http www usgs gov the land use map was available for 2010 and was obtained from landsat thematic mapper tm images using a supervised classification and manual interpretation after radiometric and geometric corrections the land use map was provided by the data center for resources and environmental sciences at the chinese academy of sciences http www resdc cn the soil data source for the study area was the 1 1 million soil dataset provided by the institute of soil science at the chinese academy of sciences http westdc westgis ac cn all the spatial data were converted to the same geographic coordinates and projections and were used to construct the swat model the hydrometeorological data including the daily precipitation from 22 rainfall stations during 1960 2015 in the study catchment were provided by the water conservancy and electric power bureau in guangdong province in china the daily maximum and minimum temperatures relative humidity wind speed and sunshine hours for 1960 2015 from five weather stations in the catchment were provided by the national meteorological information center of the china meteorological administration http www nmic gov cn these meteorological data were used to create a weather database of the swat model the observed monthly streamflow records from 1960 to 2015 at the heyuan lingxia and boluo hydrometric stations in the mainstream of the dongjiang river were provided by the water conservancy and electric power bureau in guangdong province in china and were used to calibrate and validate the simulated streamflow of swat during the period of non regulation i e before 1973 it should be noted that the heyuan hydrometric station is closest to xinfengjiang reservoir located at the outflow of xinfengjiang reservoir and the lingxia and boluo hydrometric stations are located at the downstream of the basin the boluo hydrometric station is the outlet of dongjiang river basin fig 1 there was less of an influence of the xinfengjiang reservoir on the hydrological process downstream before 1972 shi et al 2005 because in the initial stage of the xinfengjiang reservoir the water storage nearly free released for the main function of power production therefore the monthly inflow and outflow records of the xinfengjiang reservoir from 1974 the year the fushuba reservoir in the upstream was in operation to 2009 were used to analyze the effects of the reservoir operation patterns on the internal propagation of hydrological drought these hydrometeorological data have been subject to strict quality control and have been used to study hydrological and meteorological droughts in the study basin wu et al 2018 3 method according to the abrupt point of the annual streamflow series i e 1973 reported in previous studies zhang et al 2015b the hydrometeorological data were divided into two time series including the periods of pre human regulations 1960 1972 and post human regulations 1974 2015 the former period 1960 1972 was used to construct the swat model and was further divided into three periods namely 1960 1964 1965 1968 and 1969 1972 these three periods were treated as the warm up calibration and validation periods respectively during the calibration and validation periods the calculated monthly streamflow was compared with the observed data from the boluo lingxia and heyuan stations using the nash sutcliffe coefficient ens nash and sutcliffe 1970 the coefficient of determination r 2 santhi et al 2001 and the percent bias pbias legates and mccabe 1999 the ens r 2 and pbias were calculated as follows 1 e ns 1 i 1 n q obs q sim 2 i 1 n q obs q avg 2 2 pbias q sim q avg q avg 100 3 r 2 i 1 n q obs q avg q sim q sim i 1 n q obs q avg 2 i 1 n q sim q sim 2 where qbs qavg qsim and q sim are the monthly observed average observed simulated and average simulated streamflow and n is the length of the time series a higher value of the r2 and ens and a lower value of the pbias indicate good performance of the model namely the closer the r 2 and ens values are to 1 the more accurate the model is also the closer the pbias value is to 0 the more accurate the model is when all three indices are consistent with the given criteria for a monthly time step ens 0 5 r 2 0 7 pbias 25 the model was believed to be suitable for the catchment and the impact analyses moriasi et al 2007 subsequently given the meteorological data from 1974 to 2015 the calibrated swat model was used to simulate the streamflow in the human regulation period to represent the estimated natural streamflow without human regulations then the simulated streamflow was used to calculate the ssi called the simulation based ssi and was compared with the ssi which was calculated using the observed streamflow called the observation based ssi finally the characteristics of the internal propagation of hydrological drought were identified using the run theory and its truncation level the effects of the human regulations on these characteristics were determined by comparing the simulation based ssi and observation based ssi compared to an approach in which simulation based ssi time series are compared with observation based ssi time series a benefit of using naturalized data is that the same time period and input data i e weather factors and sub surface characteristics are used for the human and natural conditions the swat model does not consider human regulation factors when simulating the streamflow the simulated flow is therefore regarded as a near natural flow hence any differences in hydrological drought should be attributed to human regulations this method has been applied to assess the impact of a particular human regulation on hydrological drought characteristics e g van loon et al 2012 firoz et al 2018 which would fill in the gaps in previous studies that did not assess the influence on the hydrological drought development and recovery processes 3 1 swat model the swat model is one of the most widely used open source semi distributed and watershed scale hydrologic model it was developed by the agricultural research service of the united states department of agriculture to simulate water quantity surface runoff and quality of streamflow in river channels zhang et al 2015a according to the operation principle of the swat model a watershed is first divided into sub basins and each sub basin is subdivided into hydrologic response units hrus based on land use topography soil and slope maps the hydrologic cycle for each hru is simulated based on the water balance including precipitating interception surface runoff evapotranspiration percolation lateral flow from the soil profile and return flow from shallow aquifers here the swat model was used to simulate the streamflow because of the aforementioned advantages and the simulated streamflow was applied to calculate the ssi using the built in function of the swat model and the corresponding algorithm the dongjiang river basin was divided into 92 sub catchments by inputting the selected land use topography soil and slope maps since the swat model involves a number of parameters their calibration is quite tedious according to previous studies li et al 2018 eight sensitive parameters were selected based on the results of the swat cup sensitivity analysis i e gwqmn gw revap gw delay esco sol awc alpha bnk canmx and sol k to calibrate and validate the monthly streamflow at the boluo lingxia and heyuan hydrometric stations the sufi 2 optimization algorithm in swat cup was used for the swat model calibration as well as the parameter sensitivity analysis the sensitivity and significance degree of each parameter were represented by the t stat and p value respectively the higher the value of t stat the greater its sensitivity is and the lower the p value the greater the sensitivity is all selected parameters were automatically calibrated through swat cup with 500 simulations the calibration processes may undergo five rounds depending on whether the criteria were achieved the optimal number of simulations was further selected to determine the optimal parameter value the recommended sensitive parameters and the suitable range of values as well as the optimal values for the outlet of the dongjiang river basin i e boluo hydrometric station are listed in table 1 more detailed information on the swat model can be found in the swat documents neitsch et al 2009 3 2 identification of hydrological drought propagation characteristics 3 2 1 standardized streamflow index ssi the ssi was calculated by fitting a specific cumulative probability density function with the long term monthly streamflow of river channels 30 years and then the monthly streamflow was transformed into a standard normal distribution for the actual ssi series vicente serrano et al 2011 one of the advantages of the ssi is its ability to monitor and track the different timescales of hydrological droughts such as the short timescales of the ssi e g 1 and 3 month scales called ssi1 and ssi3 which are sensitive to short term hydrological drought correspondingly the long timescales of the ssi e g 12 and 24 month called ssi12 and ssi24 well describe the long term hydrological drought note that the ssi1 ssi3 spi12 and ssi24 represent the cumulative streamflow anomalies of the consecutive 1 3 12 and 24 month periods respectively for example the ssi3 of may is based on the cumulative streamflow anomalies from march to may whereas that of june is based on the cumulative streamflow anomalies from april to june thus the time series of ssi1 ssi3 ssi12 and ssi24 are all monthly time series but their values are different for the same month wu et al 2018b previous studies suggested that the log normal probability density function fitted to multi timescales for streamflow records resulted in better performance than the gamma and normal functions liu et al 2018 therefore the ssi was calculated using the log normal probability density function and was used to describe the hydrological drought propagation in this study the computational steps of the ssi are not discussed here because many previous papers have described the detailed procedures of calculating the ssi to ensure that readers understand the ssi clearly the calculation principle and steps of the ssi are described in the appendix referring to the study of vicente serrano et al 2011 the drought levels of the ssi are ssi 0 1 0 ssi 0 1 5 ssi 1 0 2 0 ssi 1 5 and ssi 2 and they correspond to the drought types of no drought mild drought moderate drought severe drought and extreme drought respectively 3 2 2 run theory multiple dynamic propagation characteristics such as the d s and pi result in more complex impacts for droughts than for other meteorological disasters and also make drought difficult to track hao et al 2018 the run theory is a common approach used for extracting these propagation characteristics from the drought index time series yevjevich 1967 if the drought index in a certain period remained below the tl of the run theory e g x0 in fig 2 the run was regarded as a negative run and was likely a drought event e g d 1 d 2 d 3 and d 4 in fig 2 wu et al 2017 the period of the negative run was defined as the drought duration d from the drought start to the drought termination and the cumulative deficit of all negative drought index values during the d is defined as the drought severity s i e s t s t e ssi tl the period from the start of the drought ts to the end of the drought te is the drought duration fig 2 the pi is defined as the maximum negative value of the drought index during a certain drought event in addition according to the definitions provided by parry et al 2016a b and wu et al 2018a the drought development duration ddd is regarded as the time period from the drought beginning to the pi see fig 2 and the drought recovery duration drd is defined as the time from the pi to the drought termination thus the s is divided into two parts one is the accumulation of all negative drought index values in the ddd called s1 total variation of the drought index in the development period namely the tvdp in fig 2 and the second part is the accumulation of all negative drought index values in the drd called s2 total variation of the drought index in the recovery period namely the tvrp in fig 2 here the run theory is not only used to identify the hydrological drought propagation structure including the d s and pi but also the ddd drd tvdp and the tvrp to clarify the propagation of the hydrological drought structure a minimum of 3 months d was defined as a hydrological drought event because it is difficult to extract the ddd and drd or tvdp and tvrp for 1 and 2 months d thomas et al 2014 it is worth noting that the 3 months here refer to the d for at least three months not the time scale of ssi here the truncation level x0 was set to 0 because the 0 is also considered to be the threshold for the possible hydrological drought start or end phase based on the drought threshold of ssi wu et al 2017 3 3 estimation of the percent change of hydrological drought propagation characteristics according to the results of the simulation based ssi and observation based ssi the hydrological drought propagation characteristics i e the d s pi ddd drd tvdp and tvrp were extracted using the run theory and its tl i e tl 0 fig 2 during the study period subsequently the percent change of the characteristics obtained from the observation based ssi relative to the simulation based ssi was calculated as follows 4 cp x obs x sim x sim 100 where cp is the percent change of a certain hydrological drought propagation characteristic d s pi ddd drd tvdp or tvrp under the impact of human regulations and x obs x sim are the average of a certain hydrological drought propagation characteristic over many hydrological drought events obtained from the observation based ssi and simulation based ssi respectively furthermore to clarify the influence of human regulations on the hydrological drought the characteristics of the internal propagation of hydrological drought were categorized into three scenarios scenario 1 the average of the characteristics of all hydrological drought events scenario 2 the hydrological drought events whose ds are shorter than the average of all hydrological drought events scenario 3 the hydrological drought events whose ds are longer than the average of all hydrological drought events fig 8 we used these three scenarios and the average values of the respective scenarios because they represent the general effects of reservoir operations on hydrological drought development and recovery processes rather than unique situations in addition this division was necessary because reservoir operations may have different effects on the internal propagation stages or characteristics wu et al 2018b 4 results and discussion 4 1 model calibration and validation 1965 1972 the simulated results of the swat and the observations are exhibited in fig 3 the performances of the streamflow simulations for the three hydrometric stations boluo lingxia and heyuan during the calibration and validation periods are acceptable table 2 the r 2 values for the boluo station in the calibration period 1965 1968 and validation period 1969 1972 were 0 95 and 0 88 the ens values were 0 87 and 0 88 and the absolute values of the pbias were lower than 25 for both periods for the lingxia station the r 2 values in the calibration and validation periods were 0 95 and 0 83 the ens were 0 73 and 0 81 and the absolute values of the pbias were also lower than 25 for both two periods for the heyuan station the values of r 2 ens and pbias were 0 90 0 71 and 6 14 in the calibration period and 0 69 0 56 and 7 45 in the validation period the accuracies of the monthly streamflow simulation during the validation period were lowest for the heyuan station because this station is located closest to the xinfengjiang reservoir and even though the operation capacity of the xinfengjiang reservoir was limited before 1973 it had only a slight impact on the downstream hydrological process tu et al 2018 zhang et al 2015b overall as a result of the satisfying fits of the streamflow at the three hydrometric stations for the calibration and validation periods table 2 the swat model was thought to be a reliable representation of the hydrologic process and therefore can be used to simulate the hydrological drought propagation for the investigated catchment 4 2 comparison of ssi results at different time scales 1974 2015 the results of using different timescales of the observation based ssi i e 1 month 3 month 12 month and 24 month and the simulation based ssi were compared for the period of 1974 2015 at the boluo lingxia and heyuan hydrometric stations fig 4 in addition the correlation between the observation based ssi and simulation based ssi within a year i e the correlation between the monthly simulated ssi and observed ssi resulting in twelve correlation values for one timescale of ssi during 1974 2015 was examined by using the pearson correlation coefficient pcc as shown in fig 5 the simulation based ssis were consistent with the observation based ssi for each of the four timescales and they almost coincided with the curve of the observation based ssi fig 4 except for the heyuan station in other words as the values of the observation based ssi decreased the values of the simulation based ssi also decreased at the boluo and lingxia hydrometric stations and vice versa the intra annual pcc values between the observation based ssi and simulated based ssi were high at the boluo and lingxia stations fig 5 especially for the 12 and 24 month data which means that the simulation based ssis can replace the observation based ssis for the hydrological drought propagation analysis however the fluctuations of the simulation based ssis and observation based ssis are inconsistent at the heyuan station and the correlations for each month are significantly lower than those of the boluo and lingxia stations we further compared the simulation based ssi series of the boluo lingxia and heyuan stations for different timescales fig 6 their fluctuations are in good agreement and the fitting relationships for each of the four timescales are good r2 0 97 the larger the timescale the better the fitting relationship is it should be noted that the three groups of ssis i e the simulation based ssi for boluo lingxia and heyuan were inconsistent in some years at the timescales of 12 and 24 months e g the years after 2008 fig 6 specifically the fluctuation of the simulation based ssi for the heyuan station was inconsistent with those of the boluo and lingxia stations we suspect that the reason might be related to the cumulative effects on the environment resulting from a long time drought zhang et al 2018 furthermore the lingxia and boluo stations were subject to river trough storage and the flattening of the river basin ramp which greatly decreased the effect of the human regulations on the hydrological drought overall the poor correlations between the simulation based ssis and observation based ssis at the heyuan station are related to human regulations especially the xinfengjiang reservoir operation similar results were also reported in our previous studies wu et al 2018b zhang et al 2015b in addition the heyuan hydrometric station is the closest station to the xinfengjiang reservoir therefore in the following sections we describe the effects of reservoir operation on the internal propagation of hydrological drought for only the heyuan station 4 3 internal propagation characteristics of hydrological drought at the heyuan station 4 3 1 identification of the internal propagation characteristics of hydrological drought the 3 month ssi was selected to describe the internal propagation of hydrological drought at the heyuan station because multi seasonal droughts occurred in the study area zhang and lin 2011 fig 7 shows the main hydrological drought events and their propagation characteristics obtained from the observation based ssi fig 7 a and simulation based ssi fig 7 b respectively the individual propagation characteristics including d s pi ddd drd tvdp and tvrp were extracted using the run theory and the average of these characteristics during 1974 2015 are summarized in table 3 a comparison of fig 7 a and b indicates that the most severe hydrological drought events were consistent seventeen and twenty one hydrological drought events during 1974 2015 were extracted using the run theory from the observation based ssi and simulation based ssi series respectively for the observation based ssi the most severe hydrological drought events with ds longer than the average value of 9 71 months of all hydrological drought events occurred during 1989 1991 1997 1999 2001 2002 2002 2004 2008 2009 2010 2011 and 2014 2015 fig 7 a the longest d of 25 months was from may 2002 to may 2004 the corresponding ddd and drd were 20 and 5 months and the pi for this hydrological drought event was 2 49 the average d was 16 14 months for these seven severe hydrological droughts and the corresponding average ddd and drd were 9 43 and 6 71 months for the simulation based ssi the most severe hydrological drought events with ds longer than the average 9 71 of all hydrological drought events occurred during 1980 1981 1990 1992 2003 2005 2008 2009 2010 2011 and 2014 2015 fig 7 b the longest d of 27 months was from march 2003 to may 2005 the corresponding ddd and drd were 22 and 5 months and the pi for this hydrological drought event was 1 88 the average d was 16 03 months for these six severe hydrological drought events and the corresponding averages of the ddd and drd were 10 07 and 6 07 months the s and corresponding tvdp and tvrp variables for these hydrological drought events are listed in column 4 in table 3 generally the average of the ddd and drd and the corresponding tvdp and tvrp were quite different during the hydrological drought propagation that is the average ddd tvdp was always longer larger than the average drd tvrp the reason is that a hydrological drought event is likely the result of a long period of precipitation shortage but a short time heavy rain event could cause the flow of the river to rise rapidly thereby mitigating the hydrological drought lake 2006 tallaksen and van lanen 2004 thus it is necessary to divide the hydrological drought propagation period into different stages to consider the impact of human regulations on the hydrological drought 4 3 2 the change percentage of the internal propagation of hydrological drought characteristics fig 8 shows the change percentage of the characteristics of the internal propagation of hydrological drought although the number of hydrological drought events decreased due to reservoir operation comparing the observation based to the simulation based overall the average d and s increased scenario 1 fig 8 a for the hydrological drought internal propagation characteristics the ddd tvdp and pi decreased but the drd and tvrp increased that is the increase in the average d and s for all hydrological drought events mainly resulted from the increased drd and tvrp during the recovery stage of the droughts for scenario 2 fig 8 b with the shorter droughts all internal propagation characteristics decreased however the internal propagation characteristics increased except the ddd tvdp and pi for scenario 3 fig 8 c with the longer droughts these results indicated that the xinfengjiang reservoir operation contributed to the reduction or mitigation of the short term hydrological droughts at the heyuan station whereas in general it was conducive to increasing or exacerbating the long term hydrological drought events by comparing the results of the three scenarios the following conclusions can be drawn for all hydrological drought events i e the scenario 1 at the heyuan hydrometric station i the reduction in the number of hydrological drought events is mainly the result of the lower number of short term hydrological drought events ii the increase in the average duration and severity of the hydrological drought is caused by an increase in the average duration and severity of long term of hydrological drought events iii the development time of long term and short term hydrological droughts decreased iv the increase in the average of the recovery time of the hydrological drought is the result of an increase in the recovery time of long term hydrological drought events therefore the reservoir operation causes different hydrological drought propagations and it may have a greater influence on the development than the recovery of hydrological drought the different ds of hydrological drought events should be categorized to determine the actual effects of reservoir operation to explain these phenomena box plots were created of the inflow and outflow of the xinfengjiang reservoir during the development and recovery period for scenario 1 scenario 2 and scenarios 3 fig 9 in fig 9 a scenario1 the inflow of the reservoir was lower than the outflow during the drought development period whereas the outflow of the reservoir was slightly lower than the inflow during the drought recovery period therefore the xinfengjiang reservoir tended to increase water availability during the hydrological drought development period by releasing more water than the inflow whereas it tended to store water during the recovery period it is not difficult to understand the reason why the average ddd tvdp of the hydrological drought was shortened alleviated whereas the drd tvrp increased slightly for all hydrological drought events for scenario 2 the inflows of the reservoir were lower than the outflows for both the development and recovery periods fig 9 b especially during the hydrological drought development period however the inflow outflow relationship was quite different for scenario 3 there was only a small difference between the inflow and outflow of the reservoir fig 9 c hence the xinfengjiang reservoir operation was conducive to the reduction or mitigation of the short term hydrological drought events scenario 2 which refers to the hydrological drought events whose ds are shorter than the average d of all hydrological drought events in contrast there was an opposite effect on the long term hydrological drought events scenario 3 referring to the hydrological drought events whose ds are longer than the average d of all hydrological drought events a possible physically based explanation of this phenomenon is as follow before reaching the peak intensity of the hydrological drought the reservoir tended to release water to mitigate the drought severity this is because any rational decision maker hopes to reduce losses caused by drought when the drought develops towards peak intensity however the reservoir needs to store a certain amount of water during the drought recovery period to meet other important functions as power generation and shipping especially during a long term drought at the same time the recovery of a drought indicates the increase of water sources which puts water storage possible the reservoir should store water as much as possible during flood season for two reasons 1 flood control and 2 water compensation for drought season water use these inflow outflow relationships of the xinfengjiang reservoir can be used to explain why the ddd and corresponding tvdp decreased but the drd and corresponding tvrp increased at the heyuan hydrometric station the xinfengjiang reservoir tends to release water during the early hydrological drought but stores water in the recovery period of the hydrological drought when facing severe and long hydrological drought events as a result the impacts of xinfengjiang reservoir operation e g the regulating ability of the xinfengjiang reservoir on the hydrological drought are weakened with the increase in the d of the hydrological drought these results are also generally in agreement with previous study results e g wu et al 2018b rangecroft et al 2019 the influence of reservoir operation on the hydrological drought is mainly observed for short term events and their d s is decreased mitigated whereas there is less impact on the long term hydrological drought events nevertheless the influence of reservoir operation on the internal propagation of hydrological drought e g ddd drd pi and corresponding severity were not examined in these previous studies but were addressed in our study it is worth noting that different reservoir operation patterns may have different effects on hydrological drought propagation but overall wu et al 2017 reservoir operations have aggravating or mitigating effects on hydrological drought lopez moreno et al 2009 tijdeman et al 2018 this study provides a basic framework for analyzing the impacts of human regulations especially reservoir operations on the development and recovery processes of hydrological drought in other regions of the world so that reservoir operation patterns can be optimized during drought periods in addition a recent study wu et al 2018b also used the dongjiang river basin as a study area to investigate the impacts of reservoir operation on hydrological drought by using the pre post disturbance comparative approach and analyzed the influences of the reservoir operation rules during drought periods on the multi scale correlations between hydrological drought and meteorological drought unlike this study the authors explored the changes in the hydrological drought by evaluating the changes in the correlations with the meteorological drought and by comparing the correlations between the undisturbed and disturbed period whereas we examined the impacts of human regulations especially reservoir operations on the internal propagation of hydrological drought i e development and recovery process using the simulated observed comparative approach 4 4 limitations and future study although the simulated observed comparative approach was used to determine the impact of human regulations on the internal propagation of hydrological drought in this study this approach still has some limitations the uncertainty related to the spatial data is a crucial issue that cannot be overlooked even if a suitable hydrological model is used trambauer et al 2013 wang et al 2011 for example only one stage or one date of spatial data i e dem soil type and land use was used to create the swat model and simulate the long term streamflow in the study basin and it is assumed that the spatial information may change over time mishra et al 2018 van loon et al 2019 trambauer et al 2013 although the spatial information of one stage does not effectively reflect the long term hydrological process the changes in the hydrological process in a certain period can be compared by addressing the shortcomings of the specific spatial information moreover several studies have pointed out that the swat model is insensitive to low flow because the objective functions or performance indices used to calibrate the model tend to reflect specific hydrological process e g high flow lin et al 2015 but the overall fluctuation of the simulated based ssi and observed based ssi are basically consistent see figs 4 and 5 except for the stations that are strongly affected by the xinfengjiang reservoir operation e g heyuan hydrometric station although the variation of hydrological drought propagation in a certain catchment may be affected by various factors such as climate change large scale atmospheric patterns e g enso and land use cover change it is undeniable that the main influencing factor at the heyuan hydrometric station is the operation of the xinfengjiang reservoir zhang et al 2015b tu et al 2015 we did not separate the effects of climate change reservoir operation and other human activities e g land use cover change on the internal propagation of hydrological drought owing to the currently limited research technology but overall at the heyuan hydrometric station which is located just below the outlet of the xinfengjiang reservoir the variation of the internal propagation of hydrological drought is mainly affected by the xinfengjiang reservoir operation this finding has also been confirmed by previous studies i e the influence of the reservoir decreased with increasing distance from the xinfengjiang reservoir zhang et al 2015b future studies will consider more detailed information regarding the influence of a specific human activity on the evolution of the internal propagation of hydrological drought additional spatial information more hydrometeorological gauging stations monitored in near real time and the use of other methods and satellite based precipitation products will be incorporated to establish a real time monitoring and tracking system of hydrological drought propagation in watersheds for drought prevention and mitigation we will also integrate the swat model with various suitable resources for meteorological forecasting scenarios e g global climate model gcm by using different drought forecasting methods the development and recovery process of hydrological drought will be determined which can be a help for developing an early warning hydrological drought system for the heyuan station we need to modify the reservoir parameters involved in the swat model to further optimize the simulation flow results to improve performance of the hydrological drought early warning system 5 conclusions given certain meteorological data the semi distributed hydrological model swat which is calibrated for a period with less human regulations is suitable to analyze the development and recovery processes of hydrological drought during the human regulation period based on the simulated observed comparison approach for the dongjiang river basin the short term hydrological drought was shortened and was less severe due to reservoir operation whereas the long term hydrological drought had the opposite effect the average d s increased aggravated under the effects of reservoir operation the beneficial impact of the xinfengjiang reservoir on the short term hydrological drought was achieved by increased water availability during drought periods development and recovery process however when the basin was facing severe or long lasting hydrological droughts the impact of the xinfengjiang reservoir on the hydrological drought was limited or even negative i e water was released during the drought development and was stored during the recovery period therefore the operation patterns of the xinfengjiang reservoir in the study area should be further improved to ensure water security during droughts especially during a long term hydrological drought since the operation patterns of a large reservoir and its impact on the downstream hydrological drought propagation are very complex further studies are required declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research is supported by funding from the national key r d program of china 2017yfc0405900 and the national natural science foundation of china grant no 51861125203 51809294 and the international program for ph d candidates sun yat sen university appendix ssi calculation steps from the monthly streamflow series the ssi was obtained the procedure is based on the calculation of accumulated probability f x of each monthly value and its further normalization in standard units vicente serrano et al 2011 the calculation of f x is essential to obtaining the ssi the three parameters for log normal distribution were tested to describe streamflow data the ssi in z scores is calculated using the following equation ssi w c 0 c 1 w c 2 w 2 1 d 1 w d 2 w 2 d 3 w 3 where the constants are c0 2 515517 c1 0 802853 c2 0 010328 d1 1 432788 d2 0 189269 d3 0 001308 and w 2 ln p for p 0 5 p is the probability of exceeding a determined x value p 1 f x if p 0 5 p is replaced by 1 p and the sign of the resultant ssi is reversed the ssi is normalized so that the mean is o and standard deviation is 1 
6251,although previous studies have estimated the effects of human regulations on hydrological drought few studies have examined the impacts from the perspective of the development and recovery processes of hydrological drought this study uses the dongjiang river basin in south china as an example and focuses on the influence of reservoir operation on hydrological drought the standardized streamflow index ssi and run theory were applied to determine the development and recovery processes of hydrological drought a semi distributed hydrological model the soil and water assessment tool swat was used to simulate long time series 30 years monthly streamflow data the swat model was calibrated and validated using the monthly streamflow data from periods with less human regulations streamflow was then simulated and compared by using the same model parameters as well as the sub surface features to simulate the streamflow during the human regulation periods the simulated observed comparative approach i e a comparison of the characteristics of the hydrological drought obtained from the simulation based ssi and observation based ssi was used to assess the impacts of human regulations on hydrological drought development and recovery the results showed that the simulated observed comparison approach based on the swat model exhibited a good ability to ascertain the impacts of human regulations on hydrological drought development and recovery the reservoir operation was the dominant factor affecting the hydrological drought propagation in the lower reaches of the dongjiang river basin and had mostly a short term effect by decreasing the duration and severity of drought development or recovery however the reservoirs tended to release water during the development process and to store water during the recovery process during a long term hydrological drought in the study area leading to an increase in average recovery duration and severity the results of this study may further optimize water resources management system during drought for effective drought prevention and mitigation keywords hydrological drought development and recovery reservoir operation swat dongjiang river basin 1 introduction droughts are recurrent natural hazards and may occur in any climatic region mishra and singh 2010 recent studies have reported on the trends in the frequency of severe droughts over the past decades huang et al 2016 and several studies have shown that an increase in the drought frequency has significantly affected the food supply water security and the environment wernberg et al 2013 the investigation of drought propagation is of great importance for regional eco environmental safety and water related disaster management against the background of a changing environment droughts are commonly divided into different types based on the phase of the water shortage in the hydrological cycle i e meteorological drought agricultural drought hydrological drought and socioeconomic drought american meteorological society 1997 world meteorological organization 2006 the first phase of drought is the meteorological drought which refers to a deficiency of precipitation in a certain period the continuation of the meteorological drought may result in a deficiency in soil moisture and is then called the agricultural drought further precipitation shortage leads to a decrease in water in rivers lakes and reservoirs this phase is referred to as hydrological drought and is regarded as a thorough drought dracup et al 1980 the change from a meteorological drought to a hydrological drought is commonly regarded as drought propagation apurv et al 2017 however drought propagation not only occurs among different drought types e g from meteorological to hydrological drought but also within a certain drought type which is called internal propagation parry et al 2016a wu et al 2018a the focus of the present study is on the internal propagation of hydrological drought from the beginning to the termination of the hydrological drought and including the development and recovery processes it is common to extract the propagation characteristics of hydrological drought by using a specific drought threshold to determine the start development recovery and termination of a hydrological drought parry et al 2016a the standardized hydrological drought indices such as the standardized runoff index sri shukla and wood 2008 and standardized streamflow index ssi vicente serrano et al 2011 are two common indices for quantifying the propagation characteristics of hydrological drought and they provide useful information on hydrological drought e g the beginning and termination of the drought for users and the public the sri focuses on the surface runoff of catchments whereas the ssi is obtained by using the streamflow of river channels in addition hydrological drought is characterized by multiple attributes such as duration d severity s and peak intensity pi which can be extracted from a continuous or discrete drought index time series by taking into account a certain truncation level tl of the run theory yevjevich 1967 this approach has been widely used to analyze the evolution of hydrological drought against the backdrop of a changing environment van loon et al 2012 wu et al 2017 parry et al 2016a defined the development and recovery processes of hydrological drought by using the threshold truncation method in which the former process focuses on the period from the beginning of the hydrological drought to the maximum water shortage i e the time of the pi and the latter process focuses on the period from the time of the pi to the termination time of hydrological drought the climate properties van loon et al 2014 catchment characteristics van loon et al 2016 land use land cover change cook et al 2009 groundwater storage parry et al 2018 reservoir operation wu et al 2018b and various human regulations firoz et al 2018 tijdeman et al 2018 have a significant influence on hydrological drought propagation and have been the topic of many studies because these characteristics are related to the accuracy of hydrological drought prediction and early warning hao et al 2018 especially the reservoir operation tijdeman et al 2018 the reason is that the reservoirs provide a reliable water supply and are crucial for a variety of human regulations and needs during droughts although several previous studies have analyzed the linear and nonlinear relationship of hydrological drought responding to meteorological drought under the impact of human regulations tijdeman et al 2018 these studies only focused on the phase of the meteorological drought during the formation of hydrological drought that is the critical condition of the propagation from the meteorological drought to the hydrological drought wu et al 2017 other studies have focused on the changes in the drought duration and severity during the hydrological drought period and the impacts of general human regulations but they used only the overall propagation characteristics e g the d s and pi firoz et al 2018 tijdeman et al 2018 whereas the development and recovery processes of hydrological drought were often overlooked a comparative analysis method is often used to quantify the impacts of human regulations on hydrological drought characteristics i e d s and pi and four types of comparisons are common van loon et al 2019 i in the upstream downstream comparison e g rangecroft et al 2019 researchers have used only observation data in the upstream portion that is not affected by certain human regulations e g reservoir operation whereas the downstream portion is affected by it the uncertainty depends on the possible non linear relationship between the streamflow of the upstream hydrometric stations and that of the downstream station ii in the paired catchment approach e g van loon et al 2019 two watersheds with similar characteristics e g catchment area climatic properties geology and so on except for special human regulations are compared namely one watershed is an undisturbed and the other is a disturbed catchment however it is very difficult to find two watersheds that have similar natural characteristics other than special human regulations iii the pre post disturbance period comparative approach has been commonly used to assess the impact of reservoir operations on the hydrological drought e g lopez moreno et al 2009 wu et al 2018b that is the hydrological drought characteristics of the same hydrometric station between pre reservoir operation and the post reservoir operation period are compared however this method is restricted by the timing of the reservoir operation point in the time series for example if the time series of the observation data before or after the reservoir operation point is 30 years it is not valid to conduct a comparison of the statistical results before and after because of the time required for analyzing a hydrometeorological time series iv in the simulated observed comparative approach e g van loon et al 2012 firoz et al 2018 the hydrological drought characteristics of the simulation data affected by less human regulations regarded as an approximate natural hydrological process and without human regulation impact are compared with the observation data affected by human regulations regarded as the hydrological process after human regulations however the simulated streamflow may have large deviations if the model calibration and validation during the less regulated period are not performed appropriately van loon et al 2012 although each comparative analysis method has its own advantages and disadvantages van loon et al 2019 the simulated observed comparative approach was used in our study because it can simulate the near natural hydrological processes of long time series data and this method has also been used in other earlier studies van loon et al 2012 moreover an increasing number of hydrological models have been used for hydrological drought monitoring and forecasting such as the variable infiltration capacity vic rajsekhar et al 2015 xin anjiang model xaj lai et al 2018 water evaluation and planning model weap rangecroft et al 2019 land surface hydrological models lsms mishra et al 2018 and the soil water and assessment tool swat kang and sridhar 2017 in recent years these hydrological models also represent a potential reference for hydrological drought monitoring and early warning in regions where ungauged or sparsely gauged stations exist but real time hydrometeorological resources are lacking van loon et al 2019 lai et al 2018 compared with other hydrological models the semi distributed hydrological model swat is more widely accepted because it is an open source model is well documented and has advantages regarding water quantity and quality issues in the different watersheds around the globe e g small medium and large sized watersheds kang and sridhar 2017 the main goal of this study is therefore to estimate the effects of human regulations on the hydrological drought from the perspective of the development and recovery processes using the swat model and the simulated observed comparative approach the results of this study are expected to provide a new perspective for understanding the influencing mechanism of human regulations on hydrological drought from the perspective of the development and recovery and help water conservation managers further optimize the allocation of water resources during a hydrological drought in this study the ssi a statistic based hydrological drought index focusing on the streamflow of river channels is used to describe the simulated and observed hydrological drought propagation called the simulation based ssi and observation based ssi respectively the dongjiang river basin located in the humid zone in south china is chosen as a case study because the hydrological process of the mainstream in this basin is affected by reservoir operation and it often suffers from water shortages zhang et al 2015b tu et al 2018 2 study area and data set 2 1 study area 2 1 1 the dongjiang river basin the dongjiang river basin fig 1 is mainly located in guangdong province in south china 114 47 11 5 52 e 24 20 25 12 n fig 1 it covers an area of 35 340 km2 and has a length of 562 km as a result of a typical subtropical monsoon climate rainfall is abundant with around 1800 mm of annual precipitation however the intra annual distribution is uneven 80 of the annual rainfall occurs during the flood season that is from may to october zhang et al 2015b additionally the dongjiang river is a major water supply source for the mega urban agglomeration of the pearl river delta in south china e g the big cities of guangzhou shenzhen and hongkong thus an investigation of the hydrological drought propagation is crucial for the optimal allocation of regional water resources especially for the pearl river delta area 2 1 2 the xinfengjiang reservoir and its impact on downstream flow 896 hydraulic projects with a total capacity of 19 billion m3 have been constructed in the dongjiang river basin among them the xinfengjiang reservoir built in 1961 is the largest reservoir in the basin it has a storage capacity of 13 9 billion m3 with a water surface of 1600 m3 the xinfengjiang reservoir is located on a tributary of the dongjiang river in its middle reach fig 1 the functions of the xinfengjiang reservoir are mainly flood control water supply power generation shipping and irrigation according to the relevant regulations of the guangdong provincial government several previous studies have shown that the hydrological process in the downstream portion of the basin has been changed significantly because of the xinfengjiang reservoir operation in addition it should be noted that although the xinfengjiang reservoir dam was completed in 1961 it had less influence on the downstream hydrological processes prior to the construction of other reservoirs in the basin during 1961 1972 as reported by shi et al 2005 the main reason is that in the initial stage of the xinfengjiang reservoir the water storage nearly free released for the main function of power generation although climate change land use cover and large reservoir operations are the main factors influencing the variation of hydrological conditions in the dongjiang river basin the effects of climate change averagely 1 and land use cover change averagely 9 have been minor in the past few years i e from 1956 to 2009 tu et al 2015 especially their effects on the streamflow in the dry season is very limited lin et al 2014 other studies have shown that the reservoir operation including the xinfengjiang reservoir was the main reason for the change in hydrological conditions in the dongjiang river basin zhang et al 2015b in these studies the mann kendall scanning t and rank sum test approaches were used to detect that there was only one common significant and abrupt point in 1973 at the boluo lingxia and heyuan hydrometric stations to ascertain the impact of the reservoir operation in the mainstream of the dongjiang river basin 2 2 dataset the data consist of two parts i the spatial data including a digital elevation model dem and soil type and land use data and ii hydrometeorological data the details and sources of the data are as follows a 90 m resolution dem was obtained from the u s geological survey usgs website http www usgs gov the land use map was available for 2010 and was obtained from landsat thematic mapper tm images using a supervised classification and manual interpretation after radiometric and geometric corrections the land use map was provided by the data center for resources and environmental sciences at the chinese academy of sciences http www resdc cn the soil data source for the study area was the 1 1 million soil dataset provided by the institute of soil science at the chinese academy of sciences http westdc westgis ac cn all the spatial data were converted to the same geographic coordinates and projections and were used to construct the swat model the hydrometeorological data including the daily precipitation from 22 rainfall stations during 1960 2015 in the study catchment were provided by the water conservancy and electric power bureau in guangdong province in china the daily maximum and minimum temperatures relative humidity wind speed and sunshine hours for 1960 2015 from five weather stations in the catchment were provided by the national meteorological information center of the china meteorological administration http www nmic gov cn these meteorological data were used to create a weather database of the swat model the observed monthly streamflow records from 1960 to 2015 at the heyuan lingxia and boluo hydrometric stations in the mainstream of the dongjiang river were provided by the water conservancy and electric power bureau in guangdong province in china and were used to calibrate and validate the simulated streamflow of swat during the period of non regulation i e before 1973 it should be noted that the heyuan hydrometric station is closest to xinfengjiang reservoir located at the outflow of xinfengjiang reservoir and the lingxia and boluo hydrometric stations are located at the downstream of the basin the boluo hydrometric station is the outlet of dongjiang river basin fig 1 there was less of an influence of the xinfengjiang reservoir on the hydrological process downstream before 1972 shi et al 2005 because in the initial stage of the xinfengjiang reservoir the water storage nearly free released for the main function of power production therefore the monthly inflow and outflow records of the xinfengjiang reservoir from 1974 the year the fushuba reservoir in the upstream was in operation to 2009 were used to analyze the effects of the reservoir operation patterns on the internal propagation of hydrological drought these hydrometeorological data have been subject to strict quality control and have been used to study hydrological and meteorological droughts in the study basin wu et al 2018 3 method according to the abrupt point of the annual streamflow series i e 1973 reported in previous studies zhang et al 2015b the hydrometeorological data were divided into two time series including the periods of pre human regulations 1960 1972 and post human regulations 1974 2015 the former period 1960 1972 was used to construct the swat model and was further divided into three periods namely 1960 1964 1965 1968 and 1969 1972 these three periods were treated as the warm up calibration and validation periods respectively during the calibration and validation periods the calculated monthly streamflow was compared with the observed data from the boluo lingxia and heyuan stations using the nash sutcliffe coefficient ens nash and sutcliffe 1970 the coefficient of determination r 2 santhi et al 2001 and the percent bias pbias legates and mccabe 1999 the ens r 2 and pbias were calculated as follows 1 e ns 1 i 1 n q obs q sim 2 i 1 n q obs q avg 2 2 pbias q sim q avg q avg 100 3 r 2 i 1 n q obs q avg q sim q sim i 1 n q obs q avg 2 i 1 n q sim q sim 2 where qbs qavg qsim and q sim are the monthly observed average observed simulated and average simulated streamflow and n is the length of the time series a higher value of the r2 and ens and a lower value of the pbias indicate good performance of the model namely the closer the r 2 and ens values are to 1 the more accurate the model is also the closer the pbias value is to 0 the more accurate the model is when all three indices are consistent with the given criteria for a monthly time step ens 0 5 r 2 0 7 pbias 25 the model was believed to be suitable for the catchment and the impact analyses moriasi et al 2007 subsequently given the meteorological data from 1974 to 2015 the calibrated swat model was used to simulate the streamflow in the human regulation period to represent the estimated natural streamflow without human regulations then the simulated streamflow was used to calculate the ssi called the simulation based ssi and was compared with the ssi which was calculated using the observed streamflow called the observation based ssi finally the characteristics of the internal propagation of hydrological drought were identified using the run theory and its truncation level the effects of the human regulations on these characteristics were determined by comparing the simulation based ssi and observation based ssi compared to an approach in which simulation based ssi time series are compared with observation based ssi time series a benefit of using naturalized data is that the same time period and input data i e weather factors and sub surface characteristics are used for the human and natural conditions the swat model does not consider human regulation factors when simulating the streamflow the simulated flow is therefore regarded as a near natural flow hence any differences in hydrological drought should be attributed to human regulations this method has been applied to assess the impact of a particular human regulation on hydrological drought characteristics e g van loon et al 2012 firoz et al 2018 which would fill in the gaps in previous studies that did not assess the influence on the hydrological drought development and recovery processes 3 1 swat model the swat model is one of the most widely used open source semi distributed and watershed scale hydrologic model it was developed by the agricultural research service of the united states department of agriculture to simulate water quantity surface runoff and quality of streamflow in river channels zhang et al 2015a according to the operation principle of the swat model a watershed is first divided into sub basins and each sub basin is subdivided into hydrologic response units hrus based on land use topography soil and slope maps the hydrologic cycle for each hru is simulated based on the water balance including precipitating interception surface runoff evapotranspiration percolation lateral flow from the soil profile and return flow from shallow aquifers here the swat model was used to simulate the streamflow because of the aforementioned advantages and the simulated streamflow was applied to calculate the ssi using the built in function of the swat model and the corresponding algorithm the dongjiang river basin was divided into 92 sub catchments by inputting the selected land use topography soil and slope maps since the swat model involves a number of parameters their calibration is quite tedious according to previous studies li et al 2018 eight sensitive parameters were selected based on the results of the swat cup sensitivity analysis i e gwqmn gw revap gw delay esco sol awc alpha bnk canmx and sol k to calibrate and validate the monthly streamflow at the boluo lingxia and heyuan hydrometric stations the sufi 2 optimization algorithm in swat cup was used for the swat model calibration as well as the parameter sensitivity analysis the sensitivity and significance degree of each parameter were represented by the t stat and p value respectively the higher the value of t stat the greater its sensitivity is and the lower the p value the greater the sensitivity is all selected parameters were automatically calibrated through swat cup with 500 simulations the calibration processes may undergo five rounds depending on whether the criteria were achieved the optimal number of simulations was further selected to determine the optimal parameter value the recommended sensitive parameters and the suitable range of values as well as the optimal values for the outlet of the dongjiang river basin i e boluo hydrometric station are listed in table 1 more detailed information on the swat model can be found in the swat documents neitsch et al 2009 3 2 identification of hydrological drought propagation characteristics 3 2 1 standardized streamflow index ssi the ssi was calculated by fitting a specific cumulative probability density function with the long term monthly streamflow of river channels 30 years and then the monthly streamflow was transformed into a standard normal distribution for the actual ssi series vicente serrano et al 2011 one of the advantages of the ssi is its ability to monitor and track the different timescales of hydrological droughts such as the short timescales of the ssi e g 1 and 3 month scales called ssi1 and ssi3 which are sensitive to short term hydrological drought correspondingly the long timescales of the ssi e g 12 and 24 month called ssi12 and ssi24 well describe the long term hydrological drought note that the ssi1 ssi3 spi12 and ssi24 represent the cumulative streamflow anomalies of the consecutive 1 3 12 and 24 month periods respectively for example the ssi3 of may is based on the cumulative streamflow anomalies from march to may whereas that of june is based on the cumulative streamflow anomalies from april to june thus the time series of ssi1 ssi3 ssi12 and ssi24 are all monthly time series but their values are different for the same month wu et al 2018b previous studies suggested that the log normal probability density function fitted to multi timescales for streamflow records resulted in better performance than the gamma and normal functions liu et al 2018 therefore the ssi was calculated using the log normal probability density function and was used to describe the hydrological drought propagation in this study the computational steps of the ssi are not discussed here because many previous papers have described the detailed procedures of calculating the ssi to ensure that readers understand the ssi clearly the calculation principle and steps of the ssi are described in the appendix referring to the study of vicente serrano et al 2011 the drought levels of the ssi are ssi 0 1 0 ssi 0 1 5 ssi 1 0 2 0 ssi 1 5 and ssi 2 and they correspond to the drought types of no drought mild drought moderate drought severe drought and extreme drought respectively 3 2 2 run theory multiple dynamic propagation characteristics such as the d s and pi result in more complex impacts for droughts than for other meteorological disasters and also make drought difficult to track hao et al 2018 the run theory is a common approach used for extracting these propagation characteristics from the drought index time series yevjevich 1967 if the drought index in a certain period remained below the tl of the run theory e g x0 in fig 2 the run was regarded as a negative run and was likely a drought event e g d 1 d 2 d 3 and d 4 in fig 2 wu et al 2017 the period of the negative run was defined as the drought duration d from the drought start to the drought termination and the cumulative deficit of all negative drought index values during the d is defined as the drought severity s i e s t s t e ssi tl the period from the start of the drought ts to the end of the drought te is the drought duration fig 2 the pi is defined as the maximum negative value of the drought index during a certain drought event in addition according to the definitions provided by parry et al 2016a b and wu et al 2018a the drought development duration ddd is regarded as the time period from the drought beginning to the pi see fig 2 and the drought recovery duration drd is defined as the time from the pi to the drought termination thus the s is divided into two parts one is the accumulation of all negative drought index values in the ddd called s1 total variation of the drought index in the development period namely the tvdp in fig 2 and the second part is the accumulation of all negative drought index values in the drd called s2 total variation of the drought index in the recovery period namely the tvrp in fig 2 here the run theory is not only used to identify the hydrological drought propagation structure including the d s and pi but also the ddd drd tvdp and the tvrp to clarify the propagation of the hydrological drought structure a minimum of 3 months d was defined as a hydrological drought event because it is difficult to extract the ddd and drd or tvdp and tvrp for 1 and 2 months d thomas et al 2014 it is worth noting that the 3 months here refer to the d for at least three months not the time scale of ssi here the truncation level x0 was set to 0 because the 0 is also considered to be the threshold for the possible hydrological drought start or end phase based on the drought threshold of ssi wu et al 2017 3 3 estimation of the percent change of hydrological drought propagation characteristics according to the results of the simulation based ssi and observation based ssi the hydrological drought propagation characteristics i e the d s pi ddd drd tvdp and tvrp were extracted using the run theory and its tl i e tl 0 fig 2 during the study period subsequently the percent change of the characteristics obtained from the observation based ssi relative to the simulation based ssi was calculated as follows 4 cp x obs x sim x sim 100 where cp is the percent change of a certain hydrological drought propagation characteristic d s pi ddd drd tvdp or tvrp under the impact of human regulations and x obs x sim are the average of a certain hydrological drought propagation characteristic over many hydrological drought events obtained from the observation based ssi and simulation based ssi respectively furthermore to clarify the influence of human regulations on the hydrological drought the characteristics of the internal propagation of hydrological drought were categorized into three scenarios scenario 1 the average of the characteristics of all hydrological drought events scenario 2 the hydrological drought events whose ds are shorter than the average of all hydrological drought events scenario 3 the hydrological drought events whose ds are longer than the average of all hydrological drought events fig 8 we used these three scenarios and the average values of the respective scenarios because they represent the general effects of reservoir operations on hydrological drought development and recovery processes rather than unique situations in addition this division was necessary because reservoir operations may have different effects on the internal propagation stages or characteristics wu et al 2018b 4 results and discussion 4 1 model calibration and validation 1965 1972 the simulated results of the swat and the observations are exhibited in fig 3 the performances of the streamflow simulations for the three hydrometric stations boluo lingxia and heyuan during the calibration and validation periods are acceptable table 2 the r 2 values for the boluo station in the calibration period 1965 1968 and validation period 1969 1972 were 0 95 and 0 88 the ens values were 0 87 and 0 88 and the absolute values of the pbias were lower than 25 for both periods for the lingxia station the r 2 values in the calibration and validation periods were 0 95 and 0 83 the ens were 0 73 and 0 81 and the absolute values of the pbias were also lower than 25 for both two periods for the heyuan station the values of r 2 ens and pbias were 0 90 0 71 and 6 14 in the calibration period and 0 69 0 56 and 7 45 in the validation period the accuracies of the monthly streamflow simulation during the validation period were lowest for the heyuan station because this station is located closest to the xinfengjiang reservoir and even though the operation capacity of the xinfengjiang reservoir was limited before 1973 it had only a slight impact on the downstream hydrological process tu et al 2018 zhang et al 2015b overall as a result of the satisfying fits of the streamflow at the three hydrometric stations for the calibration and validation periods table 2 the swat model was thought to be a reliable representation of the hydrologic process and therefore can be used to simulate the hydrological drought propagation for the investigated catchment 4 2 comparison of ssi results at different time scales 1974 2015 the results of using different timescales of the observation based ssi i e 1 month 3 month 12 month and 24 month and the simulation based ssi were compared for the period of 1974 2015 at the boluo lingxia and heyuan hydrometric stations fig 4 in addition the correlation between the observation based ssi and simulation based ssi within a year i e the correlation between the monthly simulated ssi and observed ssi resulting in twelve correlation values for one timescale of ssi during 1974 2015 was examined by using the pearson correlation coefficient pcc as shown in fig 5 the simulation based ssis were consistent with the observation based ssi for each of the four timescales and they almost coincided with the curve of the observation based ssi fig 4 except for the heyuan station in other words as the values of the observation based ssi decreased the values of the simulation based ssi also decreased at the boluo and lingxia hydrometric stations and vice versa the intra annual pcc values between the observation based ssi and simulated based ssi were high at the boluo and lingxia stations fig 5 especially for the 12 and 24 month data which means that the simulation based ssis can replace the observation based ssis for the hydrological drought propagation analysis however the fluctuations of the simulation based ssis and observation based ssis are inconsistent at the heyuan station and the correlations for each month are significantly lower than those of the boluo and lingxia stations we further compared the simulation based ssi series of the boluo lingxia and heyuan stations for different timescales fig 6 their fluctuations are in good agreement and the fitting relationships for each of the four timescales are good r2 0 97 the larger the timescale the better the fitting relationship is it should be noted that the three groups of ssis i e the simulation based ssi for boluo lingxia and heyuan were inconsistent in some years at the timescales of 12 and 24 months e g the years after 2008 fig 6 specifically the fluctuation of the simulation based ssi for the heyuan station was inconsistent with those of the boluo and lingxia stations we suspect that the reason might be related to the cumulative effects on the environment resulting from a long time drought zhang et al 2018 furthermore the lingxia and boluo stations were subject to river trough storage and the flattening of the river basin ramp which greatly decreased the effect of the human regulations on the hydrological drought overall the poor correlations between the simulation based ssis and observation based ssis at the heyuan station are related to human regulations especially the xinfengjiang reservoir operation similar results were also reported in our previous studies wu et al 2018b zhang et al 2015b in addition the heyuan hydrometric station is the closest station to the xinfengjiang reservoir therefore in the following sections we describe the effects of reservoir operation on the internal propagation of hydrological drought for only the heyuan station 4 3 internal propagation characteristics of hydrological drought at the heyuan station 4 3 1 identification of the internal propagation characteristics of hydrological drought the 3 month ssi was selected to describe the internal propagation of hydrological drought at the heyuan station because multi seasonal droughts occurred in the study area zhang and lin 2011 fig 7 shows the main hydrological drought events and their propagation characteristics obtained from the observation based ssi fig 7 a and simulation based ssi fig 7 b respectively the individual propagation characteristics including d s pi ddd drd tvdp and tvrp were extracted using the run theory and the average of these characteristics during 1974 2015 are summarized in table 3 a comparison of fig 7 a and b indicates that the most severe hydrological drought events were consistent seventeen and twenty one hydrological drought events during 1974 2015 were extracted using the run theory from the observation based ssi and simulation based ssi series respectively for the observation based ssi the most severe hydrological drought events with ds longer than the average value of 9 71 months of all hydrological drought events occurred during 1989 1991 1997 1999 2001 2002 2002 2004 2008 2009 2010 2011 and 2014 2015 fig 7 a the longest d of 25 months was from may 2002 to may 2004 the corresponding ddd and drd were 20 and 5 months and the pi for this hydrological drought event was 2 49 the average d was 16 14 months for these seven severe hydrological droughts and the corresponding average ddd and drd were 9 43 and 6 71 months for the simulation based ssi the most severe hydrological drought events with ds longer than the average 9 71 of all hydrological drought events occurred during 1980 1981 1990 1992 2003 2005 2008 2009 2010 2011 and 2014 2015 fig 7 b the longest d of 27 months was from march 2003 to may 2005 the corresponding ddd and drd were 22 and 5 months and the pi for this hydrological drought event was 1 88 the average d was 16 03 months for these six severe hydrological drought events and the corresponding averages of the ddd and drd were 10 07 and 6 07 months the s and corresponding tvdp and tvrp variables for these hydrological drought events are listed in column 4 in table 3 generally the average of the ddd and drd and the corresponding tvdp and tvrp were quite different during the hydrological drought propagation that is the average ddd tvdp was always longer larger than the average drd tvrp the reason is that a hydrological drought event is likely the result of a long period of precipitation shortage but a short time heavy rain event could cause the flow of the river to rise rapidly thereby mitigating the hydrological drought lake 2006 tallaksen and van lanen 2004 thus it is necessary to divide the hydrological drought propagation period into different stages to consider the impact of human regulations on the hydrological drought 4 3 2 the change percentage of the internal propagation of hydrological drought characteristics fig 8 shows the change percentage of the characteristics of the internal propagation of hydrological drought although the number of hydrological drought events decreased due to reservoir operation comparing the observation based to the simulation based overall the average d and s increased scenario 1 fig 8 a for the hydrological drought internal propagation characteristics the ddd tvdp and pi decreased but the drd and tvrp increased that is the increase in the average d and s for all hydrological drought events mainly resulted from the increased drd and tvrp during the recovery stage of the droughts for scenario 2 fig 8 b with the shorter droughts all internal propagation characteristics decreased however the internal propagation characteristics increased except the ddd tvdp and pi for scenario 3 fig 8 c with the longer droughts these results indicated that the xinfengjiang reservoir operation contributed to the reduction or mitigation of the short term hydrological droughts at the heyuan station whereas in general it was conducive to increasing or exacerbating the long term hydrological drought events by comparing the results of the three scenarios the following conclusions can be drawn for all hydrological drought events i e the scenario 1 at the heyuan hydrometric station i the reduction in the number of hydrological drought events is mainly the result of the lower number of short term hydrological drought events ii the increase in the average duration and severity of the hydrological drought is caused by an increase in the average duration and severity of long term of hydrological drought events iii the development time of long term and short term hydrological droughts decreased iv the increase in the average of the recovery time of the hydrological drought is the result of an increase in the recovery time of long term hydrological drought events therefore the reservoir operation causes different hydrological drought propagations and it may have a greater influence on the development than the recovery of hydrological drought the different ds of hydrological drought events should be categorized to determine the actual effects of reservoir operation to explain these phenomena box plots were created of the inflow and outflow of the xinfengjiang reservoir during the development and recovery period for scenario 1 scenario 2 and scenarios 3 fig 9 in fig 9 a scenario1 the inflow of the reservoir was lower than the outflow during the drought development period whereas the outflow of the reservoir was slightly lower than the inflow during the drought recovery period therefore the xinfengjiang reservoir tended to increase water availability during the hydrological drought development period by releasing more water than the inflow whereas it tended to store water during the recovery period it is not difficult to understand the reason why the average ddd tvdp of the hydrological drought was shortened alleviated whereas the drd tvrp increased slightly for all hydrological drought events for scenario 2 the inflows of the reservoir were lower than the outflows for both the development and recovery periods fig 9 b especially during the hydrological drought development period however the inflow outflow relationship was quite different for scenario 3 there was only a small difference between the inflow and outflow of the reservoir fig 9 c hence the xinfengjiang reservoir operation was conducive to the reduction or mitigation of the short term hydrological drought events scenario 2 which refers to the hydrological drought events whose ds are shorter than the average d of all hydrological drought events in contrast there was an opposite effect on the long term hydrological drought events scenario 3 referring to the hydrological drought events whose ds are longer than the average d of all hydrological drought events a possible physically based explanation of this phenomenon is as follow before reaching the peak intensity of the hydrological drought the reservoir tended to release water to mitigate the drought severity this is because any rational decision maker hopes to reduce losses caused by drought when the drought develops towards peak intensity however the reservoir needs to store a certain amount of water during the drought recovery period to meet other important functions as power generation and shipping especially during a long term drought at the same time the recovery of a drought indicates the increase of water sources which puts water storage possible the reservoir should store water as much as possible during flood season for two reasons 1 flood control and 2 water compensation for drought season water use these inflow outflow relationships of the xinfengjiang reservoir can be used to explain why the ddd and corresponding tvdp decreased but the drd and corresponding tvrp increased at the heyuan hydrometric station the xinfengjiang reservoir tends to release water during the early hydrological drought but stores water in the recovery period of the hydrological drought when facing severe and long hydrological drought events as a result the impacts of xinfengjiang reservoir operation e g the regulating ability of the xinfengjiang reservoir on the hydrological drought are weakened with the increase in the d of the hydrological drought these results are also generally in agreement with previous study results e g wu et al 2018b rangecroft et al 2019 the influence of reservoir operation on the hydrological drought is mainly observed for short term events and their d s is decreased mitigated whereas there is less impact on the long term hydrological drought events nevertheless the influence of reservoir operation on the internal propagation of hydrological drought e g ddd drd pi and corresponding severity were not examined in these previous studies but were addressed in our study it is worth noting that different reservoir operation patterns may have different effects on hydrological drought propagation but overall wu et al 2017 reservoir operations have aggravating or mitigating effects on hydrological drought lopez moreno et al 2009 tijdeman et al 2018 this study provides a basic framework for analyzing the impacts of human regulations especially reservoir operations on the development and recovery processes of hydrological drought in other regions of the world so that reservoir operation patterns can be optimized during drought periods in addition a recent study wu et al 2018b also used the dongjiang river basin as a study area to investigate the impacts of reservoir operation on hydrological drought by using the pre post disturbance comparative approach and analyzed the influences of the reservoir operation rules during drought periods on the multi scale correlations between hydrological drought and meteorological drought unlike this study the authors explored the changes in the hydrological drought by evaluating the changes in the correlations with the meteorological drought and by comparing the correlations between the undisturbed and disturbed period whereas we examined the impacts of human regulations especially reservoir operations on the internal propagation of hydrological drought i e development and recovery process using the simulated observed comparative approach 4 4 limitations and future study although the simulated observed comparative approach was used to determine the impact of human regulations on the internal propagation of hydrological drought in this study this approach still has some limitations the uncertainty related to the spatial data is a crucial issue that cannot be overlooked even if a suitable hydrological model is used trambauer et al 2013 wang et al 2011 for example only one stage or one date of spatial data i e dem soil type and land use was used to create the swat model and simulate the long term streamflow in the study basin and it is assumed that the spatial information may change over time mishra et al 2018 van loon et al 2019 trambauer et al 2013 although the spatial information of one stage does not effectively reflect the long term hydrological process the changes in the hydrological process in a certain period can be compared by addressing the shortcomings of the specific spatial information moreover several studies have pointed out that the swat model is insensitive to low flow because the objective functions or performance indices used to calibrate the model tend to reflect specific hydrological process e g high flow lin et al 2015 but the overall fluctuation of the simulated based ssi and observed based ssi are basically consistent see figs 4 and 5 except for the stations that are strongly affected by the xinfengjiang reservoir operation e g heyuan hydrometric station although the variation of hydrological drought propagation in a certain catchment may be affected by various factors such as climate change large scale atmospheric patterns e g enso and land use cover change it is undeniable that the main influencing factor at the heyuan hydrometric station is the operation of the xinfengjiang reservoir zhang et al 2015b tu et al 2015 we did not separate the effects of climate change reservoir operation and other human activities e g land use cover change on the internal propagation of hydrological drought owing to the currently limited research technology but overall at the heyuan hydrometric station which is located just below the outlet of the xinfengjiang reservoir the variation of the internal propagation of hydrological drought is mainly affected by the xinfengjiang reservoir operation this finding has also been confirmed by previous studies i e the influence of the reservoir decreased with increasing distance from the xinfengjiang reservoir zhang et al 2015b future studies will consider more detailed information regarding the influence of a specific human activity on the evolution of the internal propagation of hydrological drought additional spatial information more hydrometeorological gauging stations monitored in near real time and the use of other methods and satellite based precipitation products will be incorporated to establish a real time monitoring and tracking system of hydrological drought propagation in watersheds for drought prevention and mitigation we will also integrate the swat model with various suitable resources for meteorological forecasting scenarios e g global climate model gcm by using different drought forecasting methods the development and recovery process of hydrological drought will be determined which can be a help for developing an early warning hydrological drought system for the heyuan station we need to modify the reservoir parameters involved in the swat model to further optimize the simulation flow results to improve performance of the hydrological drought early warning system 5 conclusions given certain meteorological data the semi distributed hydrological model swat which is calibrated for a period with less human regulations is suitable to analyze the development and recovery processes of hydrological drought during the human regulation period based on the simulated observed comparison approach for the dongjiang river basin the short term hydrological drought was shortened and was less severe due to reservoir operation whereas the long term hydrological drought had the opposite effect the average d s increased aggravated under the effects of reservoir operation the beneficial impact of the xinfengjiang reservoir on the short term hydrological drought was achieved by increased water availability during drought periods development and recovery process however when the basin was facing severe or long lasting hydrological droughts the impact of the xinfengjiang reservoir on the hydrological drought was limited or even negative i e water was released during the drought development and was stored during the recovery period therefore the operation patterns of the xinfengjiang reservoir in the study area should be further improved to ensure water security during droughts especially during a long term hydrological drought since the operation patterns of a large reservoir and its impact on the downstream hydrological drought propagation are very complex further studies are required declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research is supported by funding from the national key r d program of china 2017yfc0405900 and the national natural science foundation of china grant no 51861125203 51809294 and the international program for ph d candidates sun yat sen university appendix ssi calculation steps from the monthly streamflow series the ssi was obtained the procedure is based on the calculation of accumulated probability f x of each monthly value and its further normalization in standard units vicente serrano et al 2011 the calculation of f x is essential to obtaining the ssi the three parameters for log normal distribution were tested to describe streamflow data the ssi in z scores is calculated using the following equation ssi w c 0 c 1 w c 2 w 2 1 d 1 w d 2 w 2 d 3 w 3 where the constants are c0 2 515517 c1 0 802853 c2 0 010328 d1 1 432788 d2 0 189269 d3 0 001308 and w 2 ln p for p 0 5 p is the probability of exceeding a determined x value p 1 f x if p 0 5 p is replaced by 1 p and the sign of the resultant ssi is reversed the ssi is normalized so that the mean is o and standard deviation is 1 
6252,the kinematic wave model kwm representing a simplification of the full wave model fwm under the shallow water approximation is often used in the one dimensional formulation for describing the dynamics of debris mud flood the present work investigates the applicability conditions of the kwm to analyze mud floods characterized by a rheological behaviour expressed through a power law model the study is carried out through the numerical solution of both the full and kinematic wave models changing the characteristic time of hydrograph i e the wave duration imposed at the channel inlet the predictions of the two models are compared in terms of maximum flow depth maximum discharge and peak discharge at the downstream channel end the study is performed for several values of the froude f and the kinematic wave k numbers similarly to the clear water case present results show that higher errors in applying kwm pertain to shorter flood wave durations but the performance of kwm appears to strongly depend on the value of the rheological index becoming worse as the fluid rheology becomes more shear thinning the study furnishes applicability criteria representing a guideline for practical applications in terms of minimum wave duration above which the kwm error in reproducing the considered flood characteristics the maximum flow depth the maximum discharge and the peak discharge at the downstream channel end is less or equal than 5 finally it has been shown that the wave period criterion obtained considering linearized wave dynamics may be safely applied at least for fluid with moderate shear thinning behavior and for moderate values of the dimensionless number kf 2 keywords mud flow power law rheology shear thinning fluid kinematic wave model unsteady flow 1 introduction the terms debris and mud flows used to identify rapid movement of sediment and water mixtures propagating along slopes are classified according to a nomenclature based on the different origin the composition and the flow characteristics takahashi 2014 in particular debris flows are characterized by the presence of gravel sediments and their dynamics is controlled by erosion deposition and grain collisions gregoretti et al 2019 conversely mud flows involve highly concentrated mixtures of water and fine sediments with a rheology strongly influenced by the quantity of the fine fraction due to the modification of the rainfalls and the increased urbanization the frequency of debris mud flows has increased in the last decades these phenomena have an exceptional destructive power producing severe damages on the impacted areas due to the high velocity and the large volumes of mobilized transported and deposited sediments fuchs et al 2007 jacob et al 2012 thiene et al 2017 gregoretti et al 2018 stancanelli and musumeci 2018 chen et al 2019 the individuation of hazard prone areas is of utmost importance to estimate the effect of an event on populations properties and environment harms ringdahl 2004 hurlimann et al 2006 for individuating hazard areas different approaches may be used based on environmental surveys and monitoring of previous events e g di et al 2008 he et al 2003 hungr 2011 and or numerically predicting the evolution of potentially dangerous scenarios numerous numerical methods considering the mass momentum and energy conservation equations have been proposed in literature for simulating the debris mud flow propagation a possible classification pertains to the schematization of the water solid mixture a first option for the mixture analysis consists in separately describing the liquid and the solid components and therefore adopting the so called two phase approach e g pitman and le 2005 pudasaini 2012 he et al 2014 di cristo et al 2016 the sophisticated models proposed by armanini et al 2014 and by iverson and george 2014 belong to this class more adapt for representing debris flows in which the role of the solid and liquid phase must be distinguished the former model uses the kinetic theory for the collisional regime dominant close to the free surface for the frictional regime a specific model is adopted which matches the coulomb condition at the boundary with the loose static bed in the model by iverson and george 2014 the balance equations describe the coupled evolution of the solid volume fraction the basal pore fluid pressure the flow thickness and the two components of flow velocity basal friction is evaluated using a generalized coulomb rule and the fluid motion is evaluated in a reference frame that translates with the velocity of the granular phase source terms in each of the depth averaged balance equations account for the influence of the granular dilation rate considering both the effects of elastic compressibility and of an inelastic dilatancy angle a different approach models the mixture as a homogeneous single phase medium with a non newtonian behaviour takahashi 2014 the characteristics of the apparent fluid are defined by the relationship between the shear stress and the strain rate that is called the constitutive law the most commonly used single phase models are bingham yielding with subsequent newtonian behaviour power law or ostwald de waele herschel bulkley yielding with subsequent power law behaviour moreover it is often adopted also a variant of the bingham model which introduces a quadratic term aiming to account even for the turbulent dispersive terms o brien et al 1993 the models with a yield stress often called visco plastic reproduce the stopping of the flow as a deposit in flat areas and they are therefore widely used in many engineering applications e g coussot 1997 huang and garcia 1998 chanson et al 2006 ancey 2007 chambon et al 2009 pastor et al 2014 however since a correct rheology characterization of the visco plastic fluids is a subtle task ovarlez et al 2011 chambon et al 2014 also the power law without yield stress rheology is often employed e g jeong et al 2009 values of the power law exponent larger than one refer to dilatant or shear thickening fluids while values smaller than one describe the shear thinning behavior the widely used bagnold rheology belongs to the former category the exponent is equal to two and it is employed for representing stony type debris flow in which the grain collisions are dominant pudasaini 2011 takahashi 2014 conversely the shear thinning power law model is particularly adequate for describing fine magmas and mining residuals dilute clay or kaolin suspensions and river flows with a finite fraction of coarse grains such as mud flows ng and mei 1994 hwang et al 1994 perazzo and gratton 2004 longo et al 2015 for instance shear thinning power law model has been found to mimic the rheology of both the natural estuarine mud dredged from haihe river in tianjin and mazhou island zhang et al 2010 and the catastrophic landslide occurred in 1999 in cervinara italy carotenuto et al 2015 starting from the cauchy momentum equations and assuming a power law relation between the stress and the rate of strain tensors several shallow layer models have been proposed owing to its simplicity the model deduced through the von kàrmàn momentum integral method i e saint venant equations is often preferred in environmental applications to other more rigorous models such as the ones deduced through either the asymptotic expansions of the solution of the cauchy momentum equations e g fernández nieto et al 2010 noble and vila 2013 or the weighted residual method e g ruyer quil and manneville 2002 independently on the approach used for describing the solid liquid mixture i e two or single phase the two dimensional shallow layer models allow to accurately reproduce the mud flow dynamics armanini et al 2009 2014 di cristo et al 2018 fent et al 2018 greco et al 2019 iverson and george 2014 medina et al 2008 o brien et al 1993 rosatti and begnudelli 2013 but they require considerable computational time and storage moreover their application is particularly challenging in presence of irregular topography yan et al 2013 these limitations prevent their use for large scale investigations and for civil protection studies where quick analyses of the possible impacted areas are required to overcome such a constraint simplified models are often employed for describing the dynamics of debris mud flows e g o brien et al 1993 arattano and savage 1994 chiang et al 2012 deangeli 2008 gregoretti et al 2016 2019 bernard et al 2019 simplified models are less computationally expensive than the full one and they are also characterized by a less sensitive response to the noise in the input data yu and lane 2006 weill et al 2014 aricò et al 2016 in this context is of utmost importance to define the conditions in which the simplified models can be used with enough accuracy the kinematic wave model neglecting in the momentum equation the local and convective inertia and the pressure gradient terms is the simplest approximation a direct comparison between the results of the kinematic model with experimental data has been carried out by arattano et al 2006 with reference to the debris flow occurred in 2004 in an experimental basin on the italian alps the rheological model is characterized by the presence of a yield stress plus a quadratic term honda and egashira 1997 it has been shown that the simplified model is able to describe the main features of the flood moreover the comparison with of the full model led the authors to conclude that the rheological parameters influence the results more than the inertial terms in the saint venant equations giving additional support to the use of the kinematic wave model for the debris flow prediction unfortunately as far as shear thinning fluids are concerned a similar comparison between the outcomes of the kinematic wave model with field data is not available however for this kind of fluids the validity of the kinematic approximation has been tested by longo et al 2015 in reproducing laboratory dam break tests performed in a constant slope channel with different cross sections a good agreement between the experiments and the theoretical developments in terms of front position has been observed particularly at late times in the early stage the discrepancies were attributed to the high depth to length aspect ratio the above analysis has been extended to the case of a varying longitudinal channel slope in longo et al 2016 confirming the validity of the simplified model even if applicability criteria of simplified models have been widely studied for flood of clear water e g ponce et al 1978 moussa and boequillon 1996 singh and aravamuthan 1996 tsai 2003 perumal and sahoo 2007 moramarco et al 2008a b only few works addressed the problem for non newtonian fluids with reference to a power law rheology di cristo et al 2014 deduced the applicability criteria of some approximated models i e kinematic diffusion and quasi steady in terms of dimensionless wave period of the flow perturbation assuming as initial condition the uniform one the linearized version of both the full and the approximated models in an unbounded channel has been analyzed the study has been extended to finite length channels in di cristo et al 2018b through the evaluation of the upstream and downstream channel response functions of the linearized problem recently di cristo et al 2018c have investigated the applicability conditions of the kinematic and the diffusive wave models in power law flows accounting for the non linearity of governing equations the study compared the analytical solution of the steady flow depth profiles predicted by the approximated models with those of the full dynamic wave one in a channel of finite length the results put in evidence the important effect of the rheology revealing also that the applicability ranges of both approximated models may strongly differ from the corresponding ones for the clear water case the present paper aims to extend to unsteady flow conditions the previous study of di cristo et al 2018c by investigating the applicability of the simplified kinematic wave model kwm in predicting the propagation of a mud flow this work considers the power law model proposed by ng and mei 1994 for reproducing mud flows characterized by a highly concentrated mixture of water and fine sediments which usually occur in river flows with a small fraction of coarse grains the model is deduced through the von karman momentum integral method and accounts for the variation of the velocity distribution along the flow depth in this context the accuracy of the kwm in reproducing some characteristics of mud floods with a power law rheology is analyzed the study is carried out through a numerical analysis of both the full and the kinematic wave models considering at the channel inlet discharge hydrographs characterized by different wave periods an explicit first order scheme for the temporal discretization and a second order finite volume method for the spatial discretization are used the applicability conditions are expressed considering three dimensionless parameters related to the error on the maximum flow depth the maximum discharge and the peak discharge at the downstream end of the channel the effect of the rheology on the kwm applicability is deeply investigated and the derived criteria are compared with the analytical ones obtained by di cristo et al 2014 the paper is organized as follows section 2 reports the governing equations for the full wave model fwm and of the kinematic wave model kwm while in section 3 the numerical methods are presented section 4 illustrates the results of the comparison between the fwm and the kwm and the obtained applicability criteria finally conclusions are drawn in section 5 2 governing equations let us consider a one dimensional unsteady gradually varied laminar flow of a layer of power law fluid flowing over a non erodible bed inclined of an angle θ with respect to the horizontal plane lateral inflow or outflow are not allowed the dimensional depth averaged momentum and mass conservation equations are di cristo et al 2013a 1 q t β x q 2 h g h h x cos θ g h sin θ τ b ρ 2 h t q x 0 where t is the time x is the streamwise coordinate h the flow depth q the flow rate for unit of width g and ρ the gravity and the fluid density respectively β and τ b are the momentum correction factor and the bottom stress respectively denoting with u q h the depth averaged velocity in laminar regime the expressions of the momentum correction factor and of the bottom stress are di cristo et al 2013 ng and mei 1994 respectively 3 β 2 2 n 1 3 n 2 1 4 τ b μ n 2 n 1 n u h n µn and n being the consistency and the rheological index of the power law fluid respectively if the rheological index n is smaller than one shear thinning fluid the effective viscosity decreases with the deformation amount modelling the disintegration of fluid structure under shear conversely when the rheological index is larger than one shear thickening fluids the viscosity increases with the amount of shearing implying that the fluid microstructure is build up by the fluid motion mei et al 2001 in this work only shear thinning fluids have been considered denoting with l the dimensional channel length for a given flow rate q ref for unit width the following dimensionless quantities are introduced 5 x x l t t q ref l h n h h h n q q q ref where h n denotes the dimensional normal i e uniform flow depth corresponding to q ref accounting for eq 5 and 3 eqs 1 and 2 may be rewritten in their dimensionless form as follows 6 h t q x 0 7 q t x β q 2 h x h 2 2 f 2 k h 1 q n h 2 n 1 where the normal froude number f and the kinematic wave number k are 8 f q ref h n 3 2 g cos θ k 1 f 2 l h n tan θ similarly to the turbulent clear water tcw case govindaraju et al 1988a b moramarco et al 2008a even for the power law fluids the two dimensionless numbers f and k uniquely define the problem under investigation in what follows similarly to the tcw case moramarco et al 2008a one of the two pairs f k or f k f 2 is considered it is easy to verify that the system 6 7 is hyperbolic and the expression of the characteristic slopes is di cristo et al 2017 9 λ β q h β β 1 q 2 h 2 h f 2 the kwm is obtained neglecting in the momentum equation the local and convective inertia and the pressure gradient terms represented by all terms at the l h s of eq 7 therefore starting from the simplified version of eq 7 the flow rate may be expressed only in terms of flow depth q h h 2 n 1 n and by substituting this expression in eq 6 the following equation is deduced 10 h t x h 2 n 1 n 0 conversely expressing the flow depth only in terms of flow rate h q q n 2 n 1 and rewriting eq 6 as follows 11 q t d q d h q x 0 the following version of kwm in terms of flow rate only is obtained 12 q t x 2 n 1 2 n 3 n 2 q 3 n 2 2 n 1 0 the linearized version around the reference state of eqs 10 or 12 allows to easily deduce the following expression for dimensionless celerity of the kwm 13 c k w m p l f 2 n 1 n in agreement with the findings of di cristo et al 2014 while in for turbulent clear water flows the celerity of the kwm is constant and equal to 3 2 eq 13 indicates that for a power law fluid it depends on the rheological index 3 unsteady analysis 3 1 numerical solution of full wave model the numerical solution of system 6 8 is pursued by using an explicit first order scheme for the temporal discretization and a second order finite volume method for the spatial discretization rewriting eqs 6 7 in the following compact form 14 w t f w x s w where 15 w h q f w q β q 2 h 1 2 h 2 f 2 s w 0 k h 2 n 1 q n h 2 n the corresponding discretized equation reads 16 w i k 1 w i k δ t 1 δ x f i 1 2 k f i 1 2 k s i k in which w i and s i are the averaged values of the variable w and of the source term pointwisely evaluated s in the i th volume in eq 16 the k superscript refers to time tk kδt with δt the integration time step and δx the finite volume length while f i 1 2 and f i 1 2 represent the numerical approximation of the fluxes at the volume interfaces i 1 2 and i 1 2 respectively the following expression of f is considered harten et al 1983 17 f f l if 0 λ l λ r f l λ l f r λ r λ l w r w l λ r λ l if λ r 0 λ l f r if 0 λ r with 18 λ r max r l λ 0 λ l min r l λ 0 in eq 17 w l and w r represent a piecewise linear reconstruction of w on the left and right sides of the volume interface respectively in order to preserve the monotonicity of the scheme in the reconstruction process the min mod operator gottlieb and shu 1998 is applied additional details on the numerical model may be found in di cristo et al 2017 3 2 numerical solution of kinematic wave model the kwm is solved in terms of flow rate variable q through eq 12 rewritten as follows 19 q t f q x 0 with f q 2 n 1 2 n 3 n 2 q 3 n 2 2 n 1 eq 19 is numerically integrated again through an explicit first order scheme in time and a second order finite volume scheme in space applying the euler muscl hancock method toro 2009 the discretized version reads 20 q i k 1 q i k δ t 1 δ x f i 1 2 k f i 1 2 k 0 where q i k is the flow rate averaged value in the i th volume at the time tk in eq 20 f i 1 2 k and f i 1 2 k represent the numerical approximation of the fluxes at the volume interfaces i 1 2 and i 1 2 respectively and they read toro 2009 21 f i 1 2 k f q i k 1 2 1 c f l i δ q i 1 2 δ x f i 1 2 k f q i 1 k 1 2 1 c f l i δ q i 1 2 δ x where δqi 1 2 resp δqi 1 2 is the slope of q x function at the i 1 2 resp i 1 2 interface and the cfli number is evaluated in the i th volume at the time tk as 22 cf l i 2 n 1 n q i k n 1 2 n 1 δ t δ x in order to preserve the scheme monotony the superbee limiter toro 2009 has been applied in calculating both slopes δqi 1 2 and δqi 1 2 3 3 the performed tests following moramarco et al 2008b in the present study several tests considering different flow conditions characterized by different synthetic hydrographs imposed at the channel inlet have been carried out each hydrograph is characterized by a wave duration t m t 0 with t 0 the dimensionless time to peak and m an integer larger than 1 the dimensionless time to peak t 0 is evaluated as the wave travel time of the reference flow rate and therefore accounting for the reference length scale it reads 23 t 0 1 c k w m p l f n 2 n 1 as indicated in eq 23 t 0 is only function of the fluid rheology the following four parameters pearson type iii distribution is assumed for the hydrograph shape moramarco et al 2008b 24 q 0 t q p t t 0 1 γ 1 e 1 t t 0 γ 1 where qp is the dimensionless peak discharge and γ is the shape dimensionless factor which assumes two different values for the rising γ γris for t t 0 and the recession γ γrec for t t 0 limbs respectively for all tests qp is assumed equal to 2 and γris equal to 1 3 according to moramarco et al 2008b then the γrec value is defined imposing as condition that the time averaged value of the discharge equals the dimensionless reference discharge q ref 1 expressed as 25 1 t 0 t q 0 t d t 1 in the performed tests it results that the γrec value ranges between 1 1 and 51 for m between 2 and 90 idealized input hydrographs are often used in numerical tests for flood routing e g perumal and sahoo 2007 perumal et al 2007 moramarco et al 2008b dottori et al 2009 fenton 2019 moreover zucco et al 2015 showed that the adopted hydrograph shape after calibration well lends itself to represent also the actual single peak floods occurring in natural channels considering the correlation between debris mud flow discharge and water flow discharge takahashi 1991 vandine 1985 chen et al 2008 the hydrograph shape adopted for the numerical tests may be considered also representative for debris mud flows for the kwm none additional boundary condition is required for the fdm whenever at the channel inlet the current is hypercritical i e λ assumes positive value an additional boundary condition has to be assigned imposing the validity of rating curve at each time the following flow depth value at the channel inlet is prescribed 26 h 0 t q 0 t n 2 n 1 following moramarco et al 2008b in hypocritical condition the critical flow depth is imposed as boundary condition at the channel outlet which for a power law fluid reads di cristo et al 2018c 27 h 1 t β q 2 1 t f 2 3 several tests have been carried out with the following values of the wave period t 2 t 0 3 t 0 10 t 0 20 t 0 30 t 0 60 t 0 90 t 0 the froude number have been fixed in the ranges and 0 5 0 8 and the dimensionless parameter kf 2 has been varied up to 20 as far as the rheology of the fluid is concerned the whole shear thinning range has been explored namely n 1 all simulations have been performed with fixed values of δx and δt namely δx 0 005 and δt 10 6 verifying that the courant condition has been always satisfied 4 results 4 1 unsteady analysis results the accuracy of the kwm in reproducing the results of the fwm is assessed using different dimensionless indicators moramarco et al 2008b firstly the dimensionless error on the maximum flow depth ε h max and the maximum discharge ε q max along the channel are evaluated as 28 ε h max x h max kwm x h max fwm x h max fwm x 100 29 ε q max x q max kwm x q max fwm x q max fwm x 100 in eq 28 respectively eq 29 h max fwm and h max kwm respectively q max fwm and q max kwm are the dimensionless maximum flow depth respectively discharge computed by the fwm and the kwm respectively secondly the mean values of ε h max and ε q max along the channel namely ε h max and ε q max are considered with the aim of excluding the regions where the boundary conditions may have a large influence ε h max and ε q max are computed limitedly for 0 05 x 0 95 moramarco et al 2008b finally a third index is considered namely the percent error of the peak discharge at the outlet defined as 30 ε qp q p kwm q p fwm 1 100 being q p fwm and q p kwm the peak discharge at the outlet computed by the fwm and the kwm respectively fig 1 a b represent the effect of the rheology on the maximum flow depth error ε h max along the channel considering for the sake of example kf 2 7 5 and f 0 5 two different fluids characterized by n 0 25 and n 1 are considered each curve refers to a single wave duration ranging between 2 t 0 and 90 t 0 the maximum flow depth error ε h max varies along the abscissa and it depends on both the rheological index and the wave duration for both n values and similarly to the turbulent clear water case moramarco et al 2008b all curves show a monotone behavior with respect to both the channel abscissa with an increase of ε h max in the downstream direction moreover the maximum flow depth error ε h max decreases with the wave duration with errors lower than 10 for t t 0 30 for both considered rheologies finally the rheology substantially affects the magnitude of the errors fig 1a b highest errors are observed for the smallest rheological index for instance for t t 0 2 at x 0 3 the error is 36 for n 0 25 while decreases to 7 for n 1 0 independently of the t t 0 value for n 1 0 the error is less than 5 for x 0 26 while for n 0 25 such a bound for the error is overwhelmed in the whole channel for t t 0 20 a more extensive analysis performed considering different combinations of the pair f kf 2 values results not shown confirms the qualitative results observed in fig 1 to better assess the effect of the rheology on the applicability conditions fig 2 a b report in the x kf 2 plane for f 0 5 and for four different rheological indexes n 0 25 0 50 0 75 1 0 the region where the kwm is applicable with an error less than 5 based on the ε h max indicator two different wave durations have been considered namely t t 0 2 fig 2a and t t 0 20 fig 2b for a fixed n value the region of the x kf 2 plane to the left of the curve corresponds to the conditions of local applicability of the approximated model fig 2c d are the counterparts of fig 2a b with f 0 8 for a fixed n value and independently of the froude number value and the wave duration all plots of fig 2 suggest that the applicability region enlarges with the kf 2 values similarly to the turbulent clear water case moramarco et al 2008b moreover for a given triplet kf 2 f t t 0 the performance of the approximated model deteriorates as the fluid rheology becomes more shear thinning i e when n reduces for instance considering kf 2 10 at f 0 5 and t t 0 2 fig 2a indicates that the kwm predicts with the prescribed tolerance the maximum flow depth in about the first 30 of the channel length for n 1 while this length becomes less than 10 in the n 0 25 case for a concise comparison table 1 reports the upper bounds of the channel abscissa xlim for the applicability of the kwm at kf 2 10 for the different rheological indexes the values reported in table 1 compared with the one pertaining to the turbulent clear water case x lim tcw 0 5 see fig 2 of moramarco et al 2008b allow to conclude that the laminar power law rheology strongly reduces the applicability conditions of the kwm the combined examination of fig 2a c corresponding to the same wave duration t t 0 2 but referring to different froude numbers suggests that the froude number value has a negligible influence on the performance of the kwm some small differences are observed only for small kf 2 values and for fluids with a small rheological index the same comparison for the higher wave duration t t 0 20 fig 2b and d shows differences with a larger applicability region for the lower froude number f 0 5 with increasing differences for lower rheological index values for instance in the case f 0 5 for n 0 25 and kf 2 10 the kwm is applicable in the first 85 of the channel while for f 0 8 it reduces to the 65 the enlargement of the applicability region with kf2 for a fixed value of f may be easily explained by inspecting the dimensionless momentum conservation equation eq 7 for large values of kf 2 for a fixed value of the froude number the r h s of eq 7 becomes the leading term and therefore the fwm tends to the kinematic one the effect of the wave duration on the applicability of kwm may be again theoretically explained based on the dimensionless momentum equation indeed an increase of the wave duration reduces the l h s of eq 7 reducing the local inertia term previous results demonstrate that the kwm accurately approximates fdm over the entire channel length i e ε h max 0 05 for 0 x 1 only for sufficiently large values of t t 0 a less restrictive criterion may be derived considering the mean error of the maximum flow depth ε h max for f 0 5 fig 3 reports ε h max as function of the wave duration for the same rheological indexes of fig 2 and two different kf 2 values namely kf 2 2 5 fig 3a and kf 2 20 fig 3b coherently with figs 1 and 2 in all cases the mean error ε h max decreases with t t 0 and it is affected by kf 2 similarly to tcw moramarco et al 2008b ε h max decreases when kf 2 increases the comparison among the different curves confirms the strong dependence of the error on the rheology with a reduction of ε h max for increasing n for instance for kf2 2 5 fig 3a the mean error is less than 5 for t t 0 45 and t t 0 16 for n 0 25 and n 1 0 respectively in this way it is possible to individuate the minimum value of t t 0 above which the kwm can be applied with the prescribed accuracy for f 0 5 fig 4 depicts ε q max as function of the wave duration for kf 2 2 5 fig 5 a and kf 2 20 fig 5b for different rheological indexes similarly to ε h max the mean error on the maximum discharge has a monotone reduction with t t 0 and it decreases with kf 2 comparing figs 3 and 4 it is evident that the mean error on the maximum discharge is higher than ε h max while the dependency from the rheology is similar for instance for kf 2 2 5 fig 4a ε q max is always larger than 5 for n 0 25 while for n 1 0 it is less than 5 for t t 0 30 present results suggest that the minimum value of t t 0 individuated for ε h max does not provide the same accuracy in terms of mean error on the maximum flow discharge fig 5 is the counterpart of figs 3 and 4 in terms of error of the peak discharge at the outlet ε qp has the same behavior of the other two considered indicators with respect to t t 0 and kf 2 but it has larger values than both ε h max and ε q max in fact for kf2 2 5 fig 5a ε qp for the cases n 0 25 and n 0 5 is always larger than 5 for all the considered parameters the results obtained with f 0 8 not reported herein are very similar with some differences only for the smallest rheological index value and for wave durations t t 0 20 these observations confirm a larger influence of kf2 and a minor effect of the froude value on the kwm applicability condition in conclusion the range of conditions in which the kwm has a mean error of the maximum flow depth less than 5 is wider than the one necessary for obtaining the same accuracy in terms of maximum flow discharge and peak discharge at the outlet the rheological characterization is crucial because for small n values in many cases the kwm is not able to reproduce the fdm solutions with an accuracy above 95 finally the results shown in figs 2 5 allow to define the applicability conditions of the kwm and they can be used as a guideline for practical applications 4 2 comparison with the wave period criterion in what follows the results of the performed numerical analysis are employed to assess the effectiveness of the theoretical criterion proposed by di cristo et al 2014 which generalizes to power law fluids the wave period criterion proposed by ponce et al 1978 for clear water flood routing similarly to ponce et al 1978 the criterion of di cristo et al 2014 provides a lower bound of the dimensionless wave period θ above which the kwm approximates the fdm within a prescribed accuracy it is worth of nothing that the wave period criteria are applicable only to linearly stable flow conditions therefore as far as power law fluids are concerned the criterion of di cristo et al 2014 holds only for f f f being the limiting linear stability froude number ng and mei 1994 31 f n 2 n 1 in order to verify the wave period criteria for clear water flood routing moramarco et al 2008a related the dimensionless wave period θ to the wave duration t t 0 through the following relation 32 θ k f 2 c kwm t t 0 with ckwm the dimensionless celerity of kinematic wave model therefore assuming an error less then 5 eq 32 led moramarco et al 2008a to deduce the following lower bound of wave duration 33 t t 0 5 c kwm k f 2 θ 5 in which the dimensionless celerity and the dimensionless wave period were set equal to c kwm tcw 3 2 and θ 5 tcw 171 respectively ponce et al 1978 as far as the power law fluids are concerned eq 33 may be still applied provided that the dimensionless celerity ckwm is evaluated through eq 13 i e c kwm c kwm plf and the dimensionless wave period threshold refers to the value deduced for power law fluids θ 5 plf which depends on n as shown in fig 3b of di cristo et al 2014 the θ 5 plf values corresponding to the considered n values deduced from this figure are reported in table 2 the t t 0 5 threshold given by eq 33 is depicted in fig 6 as function of kf 2 for the different n values assuming the values of θ 5 plf reported in table 2 therefore accordingly to the wave period criterion the kwm can be applied with an accuracy equal or larger than 95 in any of condition characterized by a kf 2 t t 0 pair laying above the theoretical curve based on the results of the non linear numerical simulations for any of the conditions listed in table 2 the value of t t 0 above which the error of the three considered parameters is smaller than 5 is evaluated for several values of kf 2 in fig 6 triangles circles correspond to the mean error of the maximum depth respectively discharge ε h max ε q max whereas squares represent the 5 error threshold value for the peak discharge ε qp void and filled symbols refer to the cases f f 0 5 and f f 0 8 respectively the ensemble of the results of the non linear simulations reveals that the minimum t t 0 value for kwm applicability decreases with both kf 2 and n moreover a higher t t 0 threshold is almost always required to assure the prescribed accuracy based on the ε qp parameter compared with the other two error metrics for a fixed rheology no substantial differences are observed between the two investigated froude numbers fig 6 shows also that independently of the froude number value for n 0 25 and kf 2 20 the lines representing the di cristo et al 2014 s criterion are above all points this means that it may be safely applied for predict all the considered quantities since it is more restrictive than the one resulting from the non linear simulations such a conclusion holds even in the kf2 20 case and for both the averaged maximum flow depth and the flow rate but not for the peak discharge the above results apply even for n greater than 0 5 with exception of the kf 2 2 5 f f 0 5 case for which ε h max is always higher than the prescribed accuracy results not shown the performances of the wave period criterion deteriorate for the lowest value of n in fact independently on the froude number in the kf 2 2 5 case the mean error of the maximum depth is always higher than the prescribed accuracy results not shown same conclusion holds even in for kf 2 20 as far as the maximum discharge and the peak discharge quantities are concerned 4 3 discussion and final remarks the interest in using the kwm model for reproducing mud flow propagation implies the crucial question about its applicability from a practical point of view it is important to understand in which conditions its results represent a good approximation of the full model based on the presented study the users interested in applying kwm to engineering problems involving mud flows are warned that its applicability range reduces as the rheological index decreases and it is significantly influenced by the kf2 values moreover as expected the findings of the present analysis confirm that the kwm reproduces more reliable results for high values of the wave duration the results indicate also that the applicability conditions depend on the flow variable of interest the kwm reproduces with a better accuracy the maximum flow depth respect to the maximum flow discharge and the peak discharge at the outlet in other words complying with the applicability range relative to peak discharge guaranties the required accuracy also on both the maximum flow discharge and depth the study furnishes useful indications about the applicability condition of the kwm in terms of lower bound of dimensionless wave duration in order to have a mean error less than 5 on the reproduction of the considered flow variables this parameter is equivalent to the dimensionless wave period adopted in the criteria used for clear water i e ponce et al 1978 and power law fluids i e di cristo et al 2014 the proposed criterion may be adopted to use kwm for simulating mud flow propagation in a channel after studying the fluid rheology and considering the hydrodynamic characteristics i e f and k values based on it according with the variable of interest it is possible to define the minimum wave duration for which it may be adopted a further comparison demonstrated that for power law index values larger than 0 25 the wave period criterion obtained through a previous simpler linear analysis is more restrictive than the one deduced from the presented non linear simulations then it may be safely applied for all the considered variables the presented analysis has been carried out considering two froude number values namely f 0 5 and f 0 8 with reference to the jiang jia ravine mud n 0 3 ρ 2130 kg m 3 µ 150pasn see ng and mei 1994 and assuming a flow rate for unit width equal to 10 m2 s the corresponding bottom slope is 0 3 and 0 5 for f 0 5 and f 0 8 respectively 5 conclusions the present study investigates the applicability of the simplified kinematic wave model kwm in predicting the unsteady propagation of a mud flow wave accounting for the non linearity of the governing equations the fluid characterized by a highly concentrated mixture of water and fine sediments is represented through the power law model proposed by ng and mei 1994 the analysis is performed through a numerical analysis of both the full and kinematic wave models adopting an explicit first order scheme in time and a second order finite volume method for the spatial discretization the applicability conditions are expressed considering three dimensionless indicators related to the error on the maximum flow depth the maximum discharge and the peak discharge at the downstream end of the channel the results indicate that the error on the maximum flow depth ε h max increases moving in the downstream direction and that higher errors pertain to lower wave durations the rheology substantially affects the magnitude of the errors which have been found to increase with the shear thinning behavior of the fluid i e to decrease with the rheological exponent n the analysis defines also the range of applicability of the kwm in terms lower bound of the wave duration t t 0 above which the errors are within the 5 the limiting value of t t 0 depends on the rheological index and the dimensionless parameters f and kf2 f and k being the froude and the kinematic wave numbers respectively the results indicate an increase of the t t 0 lower bound a reduction of the applicability range as n decreases and a larger influence on it of kf 2 respect to f the range of conditions in which the kwm has a mean error of the maximum flow depth less than 5 is wider than the one necessary for obtaining the same accuracy in terms of maximum flow discharge and peak discharge at the outlet finally the obtained criteria are compared with the wave period criterion theoretically deduced by di cristo et al 2014 through a linear analysis at least for moderate values of the dimensionless number kf 2 and for power law index values larger than 0 25 the wave period criterion has been found to be more restrictive than the one resulting from the non linear analysis which may be therefore safely adopted to assess the applicability of the kinematic wave model to mud routing declaration of competing interest none acknowledgement the work described in the present paper was realized in the framework of the project misalva financed by the italian minister of the environment land protection and sea cup h36c18000970005 
6252,the kinematic wave model kwm representing a simplification of the full wave model fwm under the shallow water approximation is often used in the one dimensional formulation for describing the dynamics of debris mud flood the present work investigates the applicability conditions of the kwm to analyze mud floods characterized by a rheological behaviour expressed through a power law model the study is carried out through the numerical solution of both the full and kinematic wave models changing the characteristic time of hydrograph i e the wave duration imposed at the channel inlet the predictions of the two models are compared in terms of maximum flow depth maximum discharge and peak discharge at the downstream channel end the study is performed for several values of the froude f and the kinematic wave k numbers similarly to the clear water case present results show that higher errors in applying kwm pertain to shorter flood wave durations but the performance of kwm appears to strongly depend on the value of the rheological index becoming worse as the fluid rheology becomes more shear thinning the study furnishes applicability criteria representing a guideline for practical applications in terms of minimum wave duration above which the kwm error in reproducing the considered flood characteristics the maximum flow depth the maximum discharge and the peak discharge at the downstream channel end is less or equal than 5 finally it has been shown that the wave period criterion obtained considering linearized wave dynamics may be safely applied at least for fluid with moderate shear thinning behavior and for moderate values of the dimensionless number kf 2 keywords mud flow power law rheology shear thinning fluid kinematic wave model unsteady flow 1 introduction the terms debris and mud flows used to identify rapid movement of sediment and water mixtures propagating along slopes are classified according to a nomenclature based on the different origin the composition and the flow characteristics takahashi 2014 in particular debris flows are characterized by the presence of gravel sediments and their dynamics is controlled by erosion deposition and grain collisions gregoretti et al 2019 conversely mud flows involve highly concentrated mixtures of water and fine sediments with a rheology strongly influenced by the quantity of the fine fraction due to the modification of the rainfalls and the increased urbanization the frequency of debris mud flows has increased in the last decades these phenomena have an exceptional destructive power producing severe damages on the impacted areas due to the high velocity and the large volumes of mobilized transported and deposited sediments fuchs et al 2007 jacob et al 2012 thiene et al 2017 gregoretti et al 2018 stancanelli and musumeci 2018 chen et al 2019 the individuation of hazard prone areas is of utmost importance to estimate the effect of an event on populations properties and environment harms ringdahl 2004 hurlimann et al 2006 for individuating hazard areas different approaches may be used based on environmental surveys and monitoring of previous events e g di et al 2008 he et al 2003 hungr 2011 and or numerically predicting the evolution of potentially dangerous scenarios numerous numerical methods considering the mass momentum and energy conservation equations have been proposed in literature for simulating the debris mud flow propagation a possible classification pertains to the schematization of the water solid mixture a first option for the mixture analysis consists in separately describing the liquid and the solid components and therefore adopting the so called two phase approach e g pitman and le 2005 pudasaini 2012 he et al 2014 di cristo et al 2016 the sophisticated models proposed by armanini et al 2014 and by iverson and george 2014 belong to this class more adapt for representing debris flows in which the role of the solid and liquid phase must be distinguished the former model uses the kinetic theory for the collisional regime dominant close to the free surface for the frictional regime a specific model is adopted which matches the coulomb condition at the boundary with the loose static bed in the model by iverson and george 2014 the balance equations describe the coupled evolution of the solid volume fraction the basal pore fluid pressure the flow thickness and the two components of flow velocity basal friction is evaluated using a generalized coulomb rule and the fluid motion is evaluated in a reference frame that translates with the velocity of the granular phase source terms in each of the depth averaged balance equations account for the influence of the granular dilation rate considering both the effects of elastic compressibility and of an inelastic dilatancy angle a different approach models the mixture as a homogeneous single phase medium with a non newtonian behaviour takahashi 2014 the characteristics of the apparent fluid are defined by the relationship between the shear stress and the strain rate that is called the constitutive law the most commonly used single phase models are bingham yielding with subsequent newtonian behaviour power law or ostwald de waele herschel bulkley yielding with subsequent power law behaviour moreover it is often adopted also a variant of the bingham model which introduces a quadratic term aiming to account even for the turbulent dispersive terms o brien et al 1993 the models with a yield stress often called visco plastic reproduce the stopping of the flow as a deposit in flat areas and they are therefore widely used in many engineering applications e g coussot 1997 huang and garcia 1998 chanson et al 2006 ancey 2007 chambon et al 2009 pastor et al 2014 however since a correct rheology characterization of the visco plastic fluids is a subtle task ovarlez et al 2011 chambon et al 2014 also the power law without yield stress rheology is often employed e g jeong et al 2009 values of the power law exponent larger than one refer to dilatant or shear thickening fluids while values smaller than one describe the shear thinning behavior the widely used bagnold rheology belongs to the former category the exponent is equal to two and it is employed for representing stony type debris flow in which the grain collisions are dominant pudasaini 2011 takahashi 2014 conversely the shear thinning power law model is particularly adequate for describing fine magmas and mining residuals dilute clay or kaolin suspensions and river flows with a finite fraction of coarse grains such as mud flows ng and mei 1994 hwang et al 1994 perazzo and gratton 2004 longo et al 2015 for instance shear thinning power law model has been found to mimic the rheology of both the natural estuarine mud dredged from haihe river in tianjin and mazhou island zhang et al 2010 and the catastrophic landslide occurred in 1999 in cervinara italy carotenuto et al 2015 starting from the cauchy momentum equations and assuming a power law relation between the stress and the rate of strain tensors several shallow layer models have been proposed owing to its simplicity the model deduced through the von kàrmàn momentum integral method i e saint venant equations is often preferred in environmental applications to other more rigorous models such as the ones deduced through either the asymptotic expansions of the solution of the cauchy momentum equations e g fernández nieto et al 2010 noble and vila 2013 or the weighted residual method e g ruyer quil and manneville 2002 independently on the approach used for describing the solid liquid mixture i e two or single phase the two dimensional shallow layer models allow to accurately reproduce the mud flow dynamics armanini et al 2009 2014 di cristo et al 2018 fent et al 2018 greco et al 2019 iverson and george 2014 medina et al 2008 o brien et al 1993 rosatti and begnudelli 2013 but they require considerable computational time and storage moreover their application is particularly challenging in presence of irregular topography yan et al 2013 these limitations prevent their use for large scale investigations and for civil protection studies where quick analyses of the possible impacted areas are required to overcome such a constraint simplified models are often employed for describing the dynamics of debris mud flows e g o brien et al 1993 arattano and savage 1994 chiang et al 2012 deangeli 2008 gregoretti et al 2016 2019 bernard et al 2019 simplified models are less computationally expensive than the full one and they are also characterized by a less sensitive response to the noise in the input data yu and lane 2006 weill et al 2014 aricò et al 2016 in this context is of utmost importance to define the conditions in which the simplified models can be used with enough accuracy the kinematic wave model neglecting in the momentum equation the local and convective inertia and the pressure gradient terms is the simplest approximation a direct comparison between the results of the kinematic model with experimental data has been carried out by arattano et al 2006 with reference to the debris flow occurred in 2004 in an experimental basin on the italian alps the rheological model is characterized by the presence of a yield stress plus a quadratic term honda and egashira 1997 it has been shown that the simplified model is able to describe the main features of the flood moreover the comparison with of the full model led the authors to conclude that the rheological parameters influence the results more than the inertial terms in the saint venant equations giving additional support to the use of the kinematic wave model for the debris flow prediction unfortunately as far as shear thinning fluids are concerned a similar comparison between the outcomes of the kinematic wave model with field data is not available however for this kind of fluids the validity of the kinematic approximation has been tested by longo et al 2015 in reproducing laboratory dam break tests performed in a constant slope channel with different cross sections a good agreement between the experiments and the theoretical developments in terms of front position has been observed particularly at late times in the early stage the discrepancies were attributed to the high depth to length aspect ratio the above analysis has been extended to the case of a varying longitudinal channel slope in longo et al 2016 confirming the validity of the simplified model even if applicability criteria of simplified models have been widely studied for flood of clear water e g ponce et al 1978 moussa and boequillon 1996 singh and aravamuthan 1996 tsai 2003 perumal and sahoo 2007 moramarco et al 2008a b only few works addressed the problem for non newtonian fluids with reference to a power law rheology di cristo et al 2014 deduced the applicability criteria of some approximated models i e kinematic diffusion and quasi steady in terms of dimensionless wave period of the flow perturbation assuming as initial condition the uniform one the linearized version of both the full and the approximated models in an unbounded channel has been analyzed the study has been extended to finite length channels in di cristo et al 2018b through the evaluation of the upstream and downstream channel response functions of the linearized problem recently di cristo et al 2018c have investigated the applicability conditions of the kinematic and the diffusive wave models in power law flows accounting for the non linearity of governing equations the study compared the analytical solution of the steady flow depth profiles predicted by the approximated models with those of the full dynamic wave one in a channel of finite length the results put in evidence the important effect of the rheology revealing also that the applicability ranges of both approximated models may strongly differ from the corresponding ones for the clear water case the present paper aims to extend to unsteady flow conditions the previous study of di cristo et al 2018c by investigating the applicability of the simplified kinematic wave model kwm in predicting the propagation of a mud flow this work considers the power law model proposed by ng and mei 1994 for reproducing mud flows characterized by a highly concentrated mixture of water and fine sediments which usually occur in river flows with a small fraction of coarse grains the model is deduced through the von karman momentum integral method and accounts for the variation of the velocity distribution along the flow depth in this context the accuracy of the kwm in reproducing some characteristics of mud floods with a power law rheology is analyzed the study is carried out through a numerical analysis of both the full and the kinematic wave models considering at the channel inlet discharge hydrographs characterized by different wave periods an explicit first order scheme for the temporal discretization and a second order finite volume method for the spatial discretization are used the applicability conditions are expressed considering three dimensionless parameters related to the error on the maximum flow depth the maximum discharge and the peak discharge at the downstream end of the channel the effect of the rheology on the kwm applicability is deeply investigated and the derived criteria are compared with the analytical ones obtained by di cristo et al 2014 the paper is organized as follows section 2 reports the governing equations for the full wave model fwm and of the kinematic wave model kwm while in section 3 the numerical methods are presented section 4 illustrates the results of the comparison between the fwm and the kwm and the obtained applicability criteria finally conclusions are drawn in section 5 2 governing equations let us consider a one dimensional unsteady gradually varied laminar flow of a layer of power law fluid flowing over a non erodible bed inclined of an angle θ with respect to the horizontal plane lateral inflow or outflow are not allowed the dimensional depth averaged momentum and mass conservation equations are di cristo et al 2013a 1 q t β x q 2 h g h h x cos θ g h sin θ τ b ρ 2 h t q x 0 where t is the time x is the streamwise coordinate h the flow depth q the flow rate for unit of width g and ρ the gravity and the fluid density respectively β and τ b are the momentum correction factor and the bottom stress respectively denoting with u q h the depth averaged velocity in laminar regime the expressions of the momentum correction factor and of the bottom stress are di cristo et al 2013 ng and mei 1994 respectively 3 β 2 2 n 1 3 n 2 1 4 τ b μ n 2 n 1 n u h n µn and n being the consistency and the rheological index of the power law fluid respectively if the rheological index n is smaller than one shear thinning fluid the effective viscosity decreases with the deformation amount modelling the disintegration of fluid structure under shear conversely when the rheological index is larger than one shear thickening fluids the viscosity increases with the amount of shearing implying that the fluid microstructure is build up by the fluid motion mei et al 2001 in this work only shear thinning fluids have been considered denoting with l the dimensional channel length for a given flow rate q ref for unit width the following dimensionless quantities are introduced 5 x x l t t q ref l h n h h h n q q q ref where h n denotes the dimensional normal i e uniform flow depth corresponding to q ref accounting for eq 5 and 3 eqs 1 and 2 may be rewritten in their dimensionless form as follows 6 h t q x 0 7 q t x β q 2 h x h 2 2 f 2 k h 1 q n h 2 n 1 where the normal froude number f and the kinematic wave number k are 8 f q ref h n 3 2 g cos θ k 1 f 2 l h n tan θ similarly to the turbulent clear water tcw case govindaraju et al 1988a b moramarco et al 2008a even for the power law fluids the two dimensionless numbers f and k uniquely define the problem under investigation in what follows similarly to the tcw case moramarco et al 2008a one of the two pairs f k or f k f 2 is considered it is easy to verify that the system 6 7 is hyperbolic and the expression of the characteristic slopes is di cristo et al 2017 9 λ β q h β β 1 q 2 h 2 h f 2 the kwm is obtained neglecting in the momentum equation the local and convective inertia and the pressure gradient terms represented by all terms at the l h s of eq 7 therefore starting from the simplified version of eq 7 the flow rate may be expressed only in terms of flow depth q h h 2 n 1 n and by substituting this expression in eq 6 the following equation is deduced 10 h t x h 2 n 1 n 0 conversely expressing the flow depth only in terms of flow rate h q q n 2 n 1 and rewriting eq 6 as follows 11 q t d q d h q x 0 the following version of kwm in terms of flow rate only is obtained 12 q t x 2 n 1 2 n 3 n 2 q 3 n 2 2 n 1 0 the linearized version around the reference state of eqs 10 or 12 allows to easily deduce the following expression for dimensionless celerity of the kwm 13 c k w m p l f 2 n 1 n in agreement with the findings of di cristo et al 2014 while in for turbulent clear water flows the celerity of the kwm is constant and equal to 3 2 eq 13 indicates that for a power law fluid it depends on the rheological index 3 unsteady analysis 3 1 numerical solution of full wave model the numerical solution of system 6 8 is pursued by using an explicit first order scheme for the temporal discretization and a second order finite volume method for the spatial discretization rewriting eqs 6 7 in the following compact form 14 w t f w x s w where 15 w h q f w q β q 2 h 1 2 h 2 f 2 s w 0 k h 2 n 1 q n h 2 n the corresponding discretized equation reads 16 w i k 1 w i k δ t 1 δ x f i 1 2 k f i 1 2 k s i k in which w i and s i are the averaged values of the variable w and of the source term pointwisely evaluated s in the i th volume in eq 16 the k superscript refers to time tk kδt with δt the integration time step and δx the finite volume length while f i 1 2 and f i 1 2 represent the numerical approximation of the fluxes at the volume interfaces i 1 2 and i 1 2 respectively the following expression of f is considered harten et al 1983 17 f f l if 0 λ l λ r f l λ l f r λ r λ l w r w l λ r λ l if λ r 0 λ l f r if 0 λ r with 18 λ r max r l λ 0 λ l min r l λ 0 in eq 17 w l and w r represent a piecewise linear reconstruction of w on the left and right sides of the volume interface respectively in order to preserve the monotonicity of the scheme in the reconstruction process the min mod operator gottlieb and shu 1998 is applied additional details on the numerical model may be found in di cristo et al 2017 3 2 numerical solution of kinematic wave model the kwm is solved in terms of flow rate variable q through eq 12 rewritten as follows 19 q t f q x 0 with f q 2 n 1 2 n 3 n 2 q 3 n 2 2 n 1 eq 19 is numerically integrated again through an explicit first order scheme in time and a second order finite volume scheme in space applying the euler muscl hancock method toro 2009 the discretized version reads 20 q i k 1 q i k δ t 1 δ x f i 1 2 k f i 1 2 k 0 where q i k is the flow rate averaged value in the i th volume at the time tk in eq 20 f i 1 2 k and f i 1 2 k represent the numerical approximation of the fluxes at the volume interfaces i 1 2 and i 1 2 respectively and they read toro 2009 21 f i 1 2 k f q i k 1 2 1 c f l i δ q i 1 2 δ x f i 1 2 k f q i 1 k 1 2 1 c f l i δ q i 1 2 δ x where δqi 1 2 resp δqi 1 2 is the slope of q x function at the i 1 2 resp i 1 2 interface and the cfli number is evaluated in the i th volume at the time tk as 22 cf l i 2 n 1 n q i k n 1 2 n 1 δ t δ x in order to preserve the scheme monotony the superbee limiter toro 2009 has been applied in calculating both slopes δqi 1 2 and δqi 1 2 3 3 the performed tests following moramarco et al 2008b in the present study several tests considering different flow conditions characterized by different synthetic hydrographs imposed at the channel inlet have been carried out each hydrograph is characterized by a wave duration t m t 0 with t 0 the dimensionless time to peak and m an integer larger than 1 the dimensionless time to peak t 0 is evaluated as the wave travel time of the reference flow rate and therefore accounting for the reference length scale it reads 23 t 0 1 c k w m p l f n 2 n 1 as indicated in eq 23 t 0 is only function of the fluid rheology the following four parameters pearson type iii distribution is assumed for the hydrograph shape moramarco et al 2008b 24 q 0 t q p t t 0 1 γ 1 e 1 t t 0 γ 1 where qp is the dimensionless peak discharge and γ is the shape dimensionless factor which assumes two different values for the rising γ γris for t t 0 and the recession γ γrec for t t 0 limbs respectively for all tests qp is assumed equal to 2 and γris equal to 1 3 according to moramarco et al 2008b then the γrec value is defined imposing as condition that the time averaged value of the discharge equals the dimensionless reference discharge q ref 1 expressed as 25 1 t 0 t q 0 t d t 1 in the performed tests it results that the γrec value ranges between 1 1 and 51 for m between 2 and 90 idealized input hydrographs are often used in numerical tests for flood routing e g perumal and sahoo 2007 perumal et al 2007 moramarco et al 2008b dottori et al 2009 fenton 2019 moreover zucco et al 2015 showed that the adopted hydrograph shape after calibration well lends itself to represent also the actual single peak floods occurring in natural channels considering the correlation between debris mud flow discharge and water flow discharge takahashi 1991 vandine 1985 chen et al 2008 the hydrograph shape adopted for the numerical tests may be considered also representative for debris mud flows for the kwm none additional boundary condition is required for the fdm whenever at the channel inlet the current is hypercritical i e λ assumes positive value an additional boundary condition has to be assigned imposing the validity of rating curve at each time the following flow depth value at the channel inlet is prescribed 26 h 0 t q 0 t n 2 n 1 following moramarco et al 2008b in hypocritical condition the critical flow depth is imposed as boundary condition at the channel outlet which for a power law fluid reads di cristo et al 2018c 27 h 1 t β q 2 1 t f 2 3 several tests have been carried out with the following values of the wave period t 2 t 0 3 t 0 10 t 0 20 t 0 30 t 0 60 t 0 90 t 0 the froude number have been fixed in the ranges and 0 5 0 8 and the dimensionless parameter kf 2 has been varied up to 20 as far as the rheology of the fluid is concerned the whole shear thinning range has been explored namely n 1 all simulations have been performed with fixed values of δx and δt namely δx 0 005 and δt 10 6 verifying that the courant condition has been always satisfied 4 results 4 1 unsteady analysis results the accuracy of the kwm in reproducing the results of the fwm is assessed using different dimensionless indicators moramarco et al 2008b firstly the dimensionless error on the maximum flow depth ε h max and the maximum discharge ε q max along the channel are evaluated as 28 ε h max x h max kwm x h max fwm x h max fwm x 100 29 ε q max x q max kwm x q max fwm x q max fwm x 100 in eq 28 respectively eq 29 h max fwm and h max kwm respectively q max fwm and q max kwm are the dimensionless maximum flow depth respectively discharge computed by the fwm and the kwm respectively secondly the mean values of ε h max and ε q max along the channel namely ε h max and ε q max are considered with the aim of excluding the regions where the boundary conditions may have a large influence ε h max and ε q max are computed limitedly for 0 05 x 0 95 moramarco et al 2008b finally a third index is considered namely the percent error of the peak discharge at the outlet defined as 30 ε qp q p kwm q p fwm 1 100 being q p fwm and q p kwm the peak discharge at the outlet computed by the fwm and the kwm respectively fig 1 a b represent the effect of the rheology on the maximum flow depth error ε h max along the channel considering for the sake of example kf 2 7 5 and f 0 5 two different fluids characterized by n 0 25 and n 1 are considered each curve refers to a single wave duration ranging between 2 t 0 and 90 t 0 the maximum flow depth error ε h max varies along the abscissa and it depends on both the rheological index and the wave duration for both n values and similarly to the turbulent clear water case moramarco et al 2008b all curves show a monotone behavior with respect to both the channel abscissa with an increase of ε h max in the downstream direction moreover the maximum flow depth error ε h max decreases with the wave duration with errors lower than 10 for t t 0 30 for both considered rheologies finally the rheology substantially affects the magnitude of the errors fig 1a b highest errors are observed for the smallest rheological index for instance for t t 0 2 at x 0 3 the error is 36 for n 0 25 while decreases to 7 for n 1 0 independently of the t t 0 value for n 1 0 the error is less than 5 for x 0 26 while for n 0 25 such a bound for the error is overwhelmed in the whole channel for t t 0 20 a more extensive analysis performed considering different combinations of the pair f kf 2 values results not shown confirms the qualitative results observed in fig 1 to better assess the effect of the rheology on the applicability conditions fig 2 a b report in the x kf 2 plane for f 0 5 and for four different rheological indexes n 0 25 0 50 0 75 1 0 the region where the kwm is applicable with an error less than 5 based on the ε h max indicator two different wave durations have been considered namely t t 0 2 fig 2a and t t 0 20 fig 2b for a fixed n value the region of the x kf 2 plane to the left of the curve corresponds to the conditions of local applicability of the approximated model fig 2c d are the counterparts of fig 2a b with f 0 8 for a fixed n value and independently of the froude number value and the wave duration all plots of fig 2 suggest that the applicability region enlarges with the kf 2 values similarly to the turbulent clear water case moramarco et al 2008b moreover for a given triplet kf 2 f t t 0 the performance of the approximated model deteriorates as the fluid rheology becomes more shear thinning i e when n reduces for instance considering kf 2 10 at f 0 5 and t t 0 2 fig 2a indicates that the kwm predicts with the prescribed tolerance the maximum flow depth in about the first 30 of the channel length for n 1 while this length becomes less than 10 in the n 0 25 case for a concise comparison table 1 reports the upper bounds of the channel abscissa xlim for the applicability of the kwm at kf 2 10 for the different rheological indexes the values reported in table 1 compared with the one pertaining to the turbulent clear water case x lim tcw 0 5 see fig 2 of moramarco et al 2008b allow to conclude that the laminar power law rheology strongly reduces the applicability conditions of the kwm the combined examination of fig 2a c corresponding to the same wave duration t t 0 2 but referring to different froude numbers suggests that the froude number value has a negligible influence on the performance of the kwm some small differences are observed only for small kf 2 values and for fluids with a small rheological index the same comparison for the higher wave duration t t 0 20 fig 2b and d shows differences with a larger applicability region for the lower froude number f 0 5 with increasing differences for lower rheological index values for instance in the case f 0 5 for n 0 25 and kf 2 10 the kwm is applicable in the first 85 of the channel while for f 0 8 it reduces to the 65 the enlargement of the applicability region with kf2 for a fixed value of f may be easily explained by inspecting the dimensionless momentum conservation equation eq 7 for large values of kf 2 for a fixed value of the froude number the r h s of eq 7 becomes the leading term and therefore the fwm tends to the kinematic one the effect of the wave duration on the applicability of kwm may be again theoretically explained based on the dimensionless momentum equation indeed an increase of the wave duration reduces the l h s of eq 7 reducing the local inertia term previous results demonstrate that the kwm accurately approximates fdm over the entire channel length i e ε h max 0 05 for 0 x 1 only for sufficiently large values of t t 0 a less restrictive criterion may be derived considering the mean error of the maximum flow depth ε h max for f 0 5 fig 3 reports ε h max as function of the wave duration for the same rheological indexes of fig 2 and two different kf 2 values namely kf 2 2 5 fig 3a and kf 2 20 fig 3b coherently with figs 1 and 2 in all cases the mean error ε h max decreases with t t 0 and it is affected by kf 2 similarly to tcw moramarco et al 2008b ε h max decreases when kf 2 increases the comparison among the different curves confirms the strong dependence of the error on the rheology with a reduction of ε h max for increasing n for instance for kf2 2 5 fig 3a the mean error is less than 5 for t t 0 45 and t t 0 16 for n 0 25 and n 1 0 respectively in this way it is possible to individuate the minimum value of t t 0 above which the kwm can be applied with the prescribed accuracy for f 0 5 fig 4 depicts ε q max as function of the wave duration for kf 2 2 5 fig 5 a and kf 2 20 fig 5b for different rheological indexes similarly to ε h max the mean error on the maximum discharge has a monotone reduction with t t 0 and it decreases with kf 2 comparing figs 3 and 4 it is evident that the mean error on the maximum discharge is higher than ε h max while the dependency from the rheology is similar for instance for kf 2 2 5 fig 4a ε q max is always larger than 5 for n 0 25 while for n 1 0 it is less than 5 for t t 0 30 present results suggest that the minimum value of t t 0 individuated for ε h max does not provide the same accuracy in terms of mean error on the maximum flow discharge fig 5 is the counterpart of figs 3 and 4 in terms of error of the peak discharge at the outlet ε qp has the same behavior of the other two considered indicators with respect to t t 0 and kf 2 but it has larger values than both ε h max and ε q max in fact for kf2 2 5 fig 5a ε qp for the cases n 0 25 and n 0 5 is always larger than 5 for all the considered parameters the results obtained with f 0 8 not reported herein are very similar with some differences only for the smallest rheological index value and for wave durations t t 0 20 these observations confirm a larger influence of kf2 and a minor effect of the froude value on the kwm applicability condition in conclusion the range of conditions in which the kwm has a mean error of the maximum flow depth less than 5 is wider than the one necessary for obtaining the same accuracy in terms of maximum flow discharge and peak discharge at the outlet the rheological characterization is crucial because for small n values in many cases the kwm is not able to reproduce the fdm solutions with an accuracy above 95 finally the results shown in figs 2 5 allow to define the applicability conditions of the kwm and they can be used as a guideline for practical applications 4 2 comparison with the wave period criterion in what follows the results of the performed numerical analysis are employed to assess the effectiveness of the theoretical criterion proposed by di cristo et al 2014 which generalizes to power law fluids the wave period criterion proposed by ponce et al 1978 for clear water flood routing similarly to ponce et al 1978 the criterion of di cristo et al 2014 provides a lower bound of the dimensionless wave period θ above which the kwm approximates the fdm within a prescribed accuracy it is worth of nothing that the wave period criteria are applicable only to linearly stable flow conditions therefore as far as power law fluids are concerned the criterion of di cristo et al 2014 holds only for f f f being the limiting linear stability froude number ng and mei 1994 31 f n 2 n 1 in order to verify the wave period criteria for clear water flood routing moramarco et al 2008a related the dimensionless wave period θ to the wave duration t t 0 through the following relation 32 θ k f 2 c kwm t t 0 with ckwm the dimensionless celerity of kinematic wave model therefore assuming an error less then 5 eq 32 led moramarco et al 2008a to deduce the following lower bound of wave duration 33 t t 0 5 c kwm k f 2 θ 5 in which the dimensionless celerity and the dimensionless wave period were set equal to c kwm tcw 3 2 and θ 5 tcw 171 respectively ponce et al 1978 as far as the power law fluids are concerned eq 33 may be still applied provided that the dimensionless celerity ckwm is evaluated through eq 13 i e c kwm c kwm plf and the dimensionless wave period threshold refers to the value deduced for power law fluids θ 5 plf which depends on n as shown in fig 3b of di cristo et al 2014 the θ 5 plf values corresponding to the considered n values deduced from this figure are reported in table 2 the t t 0 5 threshold given by eq 33 is depicted in fig 6 as function of kf 2 for the different n values assuming the values of θ 5 plf reported in table 2 therefore accordingly to the wave period criterion the kwm can be applied with an accuracy equal or larger than 95 in any of condition characterized by a kf 2 t t 0 pair laying above the theoretical curve based on the results of the non linear numerical simulations for any of the conditions listed in table 2 the value of t t 0 above which the error of the three considered parameters is smaller than 5 is evaluated for several values of kf 2 in fig 6 triangles circles correspond to the mean error of the maximum depth respectively discharge ε h max ε q max whereas squares represent the 5 error threshold value for the peak discharge ε qp void and filled symbols refer to the cases f f 0 5 and f f 0 8 respectively the ensemble of the results of the non linear simulations reveals that the minimum t t 0 value for kwm applicability decreases with both kf 2 and n moreover a higher t t 0 threshold is almost always required to assure the prescribed accuracy based on the ε qp parameter compared with the other two error metrics for a fixed rheology no substantial differences are observed between the two investigated froude numbers fig 6 shows also that independently of the froude number value for n 0 25 and kf 2 20 the lines representing the di cristo et al 2014 s criterion are above all points this means that it may be safely applied for predict all the considered quantities since it is more restrictive than the one resulting from the non linear simulations such a conclusion holds even in the kf2 20 case and for both the averaged maximum flow depth and the flow rate but not for the peak discharge the above results apply even for n greater than 0 5 with exception of the kf 2 2 5 f f 0 5 case for which ε h max is always higher than the prescribed accuracy results not shown the performances of the wave period criterion deteriorate for the lowest value of n in fact independently on the froude number in the kf 2 2 5 case the mean error of the maximum depth is always higher than the prescribed accuracy results not shown same conclusion holds even in for kf 2 20 as far as the maximum discharge and the peak discharge quantities are concerned 4 3 discussion and final remarks the interest in using the kwm model for reproducing mud flow propagation implies the crucial question about its applicability from a practical point of view it is important to understand in which conditions its results represent a good approximation of the full model based on the presented study the users interested in applying kwm to engineering problems involving mud flows are warned that its applicability range reduces as the rheological index decreases and it is significantly influenced by the kf2 values moreover as expected the findings of the present analysis confirm that the kwm reproduces more reliable results for high values of the wave duration the results indicate also that the applicability conditions depend on the flow variable of interest the kwm reproduces with a better accuracy the maximum flow depth respect to the maximum flow discharge and the peak discharge at the outlet in other words complying with the applicability range relative to peak discharge guaranties the required accuracy also on both the maximum flow discharge and depth the study furnishes useful indications about the applicability condition of the kwm in terms of lower bound of dimensionless wave duration in order to have a mean error less than 5 on the reproduction of the considered flow variables this parameter is equivalent to the dimensionless wave period adopted in the criteria used for clear water i e ponce et al 1978 and power law fluids i e di cristo et al 2014 the proposed criterion may be adopted to use kwm for simulating mud flow propagation in a channel after studying the fluid rheology and considering the hydrodynamic characteristics i e f and k values based on it according with the variable of interest it is possible to define the minimum wave duration for which it may be adopted a further comparison demonstrated that for power law index values larger than 0 25 the wave period criterion obtained through a previous simpler linear analysis is more restrictive than the one deduced from the presented non linear simulations then it may be safely applied for all the considered variables the presented analysis has been carried out considering two froude number values namely f 0 5 and f 0 8 with reference to the jiang jia ravine mud n 0 3 ρ 2130 kg m 3 µ 150pasn see ng and mei 1994 and assuming a flow rate for unit width equal to 10 m2 s the corresponding bottom slope is 0 3 and 0 5 for f 0 5 and f 0 8 respectively 5 conclusions the present study investigates the applicability of the simplified kinematic wave model kwm in predicting the unsteady propagation of a mud flow wave accounting for the non linearity of the governing equations the fluid characterized by a highly concentrated mixture of water and fine sediments is represented through the power law model proposed by ng and mei 1994 the analysis is performed through a numerical analysis of both the full and kinematic wave models adopting an explicit first order scheme in time and a second order finite volume method for the spatial discretization the applicability conditions are expressed considering three dimensionless indicators related to the error on the maximum flow depth the maximum discharge and the peak discharge at the downstream end of the channel the results indicate that the error on the maximum flow depth ε h max increases moving in the downstream direction and that higher errors pertain to lower wave durations the rheology substantially affects the magnitude of the errors which have been found to increase with the shear thinning behavior of the fluid i e to decrease with the rheological exponent n the analysis defines also the range of applicability of the kwm in terms lower bound of the wave duration t t 0 above which the errors are within the 5 the limiting value of t t 0 depends on the rheological index and the dimensionless parameters f and kf2 f and k being the froude and the kinematic wave numbers respectively the results indicate an increase of the t t 0 lower bound a reduction of the applicability range as n decreases and a larger influence on it of kf 2 respect to f the range of conditions in which the kwm has a mean error of the maximum flow depth less than 5 is wider than the one necessary for obtaining the same accuracy in terms of maximum flow discharge and peak discharge at the outlet finally the obtained criteria are compared with the wave period criterion theoretically deduced by di cristo et al 2014 through a linear analysis at least for moderate values of the dimensionless number kf 2 and for power law index values larger than 0 25 the wave period criterion has been found to be more restrictive than the one resulting from the non linear analysis which may be therefore safely adopted to assess the applicability of the kinematic wave model to mud routing declaration of competing interest none acknowledgement the work described in the present paper was realized in the framework of the project misalva financed by the italian minister of the environment land protection and sea cup h36c18000970005 
6253,fecal contamination poses a threat to groundwater supplies in low income regions this is often due to the coexistence of pit latrines with domestic wells in densely populated areas in this context developing alternative methodologies to map fecal pollution in shallow wells is needed a thorough survey of over 240 domestic wells and 570 pit latrines was conducted in a rural town of southern mali water samples were collected from all wells and tested for temperature ph electric conductivity total dissolved solids turbidity and thermotolerant coliforms the outcomes of the field survey were incorporated into a gis database thirteen machine leaning classifiers including different statistical algorithms instance based learners and tree based models were used to determine the spatial distribution of fecal pollution as per five explanatory variables latrine density distance to the closest latrine borehole yield water table depth and population density the best performing classifiers selected on test scores were then used to develop predictive maps random forest and logistic regression rendered prediction scores for fecal pollution in excess of 0 90 multilayer perceptrons support vector machines and quadratic discriminant analyses also proved adept at forecasting fecal pollution ensemble mapping shows that 30 50 m buffers around domestic wells may be sufficient to prevent contamination of domestic supplies in most instances this demonstrates that machine learning may provide a versatile methodological alternative to traditional darcian approaches on the other hand the practical difficulties involved in maintaining wellhead protection areas suggests the need to implement piped water supplies keywords big data contamination domestic well latrine machine learning supervised classification 1 introduction having access to proper water and sanitation facilities is crucial for the realization of all human rights as well as for leading a life in human dignity united nations 2002 2010 in 2015 about 2 3 billion people lacked access to basic sanitation facilities such as toilets or latrines unicef who 2017 besides 663 million remained without access to improved water sources unicef 2015 a figure which is most likely an underestimate martínez santos 2017a in a world where 50 of the population relies on groundwater for drinking unesco 2015 an increasing number of authors point out that there is an impending need to reconsider the tradeoffs between water and sanitation access graham and polizzotto 2013 shivendra and ramaraju 2015 sorensen et al 2016 back et al 2018 this is particularly relevant in low income regions where the gradual adoption of household scale sanitation systems such as pit latrines results in fecal pollution of shallow domestic wells wright et al 2013 odagiri et al 2016 martínez santos et al 2017 debela et al 2018 developing predictive approaches to evaluate contamination in drinking supplies is of paramount importance for these can contribute to underpin wellhead protection mechanisms the academic literature showcases a series of traditional methods for delineating buffer zones in groundwater bodies most of these aim at protecting groundwater quality in drinking supply wells and have been incorporated to legislation in many countries igme 2003 strobl and robillard 2005 parker and carlier 2009 for instance volumetric numerical analytical and tracer based approaches have proven applicable in intergranular media evers and lerner 1998 levy and ludy 2000 martínez navarrete 2002 among these the classic analytical methods by bear and jacob 1965 and wyssling 1979 have become particularly widespread in contrast only tracer based and numerical techniques are suited to fractured media and karst systems largely due to the prevalence of secondary porosity and conduit flow a recurrent problem with the application of these approaches is the absence of important data including the spatial distribution of aquifer permeability and storage coefficients groundwater heads groundwater velocities and the thickness of the unsaturated zone to name a few key variables not the least of problems is determining the distribution of cracks and fractures in fissured media or that of conduits in karst environments in the case of the more sophisticated tools such as numerical models this is further aggravated by the difficulties involved in obtaining enough information to calibrate the outcomes all this hampers attempts to protect groundwater supplies in geographical contexts where contamination represents a threat to public health for simplicity wellhead protection buffers in humanitarian standards and low income regions typically rely on rules of thumb harvey 2007 parker and carlier 2009 because these are derived directly from darcian principles some of their underlying assumptions have been challenged taylor et al 2004 for instance demonstrate that assuming that fecal contamination travels at the average linear velocity of groundwater may be far from appropriate however these rules stem from practical experience and are useful in contexts where both data and technical means may be scarce macdonald et al 2005 sphere association 2018 machine learning is a subset of artificial learning techniques which is still novel in the field of groundwater resources while the literature showcases some examples in the domain of groundwater potential mapping duan et al 2016 naghibi et al 2017 chen et al 2018 comparatively little work has been carried out in relation to domestic supplies machiwal et al 2018 studies dealing with contamination are even more limited recent and almost invariably restricted to regional scale studies thus arabgol et al 2016 used support vector machines to predict the spatial distribution of nitrate contamination in groundwater in the arak plain iran similarly sajedi hosseini et al 2018 combined boosted regression trees multivariate discriminant analysis and support vector machines to map nitrate pollution in aquifer bodies whereas barzegar et al 2018 compared the performance of extreme learning machine models with multilayer perceptrons and support vector machines in the context on fluoride contamination artificial intelligence approaches have also been coupled with traditional hydrogeological frameworks to enhance the outcomes of studies dealing with groundwater susceptibility to contamination baghapour et al 2016 nadiri et al 2018 the goal of this paper is to present and validate an artificial intelligence approach to map fecal pollution in domestic groundwater supplies this is perceived as a methodological novelty in several ways in the first place in this field there are very few precedents of local scale predictive mapping based on a high density of data points over 68 wells and 160 pit latrines per square kilometer and a thorough survey of several hundred households furthermore this technique uses a large number of machine learning classifiers 13 out of which an ensemble of the best performers is used to develop the final product this may be of use in cases where standard hydrogeological variables are difficult to take into account thus it represents an alternative to distance and isochrone based wellhead protection approaches and may provide additional insight in efforts to protect groundwater supplies 2 materials and methods 2 1 study area this study focuses on the town of beleko beleko is the capital of the rural commune of djedougou southern mali fig 1 this town comprises seven neighborhoods and is home to over 6000 people the region presents a hot semi arid climate with an average yearly temperature of 26 c rainfall is subject to the west african monsoon and amounts to 800 mm annually from the hydrogeological standpoint the study area is located within the infracambrian metasedimentary domain of southern mali traore et al 2019 from top to bottom the lithostratigraphic profile consists in a hard laterite crust whose thickness may exceed five meters this is underlain by an unconsolidated layer made up of clays with intercalations of fine sand typically 10 15 m thick the regional sandstone aquifer is immediately beneath its thickness exceeds one hundred meters over the impervious gneiss basement garcía castro and garcía rincón 2017 the water table depth oscillates between 5 and 15 m across most of the study area at the end of the dry season and between 1 and 3 m towards the end of the rainy months the geographies of water access and sanitation in the beleko area have been thoroughly described by martínez santos 2017b in the absence of permanent surface water courses the population relies exclusively on groundwater groundwater is obtained either from communal supplies standpipes or deep boreholes equipped with hand pumps or shallow domestic wells the latter are typically unlined and less than 15 m deep there is no public sanitation system instead informal sanitation facilities i e pit latrines are present in most households pit latrines are built much like wells but are shallower typically 2 6 m and wider 1x1 to 2x2 meters the vast majority are unlined and located in close proximity to domestic supplies thus posing a direct threat to shallow groundwater and human health this is particularly true of the rainy season when the water table rises close to the ground level approximately 80 of the households own a domestic well whereas close to 85 own a pit latrine martínez santos et al 2017 2 2 data collection and sampling data was collected during a three week field campaign in april and may 2018 over 240 randomly selected wells were geo referenced and examined directly well type and construction features were recorded in all cases as per the systematic described in table 1 additionally all wells were also surveyed in situ for groundwater depth temperature electric conductivity and total dissolved solids by means of a hannah hi 98121 multiparametric device turbidity was measured with a field turbidimeter fecal pollution surveys rely either on studying the concentration of different pathogens or on that of a single indicator microorganism in the latter case e coli is typically preferred because it occurs in high numbers in human and animal feces and water subject to recent fecal pollution in most circumstances populations of thermotolerant coliforms ttc are composed predominantly of e coli which is the reason why ttc are still regarded as a less reliable though acceptable indicator of fecal pollution who 2011 2017 since the goal of this research is to demonstrate the predictive mapping methodology rather than to depict the distribution of different pathogens in groundwater exhaustively the use of ttc was considered sufficiently representative for practical purposes thus a water sample was collected from each well and analyzed for ttc with an oxfam delagua portable laboratory oxfam 2009 standard transport procedures and incubation times were used colony forming units cfu per 100 ml of water were counted by hand within ten minutes of retrieval from the incubator concentrations in excess of 50 cfu 100 ml were considered too numerous to count latrines were geo referenced on a household by household basis table 2 about 570 latrines were inventoried in an area of approximately 3 5 km2 that is over 160 per square kilometer other household related variables such as the number of people or the water treatment habits at the household scale were recorded latrine and well data together with the spatial distribution of each of the explanatory variables were incorporated into a qgis 3 2 database 2 3 machine learning protocol machine learning aims at developing computer algorithms that learn automatically from experience much like humans do mitchell 1997 hastie et al 2009 the underlying assumption is that computers are powerful enough to establish complex associations among those independent variables of a given dataset that combine to explain a known outcome once this is achieved the algorithm can predict any outcome based on any blend of independent variables in many ways machine learning is closely linked to the concept of statistical inference except that machine learning algorithms are typically more complex and do not always rely on statistical principles table 3 any machine learning exercise requires a dataset and an algorithm or learner the dataset consists of two elements namely class and features the class is the dependent variable i e the outcome that the algorithm will attempt to predict while the term features refers to the ensemble of all explanatory variables that may describe such an outcome the machine learning process typically takes place in three steps data is split into a training and a test dataset first then the algorithm is trained on the former during the training stage the computer is allowed to use both the class and the features attempting to derive meaningful associations between both in the third step testing the reliability of the algorithm is checked by evaluating how accurate it is at predicting the class of the test dataset when only allowed to use the features as explained later an additional parameter fitting step can be added between training and testing in order to optimize algorithm performance this stage is generally called validation in artificial intelligence protocols 2 3 1 conceptual model in this case the aim of the machine learning model is to predict fecal contamination in groundwater due to the ease with which thermotolerant coliforms ttc can be measured this indicator was selected as the class variable ttcs are typically measured by quantifying the number of colony forming units cfu per 100 ml of water international standards establish 0 cfu 100 ml as the threshold for drinking purposes who 2011 but a concentration of fewer than 10 cfu 100 ml can be acceptable who 2002 this is particularly true of instances where due to exposure the population may have developed a certain level of tolerance data requirements become greater as models get more complex whenever large amounts of information are available it is common practice to include as many features as possible no matter which ones are expected to be relevant a priori this is because machine learning algorithms are specifically designed to deal with big data in studies such as this one however where the dataset is small by machine learning standards n 271 it is best to keep the conceptual model as simple as possible this can be achieved by minimizing the complexity of the class and the number of features thus a binary class was defined wells presenting a concentration in excess of 10 cfu 100 ml were considered polluted whereas those with lesser concentrations were labelled suitable for drinking in remote settings such as the one at hand where logistics are difficult the choice of explanatory variables is sometimes constrained by whatever can be readily obtained rather than by explicit hydrogeological considerations while detailed soil profiles indicating the permeability of each layer or transmissivity and velocity fields would provide extremely valuable information these are typically difficult to obtain in practice the idea is therefore to develop a sufficiently robust method to predict contamination based on proxy variables thus five key features explanatory variables were taken into account these include latrine density distance to closest latrine water table depth population density and borehole yield fig 2 it is self evident that the presence of a latrine can explain fecal contamination in a nearby well moreover all other factors being equal the closer the latrine the greater the threat this suggests that distance to the closest latrine would be an adequate feature to consider in any predictive attempt nevertheless this by itself would provide an incomplete picture as other factors may come into play for instance a large number of latrines within a certain radius poses a greater pollution threat than a single latrine which implies that latrine density is also important so is household population since the number of people using a given latrine will ultimately control the microbial load released into the aquifer furthermore direct observation shows that the larger households in the study area present other potential contamination sources including free roaming livestock and manure piles flow direction can be expected to govern coliform transport flow direction is obtained from the elevation of the water table and more specifically from the hydraulic gradient in this case local scale gradients can be expected to change relatively easily over time due to the large number of groundwater extraction points furthermore the only available source of elevation data was a digital elevation model which means that the outcomes are not fine enough to determine groundwater flow direction accurately in the absence of more detailed information water table depth was taken into consideration only as a protection mechanism as the thickness of the unsaturated zone delays contaminant influx in turn the effects of advective transport were assumed to be partially implicit within latrine distance and density finally borehole yield data was used as an indirect proxy for the aquifer s ability to store and transmit water 2 3 2 software mlmapper v 1 0 was used to map contamination mlmapper is a qgis 3 plugin that builds predictive maps based on point source data mlmapper implements thirteen machine learning classifiers from the scikit learn 0 19 2 toolbox pedregosa et al 2011 the theory behind which has been widely discussed in the literature kotsiantis 2007 hastie et al 2009 pedregosa et al 2011 mlmapper algorithms include support vector machines svm logistic regression lrg decision tree classifier crt random forest classifier rfc k neighbour classification knn linear discriminant analysis lda gaussian naïve bayes classification nba multilayer perceptron neural network mlp ada boost classifier abc quadratic discriminant analysis qda gradient boosting classification gbc and gaussian process classifier gpc a brief description of all these algorithms is provided in table 2 mlmapper requires two inputs fig 3 the first one is used to train and test the model this consists in a point vector shapefile that includes all class and feature values for each well the class is expressed as a boolean variable where 0 means uncontaminated and 1 means contaminated the second input is used to develop a fecal contamination map based on the outcome of the training and testing processes this is also a point vector shapefile which includes the feature values for every pixel in the spatial database outcomes are of two types on the one hand mlmapper provides a series of standard performance metrics including training and test scores precision recall and f 1 outputs confusion matrices and area under the curve obtained from receiver operator characteristic curves provide complementary means to analyze the numerical results these are all non dimensional and will be discussed later on on the other hand each run of mlmapper renders one single predictive map this is computed as the ensemble of the best performing classifiers based on a metric of choice test score in this case a specific map may also be developed from each classifier as per its own individual scores 2 3 3 handling overfitting when data associations are unclear machine learning classifiers tend to over fit the training dataset overfitting is a process whereby the computer memorizes each instance of the training dataset in order to maximize the training score instead of developing associations that can be successfully generalized thus it typically results in a good training score coupled with a low test score overfitting may also be the result of an imbalanced dataset i e too few instances of one of the class outcomes in such cases certain algorithms focus on explaining the more frequent class outcomes while ignoring the others techniques to avoid overfitting include enhancing the dataset by adding new data or by redistributing the existing class the effectiveness of these two approaches can be furthered by automated parameter fitting note that parameter fitting is typically known as validation in the machine learning terminology parameter fitting involves an additional subdivision of the input data so that it ultimately comprises training validation and test datasets the validation set is used between training and testing and its purpose is to fine tune the internal parameters of the machine learning algorithm prior to checking its generalization potential on the test set thus the algorithm undergoes two training stages instead of just one because the input dataset in this case is skewed towards the positive outcome polluted well an attempt was made to balance the classes by a considering a contamination threshold of 10 cfu 100 ml which implies that samples with a tolerable degree of ttc content were considered uncontaminated and b adding a small number of uncontaminated wells training wells in fig 1 in areas of the map where no fecal pollution due to latrines would be expected i e outside the town these two approaches were further enhanced by carrying out automated grid search optimization this is an in built mlmapper routine that aims at maximizing test score through automated parameter fitting for this purpose train test validation splits of 50 50 60 40 70 30 and 80 20 were established throughout the text splits are simplified for the sake of readability however a 70 30 split means that 70 of the original dataset was used for training while the remaining 30 was split again using a 70 30 proportion that is 9 for validation and 21 for testing these splits are commonly used in machine learning studies across different disciplines dobbin and simon 2011 naghibi et al 2017 sakr et al 2018 2 3 4 ensemble approach mlmapper allows for performing an ensemble of the best performing classifiers in this case ensembling consists in computing the arithmetic mean of the most reliable classifiers which are selected based on test score the ensemble presents three major advantages over individual classification in the first place it reduces noise by disregarding those classifiers that have failed to predict the presence or absence of contamination reliably secondly the ensemble can be used to depict uncertainty from a spatial perspective for the purpose of analysis uncertainty is interpreted as the degree of discrepancy among classifiers finally ensembling provides a graded outcome which is both more informative than the binary outcomes that result from individual classification 3 results 3 1 field survey table 4 presents some descriptive statistics of water quality in domestic wells in situ characterization shows groundwater to be slightly acidic with a low electric conductivity turbidity varies widely as expected due to the nature of the sources in general terms however the key finding of this field survey is that fecal pollution represents a widespread threat to human health almost 80 of the wells presented a ttc concentration in excess of 10 cfu 100 ml this figure is particularly relevant in view that close to 58 of the households used their wells for drinking an estimated 50 of the population drinks contaminated water on a regular basis besides the survey was carried out at the end of the dry season where the incidence of fecal contamination can be expected to be lowest thus the outcomes suggest that fecal contamination in drinking supplies poses a permanent threat well protection is a major issue two thirds of the surveyed wells were thoroughly unprotected the majority of these 80 presented a significant coliform presence approximately 68 of protected wells were polluted causes should be found in the high density of latrines 77 of the wells are located within 30 m of the nearest latrine as well as in the nearby presence of contamination sources such as free roaming livestock garbage and manure piles contamination sources other than latrines were observed within a radius of 30 m in about 85 of the households deficient well construction and operation also seem to explain contamination to a certain extent many of the protected wells were only protected at the surface i e equipped with a concrete apron lid fence or parapet but not in depth besides a relatively large number 25 were found to be uncovered at the time of the visit in approximately the same proportion of instances buckets were observed to be left lying around rather than hung 3 2 optimized classifier scores the accuracy of a machine learning classifier is typically evaluated in terms of training and test scores the training score represents how well a classifier performs on the training set in other words it provides an idea as to how the classifier fares in terms of developing associations between the independent and the dependent variables in contrast test score is defined as the number of correct guesses divided by the total number of attempts on the test dataset since the aim of this research is to predict the occurrence of groundwater contamination test score was considered the chief evaluation criteria table 5 shows a series of relevant metrics to evaluate the performance of the thirteen classifiers several algorithms were observed to underperform prior to parameter fitting mlp 0 69 nba 0 75 qda 0 77 moreover nearly half of them crt rfc abc gbc gpc overfitted the data at first training scores being up to 0 15 0 20 higher than validation scores the fact that automated parameter fitting was successful at increasing accuracy above 0 85 in all cases while simultaneously reducing overfitting attests to the importance of optimization procedures remarkably automated parameter optimization caused the test score of mlp to rise from lowest 0 69 to the top five after fine tuning 0 90 on a different note it is theoretically possible for the test score to exceed the training score this is because testing is performed on an entirely different dataset where the classifier may in fact attain a higher degree of success than in the training set in the case at hand this occurs with svm optimized training score 0 83 vs test score 0 90 and qda 0 89 vs 0 91 precision and recall are complementary metrics precision represents the number of times a certain class outcome was predicted correctly divided by the total number of times the class was predicted in practice precision is computed as the number of true positives over the sum of true and false positives for the case of the contaminated class this represents how many times contamination was predicted correctly out of all the times the algorithm predicted the outcome contaminated well since algorithms present a high prec 1 score it can be concluded that these were unlikely to return false positives uncontaminated outcome in wells that were actually contaminated in contrast the prec 0 score is a little lower on average largely due to the tendency of crt prec 0 0 69 gbc 0 69 abc 0 71 and lvc 0 71 to predict contamination in uncontaminated wells two algorithms rendered balanced outcomes namely svm prec 0 0 89 vs prec 1 0 90 and qda 0 89 vs 0 91 recall represents the number of times a class was predicted correctly over the actual number of data points of that class in other words recall is the ability of a model to detect all the relevant instances of a given class within the input dataset recall is computed as the number of true positives over the sum of true positives plus false negatives all algorithms except for crt rec 1 0 89 and gbc 0 90 presented recall values for the contaminated class well in excess of 0 90 this means that the algorithms were generally able to detect the majority of contaminated wells however the weakest performance metric for all classifiers was the recall for the uncontaminated outcome all algorithms except perhaps for crt and rfc rec 0 0 80 failed to label correctly a number of uncontaminated wells there is a tradeoff between precision and recall in practice the attempt to maximize one leads to diminish the other the f 1 score is the harmonic mean of both metrics and can be used to maximize the balance in this case rfc and lrg present the highest f 1 scores whereas nba gpc and gbc present the lowest again most algorithms performed noticeably worse in the case of the uncontaminated class in the case of all three metrics precision recall f 1 this is attributed to the comparatively smaller weight of the uncontaminated outcome in the input dataset an additional motive could be the inability of mlmapper to consider non spatially distributed variables such as well type protected unprotected receiver operator characteristic curves roc curves are commonly used to visualize the performance of binary classifiers fig 4 roc curves plot false positives horizontal axis versus true positives vertical axis the curve thus represents the diagnostic ability of a binary classifier system and how this changes with different discrimination thresholds the cutoff values imposed on the predicted probabilities for assigning observations to each class the area under the curve auc represents how well the classifier performed an auc of 1 means that the model is perfectly able to distinguish between positive and a negative outcome whereas an auc of 0 5 suggests a completely arbitrary result thus the closer the curve is to the top left corner of the chart the better the predictive ability of the model conversely the closer it is to the main diagonal of the chart the closer it is to predict at random in this case all classifiers except for abc and gpc rendered aucs of at least 0 86 which points at a reasonable solid performance across the board 3 3 sensitivity analyses and classifier selection two sensitivity analyses were conducted both based on test score the first one pertains to the dataset split fig 5 on average all splits rendered optimized test scores of at least 0 85 the highest arithmetic mean of all classifiers was obtained for a 60 40 split 0 91 whereas the lowest corresponded to the 80 20 one 0 85 furthermore the 60 40 split rendered the highest maximum and minimum values the best five classifiers in the 60 40 split lrg rfc qda mlp and svm yielded test scores between 0 90 and 0 91 with a mean of 0 906 and a standard deviation of 0 005 table 6 this suggests that these provide a reasonably robust and consistent prediction of fecal pollution in groundwater the second sensitivity analysis deals with feature importance different machine learning classifiers compute feature weights in various ways besides each family of algorithms provides its own set of metrics for feature importance it is thus necessary to find some common ground for comparison when dealing with several classifiers at the same time a useful approach is to compute the mean accuracy decrease due to each feature this consists in carrying out a series of runs of each model one feature is removed at a time to check how this affects predictive ability the underlying assumption is that removing important variables will reduce performance significantly whereas taking away the unimportant ones should barely change the test score removing unimportant variables can also be expected to decrease noise which may in fact reduce overfitting and improve the predictive ability of certain algorithms the best performing algorithms were run five times in each of these runs one of the features was removed the mean accuracy of the ensemble was observed to drop in all cases which suggests that all five features are relevant predictors features were then ranked based on the resulting test score although performance was similar in all cases water table depth proved to be the most important variable on average followed by distance to the nearest latrine latrine density population density and borehole yield fig 6 because removing each of these last three led to a relatively small drop in test score three additional runs were carried out to determine whether deleting any combination would yield better outcomes information and predictive ability losses were detected in all three cases therefore all five features were ultimately kept 3 4 contamination maps fig 7 presents the spatial generalization of machine learning classifiers into predictive maps fig 7a through 7e show the outcomes of the five best performing classifiers lrg rfc qda mlp and svm which were selected based on optimized test scores green colored pixels depict areas where the algorithm predicts low contamination whereas red pixels represent high contamination in general terms all maps render an intuitively correct picture the heavily populated areas of the northeast northwest and southeast all present high contamination while lower values are predicted for the central part and the outskirts where latrines do not exist in general terms the mlp algorithm presents the highest tendency to predict contamination 34 of the surface area is taken up by red pixels followed by the qda 31 and svm models 30 in contrast rfc 25 and lrg 27 are comparatively avert to labelling pixels as potentially contaminated svm rfc and lrg and to a lesser extent qda present a strong resemblance to each other the importance of the distance to the nearest latrine feature can be readily observed in all four cases as contaminated areas tend to form circular buffers around latrines in this regard one of the key differences is that qda and mlp tend to fill up the voids between circular buffers with high contamination predictions whereas the others do not each classifier relies on different principles thus performing an ensemble not only contributes to offset the noise derived from classifier specific nuances but also provides a measure of uncertainty i e the degree of discrepancy among classifiers in this context fig 7f represents the arithmetic mean of the best five classifiers for each pixel a green outcome implies that all classifiers agreed to label that pixel as uncontaminated whereas a red one means that all agreed on a contaminated outcome yellow pixels are those where the disagreement among classifiers is stronger the outcome of the ensemble map reinforces the idea that all explanatory variables were relevant to some extent and also suggests that the five best classifiers are largely consistent in their predictions 4 discussion 4 1 machine learning methods except for some of the simpler algorithms such as decision trees machine learning classification is largely a black box bibal and frenay 2016 samek et al 2017 the complexity and nonlinearity of these approaches coupled with that of the interrelation between classes and features typically makes it impossible to foretell which model will perform better with a given dataset furthermore the interpretation of most machine learning algorithms is as much of an art as it is of a science as it often relies on user experience and intuition over more tangible factors honegger 2018 there are however certain aspects that may contribute to explain why a given classifier outperforms another for instance random forests rfc are by design an enhanced version of decision trees crt in fact a random forest consists in the development a large number of decision trees followed by carrying out an ensemble of the outcomes while this makes the decision mechanism nearly impossible to understand it also reduces bias and noise which in turn contributes to decrease overfitting this could provide a plausible explanation as to why the optimized rfc ranked high among the top five classifiers while the optimized crt ended up within the bottom five on a more specific note simple decision trees such as crt are notably poor performers in the case of unbalanced classes and limited sample sizes both of which occur in this example lrg rfc qda mlp and svm rendered the more accurate results test scores ranging between 0 90 and 0 91 this implies that these algorithms predicted contamination acceptably as explained earlier rfc presents the most balanced set of metrics including the highest test score positive precision negative recall and f 1 outcomes table 5 this attests to the versatility of random forest approaches in dealing with small potentially noisy datasets it also means that the rfc algorithm is easy to train and suitable for handling spatially distributed data breiman 2001 cracknell and reading 2014 rfc proved computationally efficient despite its reputation for being a slow classifier which could be explained by the small dataset size low initial training and validation scores 1 00 vs 0 80 coupled with the optimized training and test score 0 97 vs 0 91 suggest the need to implement routines to prevent overfitting in this regard it is observed that several classifiers svm lrg knn lvc proved far more robust than rfc in the initial non optimized run the implementation of lrg is similar to that of multiple linear regression except that its outcomes are typically binary sperandei 2013 lrg is computationally effective and can handle nonlinear relationships between class and features because it applies a non linear log transformation of the linear regression approach lrg typically performs best with large dataset and a limited number of features but is vulnerable to interdependency and collinearity among features park 2013 the solid performance of lrg in this case comparable to that of rfc could be possibly related to fact that the conceptual model included just five variables that the relevance of these for groundwater contamination is conceptually justified in all cases and that there was little correlation among them table 7 from a case specific standpoint the results obtained by lrg provide additional insight on those obtained by martínez santos et al 2017 who applied multiple linear regression to predict contamination in domestic wells of three out of beleko s seven neighborhoods in particular lrg was able to develop stronger associations among explanatory variables and the target thus proving to be a far more reliable predictor for fecal contamination despite comparably high test scores qda mlp and svm are considered second to lrg and rfc due to lower negative outcome recall values 0 68 0 68 and 0 64 vs 0 72 and 0 80 qda and lda can be readily pitched against lrg because the latter is also a statistical learner a key difference between lrg and discriminant analyses is that these assume explanatory variables to be normally distributed moreover lda considers the covariance of each of the classes to be identical by default whereas qda does not thus the fact that lrg performed better than the other two and that qda performed better than lda suggests that the model with fewer norms was naturally better suited to deal with a non normal heterogeneous dataset such as the one at hand antonogeorgos et al 2009 liong and foo 2013 svm was the only algorithm to rank among the top five both in terms of validation and test scores a plausible explanation is the ability of this kind of classifier to succeed with relatively sparse training datasets even in those cases where the number of features may be comparatively large trustorff et al 2011 moreover svm is able to deal with both simple and highly complex classification models and by design shows little susceptibility to overfitting adding to this the input dataset did not really test the main disadvantages of svm computationally intensive with large datasets limited capacity to deal with noise and overlapping classes similar considerations apply to mlp mlp algorithms do not make any assumptions about the probability density of the underlying data this enables them to deal with large noisy datasets in an efficient manner however an extremely poor performance on the initial dataset coupled the enormous improvement experienced after optimization suggests the need for parameter fitting 4 2 practical implications and limitations taylor et al 2004 explain that wellhead protection distances typically rely on two basic principles 1 pathogen survival times in groundwater are known and 2 microorganisms are transported at the average linear velocity of groundwater flow these authors also argue that strictly speaking both assumptions are invalid regarding the first one the literature shows that the lifespan of certain pathogens exceeds the standard 25 50 day travel time that is commonly assumed for indicator bacteria which typically translates into a minimum recommended distance of 30 50 m between latrines and groundwater sources argoss 2001 harvey 2007 sphere association 2018 others including viruses are less likely to become attached to soil or rock due to a variety of factors these include microorganism size and surface charge soil texture and composition and the presence of organic matter among others bitton and harvey 1992 as a result certain pathogens may travel longer distances than indicator bacteria under similar hydrogeological conditions secondly taylor et al 2004 show that groundwater velocities in porous media approach a normal distribution statistically extreme values associated with macropores can cause a fraction of the pathogens to move quicker than expected this may endanger groundwater supplies even when theoretically safe distances are kept a practical difficulty is that allowing for 50 day travel times is likely to result in prohibitive distances of separation in many low income contexts argoss 2001 all this suggests that no matter how sophisticated contamination assessments might be further work is needed to first improve fecal indicators in the context of wellhead protection standards and second support whenever possible the ideal of piped water supplies since the machine learning approach relies on ground truth it can be seen as an alternative to velocity based benchmarks notwithstanding the above the ensemble of machine learning classifiers seems to agree that ttc wise the influence area of latrines in the study area would be in the order of 30 50 m fig 8 this is consistent with the flow velocities one would expect in fine intergranular media the fact that the outcomes resemble those obtained by darcian principles reinforces the suitability of machine learning classifiers for predictive pollution mapping machine learning is closely linked to the concept of big data applying machine learning protocols requires detailed field surveys because the predictive ability of classifiers depends above all on the quality and size of the training sample while there are methods to optimize dataset size hua et al 2015 figueroa et al 2012 the minimum number of data points in cases such as the one at hand will typically be in the order of a few hundred park 2013 this can pose a problem because it may not be feasible in areas subject to difficult logistic conditions since anything less than thousands of points is small by machine learning standards certain algorithms will be unlikely to smooth out all inconsistencies thus uncertainties in the results will be partially attributable to the limited number of field measurements a further constraint of the mapping method is that by design it cannot take into account those features that are not spatially distributed such as well construction i e protected unprotected well or the potential for water accumulation around the source this can explain for instance the presence of certain uncontaminated wells in theoretically contaminated areas fig 8 the literature also shows that finding strong correlations between fecal contamination and potentially explanatory variables is difficult wright et al 2013 martínez santos et al 2017 in this context the machine learning approach is seen as a valuable means to enhance the accuracy of predictions due to the ability of these algorithms to find nonlinear associations between features and binary outcome classes in other words machine learning offers a versatile solution in instances where factors other than distance or travel time may come into play furthermore the fact that mlmapper is integrated within the qgis environment makes it relatively simple to understand and apply for non experts in this case the conceptual model was kept deliberately simple both in terms of class and features this is due to the logistic constrains proper to working in relatively isolated environments as well as to the small sample size under more favorable conditions however the outcomes could be improved by taking into consideration additional indicator microorganisms e coli fecal streptococci enterococci associated pollution markers nitrate chloride or geological factors permeability porosity artificial intelligence approaches may further the results of traditional hydrogeological frameworks in the study of groundwater pollution arabgol et al 2016 for instance artificial neural networks support vector machines and fuzzy logic have provided a powerful means to enhance the results of the well known drastic framework nadiri et al 2017a b hamamin and nadiri 2018 however to the authors knowledge there are very few precedents of artificial intelligence approaches in contamination mapping at the scale of the study that has been conducted here even fewer deal with fecal contamination which presents important practical peculiarities in regard to the studies mentioned above in this context it is recognized that our field data is restricted to the end of the dry season as a result some of the variables that constrain fecal pollution in domestic wells during the wet months including recent rainfall and runoff could be safely ignored this is perceived as a minor setback for demonstration purposes but further work would be needed to compare the ability of the algorithms to map fecal pollution across seasons as well as to render the outcomes truly practical finally the fact that individual models attempt to predict the likelihood of contamination in binary terms implies that a positive value would arise from a probability of 0 5 whereas a negative value would be assigned to any outcome below 0 5 coarser outcomes could therefore be expected in the more uncertain areas of the map while the ensemble approach contributes partially to offset this problem multiclass predictions may be expected to improve the results in that case however it is likely that an enhanced field dataset would be needed as multiclass algorithms typically need more data to achieve satisfactory results 5 conclusions fecal pollution poses an increasingly important threat to groundwater supplies in many low income regions of the world the tradeoffs that are likely to arise between access to sanitation and safe drinking water call for proactive approaches to ensure that both universal rights can be realized while piped water supplies and on site treatment may prove efficient in preventing the effects of fecal pollution in the context of the sustainable development goals any mechanisms that contribute to the protection of groundwater resources in the meantime are valuable with this in mind this paper has presented a machine learning approach to map contamination at reduced spatial scales predictive approaches such as this one provide an alternative means to understand groundwater pollution and protect human health in areas where latrine and population density threaten shallow groundwater supplies based on ground truth by design machine learning classification opens up a whole new dimension to contamination assessment by allowing for the evaluation of wellhead protection buffers from considerations other than distance and travel time furthermore this research demonstrates that certain tree based classifiers random forests and statistical learners logistic regression may be well suited to deal with relatively small datasets as well as with a limited number of uncorrelated features these findings however should not be interpreted as an absolute as determining which type of algorithm is likely to perform better on a given dataset is often unfeasible thus an approach based on the selection of the best performing learners out of a large set is advocated as the most sensible course of action this methodology is versatile enough to be exported to any setting provided that spatially distributed sets of explanatory and target variables are available acknowledgments this research has been funded by the agencia española de cooperación al desarrollo aecid under grant numbers 2016 acde 1953 and 2018 acde 0799 and grant number rti2018 099394 b i00 of the ministerio de ciencia innovación y universidades the authors would like to thank the staff of geologists without borders for their time and support 
6253,fecal contamination poses a threat to groundwater supplies in low income regions this is often due to the coexistence of pit latrines with domestic wells in densely populated areas in this context developing alternative methodologies to map fecal pollution in shallow wells is needed a thorough survey of over 240 domestic wells and 570 pit latrines was conducted in a rural town of southern mali water samples were collected from all wells and tested for temperature ph electric conductivity total dissolved solids turbidity and thermotolerant coliforms the outcomes of the field survey were incorporated into a gis database thirteen machine leaning classifiers including different statistical algorithms instance based learners and tree based models were used to determine the spatial distribution of fecal pollution as per five explanatory variables latrine density distance to the closest latrine borehole yield water table depth and population density the best performing classifiers selected on test scores were then used to develop predictive maps random forest and logistic regression rendered prediction scores for fecal pollution in excess of 0 90 multilayer perceptrons support vector machines and quadratic discriminant analyses also proved adept at forecasting fecal pollution ensemble mapping shows that 30 50 m buffers around domestic wells may be sufficient to prevent contamination of domestic supplies in most instances this demonstrates that machine learning may provide a versatile methodological alternative to traditional darcian approaches on the other hand the practical difficulties involved in maintaining wellhead protection areas suggests the need to implement piped water supplies keywords big data contamination domestic well latrine machine learning supervised classification 1 introduction having access to proper water and sanitation facilities is crucial for the realization of all human rights as well as for leading a life in human dignity united nations 2002 2010 in 2015 about 2 3 billion people lacked access to basic sanitation facilities such as toilets or latrines unicef who 2017 besides 663 million remained without access to improved water sources unicef 2015 a figure which is most likely an underestimate martínez santos 2017a in a world where 50 of the population relies on groundwater for drinking unesco 2015 an increasing number of authors point out that there is an impending need to reconsider the tradeoffs between water and sanitation access graham and polizzotto 2013 shivendra and ramaraju 2015 sorensen et al 2016 back et al 2018 this is particularly relevant in low income regions where the gradual adoption of household scale sanitation systems such as pit latrines results in fecal pollution of shallow domestic wells wright et al 2013 odagiri et al 2016 martínez santos et al 2017 debela et al 2018 developing predictive approaches to evaluate contamination in drinking supplies is of paramount importance for these can contribute to underpin wellhead protection mechanisms the academic literature showcases a series of traditional methods for delineating buffer zones in groundwater bodies most of these aim at protecting groundwater quality in drinking supply wells and have been incorporated to legislation in many countries igme 2003 strobl and robillard 2005 parker and carlier 2009 for instance volumetric numerical analytical and tracer based approaches have proven applicable in intergranular media evers and lerner 1998 levy and ludy 2000 martínez navarrete 2002 among these the classic analytical methods by bear and jacob 1965 and wyssling 1979 have become particularly widespread in contrast only tracer based and numerical techniques are suited to fractured media and karst systems largely due to the prevalence of secondary porosity and conduit flow a recurrent problem with the application of these approaches is the absence of important data including the spatial distribution of aquifer permeability and storage coefficients groundwater heads groundwater velocities and the thickness of the unsaturated zone to name a few key variables not the least of problems is determining the distribution of cracks and fractures in fissured media or that of conduits in karst environments in the case of the more sophisticated tools such as numerical models this is further aggravated by the difficulties involved in obtaining enough information to calibrate the outcomes all this hampers attempts to protect groundwater supplies in geographical contexts where contamination represents a threat to public health for simplicity wellhead protection buffers in humanitarian standards and low income regions typically rely on rules of thumb harvey 2007 parker and carlier 2009 because these are derived directly from darcian principles some of their underlying assumptions have been challenged taylor et al 2004 for instance demonstrate that assuming that fecal contamination travels at the average linear velocity of groundwater may be far from appropriate however these rules stem from practical experience and are useful in contexts where both data and technical means may be scarce macdonald et al 2005 sphere association 2018 machine learning is a subset of artificial learning techniques which is still novel in the field of groundwater resources while the literature showcases some examples in the domain of groundwater potential mapping duan et al 2016 naghibi et al 2017 chen et al 2018 comparatively little work has been carried out in relation to domestic supplies machiwal et al 2018 studies dealing with contamination are even more limited recent and almost invariably restricted to regional scale studies thus arabgol et al 2016 used support vector machines to predict the spatial distribution of nitrate contamination in groundwater in the arak plain iran similarly sajedi hosseini et al 2018 combined boosted regression trees multivariate discriminant analysis and support vector machines to map nitrate pollution in aquifer bodies whereas barzegar et al 2018 compared the performance of extreme learning machine models with multilayer perceptrons and support vector machines in the context on fluoride contamination artificial intelligence approaches have also been coupled with traditional hydrogeological frameworks to enhance the outcomes of studies dealing with groundwater susceptibility to contamination baghapour et al 2016 nadiri et al 2018 the goal of this paper is to present and validate an artificial intelligence approach to map fecal pollution in domestic groundwater supplies this is perceived as a methodological novelty in several ways in the first place in this field there are very few precedents of local scale predictive mapping based on a high density of data points over 68 wells and 160 pit latrines per square kilometer and a thorough survey of several hundred households furthermore this technique uses a large number of machine learning classifiers 13 out of which an ensemble of the best performers is used to develop the final product this may be of use in cases where standard hydrogeological variables are difficult to take into account thus it represents an alternative to distance and isochrone based wellhead protection approaches and may provide additional insight in efforts to protect groundwater supplies 2 materials and methods 2 1 study area this study focuses on the town of beleko beleko is the capital of the rural commune of djedougou southern mali fig 1 this town comprises seven neighborhoods and is home to over 6000 people the region presents a hot semi arid climate with an average yearly temperature of 26 c rainfall is subject to the west african monsoon and amounts to 800 mm annually from the hydrogeological standpoint the study area is located within the infracambrian metasedimentary domain of southern mali traore et al 2019 from top to bottom the lithostratigraphic profile consists in a hard laterite crust whose thickness may exceed five meters this is underlain by an unconsolidated layer made up of clays with intercalations of fine sand typically 10 15 m thick the regional sandstone aquifer is immediately beneath its thickness exceeds one hundred meters over the impervious gneiss basement garcía castro and garcía rincón 2017 the water table depth oscillates between 5 and 15 m across most of the study area at the end of the dry season and between 1 and 3 m towards the end of the rainy months the geographies of water access and sanitation in the beleko area have been thoroughly described by martínez santos 2017b in the absence of permanent surface water courses the population relies exclusively on groundwater groundwater is obtained either from communal supplies standpipes or deep boreholes equipped with hand pumps or shallow domestic wells the latter are typically unlined and less than 15 m deep there is no public sanitation system instead informal sanitation facilities i e pit latrines are present in most households pit latrines are built much like wells but are shallower typically 2 6 m and wider 1x1 to 2x2 meters the vast majority are unlined and located in close proximity to domestic supplies thus posing a direct threat to shallow groundwater and human health this is particularly true of the rainy season when the water table rises close to the ground level approximately 80 of the households own a domestic well whereas close to 85 own a pit latrine martínez santos et al 2017 2 2 data collection and sampling data was collected during a three week field campaign in april and may 2018 over 240 randomly selected wells were geo referenced and examined directly well type and construction features were recorded in all cases as per the systematic described in table 1 additionally all wells were also surveyed in situ for groundwater depth temperature electric conductivity and total dissolved solids by means of a hannah hi 98121 multiparametric device turbidity was measured with a field turbidimeter fecal pollution surveys rely either on studying the concentration of different pathogens or on that of a single indicator microorganism in the latter case e coli is typically preferred because it occurs in high numbers in human and animal feces and water subject to recent fecal pollution in most circumstances populations of thermotolerant coliforms ttc are composed predominantly of e coli which is the reason why ttc are still regarded as a less reliable though acceptable indicator of fecal pollution who 2011 2017 since the goal of this research is to demonstrate the predictive mapping methodology rather than to depict the distribution of different pathogens in groundwater exhaustively the use of ttc was considered sufficiently representative for practical purposes thus a water sample was collected from each well and analyzed for ttc with an oxfam delagua portable laboratory oxfam 2009 standard transport procedures and incubation times were used colony forming units cfu per 100 ml of water were counted by hand within ten minutes of retrieval from the incubator concentrations in excess of 50 cfu 100 ml were considered too numerous to count latrines were geo referenced on a household by household basis table 2 about 570 latrines were inventoried in an area of approximately 3 5 km2 that is over 160 per square kilometer other household related variables such as the number of people or the water treatment habits at the household scale were recorded latrine and well data together with the spatial distribution of each of the explanatory variables were incorporated into a qgis 3 2 database 2 3 machine learning protocol machine learning aims at developing computer algorithms that learn automatically from experience much like humans do mitchell 1997 hastie et al 2009 the underlying assumption is that computers are powerful enough to establish complex associations among those independent variables of a given dataset that combine to explain a known outcome once this is achieved the algorithm can predict any outcome based on any blend of independent variables in many ways machine learning is closely linked to the concept of statistical inference except that machine learning algorithms are typically more complex and do not always rely on statistical principles table 3 any machine learning exercise requires a dataset and an algorithm or learner the dataset consists of two elements namely class and features the class is the dependent variable i e the outcome that the algorithm will attempt to predict while the term features refers to the ensemble of all explanatory variables that may describe such an outcome the machine learning process typically takes place in three steps data is split into a training and a test dataset first then the algorithm is trained on the former during the training stage the computer is allowed to use both the class and the features attempting to derive meaningful associations between both in the third step testing the reliability of the algorithm is checked by evaluating how accurate it is at predicting the class of the test dataset when only allowed to use the features as explained later an additional parameter fitting step can be added between training and testing in order to optimize algorithm performance this stage is generally called validation in artificial intelligence protocols 2 3 1 conceptual model in this case the aim of the machine learning model is to predict fecal contamination in groundwater due to the ease with which thermotolerant coliforms ttc can be measured this indicator was selected as the class variable ttcs are typically measured by quantifying the number of colony forming units cfu per 100 ml of water international standards establish 0 cfu 100 ml as the threshold for drinking purposes who 2011 but a concentration of fewer than 10 cfu 100 ml can be acceptable who 2002 this is particularly true of instances where due to exposure the population may have developed a certain level of tolerance data requirements become greater as models get more complex whenever large amounts of information are available it is common practice to include as many features as possible no matter which ones are expected to be relevant a priori this is because machine learning algorithms are specifically designed to deal with big data in studies such as this one however where the dataset is small by machine learning standards n 271 it is best to keep the conceptual model as simple as possible this can be achieved by minimizing the complexity of the class and the number of features thus a binary class was defined wells presenting a concentration in excess of 10 cfu 100 ml were considered polluted whereas those with lesser concentrations were labelled suitable for drinking in remote settings such as the one at hand where logistics are difficult the choice of explanatory variables is sometimes constrained by whatever can be readily obtained rather than by explicit hydrogeological considerations while detailed soil profiles indicating the permeability of each layer or transmissivity and velocity fields would provide extremely valuable information these are typically difficult to obtain in practice the idea is therefore to develop a sufficiently robust method to predict contamination based on proxy variables thus five key features explanatory variables were taken into account these include latrine density distance to closest latrine water table depth population density and borehole yield fig 2 it is self evident that the presence of a latrine can explain fecal contamination in a nearby well moreover all other factors being equal the closer the latrine the greater the threat this suggests that distance to the closest latrine would be an adequate feature to consider in any predictive attempt nevertheless this by itself would provide an incomplete picture as other factors may come into play for instance a large number of latrines within a certain radius poses a greater pollution threat than a single latrine which implies that latrine density is also important so is household population since the number of people using a given latrine will ultimately control the microbial load released into the aquifer furthermore direct observation shows that the larger households in the study area present other potential contamination sources including free roaming livestock and manure piles flow direction can be expected to govern coliform transport flow direction is obtained from the elevation of the water table and more specifically from the hydraulic gradient in this case local scale gradients can be expected to change relatively easily over time due to the large number of groundwater extraction points furthermore the only available source of elevation data was a digital elevation model which means that the outcomes are not fine enough to determine groundwater flow direction accurately in the absence of more detailed information water table depth was taken into consideration only as a protection mechanism as the thickness of the unsaturated zone delays contaminant influx in turn the effects of advective transport were assumed to be partially implicit within latrine distance and density finally borehole yield data was used as an indirect proxy for the aquifer s ability to store and transmit water 2 3 2 software mlmapper v 1 0 was used to map contamination mlmapper is a qgis 3 plugin that builds predictive maps based on point source data mlmapper implements thirteen machine learning classifiers from the scikit learn 0 19 2 toolbox pedregosa et al 2011 the theory behind which has been widely discussed in the literature kotsiantis 2007 hastie et al 2009 pedregosa et al 2011 mlmapper algorithms include support vector machines svm logistic regression lrg decision tree classifier crt random forest classifier rfc k neighbour classification knn linear discriminant analysis lda gaussian naïve bayes classification nba multilayer perceptron neural network mlp ada boost classifier abc quadratic discriminant analysis qda gradient boosting classification gbc and gaussian process classifier gpc a brief description of all these algorithms is provided in table 2 mlmapper requires two inputs fig 3 the first one is used to train and test the model this consists in a point vector shapefile that includes all class and feature values for each well the class is expressed as a boolean variable where 0 means uncontaminated and 1 means contaminated the second input is used to develop a fecal contamination map based on the outcome of the training and testing processes this is also a point vector shapefile which includes the feature values for every pixel in the spatial database outcomes are of two types on the one hand mlmapper provides a series of standard performance metrics including training and test scores precision recall and f 1 outputs confusion matrices and area under the curve obtained from receiver operator characteristic curves provide complementary means to analyze the numerical results these are all non dimensional and will be discussed later on on the other hand each run of mlmapper renders one single predictive map this is computed as the ensemble of the best performing classifiers based on a metric of choice test score in this case a specific map may also be developed from each classifier as per its own individual scores 2 3 3 handling overfitting when data associations are unclear machine learning classifiers tend to over fit the training dataset overfitting is a process whereby the computer memorizes each instance of the training dataset in order to maximize the training score instead of developing associations that can be successfully generalized thus it typically results in a good training score coupled with a low test score overfitting may also be the result of an imbalanced dataset i e too few instances of one of the class outcomes in such cases certain algorithms focus on explaining the more frequent class outcomes while ignoring the others techniques to avoid overfitting include enhancing the dataset by adding new data or by redistributing the existing class the effectiveness of these two approaches can be furthered by automated parameter fitting note that parameter fitting is typically known as validation in the machine learning terminology parameter fitting involves an additional subdivision of the input data so that it ultimately comprises training validation and test datasets the validation set is used between training and testing and its purpose is to fine tune the internal parameters of the machine learning algorithm prior to checking its generalization potential on the test set thus the algorithm undergoes two training stages instead of just one because the input dataset in this case is skewed towards the positive outcome polluted well an attempt was made to balance the classes by a considering a contamination threshold of 10 cfu 100 ml which implies that samples with a tolerable degree of ttc content were considered uncontaminated and b adding a small number of uncontaminated wells training wells in fig 1 in areas of the map where no fecal pollution due to latrines would be expected i e outside the town these two approaches were further enhanced by carrying out automated grid search optimization this is an in built mlmapper routine that aims at maximizing test score through automated parameter fitting for this purpose train test validation splits of 50 50 60 40 70 30 and 80 20 were established throughout the text splits are simplified for the sake of readability however a 70 30 split means that 70 of the original dataset was used for training while the remaining 30 was split again using a 70 30 proportion that is 9 for validation and 21 for testing these splits are commonly used in machine learning studies across different disciplines dobbin and simon 2011 naghibi et al 2017 sakr et al 2018 2 3 4 ensemble approach mlmapper allows for performing an ensemble of the best performing classifiers in this case ensembling consists in computing the arithmetic mean of the most reliable classifiers which are selected based on test score the ensemble presents three major advantages over individual classification in the first place it reduces noise by disregarding those classifiers that have failed to predict the presence or absence of contamination reliably secondly the ensemble can be used to depict uncertainty from a spatial perspective for the purpose of analysis uncertainty is interpreted as the degree of discrepancy among classifiers finally ensembling provides a graded outcome which is both more informative than the binary outcomes that result from individual classification 3 results 3 1 field survey table 4 presents some descriptive statistics of water quality in domestic wells in situ characterization shows groundwater to be slightly acidic with a low electric conductivity turbidity varies widely as expected due to the nature of the sources in general terms however the key finding of this field survey is that fecal pollution represents a widespread threat to human health almost 80 of the wells presented a ttc concentration in excess of 10 cfu 100 ml this figure is particularly relevant in view that close to 58 of the households used their wells for drinking an estimated 50 of the population drinks contaminated water on a regular basis besides the survey was carried out at the end of the dry season where the incidence of fecal contamination can be expected to be lowest thus the outcomes suggest that fecal contamination in drinking supplies poses a permanent threat well protection is a major issue two thirds of the surveyed wells were thoroughly unprotected the majority of these 80 presented a significant coliform presence approximately 68 of protected wells were polluted causes should be found in the high density of latrines 77 of the wells are located within 30 m of the nearest latrine as well as in the nearby presence of contamination sources such as free roaming livestock garbage and manure piles contamination sources other than latrines were observed within a radius of 30 m in about 85 of the households deficient well construction and operation also seem to explain contamination to a certain extent many of the protected wells were only protected at the surface i e equipped with a concrete apron lid fence or parapet but not in depth besides a relatively large number 25 were found to be uncovered at the time of the visit in approximately the same proportion of instances buckets were observed to be left lying around rather than hung 3 2 optimized classifier scores the accuracy of a machine learning classifier is typically evaluated in terms of training and test scores the training score represents how well a classifier performs on the training set in other words it provides an idea as to how the classifier fares in terms of developing associations between the independent and the dependent variables in contrast test score is defined as the number of correct guesses divided by the total number of attempts on the test dataset since the aim of this research is to predict the occurrence of groundwater contamination test score was considered the chief evaluation criteria table 5 shows a series of relevant metrics to evaluate the performance of the thirteen classifiers several algorithms were observed to underperform prior to parameter fitting mlp 0 69 nba 0 75 qda 0 77 moreover nearly half of them crt rfc abc gbc gpc overfitted the data at first training scores being up to 0 15 0 20 higher than validation scores the fact that automated parameter fitting was successful at increasing accuracy above 0 85 in all cases while simultaneously reducing overfitting attests to the importance of optimization procedures remarkably automated parameter optimization caused the test score of mlp to rise from lowest 0 69 to the top five after fine tuning 0 90 on a different note it is theoretically possible for the test score to exceed the training score this is because testing is performed on an entirely different dataset where the classifier may in fact attain a higher degree of success than in the training set in the case at hand this occurs with svm optimized training score 0 83 vs test score 0 90 and qda 0 89 vs 0 91 precision and recall are complementary metrics precision represents the number of times a certain class outcome was predicted correctly divided by the total number of times the class was predicted in practice precision is computed as the number of true positives over the sum of true and false positives for the case of the contaminated class this represents how many times contamination was predicted correctly out of all the times the algorithm predicted the outcome contaminated well since algorithms present a high prec 1 score it can be concluded that these were unlikely to return false positives uncontaminated outcome in wells that were actually contaminated in contrast the prec 0 score is a little lower on average largely due to the tendency of crt prec 0 0 69 gbc 0 69 abc 0 71 and lvc 0 71 to predict contamination in uncontaminated wells two algorithms rendered balanced outcomes namely svm prec 0 0 89 vs prec 1 0 90 and qda 0 89 vs 0 91 recall represents the number of times a class was predicted correctly over the actual number of data points of that class in other words recall is the ability of a model to detect all the relevant instances of a given class within the input dataset recall is computed as the number of true positives over the sum of true positives plus false negatives all algorithms except for crt rec 1 0 89 and gbc 0 90 presented recall values for the contaminated class well in excess of 0 90 this means that the algorithms were generally able to detect the majority of contaminated wells however the weakest performance metric for all classifiers was the recall for the uncontaminated outcome all algorithms except perhaps for crt and rfc rec 0 0 80 failed to label correctly a number of uncontaminated wells there is a tradeoff between precision and recall in practice the attempt to maximize one leads to diminish the other the f 1 score is the harmonic mean of both metrics and can be used to maximize the balance in this case rfc and lrg present the highest f 1 scores whereas nba gpc and gbc present the lowest again most algorithms performed noticeably worse in the case of the uncontaminated class in the case of all three metrics precision recall f 1 this is attributed to the comparatively smaller weight of the uncontaminated outcome in the input dataset an additional motive could be the inability of mlmapper to consider non spatially distributed variables such as well type protected unprotected receiver operator characteristic curves roc curves are commonly used to visualize the performance of binary classifiers fig 4 roc curves plot false positives horizontal axis versus true positives vertical axis the curve thus represents the diagnostic ability of a binary classifier system and how this changes with different discrimination thresholds the cutoff values imposed on the predicted probabilities for assigning observations to each class the area under the curve auc represents how well the classifier performed an auc of 1 means that the model is perfectly able to distinguish between positive and a negative outcome whereas an auc of 0 5 suggests a completely arbitrary result thus the closer the curve is to the top left corner of the chart the better the predictive ability of the model conversely the closer it is to the main diagonal of the chart the closer it is to predict at random in this case all classifiers except for abc and gpc rendered aucs of at least 0 86 which points at a reasonable solid performance across the board 3 3 sensitivity analyses and classifier selection two sensitivity analyses were conducted both based on test score the first one pertains to the dataset split fig 5 on average all splits rendered optimized test scores of at least 0 85 the highest arithmetic mean of all classifiers was obtained for a 60 40 split 0 91 whereas the lowest corresponded to the 80 20 one 0 85 furthermore the 60 40 split rendered the highest maximum and minimum values the best five classifiers in the 60 40 split lrg rfc qda mlp and svm yielded test scores between 0 90 and 0 91 with a mean of 0 906 and a standard deviation of 0 005 table 6 this suggests that these provide a reasonably robust and consistent prediction of fecal pollution in groundwater the second sensitivity analysis deals with feature importance different machine learning classifiers compute feature weights in various ways besides each family of algorithms provides its own set of metrics for feature importance it is thus necessary to find some common ground for comparison when dealing with several classifiers at the same time a useful approach is to compute the mean accuracy decrease due to each feature this consists in carrying out a series of runs of each model one feature is removed at a time to check how this affects predictive ability the underlying assumption is that removing important variables will reduce performance significantly whereas taking away the unimportant ones should barely change the test score removing unimportant variables can also be expected to decrease noise which may in fact reduce overfitting and improve the predictive ability of certain algorithms the best performing algorithms were run five times in each of these runs one of the features was removed the mean accuracy of the ensemble was observed to drop in all cases which suggests that all five features are relevant predictors features were then ranked based on the resulting test score although performance was similar in all cases water table depth proved to be the most important variable on average followed by distance to the nearest latrine latrine density population density and borehole yield fig 6 because removing each of these last three led to a relatively small drop in test score three additional runs were carried out to determine whether deleting any combination would yield better outcomes information and predictive ability losses were detected in all three cases therefore all five features were ultimately kept 3 4 contamination maps fig 7 presents the spatial generalization of machine learning classifiers into predictive maps fig 7a through 7e show the outcomes of the five best performing classifiers lrg rfc qda mlp and svm which were selected based on optimized test scores green colored pixels depict areas where the algorithm predicts low contamination whereas red pixels represent high contamination in general terms all maps render an intuitively correct picture the heavily populated areas of the northeast northwest and southeast all present high contamination while lower values are predicted for the central part and the outskirts where latrines do not exist in general terms the mlp algorithm presents the highest tendency to predict contamination 34 of the surface area is taken up by red pixels followed by the qda 31 and svm models 30 in contrast rfc 25 and lrg 27 are comparatively avert to labelling pixels as potentially contaminated svm rfc and lrg and to a lesser extent qda present a strong resemblance to each other the importance of the distance to the nearest latrine feature can be readily observed in all four cases as contaminated areas tend to form circular buffers around latrines in this regard one of the key differences is that qda and mlp tend to fill up the voids between circular buffers with high contamination predictions whereas the others do not each classifier relies on different principles thus performing an ensemble not only contributes to offset the noise derived from classifier specific nuances but also provides a measure of uncertainty i e the degree of discrepancy among classifiers in this context fig 7f represents the arithmetic mean of the best five classifiers for each pixel a green outcome implies that all classifiers agreed to label that pixel as uncontaminated whereas a red one means that all agreed on a contaminated outcome yellow pixels are those where the disagreement among classifiers is stronger the outcome of the ensemble map reinforces the idea that all explanatory variables were relevant to some extent and also suggests that the five best classifiers are largely consistent in their predictions 4 discussion 4 1 machine learning methods except for some of the simpler algorithms such as decision trees machine learning classification is largely a black box bibal and frenay 2016 samek et al 2017 the complexity and nonlinearity of these approaches coupled with that of the interrelation between classes and features typically makes it impossible to foretell which model will perform better with a given dataset furthermore the interpretation of most machine learning algorithms is as much of an art as it is of a science as it often relies on user experience and intuition over more tangible factors honegger 2018 there are however certain aspects that may contribute to explain why a given classifier outperforms another for instance random forests rfc are by design an enhanced version of decision trees crt in fact a random forest consists in the development a large number of decision trees followed by carrying out an ensemble of the outcomes while this makes the decision mechanism nearly impossible to understand it also reduces bias and noise which in turn contributes to decrease overfitting this could provide a plausible explanation as to why the optimized rfc ranked high among the top five classifiers while the optimized crt ended up within the bottom five on a more specific note simple decision trees such as crt are notably poor performers in the case of unbalanced classes and limited sample sizes both of which occur in this example lrg rfc qda mlp and svm rendered the more accurate results test scores ranging between 0 90 and 0 91 this implies that these algorithms predicted contamination acceptably as explained earlier rfc presents the most balanced set of metrics including the highest test score positive precision negative recall and f 1 outcomes table 5 this attests to the versatility of random forest approaches in dealing with small potentially noisy datasets it also means that the rfc algorithm is easy to train and suitable for handling spatially distributed data breiman 2001 cracknell and reading 2014 rfc proved computationally efficient despite its reputation for being a slow classifier which could be explained by the small dataset size low initial training and validation scores 1 00 vs 0 80 coupled with the optimized training and test score 0 97 vs 0 91 suggest the need to implement routines to prevent overfitting in this regard it is observed that several classifiers svm lrg knn lvc proved far more robust than rfc in the initial non optimized run the implementation of lrg is similar to that of multiple linear regression except that its outcomes are typically binary sperandei 2013 lrg is computationally effective and can handle nonlinear relationships between class and features because it applies a non linear log transformation of the linear regression approach lrg typically performs best with large dataset and a limited number of features but is vulnerable to interdependency and collinearity among features park 2013 the solid performance of lrg in this case comparable to that of rfc could be possibly related to fact that the conceptual model included just five variables that the relevance of these for groundwater contamination is conceptually justified in all cases and that there was little correlation among them table 7 from a case specific standpoint the results obtained by lrg provide additional insight on those obtained by martínez santos et al 2017 who applied multiple linear regression to predict contamination in domestic wells of three out of beleko s seven neighborhoods in particular lrg was able to develop stronger associations among explanatory variables and the target thus proving to be a far more reliable predictor for fecal contamination despite comparably high test scores qda mlp and svm are considered second to lrg and rfc due to lower negative outcome recall values 0 68 0 68 and 0 64 vs 0 72 and 0 80 qda and lda can be readily pitched against lrg because the latter is also a statistical learner a key difference between lrg and discriminant analyses is that these assume explanatory variables to be normally distributed moreover lda considers the covariance of each of the classes to be identical by default whereas qda does not thus the fact that lrg performed better than the other two and that qda performed better than lda suggests that the model with fewer norms was naturally better suited to deal with a non normal heterogeneous dataset such as the one at hand antonogeorgos et al 2009 liong and foo 2013 svm was the only algorithm to rank among the top five both in terms of validation and test scores a plausible explanation is the ability of this kind of classifier to succeed with relatively sparse training datasets even in those cases where the number of features may be comparatively large trustorff et al 2011 moreover svm is able to deal with both simple and highly complex classification models and by design shows little susceptibility to overfitting adding to this the input dataset did not really test the main disadvantages of svm computationally intensive with large datasets limited capacity to deal with noise and overlapping classes similar considerations apply to mlp mlp algorithms do not make any assumptions about the probability density of the underlying data this enables them to deal with large noisy datasets in an efficient manner however an extremely poor performance on the initial dataset coupled the enormous improvement experienced after optimization suggests the need for parameter fitting 4 2 practical implications and limitations taylor et al 2004 explain that wellhead protection distances typically rely on two basic principles 1 pathogen survival times in groundwater are known and 2 microorganisms are transported at the average linear velocity of groundwater flow these authors also argue that strictly speaking both assumptions are invalid regarding the first one the literature shows that the lifespan of certain pathogens exceeds the standard 25 50 day travel time that is commonly assumed for indicator bacteria which typically translates into a minimum recommended distance of 30 50 m between latrines and groundwater sources argoss 2001 harvey 2007 sphere association 2018 others including viruses are less likely to become attached to soil or rock due to a variety of factors these include microorganism size and surface charge soil texture and composition and the presence of organic matter among others bitton and harvey 1992 as a result certain pathogens may travel longer distances than indicator bacteria under similar hydrogeological conditions secondly taylor et al 2004 show that groundwater velocities in porous media approach a normal distribution statistically extreme values associated with macropores can cause a fraction of the pathogens to move quicker than expected this may endanger groundwater supplies even when theoretically safe distances are kept a practical difficulty is that allowing for 50 day travel times is likely to result in prohibitive distances of separation in many low income contexts argoss 2001 all this suggests that no matter how sophisticated contamination assessments might be further work is needed to first improve fecal indicators in the context of wellhead protection standards and second support whenever possible the ideal of piped water supplies since the machine learning approach relies on ground truth it can be seen as an alternative to velocity based benchmarks notwithstanding the above the ensemble of machine learning classifiers seems to agree that ttc wise the influence area of latrines in the study area would be in the order of 30 50 m fig 8 this is consistent with the flow velocities one would expect in fine intergranular media the fact that the outcomes resemble those obtained by darcian principles reinforces the suitability of machine learning classifiers for predictive pollution mapping machine learning is closely linked to the concept of big data applying machine learning protocols requires detailed field surveys because the predictive ability of classifiers depends above all on the quality and size of the training sample while there are methods to optimize dataset size hua et al 2015 figueroa et al 2012 the minimum number of data points in cases such as the one at hand will typically be in the order of a few hundred park 2013 this can pose a problem because it may not be feasible in areas subject to difficult logistic conditions since anything less than thousands of points is small by machine learning standards certain algorithms will be unlikely to smooth out all inconsistencies thus uncertainties in the results will be partially attributable to the limited number of field measurements a further constraint of the mapping method is that by design it cannot take into account those features that are not spatially distributed such as well construction i e protected unprotected well or the potential for water accumulation around the source this can explain for instance the presence of certain uncontaminated wells in theoretically contaminated areas fig 8 the literature also shows that finding strong correlations between fecal contamination and potentially explanatory variables is difficult wright et al 2013 martínez santos et al 2017 in this context the machine learning approach is seen as a valuable means to enhance the accuracy of predictions due to the ability of these algorithms to find nonlinear associations between features and binary outcome classes in other words machine learning offers a versatile solution in instances where factors other than distance or travel time may come into play furthermore the fact that mlmapper is integrated within the qgis environment makes it relatively simple to understand and apply for non experts in this case the conceptual model was kept deliberately simple both in terms of class and features this is due to the logistic constrains proper to working in relatively isolated environments as well as to the small sample size under more favorable conditions however the outcomes could be improved by taking into consideration additional indicator microorganisms e coli fecal streptococci enterococci associated pollution markers nitrate chloride or geological factors permeability porosity artificial intelligence approaches may further the results of traditional hydrogeological frameworks in the study of groundwater pollution arabgol et al 2016 for instance artificial neural networks support vector machines and fuzzy logic have provided a powerful means to enhance the results of the well known drastic framework nadiri et al 2017a b hamamin and nadiri 2018 however to the authors knowledge there are very few precedents of artificial intelligence approaches in contamination mapping at the scale of the study that has been conducted here even fewer deal with fecal contamination which presents important practical peculiarities in regard to the studies mentioned above in this context it is recognized that our field data is restricted to the end of the dry season as a result some of the variables that constrain fecal pollution in domestic wells during the wet months including recent rainfall and runoff could be safely ignored this is perceived as a minor setback for demonstration purposes but further work would be needed to compare the ability of the algorithms to map fecal pollution across seasons as well as to render the outcomes truly practical finally the fact that individual models attempt to predict the likelihood of contamination in binary terms implies that a positive value would arise from a probability of 0 5 whereas a negative value would be assigned to any outcome below 0 5 coarser outcomes could therefore be expected in the more uncertain areas of the map while the ensemble approach contributes partially to offset this problem multiclass predictions may be expected to improve the results in that case however it is likely that an enhanced field dataset would be needed as multiclass algorithms typically need more data to achieve satisfactory results 5 conclusions fecal pollution poses an increasingly important threat to groundwater supplies in many low income regions of the world the tradeoffs that are likely to arise between access to sanitation and safe drinking water call for proactive approaches to ensure that both universal rights can be realized while piped water supplies and on site treatment may prove efficient in preventing the effects of fecal pollution in the context of the sustainable development goals any mechanisms that contribute to the protection of groundwater resources in the meantime are valuable with this in mind this paper has presented a machine learning approach to map contamination at reduced spatial scales predictive approaches such as this one provide an alternative means to understand groundwater pollution and protect human health in areas where latrine and population density threaten shallow groundwater supplies based on ground truth by design machine learning classification opens up a whole new dimension to contamination assessment by allowing for the evaluation of wellhead protection buffers from considerations other than distance and travel time furthermore this research demonstrates that certain tree based classifiers random forests and statistical learners logistic regression may be well suited to deal with relatively small datasets as well as with a limited number of uncorrelated features these findings however should not be interpreted as an absolute as determining which type of algorithm is likely to perform better on a given dataset is often unfeasible thus an approach based on the selection of the best performing learners out of a large set is advocated as the most sensible course of action this methodology is versatile enough to be exported to any setting provided that spatially distributed sets of explanatory and target variables are available acknowledgments this research has been funded by the agencia española de cooperación al desarrollo aecid under grant numbers 2016 acde 1953 and 2018 acde 0799 and grant number rti2018 099394 b i00 of the ministerio de ciencia innovación y universidades the authors would like to thank the staff of geologists without borders for their time and support 
6254,accurate estimation of mean flow velocity is imperative for the prediction of hydrographs and sediment yield the chezy and manning equations are the most widely used for calculating flow velocity in runoff and erosion models the suitability of both equations for overland flow on bare land has been evaluated and conflicting results were obtained in previous studies the suitability of these equations for overland flow on a planted slope requires further investigation the purposes of this study were to evaluate the suitability of the chezy and manning equations under planted conditions and evaluate quantitative effect of vegetation on manning s n and then on flow velocity flume experiments were conducted at the plant basal cover ranging from 0 to 30 slope gradient varying from8 8 to 25 9 and flow discharge ranging from 0 5 to 2 0 10 3 m3 s 1 the flow depth and flow velocity were measured the coefficient of determination r 2 and nash sutcliffe model efficiency nse were used to compare the performance accuracy of both equations the results showed that both equations provided a bad accuracy when plant basal cover was greater than 15 on a planted slope the nse values were negative for manning equation and close to 0 for chezy equation it indicates that manning and chezy equations were not suitable for the planted slope manning s n decreased with the flow depth for 0 plant basal cover and increased with flow depth for plant basal cover greater than 1 25 thus a new equation for calculating manning s n was developed based on plant basal cover and flow depth and was used in the manning equation the new flow velocity equation including plant basal cover provided satisfactory accuracy with a nse of 0 995 the results indicate that the manning s n was significantly affected by flow depth and plant basal cover keywords flow velocity plant basal cover manning equation chezy equation 1 introduction flow velocity is an important factor that influences hydrological and soil erosion processes accurate estimation of mean flow velocity is imperative for the prediction of hydrographs and sediment yield ali et al 2012 the chezy eq 1 and manning eq 2 equations are the most widely used for calculating flow velocity in runoff and erosion models mügler et al 2011 1 v c h 0 5 s 0 5 2 v 1 n h 2 3 s 0 5 where v is the mean flow velocity m s h is the flow depth m s is the slope gradient m m c is the chezy coefficient m1 2 s n is the manning coefficient m 1 3 s the chezy and manning equations were originally developed for the purpose of sizing canals in the xixth century mouret 1921 the suitability of both equations for overland flow is questionable turner and chanmeesri 1984 abrahams et al 1994 rouhipour et al 1999 smith et al 2007 kirstetter et al 2016 the main reason is that these equations assume numerous restrictive conditions flow is uniform flow occurs parallel to the surface flow width is constant and grain roughness is homogeneous over the wetted perimeter and can be considered as random turner and chanmeesri 1984 smith et al 2007 however none of these conditions is met in overland flow smith et al 2007 nevertheless many researchers have attempted to determine the accuracy of two equations with respect to overland flow table 1 the results differ for bare land rouhipour et al 1999 found that manning equation received stronger support than the chezy equations but both equations were useful the results from mügler et al 2011 showed that manning s model with a water depth dependent roughness coefficient provided the best flow velocity estimation kirstetter et al 2016 found that the manning equation is not suitable for laminar flow in addition some researchers also thought that there was no important influence of slope on flow velocities in rills nearing et al 1997 govers 1992 the plant stem and basal cover effectively decrease flow velocity turner and chanmeesri 1984 weltz et al 1992 gilley and kottwitz 1994 wu 2008 cao et al 2011 fathi moghadam and drikvandi 2012 nehal et al 2012 zhao et al 2016 the main reason is that the plant stem increases flow resistance abrahams et al 1994 gilley and kottwitz 1994 jin et al 2000 nehal et al 2012 yang et al 2017 considerable research has been conducted on the effect of vegetation cover on the flow resistance coefficient the flow resistance coefficient was affected by vegetation type abrahams et al 1994 kadlec and wallace 2009 vegetation density jin et al 2000 xia and nehal 2013 vegetation stem diameter zhao et al 2016 vegetation flexibility xia and nehal 2013 bora and misra 2018 and relative flow depth to the height of vegetation hall and freeman 1994 jin et al 2000 for example abrahams et al 1994 found that the flow resistance coefficient for grassland was higher than that for shrubland jin et al 2000 found that the resistance of vegetation is proportional to the flow depth and the vegetation density yang et al 2017 reported that vegetation has a more obvious influence on flow resistance than surface roughness when the unit discharge is higher than the upper limited critical discharge the results from zhao et al 2016 showed that the flow resistance is affected by the vegetation stem diameter bora and misra 2018 found that rigid vegetation has more resistance to flow than flexible vegetation under similar flow condition and manning s n reduces with the increase in the flexibility of the vegetation for a given depth of flow although these qualitative effect of vegetation on flow resistance were studied it is very important to investigate the quantitative effect of plant basal cover on flow resistance and then on flow velocity for the simulation of hydrological and soil erosion processes some researchers found that manning equation could be rewritten as a more general form liu and hwang 1959 turner and chanmeesri 1984 tsihrintzis and madiedo 2000 kadlec and wallace 2009 as in eq 3 3 v c h x s y where c x and y are parameters when c 1 n x 2 3 and y 1 2 equation 3 becomes manning equation the manning n can be expressed as eq 4 reed et al 1995 kadlec and wallace 2009 4 n γ h 1 2 where γ is a constant which varied from 1 to 4 m1 6 s for most marshes with emergent vegetation kadlec and wallace 2009 reviewed the experiments from wetland and found that the x in eq 3 varied from 0 44 to 2 and y in eq 3 from 0 71 to 1 tsihrintzis and madiedo 2000 thought that x varied from 0 67 turbulent flow to 2 0 laminar flow and y changed from 0 5 turbulent flow to 1 0 laminar flow turner and chanmeesri 1984 found that y was 0 35 based on the experiments on water flowing through dense crops of wheat the results showed that the exponents x and y may vary under different surface conditions in addition manning s n was affected by vegetation jin et al 2000 and varied with vegetation density nehal et al 2012 västilä and järvelä 2018 the flow velocity equation can be written as eq 5 according to the momentum balance petryk and bosmajian 1975 jin et al 2000 5 v h 2 3 s 1 2 n 0 2 cd 2 g d h 4 3 1 2 where n 0 is the roughness coefficient excluding the effect of vegetation m 1 3 s c d is empirical dimensionless drag coefficient of vegetation element and varies with vegetation characteristic g is gravitational acceleration m s 2 d is vegetation density per unit width m 1 d a h l a is total frontal area of vegetation in the channel reach length l the eq 5 indicates that the flow velocity and manning s n are affected by vegetation density although considerable researches on the effect of vegetation on manning s n and flow velocity were conducted petryk and bosmajian 1975 reed et al 1995 jin et al 2000 carollo et al 2002 2005 2007 kadlec and wallace 2009 they were mainly focused on the channels with gentle slopes that are generally less than 1 holden et al 2008 the slope gradient has different effects on the soil erosion liu et al 1994 soil erosion rate slowly increases with slope gradient for slope gradient less than 9 and quickly increases for slope gradient greater than 18 the flow velocity is the control factor of soil erosion rate thus the slope gradient may have different effects on flow velocity and manning s n for overland flow on steep slopes with plant basal area cover however limited quantitative information on the effect of vegetation on manning s n and flow velocity on steep slopes was available thus the purposes of this study were to test the suitability of the chezy and manning equations for overland flow under a steep slope with plant basal cover and evaluate quantitative effect of vegetation on manning s n and then on flow velocity a new equation that includes flow velocity flow depth slope gradient and flow basal cover will be developed if these equations do not work well 2 materials and methods 2 1 experimental material the experiments were conducted at the fangshan station of beijing normal university a flume with a length of 5 0 m and a width of 0 37 m was used in the experiments fig 1 the flume consisted of a 2 4 m long covered section with plant basal cover at the bottom of the flume and a 2 3 m long bare section with a layer of sieved sediment at the top of the flume artificial gramineae stems that had a diameter of 2 0 10 3 m and a height of 0 12 m were used to simulate the effect of plant basal cover on flow velocity the artificial gramineae stems have similar flexibility to the natural grass stem and can be reused a layer of sieved sediment was glued onto the bed of the flume when the plant basal cover was 0 the stems were glued onto the bed of the flume the plant basal covers c v used in this study were approximately 0 1 25 2 5 5 10 15 20 25 and 30 the stems were arranged in a random pattern fig 1 the bed slope s of the flume could be adjusted manually from 0 to 60 three slope gradients i e 8 8 17 4 and 25 9 were used three test flow discharges q were applied 0 5 1 0 and 2 0 10 3 m3 s 1 the test flow discharges were selected according to the experiment from zhang et al 2011 to compare the effect of plant basal cover on sediment transport capacity flow discharge was controlled manually by a series of valves installed on a flow diversion box 2 2 experimental measurements flow discharge slope gradient and plant basal cover were adjusted to designated values before the experiments in each run the flow discharge was measured at the flume outlet by a volumetric method the water depth was measured using three level probes across three sections which were located at 0 3 0 6 and 0 9 m above the lower end of the flume in total 12 depths were measured at each cross section the maximum and minimum flow depths were eliminated from the observed values at each section to avoid possible outliers the remaining 10 measured values were averaged for each cross section then the average flow depth from three sections was calculated the velocity was measured with electrolyte tracer method developed by lei et al 2005 2010 the velocity measurement system included a computer installed with specially designed software for control of salt solute injection and sensed data logging an interface unit electric conductivity sensors a salt solute injector lei et al 2005 2010 the quantitative highly saturated salt solution of kcl was injected at a location 1 6 m from the inlet of water the injection of the salt solution into the water flow was controlled by an electrical valve through a computer the 5 sensors were located at 0 2 2 2 m above the bottom end of the flume and the interval was 0 5 m between two sensors the electrical conductivity values measured at the five locations were logged into the computer the designed software was run to calculate the velocities at different locations there were three replicates for each combination of flow discharge slope gradient and stem coverage the three sensors close to the bottom of the flume provided the stable flow velocity values thus the mean velocity was calculated using the velocity from the three sensors water temperatures were monitored using a platinum resistance thermometer the temperature varied from 14 c to 36 c the corresponding kinematic fluid viscosity changed from 7 0 10 7 to 1 2 10 6 m2 s the measured mean flow depth and mean velocity are shown in table 2 2 3 data analysis plant basal cover was calculated using the area of the stems and flume bed 6 c v 100 a s a f where cv is the plant basal cover a s is the area of the stems m2 and a f is the area of the flume bed m2 the froude number fr was calculated using eq 7 7 f r v gh the following statistical parameters were used to evaluate the performance of the flow velocity equations the nash sutcliffe model efficiency nse 8 nse i 1 m o i o av 2 i 1 m p i o i 2 i 1 m o i o av 2 where o i represents the observed values p i represents the predicted values o av is the mean of the observed values and m is the number of observations 9 r 2 i 1 m o i o av p i a v 2 i 1 m o i o av 2 i 1 m p i p av 2 where r 2 is the coefficient of determination and p av is the mean of the predicted value 10 a d j r 2 1 m 1 m k 1 r 2 where a d j r 2 is the adjusted r 2 k is the number of independent variable anova was used to evaluate the influence of the slope gradient flow discharge and plant basal cover on the flow velocity the f statistic value and the corresponding probability p value of a value of f greater than or equal to the observed value were calculated in the anova the variable has significant effects on the flow velocity when its p value is less than or equal to the confidence level the linear regression analysis was conducted to produce standardized regression coefficient α and then determine the relative importance of the slope gradient flow discharge plant basal cover the higher α absolute value the more important influence variable on the flow velocity the relationships between the slope gradient flow depth plant basal cover and flow velocity were analyzed by a nonlinear regression method all statistical analyses were conducted using r software version 3 4 3 3 results and discussion 3 1 effect of plant basal cover on flow pattern the flow pattern can be classified as supercritical critical and subcritical according to the froude number f r fig 2 shows that the f r decreased with increasing plant basal cover f r was greater than 1 0 for 0 plant basal cover close to 1 0 for 1 25 and 2 5 plant basal cover and less than 1 0 for plant basal cover larger than 2 5 the results indicate that the flow pattern varied from supercritical flow to subcritical flow with the increasing plant basal cover the results coincided with those provided by jing et al 2007 and ye et al 2015 the main reason was that the flow velocity decreased and flow depth increased with increasing plant basal cover which motivate the flow pattern to transform from supercritical to subcritical ones 3 2 effect of slope flow discharge and plant basal cover on velocity the mean flow velocity ranged from 0 422 to 0 935 m s for 0 plant basal cover and from 0 063 to 0 504 m s for plant basal cover greater than zero table 2 under the same experiment condition the mean flow velocity values from zhang et al 2011 varied from 0 46 to 1 12 m s for bare bed which were close to those from this study the mean velocity was significantly affected by the slope gradient flow discharge and plant basal cover at p values were close 0 fig 3 table 3 specifically the mean velocity was negatively correlated with plant basal cover at the 0 001 level and positively correlated with flow discharge and slope gradient at the 0 1 level the linear regression results showed that the absolute value of standardized regression coefficients were 0 702 0 213 and 0 188 for plant basal cover slope gradient and flow discharge respectively table 3 it indicates that the effect of plant basal cover slope gradient and flow discharge on flow velocity decreased in turn compared to flow velocity under 0 plant basal cover the flow velocity decreased by more than 60 when the plant basal cover exceeded 5 and by approximately 50 when the plant basal cover exceeded 1 5 fig 4 moreover the decreasing rate tended to remain constant 80 and was almost unaffected by slope gradient and flow discharge when the plant basal cover exceeded 15 the results indicate that the plant stems effectively reduced the flow velocity plant stems increase flow resistance and energy loss which results in a decrease in flow velocity abrahams et al 1994 cao et al 2011 the results provided by zhao et al 2016 showed that plant stems reduced mean flow velocity by approximately 24 46 compared with a bare slope the different rates of decrease may be caused by different arrangement patterns and flexibility of plant stems the rigid plant stems pvc cylinders were arranged in a triangular pattern in the experiments conducted by zhao et al 2016 the flexible plant stems were put in a random pattern in this study in general flow velocity decreased with plant basal cover as a negative exponential function eq 11 table 4 and fig 3 11 v α 1 α 2 e α 3 c v where α 1 α 2 and α 3 are parameters the r 2 for the equations were all greater than 0 90 the f test showed that the equations were significant at the 0 001 level the relationship between flow velocity and plant basal cover was affected by the slope gradient and flow discharge thus the two coefficients α 1 and α 2 in the exponential function eq 11 were also affected by the slope gradient and flow discharge and exhibited an increasing trend with increasing slope gradient and flow discharge table 4 the α 3 in the exponential function eq 11 had no obvious trend with variation of slope gradient and flow discharge 3 3 suitability of manning and chezy equations for overland flow table 5 shows the r 2 values when the h 2 3 s 0 5 variable in manning equation or the h 0 5 s 0 5 variable in the chezy equation was an independent variable and the flow velocity was a dependent variable the regression results in table 5 were significant at the 0 05 level the r 2 values for both equations were all more than 0 90 however the nse obviously varied from 0 985 to 0 789 for manning equation and from 0 954 to 0 000 for chezy equation the nse values for manning equation and chezy were greater than 0 95 for plant basal cover less than 1 25 the nse values were negative for manning equation and close to zero for chezy equation when plant basal cover was greater than 15 the results indicate that manning and chezy equations provided a good estimation of velocity when plant basal cover was less than 2 5 and a bad accuracy for estimating flow velocity on a planted slope the possible reason is that the influence of flow depth on flow resistances at low plant basal cover was much smaller than that at high cover simultaneously flow resistances were not a constant and varied with flow depth on a planted slope however chezy equation had obviously higher r 2 and nse values than the manning equation when plant basal cover was greater than 1 25 it indicates that chezy equation provides better accuracy than the manning equation for overland flow on a planted slope these results are in agreement with those of turner and chanmeesri 1984 and kirstetter et al 2016 manning s n decreased with increasing flow depth for 0 plant basal cover almost remained constant for 1 25 plant basal cover and increased with the increasing flow depth for the plant basal cover larger than 1 25 fig 5 the results indicate that flow resistance mainly from surface resistance decreased with the increasing flow depth when the plant basal cover was zero which coincides with the results from mügler et al 2011 when the plant basal cover was greater than 1 25 the form resistance from plant stems was far greater than surface resistance from the bed and the submerged sides which resulted in increasing flow resistance with increasing flow depth wu 2008 and tsihrintzis and madiedo 2000 also obtained similar results the relationship between manning s n and flow depth for each plant basal cover larger than 1 25 can be expressed as a power function eq 12 12 n a 1 h a 2 the parameters a 1 and a 2 varied with plant basal cover these results indicate that the structure of manning equation is not the most suitable and the manning s n is affect by vegetation it support the results provided by jin et al 2000 nehal et al 2012 and västilä and järvelä 2018 therefore a new relationship between flow velocity and flow depth required further investigation thus nonlinear regression was conducted and two new equations that included plant basal cover flow depth and slope gradient were obtained 13 v 1 0 010 4 633 1 e 0 061 c v 1 668 s 0 5 h 0 063 0 710 e 0 219 c v a d j r 2 0 995 n 81 14 v 1 0 010 4 561 1 e 0 061 c v 1 673 s 0 506 h 0 065 0 712 e 0 218 c v a d j r 2 0 995 n 81 the difference between eqs 13 and 14 was that exponent 0 5 of s in the manning equation was used in eq 13 and was not used in eq 14 the t test showed that the coefficients in eqs 13 and 14 were significant at the 0 001 level the f test showed that both equations were also significant at the 0 001 level but the paired t test showed that the predicted velocity from eqs 13 and 14 were not significant at the 0 05 level thus we suggest that eq 13 was used to calculate the flow velocity because of the same exponent of slope gradient as the chezy equation and manning equation the adj r 2 value of eq 13 were far greater than that from manning equation table 5 the adj r 2 has eliminated the effect of increase of explanatory variable number and is different from the r 2 which must increase when any additional variable is added to the regression simultaneously the eq 13 provided a good prediction accuracy with an nse of 0 995 the predicted values were evenly distributed on both sides of the 1 1 line fig 6 the nse values of eq 13 were obviously greater than those from manning and chezy equations when plant basal cover was greater than zero table 5 therefore these results indicate that the plant basal cover had important contribution for predicting flow velocity and the structure of eq 13 was reasonable if eq 13 was written as the form of the manning equation then the eq 13 can be written as a more general form 15 v 1 a b 1 e 0 061 c v 1 668 h 0 604 0 710 e 0 219 c v s 0 5 h 2 3 16 a n d n a b 1 e 0 061 c v 1 668 h 0 604 0 710 e 0 219 c v where a and b are the coefficients the eq 15 shows that the flow velocity increases with h 2 3 and with s 0 5 the structure of the eq 15 was also suggested by petryk and bosmajian 1975 and jin et al 2000 who obtained eq 5 according to the momentum balance it indicates that the form of eq 15 can be acceptable compared eq 15 with eq 5 we can find that both equations reflected the effect of resistance from boundary friction and from vegetation on the manning s n the difference between the two equations was the different expression of manning s n in eq 5 the roughness from the vegetation was expressed using empirical dimensionless drag coefficient of vegetation element cd and the vegetation density d compared with the parameter of vegetation cover the total frontal area of vegetation is a more difficult parameter to measure and varies with flow depth in eq 15 the plant basal cover doesn t vary with flow depth and is easy to be measured these results indicate that the structure of eq 15 is reasonable and the parameters are easily available but the coefficients including a and b in eq 15 may vary with different vegetation types as manning s n changes with different vegetation types it needs further investigation the manning n in the eq 16 shows that manning s n varied with the flow depth and plant basal cover for c v 0 n a h 0 106 which indicates that the manning s n varied with flow depth for a bare bed the boundary friction was the main source of the flow resistance it supports the opinion of mügler et al 2011 the manning n included the flow resistance from boundary friction and from the plant stem when plant stem cover was greater than 0 the flow resistance from plant stem varied with flow depth and plant stem cover and was the dominant resistance jin et al 2000 tsihrintzis and madiedo 2000 the exponent of flow depth in eq 16 varied from 0 064 to 0 603 when plant basal cover changed from 1 25 to 30 which was different from the exponent value of the eq 4 provided by reed et al 1995 and kadlec and wallace 2009 4 conclusions in this study the effects of plant basal cover slope and flow discharge on flow velocity were investigated the following conclusions can be drawn 1 the manning and chezy equations provided a bad accuracy when plant basal cover was greater than 15 manning s n decreased with the flow depth for 0 plant basal cover and increased with flow depth for plant basal cover greater than 1 25 this indicates that the structure of manning equation is not the most suitable 2 a new equation for calculating manning s n was developed based on plant basal cover and flow depth the new flow velocity equation including plant basal cover provided an accuracy with an nse of 0 995 3 compared to flow velocity under 0 plant basal cover the flow velocity decreased by approximately 50 when the plant basal cover exceeded 1 5 and by more than 60 when the plant basal cover exceeded 5 declaration of competing interest the contribution of the authors is as follows suhua fu apply the project make the experiment planning and write the manuscript hongli mu conduct the experiments baoyuan liu help to guild the experiment and modify the manuscript xianju yu help to conduct the experiments yingna liu help to conduct the experiments acknowledgments the research described in this paper was funded by the state key program of the national natural science foundation of china no 41530858 the national natural science foundation of china no 41571259 the cas light of west china program and national key research and development program of china no 2017yfd0800502 we thank letpub for its linguistic assistance during the preparation of this manuscript 
6254,accurate estimation of mean flow velocity is imperative for the prediction of hydrographs and sediment yield the chezy and manning equations are the most widely used for calculating flow velocity in runoff and erosion models the suitability of both equations for overland flow on bare land has been evaluated and conflicting results were obtained in previous studies the suitability of these equations for overland flow on a planted slope requires further investigation the purposes of this study were to evaluate the suitability of the chezy and manning equations under planted conditions and evaluate quantitative effect of vegetation on manning s n and then on flow velocity flume experiments were conducted at the plant basal cover ranging from 0 to 30 slope gradient varying from8 8 to 25 9 and flow discharge ranging from 0 5 to 2 0 10 3 m3 s 1 the flow depth and flow velocity were measured the coefficient of determination r 2 and nash sutcliffe model efficiency nse were used to compare the performance accuracy of both equations the results showed that both equations provided a bad accuracy when plant basal cover was greater than 15 on a planted slope the nse values were negative for manning equation and close to 0 for chezy equation it indicates that manning and chezy equations were not suitable for the planted slope manning s n decreased with the flow depth for 0 plant basal cover and increased with flow depth for plant basal cover greater than 1 25 thus a new equation for calculating manning s n was developed based on plant basal cover and flow depth and was used in the manning equation the new flow velocity equation including plant basal cover provided satisfactory accuracy with a nse of 0 995 the results indicate that the manning s n was significantly affected by flow depth and plant basal cover keywords flow velocity plant basal cover manning equation chezy equation 1 introduction flow velocity is an important factor that influences hydrological and soil erosion processes accurate estimation of mean flow velocity is imperative for the prediction of hydrographs and sediment yield ali et al 2012 the chezy eq 1 and manning eq 2 equations are the most widely used for calculating flow velocity in runoff and erosion models mügler et al 2011 1 v c h 0 5 s 0 5 2 v 1 n h 2 3 s 0 5 where v is the mean flow velocity m s h is the flow depth m s is the slope gradient m m c is the chezy coefficient m1 2 s n is the manning coefficient m 1 3 s the chezy and manning equations were originally developed for the purpose of sizing canals in the xixth century mouret 1921 the suitability of both equations for overland flow is questionable turner and chanmeesri 1984 abrahams et al 1994 rouhipour et al 1999 smith et al 2007 kirstetter et al 2016 the main reason is that these equations assume numerous restrictive conditions flow is uniform flow occurs parallel to the surface flow width is constant and grain roughness is homogeneous over the wetted perimeter and can be considered as random turner and chanmeesri 1984 smith et al 2007 however none of these conditions is met in overland flow smith et al 2007 nevertheless many researchers have attempted to determine the accuracy of two equations with respect to overland flow table 1 the results differ for bare land rouhipour et al 1999 found that manning equation received stronger support than the chezy equations but both equations were useful the results from mügler et al 2011 showed that manning s model with a water depth dependent roughness coefficient provided the best flow velocity estimation kirstetter et al 2016 found that the manning equation is not suitable for laminar flow in addition some researchers also thought that there was no important influence of slope on flow velocities in rills nearing et al 1997 govers 1992 the plant stem and basal cover effectively decrease flow velocity turner and chanmeesri 1984 weltz et al 1992 gilley and kottwitz 1994 wu 2008 cao et al 2011 fathi moghadam and drikvandi 2012 nehal et al 2012 zhao et al 2016 the main reason is that the plant stem increases flow resistance abrahams et al 1994 gilley and kottwitz 1994 jin et al 2000 nehal et al 2012 yang et al 2017 considerable research has been conducted on the effect of vegetation cover on the flow resistance coefficient the flow resistance coefficient was affected by vegetation type abrahams et al 1994 kadlec and wallace 2009 vegetation density jin et al 2000 xia and nehal 2013 vegetation stem diameter zhao et al 2016 vegetation flexibility xia and nehal 2013 bora and misra 2018 and relative flow depth to the height of vegetation hall and freeman 1994 jin et al 2000 for example abrahams et al 1994 found that the flow resistance coefficient for grassland was higher than that for shrubland jin et al 2000 found that the resistance of vegetation is proportional to the flow depth and the vegetation density yang et al 2017 reported that vegetation has a more obvious influence on flow resistance than surface roughness when the unit discharge is higher than the upper limited critical discharge the results from zhao et al 2016 showed that the flow resistance is affected by the vegetation stem diameter bora and misra 2018 found that rigid vegetation has more resistance to flow than flexible vegetation under similar flow condition and manning s n reduces with the increase in the flexibility of the vegetation for a given depth of flow although these qualitative effect of vegetation on flow resistance were studied it is very important to investigate the quantitative effect of plant basal cover on flow resistance and then on flow velocity for the simulation of hydrological and soil erosion processes some researchers found that manning equation could be rewritten as a more general form liu and hwang 1959 turner and chanmeesri 1984 tsihrintzis and madiedo 2000 kadlec and wallace 2009 as in eq 3 3 v c h x s y where c x and y are parameters when c 1 n x 2 3 and y 1 2 equation 3 becomes manning equation the manning n can be expressed as eq 4 reed et al 1995 kadlec and wallace 2009 4 n γ h 1 2 where γ is a constant which varied from 1 to 4 m1 6 s for most marshes with emergent vegetation kadlec and wallace 2009 reviewed the experiments from wetland and found that the x in eq 3 varied from 0 44 to 2 and y in eq 3 from 0 71 to 1 tsihrintzis and madiedo 2000 thought that x varied from 0 67 turbulent flow to 2 0 laminar flow and y changed from 0 5 turbulent flow to 1 0 laminar flow turner and chanmeesri 1984 found that y was 0 35 based on the experiments on water flowing through dense crops of wheat the results showed that the exponents x and y may vary under different surface conditions in addition manning s n was affected by vegetation jin et al 2000 and varied with vegetation density nehal et al 2012 västilä and järvelä 2018 the flow velocity equation can be written as eq 5 according to the momentum balance petryk and bosmajian 1975 jin et al 2000 5 v h 2 3 s 1 2 n 0 2 cd 2 g d h 4 3 1 2 where n 0 is the roughness coefficient excluding the effect of vegetation m 1 3 s c d is empirical dimensionless drag coefficient of vegetation element and varies with vegetation characteristic g is gravitational acceleration m s 2 d is vegetation density per unit width m 1 d a h l a is total frontal area of vegetation in the channel reach length l the eq 5 indicates that the flow velocity and manning s n are affected by vegetation density although considerable researches on the effect of vegetation on manning s n and flow velocity were conducted petryk and bosmajian 1975 reed et al 1995 jin et al 2000 carollo et al 2002 2005 2007 kadlec and wallace 2009 they were mainly focused on the channels with gentle slopes that are generally less than 1 holden et al 2008 the slope gradient has different effects on the soil erosion liu et al 1994 soil erosion rate slowly increases with slope gradient for slope gradient less than 9 and quickly increases for slope gradient greater than 18 the flow velocity is the control factor of soil erosion rate thus the slope gradient may have different effects on flow velocity and manning s n for overland flow on steep slopes with plant basal area cover however limited quantitative information on the effect of vegetation on manning s n and flow velocity on steep slopes was available thus the purposes of this study were to test the suitability of the chezy and manning equations for overland flow under a steep slope with plant basal cover and evaluate quantitative effect of vegetation on manning s n and then on flow velocity a new equation that includes flow velocity flow depth slope gradient and flow basal cover will be developed if these equations do not work well 2 materials and methods 2 1 experimental material the experiments were conducted at the fangshan station of beijing normal university a flume with a length of 5 0 m and a width of 0 37 m was used in the experiments fig 1 the flume consisted of a 2 4 m long covered section with plant basal cover at the bottom of the flume and a 2 3 m long bare section with a layer of sieved sediment at the top of the flume artificial gramineae stems that had a diameter of 2 0 10 3 m and a height of 0 12 m were used to simulate the effect of plant basal cover on flow velocity the artificial gramineae stems have similar flexibility to the natural grass stem and can be reused a layer of sieved sediment was glued onto the bed of the flume when the plant basal cover was 0 the stems were glued onto the bed of the flume the plant basal covers c v used in this study were approximately 0 1 25 2 5 5 10 15 20 25 and 30 the stems were arranged in a random pattern fig 1 the bed slope s of the flume could be adjusted manually from 0 to 60 three slope gradients i e 8 8 17 4 and 25 9 were used three test flow discharges q were applied 0 5 1 0 and 2 0 10 3 m3 s 1 the test flow discharges were selected according to the experiment from zhang et al 2011 to compare the effect of plant basal cover on sediment transport capacity flow discharge was controlled manually by a series of valves installed on a flow diversion box 2 2 experimental measurements flow discharge slope gradient and plant basal cover were adjusted to designated values before the experiments in each run the flow discharge was measured at the flume outlet by a volumetric method the water depth was measured using three level probes across three sections which were located at 0 3 0 6 and 0 9 m above the lower end of the flume in total 12 depths were measured at each cross section the maximum and minimum flow depths were eliminated from the observed values at each section to avoid possible outliers the remaining 10 measured values were averaged for each cross section then the average flow depth from three sections was calculated the velocity was measured with electrolyte tracer method developed by lei et al 2005 2010 the velocity measurement system included a computer installed with specially designed software for control of salt solute injection and sensed data logging an interface unit electric conductivity sensors a salt solute injector lei et al 2005 2010 the quantitative highly saturated salt solution of kcl was injected at a location 1 6 m from the inlet of water the injection of the salt solution into the water flow was controlled by an electrical valve through a computer the 5 sensors were located at 0 2 2 2 m above the bottom end of the flume and the interval was 0 5 m between two sensors the electrical conductivity values measured at the five locations were logged into the computer the designed software was run to calculate the velocities at different locations there were three replicates for each combination of flow discharge slope gradient and stem coverage the three sensors close to the bottom of the flume provided the stable flow velocity values thus the mean velocity was calculated using the velocity from the three sensors water temperatures were monitored using a platinum resistance thermometer the temperature varied from 14 c to 36 c the corresponding kinematic fluid viscosity changed from 7 0 10 7 to 1 2 10 6 m2 s the measured mean flow depth and mean velocity are shown in table 2 2 3 data analysis plant basal cover was calculated using the area of the stems and flume bed 6 c v 100 a s a f where cv is the plant basal cover a s is the area of the stems m2 and a f is the area of the flume bed m2 the froude number fr was calculated using eq 7 7 f r v gh the following statistical parameters were used to evaluate the performance of the flow velocity equations the nash sutcliffe model efficiency nse 8 nse i 1 m o i o av 2 i 1 m p i o i 2 i 1 m o i o av 2 where o i represents the observed values p i represents the predicted values o av is the mean of the observed values and m is the number of observations 9 r 2 i 1 m o i o av p i a v 2 i 1 m o i o av 2 i 1 m p i p av 2 where r 2 is the coefficient of determination and p av is the mean of the predicted value 10 a d j r 2 1 m 1 m k 1 r 2 where a d j r 2 is the adjusted r 2 k is the number of independent variable anova was used to evaluate the influence of the slope gradient flow discharge and plant basal cover on the flow velocity the f statistic value and the corresponding probability p value of a value of f greater than or equal to the observed value were calculated in the anova the variable has significant effects on the flow velocity when its p value is less than or equal to the confidence level the linear regression analysis was conducted to produce standardized regression coefficient α and then determine the relative importance of the slope gradient flow discharge plant basal cover the higher α absolute value the more important influence variable on the flow velocity the relationships between the slope gradient flow depth plant basal cover and flow velocity were analyzed by a nonlinear regression method all statistical analyses were conducted using r software version 3 4 3 3 results and discussion 3 1 effect of plant basal cover on flow pattern the flow pattern can be classified as supercritical critical and subcritical according to the froude number f r fig 2 shows that the f r decreased with increasing plant basal cover f r was greater than 1 0 for 0 plant basal cover close to 1 0 for 1 25 and 2 5 plant basal cover and less than 1 0 for plant basal cover larger than 2 5 the results indicate that the flow pattern varied from supercritical flow to subcritical flow with the increasing plant basal cover the results coincided with those provided by jing et al 2007 and ye et al 2015 the main reason was that the flow velocity decreased and flow depth increased with increasing plant basal cover which motivate the flow pattern to transform from supercritical to subcritical ones 3 2 effect of slope flow discharge and plant basal cover on velocity the mean flow velocity ranged from 0 422 to 0 935 m s for 0 plant basal cover and from 0 063 to 0 504 m s for plant basal cover greater than zero table 2 under the same experiment condition the mean flow velocity values from zhang et al 2011 varied from 0 46 to 1 12 m s for bare bed which were close to those from this study the mean velocity was significantly affected by the slope gradient flow discharge and plant basal cover at p values were close 0 fig 3 table 3 specifically the mean velocity was negatively correlated with plant basal cover at the 0 001 level and positively correlated with flow discharge and slope gradient at the 0 1 level the linear regression results showed that the absolute value of standardized regression coefficients were 0 702 0 213 and 0 188 for plant basal cover slope gradient and flow discharge respectively table 3 it indicates that the effect of plant basal cover slope gradient and flow discharge on flow velocity decreased in turn compared to flow velocity under 0 plant basal cover the flow velocity decreased by more than 60 when the plant basal cover exceeded 5 and by approximately 50 when the plant basal cover exceeded 1 5 fig 4 moreover the decreasing rate tended to remain constant 80 and was almost unaffected by slope gradient and flow discharge when the plant basal cover exceeded 15 the results indicate that the plant stems effectively reduced the flow velocity plant stems increase flow resistance and energy loss which results in a decrease in flow velocity abrahams et al 1994 cao et al 2011 the results provided by zhao et al 2016 showed that plant stems reduced mean flow velocity by approximately 24 46 compared with a bare slope the different rates of decrease may be caused by different arrangement patterns and flexibility of plant stems the rigid plant stems pvc cylinders were arranged in a triangular pattern in the experiments conducted by zhao et al 2016 the flexible plant stems were put in a random pattern in this study in general flow velocity decreased with plant basal cover as a negative exponential function eq 11 table 4 and fig 3 11 v α 1 α 2 e α 3 c v where α 1 α 2 and α 3 are parameters the r 2 for the equations were all greater than 0 90 the f test showed that the equations were significant at the 0 001 level the relationship between flow velocity and plant basal cover was affected by the slope gradient and flow discharge thus the two coefficients α 1 and α 2 in the exponential function eq 11 were also affected by the slope gradient and flow discharge and exhibited an increasing trend with increasing slope gradient and flow discharge table 4 the α 3 in the exponential function eq 11 had no obvious trend with variation of slope gradient and flow discharge 3 3 suitability of manning and chezy equations for overland flow table 5 shows the r 2 values when the h 2 3 s 0 5 variable in manning equation or the h 0 5 s 0 5 variable in the chezy equation was an independent variable and the flow velocity was a dependent variable the regression results in table 5 were significant at the 0 05 level the r 2 values for both equations were all more than 0 90 however the nse obviously varied from 0 985 to 0 789 for manning equation and from 0 954 to 0 000 for chezy equation the nse values for manning equation and chezy were greater than 0 95 for plant basal cover less than 1 25 the nse values were negative for manning equation and close to zero for chezy equation when plant basal cover was greater than 15 the results indicate that manning and chezy equations provided a good estimation of velocity when plant basal cover was less than 2 5 and a bad accuracy for estimating flow velocity on a planted slope the possible reason is that the influence of flow depth on flow resistances at low plant basal cover was much smaller than that at high cover simultaneously flow resistances were not a constant and varied with flow depth on a planted slope however chezy equation had obviously higher r 2 and nse values than the manning equation when plant basal cover was greater than 1 25 it indicates that chezy equation provides better accuracy than the manning equation for overland flow on a planted slope these results are in agreement with those of turner and chanmeesri 1984 and kirstetter et al 2016 manning s n decreased with increasing flow depth for 0 plant basal cover almost remained constant for 1 25 plant basal cover and increased with the increasing flow depth for the plant basal cover larger than 1 25 fig 5 the results indicate that flow resistance mainly from surface resistance decreased with the increasing flow depth when the plant basal cover was zero which coincides with the results from mügler et al 2011 when the plant basal cover was greater than 1 25 the form resistance from plant stems was far greater than surface resistance from the bed and the submerged sides which resulted in increasing flow resistance with increasing flow depth wu 2008 and tsihrintzis and madiedo 2000 also obtained similar results the relationship between manning s n and flow depth for each plant basal cover larger than 1 25 can be expressed as a power function eq 12 12 n a 1 h a 2 the parameters a 1 and a 2 varied with plant basal cover these results indicate that the structure of manning equation is not the most suitable and the manning s n is affect by vegetation it support the results provided by jin et al 2000 nehal et al 2012 and västilä and järvelä 2018 therefore a new relationship between flow velocity and flow depth required further investigation thus nonlinear regression was conducted and two new equations that included plant basal cover flow depth and slope gradient were obtained 13 v 1 0 010 4 633 1 e 0 061 c v 1 668 s 0 5 h 0 063 0 710 e 0 219 c v a d j r 2 0 995 n 81 14 v 1 0 010 4 561 1 e 0 061 c v 1 673 s 0 506 h 0 065 0 712 e 0 218 c v a d j r 2 0 995 n 81 the difference between eqs 13 and 14 was that exponent 0 5 of s in the manning equation was used in eq 13 and was not used in eq 14 the t test showed that the coefficients in eqs 13 and 14 were significant at the 0 001 level the f test showed that both equations were also significant at the 0 001 level but the paired t test showed that the predicted velocity from eqs 13 and 14 were not significant at the 0 05 level thus we suggest that eq 13 was used to calculate the flow velocity because of the same exponent of slope gradient as the chezy equation and manning equation the adj r 2 value of eq 13 were far greater than that from manning equation table 5 the adj r 2 has eliminated the effect of increase of explanatory variable number and is different from the r 2 which must increase when any additional variable is added to the regression simultaneously the eq 13 provided a good prediction accuracy with an nse of 0 995 the predicted values were evenly distributed on both sides of the 1 1 line fig 6 the nse values of eq 13 were obviously greater than those from manning and chezy equations when plant basal cover was greater than zero table 5 therefore these results indicate that the plant basal cover had important contribution for predicting flow velocity and the structure of eq 13 was reasonable if eq 13 was written as the form of the manning equation then the eq 13 can be written as a more general form 15 v 1 a b 1 e 0 061 c v 1 668 h 0 604 0 710 e 0 219 c v s 0 5 h 2 3 16 a n d n a b 1 e 0 061 c v 1 668 h 0 604 0 710 e 0 219 c v where a and b are the coefficients the eq 15 shows that the flow velocity increases with h 2 3 and with s 0 5 the structure of the eq 15 was also suggested by petryk and bosmajian 1975 and jin et al 2000 who obtained eq 5 according to the momentum balance it indicates that the form of eq 15 can be acceptable compared eq 15 with eq 5 we can find that both equations reflected the effect of resistance from boundary friction and from vegetation on the manning s n the difference between the two equations was the different expression of manning s n in eq 5 the roughness from the vegetation was expressed using empirical dimensionless drag coefficient of vegetation element cd and the vegetation density d compared with the parameter of vegetation cover the total frontal area of vegetation is a more difficult parameter to measure and varies with flow depth in eq 15 the plant basal cover doesn t vary with flow depth and is easy to be measured these results indicate that the structure of eq 15 is reasonable and the parameters are easily available but the coefficients including a and b in eq 15 may vary with different vegetation types as manning s n changes with different vegetation types it needs further investigation the manning n in the eq 16 shows that manning s n varied with the flow depth and plant basal cover for c v 0 n a h 0 106 which indicates that the manning s n varied with flow depth for a bare bed the boundary friction was the main source of the flow resistance it supports the opinion of mügler et al 2011 the manning n included the flow resistance from boundary friction and from the plant stem when plant stem cover was greater than 0 the flow resistance from plant stem varied with flow depth and plant stem cover and was the dominant resistance jin et al 2000 tsihrintzis and madiedo 2000 the exponent of flow depth in eq 16 varied from 0 064 to 0 603 when plant basal cover changed from 1 25 to 30 which was different from the exponent value of the eq 4 provided by reed et al 1995 and kadlec and wallace 2009 4 conclusions in this study the effects of plant basal cover slope and flow discharge on flow velocity were investigated the following conclusions can be drawn 1 the manning and chezy equations provided a bad accuracy when plant basal cover was greater than 15 manning s n decreased with the flow depth for 0 plant basal cover and increased with flow depth for plant basal cover greater than 1 25 this indicates that the structure of manning equation is not the most suitable 2 a new equation for calculating manning s n was developed based on plant basal cover and flow depth the new flow velocity equation including plant basal cover provided an accuracy with an nse of 0 995 3 compared to flow velocity under 0 plant basal cover the flow velocity decreased by approximately 50 when the plant basal cover exceeded 1 5 and by more than 60 when the plant basal cover exceeded 5 declaration of competing interest the contribution of the authors is as follows suhua fu apply the project make the experiment planning and write the manuscript hongli mu conduct the experiments baoyuan liu help to guild the experiment and modify the manuscript xianju yu help to conduct the experiments yingna liu help to conduct the experiments acknowledgments the research described in this paper was funded by the state key program of the national natural science foundation of china no 41530858 the national natural science foundation of china no 41571259 the cas light of west china program and national key research and development program of china no 2017yfd0800502 we thank letpub for its linguistic assistance during the preparation of this manuscript 
