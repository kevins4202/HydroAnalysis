index,text
25680,across the world wildlife must coexist with humans in modified and increasingly fragmented landscapes but balancing the competing land use objectives of economic production and conservation is challenging multi objective optimisation and spatial conservation prioritisation can inform land use planning but have not yet explicitly accounted for the way species access multiple resources at different locations in a landscape here we demonstrate a novel approach for conservation prioritisation that accounts for the spatial distribution of different resources as well as a species movements this suite of tools and models identifies pareto optimal solutions to competing objectives of economic production on the one hand with conserving a species food drink and shelter requirements and movement corridors on the other we demonstrate the broader functionality of these tools using a case study with competing objectives of clearing land for mining versus conservation of a vulnerable endemic species keywords land use mobile species multi objective optimisation pareto optimality spatial conservation prioritisation 1 introduction balancing trade offs in scenarios where economic objectives compete with conservation objectives is complicated especially in land use contexts moilanen et al 2011 approaches like multi objective optimisation otherwise known as multi criteria optimisation or pareto optimisation is a commonly used technique in environmental management scenarios such as waste and water management and land use allocation kaim et al 2018 newland et al 2018 verstegen et al 2017 xevi and khan 2005 xi et al 2010 decisions in situations with competing objectives can be aided by identifying the pareto frontier which includes all options where the improvement of one objective function can only occur at the detriment of another objective function lotov and miettinen 2008 verstegen et al 2017 for example the more land that is cleared across a region for production such as agriculture or mineral extraction the less capacity that region has to support biodiversity the important objectives of land use production and biodiversity conservation can come into conflict in a complex landscape and applying pareto optimisation to identify the set of all land allocation options that maximise biodiversity conservation for any given level of land use production or vice versa will help understand and evaluate trade offs and thus aid in managers decision making multi objective optimisation techniques are particularly appealing for spatial conservation prioritisation herzig et al 2018 kaim et al 2018 spatial conservation prioritisation scp henceforth is a set of computational tools designed for the efficient and optimal spatial allocation of priority areas for conservation actions el gabbas et al 2020 scp is most appropriate when managing regions for the protections of species and ecosystems and can resolve conflicts by benefiting both conservation and socioeconomic outcomes huang et al 2011 kaim et al 2018 in recent decades land use optimisation and spatial conservation prioritisation techniques have seen increasing implementation for management and regulation jones et al 2016 kennedy et al 2008 moilanen et al 2020 venegas li et al 2018 indeed the potential for multi objective optimisation and scp to inform decisions makes it an appealing choice for environmental management but the implementation of these methods for land use allocation can be complex optimisation and prioritisation require spatial data representing biodiversity ecosystem services stakeholder interests and economic factors as well as quantitative models that assess trade offs and allocate land use accordingly castrejón and charles 2020 jumin et al 2018 recently the potential for conservation offsetting has been included in scp tools and if applied effectively offsets have the potential to broaden the possibilities available to managers when faced with complex decisions bull et al 2013 moilanen et al 2020 several software packages tools and custom algorithms for scp have been developed including marxan zonation and priotizer ball et al 2009 hanson et al 2020 moilanen et al 2005 watts et al 2009 in general conservation prioritisation informs land use with spatial data inputs such as species distribution models sdm habitat classification environmental gradients ecosystem services remotely sensed and proxy indices or dimensionally reduced data that represent essential biodiversity variables or ecosystem services jetz et al 2019 jumin et al 2018 shen et al 2020 studwell et al 2017 however the dynamic distributions and behaviours of species particularly mobile or migratory species do not necessarily conform to probabilistic and correlative model outputs classifications or simplified measurements a model may predict high occurrence probability or habitat suitability for a species in certain areas with a given parameter set yet that species may never truly inhabit or move through that space otherwise referred to as commission errors di marco et al 2017 additionally scp models often assume locations of important habitat or critical ecosystem functions and services from classifications models or dimensional reductions el gabbas et al 2020 kaim et al 2018 kennedy et al 2008 of course these methods and data inputs are necessary to simplify the complexity of the natural world but they do not fully capture the complexity of animal behaviours and their relationship with the environment doherty and driscoll 2018 we explored a novel method of evaluating a species interaction with the environment through evaluating the spatial configuration of functional resources and habitat that supports their persistence mastrantonis et al 2019 nicholson et al 2006 the concept of functional resources relates to the set of resources that a species requires to function and complete its life history dennis et al 2003 these resources should be included in landscape evaluations as they are vital for a species persistence and this is especially true for species that are mobile camacho et al 2014 webb et al 2017 effective scp for mobile or migratory species is challenging as their movement corridors migratory pathways and dynamic resource dependencies need to be accounted for in conservation planning but methods to do so are still in their infancy runge et al 2014 evaluating species environment relationships in a manner that accounts for their dependence on the spatial configuration of habitats resources and movement dynamics is a complex task it requires a comprehensive autecological understanding of the focal species as well as detailed data representing how and where the species utilises habitats and resources moreover in multiple use landscapes additional data are required to represent locations that hold sociocultural environmental or economic value for humans and using decision support tools determine priorities for conservation while maintaining the status quo here we introduce a novel decision tool framework in the r environment that prioritises conservation based on the spatial distribution of resources habitats movements and importantly the interactions of all these factors for a species specifically we test and demonstrate this framework for a case study that represents a mining landscape in western australia using the forest red tailed black cockatoo calyptorhynchus banksii naso as our model species the decision tools presented here apply pareto optimisation to inform decisions about land use allocation when species conservation and economic objectives compete in space 2 methods 2 1 study landscape and focal species the study area used for the proof of concept of these tools and models was the northern jarrah forest near perth in south western western australia an area spanning approximately 50 km from east to west and 175 km from north to south fig 1 this open forest is dominated by jarrah eucalyptus marginata and marri corymbia calophylla with bull banksia banksia grandis sheoak allocasuarina fraseriana and snottygobble persoonia longifolia as common mid storey species and a diverse understorey of shrubs subshrubs and herbs bell and heddle 1989 the forest encompasses the darling range a plateau with deep highly weathered lateritic profiles containing extensive but spatially discontinuous bauxite deposits alcoa of australia ltd alcoa has been mining bauxite in the northern jarrah forest since 1963 typically in discrete pods averaging around 30ha in size within a mosaic of unmined forest fig 1 and undertaking a progressive rehabilitation program to restore a jarrah forest ecosystem grant and koch 2007 koch 2007 koch and hobbs 2007 the forest red tailed black cockatoo calyptorhynchus banksii naso henceforth frtbc used as our model species on which to develop the decision tools is endemic to south western western australia the species is listed as vulnerable chapman 2008 due to a contraction of about 30 from its former range in response to climate change restrictions to food and water availability and habitat loss cameron 2007 the frtbc is a monogamous hollow nester primarily nesting within the study landscape in mature marri trees on average 20 m tall and 220 years old johnstone et al 2013a nest sites are typically clustered likely due to social interactions and the proximity of nest sites to suitable food and water resources johnstone et al 2013b male cockatoos will travel long distances up to 20 km over multiple trips per day from their nests foraging for food and water for incubating or brooding females johnstone et al 2013b the diet of frtbc primarily consists of fruit from jarrah and marri trees johnstone et al 2017 fruit production of these species is complex and often dependant on stand density competition and landscape position as well as temporal factors such as climate biggs et al 2011 johnstone et al 2013a 2 2 geospatial data simulation for the purposes of this study we chose to demonstrate the proof of concept of the decision tools using geospatially simulated data due to a number of constraints on input data described in the following sections and a desire to demonstrate the application of movement corridor concepts even though these are arguably not critical for frtbc per se nevertheless the study is based on various research of frtbc ecology in the study landscape johnstone et al 2013a b 2017 mastrantonis et al 2019 providing a realistic application 2 2 1 mining extent we created a 300 300 90000 cells dimensional matrix to represent our study landscape where each cell would nominally represent a 100 100 m area on the ground no geographic or projected coordinate reference system is required for the simulated data but our models and tools can operate with any appropriate projected system to represent areas with bauxite that could potentially be cleared for mining we used a spherical variogram model from the gstat package pebesma 2004 to generate a spatially autocorrelated response across the 300 300 matrix applied an image convolution with a 5 5 window and modal function across the matrix and rounded all values to 0 or 1 where values of 0 represented areas of no mining and values of 1 represented areas to be mined fig 2 a 2 2 2 vegetation and food distribution we generated an additional spatially autocorrelated response across the same 300 300 matrix described above to represent vegetation productivity or food density distribution in the case of the frtbc fruit availability could not be adequately assessed in the field and so we relied on sentinel 2 surface spectral reflectance ndvi data as a proxy for vegetation and frtbc food availability leveau et al 2018 pettorelli et al 2006 2011 studds and marra 2011 2 2 3 nest site and water resource simulation we generated 196 locations of potential nesting sites for the frtbc by assuming that more productive vegetation would have a higher probability of containing a nest site and thus weighting the probability of a cell being sampled by the corresponding value of the vegetation productivity matrix fig 2b 196 nests were generated as this corresponds to the number of nests located during surveys additionally we performed a k means clustering n 25 of the nesting sites to generate locations of limiting resources water points in the case of the frtbc fig 2b water is a vital resource for some australian birds hawkins et al 2005 including the frtbc johnstone et al 2013a although identifying such water points across the study landscape is a challenging task 2 2 4 landscape permeability we included functionality in our decision tools to evaluate the movement corridors of species between habitats and resources through least cost pathing we describe this functionality in more detail in a later section however a requirement for this analysis is an appropriate measure of matrix permeability for the landscape though this functionality could be argued to be unnecessary in the case of the frtbc we demonstrate its functionality here using vegetation productivity as a permeability surface and we have assumed that higher productivity results in greater landscape permeability fig 2b 2 3 evaluating spatial interactions and conservation value the core of our novel approach is a method that attributes a value to each cell in the landscape based on the distributions of the resources and habitats a species depends on in the case of the frtbc these include relatively sparse water points and nesting sites and more widely distributed spatially varying food resources an approximation of foraging distance in appropriate map units for the model species can also be specified and in the case of the frtbc this was set to a circle with a radius of 75 map units in reality cockatoos will travel up to 20 km over multiple trips per day from their nests in search of food and water r johnstone et al 2013 though this is not a requirement of our approach specifying a foraging range will usually be important as animals are more likely to access resources within close proximity to maximise net energy gains i e optimal foraging theory pyke 1984 stephens and krebs 2019 first a relative value of the food within the foraging range of each of the 196 nesting sites was determined using 1 z p f i n f a f d f f i n f 1 d f where z p represents the relative value of the food within the foraging range of nesting site p f is the set of indices of all cells within the foraging range of nesting site p a f is the food density value of cell f and d f is the euclidean distance between cell f and point z p thus nests closely surrounded with cells of greater food densities will have greater values than nest sites surrounded by cells with lower densities or where higher food density cells are more distant fig 3 next we represented the way that water resources contribute or limit the value of nest sites each water resource within the foraging distance of a nest site is assigned a weight k based on a logistic distance decay kernel fig 3 carlos et al 2010 evans and shen 2021 2 k 1 1 e d l a where d is the distance of the resource to the nest site l is the location of the inflection point of the logistic curve and a is the inverse of the steepness of the curve fig 3 for each nest site we then calculate a value v p based on the number and proximity of all water resource points within the foraging range using 3 v p 1 f i n f 1 k f where v p is the relative water value of nesting site p f is the set of indices for all water resources within the foraging range and k f is the weighting for water resource f calculated from equation 2 we then calculated the total value for each nest site using 4 t p v p z p and then the value of the entire landscape is calculated by summing the values of all nest sites 5 l t p as the values for each nest site is dependent on the distribution of both the food densities and water resources in nearby cells we evaluate the contribution of each cell in the landscape to the total landscape value by removing that cell and recomputing the value of the entire landscape 5 if the value of the landscape with a cell c removed is c then the contribution of that cell to the total landscape value is then 6 m c l c this results in a matrix of values one for each cell we then applied a smoothing algorithm to this matrix of values where the new value of cell is calculated as the weighted average of all cells within a specified buffer 50 units in our case using a weighting that decays linearly with distance this buffering process results in contiguous hotspots within the landscape that hold the greatest value for the model species based on the density of nests and water resources this is particularly relevant for the frtbc as alcoa identifies in pre mine surveys and protects with a static buffer all nest sites and important habitats and applying a distance based buffer in conjunction with conservation prioritisation could improve both economic and environmental outcomes 2 4 species movement and corridors as part of these decision tools we have included the functionality to model the movement of species between habitats and resources and include movement pathways into our scp outcomes currently our tools for modelling animal movement are dependent on the gdistance package etten and sousa 2020 the gdistance package provides the functionality for least cost pathing and constrained random walks though heterogenous landscapes we leverage the least cost pathing functionality of the gdistance package to assess a species movement corridors between habitats and resources and this is achieved through an iterative process of weighted random sampling from locations in the landscape based on conservation value prior to sampling a transition layer is created from a measure of landscape permeability where movement is possible in orthogonal and diagonal directions i e a kings graph for the frtbc case study we have simply used the simulated vegetation productivity matrix as a permeability surface where higher productivity cells result in less resistance to movement through the cells though landscape permeability is not necessarily relevant for the frtbc we have included it here to showcase its functionality for species where permeability would be relevant for each iteration an origin and destination point are sampled from the nest sites and water resource locations and an appropriate path through the transition layer is determined with a minor random deviation from the shortest possible path to satisfy optimal foraging theory the least cost pathing results in a density value between 0 and 1 for cells that link the origin and destination points after a predetermined number of iterations n 196 so that every nest site has a chance of being sampled the results are spatially aggregated to determine the highest density movement corridors between nests and and these corridors were added to the results of equation 6 for scp analysis 2 5 improving landscape conservation outcomes the configuration and functional connectivity of habitats and resources plays an important role in landscape ecological processes and this is especially true for the persistence of mobile species carvalho et al 2016 mastrantonis et al 2019 in addition the introduction and management of accessible supplementary resources has been shown to influence the ranges and behaviour of mobile species while conservation offsets could alter land use allocation in conservation prioritisation models moilanen et al 2020 morris et al 2006 plummer et al 2015 another strength of our approach is that it can help determine locations for conservation offsets i e habitats or resources for a species for the frtbc this involved determining the best locations for adding artificial water points to the landscape this can be done by searching the whole landscape for each cell in the landscape add a new water point to that cell and then recalculate the landscape value and finally select the cell that adds the greatest value this can then be repeated for as many new water points as desired we also developed a heuristic search algorithm that places a water point at a random coordinate in the landscape and assesses the set of nearest neighbouring points to maximise equation 5 the resource point then moves to the coordinate that maximises equation 5 and the process continues if a point does not add value within any given set the search neighbourhood is expanded additionally if a coordinate in the expanded search neighbourhood set fails to improve the result of equation 5 the search neighbourhood is expanded a second time if these three neighbourhood sets fail to improve the results of equation 5 another random coordinate is sampled and the process repeats until a specified number of resources points are allocated for a landscape with only 90000 cells a heuristic approach is far from ideal as every single cell can easily be assessed for potential offsets however for cases where the study area spans regional or world regional extents performing a full search of the landscape would become computationally demanding and time intensive and a heuristic approach that finds a good but imperfect one is acceptable 2 6 the pareto frontier land use allocation conservation and economic enterprise often come into conflict especially in spatial management and multi objective contexts czech 2008 faleiro et al 2013 nelson et al 2009 and debate is ongoing on how to best conserve nature while maintaining socioeconomic wellbeing hobbs et al 2014 determining the best trade offs between conservation and economic activity is complex and challenging and in this case we seek to achieve the best trade offs between these two contrasting objectives using pareto optimality we have developed a tool that visualises the pareto frontier of optimal trade offs between the economic anthropogenic objective clearing for mining in this case and the conservation objective landscape conservation value and allows a user to select their desired outcomes across the frontier to generate optimal land use outcomes and allocations this tool starts by finding the cell in the potential mining extent with the least conservation value removes it from the set of unmined cells and adds it to a new set of mined cells it then continues and finds the next remaining cell in the potential mining extent with the least conservation value removes it from the set of unmined cells and adds it to the set of mined cells this iterative process continues until all cells in the potential mining extent have been removed from the set of conserved cells at each iteration the conservation value of the whole landscape is plotted against the total number of mined cells to form the pareto frontier branke et al 2008 which displays the nature of trade offs between mining production and landscape conservation value the corresponding land use allocation can be mapped spatially fig 5 3 results discussion the landscape conservation value of the frtbc based on the simulated spatial interactions and distribution of their functional resources is shown in fig 4 notably regions that hold dense nesting sites surrounded by productive vegetation and in close proximity to water resources contribute the highest conservation values to the landscape conversely regions in the landscape with relatively low values reflect limited number of water points and low density of productive vegetation within foraging range of nest sites the least cost pathing analysis shows the likely movement corridors between nests and resources in the landscape fig 4 much of the high value conservation regions and corridors in the landscape intersect with our simulated extent of mining fig 2 in this scenario we observe that the entire extent of clearing will result in half of the conservation value of the landscape being lost but this loss may be offset by supplementing water into the landscape fig 5 however through mapping the optimised trade offs between the competing objectives of mining and conservation with the pareto frontier we can observe that it is possible for example to clear 20 of the maximum mining potential while impacting less than 5 of the conservation value of the landscape fig 5 conversely clearing 80 of the maximum mining potential is predicted to result in a reduction of approx 30 of the conservation value with two other intermediate options also shown fig 5 the application of pareto optimality for scp has efficiently allocated land for mining while simultaneously conserving the most appropriate regions of the landscape for biodiversity the results of the heuristic search for locations to establish water resources indicated that adding 10 new water points into the landscape would increase the total conservation value of the system by 47 fig 6 these additional points in the landscape shift the pareto frontier outwards for a greater extent of mining with a proportionally lower net loss in conservation value across the landscape this shift in the pareto frontier demonstrates that adding resources into the landscape can improve conservation outcomes while maintaining or improving economic objectives fig 6 3 1 computational efficiency in terms of computational efficiency our landscape evaluations complete within minutes due to our c functions called through the r environment using wrapping protocols from the rccp package eddelbuettel 2013 our pareto optimality functions are also computationally efficient and generally complete within minutes depending on the intersecting extent for conflicting objectives our heuristic search function completes in minutes for 10 additional resources but this time will linearly increase with increasing numbers of desired additional resources the least cost pathing analysis can take several hours depending on the number of habitats and resources within the landscape and because we are currently relying on the gdistance package for pathing we cannot improve the computational efficiency of these functions yet the option to parallelise these function remains 4 conclusion we developed a novel approach for identifying optimal trade offs between conflicting economic and conservation land use objectives for species that require multiple resources widely distributed across a landscape we applied this approach to a real world case study identifying optimal trade offs between bauxite mining and conservation of a threatened mobile species accounting for its dependence on widely distributed food and water resources and nesting sites we demonstrated our approach on simulated data with similar characteristics to the real world data it is important to consider that even though we used relatively complex spatial interactions among multiple resources to model landscape conservation value for a species it is entirely possible to substitute the outputs of these spatial interactions with typical ecological indices such as sdm or landscape metrics hesselbarth et al 2019 as the scripts used here are provided in an open source repository and were initially developed to work with real world data the applicability and functionality of these tools is broad even without using our relatively complex evaluation methods our heuristic search tools for determining locations for the provision of additional resources could be useful in a range of conservation planning scenarios and potentially inform management decisions for the benefit of both environmental and economic interests lastly the incorporation of least cost pathing into scp has broad relevance and applicability for managing mobile species and the movement corridors they depend on hofmann et al 2021 the conservation of mobile species is a significant challenge and protecting their habitats resources and movement pathways over regional and world regional scales is critical in the face of environmental fragmentation the tools presented here may provide a steppingstone to effectively addressing these challenges software data and video availability code and data available at github https github com stanleymastrantonis rezone developers stanley mastrantonis year first available 2020 programming languages r c package requirements raster rcpp and gdistance optional packages contact email stanley mastrantonis research uwa edu au declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the australian research council arc linkage grant lp160100177 the department of biodiversity conservation and attractions and alcoa of australia 
25680,across the world wildlife must coexist with humans in modified and increasingly fragmented landscapes but balancing the competing land use objectives of economic production and conservation is challenging multi objective optimisation and spatial conservation prioritisation can inform land use planning but have not yet explicitly accounted for the way species access multiple resources at different locations in a landscape here we demonstrate a novel approach for conservation prioritisation that accounts for the spatial distribution of different resources as well as a species movements this suite of tools and models identifies pareto optimal solutions to competing objectives of economic production on the one hand with conserving a species food drink and shelter requirements and movement corridors on the other we demonstrate the broader functionality of these tools using a case study with competing objectives of clearing land for mining versus conservation of a vulnerable endemic species keywords land use mobile species multi objective optimisation pareto optimality spatial conservation prioritisation 1 introduction balancing trade offs in scenarios where economic objectives compete with conservation objectives is complicated especially in land use contexts moilanen et al 2011 approaches like multi objective optimisation otherwise known as multi criteria optimisation or pareto optimisation is a commonly used technique in environmental management scenarios such as waste and water management and land use allocation kaim et al 2018 newland et al 2018 verstegen et al 2017 xevi and khan 2005 xi et al 2010 decisions in situations with competing objectives can be aided by identifying the pareto frontier which includes all options where the improvement of one objective function can only occur at the detriment of another objective function lotov and miettinen 2008 verstegen et al 2017 for example the more land that is cleared across a region for production such as agriculture or mineral extraction the less capacity that region has to support biodiversity the important objectives of land use production and biodiversity conservation can come into conflict in a complex landscape and applying pareto optimisation to identify the set of all land allocation options that maximise biodiversity conservation for any given level of land use production or vice versa will help understand and evaluate trade offs and thus aid in managers decision making multi objective optimisation techniques are particularly appealing for spatial conservation prioritisation herzig et al 2018 kaim et al 2018 spatial conservation prioritisation scp henceforth is a set of computational tools designed for the efficient and optimal spatial allocation of priority areas for conservation actions el gabbas et al 2020 scp is most appropriate when managing regions for the protections of species and ecosystems and can resolve conflicts by benefiting both conservation and socioeconomic outcomes huang et al 2011 kaim et al 2018 in recent decades land use optimisation and spatial conservation prioritisation techniques have seen increasing implementation for management and regulation jones et al 2016 kennedy et al 2008 moilanen et al 2020 venegas li et al 2018 indeed the potential for multi objective optimisation and scp to inform decisions makes it an appealing choice for environmental management but the implementation of these methods for land use allocation can be complex optimisation and prioritisation require spatial data representing biodiversity ecosystem services stakeholder interests and economic factors as well as quantitative models that assess trade offs and allocate land use accordingly castrejón and charles 2020 jumin et al 2018 recently the potential for conservation offsetting has been included in scp tools and if applied effectively offsets have the potential to broaden the possibilities available to managers when faced with complex decisions bull et al 2013 moilanen et al 2020 several software packages tools and custom algorithms for scp have been developed including marxan zonation and priotizer ball et al 2009 hanson et al 2020 moilanen et al 2005 watts et al 2009 in general conservation prioritisation informs land use with spatial data inputs such as species distribution models sdm habitat classification environmental gradients ecosystem services remotely sensed and proxy indices or dimensionally reduced data that represent essential biodiversity variables or ecosystem services jetz et al 2019 jumin et al 2018 shen et al 2020 studwell et al 2017 however the dynamic distributions and behaviours of species particularly mobile or migratory species do not necessarily conform to probabilistic and correlative model outputs classifications or simplified measurements a model may predict high occurrence probability or habitat suitability for a species in certain areas with a given parameter set yet that species may never truly inhabit or move through that space otherwise referred to as commission errors di marco et al 2017 additionally scp models often assume locations of important habitat or critical ecosystem functions and services from classifications models or dimensional reductions el gabbas et al 2020 kaim et al 2018 kennedy et al 2008 of course these methods and data inputs are necessary to simplify the complexity of the natural world but they do not fully capture the complexity of animal behaviours and their relationship with the environment doherty and driscoll 2018 we explored a novel method of evaluating a species interaction with the environment through evaluating the spatial configuration of functional resources and habitat that supports their persistence mastrantonis et al 2019 nicholson et al 2006 the concept of functional resources relates to the set of resources that a species requires to function and complete its life history dennis et al 2003 these resources should be included in landscape evaluations as they are vital for a species persistence and this is especially true for species that are mobile camacho et al 2014 webb et al 2017 effective scp for mobile or migratory species is challenging as their movement corridors migratory pathways and dynamic resource dependencies need to be accounted for in conservation planning but methods to do so are still in their infancy runge et al 2014 evaluating species environment relationships in a manner that accounts for their dependence on the spatial configuration of habitats resources and movement dynamics is a complex task it requires a comprehensive autecological understanding of the focal species as well as detailed data representing how and where the species utilises habitats and resources moreover in multiple use landscapes additional data are required to represent locations that hold sociocultural environmental or economic value for humans and using decision support tools determine priorities for conservation while maintaining the status quo here we introduce a novel decision tool framework in the r environment that prioritises conservation based on the spatial distribution of resources habitats movements and importantly the interactions of all these factors for a species specifically we test and demonstrate this framework for a case study that represents a mining landscape in western australia using the forest red tailed black cockatoo calyptorhynchus banksii naso as our model species the decision tools presented here apply pareto optimisation to inform decisions about land use allocation when species conservation and economic objectives compete in space 2 methods 2 1 study landscape and focal species the study area used for the proof of concept of these tools and models was the northern jarrah forest near perth in south western western australia an area spanning approximately 50 km from east to west and 175 km from north to south fig 1 this open forest is dominated by jarrah eucalyptus marginata and marri corymbia calophylla with bull banksia banksia grandis sheoak allocasuarina fraseriana and snottygobble persoonia longifolia as common mid storey species and a diverse understorey of shrubs subshrubs and herbs bell and heddle 1989 the forest encompasses the darling range a plateau with deep highly weathered lateritic profiles containing extensive but spatially discontinuous bauxite deposits alcoa of australia ltd alcoa has been mining bauxite in the northern jarrah forest since 1963 typically in discrete pods averaging around 30ha in size within a mosaic of unmined forest fig 1 and undertaking a progressive rehabilitation program to restore a jarrah forest ecosystem grant and koch 2007 koch 2007 koch and hobbs 2007 the forest red tailed black cockatoo calyptorhynchus banksii naso henceforth frtbc used as our model species on which to develop the decision tools is endemic to south western western australia the species is listed as vulnerable chapman 2008 due to a contraction of about 30 from its former range in response to climate change restrictions to food and water availability and habitat loss cameron 2007 the frtbc is a monogamous hollow nester primarily nesting within the study landscape in mature marri trees on average 20 m tall and 220 years old johnstone et al 2013a nest sites are typically clustered likely due to social interactions and the proximity of nest sites to suitable food and water resources johnstone et al 2013b male cockatoos will travel long distances up to 20 km over multiple trips per day from their nests foraging for food and water for incubating or brooding females johnstone et al 2013b the diet of frtbc primarily consists of fruit from jarrah and marri trees johnstone et al 2017 fruit production of these species is complex and often dependant on stand density competition and landscape position as well as temporal factors such as climate biggs et al 2011 johnstone et al 2013a 2 2 geospatial data simulation for the purposes of this study we chose to demonstrate the proof of concept of the decision tools using geospatially simulated data due to a number of constraints on input data described in the following sections and a desire to demonstrate the application of movement corridor concepts even though these are arguably not critical for frtbc per se nevertheless the study is based on various research of frtbc ecology in the study landscape johnstone et al 2013a b 2017 mastrantonis et al 2019 providing a realistic application 2 2 1 mining extent we created a 300 300 90000 cells dimensional matrix to represent our study landscape where each cell would nominally represent a 100 100 m area on the ground no geographic or projected coordinate reference system is required for the simulated data but our models and tools can operate with any appropriate projected system to represent areas with bauxite that could potentially be cleared for mining we used a spherical variogram model from the gstat package pebesma 2004 to generate a spatially autocorrelated response across the 300 300 matrix applied an image convolution with a 5 5 window and modal function across the matrix and rounded all values to 0 or 1 where values of 0 represented areas of no mining and values of 1 represented areas to be mined fig 2 a 2 2 2 vegetation and food distribution we generated an additional spatially autocorrelated response across the same 300 300 matrix described above to represent vegetation productivity or food density distribution in the case of the frtbc fruit availability could not be adequately assessed in the field and so we relied on sentinel 2 surface spectral reflectance ndvi data as a proxy for vegetation and frtbc food availability leveau et al 2018 pettorelli et al 2006 2011 studds and marra 2011 2 2 3 nest site and water resource simulation we generated 196 locations of potential nesting sites for the frtbc by assuming that more productive vegetation would have a higher probability of containing a nest site and thus weighting the probability of a cell being sampled by the corresponding value of the vegetation productivity matrix fig 2b 196 nests were generated as this corresponds to the number of nests located during surveys additionally we performed a k means clustering n 25 of the nesting sites to generate locations of limiting resources water points in the case of the frtbc fig 2b water is a vital resource for some australian birds hawkins et al 2005 including the frtbc johnstone et al 2013a although identifying such water points across the study landscape is a challenging task 2 2 4 landscape permeability we included functionality in our decision tools to evaluate the movement corridors of species between habitats and resources through least cost pathing we describe this functionality in more detail in a later section however a requirement for this analysis is an appropriate measure of matrix permeability for the landscape though this functionality could be argued to be unnecessary in the case of the frtbc we demonstrate its functionality here using vegetation productivity as a permeability surface and we have assumed that higher productivity results in greater landscape permeability fig 2b 2 3 evaluating spatial interactions and conservation value the core of our novel approach is a method that attributes a value to each cell in the landscape based on the distributions of the resources and habitats a species depends on in the case of the frtbc these include relatively sparse water points and nesting sites and more widely distributed spatially varying food resources an approximation of foraging distance in appropriate map units for the model species can also be specified and in the case of the frtbc this was set to a circle with a radius of 75 map units in reality cockatoos will travel up to 20 km over multiple trips per day from their nests in search of food and water r johnstone et al 2013 though this is not a requirement of our approach specifying a foraging range will usually be important as animals are more likely to access resources within close proximity to maximise net energy gains i e optimal foraging theory pyke 1984 stephens and krebs 2019 first a relative value of the food within the foraging range of each of the 196 nesting sites was determined using 1 z p f i n f a f d f f i n f 1 d f where z p represents the relative value of the food within the foraging range of nesting site p f is the set of indices of all cells within the foraging range of nesting site p a f is the food density value of cell f and d f is the euclidean distance between cell f and point z p thus nests closely surrounded with cells of greater food densities will have greater values than nest sites surrounded by cells with lower densities or where higher food density cells are more distant fig 3 next we represented the way that water resources contribute or limit the value of nest sites each water resource within the foraging distance of a nest site is assigned a weight k based on a logistic distance decay kernel fig 3 carlos et al 2010 evans and shen 2021 2 k 1 1 e d l a where d is the distance of the resource to the nest site l is the location of the inflection point of the logistic curve and a is the inverse of the steepness of the curve fig 3 for each nest site we then calculate a value v p based on the number and proximity of all water resource points within the foraging range using 3 v p 1 f i n f 1 k f where v p is the relative water value of nesting site p f is the set of indices for all water resources within the foraging range and k f is the weighting for water resource f calculated from equation 2 we then calculated the total value for each nest site using 4 t p v p z p and then the value of the entire landscape is calculated by summing the values of all nest sites 5 l t p as the values for each nest site is dependent on the distribution of both the food densities and water resources in nearby cells we evaluate the contribution of each cell in the landscape to the total landscape value by removing that cell and recomputing the value of the entire landscape 5 if the value of the landscape with a cell c removed is c then the contribution of that cell to the total landscape value is then 6 m c l c this results in a matrix of values one for each cell we then applied a smoothing algorithm to this matrix of values where the new value of cell is calculated as the weighted average of all cells within a specified buffer 50 units in our case using a weighting that decays linearly with distance this buffering process results in contiguous hotspots within the landscape that hold the greatest value for the model species based on the density of nests and water resources this is particularly relevant for the frtbc as alcoa identifies in pre mine surveys and protects with a static buffer all nest sites and important habitats and applying a distance based buffer in conjunction with conservation prioritisation could improve both economic and environmental outcomes 2 4 species movement and corridors as part of these decision tools we have included the functionality to model the movement of species between habitats and resources and include movement pathways into our scp outcomes currently our tools for modelling animal movement are dependent on the gdistance package etten and sousa 2020 the gdistance package provides the functionality for least cost pathing and constrained random walks though heterogenous landscapes we leverage the least cost pathing functionality of the gdistance package to assess a species movement corridors between habitats and resources and this is achieved through an iterative process of weighted random sampling from locations in the landscape based on conservation value prior to sampling a transition layer is created from a measure of landscape permeability where movement is possible in orthogonal and diagonal directions i e a kings graph for the frtbc case study we have simply used the simulated vegetation productivity matrix as a permeability surface where higher productivity cells result in less resistance to movement through the cells though landscape permeability is not necessarily relevant for the frtbc we have included it here to showcase its functionality for species where permeability would be relevant for each iteration an origin and destination point are sampled from the nest sites and water resource locations and an appropriate path through the transition layer is determined with a minor random deviation from the shortest possible path to satisfy optimal foraging theory the least cost pathing results in a density value between 0 and 1 for cells that link the origin and destination points after a predetermined number of iterations n 196 so that every nest site has a chance of being sampled the results are spatially aggregated to determine the highest density movement corridors between nests and and these corridors were added to the results of equation 6 for scp analysis 2 5 improving landscape conservation outcomes the configuration and functional connectivity of habitats and resources plays an important role in landscape ecological processes and this is especially true for the persistence of mobile species carvalho et al 2016 mastrantonis et al 2019 in addition the introduction and management of accessible supplementary resources has been shown to influence the ranges and behaviour of mobile species while conservation offsets could alter land use allocation in conservation prioritisation models moilanen et al 2020 morris et al 2006 plummer et al 2015 another strength of our approach is that it can help determine locations for conservation offsets i e habitats or resources for a species for the frtbc this involved determining the best locations for adding artificial water points to the landscape this can be done by searching the whole landscape for each cell in the landscape add a new water point to that cell and then recalculate the landscape value and finally select the cell that adds the greatest value this can then be repeated for as many new water points as desired we also developed a heuristic search algorithm that places a water point at a random coordinate in the landscape and assesses the set of nearest neighbouring points to maximise equation 5 the resource point then moves to the coordinate that maximises equation 5 and the process continues if a point does not add value within any given set the search neighbourhood is expanded additionally if a coordinate in the expanded search neighbourhood set fails to improve the result of equation 5 the search neighbourhood is expanded a second time if these three neighbourhood sets fail to improve the results of equation 5 another random coordinate is sampled and the process repeats until a specified number of resources points are allocated for a landscape with only 90000 cells a heuristic approach is far from ideal as every single cell can easily be assessed for potential offsets however for cases where the study area spans regional or world regional extents performing a full search of the landscape would become computationally demanding and time intensive and a heuristic approach that finds a good but imperfect one is acceptable 2 6 the pareto frontier land use allocation conservation and economic enterprise often come into conflict especially in spatial management and multi objective contexts czech 2008 faleiro et al 2013 nelson et al 2009 and debate is ongoing on how to best conserve nature while maintaining socioeconomic wellbeing hobbs et al 2014 determining the best trade offs between conservation and economic activity is complex and challenging and in this case we seek to achieve the best trade offs between these two contrasting objectives using pareto optimality we have developed a tool that visualises the pareto frontier of optimal trade offs between the economic anthropogenic objective clearing for mining in this case and the conservation objective landscape conservation value and allows a user to select their desired outcomes across the frontier to generate optimal land use outcomes and allocations this tool starts by finding the cell in the potential mining extent with the least conservation value removes it from the set of unmined cells and adds it to a new set of mined cells it then continues and finds the next remaining cell in the potential mining extent with the least conservation value removes it from the set of unmined cells and adds it to the set of mined cells this iterative process continues until all cells in the potential mining extent have been removed from the set of conserved cells at each iteration the conservation value of the whole landscape is plotted against the total number of mined cells to form the pareto frontier branke et al 2008 which displays the nature of trade offs between mining production and landscape conservation value the corresponding land use allocation can be mapped spatially fig 5 3 results discussion the landscape conservation value of the frtbc based on the simulated spatial interactions and distribution of their functional resources is shown in fig 4 notably regions that hold dense nesting sites surrounded by productive vegetation and in close proximity to water resources contribute the highest conservation values to the landscape conversely regions in the landscape with relatively low values reflect limited number of water points and low density of productive vegetation within foraging range of nest sites the least cost pathing analysis shows the likely movement corridors between nests and resources in the landscape fig 4 much of the high value conservation regions and corridors in the landscape intersect with our simulated extent of mining fig 2 in this scenario we observe that the entire extent of clearing will result in half of the conservation value of the landscape being lost but this loss may be offset by supplementing water into the landscape fig 5 however through mapping the optimised trade offs between the competing objectives of mining and conservation with the pareto frontier we can observe that it is possible for example to clear 20 of the maximum mining potential while impacting less than 5 of the conservation value of the landscape fig 5 conversely clearing 80 of the maximum mining potential is predicted to result in a reduction of approx 30 of the conservation value with two other intermediate options also shown fig 5 the application of pareto optimality for scp has efficiently allocated land for mining while simultaneously conserving the most appropriate regions of the landscape for biodiversity the results of the heuristic search for locations to establish water resources indicated that adding 10 new water points into the landscape would increase the total conservation value of the system by 47 fig 6 these additional points in the landscape shift the pareto frontier outwards for a greater extent of mining with a proportionally lower net loss in conservation value across the landscape this shift in the pareto frontier demonstrates that adding resources into the landscape can improve conservation outcomes while maintaining or improving economic objectives fig 6 3 1 computational efficiency in terms of computational efficiency our landscape evaluations complete within minutes due to our c functions called through the r environment using wrapping protocols from the rccp package eddelbuettel 2013 our pareto optimality functions are also computationally efficient and generally complete within minutes depending on the intersecting extent for conflicting objectives our heuristic search function completes in minutes for 10 additional resources but this time will linearly increase with increasing numbers of desired additional resources the least cost pathing analysis can take several hours depending on the number of habitats and resources within the landscape and because we are currently relying on the gdistance package for pathing we cannot improve the computational efficiency of these functions yet the option to parallelise these function remains 4 conclusion we developed a novel approach for identifying optimal trade offs between conflicting economic and conservation land use objectives for species that require multiple resources widely distributed across a landscape we applied this approach to a real world case study identifying optimal trade offs between bauxite mining and conservation of a threatened mobile species accounting for its dependence on widely distributed food and water resources and nesting sites we demonstrated our approach on simulated data with similar characteristics to the real world data it is important to consider that even though we used relatively complex spatial interactions among multiple resources to model landscape conservation value for a species it is entirely possible to substitute the outputs of these spatial interactions with typical ecological indices such as sdm or landscape metrics hesselbarth et al 2019 as the scripts used here are provided in an open source repository and were initially developed to work with real world data the applicability and functionality of these tools is broad even without using our relatively complex evaluation methods our heuristic search tools for determining locations for the provision of additional resources could be useful in a range of conservation planning scenarios and potentially inform management decisions for the benefit of both environmental and economic interests lastly the incorporation of least cost pathing into scp has broad relevance and applicability for managing mobile species and the movement corridors they depend on hofmann et al 2021 the conservation of mobile species is a significant challenge and protecting their habitats resources and movement pathways over regional and world regional scales is critical in the face of environmental fragmentation the tools presented here may provide a steppingstone to effectively addressing these challenges software data and video availability code and data available at github https github com stanleymastrantonis rezone developers stanley mastrantonis year first available 2020 programming languages r c package requirements raster rcpp and gdistance optional packages contact email stanley mastrantonis research uwa edu au declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the australian research council arc linkage grant lp160100177 the department of biodiversity conservation and attractions and alcoa of australia 
25681,in the environmental sciences there are ongoing efforts to combine multiple models to assist the analysis of complex systems combining process based models which have encoded domain knowledge with machine learning models which can flexibly adapt to input data can improve modeling capabilities however both types of models have input data limitations we propose a methodology to overcome these issues by using a process based model to generate data aggregating them to a lower resolution to mimic real situations and developing machine learning models using a fraction of the process based model inputs we showcase this method with a case study of pasture nitrogen response rate prediction we train models of different scales and test them in sampled and unsampled location experiments to assess their practicality in terms of accuracy and generalization the resulting models provide accurate predictions and generalize well showing the usefulness of the proposed method for tactical decision support keywords machine learning digital twin data availability data resolution apsim metamodel 1 introduction digital twins are established in several industries including manufacturing he and bai 2021 healthcare liu et al 2019 automotive caputo et al 2019 their ability to replicate physical systems and provide decision support through data fusion simulation and technology integration makes them attractive to apply in complex multidisciplinary problem solving recently digital twins have drawn the attention of the environmental sciences community researchers are exploring digital twins in hydrology pedersen et al 2021 agriculture pylianidis et al 2021 smart farming verdouw et al 2021 livestock farming neethirajan and kemp 2021 remote sensing nativi et al 2021 and earth sciences guo et al 2020 recently the european union has announced plans for a high resolution earth digital twin that aims at actionable intelligence from big data streams bauer et al 2021 voosen 2020 in the us the research agenda for intelligent systems in geosciences gil et al 2018 aims to incorporate extensive knowledge about the physical geological chemical biological ecological and anthropomorphic factors that affect the earth system while leveraging recent advances in data driven research digital twins intertwine data streams from a variety of in situ or remote sensors with simulation and learning components these components are then used to estimate future system states and offer an understanding of how complex mechanisms evolve digital twins incorporate sensor data streams with process based models pbm or machine learning ml models to provide insights by analyzing what if scenarios or provide operational decision support for managing and controlling complex systems pbms implement mathematical representations of physical processes and their interactions and estimate future system states by numerical integration while pbms embody system understanding they require many inputs and tend to be computationally intensive ml models follow an empirical data driven approach in making predictions based on large collections of historical data ml models are computationally fast in making predictions and robust with noisy data but typically harder to interpret and expensive to develop from data digital twins need to be operational in a variety of data availability conditions their operation depends on the ability of the underlying models to cope with missing data streams or different resolutions problems with limited data arise when digital twins have to make decisions for the not immediate future and quantities have to be forecasted also their application in locations where data are sparse or non existent unsampled locations can be challenging another concern is that transitions between different aggregation levels may be impossible due to the difference in the detail of the data that models expect therefore digital twins need models or techniques to create models that are able to handle such cases in order to provide operational decision support ml models can be versatile to a varying extent and resolution of input data however they generally require large volumes of data for their development accompanied by labels that are not easily available in environmental sciences techniques like few shot learning yang and jiachen 2021 seem promising to learn from small datasets but still novel research is needed to develop ml approaches that incorporate prior knowledge about environmental processes karpatne et al 2017a and use it to effectively supplement the available data gil et al 2019 a path forward could be to employ synthetically generated datasets from simulations that mimic real conditions which can be effectively used for developing ml models gil et al 2019 in this work we showcase an approach to create ml models which tackles the challenges of data availability and data resolution while providing operational decision support for digital twins we propose a method which a does not need forecasted data to be operational b is applicable to locations where data are not yet available to calibrate pbms and c is applicable in cases where the available data do not have the resolution expected by the pbms we then demonstrate its usefulness in the context of a case study in the case study we create ml models of different scales to predict pasture nitrogen response rates nrr and examine their reliability by assessing their predictive and generalization capacity the rest of the paper is organized as follows in section 2 we describe the requirements of pmbs the proposed method and related work in section 3 we present the case study and the methodology to experimentally evaluate the proposed method section 4 reports the results of our experiments followed by a discussion in section 5 and the conclusion in section 6 2 simulation assisted machine learning 2 1 process based model data requirements pbms typically require several high resolution data streams as inputs to simulations julie ramanantenasoa et al 2019 kasampalis et al 2018 data availability becomes a problem with pbms when applying models in new locations where no or little data have been collected yet in such cases input data need to be estimated or collected which can be a lengthy and expensive process also when input data are available they are needed in a prolonged temporal horizon of interest for example daily weather forecasting may be necessary for in season crop model predictions togliatti et al 2017 without such detailed forecasts of inputs pbms can make estimations only up to the present day they may extend their reach to the near future if quantitative short term weather forecasts are available otherwise pbms are used with historical data to estimate probability or risk distributions based on simulations e g as in vogeler et al 2013 and often together with data assimilation techniques to integrate them with sensor observations of system states dorigo et al 2007 another factor affecting the operational use of pbms is data resolution usually sensor input is not available at the resolution required by the models for example input data streams may be available on a weekly basis while models require daily inputs cichota et al 2008 data availability and resolution are two factors that can prohibit the use of existing pbms in digital twins a depiction of the data requirements of pbms can be seen in fig 1 a 2 2 requirements for operational decision making in order to have digital twins for operational decision making we need models which are able to operate when less data are available specifically we identified three requirements first we need models which can make predictions for the future with data only until the prediction date without requiring the future values of variables second these models should be accurate in locations where historical data are available sampled locations 1 1 throughout the manuscript we use the term location s but without loss of generality this can be considered as situation s when considering non spatially explicit systems but also in locations where data have not been collected in the past unsampled locations third the models should be able to work in cases where high resolution data are not available e g due to lower frequency sampling rates or when less input data streams are available in unsampled locations the data requirements of such models can be seen in fig 1b 2 3 proposed method to satisfy the requirements for operational decision making we can train ml models on pbm input output data so they are also metamodels see paragraph 2 4 discard data we do not need and then aggregate on lower resolutions having a pbm a target variable and historical data to make simulations we propose the following steps from an application based perspective 1 define the decision horizon i e how far in the future predictions are going to be made based on this boundary we know how much data we need to retain as any data after the prediction date are going to be discarded 2 choose an aggregation level for the retained data wherever applicable with lower resolution than the original data this will allow the ml model to make predictions even when high resolution data are not available 3 generate data to generate data we need to define a hyperspace of input combinations for the model we can choose a full factorial design antony 2014 to contain all the possible combinations of the input variables or decide to retain only the physically consistent combinations 4 if possible discard inputs output datastreams of the pbm the fewer inputs the better because in this way the data requirements of the model are reduced this decision can be made based on domain knowledge or feature selection procedures 5 finally develop one or several ml models using the data resulting from the above steps evaluation is an important factor to verify that the created models are useful for operational tactical decision making a practical way to estimate the predictive capacity of the models is to compare their errors with a threshold based on domain expertise also the models should be tested for their generalization capacity a way to do this is to consider both sampled and unsampled locations for testing experiments where data from some locations are excluded from the model training sets and examine model performance in the excluded locations another evaluation aspect is to determine the appropriate training data size of the models the more variability a model has seen in its training data the more accurate prediction and generalization capacity it should have in the case where more data do not increase prediction performance it could mean that they do not add any variability and hence we do not need to generate much data in the future in our case data quantity is controlled by the amount of data that we generate with the pbm therefore an evaluation step could be to test models of different scales by including different amounts of locations years or other parameters 2 4 related work efforts to overcome the inherent shortcomings of pbms for operational decision making have been focused on combining pbms with ml through the concept of metamodeling metamodels also called surrogate or hybrid models refer to models which mimic the behavior of other models blanning 1975 ml metamodels have been used in agricultural and environmental sciences to cope with a variety of problems to instill domain knowledge to ml models the authors of karpatne et al 2017b train a neural network on pbm output using a custom loss function to predict the water temperature in lakes to reduce the long execution times of pbms metamodels have been employed to predict maize yield and compare the results with those of the pbms and ml models shahhosseini et al 2021 roberts et al 2017 to accelerate sensitivity analysis metamodels have been trained on the output of agricultural simulators gladish et al 2019 also hydrological metamodels have been evaluated for their performance in terms of speed and accuracy zhang et al 2020 villa vialaneix et al 2012 as well as generalization capacity domain adaptation in unsampled areas nolan et al 2018 likewise to extrapolate at regional and national levels metamodels have been deployed in environmental management ramanantenasoa et al 2019 lastly to work in situations where pbm inputs are not available the authors of shahhosseini et al 2019 create metamodels to predict pre season maize yield for decision support the aforementioned studies focus on each of the advantages of metamodeling individually whether it is domain knowledge imputation faster computation times improved generalization capacity over pbms or working with less data also most of these studies make an effort to create models that predict the variable of interest at any time of its evolution similar to what pbms do i e by predicting state variables for each simulation step in this work we introduce a generic method to exploit these advantages as well as to deal with data resolution problems which were not explicitly mentioned in those studies and also we do it for a specific point in time in the future of the target variable 3 methods 3 1 overview to assess the method described in 2 3 we performed a case study of grass pasture nrr prediction in different locations see fig 2 of new zealand the application of nitrogen along with environmental factors such as temperature and time of year greatly affects pasture growth gillingham et al 2008 so it is important to know the nitrogen response rate to examine the reliability of our models we performed a sampled and an unsampled location experiment in the sampled location experiment we assessed the predictive capacity of the models in cases where data from the testing locations are available in the unsampled location experiment we examined the generalization capacity of the models in cases where data from the testing locations are unavailable for both sampled and unsampled location experiments we iteratively considered each location to be a testing location to be able to better establish our verdicts to argue about the predictive and generalization capacities we used a case study specific example where we compared the models performance with a threshold that makes sense for crop practitioners also we created models of different scales by using various amounts of data for training and examined how data quantity included in training affects their performance 3 2 case study the target of our prediction was the expected two month nitrogen response rate nrr kg of additional i e compared to not applying any fertilizer of pasture dry matter grown in the two months after fertilizer application per kg of n fertilizer applied as in most countries pastures in new zealand suffer a chronic deficiency of nitrogen rotz et al 2005 whitehead 1995 and farmers apply nitrogen containing fertilizers to increase pasture growth rates clark et al 2007 pembleton et al 2013 nitrogen fertilizer can be applied regularly e g after each grazing event or more tactically to manipulate the supply of pasture available to feed stock as fertilizer costs increase environmental concerns about leaching of nitrogen increase and or the prices received for meat and milk decrease farmers become more interested in understanding when best to apply nitrogen fertilizer to obtain the best nrr current nrr estimators are based on rules of thumb that consider the month of year soil temperature soil nitrogen or pasture growth rate waikato regional council 2015 nz farm source 2021 dairynz 2012 there are pbms that can estimate nrr based on site soil properties pasture type and the prevailing conditions weather but they have limited usefulness as operational estimators of nrr because the weather for the two months after a proposed current or future application date are not known and such data are required to run the model also while there are some nrr data available from experiments they are sparse and not sufficient to train ml models 3 3 data generation we used apsim v7 10 r4191 apsim 2021 holzworth et al 2014 to generate the training and testing data pasture growth was simulated with the agpasture module li et al 2011 which has already been demonstrated to be a reasonable estimator of pasture growth in new zealand cichota et al 2013 2018 the range of input conditions covered eight contrasting locations in new zealand fig 2 and are given in table 1 pasture nrr is known to be influenced by soil water and nitrogen availability temperature and solar radiation the combinations of input conditions were designed to provide coverage across these variables along with 40 years of historical weather data from the new zealand virtual climate station network tait et al 2006 cichota et al 2008 which gave a rich source of variation in weather after fertilizer application a hyperspace of parameters was created using the full factorial of the input conditions and put into apsim the total number of generated simulations was 1 658 880 after removing the control simulations see table 1 1 382 400 remained 3 4 data preprocessing the data generated by apsim were processed to form a regression problem where the target variable was the nrr and the inputs were the weather treatment options regarding the fertilizer and irrigation and biophysical variables first the nrr was calculated at two months after fertilization for each non control simulation second from the generated daily data only the samples in a window of 28 days before fertilization were retained this window was selected because in the experience of the authors pasture loses memory of past conditions relatively quickly provided it is not under or over grazed weather data after the fertilization were also not considered as such data would be unavailable under operational conditions third the generated data were split into 80 20 training test sets based on years to avoid information leakage during the later stages of preprocessing the training and test sets included the year ranges 1979 2010 and 2011 2018 respectively fourth the weather and biophysical variables were aggregated using their weekly mean values finally only a subset of the variables was preserved this subset included weather variables simulation parameters soil water soil fertility irrigation fertilizer month fertilizer rate and biophysical variables produced by apsim above ground pasture mass net increase in herbage above ground dry matter potential growth if there was no water and no n limitation soil water stored from 0 to 300 mm soil temperature at 300 mm soil temperature at 50 mm herbage nitrogen concentration in dry matter these variables were preserved because they were considered to be likely drivers and also known prior to fertilization to ensure operational usefulness based on expert knowledge of the authors 3 5 model scale different models were created using different amounts of data we considered models on three scales local regional and national each including a different number of locations the criterion for selecting the locations differed based on whether the experiment was performed in sampled or unsampled locations in the sampled location experiment the locations were selected based on a climate matching process the degree of climatic similarity between sites was assessed using the climex match climates algorithm kriticos et al 2015 this algorithm produces a composite match index cmi from 0 to 1 which indicates the similarity between two locations in weekly average maximum and minimum temperatures total annual rainfall seasonal pattern of rainfall relative humidity and modelled soil moisture the required climate data were obtained for the nearest 0 05 location from niwa s virtual climate station network tait et al 2006 for the period 1979 to 2010 i e using data only from the training set the results were expressed as a matrix of pairwise cmis between all sites in this experiment the local model included data from the sampled location the regional model from the sampled location and the best two matches for this location and the national model data from all the locations in the unsampled location experiment the locations included in each model were selected based on minimum haversine distances from the testing locations the reason for not using climate matching with climex was the assumption that data from the unsampled locations were not available and as a result climate matching could not be performed the local model included data from the nearest neighbor of the unsampled location the regional from the three nearest neighbors and the national from all the locations except the unsampled one see fig 3 for a visualization of training models of different size and table a1 in the appendix for the locations included in each model 3 6 machine learning pipeline the models were developed with the random forest algorithm random forest was selected based on the results of preliminary exploration see table b1 in the appendix feature selection was not performed since we had only a few features which were all considered explanatory training data were standardized for each location and experiment and test set data were standardized with the corresponding scaler categorical variables like irrigation on off were converted to ordinal hyperparameter tuning was performed using bayesian optimization with 25 iterations and the 5 fold cross validation score as a metric for each iteration the tuned parameters can be seen in table 2 3 7 evaluation the predictive capacity of the models was evaluated using the root mean squared error rmse and the residuals of the models on a monthly and yearly basis a threshold of 5 kgdm ha kgn 2 2 kg of dry matter ha kg of nitrogen nrr was selected based on expert knowledge to investigate if the models were accurate enough from a practical perspective to test the generalization of the models rmse and residuals were also examined against the threshold of 5 in unsampled locations 3 8 experimental setup the data preprocessing stage was carried out utilizing the apache spark framework in standalone mode the machine learning models were developed using the scikit learn library in python the experiments took place in a computing node with an intel xeon e5 2630 v4 cpu and 120 gb of ram 3 9 software availability the code used for the case study of this paper can be found in https github com bigdatawur simulation assisted ml 4 results in the following sections we present rmse values and residual plots for sampled and unsampled locations the errors of the models fluctuated depending on model scale location month and year of application and whether the location was considered to be sampled unsampled none of the models proved to be universally better on all the locations or in both the sampled unsampled testing experiments however some of them showed higher performance and generalization capacity than others in certain cases 4 1 sampled locations experiment for the sampled location experiment regional models had lower rmses than the local and national in 4 out of 8 locations but the error differences between the models were smaller than 0 03 national models had the second best performance rmses for each model and location can be seen in table 3 prediction residuals are illustrated in fig 4 we observe that errors were mostly below the operational threshold of 5 kgdm ha kgn exceptions were the months january and february which showed errors close to 5 in some cases on a closer inspection we observed large fluctuations based on whether there was irrigation or not fig c1 in the non irrigated case we noticed that for january february and december the residuals were larger than our threshold of 5 kgdm ha kgn for the other months the performance was well below our threshold in the irrigated case the residuals took considerably smaller values on a yearly basis fig 5 the candles of the residuals were below 2 5 except for ruakura in 2016 and some years in lincoln which were higher than 2 5 but still lower than 5 separating the irrigated and non irrigated cases we found that the irrigated cases had residuals consistently lower than our threshold for the non irrigated cases fig c3 we observed that the years 2015 2016 had larger residuals in several locations 4 2 unsampled location experiment in the unsampled location experiment fig 4 we observed that the performance of the models generally decreased compared to the sampled experiment this decrease was more evident in lincoln and kokatahi while in the rest of the locations the differences are minor the regional models outperformed the national and local models in 4 locations fig 3 the performance of the regional models was close to that of the national models in many cases the only location where a local model outperformed the other two was in mahana from the residual plots on a monthly basis fig c2 we observed considerable variation in the residuals between the irrigated and non irrigated cases also we noticed that the interquartile ranges had been increased compared to the sampled locations especially for the local models and were higher than 5 in many occasions with the largest errors happening in lincoln from the residual plots on a yearly basis fig c4 we observed that the interquartile ranges had been increased compared to the sampled location experiment again the years 2014 2016 had the widest interquartile ranges with those of the lincoln local model displaying the largest errors except for those years we could say that the performance of each model is stable across the years for each location 5 discussion in our experiments the models captured in most cases sufficient variation from the data to achieve rmses lower than the threshold of 5 kgdm ha kgn this means that they could be potentially used in practical applications where weather data after fertilization are missing or data are on a lower resolution than those that apsim expects these results persisted in the unsampled location experiment thus providing evidence that the models are operational in locations where data do not exist to calibrate pbms as well as locations not included in the training set of the models in the following sections we interpret the results of the local regional and national models and discuss the models as a product of the proposed model development methodology 5 1 predictive capacity sampled locations when separately analyzing the irrigated non irrigated cases we observed that the lack of irrigation hindered the predictive capacity of the models the reason for this impediment is that when no irrigation is provided the weather conditions become the driving factor of the nrr because the grass relies solely on rain to grow therefore as several uncertainty factors pile up weather volatility nrr sensitivity to weather predictions two months in the future without knowing the weather the results are expected to deteriorate but they are not indicative of the general model performance the deterioration was sharper during the spring summer months november december january and february because irregular rainfall is most critical in these seasons also the performance degradation was not the same in all the locations since some locations have more favorable weather conditions than others comparing the models we observed small differences in model performance at first glance this seems counter intuitive since bigger models were trained on supersets of the smaller model data this means that they have the same information to learn from and thus they should perform at least equally well however this is not the case since the smaller models seem to benefit more from additional data from locations with similar climates than from having more data from locations with less similar climates regarding the national models they have somewhat higher rmses than the other two models because they include data from all the locations which makes them harder to adapt to local conditions the models showed good performance through each month of the year for the irrigated case interquartile ranges were mostly below 5 which means that 50 of the values lie within this range in different locations we see different months having the largest residuals this has to do with the variation in their microclimates since rainfall and temperatures can be disparate residuals went as high as 7 5 in lincoln which is characterized by low precipitation amounts as can be seen in fig d1 also we observed that the errors of the models are consistent throughout the years there is some variation for 2016 and 2018 in kokatahi and wairoa mainly due to local weather conditions and extreme events which the models were unable to capture this aligns with our expectation since extreme events are rare so there are only a few in the dataset and also because their presence may be imbalanced between the training test sets yet in most cases the model shows adequate predictive capacity even eight years after the last year that was included in the training set from the perspective of model operationalization the models proved that they can complement the pbms to provide predictions of adequate accuracy and overcome the problem of data availability to a certain degree this degree depends on the level of uncertainty involved in the predictions and the ml pipeline used to build the models in a digital twin these models could provide the first line against working with limited data several models could be included with different tasks for example a model providing predictions for the irrigated case of a specific location with a specific soil type one trained on extreme weather one on non irrigated cases and so on these ensembles of models could potentially capture a large degree of variation while waiting for more data to become available 5 2 model generalization unsampled locations in the unsampled location experiment the differences between the models became more evident as the local models performance deteriorated more than the others fig c2 the reason behind this difference is that the local models had data from only one location which was not the location where the testing happened on the other hand the bigger models were favored in this experiment since they included data from multiple locations and could extrapolate better this phenomenon is more noticeable in the non irrigated cases fig c2a where the local model shows high deviations from the simulated nrrs having said that with the exception of january february and december the rmses were below 5 for all models those three months included temperatures higher than 25 c fig d2 which can be harmful to the grass and when combined with the non irrigated case the uncertainty for the future increases on the monthly residual plots of the unsampled location experiment fig c2a we saw a more detailed picture of model performance with respect to size many times the residuals surpassed our threshold especially those of the local and regional models from these cases we can deduce that the national models are superior to the local and regional ones the cases where the national models had increased interquartile ranges happened on the same months and locations as in the sampled location experiment e g january february in ruakura february march in marton the latter observation means that the increased ranges are not a matter of hindered generalization among locations but of an inability to capture variability in those climates due to the features included in the models from the residuals on a yearly basis we observed that the errors are mostly consistent across the years in each location the local models showed the highest fluctuations throughout the years like in ruakura and lincoln the regional models had the second highest discrepancies throughout the years like in mahana lincoln the national models were the most stable ones this behavior can be attributed to the amount of data included in each model because the more data from different locations a model includes the more divergent weather conditions it has seen this means that it can generalize better in the weather conditions of the years to come also it is interesting to see that models can generalize in unsampled locations many years 8 later since the last year included in the training sets from an operational perspective the models showed a capacity to generalize in previously unseen conditions a recommendation we would make when starting modeling in unsampled locations would be to begin with a national model rather than a model from the single nearest similar location in digital twins where existing models cannot be applied due to lack of calibration data or insufficient observation training data these models can provide a first impression of variables of interest in the future even though there are still limitations again the model performance could be improved by training for more specific scenarios and using more advanced ml techniques 5 3 future work this line of research could be improved further by generating data from multiple pbms and by trying different aggregation levels to find a balance between performance and working with low resolution data also it would be beneficial to evaluate model performance against ground truth data which were not available for this case study regarding the case study the data preprocessing and ml procedures could be adapted to better fit the domain of the application by using custom features performing training test splits which better balance underrepresented phenomena between the sets or using stratified sampling to select which simulations are going to be included in each set more elaborated ml model architectures could further improve performance metrics 6 conclusions in this work we introduced a method to develop operational digital twins by creating models which overcome the problems of data availability and data resolution we showcased this method using a grass pasture nitrogen response rate case study experimental results verified that this method is able to produce digital twins to offer tactical advice in highly non linear situations where local conditions and treatment options affect the outcome of the predictions the ability of the models to provide accurate predictions in different locations for both sampled and unsampled experiments indicates that they can adequately capture the variability encoded in process based models the developed models were able to capture the target variable even without having the complete weather and biophysical time series this practically allows to develop operational digital twins in cases of limited data availability also model predictions were made on field level using weekly data instead of daily data that a process based model would require as a result digital twins using these models are capable of operating in situations where process based models cannot these advantages combined with the fact that we did not need to forecast any future weather values to get those results differentiate this method from the creation of metamodels which just summarize process based models and demonstrate that simulation assisted machine learning is able to offer advice in practical conditions declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests given his role as associate editor of environmental modelling software i n athanasiadis was not involved in the peer review of this article and had no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to the editor in chief d p ames acknowledgements funding this work has been partially supported by the european union horizon 2020 research and innovation program grant 810 775 dragon the wageningen university and research investment program digital twins and agresearch strategic science investment fund ssif emulation of pasture growth response to nitrogen application appendices alocations included in each model table a 1 location data included in each model the locations of the sampled location experiment were chosen based on climate similarity while the ones of the unsampled location experiment were based on haversine distance table a 1 target location scenario sampled location unsampled location local regional global local regional global waiotu waiotu waiotu wairoa ruakura all ruakura ruakura wairoa marton all except waiotu ruakura ruakura ruakura marton wairoa all wairoa wairoa marton waiotu all except ruakura wairoa wairoa wairoa ruakura waiotu all marton marton ruakura mahana all except wairoa marton marton marton mahana ruakura all wairoa wairoa mahana ruakura all except marton mahana mahana mahana marton ruakura all marton marton kokatahi lincoln all except mahana kokatahi kokatahi kokatahi waiotu wairoa all lincoln lincoln mahana wyndham all except kokatahi lincoln lincoln lincoln mahana marton all kokatahi kokatahi mahana wyndham all except lincoln wyndham wyndham wyndham marton mahana all lincoln lincoln kokatahi mahana all except wyndham bpreliminary machine learning algorithm comparison table b 1 the gridsearch and rmse results of different machine learning algorithms for training in ruakura and testing in waiotu with the yearly split mentioned in the text as a preliminary test to choose an algorithm the gridsearch parameters as denoted as found in scikit learn s documentation the parameters in bold are those that gridsearch selected for each algorithm table b 1 gridsearch parameters rmse random forest n estimators 100 200 max depth 3 7 12 min samples split 10 20 min samples leaf 10 30 max features 0 33 2 51 gradient boosting trees learning rate 0 05 0 1 0 2 n estimators 100 200 min samples split 10 20 min samples leaf 10 30 max depth 3 7 12 max features 0 33 2 52 linear support vector regression c 0 2 0 5 1 epsilon 0 05 0 1 0 2 loss epsilon insensitive squared epsilon insensitive 2 68 elastic net alpha 0 2 0 5 1 max iter 500 1000 2000 l1 ratio 0 2 0 5 0 8 2 69 support vector regression kernel rbf c 0 2 0 5 1 epsilon 0 05 0 1 0 2 2 78 multi layer perceptron hidden layer sizes 40 40 40 60 60 activation relu batch size 32 64 max iter 100 early stopping true n iter no change 20 3 98 crainfed vs irrigated plots fig c 1 monthly test set residuals of models for sampled locations in rainfed a and irrigated cases b fig c 1 fig c 2 monthly test set residuals of models for unsampled locations in rainfed a and irrigated cases b fig c 2 fig c 3 yearly test set residuals of models for sampled locations in rainfed a and irrigated cases b fig c 3 fig c 4 yearly test set residuals of models for sampled locations in rainfed a and irrigated cases b fig c 4 d weather plots fig d 1 average rainfall per month and location for the four weeks that we assume to have data fig d 1 fig d 2 average maximum temperature per month and location for the four weeks that we assume to have data fig d 2 
25681,in the environmental sciences there are ongoing efforts to combine multiple models to assist the analysis of complex systems combining process based models which have encoded domain knowledge with machine learning models which can flexibly adapt to input data can improve modeling capabilities however both types of models have input data limitations we propose a methodology to overcome these issues by using a process based model to generate data aggregating them to a lower resolution to mimic real situations and developing machine learning models using a fraction of the process based model inputs we showcase this method with a case study of pasture nitrogen response rate prediction we train models of different scales and test them in sampled and unsampled location experiments to assess their practicality in terms of accuracy and generalization the resulting models provide accurate predictions and generalize well showing the usefulness of the proposed method for tactical decision support keywords machine learning digital twin data availability data resolution apsim metamodel 1 introduction digital twins are established in several industries including manufacturing he and bai 2021 healthcare liu et al 2019 automotive caputo et al 2019 their ability to replicate physical systems and provide decision support through data fusion simulation and technology integration makes them attractive to apply in complex multidisciplinary problem solving recently digital twins have drawn the attention of the environmental sciences community researchers are exploring digital twins in hydrology pedersen et al 2021 agriculture pylianidis et al 2021 smart farming verdouw et al 2021 livestock farming neethirajan and kemp 2021 remote sensing nativi et al 2021 and earth sciences guo et al 2020 recently the european union has announced plans for a high resolution earth digital twin that aims at actionable intelligence from big data streams bauer et al 2021 voosen 2020 in the us the research agenda for intelligent systems in geosciences gil et al 2018 aims to incorporate extensive knowledge about the physical geological chemical biological ecological and anthropomorphic factors that affect the earth system while leveraging recent advances in data driven research digital twins intertwine data streams from a variety of in situ or remote sensors with simulation and learning components these components are then used to estimate future system states and offer an understanding of how complex mechanisms evolve digital twins incorporate sensor data streams with process based models pbm or machine learning ml models to provide insights by analyzing what if scenarios or provide operational decision support for managing and controlling complex systems pbms implement mathematical representations of physical processes and their interactions and estimate future system states by numerical integration while pbms embody system understanding they require many inputs and tend to be computationally intensive ml models follow an empirical data driven approach in making predictions based on large collections of historical data ml models are computationally fast in making predictions and robust with noisy data but typically harder to interpret and expensive to develop from data digital twins need to be operational in a variety of data availability conditions their operation depends on the ability of the underlying models to cope with missing data streams or different resolutions problems with limited data arise when digital twins have to make decisions for the not immediate future and quantities have to be forecasted also their application in locations where data are sparse or non existent unsampled locations can be challenging another concern is that transitions between different aggregation levels may be impossible due to the difference in the detail of the data that models expect therefore digital twins need models or techniques to create models that are able to handle such cases in order to provide operational decision support ml models can be versatile to a varying extent and resolution of input data however they generally require large volumes of data for their development accompanied by labels that are not easily available in environmental sciences techniques like few shot learning yang and jiachen 2021 seem promising to learn from small datasets but still novel research is needed to develop ml approaches that incorporate prior knowledge about environmental processes karpatne et al 2017a and use it to effectively supplement the available data gil et al 2019 a path forward could be to employ synthetically generated datasets from simulations that mimic real conditions which can be effectively used for developing ml models gil et al 2019 in this work we showcase an approach to create ml models which tackles the challenges of data availability and data resolution while providing operational decision support for digital twins we propose a method which a does not need forecasted data to be operational b is applicable to locations where data are not yet available to calibrate pbms and c is applicable in cases where the available data do not have the resolution expected by the pbms we then demonstrate its usefulness in the context of a case study in the case study we create ml models of different scales to predict pasture nitrogen response rates nrr and examine their reliability by assessing their predictive and generalization capacity the rest of the paper is organized as follows in section 2 we describe the requirements of pmbs the proposed method and related work in section 3 we present the case study and the methodology to experimentally evaluate the proposed method section 4 reports the results of our experiments followed by a discussion in section 5 and the conclusion in section 6 2 simulation assisted machine learning 2 1 process based model data requirements pbms typically require several high resolution data streams as inputs to simulations julie ramanantenasoa et al 2019 kasampalis et al 2018 data availability becomes a problem with pbms when applying models in new locations where no or little data have been collected yet in such cases input data need to be estimated or collected which can be a lengthy and expensive process also when input data are available they are needed in a prolonged temporal horizon of interest for example daily weather forecasting may be necessary for in season crop model predictions togliatti et al 2017 without such detailed forecasts of inputs pbms can make estimations only up to the present day they may extend their reach to the near future if quantitative short term weather forecasts are available otherwise pbms are used with historical data to estimate probability or risk distributions based on simulations e g as in vogeler et al 2013 and often together with data assimilation techniques to integrate them with sensor observations of system states dorigo et al 2007 another factor affecting the operational use of pbms is data resolution usually sensor input is not available at the resolution required by the models for example input data streams may be available on a weekly basis while models require daily inputs cichota et al 2008 data availability and resolution are two factors that can prohibit the use of existing pbms in digital twins a depiction of the data requirements of pbms can be seen in fig 1 a 2 2 requirements for operational decision making in order to have digital twins for operational decision making we need models which are able to operate when less data are available specifically we identified three requirements first we need models which can make predictions for the future with data only until the prediction date without requiring the future values of variables second these models should be accurate in locations where historical data are available sampled locations 1 1 throughout the manuscript we use the term location s but without loss of generality this can be considered as situation s when considering non spatially explicit systems but also in locations where data have not been collected in the past unsampled locations third the models should be able to work in cases where high resolution data are not available e g due to lower frequency sampling rates or when less input data streams are available in unsampled locations the data requirements of such models can be seen in fig 1b 2 3 proposed method to satisfy the requirements for operational decision making we can train ml models on pbm input output data so they are also metamodels see paragraph 2 4 discard data we do not need and then aggregate on lower resolutions having a pbm a target variable and historical data to make simulations we propose the following steps from an application based perspective 1 define the decision horizon i e how far in the future predictions are going to be made based on this boundary we know how much data we need to retain as any data after the prediction date are going to be discarded 2 choose an aggregation level for the retained data wherever applicable with lower resolution than the original data this will allow the ml model to make predictions even when high resolution data are not available 3 generate data to generate data we need to define a hyperspace of input combinations for the model we can choose a full factorial design antony 2014 to contain all the possible combinations of the input variables or decide to retain only the physically consistent combinations 4 if possible discard inputs output datastreams of the pbm the fewer inputs the better because in this way the data requirements of the model are reduced this decision can be made based on domain knowledge or feature selection procedures 5 finally develop one or several ml models using the data resulting from the above steps evaluation is an important factor to verify that the created models are useful for operational tactical decision making a practical way to estimate the predictive capacity of the models is to compare their errors with a threshold based on domain expertise also the models should be tested for their generalization capacity a way to do this is to consider both sampled and unsampled locations for testing experiments where data from some locations are excluded from the model training sets and examine model performance in the excluded locations another evaluation aspect is to determine the appropriate training data size of the models the more variability a model has seen in its training data the more accurate prediction and generalization capacity it should have in the case where more data do not increase prediction performance it could mean that they do not add any variability and hence we do not need to generate much data in the future in our case data quantity is controlled by the amount of data that we generate with the pbm therefore an evaluation step could be to test models of different scales by including different amounts of locations years or other parameters 2 4 related work efforts to overcome the inherent shortcomings of pbms for operational decision making have been focused on combining pbms with ml through the concept of metamodeling metamodels also called surrogate or hybrid models refer to models which mimic the behavior of other models blanning 1975 ml metamodels have been used in agricultural and environmental sciences to cope with a variety of problems to instill domain knowledge to ml models the authors of karpatne et al 2017b train a neural network on pbm output using a custom loss function to predict the water temperature in lakes to reduce the long execution times of pbms metamodels have been employed to predict maize yield and compare the results with those of the pbms and ml models shahhosseini et al 2021 roberts et al 2017 to accelerate sensitivity analysis metamodels have been trained on the output of agricultural simulators gladish et al 2019 also hydrological metamodels have been evaluated for their performance in terms of speed and accuracy zhang et al 2020 villa vialaneix et al 2012 as well as generalization capacity domain adaptation in unsampled areas nolan et al 2018 likewise to extrapolate at regional and national levels metamodels have been deployed in environmental management ramanantenasoa et al 2019 lastly to work in situations where pbm inputs are not available the authors of shahhosseini et al 2019 create metamodels to predict pre season maize yield for decision support the aforementioned studies focus on each of the advantages of metamodeling individually whether it is domain knowledge imputation faster computation times improved generalization capacity over pbms or working with less data also most of these studies make an effort to create models that predict the variable of interest at any time of its evolution similar to what pbms do i e by predicting state variables for each simulation step in this work we introduce a generic method to exploit these advantages as well as to deal with data resolution problems which were not explicitly mentioned in those studies and also we do it for a specific point in time in the future of the target variable 3 methods 3 1 overview to assess the method described in 2 3 we performed a case study of grass pasture nrr prediction in different locations see fig 2 of new zealand the application of nitrogen along with environmental factors such as temperature and time of year greatly affects pasture growth gillingham et al 2008 so it is important to know the nitrogen response rate to examine the reliability of our models we performed a sampled and an unsampled location experiment in the sampled location experiment we assessed the predictive capacity of the models in cases where data from the testing locations are available in the unsampled location experiment we examined the generalization capacity of the models in cases where data from the testing locations are unavailable for both sampled and unsampled location experiments we iteratively considered each location to be a testing location to be able to better establish our verdicts to argue about the predictive and generalization capacities we used a case study specific example where we compared the models performance with a threshold that makes sense for crop practitioners also we created models of different scales by using various amounts of data for training and examined how data quantity included in training affects their performance 3 2 case study the target of our prediction was the expected two month nitrogen response rate nrr kg of additional i e compared to not applying any fertilizer of pasture dry matter grown in the two months after fertilizer application per kg of n fertilizer applied as in most countries pastures in new zealand suffer a chronic deficiency of nitrogen rotz et al 2005 whitehead 1995 and farmers apply nitrogen containing fertilizers to increase pasture growth rates clark et al 2007 pembleton et al 2013 nitrogen fertilizer can be applied regularly e g after each grazing event or more tactically to manipulate the supply of pasture available to feed stock as fertilizer costs increase environmental concerns about leaching of nitrogen increase and or the prices received for meat and milk decrease farmers become more interested in understanding when best to apply nitrogen fertilizer to obtain the best nrr current nrr estimators are based on rules of thumb that consider the month of year soil temperature soil nitrogen or pasture growth rate waikato regional council 2015 nz farm source 2021 dairynz 2012 there are pbms that can estimate nrr based on site soil properties pasture type and the prevailing conditions weather but they have limited usefulness as operational estimators of nrr because the weather for the two months after a proposed current or future application date are not known and such data are required to run the model also while there are some nrr data available from experiments they are sparse and not sufficient to train ml models 3 3 data generation we used apsim v7 10 r4191 apsim 2021 holzworth et al 2014 to generate the training and testing data pasture growth was simulated with the agpasture module li et al 2011 which has already been demonstrated to be a reasonable estimator of pasture growth in new zealand cichota et al 2013 2018 the range of input conditions covered eight contrasting locations in new zealand fig 2 and are given in table 1 pasture nrr is known to be influenced by soil water and nitrogen availability temperature and solar radiation the combinations of input conditions were designed to provide coverage across these variables along with 40 years of historical weather data from the new zealand virtual climate station network tait et al 2006 cichota et al 2008 which gave a rich source of variation in weather after fertilizer application a hyperspace of parameters was created using the full factorial of the input conditions and put into apsim the total number of generated simulations was 1 658 880 after removing the control simulations see table 1 1 382 400 remained 3 4 data preprocessing the data generated by apsim were processed to form a regression problem where the target variable was the nrr and the inputs were the weather treatment options regarding the fertilizer and irrigation and biophysical variables first the nrr was calculated at two months after fertilization for each non control simulation second from the generated daily data only the samples in a window of 28 days before fertilization were retained this window was selected because in the experience of the authors pasture loses memory of past conditions relatively quickly provided it is not under or over grazed weather data after the fertilization were also not considered as such data would be unavailable under operational conditions third the generated data were split into 80 20 training test sets based on years to avoid information leakage during the later stages of preprocessing the training and test sets included the year ranges 1979 2010 and 2011 2018 respectively fourth the weather and biophysical variables were aggregated using their weekly mean values finally only a subset of the variables was preserved this subset included weather variables simulation parameters soil water soil fertility irrigation fertilizer month fertilizer rate and biophysical variables produced by apsim above ground pasture mass net increase in herbage above ground dry matter potential growth if there was no water and no n limitation soil water stored from 0 to 300 mm soil temperature at 300 mm soil temperature at 50 mm herbage nitrogen concentration in dry matter these variables were preserved because they were considered to be likely drivers and also known prior to fertilization to ensure operational usefulness based on expert knowledge of the authors 3 5 model scale different models were created using different amounts of data we considered models on three scales local regional and national each including a different number of locations the criterion for selecting the locations differed based on whether the experiment was performed in sampled or unsampled locations in the sampled location experiment the locations were selected based on a climate matching process the degree of climatic similarity between sites was assessed using the climex match climates algorithm kriticos et al 2015 this algorithm produces a composite match index cmi from 0 to 1 which indicates the similarity between two locations in weekly average maximum and minimum temperatures total annual rainfall seasonal pattern of rainfall relative humidity and modelled soil moisture the required climate data were obtained for the nearest 0 05 location from niwa s virtual climate station network tait et al 2006 for the period 1979 to 2010 i e using data only from the training set the results were expressed as a matrix of pairwise cmis between all sites in this experiment the local model included data from the sampled location the regional model from the sampled location and the best two matches for this location and the national model data from all the locations in the unsampled location experiment the locations included in each model were selected based on minimum haversine distances from the testing locations the reason for not using climate matching with climex was the assumption that data from the unsampled locations were not available and as a result climate matching could not be performed the local model included data from the nearest neighbor of the unsampled location the regional from the three nearest neighbors and the national from all the locations except the unsampled one see fig 3 for a visualization of training models of different size and table a1 in the appendix for the locations included in each model 3 6 machine learning pipeline the models were developed with the random forest algorithm random forest was selected based on the results of preliminary exploration see table b1 in the appendix feature selection was not performed since we had only a few features which were all considered explanatory training data were standardized for each location and experiment and test set data were standardized with the corresponding scaler categorical variables like irrigation on off were converted to ordinal hyperparameter tuning was performed using bayesian optimization with 25 iterations and the 5 fold cross validation score as a metric for each iteration the tuned parameters can be seen in table 2 3 7 evaluation the predictive capacity of the models was evaluated using the root mean squared error rmse and the residuals of the models on a monthly and yearly basis a threshold of 5 kgdm ha kgn 2 2 kg of dry matter ha kg of nitrogen nrr was selected based on expert knowledge to investigate if the models were accurate enough from a practical perspective to test the generalization of the models rmse and residuals were also examined against the threshold of 5 in unsampled locations 3 8 experimental setup the data preprocessing stage was carried out utilizing the apache spark framework in standalone mode the machine learning models were developed using the scikit learn library in python the experiments took place in a computing node with an intel xeon e5 2630 v4 cpu and 120 gb of ram 3 9 software availability the code used for the case study of this paper can be found in https github com bigdatawur simulation assisted ml 4 results in the following sections we present rmse values and residual plots for sampled and unsampled locations the errors of the models fluctuated depending on model scale location month and year of application and whether the location was considered to be sampled unsampled none of the models proved to be universally better on all the locations or in both the sampled unsampled testing experiments however some of them showed higher performance and generalization capacity than others in certain cases 4 1 sampled locations experiment for the sampled location experiment regional models had lower rmses than the local and national in 4 out of 8 locations but the error differences between the models were smaller than 0 03 national models had the second best performance rmses for each model and location can be seen in table 3 prediction residuals are illustrated in fig 4 we observe that errors were mostly below the operational threshold of 5 kgdm ha kgn exceptions were the months january and february which showed errors close to 5 in some cases on a closer inspection we observed large fluctuations based on whether there was irrigation or not fig c1 in the non irrigated case we noticed that for january february and december the residuals were larger than our threshold of 5 kgdm ha kgn for the other months the performance was well below our threshold in the irrigated case the residuals took considerably smaller values on a yearly basis fig 5 the candles of the residuals were below 2 5 except for ruakura in 2016 and some years in lincoln which were higher than 2 5 but still lower than 5 separating the irrigated and non irrigated cases we found that the irrigated cases had residuals consistently lower than our threshold for the non irrigated cases fig c3 we observed that the years 2015 2016 had larger residuals in several locations 4 2 unsampled location experiment in the unsampled location experiment fig 4 we observed that the performance of the models generally decreased compared to the sampled experiment this decrease was more evident in lincoln and kokatahi while in the rest of the locations the differences are minor the regional models outperformed the national and local models in 4 locations fig 3 the performance of the regional models was close to that of the national models in many cases the only location where a local model outperformed the other two was in mahana from the residual plots on a monthly basis fig c2 we observed considerable variation in the residuals between the irrigated and non irrigated cases also we noticed that the interquartile ranges had been increased compared to the sampled locations especially for the local models and were higher than 5 in many occasions with the largest errors happening in lincoln from the residual plots on a yearly basis fig c4 we observed that the interquartile ranges had been increased compared to the sampled location experiment again the years 2014 2016 had the widest interquartile ranges with those of the lincoln local model displaying the largest errors except for those years we could say that the performance of each model is stable across the years for each location 5 discussion in our experiments the models captured in most cases sufficient variation from the data to achieve rmses lower than the threshold of 5 kgdm ha kgn this means that they could be potentially used in practical applications where weather data after fertilization are missing or data are on a lower resolution than those that apsim expects these results persisted in the unsampled location experiment thus providing evidence that the models are operational in locations where data do not exist to calibrate pbms as well as locations not included in the training set of the models in the following sections we interpret the results of the local regional and national models and discuss the models as a product of the proposed model development methodology 5 1 predictive capacity sampled locations when separately analyzing the irrigated non irrigated cases we observed that the lack of irrigation hindered the predictive capacity of the models the reason for this impediment is that when no irrigation is provided the weather conditions become the driving factor of the nrr because the grass relies solely on rain to grow therefore as several uncertainty factors pile up weather volatility nrr sensitivity to weather predictions two months in the future without knowing the weather the results are expected to deteriorate but they are not indicative of the general model performance the deterioration was sharper during the spring summer months november december january and february because irregular rainfall is most critical in these seasons also the performance degradation was not the same in all the locations since some locations have more favorable weather conditions than others comparing the models we observed small differences in model performance at first glance this seems counter intuitive since bigger models were trained on supersets of the smaller model data this means that they have the same information to learn from and thus they should perform at least equally well however this is not the case since the smaller models seem to benefit more from additional data from locations with similar climates than from having more data from locations with less similar climates regarding the national models they have somewhat higher rmses than the other two models because they include data from all the locations which makes them harder to adapt to local conditions the models showed good performance through each month of the year for the irrigated case interquartile ranges were mostly below 5 which means that 50 of the values lie within this range in different locations we see different months having the largest residuals this has to do with the variation in their microclimates since rainfall and temperatures can be disparate residuals went as high as 7 5 in lincoln which is characterized by low precipitation amounts as can be seen in fig d1 also we observed that the errors of the models are consistent throughout the years there is some variation for 2016 and 2018 in kokatahi and wairoa mainly due to local weather conditions and extreme events which the models were unable to capture this aligns with our expectation since extreme events are rare so there are only a few in the dataset and also because their presence may be imbalanced between the training test sets yet in most cases the model shows adequate predictive capacity even eight years after the last year that was included in the training set from the perspective of model operationalization the models proved that they can complement the pbms to provide predictions of adequate accuracy and overcome the problem of data availability to a certain degree this degree depends on the level of uncertainty involved in the predictions and the ml pipeline used to build the models in a digital twin these models could provide the first line against working with limited data several models could be included with different tasks for example a model providing predictions for the irrigated case of a specific location with a specific soil type one trained on extreme weather one on non irrigated cases and so on these ensembles of models could potentially capture a large degree of variation while waiting for more data to become available 5 2 model generalization unsampled locations in the unsampled location experiment the differences between the models became more evident as the local models performance deteriorated more than the others fig c2 the reason behind this difference is that the local models had data from only one location which was not the location where the testing happened on the other hand the bigger models were favored in this experiment since they included data from multiple locations and could extrapolate better this phenomenon is more noticeable in the non irrigated cases fig c2a where the local model shows high deviations from the simulated nrrs having said that with the exception of january february and december the rmses were below 5 for all models those three months included temperatures higher than 25 c fig d2 which can be harmful to the grass and when combined with the non irrigated case the uncertainty for the future increases on the monthly residual plots of the unsampled location experiment fig c2a we saw a more detailed picture of model performance with respect to size many times the residuals surpassed our threshold especially those of the local and regional models from these cases we can deduce that the national models are superior to the local and regional ones the cases where the national models had increased interquartile ranges happened on the same months and locations as in the sampled location experiment e g january february in ruakura february march in marton the latter observation means that the increased ranges are not a matter of hindered generalization among locations but of an inability to capture variability in those climates due to the features included in the models from the residuals on a yearly basis we observed that the errors are mostly consistent across the years in each location the local models showed the highest fluctuations throughout the years like in ruakura and lincoln the regional models had the second highest discrepancies throughout the years like in mahana lincoln the national models were the most stable ones this behavior can be attributed to the amount of data included in each model because the more data from different locations a model includes the more divergent weather conditions it has seen this means that it can generalize better in the weather conditions of the years to come also it is interesting to see that models can generalize in unsampled locations many years 8 later since the last year included in the training sets from an operational perspective the models showed a capacity to generalize in previously unseen conditions a recommendation we would make when starting modeling in unsampled locations would be to begin with a national model rather than a model from the single nearest similar location in digital twins where existing models cannot be applied due to lack of calibration data or insufficient observation training data these models can provide a first impression of variables of interest in the future even though there are still limitations again the model performance could be improved by training for more specific scenarios and using more advanced ml techniques 5 3 future work this line of research could be improved further by generating data from multiple pbms and by trying different aggregation levels to find a balance between performance and working with low resolution data also it would be beneficial to evaluate model performance against ground truth data which were not available for this case study regarding the case study the data preprocessing and ml procedures could be adapted to better fit the domain of the application by using custom features performing training test splits which better balance underrepresented phenomena between the sets or using stratified sampling to select which simulations are going to be included in each set more elaborated ml model architectures could further improve performance metrics 6 conclusions in this work we introduced a method to develop operational digital twins by creating models which overcome the problems of data availability and data resolution we showcased this method using a grass pasture nitrogen response rate case study experimental results verified that this method is able to produce digital twins to offer tactical advice in highly non linear situations where local conditions and treatment options affect the outcome of the predictions the ability of the models to provide accurate predictions in different locations for both sampled and unsampled experiments indicates that they can adequately capture the variability encoded in process based models the developed models were able to capture the target variable even without having the complete weather and biophysical time series this practically allows to develop operational digital twins in cases of limited data availability also model predictions were made on field level using weekly data instead of daily data that a process based model would require as a result digital twins using these models are capable of operating in situations where process based models cannot these advantages combined with the fact that we did not need to forecast any future weather values to get those results differentiate this method from the creation of metamodels which just summarize process based models and demonstrate that simulation assisted machine learning is able to offer advice in practical conditions declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests given his role as associate editor of environmental modelling software i n athanasiadis was not involved in the peer review of this article and had no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to the editor in chief d p ames acknowledgements funding this work has been partially supported by the european union horizon 2020 research and innovation program grant 810 775 dragon the wageningen university and research investment program digital twins and agresearch strategic science investment fund ssif emulation of pasture growth response to nitrogen application appendices alocations included in each model table a 1 location data included in each model the locations of the sampled location experiment were chosen based on climate similarity while the ones of the unsampled location experiment were based on haversine distance table a 1 target location scenario sampled location unsampled location local regional global local regional global waiotu waiotu waiotu wairoa ruakura all ruakura ruakura wairoa marton all except waiotu ruakura ruakura ruakura marton wairoa all wairoa wairoa marton waiotu all except ruakura wairoa wairoa wairoa ruakura waiotu all marton marton ruakura mahana all except wairoa marton marton marton mahana ruakura all wairoa wairoa mahana ruakura all except marton mahana mahana mahana marton ruakura all marton marton kokatahi lincoln all except mahana kokatahi kokatahi kokatahi waiotu wairoa all lincoln lincoln mahana wyndham all except kokatahi lincoln lincoln lincoln mahana marton all kokatahi kokatahi mahana wyndham all except lincoln wyndham wyndham wyndham marton mahana all lincoln lincoln kokatahi mahana all except wyndham bpreliminary machine learning algorithm comparison table b 1 the gridsearch and rmse results of different machine learning algorithms for training in ruakura and testing in waiotu with the yearly split mentioned in the text as a preliminary test to choose an algorithm the gridsearch parameters as denoted as found in scikit learn s documentation the parameters in bold are those that gridsearch selected for each algorithm table b 1 gridsearch parameters rmse random forest n estimators 100 200 max depth 3 7 12 min samples split 10 20 min samples leaf 10 30 max features 0 33 2 51 gradient boosting trees learning rate 0 05 0 1 0 2 n estimators 100 200 min samples split 10 20 min samples leaf 10 30 max depth 3 7 12 max features 0 33 2 52 linear support vector regression c 0 2 0 5 1 epsilon 0 05 0 1 0 2 loss epsilon insensitive squared epsilon insensitive 2 68 elastic net alpha 0 2 0 5 1 max iter 500 1000 2000 l1 ratio 0 2 0 5 0 8 2 69 support vector regression kernel rbf c 0 2 0 5 1 epsilon 0 05 0 1 0 2 2 78 multi layer perceptron hidden layer sizes 40 40 40 60 60 activation relu batch size 32 64 max iter 100 early stopping true n iter no change 20 3 98 crainfed vs irrigated plots fig c 1 monthly test set residuals of models for sampled locations in rainfed a and irrigated cases b fig c 1 fig c 2 monthly test set residuals of models for unsampled locations in rainfed a and irrigated cases b fig c 2 fig c 3 yearly test set residuals of models for sampled locations in rainfed a and irrigated cases b fig c 3 fig c 4 yearly test set residuals of models for sampled locations in rainfed a and irrigated cases b fig c 4 d weather plots fig d 1 average rainfall per month and location for the four weeks that we assume to have data fig d 1 fig d 2 average maximum temperature per month and location for the four weeks that we assume to have data fig d 2 
25682,despite widespread use of factor fixing in environmental modeling its effect on model predictions has received little attention and is instead commonly presumed to be negligible we propose a proof of concept adaptive method for systematically investigating the impact of factor fixing the method uses global sensitivity analysis methods to identify groups of sensitive parameters then quantifies which groups can be safely fixed at nominal values without exceeding a maximum acceptable error demonstrated using the 21 dimensional sobol g function three error measures are considered for quantities of interest namely relative mean absolute error pearson product moment correlation and relative variance results demonstrate that factor fixing may cause large errors in the model results unexpectedly when preliminary analysis suggests otherwise and that the default value selected affects the number of factors to fix to improve the applicability and methodological development of factor fixing a new research agenda encompassing five opportunities is discussed for further attention keywords sensitivity analysis factor fixing error measure adaptive method partial sort ranking 1 introduction complex models incorporating many processes and variables are increasingly used to represent real world problems to generate understanding and or inform management tscheikner gratl et al 2019 yang et al 2014 however complex models such as environmental models that describe biophysical and socio economic systems present a system identification problem guillaume et al 2019 in the sense that they contain many uncertain parameters that cannot be measured and need to be estimated on the other hand estimation is thwarted by the fact that there is rarely enough information in the input and output model variables to identify estimate those parameters uniquely guillaume et al 2019 the usual solution to this non identifiability is to make assumptions that constrain the space of parameter solutions parameter fixing pf is one approach that attempts to set parameters deemed insensitive to fixed values and then estimates the more sensitive ones from the available data more generally in an uncertainty quantification or exploratory modeling context other static inputs that do not need to be estimated from data are also commonly held constant maier et al 2016 to reduce the scenario space that needs to be explored guillaume et al 2012 given their similarity static inputs and parameters are often referred to collectively as factors and we therefore focus here on factor fixing ff generally model calibration and uncertainty analysis are common practices to inform selection of parameter values and their impact on predictions yet conducting accurate calibration and uncertainty analysis can be challenging when the computational cost of running a model is high moreover this challenge is amplified when the model has a large number of parameters becker et al 2018 yang et al 2018 because the number of model runs needed for accurate analysis typically increases with the dimensionality of the parameters though it could in some circumstances be possible to run such analysis over the full factor space with supercomputing infrastructure reducing the factor dimensionality by for example ff can benefit the further use of a model at the cost of requiring additional computation for ff analysis several advantages can be achieved which include 1 obtaining a reduced model that increases the computational efficiency of future model analyses by constraining the explored factor space 2 identifying redundant factors and decreasing unnecessary model complexity monari and strachan 2017 and 3 improving the estimation of sensitive factors using the available data factor fixing can address the issue of high dimensionality by excluding insensitive parameters from further analysis of central interest in this paper is the exclusion of those factors which have no effect or negligible effect on the quantities of interest qoi qois may consist of model predictions or some function of them relevant to the objectives of the modeling exercise e g calibration objective functions guillaume et al 2019 unimportant parameters can be identified using a variety of global sensitivity analysis gsa methods razavi et al 2021 which are implemented in numerous software packages douglas smith et al 2020 factor fixing or factor screening is therefore approached through identification of non influential parameters norton 2015 pianosi et al 2016 wagener and pianosi 2019 subsequent reductions in the number of model runs evaluations have been reported with ff including with respect to hydrologic models cuntz et al 2015 van werkhoven et al 2009 for example gou et al 2020 concluded that optimized results were improved by applying gsa to classify the importance of factors instead of fixing default factors as is usually recommended for the vic hydrological model calibration across china studies of applying screening approaches however have mainly focused on comparing the performance and efficiency of different methods cosenza et al 2013 jaxa rozen and kwakkel 2018 mara et al 2017 decreasing the computing cost becker et al 2018 cuntz et al 2015 jakeman et al 2019 and distinguishing sensitive and insensitive factors using sensitivity indices nossent et al 2011 sarrazin et al 2016 examples of method comparisons in the environmental modeling field include vanrolleghem et al 2015 who concluded that the morris screening and extended fast methods showed substantial agreement with respect to ff khorashadi zadeh et al 2017 who identified the same unimportant factors by sobol and pawn approaches in an application to the swat model and koo et al 2020 who used the extended fast method on a swat model to determine the relative effect of 15 factors on model uncertainty the selection of a gsa method whatever its purpose depends partially on the computational budget whereas the best performing measure depends on the model becker et al 2018 given that computing time can be prohibitive for assessing sensitivity and uncertainty in high model dimensionalities another important consideration is using more efficient sampling techniques such as multi criteria trajectory based sampling khare et al 2015 and sequential screening strategies cuntz et al 2015 before comparing different methods and applying more efficient sampling techniques it is vital nevertheless to separate influential and non influential factors successfully approaches developed for such separation can be classified into three types screening thresholds grouping techniques and statistical testing use of a screening threshold appears to be the most commonly adopted approach and involves classifying a factor as insensitive if the sensitivity index is below a pre specified threshold for variance based gsa methods factors responsible for less than a certain threshold of the variance e g total sensitivity index 0 01 0 1 or 0 2 are often screened out cleaves et al 2019 hübler 2020 vanrolleghem et al 2015 wang et al 2010 the threshold for the morris method in contrast is often case specific in general thresholds when set should have values that are highly related to the qois examined another way of determining the screening threshold involves adding a dummy factor a variable that is not used in the model and any factor for which confidence intervals cis of its sensitivity index overlaps with that of the dummy factor is identified as insensitive khorashadi zadeh et al 2017 screening thresholds for fixing non influential factors however may not be sufficiently indicative of actual errors in qoi s for a qoi not related to output variance it may not be possible to derive an associated error estimate from sobol or other variance based sensitivity indices moreover error estimates are generally obtained by marginalizing over factor values instead of considering the default value at which the factor will actually be fixed in practice the difference between these error estimates may be further accentuated when multiple factors are fixed grouping techniques categorize factors into pre determined groups based on considerations such as expertise and experience of the problem or model intuition the pre specified threshold for sensitivity indices or into objectively optimized groups using an automatic approach hou et al 2015 saltelli et al 2007 sheikholeslami et al 2019 those factors in the insensitive group are then treated as constants in the model statistical testing methods build on the screening threshold approach by evaluating whether the entire set of factors associated with what are called the unconditional outputs is reliably divided into sets of influential sensitive factors i e the associated conditional outputs and non influential factor space sarrazin et al 2016 used the statistical significance of the kolmogorov smirnov ks test to validate whether factors were indeed insensitive with the consideration that the correlation coefficient of types in for example branger et al 2015 nossent et al 2011 and tang et al 2007 may still be high when unconditional and conditional outputs follow a non 45 straight line sarrazin et al 2016 the ks statistic used in sarrazin et al 2016 is a more conservative test as it uses the most sensitive location of a non influential factor for validation they repeat this test with the factors considered fixed at a variety of values rather than explicitly analyzing the representativeness and effect on predictions resulting from the default values used for fixing visual techniques are also employed to aid in identifying factors to fix garcia et al 2019 they tend to informally use a combination of approaches including screening retaining the top important factors and factor grouping given that there is uncertainty about the screening threshold the number of factors to retain and the separation of groups traditional factor screening is more qualitative identifying factors to fix with fewer model runs song et al 2015 and lacking a sufficient indication of errors induced in qois one approach is to approximate the error from setting unimportant factors to the average values over the factors distributions using the analysis of sobol et al 2007 as described in jawad et al 2015 on the whole there are remarkably few studies that explicitly determine the set of factors to fix on the basis of the errors caused in the prediction s of qois there is an identified need for a theoretical error analysis approach to factor fixing alexanderian et al 2020 and methods are needed to answer the fundamental question can all insensitive factors or any of the weakly influential factors be set to default values with model performance remaining acceptable the resulting challenge is the high computational cost required to evaluate errors induced by factor fixing because of the need to compute both conditional and unconditional qois i e with and without factor fixing it is therefore necessary to reduce the number of evaluations needed by testing the fixing of as few unique combinations of factors as possible a further consideration is that sampling based sensitivity indices are uncertain due to numerical errors sampling variation and the approximation of sensitivity indices the full convergence of indices is not always possible even with a large sample size nossent et al 2011 sarrazin et al 2016 which is a challenge for threshold based factor fixing on the other hand studies generally show that factor rankings are less variable and converge faster than sensitivity indices sarrazin et al 2016 sheikholeslami et al 2019 wang et al 2013 in this paper we therefore explore the effects of factor fixing with a proof of concept adaptive process able to 1 systematically quantify the error in a nominated qoi with given default values for factors and 2 identify the number of factors that can be fixed without exceeding a maximum acceptable error whilst 3 minimizing the number of evaluations required for acceptable error an uncertainty based partial sort approach via bootstrapping is developed for factor ranking to reduce the number of model runs used for gsa and increase the robustness of rankings factors are fixed in groups to decrease the frequency of error estimation by identifying the smallest number of unique factor combinations to fix treating model predictions with all factors varying as unconditional values and model outputs with some factors fixed as conditional values we measure the differences between unconditional and conditional outputs using multiple metrics error measures are evaluated preliminarily using a small sample set to define factors that are safe to fix or to vary a larger number of model runs is conducted if it is difficult to decide whether to fix a factor group due to the large uncertainty in the error measures the largest set of factors that can be fixed whilst keeping errors under a given threshold is then determined in an adaptive fashion the method seems widely applicable and can be performed so as to minimise the number of times that errors need to be estimated section 2 presents an intuitive example of the effect of fixing factors on the predictions of qois the proposed method is elaborated in section 3 and then applied in section 4 to the commonly used sobol g function with two gsa methods being used to generate the sensitivity indices described there the interpretation of results including evaluating the effects of sensitivity from different gsa methods error measures and default values to fix factors is given in section 4 this is followed by discussion of emerging opportunities within a new research agenda on the topic of ff in section 5 our conclusions are given in section 6 2 the effect of fixing factors on predictions overall the impacts of fixing factors on model predictions can be treated as a source of bias fixing truly non influential variables would theoretically not affect the model outputs over the factor space however factors screened as non influential often have non zero sensitivity which will still cause some error in predictions leading to a reduction in the precision of predictions across factor space compared with model results obtained without factor fixing fig 1 provides an example of the potential effects of factor fixing on predictions the left plots fig 1 a and c depict the contours of the function f x 1 x 2 w x 1 x 2 2 where x 1 and x 2 are uniform variables on 0 1 and x 1 is sensitive when w 0 5 and insensitive for w 0 the effect of fixing x 1 on the outputs from f is shown for two different values of x 1 vertical lines in cyan and red colors for x 1 0 50 and 0 01 respectively when x 1 is set to 0 50 or 0 01 the two factor model is reduced to a one dimension model f x 1 x 2 x 2 x 1 0 50 or f x 1 x 2 x 2 x 1 0 01 and the value f 0 70 0 50 marked by the black dot is approximated by f 0 50 0 50 or f 0 01 0 50 marked in cyan red as shown in fig 1 a and c when x 1 is insensitive top panel the probability density function pdf of the outputs by f x 1 x 2 x 2 x 1 0 50 and f x 1 x 2 x 2 x 1 0 01 are identical to that of f x 1 x 2 indicating that fixing x 1 has no effect on f as plotted in fig 1b when x 1 is sensitive bottom panel the pdf of f depends significantly on the value at which x 1 is fixed fig 1d whether or not the error induced by fixing x 1 is important depends on the modeling objective and accuracy requirements the true mean of f is 0 67 and if we set x 1 0 50 the mean of the conditional function f x 1 x 2 x 2 x 1 0 50 is 0 64 which may be a reasonable approximation however if there is concern for predicting small extreme values fixing x 1 at 0 50 would be inappropriate the pdf in fig 1d shows that when x 1 0 50 the model is incapable of predicting values of f near 0 and when x 1 0 01 it cannot predict large values this low dimensional example demonstrates that fixing a factor can lead to a significant error when the factor has non zero sensitivity for high dimensional cases the projection in factor space implied by fixing factors cannot be visualized as easily which is why a computational analysis to estimate errors is needed 3 methods for evaluating effects of fixing factors the goal of our proposed algorithm is to determine the largest set of unimportant factors that can be fixed without the error in the qoi of the resulting lower dimensional model exceeding an acceptable threshold we fix the factors in groups to reduce the number of times that error measures need to be evaluated the groups are ordered by increasing sensitivity and the least sensitive groups are fixed first the groups emerge by partial sorting of factors according to confidence intervals of their ranking obtained through sensitivity analysis if groups are found to be too coarse the computational budget is increased to refine them this adaptive approach also reduces the number of model runs needed for sensitivity analysis the applicability of the method is illustrated with two gsa techniques sobol and morris methods three contrasting metrics are selected to evaluate errors due to factor fixing relative mean absolute error rmae pearson product moment correlation r and relative variance rv the error measures are evaluated adaptively with a preliminary estimate at an initial small number of model runs to screen out those partial groups for which fixing would either result in errors far beyond or below the given error threshold algorithms 1 the adaptive method for factor fixing image 1 algorithms 2 function partial of factors image 2 algorithms 3 function for error calculation caused for factor fixing image 3 algorithms 4 function for checking convergence image 4 algorithms 1 4 algorithms for the proposed adaptive method for factor fixing and sample size determination given a selected gsa method and model algorithm 1 covers the overall adaptive procedure with algorithms 2 4 describing the three main steps involved 3 1 an adaptive method for simultaneous factor fixing and sample size determination the adaptive method is specified in algorithms 1 4 as pseudo code algorithm 1 describes the overall procedure and algorithms 2 4 emphasize the three main steps involved partial sorting of factors error calculation and checking of convergence model input factors x the quantity of interest y and other parameters requiring determination for the adaptive analysis are identified in lines 1 5 of algorithm 1 the smallest and largest sample sizes to consider n s t a r t and n s t o p respectively and the number of samples by which to iteratively increase n s t e p the sample size indicate the number of model runs to be used and depend on the given gsa method as well as the computational budget the confidence level p is used to calculate the confidence intervals of factor rankings error measures η should reflect the requirements of the problem and relevant interest groups as discussed in section 3 3 the symbol ε denotes the maximum error measured by η e g 5 change of the variance of model outputs that is considered acceptable in the model outputs the matrix x t denotes samples generated for evaluating the errors with the sampling technique s which may be different from the samples used for sensitivity analysis the x t for a small sample size is generated first leading to an initial estimation of η checking of convergence requires specifying the number of repetitions λ to establish stability of results as the sample size of the sensitivity analysis increases with an initial n s t a r t samples l12 19 the selected gsa method for example the morris method is used to generate confidence intervals of factor rankings using bootstrap resampling algorithm 2 l2 6 see section 3 2 for more details factors are then partially sorted such that they form groups g ordered by sensitivity algorithm 2 l7 12 algorithm 3 then calculates errors by fixing factors in groups that are stable with the least sensitive group first the group with second lowest sensitivity will then be fixed together with the previous group followed by the calculation of error measures the process is repeated until the error exceeds the acceptable threshold the sorting of factors into groups will depend on the sample size used as part of the gsa method larger sample sizes are better able to identify the ranking of factors splitting factors into smaller groups but at a higher computational cost an adaptive method is therefore used to simultaneously determine the factors to fix and assess whether increasing the sample size would be beneficial in producing more stable partial groups with convergence evaluated by algorithm 4 the combination of factors for fixing that remains unchanged over at least λ consecutive iterations is treated as being stable and indicative of increased confidence in results the larger the λ is the more robust the partial groups are but the more model runs are needed the pseudo code in lines 5 15 of algorithm 4 partitions factor groups into three main sets if the error e i in e is small and acceptable factors in the corresponding group g i and less sensitive factors are included in the fixing set g f i x alternatively if e i is large caused by fixing a group of highly sensitive factors these factors will be added to g v a r y and will no longer be considered for fixing typically sensitivity analysis with more samples is required to resolve the moderately sensitive factors so as to determine whether a subset of the factors within such groups can be fixed reliably the group needing more samples for enhanced resolving is denoted as g s a m p l e if no more factors are in this group convergence is then achieved during the adaptive process the user can also decide to adjust ε considering the trade off between the increased computation and the acceptable error increasing the size of x t may also be considered if it is impossible to decide to which set g v a r y g s a m p l e or g f i x a factor group belongs due to the large uncertainty in the initially estimated η in the adaptive process factors are not fixed in the sensitivity analysis so that samples from the previous iteration can be reused error calculations for groups of factors can be cached rather than re evaluated at each iteration overall two criteria have been included to improve confidence in the robustness of the result robustness in factor ranking through uncertainty based partial rankings and the stability of results when increasing the sample size λ times razavi and gupta 2016 sarrazin et al 2016 3 2 partially sorting factors using bootstrapping traditionally the sensitivity rankings r r 1 r i r d of d factors x x 1 x i x d in a model are generated by referring to sensitivity indices informed by n data points over the factor space using one gsa method factors are ranked least sensitive to most sensitive so that a lower value of ranking r i for factor x i here represents a smaller sensitivity uncertainty in the ranking r is expected due to the variability of sampling garcia et al 2019 sheikholeslami et al 2019 and subsequent approximation of the sensitivity indices the ranks r i may vary within certain ranges during bootstrapping to obtain converged factor rankings additional samples are needed to better define the ranking of that factor we can account for the uncertainty in rankings via partial sorting as an example consider three factors x 1 x 2 x 3 of a model the confidence intervals cis of the factor rankings calculated with the percentile method being distribution free and easy to implement are shown in fig 2 details of the calculation of cis are described below at sample size n1 factors x 1 and x 3 are classified into the same group that is less sensitive than x 2 due to the cis of rankings both ranging from 1 to 2 with an increased sample size n2 the three factors are instead ordered as x 1 x 3 x 2 the steps needed to partially sort factors in algorithm 2 are explained below step 1 determine the bootstrap resampling sample size m and the confidence level p step 2 conduct bootstrap resampling with replacement m times for each resample k k 1 2 m compute the sensitivity rankings with use of a gsa method for each factor the distribution of the rankings is then obtained with a total of m estimates step 3 calculate the confidence intervals of factor rankings at confidence level p e g p 0 05 leads to cis of 95 for d factors form the cis as r c i r 1 l r i l r d l r 1 u r i u r d u with u denoting their upper bound and l their lower step 4 partially sort factors by constructing a directed acyclic graph describing the rank order of pairs of factors using r c i calculated in step 3 l8 10 in algorithm 2 this is analogous to a dependency graph specifically find all factors x j that satisfy r j l r i u j 1 2 d j i for each factor x i and use x l i s t i to represent x j r j l r i u j i factors not in x l i s t i are in the same group as x i due to the fact that their ranking cis overlap with that of x i using the collection of ordering relations x i x l i s t i topological sorting then separates factors into totally ordered groups which is undertaken here with the toposort python package smith 2016 the level of confidence represents the robustness of factor rankings at a user defined value bearing in mind the computational budget 3 3 evaluation of metrics of error due to factor fixing 3 3 1 description of metrics of errors based on existing literature bennett et al 2013 nossent et al 2011 tang et al 2007 three measures η are used in this paper to capture different potential influences on model outcomes caused by fixing factors as illustrated by fig 1 relative mean absolute error rmae is an equal weighting metric for residuals i e calculates the difference between two data sets point by point herein conditional and unconditional outputs of interest and easy to interpret in terms of acceptable error the pearson product moment correlation r describes the extent to which the two datasets are linearly correlated ranging from 1 to 1 a value of 1 for r implies a totally positive linear correlation between two datasets and can be obtained if the two data series only have a constant offset relative variance rv is a direct value comparison method that compares the two data sets in terms of their variance being a summary characteristic of the spread in the data equations 1 3 express the formulae for these three error measures with use of x t mentioned in section 3 1 1 rmae k 1 n y t k y c k k 1 n y t 2 r k 1 n y c k y c y t k y t k 1 n y t k y t 2 k 1 n y c k y c 2 1 2 3 rv k 1 n y c k y c 2 k 1 n y t k y t 2 where n is the size of x t k k 1 2 n is the sample index x c k denotes the sample point x t k with some factors fixed y t k is the unconditional value at the sampling point x t k with no factors fixed y c k is the conditional modeling result at x c k and y t and y c denote the average of y t and y c over all sampling points each time a combination of factors is fixed n model evaluations need to be run to generate the conditional outputs y c k and the same model outputs can then be used to calculate different error metrics e g different statistics of a time series when no factor is fixed r and rv will be 1 the threshold for errors is set here to be 5 which means the acceptable r and rv 0 95 n is denoted as n y to represent the size of y t so as to distinguish the sample size n in gsa methods in the following sections before continuing we remark that our method can be used with a large class of error measures including but not limited to those quantifying changes to the variance the choice of this measure should be dictated by user concerns and the modeling requirements including the purpose of the exercise for example if there is concern about the sensitivity of tail events to input parameters then the metric used should reflect this if the distribution of the model output is symmetric the variance can give a good indication of the variability of a model output however if the distribution is asymmetric other metrics based upon measures such as the probability of failure or conditional value at risk should be used the determination of an admissible error threshold is crucial and needs to be contextualized according to the purpose of the modeling for example if a high accuracy of predicting extreme events is sought then this may require a small error in conditional value at risk 3 3 2 determine sample size needed for error measures evaluation all the error measures are calculated by sampling of factors resulting in variation between samples as mentioned in the previous section conditional model outputs y c require the same number of model runs as y t when fixing each unique factor set thus the total number of model evaluations needed for calculating error measures is n n y z 1 where z is the number of combinations of factors to fix to achieve a balance between the computational cost n model evaluations and the accuracy of error measures produced error measures are assessed adaptively initializing n y to a small value uncertainties of error measures are quantified with both bootstrap and replicate sampling techniques respectively noting that this is a separate uncertainty quantification step to the bootstrap used for partial ranking of factors for the replicate sampling method the sobol sequence is adopted given that quasi monte carlo methods can outperform the standard monte carlo hou et al 2019 a total of m independent factor sets x t i of size n y are generated where i 1 m the calculation of error measures e g rmae is then performed m times resulting in m independent estimates of the error measures e i n y i 1 m the average of e i n y i 1 m is then 4 e n y 1 m i 1 m e i n y and the standard deviation sd of the e n y is 5 s d e n y 1 m 1 i 1 m e i n y e n y 2 it is often appropriate to select m to be of a small value leading to n n y z 1 m model evaluations for z combinations of factors to fix and m independent realizations a value of m 20 is used in the paper the 95 confidence intervals of each error measure can be written as e n y 1 96 s d e n y e n y 1 96 s d e n y bootstrapping on the other hand uses monte carlo sampling given the risk of bias associated with resampling of quasi monte carlo sequences the mean and the standard deviation of error measures are calculated with equation 4 and equation 5 here m b represents the number of bootstrap resamples in the equations with m b 1000 used in this paper error measures e n y calculated from a small n y may result in a large variation but could still provide valuable information if the upper bound of an error measure is below beyond the error threshold ε the corresponding factor set is to fix vary 3 4 gsa methods two gsa methods are chosen sobol morris to illustrate the partial sort method and to explore the differences due to their different perspectives on measuring sensitivity note that the proposed parameter fixing approach can be utilized with any gsa technique for which confidence intervals on rankings can be calculated our focus is on demonstrating the effects on the qoi with the proposed adaptive framework determining the best gsa method is problem dependent and beyond the scope of this paper 3 4 1 morris screening method the morris screening method morris 1991 is a commonly used gsa method for evaluating the effect of varying factors locally one at a time oat at repeated points across factor space it is efficient in factor screening and well suited to high dimensional cases as can occur with environmental models herman et al 2013 jia et al 2018 sun et al 2012 in morris the overall effect of a factor is measured here by the sensitivity measure μ which is the average of the absolute elementary effects ees computed at points that are evenly distributed over the factor space for the parameter input at different levels campolongo et al 2007 following the sampling strategy suggested by morris n trajectories containing d 1 points d is the number of factors are generated resulting in a total n n d 1 model evaluations ee and μ are calculated according to 6 e e i k f x 1 k x 2 k x i k δ i k x d k f x 1 k x 2 k x i k x d k δ i k 7 μ i 1 n k 1 n e e i k where n is the number of trajectories δ i k is the size of variation for factor x i at trajectory k and determined by the number of levels used and μ i is the sensitivity measure for factor x i values of μ are highly related to the choice of factors and qois such that they generally should not be compared across applications herein factors were ranked according to μ it is also common to consider the standard deviation of the ees as a measure of non linearity and factor interactions or some combination of the mean and standard deviation 3 4 2 sobol sensitivity analysis the sobol sensitivity method a variance based sensitivity analysis technique evaluates influences of factors by decomposing the unconditioned variance of the model results into contributions from individual factors and their interactions saltelli 2002 sobol 2001 consider the model as a function 8 y f x f x 1 x d where y or some property of it is the qoi and x x 1 x d is the set of factors denote the unconditional variance of f as v y following the sobol method v y can be decomposed as 9 v y i 1 d v i i 1 d 1 j i 1 d v i j v 1 d where v i is the amount of variance explained by an individual factor x i and v i j is the contribution of the pairwise interactions of factors x i and x j higher order interactions are also typically included the most commonly used measures of sensitivity are first order and total order indices that are defined in equation 10 and equation 11 respectively as 10 s i v i v y 11 s t i 1 v i v y where v i represents the variance explained by all factors except x i the first order index s i also called the main effect quantifies the contribution of an individual factor x i to the variance of the qoi s t i the total order index measures the total effect of x i including its interactions with other factors and is therefore more conservative for factor screening compared with s i sobol indices are commonly approximated by numerical integration due to the complexity of highly non linear models given that μ can be a better proxy of the s t i campolongo et al 2007 than μ the average of ees in this paper factors were ranked according to s t i using the saltelli sampling method saltelli 2002 which is an extension of sobol sampling the formula adopted for calculating total order effects is the jansen method jansen 1999 and a total of n d 2 model evaluations are required to calculate both s i and s t i salib an open source python package douglas smith et al 2020 herman and usher 2017 was used to conduct morris screening and sobol analysis 4 application and results 4 1 application to sobol g function given the focus of this paper on providing a proof of concept for adaptively evaluating errors due to factor fixing an example function is selected that is simple with known behavior of its factor sensitivities we have thus selected the commonly used mathematical test function the sobol g function which is defined as 12 y x 1 x 2 x d i 1 d 4 x i 2 a i 1 a i where factors x i i 1 2 d are uniformly distributed over a 0 1 d hypercube and the a i are non negative constants here y is our predictive qoi low values of a i result in a large main effect for x i more detailed explanations of the sobol g function can be found in saltelli et al 2010 some 21 coefficients which can be grouped into four influential groups have been selected following sheikholeslami et al 2019 unless otherwise stated when factors x i are fixed they are set to 0 25 from equation 12 this removes the ith parameter which also causes the least error while conducting factor fixing as will be shown in section 4 5 below see table 1 for the coefficients used in this study for the 21 dimensional sobol g function to obtain the initial estimate of error measures 10 data points of each independent sample set were generated using the sobol sampling technique for the replicate sampling samples used in sobol sensitivity analysis are included for the error calculation 100 data points were first sampled over the factor space by a monte carlo sampling approach and resampled for a bootstrap analysis factors were fixed from the least sensitive group with groups of higher importance included gradually as described in section 3 the number of bootstrap resamples for both morris and sobol was set to m 1000 to obtain non parametric bootstrap based cis on rankings at confidence level 0 05 with the morris method the bootstrap is conducted with use of ees from the n trajectories a new sample of n trajectories is selected randomly m times with replacement with each bootstrap resampling providing estimates of μ for all factors for sobol sensitivity analysis the resampling is repeated m times from the model output samples and sensitivity indices calculated for each bootstrap the 95 cis of rankings can then be generated for both sensitivity methods the trajectories used for the morris method started from 20 in total n s t a r t 440 model runs given 21 factors with an increment of n s t e p 210 model runs 10 trajectories the number of levels is 4 as recommended in the literature campolongo et al 2007 in sobol sensitivity analysis the sample size started from n 200 and increased with a step size of 200 n s t a r t n s t e p 4600 model runs while gsa parameters could be further optimized based on an understanding of the test function it is treated here as a black box 4 2 adaptively identify factors for fixing using rmae the results of the adaptive process are shown in fig 3 for the error measure rmae using the morris screening method and bootstrap to quantify the uncertainty in error measures rmae was initially estimated with 100 samples n y the number in the first column indicates the total number of factors fixed to calculate rmae in each row upper bounds of the 95 confidence intervals of rmae are presented and used to determine whether the error is acceptable the sensitivities of the factors increase from the bottom group to the top one factors partially sorted into a group were fixed together as rankings within a group cannot be further discriminated because the 95 cis of rankings overlap accordingly one rmae value is obtained for each group and is presented in the top cell of each group block with the remainder of the cells left empty as explained in section 3 3 1 one error evaluation is conducted for each combination of fixed factors in order to reduce the number of model runs the error measure is calculated when the partial groups are stable given λ 3 no rmae is produced for 20 and 30 trajectories colored in grey in fig 3 readers can also refer to table a2 in the appendix for cis of factor rankings resulting from 20 morris trajectories the changing colored blocks demonstrate the adaptive process to determine fixed factors and the sample size needed for sensitivity analysis generally increasing the number of morris trajectories results in better discrimination of the factor rankings resulting in more and smaller groups in fig 3a three partial groups containing 6 factors in blue 4 factors in green and 11 factors in cream were identified as stable at 40 trajectories an rmae 1 83 from fixing the bottom six factors in blue was below the acceptable rmae error threshold of ε 5 thus the least sensitive factors are reliably fixed g f i x when up to 10 factors are fixed the resulting rmae is 7 83 above 5 as described in algorithm 4 in section 3 1 more trajectories are then needed to split the four factors in green into smaller groups to ascertain whether more factors can be fixed denoted as g s a m p l e the same percentage in a row is due to the same combination of factors being fixed a sign of stability also note that no additional calculation is needed when there are repetitions of the same values induced by fixing the same factors at 80 morris trajectories the partial rankings of another three factors shown in orange out of the green block appeared to be stable so the error is evaluated with an estimate of 6 40 for the 95 percentile of rmae due to the rmae being estimated with n y 100 the uncertainty of rmae can also be overestimated the rmae from fixing 9 factors is then calculated with more samples so as to determine whether those three factors in orange are safe to fix fig 3b plots rmae as a function of increasing sample size where a declining uncertainty is observed rmae generally ranges from 4 5 to 6 with sample size n y from 400 to 1000 i e its upper bound still exceeds the threshold of 5 at this point we need to decide whether to retain our a priori decision to limit error to 5 or to update our target error in order to avoid increasing the sample size further for sensitivity analysis that is there is a trade off between further computational cost and acceptable error changing a threshold can be a rational practical response in such decisions involving trade offs and even more so in adaptive decision making situations therefore for illustration we update the threshold to 6 for the remaining analyses the three factors in the orange block in fig 3a will then also be fixed g f i x similar results are obtained using the replicate sampling technique see figure a1 in appendix though this requires a significantly larger number of model runs to achieve the conclusion that 9 parameters are safe to fix note that the sample size required for error evaluation is case specific and also affected by the design of the numerical experiments e g the sampling technique and the approach for calculating uncertainty at this stage we have a preliminary determination of the number of factors to fix after evaluating the error for three unique factor combinations which results in three unique error estimates in fig 3a therefore this method compares favorably to fixing all factors in increasing order without considering uncertainty in the ranking nine error evaluations of error measures or 2 million factor fixing settings 221 if all combinations of factors were tested without using sensitivity information at all 4 3 comparison of the sobol and morris methods for each of the two gsa methods the sample size needed was determined following the adaptive method described in section 3 1 the additional group of factors with acceptable error referred to as the orange block in fig 3 were identified with n 800 n 18 400 λ 3 samples using the sobol method partial sorts for each gsa method n 1760 for morris and n 18 400 for the sobol method are shown in table 2 factors in a group are clustered in the same cell and sensitivities of factor groups increase from left to right partial sort rankings using morris and sobol methods are similar but there are differences in the composition of partial groupings a group containing factors x 18 and x 20 is separated as well as the group of x 11 and x 10 when using the sobol method the factor x 9 is in a single group for morris and less sensitive than factors x 11 and x 10 which is different from what the sobol method defines the two groups however did not affect the determination of which factors to fix the same nine factors i e x 21 x 20 x 19 x 18 x 17 x 16 x 15 x 14 x 13 were identified as being the least sensitive and safe to fix by the two gsa methods the morris screening method needs much fewer samples whilst yielding similar ranking as is well known even if optimizations of the gsa methods may further reduce the number of model runs required for both methods 4 4 comparison of different error measures we use rmae r and rv to measure the performance of the dimensionality reduced sobol g function in addition the decrease in variance normalized by the total variance is also analytically calculated for comparison which is denoted as decrease in variance in fig 4 this is analogous to the relative variance error metric rv the total variance of the reduced sobol g function can be obtained analytically using the formula for the 21 dimension function where the fixed factor plays the role of a multiplier this produces an independent measure of decrease in variance as discussed in section 4 3 sobol analysis with n 800 resulted in more and smaller partial groups following the corresponding factor rankings by the sobol method the three error measures and the decrease in variance are visualized in fig 4 excluding factors with rmae greater than 50 where the vertical lines indicate confidence intervals for each error measure with n y 1000 samples and m b 1000 bootstrap replicates changes in r increase dramatically when up to 17 factors are set to 0 25 if r was used to measure the error induced by using the simplified sobol g function 12 factors would finally be screened for factor fixing the change in r is then under 6 in contrast rmae is nearly 20 and the upper bound in the changes of rv is also above 6 the decrease in variance is within the 95 cis of rv indicating that sobol indices provide expected changes in the variance of qois the other error measures could not be directly calculated from the sobol indices direct evaluation of error is necessary and particularly important in this case as rmae is the first to exceed the acceptable error threshold the large uncertainty in rv from fixing 9 10 and 12 factors requires a more precise estimation with more model runs considering that the mean of errors is far below the threshold while the upper bounds of confidence are beyond it fig 5 plots the variation of all three metrics along with increased sample size n y where rv shows the highest variation and r the smallest the large variation in the mean of 1 rv and 1 r with n y from 100 to 1000 suggests that the corresponding samples did not cover the factor space adequately and caused biases to the estimation of error measures as samples are reused as the sample size increases this bias then decreases gradually as might be expected different error measures can necessitate different sample sizes we can see that correlation r shows an acceptable error 6 even with low sample sizes in the case of rv n y of up to 2500 is required to determine that fixing 10 factors yields an acceptable error which is much larger than the number of model runs required by the other two error measures rmae rises rapidly as the number of groups fixed increases yielding nine factors that could be fixed as described in section 4 2 this is consistent with the finding of sarrazin et al 2016 that r could still be high even if the difference between each pair of conditional and unconditional outputs may be large therefore the residual performance measure rmae dominates the determination of the fixed factor set for the sobol g function of course in practice the choice of error measure would also be dictated by the objectives of the modeling exercise 4 5 default value evaluation the choice of default values for fixed factors can affect errors in qois and therefore those factors that can be reliably fixed as illustrated in section 2 fig 6 shows the effect of the selected default values on the three error measures for the sobol g function based on partial sorting using the morris method with n 80 in section 4 2 we concluded that factors in two blocks in blue and orange could be fixed if 0 25 was treated as the default value here in order to assess the effects of default values on the decision multiple default values for variable x were selected ranging from 0 0 to 0 5 the response from 0 5 to 1 is similar due to the symmetry of the sobol g function more values were sampled between 0 20 and 0 30 to refine the shape of the curve sobol et al 2007 provide a similar analysis using squared errors normalized by the variance but with a larger number of factor values the error measures rmae rv and r respond quite differently when the default value of x is varied group 1 and group 2 in different colors exhibit a similar trend for each measure while as expected errors from fixing group 1 are much smaller overall for x 0 5 the closer x is to 0 25 the smaller rmae is this indicates that selecting 0 25 as the fixed value maximizes the number of factors that can be fixed for a given error threshold when x was set to 0 0 or 0 5 the rmae value due to fixing factors in the non influential group alone group 1 can even reach 7 4 however changes in rv decrease with x close to 0 25 and rv is within 1 0 0 05 if the default x ranges from 0 2 to 0 3 when fixing group 1 in common both rmae and rv will decrease by reducing the applicable range of factors to for example 0 2 0 3 rv could be larger than 1 this is caused by the sobol g function being the product of linear effects of each variable let f z 1 z 2 g z 1 h z 2 and g and h have unit variance and zero mean the variance of f is then just the product of the variances of g and h i e 1 if we fix z 2 at some value w such that h w 1 then the variance v f z 1 z 2 w v f z 1 z 2 considering that decrease in variance is within the 95 cis of rv with 0 25 as the default values in section 4 4 the decrease in variance therefore is not a sufficient measure of the effects caused by fixing factors at specific values this is because typically used variance based metrics marginalize out the insensitive parameters but in practice the variables are not marginalized but fixed this means that the contribution of a factor to the total variance estimated by these approaches can be misleading interestingly r remains almost the same for each group indicating that r is not affected by different default values which could be explained by the property that the sobol g function is the product of linear effects of each variable therefore r might result in more factors being fixed if used as the only error measure in summary the effect of default values suggests measuring the errors due to factor fixing at a fixed position may cause large error even when the expected sensitivity of fixed factors is small developing an efficient algorithm for choosing the best values to use for fixing is interesting but beyond the scope of this paper 5 discussion 5 1 comparison of errors using different screening rules in this paper we have discussed the effect of factor fixing on model quantities of interest and introduced an adaptive strategy for factor screening the screening is based on evaluation of model errors using a partial sort strategy to consider the uncertainty in factor importance rankings factors that can be set to default values with output errors under a given percentage here 6 is used for illustration are then identified a summary of the computational cost in calculating error measures is provided in table a1 in the appendix the same nine factors for fixing were identified by both morris and sobol gsa methods for the application to a test function sobol g function which include all the non influential and some of the weakly influential factors defined by sheikholeslami et al 2019 these results can be compared with other factor screening methods that do not directly evaluate the error due to factor fixing if using 1 as the threshold for the sobol total sensitivity index in distinguishing insensitive factors then all the ten factors in the bottom two groups in table 1 would be screened as insensitive and the resulting rmae would be 7 7 higher than the desired threshold of 6 the threshold of 5 used in jawad et al 2015 would screen 11 factors for fixing for morris screening the six least sensitive factors in blue in fig 3 would be screened as non influential and set to default values when adopting the 1 cumulative sensitivity indices cut off used in previous studies jia et al 2018 sarrazin et al 2016 made a similar finding that the typical screening threshold used for variance based gsa methods 0 01 is more generous than a test based on the error in qois in their case the ks test it is also worth noting that if the error threshold is more strict or more lenient fewer or more factors may be identified for fixing using the same process 5 2 limitations and advantages of the method for sensitivity ranking the presented study uses bootstrapping of sensitivity indices in order to quantify the uncertainty in the ranking of factors however bootstrapping may provide misleading results if factor space has been insufficiently sampled or structured rather than random sampling methods are used in such cases uncertainty in factor ranking would need to be quantified using other methods such as repeated sampling without replacement nevertheless it is worth noting that in the proposed method uncertainty in factor ranking is primarily used to reduce noise in the results factors with an uncertain ordering can be fixed as a group rather than individually from this point of view some uncertainty in bootstrapping is permissible which is also why we invoke 95 confidence intervals this uncertainty is then mitigated by the fact that the proposed method then tests whether results stay the same when the sample size for gsa is further increased and increasing sample size tends to increase the certainty in the ranking and therefore reduce the effect of any error in this noise filtering process 5 3 emerging opportunities for a new research agenda factor fixing is an important practice for reducing the factor space in many environmental modeling contexts garcia et al 2019 varella et al 2010 the key argument of this paper is that errors of interest due to factor fixing need to be estimated here we have taken a necessary first step of demonstrating a proof of concept on a well known function for which there are existing results in the literature it provides guidance on fundamental issues in factor fixing and a framework for undertaking ff for other test functions and for real world applications such as to spatio temporal environmental models only by testing such a framework on many applications might its methods be enhanced we contend that evaluating error due to factor fixing opens a new field of work that echoes and reinforces certain key trends in best practice environmental modeling this paper aims to open up opportunities within a new research agenda rather than providing the last say on the topic we identify five specific opportunities in the use or development of factor fixing from the perspective of computational considerations we would like to draw attention to two aspects how to evaluate error efficiently and for which combinations of factors to evaluate error tackling these two points is particularly important when applying the approach to higher dimensional modeling problems for which factor fixing becomes more essential to improve the broader applicability of factor fixing three opportunities stand out for further investigation determination of error metrics and error thresholds accounting for error due to factor fixing in selection of default values and the transferability of error estimates as an example of how these opportunities come together we discuss specific challenges and opportunities of factor fixing for inverse problems and integrated environmental modeling 1 how to evaluate error efficiently while the proposed method aims to reduce the number of error evaluations even a single error evaluation may still be potentially prohibitive for very expensive models or those requiring large numbers of model runs to explore factor space an obvious strategy would involve using model emulation asher et al 2015 mara et al 2017 palar et al 2018 ratto et al 2012 to assist in assessing factor fixing options for environmental models which can substantially reduce computational costs emulators may introduce further biases in predictions such that resulting uncertainties in sensitivity analysis and the calculation of error measures will need to be addressed developing adaptive sampling approaches for error evaluation is another promising way to reduce the computational burden for example aim to calculate errors in qois by recycling samples available including those from sensitivity analysis the uncertainty or bias of errors in such a process should then be recognized due to the samples being structured or insufficiently taken if it is not possible to evaluate the error due to factor fixing at all this should however raise questions about the effect of factor fixing on the credibility of the qois produced 2 selecting combinations of factors requiring error evaluation building on the tradition of factor fixing based on sensitivity analysis in environmental modeling this paper has evaluated error for combinations of factors defined by their ranking however this assumes a sufficient alignment between the sensitivity indices and the qoi for which error will be tested in principle sensitivity analysis is being used to avoid testing all possible combinations of factors for fixing in a high dimensional model which in turn impacts on both the computation for error evaluation and the reliability of a reduced model of a given order there is potential to explore other heuristics or algorithms to achieve this alignment aim some of these may indeed be variants of existing sensitivity indices that capture the effect of factors on different summary metrics the sequence in which combinations of factors are tested also matters in the presence of interactions between factors where the effect of fixing one factor may be compensated by another suggesting that factors may need to be considered as a group rather than individually future work may test how sobol sensitivity indices for instance that elicit combinations of factors could be used for this purpose 3 determination of error metrics and error thresholds determining what metrics can or should be used for different problems to evaluate the performance of models is a broad and perennial topic in environmental modeling bennett et al 2013 moriasi et al 2007 gleckler et al 2008 jackson et al 2019 factor fixing emphasizes two key requirements specification of acceptable error thresholds and measurement of the difference between a full model and its reduced model the use of metrics is to some extent left to the modeler s discretion but needs to reflect the purpose of modeling be decision relevant and have an intuitive interpretation in order to be able to evaluate whether error is acceptable for example metrics indicating the changes in appropriate uncertainties are preferred if a reduced model is to be used for uncertainty analysis multiple metrics are recommended in practice as the multiple influences of factor fixing on the qoi are unlikely to be reflected by a single metric jackson et al 2019 acceptable error thresholds also need to account for associated uncertainty in the error estimate and the potential for acceptability of errors to be dependent on what is achievable thus whenever the precise purpose of modeling can be considered somewhat flexible in a case study to support decision making needs error thresholds may be defined iteratively in order to chase useful conclusions from the analysis the future therefore requires both exploration of error metrics suitable for a factor fixing context in a wide scope of cases and pursuit of guidelines for the assessment of errors 4 how selection of default values should take into account error due to factor fixing the determination of default values at which to fix factors is another challenge in the application to real world models especially when there is little knowledge of what values are appropriate in advance three common practices are to fix factors at their nominal values in the model the average of their distributions considering that the variation caused by fixing at average values might be smaller and values at random by treating the model as a stochastic model if the computational expense to find their location is affordable and the aim is to find the maximum number of factors for fixing values in the applicable range resulting in the least error while fixing factors might be preferred it would also be more conservative for the screening of parameters to evaluate error by fixing factors at values that cause the largest error this minimizes the risk of underestimating the errors from factor fixing some of these approaches take into account the error due to factor fixing explicitly or implicitly and others simply take the default value as externally specified best or reasonable options in different case studies will need to be tested so as to provide ongoing advice for the community 5 the transferability of error estimates the transferability of error estimates needs to be seen in a broader context related to all four points discussed above as well as the conditions under which a model is used caution should be taken when transferring a reduced model from a different study as the experimental design and conclusions depend on each research specific context for example factor combinations in hydrologic modeling that are determined for fixing for annual discharge volume may not work for daily flow simulation as with performance assessment in environmental modeling generally estimation of factor fixing faces issues under non stationary conditions i e where calibrated model parameters change according to time period analysed on one hand the importance of a factor varies among different conditions e g wet versus dry years in a hydrological model response basijokaite and kelleher 2021 on the other hand the reasonable values at which to fix a factor may change due to non stationarity pathiraja et al 2016 and a fixed parameterization is thus not sufficient to represent varied patterns sadegh et al 2015 to spot the mismatch in transferability one can detect whether there is non stationarity possibly by checking the long term trends such as precipitation sequence or other physical characteristics in a catchment kundzewicz and robson 2004 sadegh et al 2015 common strategies include understanding the conceptual role of factors under varying conditions e g the climate condition the qoi especially those fixed and comparing errors in calibration and validation if the error is small in calibration but large in the validation conditions it could be caused by a fixed factor determination of factor fixing and default values then requires re evaluation evaluation of errors due to factor fixing is at the cliff face of how model complexity relates to non stationarity it literally evaluates how removing a degree of freedom affects performance in one or multiple different conditions 5 3 1 key application area factor fixing for inverse problems inverse problems are a key application area for factor fixing where it is commonly implemented before conditioning parameters with observations error due to factor fixing is however complex in this setting factor fixing collapses the posterior for that parameter resulting in other parameters taking on a compensatory role error in the multi variate posterior is therefore already large but error in the marginal distributions of other parameters or their optimized values might still be acceptable as might error in model predictions and predictive uncertainty for non identifiable parameters the error introduced into the distribution amounts to treating the default value as additional information which again may or may not impact on predictions in any given conditions evaluating error in predictions due to factor fixing requires solving the full inverse problem raising the stakes for achieving computationally efficient estimates with an acceptable error at the same time when model pre calibration is accessible factor fixing needs to integrate the effects of observational data parameter uncertainty and model prediction for example by using the prediction sensitivity of parameters as indicators of parameter importance especially if a model is used for prediction tiedeman et al 2003 2004 while further work is required focusing on error due to factor fixing in inverse problems potentially represents a paradigm shift in which analysis is driven by adaptive construction of surrogate models that help to quantitatively explore the dynamics between providing additional information in the form of fixed parameters changes to the posterior distribution of the remaining parameters and changes to predictions all while simultaneously obtaining iterative preliminary estimates of both parameter and predictive uncertainty recognizing the necessity of error evaluation is just the start of a new conversation 5 3 2 implications of research agenda for integrated environmental models factor fixing in environmental modeling to some extent simply highlights existing issues because it involves modelers consciously inducing an error relative to a reference more complete model de baan 2020 matott et al 2009 environmental models can be composed of multiple sub models with many uncertain parameters fu et al 2019 tscheikner gratl et al 2019 and fixing factors in a component model may cause cascading effects on the whole integrated system due to the computational ease of running a sub model rather than the whole integrated system error evaluation in the sub model can be checked first before calculating the final output for the sake of supporting decision making on environmental management problems of concern error measures selected for ff also need to be easily communicated or even agreed upon to ensure the qoi are those that stakeholders care about in the case of integrated models issues with high dimensionality are further highlighted because factor fixing or its variants are so often used as a solution without testing their implications for example parameters in environment models such as catchment water quality models are spatially variable a parameter may be critical in a sub area even though it might be of little importance as indicated by sensitivity analysis over the whole study region fixing such a parameter would result in inaccurate simulation locally in principle we call for efforts to quantitatively evaluate the error induced by dimensionality reduction techniques even in large multi component integrated models in practice we recognize that this is a research agenda that will be open for some time to come and that transparent pragmatic discussion around this topic will remain a key part of the solution to ensure models are credible legitimate and salient 6 conclusions an adaptive method has been proposed and evaluated against previous practice on a well known test function its basis and methods seem to warrant application to real world models where factor fixing is often considered necessary to reduce model dimensionality and computational load the work has motivated questions and issues that are outlined as part of an ongoing research agenda with respect to factor fixing and environmental modeling in general based on all the analysis in the paper we conclude as follows 1 fixing sensitive factors can lead to substantial errors that have commonly been ignored estimating predictive error in the quantity of interest is a necessary method for modelers to understand the loss incurred by factor fixing and one can determine the fixed factor set adaptively by considering the trade off between the accuracy of model performance and affordable computation 2 the effect of factor fixing can be investigated systematically within the adaptive framework described a bootstrap based partial sort is an efficient approach for identifying the relative importance of factors in consort with global sensitivity analysis the gsa methods of morris and sobol share similar results here for the g function in terms of factor screening albeit with morris being much more computationally efficient 3 the selection of default values can considerably influence the error caused by factor fixing even if the sensitivity indices suggest the factor is of low importance this in turn means that the use of default values in models can be critical and warrants investigation in determining the number of factors to fix when the decision is based on anticipated error 4 a focus on evaluation of error induced by factor fixing and potentially other dimensionality reduction techniques appears to open an interesting new research agenda that both highlights and will continue to provide new perspectives on common issues in best practice environmental modeling declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank all reviewers and editors for their helpful comments joseph guillaume received funding from an australian research council discovery early career researcher award project no de190100317 qian wang s research was supported by the national natural science foundation of china grant no 51879068 and key r d program of ningxia grant no 20175046802 john jakeman s work was supported by the u s department of energy office of science office of advanced scientific computing research scientific discovery through advanced computing scidac program sandia national laboratories is a multi mission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 the views expressed in the article do not necessarily represent the views of the u s department of energy or the united states government appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105290 
25682,despite widespread use of factor fixing in environmental modeling its effect on model predictions has received little attention and is instead commonly presumed to be negligible we propose a proof of concept adaptive method for systematically investigating the impact of factor fixing the method uses global sensitivity analysis methods to identify groups of sensitive parameters then quantifies which groups can be safely fixed at nominal values without exceeding a maximum acceptable error demonstrated using the 21 dimensional sobol g function three error measures are considered for quantities of interest namely relative mean absolute error pearson product moment correlation and relative variance results demonstrate that factor fixing may cause large errors in the model results unexpectedly when preliminary analysis suggests otherwise and that the default value selected affects the number of factors to fix to improve the applicability and methodological development of factor fixing a new research agenda encompassing five opportunities is discussed for further attention keywords sensitivity analysis factor fixing error measure adaptive method partial sort ranking 1 introduction complex models incorporating many processes and variables are increasingly used to represent real world problems to generate understanding and or inform management tscheikner gratl et al 2019 yang et al 2014 however complex models such as environmental models that describe biophysical and socio economic systems present a system identification problem guillaume et al 2019 in the sense that they contain many uncertain parameters that cannot be measured and need to be estimated on the other hand estimation is thwarted by the fact that there is rarely enough information in the input and output model variables to identify estimate those parameters uniquely guillaume et al 2019 the usual solution to this non identifiability is to make assumptions that constrain the space of parameter solutions parameter fixing pf is one approach that attempts to set parameters deemed insensitive to fixed values and then estimates the more sensitive ones from the available data more generally in an uncertainty quantification or exploratory modeling context other static inputs that do not need to be estimated from data are also commonly held constant maier et al 2016 to reduce the scenario space that needs to be explored guillaume et al 2012 given their similarity static inputs and parameters are often referred to collectively as factors and we therefore focus here on factor fixing ff generally model calibration and uncertainty analysis are common practices to inform selection of parameter values and their impact on predictions yet conducting accurate calibration and uncertainty analysis can be challenging when the computational cost of running a model is high moreover this challenge is amplified when the model has a large number of parameters becker et al 2018 yang et al 2018 because the number of model runs needed for accurate analysis typically increases with the dimensionality of the parameters though it could in some circumstances be possible to run such analysis over the full factor space with supercomputing infrastructure reducing the factor dimensionality by for example ff can benefit the further use of a model at the cost of requiring additional computation for ff analysis several advantages can be achieved which include 1 obtaining a reduced model that increases the computational efficiency of future model analyses by constraining the explored factor space 2 identifying redundant factors and decreasing unnecessary model complexity monari and strachan 2017 and 3 improving the estimation of sensitive factors using the available data factor fixing can address the issue of high dimensionality by excluding insensitive parameters from further analysis of central interest in this paper is the exclusion of those factors which have no effect or negligible effect on the quantities of interest qoi qois may consist of model predictions or some function of them relevant to the objectives of the modeling exercise e g calibration objective functions guillaume et al 2019 unimportant parameters can be identified using a variety of global sensitivity analysis gsa methods razavi et al 2021 which are implemented in numerous software packages douglas smith et al 2020 factor fixing or factor screening is therefore approached through identification of non influential parameters norton 2015 pianosi et al 2016 wagener and pianosi 2019 subsequent reductions in the number of model runs evaluations have been reported with ff including with respect to hydrologic models cuntz et al 2015 van werkhoven et al 2009 for example gou et al 2020 concluded that optimized results were improved by applying gsa to classify the importance of factors instead of fixing default factors as is usually recommended for the vic hydrological model calibration across china studies of applying screening approaches however have mainly focused on comparing the performance and efficiency of different methods cosenza et al 2013 jaxa rozen and kwakkel 2018 mara et al 2017 decreasing the computing cost becker et al 2018 cuntz et al 2015 jakeman et al 2019 and distinguishing sensitive and insensitive factors using sensitivity indices nossent et al 2011 sarrazin et al 2016 examples of method comparisons in the environmental modeling field include vanrolleghem et al 2015 who concluded that the morris screening and extended fast methods showed substantial agreement with respect to ff khorashadi zadeh et al 2017 who identified the same unimportant factors by sobol and pawn approaches in an application to the swat model and koo et al 2020 who used the extended fast method on a swat model to determine the relative effect of 15 factors on model uncertainty the selection of a gsa method whatever its purpose depends partially on the computational budget whereas the best performing measure depends on the model becker et al 2018 given that computing time can be prohibitive for assessing sensitivity and uncertainty in high model dimensionalities another important consideration is using more efficient sampling techniques such as multi criteria trajectory based sampling khare et al 2015 and sequential screening strategies cuntz et al 2015 before comparing different methods and applying more efficient sampling techniques it is vital nevertheless to separate influential and non influential factors successfully approaches developed for such separation can be classified into three types screening thresholds grouping techniques and statistical testing use of a screening threshold appears to be the most commonly adopted approach and involves classifying a factor as insensitive if the sensitivity index is below a pre specified threshold for variance based gsa methods factors responsible for less than a certain threshold of the variance e g total sensitivity index 0 01 0 1 or 0 2 are often screened out cleaves et al 2019 hübler 2020 vanrolleghem et al 2015 wang et al 2010 the threshold for the morris method in contrast is often case specific in general thresholds when set should have values that are highly related to the qois examined another way of determining the screening threshold involves adding a dummy factor a variable that is not used in the model and any factor for which confidence intervals cis of its sensitivity index overlaps with that of the dummy factor is identified as insensitive khorashadi zadeh et al 2017 screening thresholds for fixing non influential factors however may not be sufficiently indicative of actual errors in qoi s for a qoi not related to output variance it may not be possible to derive an associated error estimate from sobol or other variance based sensitivity indices moreover error estimates are generally obtained by marginalizing over factor values instead of considering the default value at which the factor will actually be fixed in practice the difference between these error estimates may be further accentuated when multiple factors are fixed grouping techniques categorize factors into pre determined groups based on considerations such as expertise and experience of the problem or model intuition the pre specified threshold for sensitivity indices or into objectively optimized groups using an automatic approach hou et al 2015 saltelli et al 2007 sheikholeslami et al 2019 those factors in the insensitive group are then treated as constants in the model statistical testing methods build on the screening threshold approach by evaluating whether the entire set of factors associated with what are called the unconditional outputs is reliably divided into sets of influential sensitive factors i e the associated conditional outputs and non influential factor space sarrazin et al 2016 used the statistical significance of the kolmogorov smirnov ks test to validate whether factors were indeed insensitive with the consideration that the correlation coefficient of types in for example branger et al 2015 nossent et al 2011 and tang et al 2007 may still be high when unconditional and conditional outputs follow a non 45 straight line sarrazin et al 2016 the ks statistic used in sarrazin et al 2016 is a more conservative test as it uses the most sensitive location of a non influential factor for validation they repeat this test with the factors considered fixed at a variety of values rather than explicitly analyzing the representativeness and effect on predictions resulting from the default values used for fixing visual techniques are also employed to aid in identifying factors to fix garcia et al 2019 they tend to informally use a combination of approaches including screening retaining the top important factors and factor grouping given that there is uncertainty about the screening threshold the number of factors to retain and the separation of groups traditional factor screening is more qualitative identifying factors to fix with fewer model runs song et al 2015 and lacking a sufficient indication of errors induced in qois one approach is to approximate the error from setting unimportant factors to the average values over the factors distributions using the analysis of sobol et al 2007 as described in jawad et al 2015 on the whole there are remarkably few studies that explicitly determine the set of factors to fix on the basis of the errors caused in the prediction s of qois there is an identified need for a theoretical error analysis approach to factor fixing alexanderian et al 2020 and methods are needed to answer the fundamental question can all insensitive factors or any of the weakly influential factors be set to default values with model performance remaining acceptable the resulting challenge is the high computational cost required to evaluate errors induced by factor fixing because of the need to compute both conditional and unconditional qois i e with and without factor fixing it is therefore necessary to reduce the number of evaluations needed by testing the fixing of as few unique combinations of factors as possible a further consideration is that sampling based sensitivity indices are uncertain due to numerical errors sampling variation and the approximation of sensitivity indices the full convergence of indices is not always possible even with a large sample size nossent et al 2011 sarrazin et al 2016 which is a challenge for threshold based factor fixing on the other hand studies generally show that factor rankings are less variable and converge faster than sensitivity indices sarrazin et al 2016 sheikholeslami et al 2019 wang et al 2013 in this paper we therefore explore the effects of factor fixing with a proof of concept adaptive process able to 1 systematically quantify the error in a nominated qoi with given default values for factors and 2 identify the number of factors that can be fixed without exceeding a maximum acceptable error whilst 3 minimizing the number of evaluations required for acceptable error an uncertainty based partial sort approach via bootstrapping is developed for factor ranking to reduce the number of model runs used for gsa and increase the robustness of rankings factors are fixed in groups to decrease the frequency of error estimation by identifying the smallest number of unique factor combinations to fix treating model predictions with all factors varying as unconditional values and model outputs with some factors fixed as conditional values we measure the differences between unconditional and conditional outputs using multiple metrics error measures are evaluated preliminarily using a small sample set to define factors that are safe to fix or to vary a larger number of model runs is conducted if it is difficult to decide whether to fix a factor group due to the large uncertainty in the error measures the largest set of factors that can be fixed whilst keeping errors under a given threshold is then determined in an adaptive fashion the method seems widely applicable and can be performed so as to minimise the number of times that errors need to be estimated section 2 presents an intuitive example of the effect of fixing factors on the predictions of qois the proposed method is elaborated in section 3 and then applied in section 4 to the commonly used sobol g function with two gsa methods being used to generate the sensitivity indices described there the interpretation of results including evaluating the effects of sensitivity from different gsa methods error measures and default values to fix factors is given in section 4 this is followed by discussion of emerging opportunities within a new research agenda on the topic of ff in section 5 our conclusions are given in section 6 2 the effect of fixing factors on predictions overall the impacts of fixing factors on model predictions can be treated as a source of bias fixing truly non influential variables would theoretically not affect the model outputs over the factor space however factors screened as non influential often have non zero sensitivity which will still cause some error in predictions leading to a reduction in the precision of predictions across factor space compared with model results obtained without factor fixing fig 1 provides an example of the potential effects of factor fixing on predictions the left plots fig 1 a and c depict the contours of the function f x 1 x 2 w x 1 x 2 2 where x 1 and x 2 are uniform variables on 0 1 and x 1 is sensitive when w 0 5 and insensitive for w 0 the effect of fixing x 1 on the outputs from f is shown for two different values of x 1 vertical lines in cyan and red colors for x 1 0 50 and 0 01 respectively when x 1 is set to 0 50 or 0 01 the two factor model is reduced to a one dimension model f x 1 x 2 x 2 x 1 0 50 or f x 1 x 2 x 2 x 1 0 01 and the value f 0 70 0 50 marked by the black dot is approximated by f 0 50 0 50 or f 0 01 0 50 marked in cyan red as shown in fig 1 a and c when x 1 is insensitive top panel the probability density function pdf of the outputs by f x 1 x 2 x 2 x 1 0 50 and f x 1 x 2 x 2 x 1 0 01 are identical to that of f x 1 x 2 indicating that fixing x 1 has no effect on f as plotted in fig 1b when x 1 is sensitive bottom panel the pdf of f depends significantly on the value at which x 1 is fixed fig 1d whether or not the error induced by fixing x 1 is important depends on the modeling objective and accuracy requirements the true mean of f is 0 67 and if we set x 1 0 50 the mean of the conditional function f x 1 x 2 x 2 x 1 0 50 is 0 64 which may be a reasonable approximation however if there is concern for predicting small extreme values fixing x 1 at 0 50 would be inappropriate the pdf in fig 1d shows that when x 1 0 50 the model is incapable of predicting values of f near 0 and when x 1 0 01 it cannot predict large values this low dimensional example demonstrates that fixing a factor can lead to a significant error when the factor has non zero sensitivity for high dimensional cases the projection in factor space implied by fixing factors cannot be visualized as easily which is why a computational analysis to estimate errors is needed 3 methods for evaluating effects of fixing factors the goal of our proposed algorithm is to determine the largest set of unimportant factors that can be fixed without the error in the qoi of the resulting lower dimensional model exceeding an acceptable threshold we fix the factors in groups to reduce the number of times that error measures need to be evaluated the groups are ordered by increasing sensitivity and the least sensitive groups are fixed first the groups emerge by partial sorting of factors according to confidence intervals of their ranking obtained through sensitivity analysis if groups are found to be too coarse the computational budget is increased to refine them this adaptive approach also reduces the number of model runs needed for sensitivity analysis the applicability of the method is illustrated with two gsa techniques sobol and morris methods three contrasting metrics are selected to evaluate errors due to factor fixing relative mean absolute error rmae pearson product moment correlation r and relative variance rv the error measures are evaluated adaptively with a preliminary estimate at an initial small number of model runs to screen out those partial groups for which fixing would either result in errors far beyond or below the given error threshold algorithms 1 the adaptive method for factor fixing image 1 algorithms 2 function partial of factors image 2 algorithms 3 function for error calculation caused for factor fixing image 3 algorithms 4 function for checking convergence image 4 algorithms 1 4 algorithms for the proposed adaptive method for factor fixing and sample size determination given a selected gsa method and model algorithm 1 covers the overall adaptive procedure with algorithms 2 4 describing the three main steps involved 3 1 an adaptive method for simultaneous factor fixing and sample size determination the adaptive method is specified in algorithms 1 4 as pseudo code algorithm 1 describes the overall procedure and algorithms 2 4 emphasize the three main steps involved partial sorting of factors error calculation and checking of convergence model input factors x the quantity of interest y and other parameters requiring determination for the adaptive analysis are identified in lines 1 5 of algorithm 1 the smallest and largest sample sizes to consider n s t a r t and n s t o p respectively and the number of samples by which to iteratively increase n s t e p the sample size indicate the number of model runs to be used and depend on the given gsa method as well as the computational budget the confidence level p is used to calculate the confidence intervals of factor rankings error measures η should reflect the requirements of the problem and relevant interest groups as discussed in section 3 3 the symbol ε denotes the maximum error measured by η e g 5 change of the variance of model outputs that is considered acceptable in the model outputs the matrix x t denotes samples generated for evaluating the errors with the sampling technique s which may be different from the samples used for sensitivity analysis the x t for a small sample size is generated first leading to an initial estimation of η checking of convergence requires specifying the number of repetitions λ to establish stability of results as the sample size of the sensitivity analysis increases with an initial n s t a r t samples l12 19 the selected gsa method for example the morris method is used to generate confidence intervals of factor rankings using bootstrap resampling algorithm 2 l2 6 see section 3 2 for more details factors are then partially sorted such that they form groups g ordered by sensitivity algorithm 2 l7 12 algorithm 3 then calculates errors by fixing factors in groups that are stable with the least sensitive group first the group with second lowest sensitivity will then be fixed together with the previous group followed by the calculation of error measures the process is repeated until the error exceeds the acceptable threshold the sorting of factors into groups will depend on the sample size used as part of the gsa method larger sample sizes are better able to identify the ranking of factors splitting factors into smaller groups but at a higher computational cost an adaptive method is therefore used to simultaneously determine the factors to fix and assess whether increasing the sample size would be beneficial in producing more stable partial groups with convergence evaluated by algorithm 4 the combination of factors for fixing that remains unchanged over at least λ consecutive iterations is treated as being stable and indicative of increased confidence in results the larger the λ is the more robust the partial groups are but the more model runs are needed the pseudo code in lines 5 15 of algorithm 4 partitions factor groups into three main sets if the error e i in e is small and acceptable factors in the corresponding group g i and less sensitive factors are included in the fixing set g f i x alternatively if e i is large caused by fixing a group of highly sensitive factors these factors will be added to g v a r y and will no longer be considered for fixing typically sensitivity analysis with more samples is required to resolve the moderately sensitive factors so as to determine whether a subset of the factors within such groups can be fixed reliably the group needing more samples for enhanced resolving is denoted as g s a m p l e if no more factors are in this group convergence is then achieved during the adaptive process the user can also decide to adjust ε considering the trade off between the increased computation and the acceptable error increasing the size of x t may also be considered if it is impossible to decide to which set g v a r y g s a m p l e or g f i x a factor group belongs due to the large uncertainty in the initially estimated η in the adaptive process factors are not fixed in the sensitivity analysis so that samples from the previous iteration can be reused error calculations for groups of factors can be cached rather than re evaluated at each iteration overall two criteria have been included to improve confidence in the robustness of the result robustness in factor ranking through uncertainty based partial rankings and the stability of results when increasing the sample size λ times razavi and gupta 2016 sarrazin et al 2016 3 2 partially sorting factors using bootstrapping traditionally the sensitivity rankings r r 1 r i r d of d factors x x 1 x i x d in a model are generated by referring to sensitivity indices informed by n data points over the factor space using one gsa method factors are ranked least sensitive to most sensitive so that a lower value of ranking r i for factor x i here represents a smaller sensitivity uncertainty in the ranking r is expected due to the variability of sampling garcia et al 2019 sheikholeslami et al 2019 and subsequent approximation of the sensitivity indices the ranks r i may vary within certain ranges during bootstrapping to obtain converged factor rankings additional samples are needed to better define the ranking of that factor we can account for the uncertainty in rankings via partial sorting as an example consider three factors x 1 x 2 x 3 of a model the confidence intervals cis of the factor rankings calculated with the percentile method being distribution free and easy to implement are shown in fig 2 details of the calculation of cis are described below at sample size n1 factors x 1 and x 3 are classified into the same group that is less sensitive than x 2 due to the cis of rankings both ranging from 1 to 2 with an increased sample size n2 the three factors are instead ordered as x 1 x 3 x 2 the steps needed to partially sort factors in algorithm 2 are explained below step 1 determine the bootstrap resampling sample size m and the confidence level p step 2 conduct bootstrap resampling with replacement m times for each resample k k 1 2 m compute the sensitivity rankings with use of a gsa method for each factor the distribution of the rankings is then obtained with a total of m estimates step 3 calculate the confidence intervals of factor rankings at confidence level p e g p 0 05 leads to cis of 95 for d factors form the cis as r c i r 1 l r i l r d l r 1 u r i u r d u with u denoting their upper bound and l their lower step 4 partially sort factors by constructing a directed acyclic graph describing the rank order of pairs of factors using r c i calculated in step 3 l8 10 in algorithm 2 this is analogous to a dependency graph specifically find all factors x j that satisfy r j l r i u j 1 2 d j i for each factor x i and use x l i s t i to represent x j r j l r i u j i factors not in x l i s t i are in the same group as x i due to the fact that their ranking cis overlap with that of x i using the collection of ordering relations x i x l i s t i topological sorting then separates factors into totally ordered groups which is undertaken here with the toposort python package smith 2016 the level of confidence represents the robustness of factor rankings at a user defined value bearing in mind the computational budget 3 3 evaluation of metrics of error due to factor fixing 3 3 1 description of metrics of errors based on existing literature bennett et al 2013 nossent et al 2011 tang et al 2007 three measures η are used in this paper to capture different potential influences on model outcomes caused by fixing factors as illustrated by fig 1 relative mean absolute error rmae is an equal weighting metric for residuals i e calculates the difference between two data sets point by point herein conditional and unconditional outputs of interest and easy to interpret in terms of acceptable error the pearson product moment correlation r describes the extent to which the two datasets are linearly correlated ranging from 1 to 1 a value of 1 for r implies a totally positive linear correlation between two datasets and can be obtained if the two data series only have a constant offset relative variance rv is a direct value comparison method that compares the two data sets in terms of their variance being a summary characteristic of the spread in the data equations 1 3 express the formulae for these three error measures with use of x t mentioned in section 3 1 1 rmae k 1 n y t k y c k k 1 n y t 2 r k 1 n y c k y c y t k y t k 1 n y t k y t 2 k 1 n y c k y c 2 1 2 3 rv k 1 n y c k y c 2 k 1 n y t k y t 2 where n is the size of x t k k 1 2 n is the sample index x c k denotes the sample point x t k with some factors fixed y t k is the unconditional value at the sampling point x t k with no factors fixed y c k is the conditional modeling result at x c k and y t and y c denote the average of y t and y c over all sampling points each time a combination of factors is fixed n model evaluations need to be run to generate the conditional outputs y c k and the same model outputs can then be used to calculate different error metrics e g different statistics of a time series when no factor is fixed r and rv will be 1 the threshold for errors is set here to be 5 which means the acceptable r and rv 0 95 n is denoted as n y to represent the size of y t so as to distinguish the sample size n in gsa methods in the following sections before continuing we remark that our method can be used with a large class of error measures including but not limited to those quantifying changes to the variance the choice of this measure should be dictated by user concerns and the modeling requirements including the purpose of the exercise for example if there is concern about the sensitivity of tail events to input parameters then the metric used should reflect this if the distribution of the model output is symmetric the variance can give a good indication of the variability of a model output however if the distribution is asymmetric other metrics based upon measures such as the probability of failure or conditional value at risk should be used the determination of an admissible error threshold is crucial and needs to be contextualized according to the purpose of the modeling for example if a high accuracy of predicting extreme events is sought then this may require a small error in conditional value at risk 3 3 2 determine sample size needed for error measures evaluation all the error measures are calculated by sampling of factors resulting in variation between samples as mentioned in the previous section conditional model outputs y c require the same number of model runs as y t when fixing each unique factor set thus the total number of model evaluations needed for calculating error measures is n n y z 1 where z is the number of combinations of factors to fix to achieve a balance between the computational cost n model evaluations and the accuracy of error measures produced error measures are assessed adaptively initializing n y to a small value uncertainties of error measures are quantified with both bootstrap and replicate sampling techniques respectively noting that this is a separate uncertainty quantification step to the bootstrap used for partial ranking of factors for the replicate sampling method the sobol sequence is adopted given that quasi monte carlo methods can outperform the standard monte carlo hou et al 2019 a total of m independent factor sets x t i of size n y are generated where i 1 m the calculation of error measures e g rmae is then performed m times resulting in m independent estimates of the error measures e i n y i 1 m the average of e i n y i 1 m is then 4 e n y 1 m i 1 m e i n y and the standard deviation sd of the e n y is 5 s d e n y 1 m 1 i 1 m e i n y e n y 2 it is often appropriate to select m to be of a small value leading to n n y z 1 m model evaluations for z combinations of factors to fix and m independent realizations a value of m 20 is used in the paper the 95 confidence intervals of each error measure can be written as e n y 1 96 s d e n y e n y 1 96 s d e n y bootstrapping on the other hand uses monte carlo sampling given the risk of bias associated with resampling of quasi monte carlo sequences the mean and the standard deviation of error measures are calculated with equation 4 and equation 5 here m b represents the number of bootstrap resamples in the equations with m b 1000 used in this paper error measures e n y calculated from a small n y may result in a large variation but could still provide valuable information if the upper bound of an error measure is below beyond the error threshold ε the corresponding factor set is to fix vary 3 4 gsa methods two gsa methods are chosen sobol morris to illustrate the partial sort method and to explore the differences due to their different perspectives on measuring sensitivity note that the proposed parameter fixing approach can be utilized with any gsa technique for which confidence intervals on rankings can be calculated our focus is on demonstrating the effects on the qoi with the proposed adaptive framework determining the best gsa method is problem dependent and beyond the scope of this paper 3 4 1 morris screening method the morris screening method morris 1991 is a commonly used gsa method for evaluating the effect of varying factors locally one at a time oat at repeated points across factor space it is efficient in factor screening and well suited to high dimensional cases as can occur with environmental models herman et al 2013 jia et al 2018 sun et al 2012 in morris the overall effect of a factor is measured here by the sensitivity measure μ which is the average of the absolute elementary effects ees computed at points that are evenly distributed over the factor space for the parameter input at different levels campolongo et al 2007 following the sampling strategy suggested by morris n trajectories containing d 1 points d is the number of factors are generated resulting in a total n n d 1 model evaluations ee and μ are calculated according to 6 e e i k f x 1 k x 2 k x i k δ i k x d k f x 1 k x 2 k x i k x d k δ i k 7 μ i 1 n k 1 n e e i k where n is the number of trajectories δ i k is the size of variation for factor x i at trajectory k and determined by the number of levels used and μ i is the sensitivity measure for factor x i values of μ are highly related to the choice of factors and qois such that they generally should not be compared across applications herein factors were ranked according to μ it is also common to consider the standard deviation of the ees as a measure of non linearity and factor interactions or some combination of the mean and standard deviation 3 4 2 sobol sensitivity analysis the sobol sensitivity method a variance based sensitivity analysis technique evaluates influences of factors by decomposing the unconditioned variance of the model results into contributions from individual factors and their interactions saltelli 2002 sobol 2001 consider the model as a function 8 y f x f x 1 x d where y or some property of it is the qoi and x x 1 x d is the set of factors denote the unconditional variance of f as v y following the sobol method v y can be decomposed as 9 v y i 1 d v i i 1 d 1 j i 1 d v i j v 1 d where v i is the amount of variance explained by an individual factor x i and v i j is the contribution of the pairwise interactions of factors x i and x j higher order interactions are also typically included the most commonly used measures of sensitivity are first order and total order indices that are defined in equation 10 and equation 11 respectively as 10 s i v i v y 11 s t i 1 v i v y where v i represents the variance explained by all factors except x i the first order index s i also called the main effect quantifies the contribution of an individual factor x i to the variance of the qoi s t i the total order index measures the total effect of x i including its interactions with other factors and is therefore more conservative for factor screening compared with s i sobol indices are commonly approximated by numerical integration due to the complexity of highly non linear models given that μ can be a better proxy of the s t i campolongo et al 2007 than μ the average of ees in this paper factors were ranked according to s t i using the saltelli sampling method saltelli 2002 which is an extension of sobol sampling the formula adopted for calculating total order effects is the jansen method jansen 1999 and a total of n d 2 model evaluations are required to calculate both s i and s t i salib an open source python package douglas smith et al 2020 herman and usher 2017 was used to conduct morris screening and sobol analysis 4 application and results 4 1 application to sobol g function given the focus of this paper on providing a proof of concept for adaptively evaluating errors due to factor fixing an example function is selected that is simple with known behavior of its factor sensitivities we have thus selected the commonly used mathematical test function the sobol g function which is defined as 12 y x 1 x 2 x d i 1 d 4 x i 2 a i 1 a i where factors x i i 1 2 d are uniformly distributed over a 0 1 d hypercube and the a i are non negative constants here y is our predictive qoi low values of a i result in a large main effect for x i more detailed explanations of the sobol g function can be found in saltelli et al 2010 some 21 coefficients which can be grouped into four influential groups have been selected following sheikholeslami et al 2019 unless otherwise stated when factors x i are fixed they are set to 0 25 from equation 12 this removes the ith parameter which also causes the least error while conducting factor fixing as will be shown in section 4 5 below see table 1 for the coefficients used in this study for the 21 dimensional sobol g function to obtain the initial estimate of error measures 10 data points of each independent sample set were generated using the sobol sampling technique for the replicate sampling samples used in sobol sensitivity analysis are included for the error calculation 100 data points were first sampled over the factor space by a monte carlo sampling approach and resampled for a bootstrap analysis factors were fixed from the least sensitive group with groups of higher importance included gradually as described in section 3 the number of bootstrap resamples for both morris and sobol was set to m 1000 to obtain non parametric bootstrap based cis on rankings at confidence level 0 05 with the morris method the bootstrap is conducted with use of ees from the n trajectories a new sample of n trajectories is selected randomly m times with replacement with each bootstrap resampling providing estimates of μ for all factors for sobol sensitivity analysis the resampling is repeated m times from the model output samples and sensitivity indices calculated for each bootstrap the 95 cis of rankings can then be generated for both sensitivity methods the trajectories used for the morris method started from 20 in total n s t a r t 440 model runs given 21 factors with an increment of n s t e p 210 model runs 10 trajectories the number of levels is 4 as recommended in the literature campolongo et al 2007 in sobol sensitivity analysis the sample size started from n 200 and increased with a step size of 200 n s t a r t n s t e p 4600 model runs while gsa parameters could be further optimized based on an understanding of the test function it is treated here as a black box 4 2 adaptively identify factors for fixing using rmae the results of the adaptive process are shown in fig 3 for the error measure rmae using the morris screening method and bootstrap to quantify the uncertainty in error measures rmae was initially estimated with 100 samples n y the number in the first column indicates the total number of factors fixed to calculate rmae in each row upper bounds of the 95 confidence intervals of rmae are presented and used to determine whether the error is acceptable the sensitivities of the factors increase from the bottom group to the top one factors partially sorted into a group were fixed together as rankings within a group cannot be further discriminated because the 95 cis of rankings overlap accordingly one rmae value is obtained for each group and is presented in the top cell of each group block with the remainder of the cells left empty as explained in section 3 3 1 one error evaluation is conducted for each combination of fixed factors in order to reduce the number of model runs the error measure is calculated when the partial groups are stable given λ 3 no rmae is produced for 20 and 30 trajectories colored in grey in fig 3 readers can also refer to table a2 in the appendix for cis of factor rankings resulting from 20 morris trajectories the changing colored blocks demonstrate the adaptive process to determine fixed factors and the sample size needed for sensitivity analysis generally increasing the number of morris trajectories results in better discrimination of the factor rankings resulting in more and smaller groups in fig 3a three partial groups containing 6 factors in blue 4 factors in green and 11 factors in cream were identified as stable at 40 trajectories an rmae 1 83 from fixing the bottom six factors in blue was below the acceptable rmae error threshold of ε 5 thus the least sensitive factors are reliably fixed g f i x when up to 10 factors are fixed the resulting rmae is 7 83 above 5 as described in algorithm 4 in section 3 1 more trajectories are then needed to split the four factors in green into smaller groups to ascertain whether more factors can be fixed denoted as g s a m p l e the same percentage in a row is due to the same combination of factors being fixed a sign of stability also note that no additional calculation is needed when there are repetitions of the same values induced by fixing the same factors at 80 morris trajectories the partial rankings of another three factors shown in orange out of the green block appeared to be stable so the error is evaluated with an estimate of 6 40 for the 95 percentile of rmae due to the rmae being estimated with n y 100 the uncertainty of rmae can also be overestimated the rmae from fixing 9 factors is then calculated with more samples so as to determine whether those three factors in orange are safe to fix fig 3b plots rmae as a function of increasing sample size where a declining uncertainty is observed rmae generally ranges from 4 5 to 6 with sample size n y from 400 to 1000 i e its upper bound still exceeds the threshold of 5 at this point we need to decide whether to retain our a priori decision to limit error to 5 or to update our target error in order to avoid increasing the sample size further for sensitivity analysis that is there is a trade off between further computational cost and acceptable error changing a threshold can be a rational practical response in such decisions involving trade offs and even more so in adaptive decision making situations therefore for illustration we update the threshold to 6 for the remaining analyses the three factors in the orange block in fig 3a will then also be fixed g f i x similar results are obtained using the replicate sampling technique see figure a1 in appendix though this requires a significantly larger number of model runs to achieve the conclusion that 9 parameters are safe to fix note that the sample size required for error evaluation is case specific and also affected by the design of the numerical experiments e g the sampling technique and the approach for calculating uncertainty at this stage we have a preliminary determination of the number of factors to fix after evaluating the error for three unique factor combinations which results in three unique error estimates in fig 3a therefore this method compares favorably to fixing all factors in increasing order without considering uncertainty in the ranking nine error evaluations of error measures or 2 million factor fixing settings 221 if all combinations of factors were tested without using sensitivity information at all 4 3 comparison of the sobol and morris methods for each of the two gsa methods the sample size needed was determined following the adaptive method described in section 3 1 the additional group of factors with acceptable error referred to as the orange block in fig 3 were identified with n 800 n 18 400 λ 3 samples using the sobol method partial sorts for each gsa method n 1760 for morris and n 18 400 for the sobol method are shown in table 2 factors in a group are clustered in the same cell and sensitivities of factor groups increase from left to right partial sort rankings using morris and sobol methods are similar but there are differences in the composition of partial groupings a group containing factors x 18 and x 20 is separated as well as the group of x 11 and x 10 when using the sobol method the factor x 9 is in a single group for morris and less sensitive than factors x 11 and x 10 which is different from what the sobol method defines the two groups however did not affect the determination of which factors to fix the same nine factors i e x 21 x 20 x 19 x 18 x 17 x 16 x 15 x 14 x 13 were identified as being the least sensitive and safe to fix by the two gsa methods the morris screening method needs much fewer samples whilst yielding similar ranking as is well known even if optimizations of the gsa methods may further reduce the number of model runs required for both methods 4 4 comparison of different error measures we use rmae r and rv to measure the performance of the dimensionality reduced sobol g function in addition the decrease in variance normalized by the total variance is also analytically calculated for comparison which is denoted as decrease in variance in fig 4 this is analogous to the relative variance error metric rv the total variance of the reduced sobol g function can be obtained analytically using the formula for the 21 dimension function where the fixed factor plays the role of a multiplier this produces an independent measure of decrease in variance as discussed in section 4 3 sobol analysis with n 800 resulted in more and smaller partial groups following the corresponding factor rankings by the sobol method the three error measures and the decrease in variance are visualized in fig 4 excluding factors with rmae greater than 50 where the vertical lines indicate confidence intervals for each error measure with n y 1000 samples and m b 1000 bootstrap replicates changes in r increase dramatically when up to 17 factors are set to 0 25 if r was used to measure the error induced by using the simplified sobol g function 12 factors would finally be screened for factor fixing the change in r is then under 6 in contrast rmae is nearly 20 and the upper bound in the changes of rv is also above 6 the decrease in variance is within the 95 cis of rv indicating that sobol indices provide expected changes in the variance of qois the other error measures could not be directly calculated from the sobol indices direct evaluation of error is necessary and particularly important in this case as rmae is the first to exceed the acceptable error threshold the large uncertainty in rv from fixing 9 10 and 12 factors requires a more precise estimation with more model runs considering that the mean of errors is far below the threshold while the upper bounds of confidence are beyond it fig 5 plots the variation of all three metrics along with increased sample size n y where rv shows the highest variation and r the smallest the large variation in the mean of 1 rv and 1 r with n y from 100 to 1000 suggests that the corresponding samples did not cover the factor space adequately and caused biases to the estimation of error measures as samples are reused as the sample size increases this bias then decreases gradually as might be expected different error measures can necessitate different sample sizes we can see that correlation r shows an acceptable error 6 even with low sample sizes in the case of rv n y of up to 2500 is required to determine that fixing 10 factors yields an acceptable error which is much larger than the number of model runs required by the other two error measures rmae rises rapidly as the number of groups fixed increases yielding nine factors that could be fixed as described in section 4 2 this is consistent with the finding of sarrazin et al 2016 that r could still be high even if the difference between each pair of conditional and unconditional outputs may be large therefore the residual performance measure rmae dominates the determination of the fixed factor set for the sobol g function of course in practice the choice of error measure would also be dictated by the objectives of the modeling exercise 4 5 default value evaluation the choice of default values for fixed factors can affect errors in qois and therefore those factors that can be reliably fixed as illustrated in section 2 fig 6 shows the effect of the selected default values on the three error measures for the sobol g function based on partial sorting using the morris method with n 80 in section 4 2 we concluded that factors in two blocks in blue and orange could be fixed if 0 25 was treated as the default value here in order to assess the effects of default values on the decision multiple default values for variable x were selected ranging from 0 0 to 0 5 the response from 0 5 to 1 is similar due to the symmetry of the sobol g function more values were sampled between 0 20 and 0 30 to refine the shape of the curve sobol et al 2007 provide a similar analysis using squared errors normalized by the variance but with a larger number of factor values the error measures rmae rv and r respond quite differently when the default value of x is varied group 1 and group 2 in different colors exhibit a similar trend for each measure while as expected errors from fixing group 1 are much smaller overall for x 0 5 the closer x is to 0 25 the smaller rmae is this indicates that selecting 0 25 as the fixed value maximizes the number of factors that can be fixed for a given error threshold when x was set to 0 0 or 0 5 the rmae value due to fixing factors in the non influential group alone group 1 can even reach 7 4 however changes in rv decrease with x close to 0 25 and rv is within 1 0 0 05 if the default x ranges from 0 2 to 0 3 when fixing group 1 in common both rmae and rv will decrease by reducing the applicable range of factors to for example 0 2 0 3 rv could be larger than 1 this is caused by the sobol g function being the product of linear effects of each variable let f z 1 z 2 g z 1 h z 2 and g and h have unit variance and zero mean the variance of f is then just the product of the variances of g and h i e 1 if we fix z 2 at some value w such that h w 1 then the variance v f z 1 z 2 w v f z 1 z 2 considering that decrease in variance is within the 95 cis of rv with 0 25 as the default values in section 4 4 the decrease in variance therefore is not a sufficient measure of the effects caused by fixing factors at specific values this is because typically used variance based metrics marginalize out the insensitive parameters but in practice the variables are not marginalized but fixed this means that the contribution of a factor to the total variance estimated by these approaches can be misleading interestingly r remains almost the same for each group indicating that r is not affected by different default values which could be explained by the property that the sobol g function is the product of linear effects of each variable therefore r might result in more factors being fixed if used as the only error measure in summary the effect of default values suggests measuring the errors due to factor fixing at a fixed position may cause large error even when the expected sensitivity of fixed factors is small developing an efficient algorithm for choosing the best values to use for fixing is interesting but beyond the scope of this paper 5 discussion 5 1 comparison of errors using different screening rules in this paper we have discussed the effect of factor fixing on model quantities of interest and introduced an adaptive strategy for factor screening the screening is based on evaluation of model errors using a partial sort strategy to consider the uncertainty in factor importance rankings factors that can be set to default values with output errors under a given percentage here 6 is used for illustration are then identified a summary of the computational cost in calculating error measures is provided in table a1 in the appendix the same nine factors for fixing were identified by both morris and sobol gsa methods for the application to a test function sobol g function which include all the non influential and some of the weakly influential factors defined by sheikholeslami et al 2019 these results can be compared with other factor screening methods that do not directly evaluate the error due to factor fixing if using 1 as the threshold for the sobol total sensitivity index in distinguishing insensitive factors then all the ten factors in the bottom two groups in table 1 would be screened as insensitive and the resulting rmae would be 7 7 higher than the desired threshold of 6 the threshold of 5 used in jawad et al 2015 would screen 11 factors for fixing for morris screening the six least sensitive factors in blue in fig 3 would be screened as non influential and set to default values when adopting the 1 cumulative sensitivity indices cut off used in previous studies jia et al 2018 sarrazin et al 2016 made a similar finding that the typical screening threshold used for variance based gsa methods 0 01 is more generous than a test based on the error in qois in their case the ks test it is also worth noting that if the error threshold is more strict or more lenient fewer or more factors may be identified for fixing using the same process 5 2 limitations and advantages of the method for sensitivity ranking the presented study uses bootstrapping of sensitivity indices in order to quantify the uncertainty in the ranking of factors however bootstrapping may provide misleading results if factor space has been insufficiently sampled or structured rather than random sampling methods are used in such cases uncertainty in factor ranking would need to be quantified using other methods such as repeated sampling without replacement nevertheless it is worth noting that in the proposed method uncertainty in factor ranking is primarily used to reduce noise in the results factors with an uncertain ordering can be fixed as a group rather than individually from this point of view some uncertainty in bootstrapping is permissible which is also why we invoke 95 confidence intervals this uncertainty is then mitigated by the fact that the proposed method then tests whether results stay the same when the sample size for gsa is further increased and increasing sample size tends to increase the certainty in the ranking and therefore reduce the effect of any error in this noise filtering process 5 3 emerging opportunities for a new research agenda factor fixing is an important practice for reducing the factor space in many environmental modeling contexts garcia et al 2019 varella et al 2010 the key argument of this paper is that errors of interest due to factor fixing need to be estimated here we have taken a necessary first step of demonstrating a proof of concept on a well known function for which there are existing results in the literature it provides guidance on fundamental issues in factor fixing and a framework for undertaking ff for other test functions and for real world applications such as to spatio temporal environmental models only by testing such a framework on many applications might its methods be enhanced we contend that evaluating error due to factor fixing opens a new field of work that echoes and reinforces certain key trends in best practice environmental modeling this paper aims to open up opportunities within a new research agenda rather than providing the last say on the topic we identify five specific opportunities in the use or development of factor fixing from the perspective of computational considerations we would like to draw attention to two aspects how to evaluate error efficiently and for which combinations of factors to evaluate error tackling these two points is particularly important when applying the approach to higher dimensional modeling problems for which factor fixing becomes more essential to improve the broader applicability of factor fixing three opportunities stand out for further investigation determination of error metrics and error thresholds accounting for error due to factor fixing in selection of default values and the transferability of error estimates as an example of how these opportunities come together we discuss specific challenges and opportunities of factor fixing for inverse problems and integrated environmental modeling 1 how to evaluate error efficiently while the proposed method aims to reduce the number of error evaluations even a single error evaluation may still be potentially prohibitive for very expensive models or those requiring large numbers of model runs to explore factor space an obvious strategy would involve using model emulation asher et al 2015 mara et al 2017 palar et al 2018 ratto et al 2012 to assist in assessing factor fixing options for environmental models which can substantially reduce computational costs emulators may introduce further biases in predictions such that resulting uncertainties in sensitivity analysis and the calculation of error measures will need to be addressed developing adaptive sampling approaches for error evaluation is another promising way to reduce the computational burden for example aim to calculate errors in qois by recycling samples available including those from sensitivity analysis the uncertainty or bias of errors in such a process should then be recognized due to the samples being structured or insufficiently taken if it is not possible to evaluate the error due to factor fixing at all this should however raise questions about the effect of factor fixing on the credibility of the qois produced 2 selecting combinations of factors requiring error evaluation building on the tradition of factor fixing based on sensitivity analysis in environmental modeling this paper has evaluated error for combinations of factors defined by their ranking however this assumes a sufficient alignment between the sensitivity indices and the qoi for which error will be tested in principle sensitivity analysis is being used to avoid testing all possible combinations of factors for fixing in a high dimensional model which in turn impacts on both the computation for error evaluation and the reliability of a reduced model of a given order there is potential to explore other heuristics or algorithms to achieve this alignment aim some of these may indeed be variants of existing sensitivity indices that capture the effect of factors on different summary metrics the sequence in which combinations of factors are tested also matters in the presence of interactions between factors where the effect of fixing one factor may be compensated by another suggesting that factors may need to be considered as a group rather than individually future work may test how sobol sensitivity indices for instance that elicit combinations of factors could be used for this purpose 3 determination of error metrics and error thresholds determining what metrics can or should be used for different problems to evaluate the performance of models is a broad and perennial topic in environmental modeling bennett et al 2013 moriasi et al 2007 gleckler et al 2008 jackson et al 2019 factor fixing emphasizes two key requirements specification of acceptable error thresholds and measurement of the difference between a full model and its reduced model the use of metrics is to some extent left to the modeler s discretion but needs to reflect the purpose of modeling be decision relevant and have an intuitive interpretation in order to be able to evaluate whether error is acceptable for example metrics indicating the changes in appropriate uncertainties are preferred if a reduced model is to be used for uncertainty analysis multiple metrics are recommended in practice as the multiple influences of factor fixing on the qoi are unlikely to be reflected by a single metric jackson et al 2019 acceptable error thresholds also need to account for associated uncertainty in the error estimate and the potential for acceptability of errors to be dependent on what is achievable thus whenever the precise purpose of modeling can be considered somewhat flexible in a case study to support decision making needs error thresholds may be defined iteratively in order to chase useful conclusions from the analysis the future therefore requires both exploration of error metrics suitable for a factor fixing context in a wide scope of cases and pursuit of guidelines for the assessment of errors 4 how selection of default values should take into account error due to factor fixing the determination of default values at which to fix factors is another challenge in the application to real world models especially when there is little knowledge of what values are appropriate in advance three common practices are to fix factors at their nominal values in the model the average of their distributions considering that the variation caused by fixing at average values might be smaller and values at random by treating the model as a stochastic model if the computational expense to find their location is affordable and the aim is to find the maximum number of factors for fixing values in the applicable range resulting in the least error while fixing factors might be preferred it would also be more conservative for the screening of parameters to evaluate error by fixing factors at values that cause the largest error this minimizes the risk of underestimating the errors from factor fixing some of these approaches take into account the error due to factor fixing explicitly or implicitly and others simply take the default value as externally specified best or reasonable options in different case studies will need to be tested so as to provide ongoing advice for the community 5 the transferability of error estimates the transferability of error estimates needs to be seen in a broader context related to all four points discussed above as well as the conditions under which a model is used caution should be taken when transferring a reduced model from a different study as the experimental design and conclusions depend on each research specific context for example factor combinations in hydrologic modeling that are determined for fixing for annual discharge volume may not work for daily flow simulation as with performance assessment in environmental modeling generally estimation of factor fixing faces issues under non stationary conditions i e where calibrated model parameters change according to time period analysed on one hand the importance of a factor varies among different conditions e g wet versus dry years in a hydrological model response basijokaite and kelleher 2021 on the other hand the reasonable values at which to fix a factor may change due to non stationarity pathiraja et al 2016 and a fixed parameterization is thus not sufficient to represent varied patterns sadegh et al 2015 to spot the mismatch in transferability one can detect whether there is non stationarity possibly by checking the long term trends such as precipitation sequence or other physical characteristics in a catchment kundzewicz and robson 2004 sadegh et al 2015 common strategies include understanding the conceptual role of factors under varying conditions e g the climate condition the qoi especially those fixed and comparing errors in calibration and validation if the error is small in calibration but large in the validation conditions it could be caused by a fixed factor determination of factor fixing and default values then requires re evaluation evaluation of errors due to factor fixing is at the cliff face of how model complexity relates to non stationarity it literally evaluates how removing a degree of freedom affects performance in one or multiple different conditions 5 3 1 key application area factor fixing for inverse problems inverse problems are a key application area for factor fixing where it is commonly implemented before conditioning parameters with observations error due to factor fixing is however complex in this setting factor fixing collapses the posterior for that parameter resulting in other parameters taking on a compensatory role error in the multi variate posterior is therefore already large but error in the marginal distributions of other parameters or their optimized values might still be acceptable as might error in model predictions and predictive uncertainty for non identifiable parameters the error introduced into the distribution amounts to treating the default value as additional information which again may or may not impact on predictions in any given conditions evaluating error in predictions due to factor fixing requires solving the full inverse problem raising the stakes for achieving computationally efficient estimates with an acceptable error at the same time when model pre calibration is accessible factor fixing needs to integrate the effects of observational data parameter uncertainty and model prediction for example by using the prediction sensitivity of parameters as indicators of parameter importance especially if a model is used for prediction tiedeman et al 2003 2004 while further work is required focusing on error due to factor fixing in inverse problems potentially represents a paradigm shift in which analysis is driven by adaptive construction of surrogate models that help to quantitatively explore the dynamics between providing additional information in the form of fixed parameters changes to the posterior distribution of the remaining parameters and changes to predictions all while simultaneously obtaining iterative preliminary estimates of both parameter and predictive uncertainty recognizing the necessity of error evaluation is just the start of a new conversation 5 3 2 implications of research agenda for integrated environmental models factor fixing in environmental modeling to some extent simply highlights existing issues because it involves modelers consciously inducing an error relative to a reference more complete model de baan 2020 matott et al 2009 environmental models can be composed of multiple sub models with many uncertain parameters fu et al 2019 tscheikner gratl et al 2019 and fixing factors in a component model may cause cascading effects on the whole integrated system due to the computational ease of running a sub model rather than the whole integrated system error evaluation in the sub model can be checked first before calculating the final output for the sake of supporting decision making on environmental management problems of concern error measures selected for ff also need to be easily communicated or even agreed upon to ensure the qoi are those that stakeholders care about in the case of integrated models issues with high dimensionality are further highlighted because factor fixing or its variants are so often used as a solution without testing their implications for example parameters in environment models such as catchment water quality models are spatially variable a parameter may be critical in a sub area even though it might be of little importance as indicated by sensitivity analysis over the whole study region fixing such a parameter would result in inaccurate simulation locally in principle we call for efforts to quantitatively evaluate the error induced by dimensionality reduction techniques even in large multi component integrated models in practice we recognize that this is a research agenda that will be open for some time to come and that transparent pragmatic discussion around this topic will remain a key part of the solution to ensure models are credible legitimate and salient 6 conclusions an adaptive method has been proposed and evaluated against previous practice on a well known test function its basis and methods seem to warrant application to real world models where factor fixing is often considered necessary to reduce model dimensionality and computational load the work has motivated questions and issues that are outlined as part of an ongoing research agenda with respect to factor fixing and environmental modeling in general based on all the analysis in the paper we conclude as follows 1 fixing sensitive factors can lead to substantial errors that have commonly been ignored estimating predictive error in the quantity of interest is a necessary method for modelers to understand the loss incurred by factor fixing and one can determine the fixed factor set adaptively by considering the trade off between the accuracy of model performance and affordable computation 2 the effect of factor fixing can be investigated systematically within the adaptive framework described a bootstrap based partial sort is an efficient approach for identifying the relative importance of factors in consort with global sensitivity analysis the gsa methods of morris and sobol share similar results here for the g function in terms of factor screening albeit with morris being much more computationally efficient 3 the selection of default values can considerably influence the error caused by factor fixing even if the sensitivity indices suggest the factor is of low importance this in turn means that the use of default values in models can be critical and warrants investigation in determining the number of factors to fix when the decision is based on anticipated error 4 a focus on evaluation of error induced by factor fixing and potentially other dimensionality reduction techniques appears to open an interesting new research agenda that both highlights and will continue to provide new perspectives on common issues in best practice environmental modeling declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank all reviewers and editors for their helpful comments joseph guillaume received funding from an australian research council discovery early career researcher award project no de190100317 qian wang s research was supported by the national natural science foundation of china grant no 51879068 and key r d program of ningxia grant no 20175046802 john jakeman s work was supported by the u s department of energy office of science office of advanced scientific computing research scientific discovery through advanced computing scidac program sandia national laboratories is a multi mission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 the views expressed in the article do not necessarily represent the views of the u s department of energy or the united states government appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105290 
25683,groundwater management of coastal aquifers is a scientific and engineering challenge as coastal aquifer systems are hydrogeologic features that are hydraulically connected with the sea while at the same time are stressed with extensive abstraction practices for irrigation to remedy this managed aquifer recharge mar is proved to be a sound and effective technology that is able to fill the gap between groundwater supply and demand essential part of this procedure is reliable groundwater modeling that simulates effectively all involved hydrologic processes in order to be able to predict the response of the system under different natural or artificially induced conditions this paper describes the application of a modeling tool for the simulation of the relative hydrologic processes between the open filter pipe of the injection well and the surrounding aquifer material with the finite difference method more specifically a mar pilot setup with a horizontal direction injection well was simulated with the conduit flow process cfp of modflow 2005 code to investigate further the applicability of cfp code the hddw was conceptualized with the simpler modflow wel package and the results of the two models were compared for the two models sensitivity analysis was conducted using ucode 2014 both models were calibrated successfully as the simulated values showed an acceptable fit to the observed data the main difference between the two packages was observed in the rise of groundwater level along the horizontal well to this end it was indicated that cfp offered a more realistic representation of the horizontal well pipe graphical abstract model conceptualization of the modflow cfp application for hddw injection well simulation image 1 keywords conduit flow process cfp for modflow 2005 managed aquifer recharge mar horizontal direction drilled well hddw ucode 2014 1 introduction managed aquifer recharge mar refers to processes and methods used for introduction of freshwater and or treated water into a contaminated and or depleted aquifer system the main aims of the mar approach are to i quality and quantity rehabilitation of the treated system and or ii freshwater storage for later use dillon 2005 summarizes and defines some of the most common mar techniques amongst other strategies mar techniques have a wide range of applicability in coastal aquifers as a measure of management and control of seawater intrusion as well as to restore groundwater equilibrium maliva et al 2020 case studies from many places around the world have presented different mar schemes for mitigation of seawater intrusion and rehabilitation of contaminated or depleted groundwater systems e g dillon et al 2019 masciopinto 2013 zuurbier et al 2017 ganot et al 2017 janardhana raju et al 2013 aquifer storage and recovery asr has been widely applied as a way of freshwater injection into brackish aquifers through vertical well s with main objective freshwater storage for later use hussain et al 2016 chen 2014 pyne 2015a b the different densities between saline water into the aquifer and freshwater of the injectate lead to density effects that may cause mixing of freshwater with saline water locally resulting in saline water withdrawal during well recovery ward et al 2007 zuurbier et al 2015 introduced the concept of replacing a vertical well of a typical asr setup with a horizontal directional drilling well hddw as a solution to reduce the consequences of buoyancy effects mixing and freshwater displacement and increase recovery efficiency horizontal or slanted wells have a wide range of applicability in oil and gas industry e g jamiolahmady and danesh 2007 chang et al 1989 lin and zhu 2014 in the groundwater sector these wells usually function as collector wells for mine dewatering water supply or aquifer drainage hantush and papadopoulos 1962 horizontal wells have also been utilized for groundwater remediation as an effective way to access a larger contaminated area due to the horizontal well screen so as to apply a remedial treatment such as fluid abstraction injection and or abstraction of air miller 1996 weesner et al 1998 mohamed and rushton 2006 set up a field experiment in a shallow aquifer in loba a coastal village in malaysia to test the performance of the well to provide freshwater another application of hddws in the groundwater sector is aquifer thermal energy storage considering the big advantage of horizontal wells to be structured underneath buildings and in shallow aquifer layers offering a wider injection field aquifer thermal energy storage ates n d although hddws are not commonly employed in full scale mar structures experimental facilities have incorporated hddw technology in coastal aquifers affected by seawater intrusion and groundwater depletion zuurbier et al 2014 proposed the freshmaker set up in which two different hddws were constructed in a pilot area in order to create a freshwater lens in a saline aquifer in zeeland the netherlands the set up included two parallel horizontal wells of 70 m length and 300 mm diameter in different depths where the shallower was injecting treated water and the deeper pumped brackish water simultaneously the simulation results with seawat indicated the shift of freshwater saltwater interface towards the sea as well as the increase of freshwater lenses thickness in the aquifer in order to propose and explore an effective scheme for groundwater management it is important to predict the impact of it in a certain groundwater system distributed numerical modeling for the simulation of groundwater flow processes constitute an efficient and widely used approach for forecasting groundwater quality and quantity changes determine protection and mitigation strategies and investigate a contaminant source through the application of flow and transport models of several numerical methods karatzas 2017 numerical modeling has been utilized for numerous groundwater problems for example zhou et al 2017 investigated the effect of pumping on groundwater levels under the influence of tides through the variable density seawat model langevin et al 2007 in china sadeghi tabas et al 2017 coupled a modflow model harbaugh 2005 with optimization algorithms to define the optimal solution for the pumping regime in a basin of iran while deng et al 2018 used modflow code to simulate the effect of groundwater abstraction to land subsistence problems groundwater modeling is also applied in arid and semi arid regions as a supporting tool for effective management planning and decision making in the groundwater sector bittner et al 2020 rossetto et al 2018 pisinaras et al 2013 koltsida and kallioras 2019 roy et al 2016 boskidis et al 2012 vernoux et al 2020 kopsiaftis et al 2009 lautz and siegel 2006 several studies include the implementation of modeling activities in a mar project as a tool to predict the response of a hydrogeological system to an artificial recharge program and future impact ringleb et al 2016 a wide range of numerical models have been applied for the investigation of mar projects russo et al 2015 simulated potential mar concepts in a coastal basin using modflow 2005 harbaugh 2005 coupled with farm process schmid and hanson 2009 sallwey et al 2018 used hydrus 2d 3d for the unsaturated zone so as to design a pilot scale infiltration basin and tzoraki et al 2018 simulated the efficiency of a coastal mar system in cyprus utilizing geochemical model phreeqc parkhurst and appelo 2013 and groundwater feflow trefry and muffels 2007 finite element model maples et al 2019 proposed the locations mar in central valley california through geostatistical modeling and then simulated the performance of infiltration basins with parflow ashby and falgout 1996 a three dimensional 3d variably saturated hydrologic modeling code that couples surface and subsurface flow clark et al 2015 addressed potable water supply reliability from a stormwater mar system with watercress waterselect n d hydrological model by simulating runoff recharge and recovery for different rainfall catchment and aquifer conditions in a brackish coastal aquifer in salisbury australia a physical experiment combined with feflow 7 2 diersch 2014 was utilized by wu et al 2021 to test the performance of mar in geologically heterogenous aquifers based on tailan basin in nw china the authors concluded that clay lenses increase residence time of injected water with little effect on regional flow while high permeability layers decrease freshwater lenses and move stagnation points downstream horizontal groundwater wells have been simulated as collector wells in many groundwater modeling case studies ismail et al 2012 and kelson 2012 simulated horizontal collector wells using modflow drain package drn haitjema et al 2010 suggest that in a 3d model the horizontal well can be simulated by small constant head cells mnw2 package incorporated in modflow 2005 is designed for simulating wells that penetrate more than one layers or extend in multiple cells such as slanted or horizontal wells konikow et al 2009 as for horizontal injection abstraction wells wang et al 2014 proposed an analytical solution model that simulates horizontal well performance as a specified flux boundary in 2 d environment in order to explore groundwater flow characteristics including release capture zones from the horizontal well also zuurbier et al simulated a horizontal well mar system with the modflow wel package zuurbier et al 2014 in the present work the simulation of the related hydrologic processes between the open filter pipe parts of an injection well and the surrounding aquifer are described the goal of this research is to gain insight on modelling the performance of hddw as recharge wells a modeling code that was developed to simulate processes and interactions between karstic aquifer conduits and geological matrix was utilized more specifically a horizontal direction injection well was simulated with the conduit flow package cfp of modflow 2005 code shoemaker et al 2007 while sensitivity analysis was conducted using ucode 2014 poeter et al 2014 the code mainly operates under three distinct modes incorporating reynolds numbers re to tackle the limitations of darcy s law for porous media ground water flow a cfpm1 that couples a discrete pipe network to modflow 2005 b cfpm2 that computes horizontal turbulent flow in preferential flow layers and c cfpm3 that couples simultaneously both the above the program is generally utilized to simulate groundwater flow in fractured and or karstic aquifers pouliaris 2019 simulated a karstic aquifer in lavrion greece by incorporating modflow cfp code to model a network of discrete conduits in the fractured rock hill et al 2010 conducted a modeling exercise to compare the performance of modflow cfp against modflow 2005 for a fractured karstic aquifer at a local scale site near weeki wachee florida this research suggested that cfpm1 improved the match between observed and simulated discharges zheng et al 2021 used modflow cfp to evaluate the hydrogeological impact of tunnelling in karstic aquifers in their study both the tunnel and the karstic underground conduits were simulated as discrete conduit elements with cfpm1 gholizadeh et al 2020 simulated the interaction between a water conveyance tunnel and the surrounding aquifer that is forms in fractured rocks located in urumieh dokhtar volcanic belt in iran their results indicated that modflow cfp code predicted with high accuracy the spatial variation groundwater ingress and hydraulic head in the tunnel to the best of our knowledge this is the first attempt where a hddw is simulated using modflow and cfp package for an experimental pilot setup to test further cfp performance the hddw was also simulated with the simpler wel package and the results of the two models were compared moreover the accuracy of the two approaches is further tested using in situ data which were gathered during a monitoring campaign in february 2018 finally the sensitivity of the proposed approach is studied and reported by the means of sensitivity analysis 2 materials and methods 2 1 study area the study area is located in marathon ne part of attica greece and it covers an area of approximately 40 km2 groundwater management is of ample importance since many anthropogenic activities such as agriculture tourism and domestic development are highly dependent on groundwater resources for their water supply irrigation being the dominant groundwater consumer intensifies the environmental footprint in the local hydrosystem due to the exploitation of shallow groundwater resources zavridou et al 2018 perdikaki et al 2017 fig 1 presents the hydrogeological conditions in marathon plain where the two dominant aquifer units are the lower karstified marbles and the upper alluvial unconsolidated formation the alluvial aquifer is of lower hydraulic conductivity from 10 6 to 10 5 m s and has a thickness of a few meters up to 80 m forming a coastal plain area while the underlying and surrounding karstified formation of higher conductivity it ranges between 10 5 and 10 4 m s has a thickness of up to 400 m siemos 2010 the upper granular unit consists of sediments with uneven size distribution gravel sand clay and silty clay and additionally impermeable clay layers of small thickness lenses exist in various locations at the plain melissaris and stavropoulos 1999 the two units are hydraulically connected along their contacts both being in contact with the sea a critical point in the study area is a local karstic spring located at the ne part of the plain fig 1 with a mean discharge rate of 810 m3 h perleros 2001 both alluvial and karstic formations are exploited through multiple private groundwater wells fig 1 of two main characteristics a the ones that extend throughout the irrigation area within the alluvial aquifer mostly being shallow and b deeper wells that have penetrated the karstic formation all located within the area of contact between the alluvial and the karstic layer uncontrollable groundwater withdrawals in combination with un regulated irrigation scheme for the region has led to seawater intrusion for both aquifer layers psychoyou et al 2007 koumantakis et al 1993 floros 2016 the problem is more pronounced in the upper alluvial aquifer especially during the dry season when the precipitation is limited and the irrigation water demand is increased due agricultural activities perdikaki et al 2020 2 2 set up of the mar pilot and operation an integrated research scheme was conducted at the coastal zone of marathon that involved unsaturated zone zavridou et al 2018 and surface groundwater hydrology perdikaki et al 2017 field and laboratory measurements a complex pilot mar scheme was introduced developed and tested in order to mitigate groundwater quantity and quality issues in the study area the experimental pilot site involved a combined system that abstracts groundwater from the deeper karstic aquifer formation after proper treatment with reverse osmosis ro for desalination and advanced oxidation process aop for the reduction of organic pollutants karstic water is injected directly into the saturated zone of the upper alluvial aquifer layer fig 2 as shown in fig 2 high salinity groundwater app 4500 μs cm is being abstracted from a karstic aquifer directly pumped from a karstic spring which is directly transferred to a hybrid treatment unit which operates in dual mode advanced oxidation is being used to reduce the agrochemical loads within the abstracted groundwater while reverse osmosis is applied in order to reduce salinity approaching drinking water standards after the treatment process the treated water is directly injected into the unconsolidated formation through a hddw that consists of a pvc pipe of 75 mm diameter and penetrates both unsaturated and the saturated zone of the alluvial aquifer while the recharge water is injected through an open screen filter pipe into the saturated zone the directional borehole is 50 m in total length while it reaches a maximum depth of 4 1 m at the middle point of its total length the pilot site also includes the installation of two vertical piezometers that are used for the continuous monitoring of the hydrodynamic conditions of the mar experiment as well as to determine the boundary conditions of the model 2 3 hydrogeological setting of the pilot site the stratigraphy of the selected location was investigated during a vertical well drilling next to the hddw at that point the aquifer unit is not homogenous due to different grain size distribution the first few meters approximately 3 m thick are covered with fine grained silty clay material with low permeability below this layer the target aquifer is a layer of higher permeability with a thickness of 20 m composed of gravel sand deposits the basement of the latter unit is composed of a clayey impermeable layer both layers were modeled as confined units due to the existence of the fine grained layer at the top of the model the depth to the groundwater in the pilot area is approximately 2 4 m while the piezometric surface ranges between 0 1 and 0 7 m above mean sea level amsl 2 4 simulation of groundwater injection using the conduit flow process program for modflow 2005 in this study the pilot mar scheme is simulated using modflow 2005 harbaugh 2005 code of usgs primarily focused on the performance of the hddw the code solves the three dimensional equation eq 1 of groundwater flow in a porous medium as described by mcdonald and harbaugh 1986 1 x k x x h x y k y y h y x k z z h z w s s x t where k xx k yy and k zz are the hydraulic conductivities along x y and z axis respectively l t h is the potentiometric head l w is a volumetric flux per unit that represents the sinks and sources of water in or out of the groundwater system t 1 ss is the specific storage of the porous medium l 1 and t is time t due to the complexity of eq 1 an analytical solution is almost always impossible so that a finite difference method is applied in modflow 2005 code in order to simulate groundwater flow of a system in space and time harbaugh 2005 variable density flow was not applied in this study the model domain represents a rectangular area of 112 000 m2 with a fine grid of 2 2 m the model contains two different layers a the first layer is 3 m thick and is of low hydraulic conductivity representing a shallow silty clay layer while b the second layer represents the main study aquifer that has a thickness of 20 m and is composed of materials of larger particle size with higher hydraulic conductivity the two aquifer layers are considered homogenous the hddw is 50 m long and it reaches the depth of 4 1 m the modeling task of simulating the hydrodynamics of water injection into an unconsolidated aquifer formation through a hddw was realized with the application of cfp mode 1 cfpm1 that incorporates flow within the aquifer modflow 2005 flow in cylindrical pipes hddw and groundwater exchange between the aquifer and the pipe the flow equation within the conduit is solved with two different equations for laminar and turbulent flow for laminar flow the hagen poiseuille equation is 2 q a ρ g d 2 δ h 32 μ δ l τ where q is the volumetric flow rate l3t 1 a is the cross sectional perpendicular to the flow l2 ρ is the density of the water ml 3 g is the gravitational acceleration constant lt 1 δh is the head loss along the pipe l of length δl l μ is the absolute or dynamic viscosity of water ml 1t 1 and τ is the tortuosity unitless of the pipe shoemaker et al 2007 for turbulent flow the darcy weisbach equation is solved as 3 q a δ h d 2 g f δ l where f is the friction factor dimensionless shoemaker et al 2007 flow equations in cfpμ1 also include corrections for the flow in partially filled pipes and correction for groundwater exchange between the aquifer and partially filled pipes another asset of cfp program is the incorporation of the recharge package crch that has the ability to simulate the artificial water injection in the hddw moreover we employed the already used wel package for simulating the effect of mar through the hddw and we also compared the results with cfp scheme previously described the hddw was conceptualized through eight wells fig 4b the screen of each well was placed at different depth in order to capture the various depths of the constructed horizontal well the depths ranged from 3 1 m to 4 1 m below ground and are all located at the bottom layer groundwater injection for the proposed mar setup was distributed equally at the eight wells the boundary conditions that represent sinks and sources for the aquifer are presented in table 1 fig 4 shows the location of each boundary condition for the two models the main difference is the use of the two different packages for the representation of the hddw cfp and wel package constant head boundary chb were assigned at the model cells that are in hydraulic contact with the sea whereas at the ne and sw boundaries of the model no flow conditions were used last the general head boundary ghb package was used to simulate the lateral inflows from the nw upstream part where the hydraulic heads of the boundary were based on continuous monitoring through monitoring wells installed at the head observation wells as shown in fig 3 the model was simulated for 10 stress periods with a total duration of 300mins table 2 and then calibrated using measurements that were conducted during experiments of freshwater injection r o water into the aquifer through the hddw freshwater injection in the hddw varied from 0 94 to 3 5 m3 h 2 5 sensitivity analysis and calibration of model parameters sensitivity analysis and parameter estimation was performed effectively with ucode 2014 of usgs poeter et al 2014 using experimental measurements conducted in the mar pilot experiment in the cfp model the geometrical characteristics of the conduit was known diameter tortuosity for roughness height a small value for smooth pipes was obtained moody 1944 while upper and lower reynolds number were included to the sensitivity analysis for the groundwater well pipe permeability conduit wall conductance lower reynolds number and upper reynolds number were also included as a parameter in the parameter estimation process these parameters were added manually in ucode 2014 as modflow 2005 code does not recognize any parameter type for the cfp package all the parameters assigned in ucode 2014 along with the starting values are presented in table 3 cond cfp low re and high re parameters were utilized only at the sensitivity analysis of the cfp model as they are those that define conduit performance in ucode 2014 poeter et al 2014 sensitivity analysis is performed for each parameter through an iterative process the code quantifies the sensitivity of a model parameter by using fit independent statistics those statistics are useful tools so as to identify the importance of the observations for each parameter the importance of a parameter to prediction and the importance of each observation to prediction hill and tiedeman 2007 parameter sensitivity levels were calculated through composite scaled sensitivities css that counts the available information given from the observations in order to estimate one parameter css is a function of dimensionless scaled sensitivities dss so that cssj for the jth parameter calculated for nd observations so that 4 c s s i 1 n d d s s i j 2 b n d 1 2 dss measures how much the change in a parameter affects the model by comparing the initial parameter value with a perturbed one for a specific iteration while it is calculated using the following function 5 d s s y i b j b b i ω i i 1 2 where y i is a simulated value b j the jth parameter y i bj is a derivative that counts for the sensitivity of the ith observation to the jth parameter b is a vector containing the parameter values and ω ii is the weight that is assigned to the ith observation hill and tiedeman 2007 in general css provides a comparison between parameters in order to identify those for which the observations provide more information another important value is the parameter correlation coefficients pcc that indicate the parameters that are highly correlated with one another in a case of extreme correlation those parameters cannot be estimated independently pcc is calculated using the following equation 6 c o v b j k v a r b j j v a r b k k where cov b jk is the covariance between two parameters and var b jj var b kk the variances of each parameter highly correlated parameters have a pcc absolute value of more than 0 95 hill and tiedeman 2007 as for the parameter estimation process through ucode2014 it is also an iterative process and includes an objective function that is defined as a sum of squared and weighted residuals of the observation minus the associated simulated value the main purpose of the parameter estimation process is to find a set of parameters that produces the smallest value for the objective function poeter et al 2005 3 results 3 1 sensitivity analysis results sensitivity analysis performed revealed the level of sensitivity for all the parameters presented in table 3 and the correlation amongst them based on groundwater level values measured during experimental aquifer recharge in the pilot area for the cfp model sensitivity analysis results suggested that the model is highly affected by the parameters that are related to the bottom aquifer layer and the recharge of the pilot area from the surrounding aquifer fig 5 a the highest css values were estimated for hk par2 along with ghb par parameter while the cond cfp parameter has a much lower css value parameters hk par1 ss par low re and high re have almost zero level of sensitivity according to the analysis the output of the sensitivity analysis proves the importance of hk par2 for the simulated results as it is the parameter that relates to groundwater movement in the target aquifer layer bottom layer as far as the artificial recharge is concerned four sets of parameters are highly correlated fig 5b as the absolute value of pcc is equal to 1 the extreme correlation between the hydraulic conductivities of the two layers derives from their interconnection the highly sensitive parameter of ghb in conjunction with the extreme correlation of the hydraulic conductivity parameters with the ghb conductance relates to the fact that this boundary is a major source of inflow for the two layers the sensitivity analysis results of the model with the wel boundary are presented in fig 6 the two most important parameters are hk par2 and ghb par1 in this analysis ss par1 is an influential parameter for model output indicating the change in storage in the aquifer the sensitivity analysis results revealed that there is no correlation between the parameters assessed 3 2 model results a parameter estimation process performed for the two models with respect to the sensitivity analysis results in the cfp model parameters hk par2 ghb par1 and cond cfp were utilized in the process and the final estimated values were 0 00105 m min 1 87e 5 m2 min and 5 23e 6 m2 min respectively the model was calibrated successfully and was also validated using different field experimental measurements in the wel model hk par2 ghb par and ss par were estimated with ucode the best fitted parameter set was 1 81e 04 m min 2 57e 6 m2 min and 0 001347 m 1 respectively fig 7 and fig 8 show the differences between the results of the simulated model against the measured values in fig 7 the evolution of observed and simulated heads for the two observation wells w1 w2 is represented for each stress period during artificial aquifer recharge the observed vs simulated fig 8 diagram presents an acceptable fit of the model results to the observed values for both cfp and wel models as the residuals do not exceed 0 1 cm in the next figure fig 9 the cumulative water budget of the two models at the end of the final stress period stress period 10 is presented the water budget is convenient in order to specify the fluxes to the aquifer and understand the interaction of several components of the hydrosystem in this case study the main inflow of the model originates from the cfp and the wel package in fig 9a and b respectively i e the hddw the ghb inflows are also noteworthy for the cfp model the change in storage in the groundwater budget of both models fig 9a and b occurs due to release of water through the aquifer when water level changes as a result of water injection a significant amount of water outflows from the chd package that represents the discharge of groundwater to the mediterranean sea fig 10 shows the piezometric conditions of the model at the beginning and the end of the simulation piezometry before groundwater injection is distributed uniformly throughout the domain and the piezometric head is being gradually reduced from the upper ghb boundary to the coast fig 10a the hydraulic head at the vicinity of the well in the first stress period is presented in fig 10b and is app 0 6 m amsl above mean sea level at the end of the simulation the hydraulic head rises in the area of the well for the cfp fig 10c and the wel model fig 10d as well there is a big difference between the results of the two models as in the cfp model the maximum level rise due to artificial recharge is 0 48 m while in the wel model the aquifer rises more than 1 8 m above the starting head 4 discussion although horizontal directional drilling has gained traction in the groundwater sector due to the decline in operational cost and several technical advantages over vertically drilled wells wang and zhan 2017 there is limited research in the performance of hddws for freshwater injection in a mar project e g zuurbier et al 2015 previous work has mainly focused on the investigation of horizontal or slanted wells and their performance as pumping collector wells rather than injecting a volume of freshwater in the aquifer e g collins and houben 2020 thus we considered that there is a gap in the literature for the use of horizontal wells in asr systems as well as the implementation of horizontal recharge wells in numerical modelling problems modflow 2005 cfp gave insight into groundwater flow in the target aquifer horizontal well performance and the water table hddw interaction observed data obtained from marathon pilot setup during artificial recharge through the hddw were considered significant to validate the applicability of modflow 2005cfp code for groundwater injection with horizontal wells to the best of our knowledge the application of cfp package in mar was verified with field observations for the first time furthermore the sensitivity analysis conducted indicated that the most influential parameters are related to specific aquifer characteristics i e hydraulic conductivity and external inflows i e ghb while the conduit parameters are of less i e conduit conductance or no lower reynolds number upper reynolds number importance for the model results this outcome could be useful in other modelling studies with horizontal injection wells compared to other packages that have been used by several researchers in the past for instance drn mnw2 wel this program has relatively low data requirements it focuses on the geometry of the conduit without ignoring groundwater movement in the conduit the exchange of water between the conduit and the aquifer friction factor and the head loss along the pipe in this research the drn package was rejected as it serves the opposite purpose of the study i e to drain water from the aquifer instead of inject water into the system in comparison to mnw2 cfp package requires less data to be assigned by the user while turbulent and laminar flow conditions are computed by utilizing the equations that describe adequately the flow in network pipes e g eck and mevissen 2015 with respect to pipe material roughness height offering a realistic representation of the horizontal well pipe the implementation of the simpler wel package for simulating the hddw was also utilized to investigate the main differences between the two packages the use of a single vertical well wel package would only simulate the amount of water that is injected at a certain point of the aquifer without taking into consideration the extended screen of the horizontal well this problem can be solved by utilizing multiple single wells at the extension of the open screen filter pipe perdikaki et al 2019 the use of multiple wells at specific depths along the length of the constructed hddw was considered critical so as to capture the area of influence but still it is not a realistic representation of the boundary in contrast to cfp the sensitivity analysis results were in accordance with the sensitivity analysis of the cfp model as the most influential parameters were the hydraulic conductivity of the target aquifer and ghb conductance both models had high coefficient of determination r2 and low root mean square error rmse as is presented in fig 8 showing satisfactory agreement between observed and simulated groundwater head values according to the results of the cfp model the hydraulic head at the end of the simulation is distributed evenly along the hddw and the release zone is well represented fig 10c on the contrary the results of the wel package fig 10d compared to the cfp showed greater discrepancy the difference in piezometry between the cfp and wel reaches 1 3 m the greater differences are observed at the locations of the wells conceptualization of the hddw with the wel package leads to local maximums of hydraulic head during artificial recharge as a result of the point recharge by several wells rather than diffuse recharge along the conduit under natural flow conditions this is not the case overall we consider that the natural hydrologic processes during artificial recharge with horizontal wells is best represented by the cfp package a fact that is indicated by the shape and elevation of piezometric curves variable density flow was not modeled in this study as modflow cfp is not being coupled with seawat langevin et al 2007 an option that would substantially improve its applicability to a wider range of hydrogeological problems although a variable density flow and solute transport conduit flow process model has been developed by xu et al 2019 there is not an open access software yet the incorporation of variable density flow in modflow cfp code would benefit on the investigation of new techniques including horizontal well technology for seawater intrusion for example the performance of horizontal wells as hydraulic barriers to seawater intrusion could be investigated in modeling studies as an evolution to the existing studies of hydraulic barriers e g pool and carrera 2010 botero acosta and donado 2015 the horizontal well screen along the coastline might create a more extended seaward hydraulic gradient our work has some limitations mainly concerning the limited experimental data the main aim of the present work was to investigate the capability of modflow2005 cfp to represent the actual hydrological processes in the horizontal well and the groundwater system and to compare it with a widely used package for water injection i e wel as a result the present research serve as an operational framework for modeling hddws for groundwater injection through a physically based software in our study we have chosen to use a high resolution grid i e 2mx 2 m in order to simulate the hydrological processes during the injection of the recharge water through the hddw based on our engineering judgement this approach can be deemed as reasonable for the specific case study as the pilot scale has low computational requirements however in order to model aquifer s response on a regional scale one should use appropriate grid refinement tools e g uniform fine grid variably spaced grid modflow lgr 5 conclusions among several methods and types an innovative mar technique involves the use of horizontal directional drilling wells as a way to restrain or prevent seawater encroachment towards the mainland as well as store freshwater within the aquifer future impacts of mar facilities can be safely predicted through the application of mathematical models that are able to efficiently simulate the relevant hydrologic processes that are dominant in association with each different mar technique the conceptualization of a water injection horizontal well took place using two several 3d finite difference modflow models in order to test the applicability of cfp package for hddw simulation and compare it with the simple wel package sensitivity analysis and model calibration was performed with ucode in both models by utilizing observed piezometric data obtained during artificial recharge in the pilot setup the two models were compared in terms of sensitivity analysis results model fit observed vs simulated values maximum hydraulic head values and piezometric conditions our results suggest that in both models hydraulic conductivity of the recharged aquifer and ghb conductance are of ample importance for the simulation in the cfp model the most influential parameters are associated with the aquifer properties rather than the conduit parameters more specifically conduit parameter values conduit wall conductance lower reynolds number and upper reynolds number are of lower importance for the results of the model however it is important to be stated that the results of the sensitivity analysis are case and metric dependent to this end a calibration procedure took place for the most sensitive parameters cfp and wel model were calibrated successfully the precision in the results of the two calibrated models compared to the observed hydraulic head during water injection indicate that both packages effectively simulate the performance of the horizontal injection well within an aquifer nevertheless piezometric head value and distribution around the horizontal well seems to be unrealistic in the case of the wel package as high values are presented locally at the location of the wells even though the conceptualization of the hddw with cfp package produced more realistic results as the piezometric surface rose evenly around the conduit considering all of the above we conclude that modflow2005 cfp code is a useful tool for simulating a horizontal directional drilling well that is incorporated in a mar setup and performs for freshwater injection purposes we propose further investigation of this modelling technique with longer data series as well as a full scale case study that implements the proposed mar scheme in order to test its feasibility for the reclamation of depleted aquifer systems and considering the technical assets of hddws finally we hope that our research could pave the way and encourage other researchers to incorporate the proposed methodology in order to test through mathematical simulation the adaptivity of artificial recharge with hddws in the scope of an integrated groundwater management plan that targets to groundwater storage and aquifer rehabilitation declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests andreas kallioras reports financial support was provided by european union acknowledgements this research is part of the project subsol bringing coastal subsurface water solutions to the market subsol has received funding from the european union s horizon 2020 research and innovation programme under grant agreement no 642228 
25683,groundwater management of coastal aquifers is a scientific and engineering challenge as coastal aquifer systems are hydrogeologic features that are hydraulically connected with the sea while at the same time are stressed with extensive abstraction practices for irrigation to remedy this managed aquifer recharge mar is proved to be a sound and effective technology that is able to fill the gap between groundwater supply and demand essential part of this procedure is reliable groundwater modeling that simulates effectively all involved hydrologic processes in order to be able to predict the response of the system under different natural or artificially induced conditions this paper describes the application of a modeling tool for the simulation of the relative hydrologic processes between the open filter pipe of the injection well and the surrounding aquifer material with the finite difference method more specifically a mar pilot setup with a horizontal direction injection well was simulated with the conduit flow process cfp of modflow 2005 code to investigate further the applicability of cfp code the hddw was conceptualized with the simpler modflow wel package and the results of the two models were compared for the two models sensitivity analysis was conducted using ucode 2014 both models were calibrated successfully as the simulated values showed an acceptable fit to the observed data the main difference between the two packages was observed in the rise of groundwater level along the horizontal well to this end it was indicated that cfp offered a more realistic representation of the horizontal well pipe graphical abstract model conceptualization of the modflow cfp application for hddw injection well simulation image 1 keywords conduit flow process cfp for modflow 2005 managed aquifer recharge mar horizontal direction drilled well hddw ucode 2014 1 introduction managed aquifer recharge mar refers to processes and methods used for introduction of freshwater and or treated water into a contaminated and or depleted aquifer system the main aims of the mar approach are to i quality and quantity rehabilitation of the treated system and or ii freshwater storage for later use dillon 2005 summarizes and defines some of the most common mar techniques amongst other strategies mar techniques have a wide range of applicability in coastal aquifers as a measure of management and control of seawater intrusion as well as to restore groundwater equilibrium maliva et al 2020 case studies from many places around the world have presented different mar schemes for mitigation of seawater intrusion and rehabilitation of contaminated or depleted groundwater systems e g dillon et al 2019 masciopinto 2013 zuurbier et al 2017 ganot et al 2017 janardhana raju et al 2013 aquifer storage and recovery asr has been widely applied as a way of freshwater injection into brackish aquifers through vertical well s with main objective freshwater storage for later use hussain et al 2016 chen 2014 pyne 2015a b the different densities between saline water into the aquifer and freshwater of the injectate lead to density effects that may cause mixing of freshwater with saline water locally resulting in saline water withdrawal during well recovery ward et al 2007 zuurbier et al 2015 introduced the concept of replacing a vertical well of a typical asr setup with a horizontal directional drilling well hddw as a solution to reduce the consequences of buoyancy effects mixing and freshwater displacement and increase recovery efficiency horizontal or slanted wells have a wide range of applicability in oil and gas industry e g jamiolahmady and danesh 2007 chang et al 1989 lin and zhu 2014 in the groundwater sector these wells usually function as collector wells for mine dewatering water supply or aquifer drainage hantush and papadopoulos 1962 horizontal wells have also been utilized for groundwater remediation as an effective way to access a larger contaminated area due to the horizontal well screen so as to apply a remedial treatment such as fluid abstraction injection and or abstraction of air miller 1996 weesner et al 1998 mohamed and rushton 2006 set up a field experiment in a shallow aquifer in loba a coastal village in malaysia to test the performance of the well to provide freshwater another application of hddws in the groundwater sector is aquifer thermal energy storage considering the big advantage of horizontal wells to be structured underneath buildings and in shallow aquifer layers offering a wider injection field aquifer thermal energy storage ates n d although hddws are not commonly employed in full scale mar structures experimental facilities have incorporated hddw technology in coastal aquifers affected by seawater intrusion and groundwater depletion zuurbier et al 2014 proposed the freshmaker set up in which two different hddws were constructed in a pilot area in order to create a freshwater lens in a saline aquifer in zeeland the netherlands the set up included two parallel horizontal wells of 70 m length and 300 mm diameter in different depths where the shallower was injecting treated water and the deeper pumped brackish water simultaneously the simulation results with seawat indicated the shift of freshwater saltwater interface towards the sea as well as the increase of freshwater lenses thickness in the aquifer in order to propose and explore an effective scheme for groundwater management it is important to predict the impact of it in a certain groundwater system distributed numerical modeling for the simulation of groundwater flow processes constitute an efficient and widely used approach for forecasting groundwater quality and quantity changes determine protection and mitigation strategies and investigate a contaminant source through the application of flow and transport models of several numerical methods karatzas 2017 numerical modeling has been utilized for numerous groundwater problems for example zhou et al 2017 investigated the effect of pumping on groundwater levels under the influence of tides through the variable density seawat model langevin et al 2007 in china sadeghi tabas et al 2017 coupled a modflow model harbaugh 2005 with optimization algorithms to define the optimal solution for the pumping regime in a basin of iran while deng et al 2018 used modflow code to simulate the effect of groundwater abstraction to land subsistence problems groundwater modeling is also applied in arid and semi arid regions as a supporting tool for effective management planning and decision making in the groundwater sector bittner et al 2020 rossetto et al 2018 pisinaras et al 2013 koltsida and kallioras 2019 roy et al 2016 boskidis et al 2012 vernoux et al 2020 kopsiaftis et al 2009 lautz and siegel 2006 several studies include the implementation of modeling activities in a mar project as a tool to predict the response of a hydrogeological system to an artificial recharge program and future impact ringleb et al 2016 a wide range of numerical models have been applied for the investigation of mar projects russo et al 2015 simulated potential mar concepts in a coastal basin using modflow 2005 harbaugh 2005 coupled with farm process schmid and hanson 2009 sallwey et al 2018 used hydrus 2d 3d for the unsaturated zone so as to design a pilot scale infiltration basin and tzoraki et al 2018 simulated the efficiency of a coastal mar system in cyprus utilizing geochemical model phreeqc parkhurst and appelo 2013 and groundwater feflow trefry and muffels 2007 finite element model maples et al 2019 proposed the locations mar in central valley california through geostatistical modeling and then simulated the performance of infiltration basins with parflow ashby and falgout 1996 a three dimensional 3d variably saturated hydrologic modeling code that couples surface and subsurface flow clark et al 2015 addressed potable water supply reliability from a stormwater mar system with watercress waterselect n d hydrological model by simulating runoff recharge and recovery for different rainfall catchment and aquifer conditions in a brackish coastal aquifer in salisbury australia a physical experiment combined with feflow 7 2 diersch 2014 was utilized by wu et al 2021 to test the performance of mar in geologically heterogenous aquifers based on tailan basin in nw china the authors concluded that clay lenses increase residence time of injected water with little effect on regional flow while high permeability layers decrease freshwater lenses and move stagnation points downstream horizontal groundwater wells have been simulated as collector wells in many groundwater modeling case studies ismail et al 2012 and kelson 2012 simulated horizontal collector wells using modflow drain package drn haitjema et al 2010 suggest that in a 3d model the horizontal well can be simulated by small constant head cells mnw2 package incorporated in modflow 2005 is designed for simulating wells that penetrate more than one layers or extend in multiple cells such as slanted or horizontal wells konikow et al 2009 as for horizontal injection abstraction wells wang et al 2014 proposed an analytical solution model that simulates horizontal well performance as a specified flux boundary in 2 d environment in order to explore groundwater flow characteristics including release capture zones from the horizontal well also zuurbier et al simulated a horizontal well mar system with the modflow wel package zuurbier et al 2014 in the present work the simulation of the related hydrologic processes between the open filter pipe parts of an injection well and the surrounding aquifer are described the goal of this research is to gain insight on modelling the performance of hddw as recharge wells a modeling code that was developed to simulate processes and interactions between karstic aquifer conduits and geological matrix was utilized more specifically a horizontal direction injection well was simulated with the conduit flow package cfp of modflow 2005 code shoemaker et al 2007 while sensitivity analysis was conducted using ucode 2014 poeter et al 2014 the code mainly operates under three distinct modes incorporating reynolds numbers re to tackle the limitations of darcy s law for porous media ground water flow a cfpm1 that couples a discrete pipe network to modflow 2005 b cfpm2 that computes horizontal turbulent flow in preferential flow layers and c cfpm3 that couples simultaneously both the above the program is generally utilized to simulate groundwater flow in fractured and or karstic aquifers pouliaris 2019 simulated a karstic aquifer in lavrion greece by incorporating modflow cfp code to model a network of discrete conduits in the fractured rock hill et al 2010 conducted a modeling exercise to compare the performance of modflow cfp against modflow 2005 for a fractured karstic aquifer at a local scale site near weeki wachee florida this research suggested that cfpm1 improved the match between observed and simulated discharges zheng et al 2021 used modflow cfp to evaluate the hydrogeological impact of tunnelling in karstic aquifers in their study both the tunnel and the karstic underground conduits were simulated as discrete conduit elements with cfpm1 gholizadeh et al 2020 simulated the interaction between a water conveyance tunnel and the surrounding aquifer that is forms in fractured rocks located in urumieh dokhtar volcanic belt in iran their results indicated that modflow cfp code predicted with high accuracy the spatial variation groundwater ingress and hydraulic head in the tunnel to the best of our knowledge this is the first attempt where a hddw is simulated using modflow and cfp package for an experimental pilot setup to test further cfp performance the hddw was also simulated with the simpler wel package and the results of the two models were compared moreover the accuracy of the two approaches is further tested using in situ data which were gathered during a monitoring campaign in february 2018 finally the sensitivity of the proposed approach is studied and reported by the means of sensitivity analysis 2 materials and methods 2 1 study area the study area is located in marathon ne part of attica greece and it covers an area of approximately 40 km2 groundwater management is of ample importance since many anthropogenic activities such as agriculture tourism and domestic development are highly dependent on groundwater resources for their water supply irrigation being the dominant groundwater consumer intensifies the environmental footprint in the local hydrosystem due to the exploitation of shallow groundwater resources zavridou et al 2018 perdikaki et al 2017 fig 1 presents the hydrogeological conditions in marathon plain where the two dominant aquifer units are the lower karstified marbles and the upper alluvial unconsolidated formation the alluvial aquifer is of lower hydraulic conductivity from 10 6 to 10 5 m s and has a thickness of a few meters up to 80 m forming a coastal plain area while the underlying and surrounding karstified formation of higher conductivity it ranges between 10 5 and 10 4 m s has a thickness of up to 400 m siemos 2010 the upper granular unit consists of sediments with uneven size distribution gravel sand clay and silty clay and additionally impermeable clay layers of small thickness lenses exist in various locations at the plain melissaris and stavropoulos 1999 the two units are hydraulically connected along their contacts both being in contact with the sea a critical point in the study area is a local karstic spring located at the ne part of the plain fig 1 with a mean discharge rate of 810 m3 h perleros 2001 both alluvial and karstic formations are exploited through multiple private groundwater wells fig 1 of two main characteristics a the ones that extend throughout the irrigation area within the alluvial aquifer mostly being shallow and b deeper wells that have penetrated the karstic formation all located within the area of contact between the alluvial and the karstic layer uncontrollable groundwater withdrawals in combination with un regulated irrigation scheme for the region has led to seawater intrusion for both aquifer layers psychoyou et al 2007 koumantakis et al 1993 floros 2016 the problem is more pronounced in the upper alluvial aquifer especially during the dry season when the precipitation is limited and the irrigation water demand is increased due agricultural activities perdikaki et al 2020 2 2 set up of the mar pilot and operation an integrated research scheme was conducted at the coastal zone of marathon that involved unsaturated zone zavridou et al 2018 and surface groundwater hydrology perdikaki et al 2017 field and laboratory measurements a complex pilot mar scheme was introduced developed and tested in order to mitigate groundwater quantity and quality issues in the study area the experimental pilot site involved a combined system that abstracts groundwater from the deeper karstic aquifer formation after proper treatment with reverse osmosis ro for desalination and advanced oxidation process aop for the reduction of organic pollutants karstic water is injected directly into the saturated zone of the upper alluvial aquifer layer fig 2 as shown in fig 2 high salinity groundwater app 4500 μs cm is being abstracted from a karstic aquifer directly pumped from a karstic spring which is directly transferred to a hybrid treatment unit which operates in dual mode advanced oxidation is being used to reduce the agrochemical loads within the abstracted groundwater while reverse osmosis is applied in order to reduce salinity approaching drinking water standards after the treatment process the treated water is directly injected into the unconsolidated formation through a hddw that consists of a pvc pipe of 75 mm diameter and penetrates both unsaturated and the saturated zone of the alluvial aquifer while the recharge water is injected through an open screen filter pipe into the saturated zone the directional borehole is 50 m in total length while it reaches a maximum depth of 4 1 m at the middle point of its total length the pilot site also includes the installation of two vertical piezometers that are used for the continuous monitoring of the hydrodynamic conditions of the mar experiment as well as to determine the boundary conditions of the model 2 3 hydrogeological setting of the pilot site the stratigraphy of the selected location was investigated during a vertical well drilling next to the hddw at that point the aquifer unit is not homogenous due to different grain size distribution the first few meters approximately 3 m thick are covered with fine grained silty clay material with low permeability below this layer the target aquifer is a layer of higher permeability with a thickness of 20 m composed of gravel sand deposits the basement of the latter unit is composed of a clayey impermeable layer both layers were modeled as confined units due to the existence of the fine grained layer at the top of the model the depth to the groundwater in the pilot area is approximately 2 4 m while the piezometric surface ranges between 0 1 and 0 7 m above mean sea level amsl 2 4 simulation of groundwater injection using the conduit flow process program for modflow 2005 in this study the pilot mar scheme is simulated using modflow 2005 harbaugh 2005 code of usgs primarily focused on the performance of the hddw the code solves the three dimensional equation eq 1 of groundwater flow in a porous medium as described by mcdonald and harbaugh 1986 1 x k x x h x y k y y h y x k z z h z w s s x t where k xx k yy and k zz are the hydraulic conductivities along x y and z axis respectively l t h is the potentiometric head l w is a volumetric flux per unit that represents the sinks and sources of water in or out of the groundwater system t 1 ss is the specific storage of the porous medium l 1 and t is time t due to the complexity of eq 1 an analytical solution is almost always impossible so that a finite difference method is applied in modflow 2005 code in order to simulate groundwater flow of a system in space and time harbaugh 2005 variable density flow was not applied in this study the model domain represents a rectangular area of 112 000 m2 with a fine grid of 2 2 m the model contains two different layers a the first layer is 3 m thick and is of low hydraulic conductivity representing a shallow silty clay layer while b the second layer represents the main study aquifer that has a thickness of 20 m and is composed of materials of larger particle size with higher hydraulic conductivity the two aquifer layers are considered homogenous the hddw is 50 m long and it reaches the depth of 4 1 m the modeling task of simulating the hydrodynamics of water injection into an unconsolidated aquifer formation through a hddw was realized with the application of cfp mode 1 cfpm1 that incorporates flow within the aquifer modflow 2005 flow in cylindrical pipes hddw and groundwater exchange between the aquifer and the pipe the flow equation within the conduit is solved with two different equations for laminar and turbulent flow for laminar flow the hagen poiseuille equation is 2 q a ρ g d 2 δ h 32 μ δ l τ where q is the volumetric flow rate l3t 1 a is the cross sectional perpendicular to the flow l2 ρ is the density of the water ml 3 g is the gravitational acceleration constant lt 1 δh is the head loss along the pipe l of length δl l μ is the absolute or dynamic viscosity of water ml 1t 1 and τ is the tortuosity unitless of the pipe shoemaker et al 2007 for turbulent flow the darcy weisbach equation is solved as 3 q a δ h d 2 g f δ l where f is the friction factor dimensionless shoemaker et al 2007 flow equations in cfpμ1 also include corrections for the flow in partially filled pipes and correction for groundwater exchange between the aquifer and partially filled pipes another asset of cfp program is the incorporation of the recharge package crch that has the ability to simulate the artificial water injection in the hddw moreover we employed the already used wel package for simulating the effect of mar through the hddw and we also compared the results with cfp scheme previously described the hddw was conceptualized through eight wells fig 4b the screen of each well was placed at different depth in order to capture the various depths of the constructed horizontal well the depths ranged from 3 1 m to 4 1 m below ground and are all located at the bottom layer groundwater injection for the proposed mar setup was distributed equally at the eight wells the boundary conditions that represent sinks and sources for the aquifer are presented in table 1 fig 4 shows the location of each boundary condition for the two models the main difference is the use of the two different packages for the representation of the hddw cfp and wel package constant head boundary chb were assigned at the model cells that are in hydraulic contact with the sea whereas at the ne and sw boundaries of the model no flow conditions were used last the general head boundary ghb package was used to simulate the lateral inflows from the nw upstream part where the hydraulic heads of the boundary were based on continuous monitoring through monitoring wells installed at the head observation wells as shown in fig 3 the model was simulated for 10 stress periods with a total duration of 300mins table 2 and then calibrated using measurements that were conducted during experiments of freshwater injection r o water into the aquifer through the hddw freshwater injection in the hddw varied from 0 94 to 3 5 m3 h 2 5 sensitivity analysis and calibration of model parameters sensitivity analysis and parameter estimation was performed effectively with ucode 2014 of usgs poeter et al 2014 using experimental measurements conducted in the mar pilot experiment in the cfp model the geometrical characteristics of the conduit was known diameter tortuosity for roughness height a small value for smooth pipes was obtained moody 1944 while upper and lower reynolds number were included to the sensitivity analysis for the groundwater well pipe permeability conduit wall conductance lower reynolds number and upper reynolds number were also included as a parameter in the parameter estimation process these parameters were added manually in ucode 2014 as modflow 2005 code does not recognize any parameter type for the cfp package all the parameters assigned in ucode 2014 along with the starting values are presented in table 3 cond cfp low re and high re parameters were utilized only at the sensitivity analysis of the cfp model as they are those that define conduit performance in ucode 2014 poeter et al 2014 sensitivity analysis is performed for each parameter through an iterative process the code quantifies the sensitivity of a model parameter by using fit independent statistics those statistics are useful tools so as to identify the importance of the observations for each parameter the importance of a parameter to prediction and the importance of each observation to prediction hill and tiedeman 2007 parameter sensitivity levels were calculated through composite scaled sensitivities css that counts the available information given from the observations in order to estimate one parameter css is a function of dimensionless scaled sensitivities dss so that cssj for the jth parameter calculated for nd observations so that 4 c s s i 1 n d d s s i j 2 b n d 1 2 dss measures how much the change in a parameter affects the model by comparing the initial parameter value with a perturbed one for a specific iteration while it is calculated using the following function 5 d s s y i b j b b i ω i i 1 2 where y i is a simulated value b j the jth parameter y i bj is a derivative that counts for the sensitivity of the ith observation to the jth parameter b is a vector containing the parameter values and ω ii is the weight that is assigned to the ith observation hill and tiedeman 2007 in general css provides a comparison between parameters in order to identify those for which the observations provide more information another important value is the parameter correlation coefficients pcc that indicate the parameters that are highly correlated with one another in a case of extreme correlation those parameters cannot be estimated independently pcc is calculated using the following equation 6 c o v b j k v a r b j j v a r b k k where cov b jk is the covariance between two parameters and var b jj var b kk the variances of each parameter highly correlated parameters have a pcc absolute value of more than 0 95 hill and tiedeman 2007 as for the parameter estimation process through ucode2014 it is also an iterative process and includes an objective function that is defined as a sum of squared and weighted residuals of the observation minus the associated simulated value the main purpose of the parameter estimation process is to find a set of parameters that produces the smallest value for the objective function poeter et al 2005 3 results 3 1 sensitivity analysis results sensitivity analysis performed revealed the level of sensitivity for all the parameters presented in table 3 and the correlation amongst them based on groundwater level values measured during experimental aquifer recharge in the pilot area for the cfp model sensitivity analysis results suggested that the model is highly affected by the parameters that are related to the bottom aquifer layer and the recharge of the pilot area from the surrounding aquifer fig 5 a the highest css values were estimated for hk par2 along with ghb par parameter while the cond cfp parameter has a much lower css value parameters hk par1 ss par low re and high re have almost zero level of sensitivity according to the analysis the output of the sensitivity analysis proves the importance of hk par2 for the simulated results as it is the parameter that relates to groundwater movement in the target aquifer layer bottom layer as far as the artificial recharge is concerned four sets of parameters are highly correlated fig 5b as the absolute value of pcc is equal to 1 the extreme correlation between the hydraulic conductivities of the two layers derives from their interconnection the highly sensitive parameter of ghb in conjunction with the extreme correlation of the hydraulic conductivity parameters with the ghb conductance relates to the fact that this boundary is a major source of inflow for the two layers the sensitivity analysis results of the model with the wel boundary are presented in fig 6 the two most important parameters are hk par2 and ghb par1 in this analysis ss par1 is an influential parameter for model output indicating the change in storage in the aquifer the sensitivity analysis results revealed that there is no correlation between the parameters assessed 3 2 model results a parameter estimation process performed for the two models with respect to the sensitivity analysis results in the cfp model parameters hk par2 ghb par1 and cond cfp were utilized in the process and the final estimated values were 0 00105 m min 1 87e 5 m2 min and 5 23e 6 m2 min respectively the model was calibrated successfully and was also validated using different field experimental measurements in the wel model hk par2 ghb par and ss par were estimated with ucode the best fitted parameter set was 1 81e 04 m min 2 57e 6 m2 min and 0 001347 m 1 respectively fig 7 and fig 8 show the differences between the results of the simulated model against the measured values in fig 7 the evolution of observed and simulated heads for the two observation wells w1 w2 is represented for each stress period during artificial aquifer recharge the observed vs simulated fig 8 diagram presents an acceptable fit of the model results to the observed values for both cfp and wel models as the residuals do not exceed 0 1 cm in the next figure fig 9 the cumulative water budget of the two models at the end of the final stress period stress period 10 is presented the water budget is convenient in order to specify the fluxes to the aquifer and understand the interaction of several components of the hydrosystem in this case study the main inflow of the model originates from the cfp and the wel package in fig 9a and b respectively i e the hddw the ghb inflows are also noteworthy for the cfp model the change in storage in the groundwater budget of both models fig 9a and b occurs due to release of water through the aquifer when water level changes as a result of water injection a significant amount of water outflows from the chd package that represents the discharge of groundwater to the mediterranean sea fig 10 shows the piezometric conditions of the model at the beginning and the end of the simulation piezometry before groundwater injection is distributed uniformly throughout the domain and the piezometric head is being gradually reduced from the upper ghb boundary to the coast fig 10a the hydraulic head at the vicinity of the well in the first stress period is presented in fig 10b and is app 0 6 m amsl above mean sea level at the end of the simulation the hydraulic head rises in the area of the well for the cfp fig 10c and the wel model fig 10d as well there is a big difference between the results of the two models as in the cfp model the maximum level rise due to artificial recharge is 0 48 m while in the wel model the aquifer rises more than 1 8 m above the starting head 4 discussion although horizontal directional drilling has gained traction in the groundwater sector due to the decline in operational cost and several technical advantages over vertically drilled wells wang and zhan 2017 there is limited research in the performance of hddws for freshwater injection in a mar project e g zuurbier et al 2015 previous work has mainly focused on the investigation of horizontal or slanted wells and their performance as pumping collector wells rather than injecting a volume of freshwater in the aquifer e g collins and houben 2020 thus we considered that there is a gap in the literature for the use of horizontal wells in asr systems as well as the implementation of horizontal recharge wells in numerical modelling problems modflow 2005 cfp gave insight into groundwater flow in the target aquifer horizontal well performance and the water table hddw interaction observed data obtained from marathon pilot setup during artificial recharge through the hddw were considered significant to validate the applicability of modflow 2005cfp code for groundwater injection with horizontal wells to the best of our knowledge the application of cfp package in mar was verified with field observations for the first time furthermore the sensitivity analysis conducted indicated that the most influential parameters are related to specific aquifer characteristics i e hydraulic conductivity and external inflows i e ghb while the conduit parameters are of less i e conduit conductance or no lower reynolds number upper reynolds number importance for the model results this outcome could be useful in other modelling studies with horizontal injection wells compared to other packages that have been used by several researchers in the past for instance drn mnw2 wel this program has relatively low data requirements it focuses on the geometry of the conduit without ignoring groundwater movement in the conduit the exchange of water between the conduit and the aquifer friction factor and the head loss along the pipe in this research the drn package was rejected as it serves the opposite purpose of the study i e to drain water from the aquifer instead of inject water into the system in comparison to mnw2 cfp package requires less data to be assigned by the user while turbulent and laminar flow conditions are computed by utilizing the equations that describe adequately the flow in network pipes e g eck and mevissen 2015 with respect to pipe material roughness height offering a realistic representation of the horizontal well pipe the implementation of the simpler wel package for simulating the hddw was also utilized to investigate the main differences between the two packages the use of a single vertical well wel package would only simulate the amount of water that is injected at a certain point of the aquifer without taking into consideration the extended screen of the horizontal well this problem can be solved by utilizing multiple single wells at the extension of the open screen filter pipe perdikaki et al 2019 the use of multiple wells at specific depths along the length of the constructed hddw was considered critical so as to capture the area of influence but still it is not a realistic representation of the boundary in contrast to cfp the sensitivity analysis results were in accordance with the sensitivity analysis of the cfp model as the most influential parameters were the hydraulic conductivity of the target aquifer and ghb conductance both models had high coefficient of determination r2 and low root mean square error rmse as is presented in fig 8 showing satisfactory agreement between observed and simulated groundwater head values according to the results of the cfp model the hydraulic head at the end of the simulation is distributed evenly along the hddw and the release zone is well represented fig 10c on the contrary the results of the wel package fig 10d compared to the cfp showed greater discrepancy the difference in piezometry between the cfp and wel reaches 1 3 m the greater differences are observed at the locations of the wells conceptualization of the hddw with the wel package leads to local maximums of hydraulic head during artificial recharge as a result of the point recharge by several wells rather than diffuse recharge along the conduit under natural flow conditions this is not the case overall we consider that the natural hydrologic processes during artificial recharge with horizontal wells is best represented by the cfp package a fact that is indicated by the shape and elevation of piezometric curves variable density flow was not modeled in this study as modflow cfp is not being coupled with seawat langevin et al 2007 an option that would substantially improve its applicability to a wider range of hydrogeological problems although a variable density flow and solute transport conduit flow process model has been developed by xu et al 2019 there is not an open access software yet the incorporation of variable density flow in modflow cfp code would benefit on the investigation of new techniques including horizontal well technology for seawater intrusion for example the performance of horizontal wells as hydraulic barriers to seawater intrusion could be investigated in modeling studies as an evolution to the existing studies of hydraulic barriers e g pool and carrera 2010 botero acosta and donado 2015 the horizontal well screen along the coastline might create a more extended seaward hydraulic gradient our work has some limitations mainly concerning the limited experimental data the main aim of the present work was to investigate the capability of modflow2005 cfp to represent the actual hydrological processes in the horizontal well and the groundwater system and to compare it with a widely used package for water injection i e wel as a result the present research serve as an operational framework for modeling hddws for groundwater injection through a physically based software in our study we have chosen to use a high resolution grid i e 2mx 2 m in order to simulate the hydrological processes during the injection of the recharge water through the hddw based on our engineering judgement this approach can be deemed as reasonable for the specific case study as the pilot scale has low computational requirements however in order to model aquifer s response on a regional scale one should use appropriate grid refinement tools e g uniform fine grid variably spaced grid modflow lgr 5 conclusions among several methods and types an innovative mar technique involves the use of horizontal directional drilling wells as a way to restrain or prevent seawater encroachment towards the mainland as well as store freshwater within the aquifer future impacts of mar facilities can be safely predicted through the application of mathematical models that are able to efficiently simulate the relevant hydrologic processes that are dominant in association with each different mar technique the conceptualization of a water injection horizontal well took place using two several 3d finite difference modflow models in order to test the applicability of cfp package for hddw simulation and compare it with the simple wel package sensitivity analysis and model calibration was performed with ucode in both models by utilizing observed piezometric data obtained during artificial recharge in the pilot setup the two models were compared in terms of sensitivity analysis results model fit observed vs simulated values maximum hydraulic head values and piezometric conditions our results suggest that in both models hydraulic conductivity of the recharged aquifer and ghb conductance are of ample importance for the simulation in the cfp model the most influential parameters are associated with the aquifer properties rather than the conduit parameters more specifically conduit parameter values conduit wall conductance lower reynolds number and upper reynolds number are of lower importance for the results of the model however it is important to be stated that the results of the sensitivity analysis are case and metric dependent to this end a calibration procedure took place for the most sensitive parameters cfp and wel model were calibrated successfully the precision in the results of the two calibrated models compared to the observed hydraulic head during water injection indicate that both packages effectively simulate the performance of the horizontal injection well within an aquifer nevertheless piezometric head value and distribution around the horizontal well seems to be unrealistic in the case of the wel package as high values are presented locally at the location of the wells even though the conceptualization of the hddw with cfp package produced more realistic results as the piezometric surface rose evenly around the conduit considering all of the above we conclude that modflow2005 cfp code is a useful tool for simulating a horizontal directional drilling well that is incorporated in a mar setup and performs for freshwater injection purposes we propose further investigation of this modelling technique with longer data series as well as a full scale case study that implements the proposed mar scheme in order to test its feasibility for the reclamation of depleted aquifer systems and considering the technical assets of hddws finally we hope that our research could pave the way and encourage other researchers to incorporate the proposed methodology in order to test through mathematical simulation the adaptivity of artificial recharge with hddws in the scope of an integrated groundwater management plan that targets to groundwater storage and aquifer rehabilitation declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests andreas kallioras reports financial support was provided by european union acknowledgements this research is part of the project subsol bringing coastal subsurface water solutions to the market subsol has received funding from the european union s horizon 2020 research and innovation programme under grant agreement no 642228 
25684,most carbon stocks and fluxes in the western united states are found in mountainous terrain where observations and modeling are difficult terrestrial biosphere models generally underestimate above ground biomass agb over this region here we identify methods to reduce this underestimation by focusing upon 1 biases in meteorological datasets 2 model representation of water stress and 3 spatial resolution we adopted the widely used community land model version 4 5 clm 4 5 with six different meteorological datasets and found a 6 fold variation in simulated agb across utah colorado simulations underestimated agb because of warm and dry biases within the meteorological datasets that reduced water availability and restricted plant growth to eliminate the agb underestimation we adopted a meteorological dataset designed for complex terrain gridmet combined with a representation of plant hydraulic stress clm 5 0 conversely changes in spatial resolution meteorological variables and land surface description had negligible impact on simulated agb keywords community land model meteorological forcing complex terrain vegetation biomass 1 introduction while the western united states accounts for a significant fraction of the country s total land carbon sink lu et al 2015 schimel et al 2000 forests in this region are also particularly vulnerable to disturbances such as drought insect damage and wildfires anderegg et al 2016 williams et al 2016 these disturbances have the potential to rapidly reduce the biomass within these ecosystems and release significant amounts of carbon to the atmosphere with climate change and increased warming and drying within the western united states these forest stressors are expected to increase in severity bentz et al 2010 buotte et al 2019 rocca et al 2014 seager et al 2007 and likely lead to rapid changes to these mountainous ecosystems thereby influencing carbon and water cycling availability of natural resources and recreational opportunities most of the carbon stocks and fluxes in the western united states are found in mountainous complex terrain which are inherently difficult to observe and model e g desai et al 2011 lin et al 2017 schimel et al 2002 sun et al 2010 modeling challenges can arise for many reasons including the definition of boundary conditions e g meteorology and land surface characteristics model parameters and model structure bonan et al 2019 dietze 2017 see also bonan and doney 2018 lin et al 2011 lovenduski and bonan 2017 the prescription of accurate boundary conditions in complex terrain poses a particularly daunting challenge first meteorological processes and land surface properties are highly variable because of large changes in elevation across fine spatial scales typical global and regional meteorological reanalysis products have increased uncertainty across complex terrain due to the relatively coarse horizontal grid spacing from 101 to 102 km and sparse meteorological observations with which to inform the products gao et al 2012 rose and apt 2016 second meteorological datasets are subject to strong biases in shortwave radiation and precipitation in complex terrain which reduce the accuracy of land surface simulations e g gómez navarro et al 2018 schroeder et al 2009 beyond accuracy the extent to which atmospheric conditions and land surface characteristics are spatially resolved may strongly impact terrestrial biosphere model tbm simulations for example the surface energy balance across complex terrain is influenced by the interaction of the solar angle with the slope aspect of the land surface this interaction promotes plant growth on south facing slopes at the highest elevations in the northern hemisphere and the opposite at relatively warm low elevations pelletier et al 2018 similarly soil water availability is influenced by slope aspect soil depth and soil texture swenson et al 2019 to what extent explicit spatial representation of boundary conditions and model processes are required to represent large scale water energy and biogeochemical fluxes across complex terrain however remains unclear fan et al 2019 although finely resolved surface and meteorological data are required to capture fine scale spatial variation of land surface processes fiddes and gruber 2014 it is less well known whether the fine scale information fed into the tbms is important for large spatial scale simulations do the effects of fine scale features simply average out at coarser scales hao et al 2021 recently demonstrated with the e3sm land model that sub grid topographical effects on solar radiation cannot be neglected even at coarse spatial scales 2 with differences in simulated snow cover fraction and surface heat fluxes of up to 20 in the tibetan plateau however the influence on carbon dynamics was not investigated in their study as they used prescribed vegetation phenology from satellite data parameterizations of sub grid topographical effects on precipitation were also implemented in e3sm recently tesfa et al 2020 but their impact on model simulations still remains to be investigated for reasons of both accuracy and spatial representation simulations of ecosystem function and structure are intrinsically sensitive to the meteorological forcing bonan et al 2019 medvigy et al 2010 wu et al 2017 zhao et al 2006 for example bonan et al 2019 found that the contribution of meteorological uncertainty to overall uncertainty in global carbon cycle simulations with clm for the historical period 1850 2014 can surpass model structure uncertainty finally many western u s forests exist upon a mortality boundary in which deviations towards warmer and drier conditions may increase forest mortality due to water stress that can lead to hydraulic failure carbon starvation insect damage and wildfire see anderegg et al 2013 goulden and bales 2019 hicke et al 2013 mcdowell et al 2008 therefore researchers need to understand what level of accuracy and explicit spatial representation is required of meteorology and land surface characteristics to provide realistic simulations of land surface behavior across complex terrain meteorological datasets have been developed that specifically account for the influence of complex terrain upon atmospheric conditions with spatial resolution on the order of 1 km e g prism daly et al 2008 daymet thornton et al 2017 gridmet abatzoglou 2013 despite their fine spatial resolution these meteorological datasets generally have coarse temporal resolution daily to monthly time steps whereas tbms typically require sub daily data a workaround to this limitation is to temporally downscale the daily or monthly meteorology with a reference meteorological data product available at finer temporal resolution for example buotte et al 2019 used this approach to temporally resolve daily gridmet data into a 3 hourly product based on narr data mesinger et al 2006 different tbms in the coupled model intercomparison project phase 5 cmip5 taylor et al 2012 generally underestimate biomass significantly in the western united states over the historical period pre industrial era to present collier et al 2018 see also https ilamb ornl gov cmip5 consistent with results from the international land model benchmarking project ilamb buotte et al 2019 carried out simulations for the western united states with the community land model clm version 4 5 oleson et al 2013 and found that the default model configuration and meteorological dataset underestimated total simulated biomass by one order of magnitude when compared against an observation based reference product wilson et al 2013 after substantial modifications buotte et al 2019 were able to boost forest productivity with total simulated biomass reaching twice the value of the observation based reference product these modifications included 1 the use of a high resolution meteorological dataset 1 24 1 24 gridmet disaggregated from daily to 3 hourly rupp and buotte 2020 2 high resolution 1 24 1 24 surface maps input surface variables such as topography soil properties and distribution of plant functional types 3 reassignment of original plant functional types within clm to major western u s forest types 4 changes in leaf shed rates to include effects of water stress and 5 changes in the fire module to reduce overestimation of burned area all modifications 1 to 5 above were incorporated in the buotte et al 2019 results the separate contribution from each of these individual modifications to improving the biomass simulation however was not investigated in their study here we focus on the impact that meteorological datasets alone have upon the simulation of carbon stocks and fluxes across the western united states the questions we address are to what extent can the underestimation of biomass by tbms in the western united states be improved with the use of more accurate meteorological datasets even in the absence of improvements in model parameters or structure furthermore to what extent can tbm performance be improved by a increased spatial resolution in land surface datasets or in combination with high resolution meteorological datasets and b more realistic representation of water carbon coupling to answer these questions we investigate the impact of six different meteorological datasets on the simulation of above ground biomass agb with clm version 4 5 in the southern rocky mountains focusing on the states of utah and colorado we compare the default meteorological dataset in clm 4 5 cru ncep ucar 2016 that led to low agb against the high resolution meteorological dataset developed in buotte et al 2019 and four other recently developed datasets that have been specifically corrected to remove biases in precipitation and downwelling shortwave radiation wang et al 2016 wei et al 2014 we also perform these simulations at varying levels of spatial resolution and with a more recent version of clm 5 0 which includes an improved representation of water stress based on plant hydraulics to determine if the underestimation in biomass can be removed we track simulated water and nitrogen limitation both regionally and at the site level to diagnose the constraints on plant growth and carbon cycling within the model 2 methods 2 1 model description we used clm 4 5 oleson et al 2013 for most of the simulations in our study sect 2 5 clm is the land component of the community earth system model cesm a fully coupled global climate model widely used by the scientific community with numerous applications to simulations of vegetation biomass and carbon cycling http www cesm ucar edu publications including applications across the western united states e g buotte et al 2019 duarte et al 2017 hudiburg et al 2013 raczka et al 2016 wieder et al 2017 biophysical and biogeochemical processes simulated by clm include surface energy and momentum fluxes and carbon water and nitrogen cycling the details of clm 4 5 are provided in sect 2 1 1 or within the technical description of oleson et al 2013 we refer to the standard version of clm 4 5 that internally predicts the vegetation state as clm cn we also use a special configuration of clm 4 5 that prescribes the vegetation state through satellite measured phenology clm sp more details of clm sp are provided in the supplement text s2 we used the latest release of clm version 5 0 to quantify the impact that the representation of water limitation has upon biomass growth bonan et al 2019 kennedy et al 2019 lawrence et al 2018 we perform two separate simulations with clm 5 0 the first includes a more mechanistic representation of water limitation through plant hydraulics clm5 phs kennedy et al 2019 the second uses an empirical sect 2 1 1 representation of water limitation that uses soil moisture stress clm5 sms more details about clm 5 0 are provided in section 2 1 2 2 1 1 clm 4 5 photosynthesis stomatal conductance carbon allocation and plant mortality here we focus on the clm 4 5 model components most important for the simulation of agb the photosynthesis in c3 plants is calculated based on the model proposed by farquhar et al 1980 net leaf photosynthesis a n is calculated as 1 a n min a c a j a p r d where r d is leaf level respiration and a c a j and a p are the rubisco limited light limited and product limited rates of carboxylation respectively the rubisco limited rate of carboxylation in c3 plants is calculated as 2 a c β t v c m a x c i γ c i k c 1 o i k o where β t is a soil moisture stress factor defined in eq 5 v c m a x is the maximum rate of carboxylation in the absence of soil moisture stress c i is the leaf intracellular co2 partial pressure γ is the co2 compensation point k c is the michaelis menten constant for co2 o i is the atmospheric o2 partial pressure and k o is the michaelis menten constant for o2 for brevity the formulations for a j and a p are omitted here see oleson et al 2013 for details leaf level respiration is calculated as 3 r d r d 25 f θ β t where r d 25 is the value of leaf level respiration at 25 c in the absence of soil moisture stress and f θ is a temperature correction factor leaf stomatal conductance g s is calculated based on the ball berry model as implemented by sellers et al 1996 in the sib2 model 4 g s m a n c s p a t m r h l e a f b β t where c s is the co2 partial pressure at leaf surface p a t m is the atmospheric pressure r h l e a f is the relative humidity at leaf surface and m and b are plant functional type pft based parameters the soil moisture stress factor used in eqs 2 4 is defined as 5 β t i 1 n w i r i where w i and r i are respectively plant wilting term and the fraction of roots at soil layer i the summation in eq 5 is over the entire soil column n soil layers clm calculates r i using an exponential root distribution function which depends on pft specific parameters i e the root distribution is fixed for each pft the plant wilting term is calculated for each soil layer as 6 w i ψ c ψ i ψ c ψ o θ s a t i θ i c e i θ s a t i 1 t s i 271 15 k and θ l i q i 0 0 t s i 271 15 k or θ l i q i 0 where ψ i is the soil water matric potential ψ c and ψ o are the soil water potential at full stomatal closure and opening respectively pft based parameters θ s a t i is the saturated volumetric water content θ i c e i is the volumetric ice content θ l i q i is the volumetric liquid water content and t s i is the soil temperature at soil layer i the resulting β t values range from 0 to 1 high to low soil moisture stress and are used to downscale a n and g s see eqs 2 4 clm solves for a n g s c i and c s using an iterative algorithm that is executed until the solution for c i converges calculations are carried out separately for sunlit and shaded leaves the potential gross primary production without nitrogen limitation g p p p o t is calculated as 7 g p p p o t a n s u n r d s u n l a i s u n a n s h a r d s h a l a i s h a where the superscripts sun and sha denote sunlit and shaded leaves respectively l a i is the leaf area index allocation of newly assimilated carbon g p p p o t is prioritized in the following order 1 support of maintenance respiration of live plant tissues 2 replenishment of the excess maintenance respiration pool a storage pool that is allowed to run a negative state and is used to support maintenance respiration when g p p p o t is insufficient e g during nighttime winter or stress periods 3 support of plant growth the amount of carbon that is actually allocated to new growth is limited by nitrogen availability therefore clm calculates the actual gross primary production g p p as 8 g p p g p p p o t 1 d where d is a nitrogen limitation factor calculated as 9 d c f a v a i l a l l o c c f a l l o c g p p p o t where c f a v a i l a l l o c is the carbon flux from photosynthesis that is available to new growth allocation i e g p p p o t minus the allocation fluxes to maintenance respiration and to the excess maintenance respiration pool and c f a l l o c is the actual carbon allocation to new growth restricted by nitrogen availability structural carbon and nitrogen pools simulated by clm include leaf stem live and dead tissues coarse root live and dead tissues and fine root the c n ratios of each of these plant tissues and allometric ratios are defined for each pft these parameters are used to determine c f a l l o c and how carbon is allocated to each pool simulated agb corresponds to the sum of the leaf and stem carbon pools leafc livestemc and deadstemc variables in clm clm represents whole plant mortality with a fixed empirical rate of 2 year 1 that includes mortality associated with drought pests diseases wind throw and other natural disturbances except fire fire related mortality is simulated dynamically with a separate module mortality due to anthropogenic land cover change is also calculated separately based on the prescribed land cover within the model which is a function of time recruitment is implicitly accounted for in the c f a l l o c term actual carbon allocation to new growth eq 9 our simulations considered the fixed empirical mortality rate of 2 year 1 due to natural disturbances and the mortality due to anthropogenic land cover change during a transient period from pre industrial era to near present day based on prescribed data see sect 2 5 here we opted to turn off the fire module as it tends to overestimate burn area buotte et al 2019 duarte et al 2017 raczka et al 2016 zhang et al 2016 note that the fixed 2 year 1 rate means that clm does not have a mechanistic representation of natural mortality it corresponds to a middle range value based upon observations across a wide sampling of biomes lawrence et al 2018 this moderate mortality value in combination with fire exclusion led us to treat the persistent underestimation of biomass within clm as due to suppression of plant productivity rather than due to excessive mortality or fire activity 2 1 2 clm 5 0 we provide a brief overview of the differences between clm 5 0 and 4 5 most relevant for agb simulations across the western united states more details about clm 5 0 are provided in lawrence et al 2018 foremost clm 5 0 includes a representation of plant hydraulic transport which calculates the water potential for roots xylem and leaves leaf transpiration is modulated through stomatal conductance in such a way to avoid excessively high xylem tension and low leaf water potential this mechanistic approach replaces the empirical soil moisture stress approach in clm 4 5 eqs 5 and 6 above which tends to exaggerate the limitation of photosynthesis powell et al 2013 the maximum stomatal conductance is based upon atmospheric vapor pressure deficit medlyn et al 2011 which replaces the ball berry relative humidity approach eq 4 clm 5 0 includes a spatially varying soil depth as well as an improved representation of soil moisture evaporation the new nitrogen limitation approach calculates leaf c n ratios that are used to simulate a dynamic photosynthetic capacity avoiding the instantaneous downregulation of photosynthesis through nitrogen availability eqs 8 and 9 above and allowing for a more realistic relationship between n limitation and stomatal conductance ghimire et al 2016 a fixed and dynamic carbon allocation scheme is used for clm 5 0 and clm 4 5 respectively the change to the fixed allocation scheme reflects the fact that plant biomass saturates with increased productivity in contrast with the dynamic allocation scheme that continually increases allocation with increased net primary production npp negrón juárez et al 2015 the default configuration of clm 5 0 includes all of these changes including the plant hydraulic stress representation of water carbon coupling clm5 phs the clm5 sms configuration is identical except that the water carbon coupling is represented through the same soil moisture stress configuration used in clm 4 5 2 2 study regions utah colorado colorado mountain transect and niwot ridge co most simulations were performed across the states of utah and colorado with the model domain defined between 37 00 n and 42 00 n and 114 05 w and 102 05 w fig 1 utah and colorado are representative of the western u s region in terms of topography and drought susceptibility a significant fraction of this region falls within the southern rocky mountains srm ecoregion bailey 1995 drummond 2012 which is characterized by complex topography several hundred peaks with elevation above 3600 m a s l are located within the srm ecoregion drummond 2012 including the highest peak of the rocky mountains system mount elbert colorado 4401 m a s l the srm climate is characterized by mean annual temperature from 2 to 10 c and mean annual precipitation ranging between 260 and 1020 mm bailey 1995 most precipitation falls as snow leading to significant snowpack at higher elevations critical for water supply for the srm and surrounding ecoregions drummond 2012 vegetation distribution has a strong dependence on altitude latitude predominant wind direction and aspect bailey 1995 common vegetation types include drummond 2012 engelmann spruce picea engelmannii and subalpine fir abies lasiocarpa in the subalpine zone ponderosa pine pinus ponderosa aspen populus tremuloides juniper juniperus spp and oak quercus spp in the montane zone sagebrush artemisia tridentata oak pinyon juniper woodland pinus edulis and juniperus scopulorum monosperma and osteosperma and blue grama grass bouteloua gracilis in the foothill zone grasslands and shrublands in the low elevation valleys additional simulations were performed across a more limited domain of a colorado mountain transect fig 1 to specifically test the impact of spatial resolution within the meteorological dataset and land surface description site level clm simulations were performed at the niwot ridge ameriflux core site us nr1 burns et al 2015 monson et al 2002 a high elevation site in colorado 40 03 n 105 55 w 3050 m a s l see fig 1 niwot ridge was chosen based on its extensive records of biological meteorological and flux observations allowing us to directly evaluate both the meteorological datasets and the model s performance in terms of carbon fluxes and agb niwot ridge is located on a gentle 3 4 east facing slope the site is characterized by 120 year old subalpine forest composed mainly of subalpine fir engelmann spruce and lodgepole pine pinus contorta and mean annual temperature and precipitation of 1 5 c and 800 mm respectively monson et al 2005 2 3 meteorological datasets the clm 4 5 simulations used six different meteorological datasets described in table 1 first we used the default dataset in clm 4 5 cru ncep as a baseline next we used cfsr wang era wang and merra wang datasets that all include a bias correction for precipitation global mean 12 higher than cru ncep and generally improve global simulations of hydrological variables with clm sp in comparison with cru ncep wang et al 2016 we also used narr mstmip a dataset that includes bias corrections for precipitation and downwelling shortwave radiation and is originally available at a finer grid spacing 1 4 1 4 vs 1 2 1 2 in cru ncep and wang datasets finally we used gridmet a dataset tailored for complex terrain applications and originally available at a very fine grid spacing 1 24 1 24 see text s1d for consistency across all datasets we coarsened narr mstmip and gridmet to 1 2 1 2 the site level simulations used the same meteorological datasets but used the grid cell that corresponded to the location of niwot ridge we also used hourly tower based observations at the site see blanken 1998 present to evaluate clm for consistency between tower observations and the other datasets we only used 1998 2007 meteorological data to drive the site level simulations additional information on all meteorological data is provided in the supplement text s1 the meteorological variables required to drive clm are downwelling shortwave radiation s a t m precipitation p t a t m and near surface air temperature t a t m specific humidity q a t m and wind speed w a t m relative humidity r h a t m or dew point temperature t d a t m can also be used instead of q a t m downwelling longwave radiation l a t m near surface atmospheric pressure p a t m and the partitioning of s a t m into diffuse and direct components are optional clm estimates these variables if they are missing clm automatically interpolates the prescribed meteorological variable in both space and time down to the same grid size of the provided surface maps and the time interval used for model calculations default of 30 min spatial interpolation is carried out via a bilinear algorithm whereas the temporal interpolation is linear for all variables except for s a t m and p t a t m which are interpolated as the cosine of the solar zenith angle and by the nearest value in time respectively the exact meteorological variables we used to drive clm are listed in table 1 for each dataset considered 2 4 biomass datasets we used above ground biomass data from the national biomass and carbon dataset for the year 2000 version 2 nbcd2000 kellndorfer et al 2013 2007 2009 to assess our regional clm simulations the original dataset includes above ground estimates of live dry biomass over the conterminous united states at fine grid spacing 30 m 30 m combining forest inventory and analysis fia data from the united states forest service with high resolution interferometric synthetic aperture radar insar data from the 2000 shuttle radar topography mission srtm and landsat enhanced thematic mapper plus etm data further details can be found in kellndorfer et al 2013 for our analysis we used the nbcd2000 data product in which tree level biomass estimates were obtained from fia s database drybiot variable we used the spatial data access tool sdat ornl daac 2017 available at the nbcd2000 product web site kellndorfer et al 2013 to download a customized version of the dataset for our study region at 0 01 0 01 grid spacing we later aggregated the product at 0 2 0 2 grid spacing and also converted the values to above ground live biomass carbon defined here as 50 of originally reported dry biomass to allow a direct comparison with our regional clm simulations the nbcd2000 dataset has the advantage of being a finely resolved gridded product with continuous coverage in the conterminous united states making it a recurrent benchmark in many modeling studies e g ilamb project collier et al 2018 see also the extensive publication list in kellndorfer et al 2013 however it is important to note that nbcd2000 is a data product subject to errors for the ecoregions 16 and 28 see map and documentation in kellndorfer et al 2013 encompassing the mountainous regions of utah and colorado the bootstrap validation of nbcd2000 against fia plot data indicates low mean bias error visual inspection and rmse of 2 kg c m 2 at the niwot ridge site we used field measurements of above ground biomass bradford et al 2008 for model assessment the agb values were obtained via allometric equations and measurements of tree height and diameter at breast height in 36 plots distributed over an area of 1 km2 surrounding the ameriflux tower 2 5 model configuration and simulations a complete list of the regional site level and diagnostic simulations performed within this study is shown in table 2 the core regional simulations across the utah colorado domain used active biogeochemistry with carbon and nitrogen cycling turned on and allowing for a dynamic vegetation state clm cn these core simulations were run with each of the 6 meteorological datasets discussed above 6 total runs and spun up from a bare ground state to understand the impact upon biomass we performed an identical set of clm cn simulations at a site near niwot ridge colorado us nr1 the meteorology biomass and carbon flux observations at us nr1 allowed us to test the accuracy of the 6 meteorological datasets and how the meteorological inputs influenced the accuracy in model simulations text s2a we also performed a series of diagnostic simulations to help understand the impact of 1 meteorological forcing clm sp runs 2 spatial resolution hires runs and 3 model structure clm5 phs clm5 sms runs upon model behavior we performed utah colorado simulations with satellite phenology clm sp that prescribed the vegetation state from satellite observations allowing for a controlled comparison of the impact of meteorological data upon carbon fluxes and water limitation text s2b next we isolated the impact of spatial resolution upon model performance by performing simulations hires with fine and coarse meteorological data gridmet and land surface characteristics across the colorado mountain transect text s2c fig 1 finally we used gridmet meteorology to drive utah colorado simulations using the improved and latest release of clm with contrasting representation of water stress clm5 phs clm5 sms text s2d the regional clm simulations were performed with default pft definitions and parameter values the site level clm simulations were performed with a single pft needleleaf evergreen boreal tree and adjusted parameter values for niwot ridge raczka et al 2016 for the regional model simulations we performed diagnostics for two elevation ranges separated by a threshold elevation z high elevation regions z z characterized by high agb versus low elevation regions z z characterized by low agb we chose a threshold elevation of z 2235 m a s l marking the elevation above which 75 of the total regional agb was located according to the nbcd2000 data product for the high elevation regions we further performed diagnostics for three terrain slope classes gentle s 3 33 moderate 3 33 s 6 15 and steep s 6 15 slopes where s is the slope angle the threshold values correspond to the 25th and 75th percentiles for the high elevation regions characterized by moderate or steep slopes we further performed diagnostics for eight slope aspect classes n ne e se s sw w nw 3 results 3 1 meteorological dataset comparison at high elevations the differences in downwelling shortwave radiation precipitation and specific humidity across datasets were substantial while the differences in air temperature were small fig 2 a d on average precipitation and specific humidity for the meteorological datasets tested here were consistently greater 9 45 and 33 56 respectively than in cru ncep default dataset in clm 4 5 the gridmet dataset clearly stood out with substantially greater precipitation values throughout the year mean value 45 greater than in cru ncep and 22 greater than in narr mstmip the second wettest dataset downwelling shortwave radiation values in gridmet and in narr mstmip were reasonably close to the values in cru ncep mean biases within 6 of cru ncep values while cfsr wang era wang and merra wang had greater values 14 20 the same general patterns described here were also found across different slope and aspect classes figs s1 s8 except for precipitation figs s2 and s6 gridmet precipitation was similar to the other datasets in the gentle slope class but clearly stood out with greater values in the moderate and steep slope classes mean values 45 and 49 greater than in cru ncep respectively fig s2 in the latter two classes there was a closer agreement across precipitation datasets for leeward facing slopes ne e and se note the markedly similar values of cfsr era merra wang and narr mstmip in figs s6c e g for the other slope aspects gridmet and narr mstmip clearly stood out with greater precipitation values than in cfsr era merra wang and cru ncep mean values 41 58 and 14 27 greater than in cru ncep respectively figs s6a b d f h at low elevations the meteorological datasets were in much closer agreement in terms of precipitation and specific humidity fig 2e h similar to the high elevation results precipitation and specific humidity values in cru ncep were generally smaller than in the other meteorological datasets but the differences were less pronounced differences in downwelling shortwave radiation across datasets were substantial and similar to the high elevation results while differences in air temperature were moderate and larger at low elevations compared to the niwot ridge tower observations in general the meteorological datasets were too warm and dry with mean temperature downwelling shortwave radiation and precipitation biases of 0 46 c 10 14 w m 2 and 0 53 mm day 1 respectively fig 3 the one important exception was gridmet the most accurate dataset overall which had both a cool 1 45 c and slight wet 0 2 mm day 1 bias specific and relative humidity across all datasets had mean biases of 0 05 g kg 1 and 4 91 respectively 3 2 impact of meteorological dataset upon clm 4 5 agb the modeled agb was highly sensitive to the meteorological datasets with the total regional agb ranging from 0 06 to 0 39 pg c figs 4 and 5 see also fig s9 simulations with the default meteorology cru ncep significantly underestimated agb in comparison with the nbcd2000 observation product total regional agb of 0 15 and 0 51 pg c respectively especially at high elevations above ground biomass simulated with cfsr wang merra wang and era wang meteorology was even smaller than with cru ncep at both high and low elevations with the most severe underestimation simulated with cfsr wang on the other hand clm performed significantly better when driven by narr mstmip or gridmet the gridmet simulation was the most consistent with nbcd2000 with a 242 increase in agb at high elevations and a 60 increase at low elevations in comparison with cru ncep helping to reduce the underestimation in total carbon from 0 36 to 0 12 pg c in terms of the distribution of agb between high and low elevations the cru ncep simulation was evenly distributed 55 total agb within high elevations while the gridmet simulation was nearly identical to nbcd2000 72 vs 75 total agb within high elevations respectively at high elevations the gridmet simulation consistently had the best performance across different slope and aspect classes figs s10 and s11 the deviation of the gridmet simulation from nbcd2000 was small in the gentle slope class 12 total agb but increased in the moderate 34 and steep 29 slope classes fig s10 overall the deviation was smaller for leeward facing slopes ne 28 e 24 se 35 than windward facing slopes sw 39 w 38 nw 30 fig s11 similar to the regional simulations there was significant variation in model performance agb associated with the meteorological data at niwot ridge fig 6 clm performed best when driven with gridmet presenting the smallest agb bias in respect to direct measurements and to simulated values using tower meteorology in comparison with the simulation using cru ncep agb increased 4 fold with gridmet 1 82 9 28 kg c m 2 slightly overshooting the site measurements 8 12 kg c m 2 3 3 impact of water stress representation upon clm 4 5 agb across all the dynamic vegetation state simulations with clm cn table 2 water limitation was a much stronger limiting factor than nitrogen limitation fig s12 this is consistent with the clm sp results that simulated the lowest soil moisture stress at high elevations when gridmet was used fig 7 b see also figs s14 and s17 humidity stress was also the lowest as indicated by the high r h l e a f values fig 7c see also figs s15 and s18 as described in sect 2 1 1 both β t and rhleaf have a direct impact on simulated stomatal conductance and photosynthesis see eqs 1 4 accordingly the modeled g p p p o t was significantly greater with gridmet fig 7a see also figs s13 and s16 at low elevations the results were less clear but still showed reduced water stress and increased g p p p o t for the gridmet simulation fig 7d f amongst all meteorological datasets gridmet led to simulated agb that was closest to the niwot ridge observations agb bias of 1 16 kg c m 2 which was comparable with the model performance when using direct tower observations as drivers agb bias of 0 95 kg c m 2 in general site level simulations at niwot ridge show a strong reduction of gpp fig 8 a and npp fig 8b during spring and summer because of a reduction in stomatal conductance fig 8c caused by significant soil moisture stress i e lower β t values fig 8d and reduced r h l e a f fig 8e the strongest water stress and npp reduction were observed with the cru ncep dataset while the weakest was observed with gridmet in alignment with the agb results in fig 6 3 4 impact of spatial resolution upon clm 4 5 we found that fine spatial resolution surface maps and meteorological data captured additional agb spatial variation within the colorado mountain transect however this additional spatial resolution did not significantly change the total agb simulated across the transect by refining the surface maps from 1 5 1 5 to 1 24 1 24 while keeping the meteorological forcing at 1 2 1 2 the total regional agb had a small reduction from 0 0286 to 0 0259 pg c fig 9 a and c respectively by refining the meteorological data from 1 2 1 2 to 1 24 1 24 while keeping the surface maps at 1 24 1 24 fig 9c and e respectively see also fig s19 the total regional agb had a minimal reduction from 0 0259 to 0 0258 pg c the results indicate that the coarser grids are adequate to simulate regional agb yielding similar values in comparison with the finer grids at a smaller computational cost 3 5 impact of adopting latest version of clm 5 0 upon agb the more recent release of clm clm5 phs simulated higher agb than clm 4 5 for utah colorado and when driven with gridmet was the only simulation that eliminated the underestimation in agb 0 15 pg c bias fig 5 when the plant hydraulic water stress model within clm5 phs was replaced with a soil moisture stress model clm5 sms the biomass decreased by 38 and was nearly identical to the biomass for clm 4 5 fig 5 this strongly suggests that the different representations of water limitation between clm model versions sects 2 1 1 and 2 1 2 led to the difference in agb the distribution of biomass for the clm 5 0 simulations was less accurate than clm 4 5 with 88 clm5 phs 98 clm5 sms and 72 clm4 5 of biomass above the 2235 m elevation threshold figs 4 5 and s9 for reference the nbcd2000 observation data product indicates only 75 of biomass above the elevation threshold the clm 5 0 simulations generally overestimated biomass at the highest elevations and underestimated at lower elevations degrading the overall spatial pattern of biomass with a larger pixel based average absolute bias for clm5 phs 0 69 kg c m 2 and clm5 sms 0 75 kg c m 2 compared to clm 4 5 0 45 kg c m 2 figs 4 and s9 interestingly for the high elevation regions z 2235 m the clm5 sms simulation was the most consistent with nbcd2000 in terms of total agb 0 59 0 40 0 28 and 0 39 pg c for clm5 phs clm5 sms clm 4 5 and nbcd2000 respectively fig 5 and mean agb for different slope and aspect classes figs s10 and s11 however the clm5 sms simulation had the worst overall spatial pattern of biomass with the largest pixel based average absolute bias 1 88 kg c m 2 versus 1 80 kg c m 2 in clm5 phs and 1 05 kg c m 2 in clm 4 5 figs 4 and s9 4 discussion 4 1 impact of meteorological forcing on model performance the clm simulations of agb were highly sensitive to the type of meteorological dataset and persistently underestimated the observed nbcd2000 values figs 4 5 and s9 to s11 model diagnostics indicate that vegetation productivity was suppressed primarily from water limitation whereas nitrogen limitation played a smaller role fig s12 water limitation was most effectively relieved when using the gridmet meteorological dataset which was specifically designed for complex terrain text s1d and exhibited the most accuracy when compared against tower based meteorological observations especially in terms of precipitation and downwelling shortwave radiation the cooler temperature aligned with increased precipitation of gridmet during winter figs 2 s2 s3 s6 and s7 was conducive to the formation and maintenance of a snowpack helping to further mitigate water stress during the drier summer season the relatively wet high prescribed precipitation and cool low prescribed atmospheric temperature conditions of gridmet aligned with moderate downwelling shortwave radiation values also led to moderate surface temperature figs s20 s26 promoting increased net primary productivity and agb by limiting autotrophic respiration losses and atmospheric water stress in contrast to the gridmet simulations the wang meteorological datasets cfsr wang era wang and merra wang led to simulations that were strongly limited by water stress resulting in drastic underestimation of agb comparable to the simulations using the default cru ncep figs 4 5 and s9 to s11 this result was unexpected given that a clm hydrology analysis found improved simulated surface runoff for these same meteorological datasets across the western united states wang et al 2016 this improved performance for simulated hydrology wang et al 2016 and not for agb in our analysis figs 4 5 and s9 to s11 was likely because the surface runoff and subsurface drainage of water are routed directly to the river model from the grid cells in clm and are much less affected by positive biases in downwelling shortwave radiation and surface temperature plant growth and agb on the other hand are highly sensitive to surface temperature and the positive biases in downwelling shortwave radiation figs 2 3 s1 and s5 which should have led to unrealistically high evaporative demand of soil moisture limiting carbon uptake note that the positive biases in shortwave radiation could also be a reason for the early snowmelt and peak runoff reported by wang et al 2016 the simulations using narr mstmip led to agb in between the gridmet and cru ncep wang results figs 4 5 s10 and s11 an important reason for the improved performance relative to the wang datasets was that narr mstmip received both precipitation and downwelling shortwave radiation corrections text s1c whereas wang datasets received a precipitation correction only text s1b focusing on high elevation areas figs 2a d and s1 to s8 the precipitation and specific humidity were substantially higher in narr mstmip than in cru ncep while downwelling shortwave radiation was lower resulting in lower evaporative demand less water stress and increased plant productivity as supported by the clm sp results figs 7a c and s13 to s18 and also by the site level clm results at niwot ridge fig 8 showing higher β t r h l e a f and gpp for narr mstmip as expected for the high elevation regions the six clm 4 5 cn simulations performed better for gentle slopes fig s10 the greater underestimation of agb for moderate and steep slopes more evident for the cn cfsr merra era wang simulations was likely not only due to a reduced meteorological accuracy in those regions but also to structural limitations and parametric uncertainty in clm impacting the simulation of lateral flow snowpack and surface energy balance note that there is no representation of hillslope hydrology and sub grid topographical effects on solar radiation and precipitation in clm in addition to limitations in the definition of pfts and their parameterization overall the simulations performed worse for windward facing slopes sw w and nw than leeward facing slopes ne e and se fig s11 likely due to greater precipitation biases in the former group more evident for the cfsr merra era wang datasets see also fig s6 note that clm does not consider slope aspect directly but indirectly through its input datasets the persistent underestimation of agb across complex terrain is supported by the literature for example both the cmip5 taylor et al 2012 see ilamb https ilamb ornl gov cmpi5 and mstmip huntzinger et al 2013 2018 model inter comparison projects that included a wide range of earth system models coupled simulations and land surface models uncoupled simulations respectively generally underestimated biomass across the western united states our analysis that links biased meteorology enhanced water stress and limitation of vegetation productivity suggests that water carbon coupling is the source of this persistent underestimation in agb 4 2 impact of spatial resolution on model performance whereas the clm simulations of agb and carbon uptake were highly sensitive to the type of meteorological dataset the hires diagnostic simulations demonstrated clm was generally insensitive to changes in spatial resolution the more spatially resolved hires simulations 1 24 meteorology and land surface provided new information about biomass distribution at fine spatial scales fig 9 however the increased spatial resolution did not significantly influence the overall magnitude of biomass and carbon flux across the domain this finding directly addresses a critical question within the tbm community fan et al 2019 to what extent abiotic and biotic processes within complex terrain need to be resolved explicitly or parameterized implicitly to provide accurate simulations at larger domains in this case we show that the effects of finely distributed biotic and abiotic processes on the water carbon cycle tend to average out at coarser scales this result is somewhat surprising given the complexity of clm 4 5 and the non linear relationships that may exist between soil moisture evapotranspiration and processes that depend upon it baker et al 2017 we speculate that this could indicate limitations in the way that clm 4 5 represents subsurface hydrology that omits subsurface transfer of water between grid cells that convey soil moisture from uphill divergent locations to downhill convergent locations within the landscape swenson et al 2019 clm 4 5 also does not consider sub grid topographical effects on solar radiation and precipitation and lacks explicit representation of plant demographics including mortality recruitment and competition finally this analysis compared coarse 1 5 and fine 1 24 spatial resolution simulations yet not all land surface characteristics were available at the fine resolution table s1 to fully resolve the land surface characteristics at the mountain slope scale 10 km it is also worth noting that the 1 24 simulations finely resolved the distribution of pfts across the land surface but adopted the default broad pft definitions in clm as discussed in sect 4 3 higher pft species specificity was likely very important for removing the negative biases in agb within buotte et al 2019 our results align with those reported by slevin et al 2017 for a different tbm jules and coarser spatial resolutions surface and meteorological forcing data at 1 2 1 2 1 1 2 2 they found their gpp simulations to be globally and regionally insensitive to the changes in spatial resolution this is likely a limitation for most tbms which are primarily developed for global applications and lack the fine scale representation of features important for watersheds within complex terrain hao et al 2021 recently showed that the implementation of sub grid topographical effects on solar radiation in the e3sm land model resulted in improved simulations of surface energy balance snow cover and surface temperature over complex terrain in the tibetan plateau they demonstrated that the sub grid topographical effects cannot be neglected even at coarse spatial scales 2 we expect improvements in the simulation of carbon dynamics with this new model scheme but such impact still remains to be investigated the recent implementation of sub grid topographical effects on precipitation in e3sm tesfa et al 2020 is also promising but its impact on model simulations also remains to be investigated 4 3 further improving biomass simulations in the western united states with structural changes in clm improved meteorology by itself substantially reduced the underestimation in biomass for utah colorado sect 4 1 however the underestimation was completely eliminated only when combined with a more advanced representation of plant processes clm5 phs fig 5 although a full attribution of how all of the structural changes between clm 4 5 and 5 0 influenced the simulation is out of scope for this analysis sect 2 1 2 the fact that clm5 sms provided almost identical total biomass as compared to clm 4 5 whereas clm5 phs overestimated the biomass fig 5 strongly suggests plant water hydraulics kennedy et al 2019 lawrence et al 2018 played a major role soil moisture stress in the clm 4 5 and clm5 sms simulations were calculated using an empirical β t function eq 5 which is a common approach used in other tbms ibis foley et al 1996 jules clark et al 2011 sib3 baker et al 2008 however the simplicity of the β t function approach is subject to known limitations to simulating water carbon coupling contributing to unrealistic large scale drought response powell et al 2013 and large uncertainty in carbon cycle simulations trugman et al 2018 the clm5 phs formulation on the other hand provides a mechanistic approach based on plant hydraulic theory that explicitly tracks water flow through the soil roots and xylem in order to calculate a dynamic leaf water potential and stomatal conductance the relatively high amount of biomass for clm5 phs is consistent with the tendency for the plant hydraulics scheme to access deep water reserves and reduce water limitation overall kennedy et al 2019 the use of clm5 phs eliminated the underestimation of biomass for utah colorado however the accuracy of the simulation did not improve and the spatial distribution was worse than for clm 4 5 with biomass overestimation at the highest elevations and underestimation at lower elevations figs 4 5 and s9 this degraded performance with a more advanced and skillful tbm lawrence et al 2018 is not surprising given that out of the box parameterizations are designed to work globally but generally require tuning to function at a specific location or sub region the sensitivity of our results to the representation of plant hydraulic stress points to the need for hydraulic trait data to better parameterize tree species across the western united states anderegg 2015 anderegg and venturas 2020 the importance of species specific trait data related to water carbon coupling is supported by the literature for example improved simulations of forested biomass across the western united states were achieved by using gridmet meteorology in combination with a version of clm 4 5 that modified the default description of forest pfts from 6 generic temperate boreal forest pfts to 13 species specific pfts representing major forest types in the western united states buotte et al 2019 an important parameter change was the use of species specific trait data to assign the soil water potential at which stomata are fully closed with values changing from a range of 2 4 to 2 5 mpa default parameterization to a range of 2 7 to 8 6 mpa across major tree species buotte et al 2019 although buotte et al 2019 included other parameter changes combined with structural changes related to increased leaf shedding and fire module adjustments within their clm 4 5 model simulation the parameter adjustment specific to stomate opening likely provided a substantial impact in alleviating the biomass underestimation we recommend a similar approach to buotte et al 2019 be applied here yet focusing on species specific hydraulic traits within clm 5 0 4 4 relevance to other models and complex terrain regions the accuracy of meteorological data was crucial to our simulated vegetation in utah and colorado and we expect this to be true across the entire western united states similar biases in precipitation and shortwave radiation that limited the accuracy of the utah and colorado simulations are also found elsewhere in the western us for example cru ncep systematically underestimated precipitation across the western us as compared to gridmet fig s27 in general tbm simulations with insufficient precipitation in regions such as the western us where vegetation productivity is water limited berner et al 2017 hashimoto et al 2019 will lead to unrealistic low productivity and biomass we speculate that tbms exhibit less sensitivity to precipitation biases in the pacific northwest region where higher annual precipitation and lower summer temperatures tend to reduce the impact of water limitation upon plant growth hashimoto et al 2019 lower sensitivity of the pacific northwest to meteorological biases is supported by a meteorology inter comparison 15 products using the tbm lpj guess ahlström et al 2017 in that analysis the western us expressed the highest amount of variation in simulated ecosystem carbon coefficient of variation 50 as compared to anywhere else in the lower 48 an indication of divergence of meteorology fields within complex topography yet this high variation was limited primarily to the interior western us and not the pacific northwest we also expect the accuracy of meteorological data to be particularly important for carbon cycle simulations in complex terrain across the globe especially where the terrain coincides with water limitation hashimoto et al 2019 for example global simulations of ecosystem carbon with lpj guess and 15 meteorological datasets indicate greater uncertainty at higher elevations compared to surrounding regions ahlström et al 2017 overall the variation in global ecosystem carbon from the ahlström et al 2017 analysis accounted for about 40 of the range reported for 18 cmip5 earth system models for 1850 2100 historical period and rcp 8 5 scenario anav et al 2013 in another global study bonan et al 2019 varied model structure clm 4 0 4 5 and 5 0 and meteorological dataset cru ncep and gswp3 and found that meteorological data contributed to 70 95 of the overall uncertainty in global vegetation carbon simulations for the historical period 1850 2014 the exact contribution of complex terrain to these global results however remains to be determined in comparison with the ahlström et al 2017 and bonan et al 2019 uncertainty analyses our results show an even stronger impact of meteorological uncertainty on terrestrial carbon simulations 6 fold variation in agb in the western us it is important to keep in mind that these prior studies were based on global simulations with coarsely resolved meteorological products our current study on the other hand provides a more complete representation of meteorological uncertainty as it includes not only global coarsely resolved products cru ncep and wang products but also regional finely resolved products narr mstmip and gridmet the gridmet dataset in particular uses an algorithm specific to complex terrain and is originally available at very fine spatial resolution 1 24 5 conclusions our results highlight the critical role meteorological forcing plays on simulations of carbon fluxes and stocks across complex terrain our simulations of vegetation biomass in the southern rockies varied substantially as a function of the meteorological dataset with almost all results exhibiting underestimations in agb we found water stress exacerbated by negative biases in precipitation and positive biases in downwelling shortwave radiation to be a critical restriction on plant growth among all the meteorological datasets gridmet had the smallest precipitation and downwelling shortwave radiation biases when compared against tower observations at a high elevation site in the colorado rockies niwot ridge and reduced the vegetation biomass biases the most when compared against site measurements niwot ridge and a regional inventory remote sensing based biomass data product nbcd2000 importantly it was both the accuracy of the meteorological dataset combined with an improved representation of water stress plant hydraulics that eliminated the biomass underestimation for utah colorado however the use of a more advanced and skillful tbm clm 5 0 actually showed degraded performance in terms of overall bias and spatial distribution of biomass with biomass overestimation at the highest elevations and underestimation at lower elevations this suggests species specific hydraulic trait data should be included in future simulations whereas it is likely that developing meteorological datasets at fine spatial scales helps improve meteorology accuracy e g gridmet the added computational expense of the implementation of model simulations at high resolution was not warranted given the minimal changes to carbon water dynamics at larger spatial domains this finding should be revisited for model representations that include higher pft species specificity and take into account other potentially important non linear processes in tbms including explicit or implicit representation of hillslope hydrology topographical effects on solar radiation and precipitation and demographic processes including mortality competition and recruitment we expect our findings to be applicable for complex terrain across the globe especially in water limited regions although the gridmet meteorological dataset is not available outside of the conterminous united states we recommend a similar approach be used in the meteorological forcing development that accounts for fine spatial features in the terrain that best removes meteorological biases software and data availability clm is the land component of cesm which is sponsored by the national science foundation nsf and the u s department of energy doe and maintained by the climate and global dynamics laboratory at the national center for atmospheric research ncar in boulder co usa clm 4 5 and 5 0 source code and input datasets are freely available at http www cesm ucar edu models cesm1 2 and https github com escomp ctsm respectively niwot ridge ameriflux site data are available in blanken 1998 present nbcd 2000 data are available in kellndorfer et al 2013 cfsr wang era wang and merra wang data are available from wang et al 2016 narr mstmip data are available in wei et al 2014b cru ncep data are available in ucar 2016 three hourly gridmet data are available in rupp and buotte 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the u s doe office of science terrestrial ecosystem science program awards de sc0010624 and de sc0010625 and by the nasa cms project awards nnx16ap33g and 80nssc20k0010 we would like to thank the center for high performance computing at the university of utah we would like to also acknowledge high performance computing support from cheyenne https doi org 10 5065 d6rx99hx provided by ncar s computational and information systems laboratory sponsored by nsf through allocation awards uusl0005 and uusl0007 funding for the ameriflux core site data us nr1 niwot ridge pi peter blanken was provided by the u s doe office of science funding for the nbcd 2000 project was provided by nasa s terrestrial ecology program award nng05g127g the development of the atmospheric forcing datasets here referred to as cfsr wang era wang and merra wang wang et al 2016 was supported by the national science foundation of china awards 41275110 and 41405087 and nsf award ags 0944101 funding for the multi scale synthesis and terrestrial model intercomparison project mstmip https nacp ornl gov mstmip shtml activity was provided through nasa roses award nnx10ag01a data management support for preparing documenting and distributing model driver and output data was performed by the modeling and synthesis thematic data center at oak ridge national laboratory ornl http nacp ornl gov with funding through nasa roses award nnh10an681 we thank peter blanken and sean burns for the us nr1 niwot ridge ameriflux data supported by the u s doe office of science through the ameriflux management project at lawrence berkeley national laboratory under award 7094866 we thank david rupp for disaggregating the daily gridmet data used here and in buotte et al 2019 we also thank anna trugman and bill anderegg for helpful discussions on this work we gratefully acknowledge the comments and suggestions of two anonymous reviewers which have greatly improved our paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105288 
25684,most carbon stocks and fluxes in the western united states are found in mountainous terrain where observations and modeling are difficult terrestrial biosphere models generally underestimate above ground biomass agb over this region here we identify methods to reduce this underestimation by focusing upon 1 biases in meteorological datasets 2 model representation of water stress and 3 spatial resolution we adopted the widely used community land model version 4 5 clm 4 5 with six different meteorological datasets and found a 6 fold variation in simulated agb across utah colorado simulations underestimated agb because of warm and dry biases within the meteorological datasets that reduced water availability and restricted plant growth to eliminate the agb underestimation we adopted a meteorological dataset designed for complex terrain gridmet combined with a representation of plant hydraulic stress clm 5 0 conversely changes in spatial resolution meteorological variables and land surface description had negligible impact on simulated agb keywords community land model meteorological forcing complex terrain vegetation biomass 1 introduction while the western united states accounts for a significant fraction of the country s total land carbon sink lu et al 2015 schimel et al 2000 forests in this region are also particularly vulnerable to disturbances such as drought insect damage and wildfires anderegg et al 2016 williams et al 2016 these disturbances have the potential to rapidly reduce the biomass within these ecosystems and release significant amounts of carbon to the atmosphere with climate change and increased warming and drying within the western united states these forest stressors are expected to increase in severity bentz et al 2010 buotte et al 2019 rocca et al 2014 seager et al 2007 and likely lead to rapid changes to these mountainous ecosystems thereby influencing carbon and water cycling availability of natural resources and recreational opportunities most of the carbon stocks and fluxes in the western united states are found in mountainous complex terrain which are inherently difficult to observe and model e g desai et al 2011 lin et al 2017 schimel et al 2002 sun et al 2010 modeling challenges can arise for many reasons including the definition of boundary conditions e g meteorology and land surface characteristics model parameters and model structure bonan et al 2019 dietze 2017 see also bonan and doney 2018 lin et al 2011 lovenduski and bonan 2017 the prescription of accurate boundary conditions in complex terrain poses a particularly daunting challenge first meteorological processes and land surface properties are highly variable because of large changes in elevation across fine spatial scales typical global and regional meteorological reanalysis products have increased uncertainty across complex terrain due to the relatively coarse horizontal grid spacing from 101 to 102 km and sparse meteorological observations with which to inform the products gao et al 2012 rose and apt 2016 second meteorological datasets are subject to strong biases in shortwave radiation and precipitation in complex terrain which reduce the accuracy of land surface simulations e g gómez navarro et al 2018 schroeder et al 2009 beyond accuracy the extent to which atmospheric conditions and land surface characteristics are spatially resolved may strongly impact terrestrial biosphere model tbm simulations for example the surface energy balance across complex terrain is influenced by the interaction of the solar angle with the slope aspect of the land surface this interaction promotes plant growth on south facing slopes at the highest elevations in the northern hemisphere and the opposite at relatively warm low elevations pelletier et al 2018 similarly soil water availability is influenced by slope aspect soil depth and soil texture swenson et al 2019 to what extent explicit spatial representation of boundary conditions and model processes are required to represent large scale water energy and biogeochemical fluxes across complex terrain however remains unclear fan et al 2019 although finely resolved surface and meteorological data are required to capture fine scale spatial variation of land surface processes fiddes and gruber 2014 it is less well known whether the fine scale information fed into the tbms is important for large spatial scale simulations do the effects of fine scale features simply average out at coarser scales hao et al 2021 recently demonstrated with the e3sm land model that sub grid topographical effects on solar radiation cannot be neglected even at coarse spatial scales 2 with differences in simulated snow cover fraction and surface heat fluxes of up to 20 in the tibetan plateau however the influence on carbon dynamics was not investigated in their study as they used prescribed vegetation phenology from satellite data parameterizations of sub grid topographical effects on precipitation were also implemented in e3sm recently tesfa et al 2020 but their impact on model simulations still remains to be investigated for reasons of both accuracy and spatial representation simulations of ecosystem function and structure are intrinsically sensitive to the meteorological forcing bonan et al 2019 medvigy et al 2010 wu et al 2017 zhao et al 2006 for example bonan et al 2019 found that the contribution of meteorological uncertainty to overall uncertainty in global carbon cycle simulations with clm for the historical period 1850 2014 can surpass model structure uncertainty finally many western u s forests exist upon a mortality boundary in which deviations towards warmer and drier conditions may increase forest mortality due to water stress that can lead to hydraulic failure carbon starvation insect damage and wildfire see anderegg et al 2013 goulden and bales 2019 hicke et al 2013 mcdowell et al 2008 therefore researchers need to understand what level of accuracy and explicit spatial representation is required of meteorology and land surface characteristics to provide realistic simulations of land surface behavior across complex terrain meteorological datasets have been developed that specifically account for the influence of complex terrain upon atmospheric conditions with spatial resolution on the order of 1 km e g prism daly et al 2008 daymet thornton et al 2017 gridmet abatzoglou 2013 despite their fine spatial resolution these meteorological datasets generally have coarse temporal resolution daily to monthly time steps whereas tbms typically require sub daily data a workaround to this limitation is to temporally downscale the daily or monthly meteorology with a reference meteorological data product available at finer temporal resolution for example buotte et al 2019 used this approach to temporally resolve daily gridmet data into a 3 hourly product based on narr data mesinger et al 2006 different tbms in the coupled model intercomparison project phase 5 cmip5 taylor et al 2012 generally underestimate biomass significantly in the western united states over the historical period pre industrial era to present collier et al 2018 see also https ilamb ornl gov cmip5 consistent with results from the international land model benchmarking project ilamb buotte et al 2019 carried out simulations for the western united states with the community land model clm version 4 5 oleson et al 2013 and found that the default model configuration and meteorological dataset underestimated total simulated biomass by one order of magnitude when compared against an observation based reference product wilson et al 2013 after substantial modifications buotte et al 2019 were able to boost forest productivity with total simulated biomass reaching twice the value of the observation based reference product these modifications included 1 the use of a high resolution meteorological dataset 1 24 1 24 gridmet disaggregated from daily to 3 hourly rupp and buotte 2020 2 high resolution 1 24 1 24 surface maps input surface variables such as topography soil properties and distribution of plant functional types 3 reassignment of original plant functional types within clm to major western u s forest types 4 changes in leaf shed rates to include effects of water stress and 5 changes in the fire module to reduce overestimation of burned area all modifications 1 to 5 above were incorporated in the buotte et al 2019 results the separate contribution from each of these individual modifications to improving the biomass simulation however was not investigated in their study here we focus on the impact that meteorological datasets alone have upon the simulation of carbon stocks and fluxes across the western united states the questions we address are to what extent can the underestimation of biomass by tbms in the western united states be improved with the use of more accurate meteorological datasets even in the absence of improvements in model parameters or structure furthermore to what extent can tbm performance be improved by a increased spatial resolution in land surface datasets or in combination with high resolution meteorological datasets and b more realistic representation of water carbon coupling to answer these questions we investigate the impact of six different meteorological datasets on the simulation of above ground biomass agb with clm version 4 5 in the southern rocky mountains focusing on the states of utah and colorado we compare the default meteorological dataset in clm 4 5 cru ncep ucar 2016 that led to low agb against the high resolution meteorological dataset developed in buotte et al 2019 and four other recently developed datasets that have been specifically corrected to remove biases in precipitation and downwelling shortwave radiation wang et al 2016 wei et al 2014 we also perform these simulations at varying levels of spatial resolution and with a more recent version of clm 5 0 which includes an improved representation of water stress based on plant hydraulics to determine if the underestimation in biomass can be removed we track simulated water and nitrogen limitation both regionally and at the site level to diagnose the constraints on plant growth and carbon cycling within the model 2 methods 2 1 model description we used clm 4 5 oleson et al 2013 for most of the simulations in our study sect 2 5 clm is the land component of the community earth system model cesm a fully coupled global climate model widely used by the scientific community with numerous applications to simulations of vegetation biomass and carbon cycling http www cesm ucar edu publications including applications across the western united states e g buotte et al 2019 duarte et al 2017 hudiburg et al 2013 raczka et al 2016 wieder et al 2017 biophysical and biogeochemical processes simulated by clm include surface energy and momentum fluxes and carbon water and nitrogen cycling the details of clm 4 5 are provided in sect 2 1 1 or within the technical description of oleson et al 2013 we refer to the standard version of clm 4 5 that internally predicts the vegetation state as clm cn we also use a special configuration of clm 4 5 that prescribes the vegetation state through satellite measured phenology clm sp more details of clm sp are provided in the supplement text s2 we used the latest release of clm version 5 0 to quantify the impact that the representation of water limitation has upon biomass growth bonan et al 2019 kennedy et al 2019 lawrence et al 2018 we perform two separate simulations with clm 5 0 the first includes a more mechanistic representation of water limitation through plant hydraulics clm5 phs kennedy et al 2019 the second uses an empirical sect 2 1 1 representation of water limitation that uses soil moisture stress clm5 sms more details about clm 5 0 are provided in section 2 1 2 2 1 1 clm 4 5 photosynthesis stomatal conductance carbon allocation and plant mortality here we focus on the clm 4 5 model components most important for the simulation of agb the photosynthesis in c3 plants is calculated based on the model proposed by farquhar et al 1980 net leaf photosynthesis a n is calculated as 1 a n min a c a j a p r d where r d is leaf level respiration and a c a j and a p are the rubisco limited light limited and product limited rates of carboxylation respectively the rubisco limited rate of carboxylation in c3 plants is calculated as 2 a c β t v c m a x c i γ c i k c 1 o i k o where β t is a soil moisture stress factor defined in eq 5 v c m a x is the maximum rate of carboxylation in the absence of soil moisture stress c i is the leaf intracellular co2 partial pressure γ is the co2 compensation point k c is the michaelis menten constant for co2 o i is the atmospheric o2 partial pressure and k o is the michaelis menten constant for o2 for brevity the formulations for a j and a p are omitted here see oleson et al 2013 for details leaf level respiration is calculated as 3 r d r d 25 f θ β t where r d 25 is the value of leaf level respiration at 25 c in the absence of soil moisture stress and f θ is a temperature correction factor leaf stomatal conductance g s is calculated based on the ball berry model as implemented by sellers et al 1996 in the sib2 model 4 g s m a n c s p a t m r h l e a f b β t where c s is the co2 partial pressure at leaf surface p a t m is the atmospheric pressure r h l e a f is the relative humidity at leaf surface and m and b are plant functional type pft based parameters the soil moisture stress factor used in eqs 2 4 is defined as 5 β t i 1 n w i r i where w i and r i are respectively plant wilting term and the fraction of roots at soil layer i the summation in eq 5 is over the entire soil column n soil layers clm calculates r i using an exponential root distribution function which depends on pft specific parameters i e the root distribution is fixed for each pft the plant wilting term is calculated for each soil layer as 6 w i ψ c ψ i ψ c ψ o θ s a t i θ i c e i θ s a t i 1 t s i 271 15 k and θ l i q i 0 0 t s i 271 15 k or θ l i q i 0 where ψ i is the soil water matric potential ψ c and ψ o are the soil water potential at full stomatal closure and opening respectively pft based parameters θ s a t i is the saturated volumetric water content θ i c e i is the volumetric ice content θ l i q i is the volumetric liquid water content and t s i is the soil temperature at soil layer i the resulting β t values range from 0 to 1 high to low soil moisture stress and are used to downscale a n and g s see eqs 2 4 clm solves for a n g s c i and c s using an iterative algorithm that is executed until the solution for c i converges calculations are carried out separately for sunlit and shaded leaves the potential gross primary production without nitrogen limitation g p p p o t is calculated as 7 g p p p o t a n s u n r d s u n l a i s u n a n s h a r d s h a l a i s h a where the superscripts sun and sha denote sunlit and shaded leaves respectively l a i is the leaf area index allocation of newly assimilated carbon g p p p o t is prioritized in the following order 1 support of maintenance respiration of live plant tissues 2 replenishment of the excess maintenance respiration pool a storage pool that is allowed to run a negative state and is used to support maintenance respiration when g p p p o t is insufficient e g during nighttime winter or stress periods 3 support of plant growth the amount of carbon that is actually allocated to new growth is limited by nitrogen availability therefore clm calculates the actual gross primary production g p p as 8 g p p g p p p o t 1 d where d is a nitrogen limitation factor calculated as 9 d c f a v a i l a l l o c c f a l l o c g p p p o t where c f a v a i l a l l o c is the carbon flux from photosynthesis that is available to new growth allocation i e g p p p o t minus the allocation fluxes to maintenance respiration and to the excess maintenance respiration pool and c f a l l o c is the actual carbon allocation to new growth restricted by nitrogen availability structural carbon and nitrogen pools simulated by clm include leaf stem live and dead tissues coarse root live and dead tissues and fine root the c n ratios of each of these plant tissues and allometric ratios are defined for each pft these parameters are used to determine c f a l l o c and how carbon is allocated to each pool simulated agb corresponds to the sum of the leaf and stem carbon pools leafc livestemc and deadstemc variables in clm clm represents whole plant mortality with a fixed empirical rate of 2 year 1 that includes mortality associated with drought pests diseases wind throw and other natural disturbances except fire fire related mortality is simulated dynamically with a separate module mortality due to anthropogenic land cover change is also calculated separately based on the prescribed land cover within the model which is a function of time recruitment is implicitly accounted for in the c f a l l o c term actual carbon allocation to new growth eq 9 our simulations considered the fixed empirical mortality rate of 2 year 1 due to natural disturbances and the mortality due to anthropogenic land cover change during a transient period from pre industrial era to near present day based on prescribed data see sect 2 5 here we opted to turn off the fire module as it tends to overestimate burn area buotte et al 2019 duarte et al 2017 raczka et al 2016 zhang et al 2016 note that the fixed 2 year 1 rate means that clm does not have a mechanistic representation of natural mortality it corresponds to a middle range value based upon observations across a wide sampling of biomes lawrence et al 2018 this moderate mortality value in combination with fire exclusion led us to treat the persistent underestimation of biomass within clm as due to suppression of plant productivity rather than due to excessive mortality or fire activity 2 1 2 clm 5 0 we provide a brief overview of the differences between clm 5 0 and 4 5 most relevant for agb simulations across the western united states more details about clm 5 0 are provided in lawrence et al 2018 foremost clm 5 0 includes a representation of plant hydraulic transport which calculates the water potential for roots xylem and leaves leaf transpiration is modulated through stomatal conductance in such a way to avoid excessively high xylem tension and low leaf water potential this mechanistic approach replaces the empirical soil moisture stress approach in clm 4 5 eqs 5 and 6 above which tends to exaggerate the limitation of photosynthesis powell et al 2013 the maximum stomatal conductance is based upon atmospheric vapor pressure deficit medlyn et al 2011 which replaces the ball berry relative humidity approach eq 4 clm 5 0 includes a spatially varying soil depth as well as an improved representation of soil moisture evaporation the new nitrogen limitation approach calculates leaf c n ratios that are used to simulate a dynamic photosynthetic capacity avoiding the instantaneous downregulation of photosynthesis through nitrogen availability eqs 8 and 9 above and allowing for a more realistic relationship between n limitation and stomatal conductance ghimire et al 2016 a fixed and dynamic carbon allocation scheme is used for clm 5 0 and clm 4 5 respectively the change to the fixed allocation scheme reflects the fact that plant biomass saturates with increased productivity in contrast with the dynamic allocation scheme that continually increases allocation with increased net primary production npp negrón juárez et al 2015 the default configuration of clm 5 0 includes all of these changes including the plant hydraulic stress representation of water carbon coupling clm5 phs the clm5 sms configuration is identical except that the water carbon coupling is represented through the same soil moisture stress configuration used in clm 4 5 2 2 study regions utah colorado colorado mountain transect and niwot ridge co most simulations were performed across the states of utah and colorado with the model domain defined between 37 00 n and 42 00 n and 114 05 w and 102 05 w fig 1 utah and colorado are representative of the western u s region in terms of topography and drought susceptibility a significant fraction of this region falls within the southern rocky mountains srm ecoregion bailey 1995 drummond 2012 which is characterized by complex topography several hundred peaks with elevation above 3600 m a s l are located within the srm ecoregion drummond 2012 including the highest peak of the rocky mountains system mount elbert colorado 4401 m a s l the srm climate is characterized by mean annual temperature from 2 to 10 c and mean annual precipitation ranging between 260 and 1020 mm bailey 1995 most precipitation falls as snow leading to significant snowpack at higher elevations critical for water supply for the srm and surrounding ecoregions drummond 2012 vegetation distribution has a strong dependence on altitude latitude predominant wind direction and aspect bailey 1995 common vegetation types include drummond 2012 engelmann spruce picea engelmannii and subalpine fir abies lasiocarpa in the subalpine zone ponderosa pine pinus ponderosa aspen populus tremuloides juniper juniperus spp and oak quercus spp in the montane zone sagebrush artemisia tridentata oak pinyon juniper woodland pinus edulis and juniperus scopulorum monosperma and osteosperma and blue grama grass bouteloua gracilis in the foothill zone grasslands and shrublands in the low elevation valleys additional simulations were performed across a more limited domain of a colorado mountain transect fig 1 to specifically test the impact of spatial resolution within the meteorological dataset and land surface description site level clm simulations were performed at the niwot ridge ameriflux core site us nr1 burns et al 2015 monson et al 2002 a high elevation site in colorado 40 03 n 105 55 w 3050 m a s l see fig 1 niwot ridge was chosen based on its extensive records of biological meteorological and flux observations allowing us to directly evaluate both the meteorological datasets and the model s performance in terms of carbon fluxes and agb niwot ridge is located on a gentle 3 4 east facing slope the site is characterized by 120 year old subalpine forest composed mainly of subalpine fir engelmann spruce and lodgepole pine pinus contorta and mean annual temperature and precipitation of 1 5 c and 800 mm respectively monson et al 2005 2 3 meteorological datasets the clm 4 5 simulations used six different meteorological datasets described in table 1 first we used the default dataset in clm 4 5 cru ncep as a baseline next we used cfsr wang era wang and merra wang datasets that all include a bias correction for precipitation global mean 12 higher than cru ncep and generally improve global simulations of hydrological variables with clm sp in comparison with cru ncep wang et al 2016 we also used narr mstmip a dataset that includes bias corrections for precipitation and downwelling shortwave radiation and is originally available at a finer grid spacing 1 4 1 4 vs 1 2 1 2 in cru ncep and wang datasets finally we used gridmet a dataset tailored for complex terrain applications and originally available at a very fine grid spacing 1 24 1 24 see text s1d for consistency across all datasets we coarsened narr mstmip and gridmet to 1 2 1 2 the site level simulations used the same meteorological datasets but used the grid cell that corresponded to the location of niwot ridge we also used hourly tower based observations at the site see blanken 1998 present to evaluate clm for consistency between tower observations and the other datasets we only used 1998 2007 meteorological data to drive the site level simulations additional information on all meteorological data is provided in the supplement text s1 the meteorological variables required to drive clm are downwelling shortwave radiation s a t m precipitation p t a t m and near surface air temperature t a t m specific humidity q a t m and wind speed w a t m relative humidity r h a t m or dew point temperature t d a t m can also be used instead of q a t m downwelling longwave radiation l a t m near surface atmospheric pressure p a t m and the partitioning of s a t m into diffuse and direct components are optional clm estimates these variables if they are missing clm automatically interpolates the prescribed meteorological variable in both space and time down to the same grid size of the provided surface maps and the time interval used for model calculations default of 30 min spatial interpolation is carried out via a bilinear algorithm whereas the temporal interpolation is linear for all variables except for s a t m and p t a t m which are interpolated as the cosine of the solar zenith angle and by the nearest value in time respectively the exact meteorological variables we used to drive clm are listed in table 1 for each dataset considered 2 4 biomass datasets we used above ground biomass data from the national biomass and carbon dataset for the year 2000 version 2 nbcd2000 kellndorfer et al 2013 2007 2009 to assess our regional clm simulations the original dataset includes above ground estimates of live dry biomass over the conterminous united states at fine grid spacing 30 m 30 m combining forest inventory and analysis fia data from the united states forest service with high resolution interferometric synthetic aperture radar insar data from the 2000 shuttle radar topography mission srtm and landsat enhanced thematic mapper plus etm data further details can be found in kellndorfer et al 2013 for our analysis we used the nbcd2000 data product in which tree level biomass estimates were obtained from fia s database drybiot variable we used the spatial data access tool sdat ornl daac 2017 available at the nbcd2000 product web site kellndorfer et al 2013 to download a customized version of the dataset for our study region at 0 01 0 01 grid spacing we later aggregated the product at 0 2 0 2 grid spacing and also converted the values to above ground live biomass carbon defined here as 50 of originally reported dry biomass to allow a direct comparison with our regional clm simulations the nbcd2000 dataset has the advantage of being a finely resolved gridded product with continuous coverage in the conterminous united states making it a recurrent benchmark in many modeling studies e g ilamb project collier et al 2018 see also the extensive publication list in kellndorfer et al 2013 however it is important to note that nbcd2000 is a data product subject to errors for the ecoregions 16 and 28 see map and documentation in kellndorfer et al 2013 encompassing the mountainous regions of utah and colorado the bootstrap validation of nbcd2000 against fia plot data indicates low mean bias error visual inspection and rmse of 2 kg c m 2 at the niwot ridge site we used field measurements of above ground biomass bradford et al 2008 for model assessment the agb values were obtained via allometric equations and measurements of tree height and diameter at breast height in 36 plots distributed over an area of 1 km2 surrounding the ameriflux tower 2 5 model configuration and simulations a complete list of the regional site level and diagnostic simulations performed within this study is shown in table 2 the core regional simulations across the utah colorado domain used active biogeochemistry with carbon and nitrogen cycling turned on and allowing for a dynamic vegetation state clm cn these core simulations were run with each of the 6 meteorological datasets discussed above 6 total runs and spun up from a bare ground state to understand the impact upon biomass we performed an identical set of clm cn simulations at a site near niwot ridge colorado us nr1 the meteorology biomass and carbon flux observations at us nr1 allowed us to test the accuracy of the 6 meteorological datasets and how the meteorological inputs influenced the accuracy in model simulations text s2a we also performed a series of diagnostic simulations to help understand the impact of 1 meteorological forcing clm sp runs 2 spatial resolution hires runs and 3 model structure clm5 phs clm5 sms runs upon model behavior we performed utah colorado simulations with satellite phenology clm sp that prescribed the vegetation state from satellite observations allowing for a controlled comparison of the impact of meteorological data upon carbon fluxes and water limitation text s2b next we isolated the impact of spatial resolution upon model performance by performing simulations hires with fine and coarse meteorological data gridmet and land surface characteristics across the colorado mountain transect text s2c fig 1 finally we used gridmet meteorology to drive utah colorado simulations using the improved and latest release of clm with contrasting representation of water stress clm5 phs clm5 sms text s2d the regional clm simulations were performed with default pft definitions and parameter values the site level clm simulations were performed with a single pft needleleaf evergreen boreal tree and adjusted parameter values for niwot ridge raczka et al 2016 for the regional model simulations we performed diagnostics for two elevation ranges separated by a threshold elevation z high elevation regions z z characterized by high agb versus low elevation regions z z characterized by low agb we chose a threshold elevation of z 2235 m a s l marking the elevation above which 75 of the total regional agb was located according to the nbcd2000 data product for the high elevation regions we further performed diagnostics for three terrain slope classes gentle s 3 33 moderate 3 33 s 6 15 and steep s 6 15 slopes where s is the slope angle the threshold values correspond to the 25th and 75th percentiles for the high elevation regions characterized by moderate or steep slopes we further performed diagnostics for eight slope aspect classes n ne e se s sw w nw 3 results 3 1 meteorological dataset comparison at high elevations the differences in downwelling shortwave radiation precipitation and specific humidity across datasets were substantial while the differences in air temperature were small fig 2 a d on average precipitation and specific humidity for the meteorological datasets tested here were consistently greater 9 45 and 33 56 respectively than in cru ncep default dataset in clm 4 5 the gridmet dataset clearly stood out with substantially greater precipitation values throughout the year mean value 45 greater than in cru ncep and 22 greater than in narr mstmip the second wettest dataset downwelling shortwave radiation values in gridmet and in narr mstmip were reasonably close to the values in cru ncep mean biases within 6 of cru ncep values while cfsr wang era wang and merra wang had greater values 14 20 the same general patterns described here were also found across different slope and aspect classes figs s1 s8 except for precipitation figs s2 and s6 gridmet precipitation was similar to the other datasets in the gentle slope class but clearly stood out with greater values in the moderate and steep slope classes mean values 45 and 49 greater than in cru ncep respectively fig s2 in the latter two classes there was a closer agreement across precipitation datasets for leeward facing slopes ne e and se note the markedly similar values of cfsr era merra wang and narr mstmip in figs s6c e g for the other slope aspects gridmet and narr mstmip clearly stood out with greater precipitation values than in cfsr era merra wang and cru ncep mean values 41 58 and 14 27 greater than in cru ncep respectively figs s6a b d f h at low elevations the meteorological datasets were in much closer agreement in terms of precipitation and specific humidity fig 2e h similar to the high elevation results precipitation and specific humidity values in cru ncep were generally smaller than in the other meteorological datasets but the differences were less pronounced differences in downwelling shortwave radiation across datasets were substantial and similar to the high elevation results while differences in air temperature were moderate and larger at low elevations compared to the niwot ridge tower observations in general the meteorological datasets were too warm and dry with mean temperature downwelling shortwave radiation and precipitation biases of 0 46 c 10 14 w m 2 and 0 53 mm day 1 respectively fig 3 the one important exception was gridmet the most accurate dataset overall which had both a cool 1 45 c and slight wet 0 2 mm day 1 bias specific and relative humidity across all datasets had mean biases of 0 05 g kg 1 and 4 91 respectively 3 2 impact of meteorological dataset upon clm 4 5 agb the modeled agb was highly sensitive to the meteorological datasets with the total regional agb ranging from 0 06 to 0 39 pg c figs 4 and 5 see also fig s9 simulations with the default meteorology cru ncep significantly underestimated agb in comparison with the nbcd2000 observation product total regional agb of 0 15 and 0 51 pg c respectively especially at high elevations above ground biomass simulated with cfsr wang merra wang and era wang meteorology was even smaller than with cru ncep at both high and low elevations with the most severe underestimation simulated with cfsr wang on the other hand clm performed significantly better when driven by narr mstmip or gridmet the gridmet simulation was the most consistent with nbcd2000 with a 242 increase in agb at high elevations and a 60 increase at low elevations in comparison with cru ncep helping to reduce the underestimation in total carbon from 0 36 to 0 12 pg c in terms of the distribution of agb between high and low elevations the cru ncep simulation was evenly distributed 55 total agb within high elevations while the gridmet simulation was nearly identical to nbcd2000 72 vs 75 total agb within high elevations respectively at high elevations the gridmet simulation consistently had the best performance across different slope and aspect classes figs s10 and s11 the deviation of the gridmet simulation from nbcd2000 was small in the gentle slope class 12 total agb but increased in the moderate 34 and steep 29 slope classes fig s10 overall the deviation was smaller for leeward facing slopes ne 28 e 24 se 35 than windward facing slopes sw 39 w 38 nw 30 fig s11 similar to the regional simulations there was significant variation in model performance agb associated with the meteorological data at niwot ridge fig 6 clm performed best when driven with gridmet presenting the smallest agb bias in respect to direct measurements and to simulated values using tower meteorology in comparison with the simulation using cru ncep agb increased 4 fold with gridmet 1 82 9 28 kg c m 2 slightly overshooting the site measurements 8 12 kg c m 2 3 3 impact of water stress representation upon clm 4 5 agb across all the dynamic vegetation state simulations with clm cn table 2 water limitation was a much stronger limiting factor than nitrogen limitation fig s12 this is consistent with the clm sp results that simulated the lowest soil moisture stress at high elevations when gridmet was used fig 7 b see also figs s14 and s17 humidity stress was also the lowest as indicated by the high r h l e a f values fig 7c see also figs s15 and s18 as described in sect 2 1 1 both β t and rhleaf have a direct impact on simulated stomatal conductance and photosynthesis see eqs 1 4 accordingly the modeled g p p p o t was significantly greater with gridmet fig 7a see also figs s13 and s16 at low elevations the results were less clear but still showed reduced water stress and increased g p p p o t for the gridmet simulation fig 7d f amongst all meteorological datasets gridmet led to simulated agb that was closest to the niwot ridge observations agb bias of 1 16 kg c m 2 which was comparable with the model performance when using direct tower observations as drivers agb bias of 0 95 kg c m 2 in general site level simulations at niwot ridge show a strong reduction of gpp fig 8 a and npp fig 8b during spring and summer because of a reduction in stomatal conductance fig 8c caused by significant soil moisture stress i e lower β t values fig 8d and reduced r h l e a f fig 8e the strongest water stress and npp reduction were observed with the cru ncep dataset while the weakest was observed with gridmet in alignment with the agb results in fig 6 3 4 impact of spatial resolution upon clm 4 5 we found that fine spatial resolution surface maps and meteorological data captured additional agb spatial variation within the colorado mountain transect however this additional spatial resolution did not significantly change the total agb simulated across the transect by refining the surface maps from 1 5 1 5 to 1 24 1 24 while keeping the meteorological forcing at 1 2 1 2 the total regional agb had a small reduction from 0 0286 to 0 0259 pg c fig 9 a and c respectively by refining the meteorological data from 1 2 1 2 to 1 24 1 24 while keeping the surface maps at 1 24 1 24 fig 9c and e respectively see also fig s19 the total regional agb had a minimal reduction from 0 0259 to 0 0258 pg c the results indicate that the coarser grids are adequate to simulate regional agb yielding similar values in comparison with the finer grids at a smaller computational cost 3 5 impact of adopting latest version of clm 5 0 upon agb the more recent release of clm clm5 phs simulated higher agb than clm 4 5 for utah colorado and when driven with gridmet was the only simulation that eliminated the underestimation in agb 0 15 pg c bias fig 5 when the plant hydraulic water stress model within clm5 phs was replaced with a soil moisture stress model clm5 sms the biomass decreased by 38 and was nearly identical to the biomass for clm 4 5 fig 5 this strongly suggests that the different representations of water limitation between clm model versions sects 2 1 1 and 2 1 2 led to the difference in agb the distribution of biomass for the clm 5 0 simulations was less accurate than clm 4 5 with 88 clm5 phs 98 clm5 sms and 72 clm4 5 of biomass above the 2235 m elevation threshold figs 4 5 and s9 for reference the nbcd2000 observation data product indicates only 75 of biomass above the elevation threshold the clm 5 0 simulations generally overestimated biomass at the highest elevations and underestimated at lower elevations degrading the overall spatial pattern of biomass with a larger pixel based average absolute bias for clm5 phs 0 69 kg c m 2 and clm5 sms 0 75 kg c m 2 compared to clm 4 5 0 45 kg c m 2 figs 4 and s9 interestingly for the high elevation regions z 2235 m the clm5 sms simulation was the most consistent with nbcd2000 in terms of total agb 0 59 0 40 0 28 and 0 39 pg c for clm5 phs clm5 sms clm 4 5 and nbcd2000 respectively fig 5 and mean agb for different slope and aspect classes figs s10 and s11 however the clm5 sms simulation had the worst overall spatial pattern of biomass with the largest pixel based average absolute bias 1 88 kg c m 2 versus 1 80 kg c m 2 in clm5 phs and 1 05 kg c m 2 in clm 4 5 figs 4 and s9 4 discussion 4 1 impact of meteorological forcing on model performance the clm simulations of agb were highly sensitive to the type of meteorological dataset and persistently underestimated the observed nbcd2000 values figs 4 5 and s9 to s11 model diagnostics indicate that vegetation productivity was suppressed primarily from water limitation whereas nitrogen limitation played a smaller role fig s12 water limitation was most effectively relieved when using the gridmet meteorological dataset which was specifically designed for complex terrain text s1d and exhibited the most accuracy when compared against tower based meteorological observations especially in terms of precipitation and downwelling shortwave radiation the cooler temperature aligned with increased precipitation of gridmet during winter figs 2 s2 s3 s6 and s7 was conducive to the formation and maintenance of a snowpack helping to further mitigate water stress during the drier summer season the relatively wet high prescribed precipitation and cool low prescribed atmospheric temperature conditions of gridmet aligned with moderate downwelling shortwave radiation values also led to moderate surface temperature figs s20 s26 promoting increased net primary productivity and agb by limiting autotrophic respiration losses and atmospheric water stress in contrast to the gridmet simulations the wang meteorological datasets cfsr wang era wang and merra wang led to simulations that were strongly limited by water stress resulting in drastic underestimation of agb comparable to the simulations using the default cru ncep figs 4 5 and s9 to s11 this result was unexpected given that a clm hydrology analysis found improved simulated surface runoff for these same meteorological datasets across the western united states wang et al 2016 this improved performance for simulated hydrology wang et al 2016 and not for agb in our analysis figs 4 5 and s9 to s11 was likely because the surface runoff and subsurface drainage of water are routed directly to the river model from the grid cells in clm and are much less affected by positive biases in downwelling shortwave radiation and surface temperature plant growth and agb on the other hand are highly sensitive to surface temperature and the positive biases in downwelling shortwave radiation figs 2 3 s1 and s5 which should have led to unrealistically high evaporative demand of soil moisture limiting carbon uptake note that the positive biases in shortwave radiation could also be a reason for the early snowmelt and peak runoff reported by wang et al 2016 the simulations using narr mstmip led to agb in between the gridmet and cru ncep wang results figs 4 5 s10 and s11 an important reason for the improved performance relative to the wang datasets was that narr mstmip received both precipitation and downwelling shortwave radiation corrections text s1c whereas wang datasets received a precipitation correction only text s1b focusing on high elevation areas figs 2a d and s1 to s8 the precipitation and specific humidity were substantially higher in narr mstmip than in cru ncep while downwelling shortwave radiation was lower resulting in lower evaporative demand less water stress and increased plant productivity as supported by the clm sp results figs 7a c and s13 to s18 and also by the site level clm results at niwot ridge fig 8 showing higher β t r h l e a f and gpp for narr mstmip as expected for the high elevation regions the six clm 4 5 cn simulations performed better for gentle slopes fig s10 the greater underestimation of agb for moderate and steep slopes more evident for the cn cfsr merra era wang simulations was likely not only due to a reduced meteorological accuracy in those regions but also to structural limitations and parametric uncertainty in clm impacting the simulation of lateral flow snowpack and surface energy balance note that there is no representation of hillslope hydrology and sub grid topographical effects on solar radiation and precipitation in clm in addition to limitations in the definition of pfts and their parameterization overall the simulations performed worse for windward facing slopes sw w and nw than leeward facing slopes ne e and se fig s11 likely due to greater precipitation biases in the former group more evident for the cfsr merra era wang datasets see also fig s6 note that clm does not consider slope aspect directly but indirectly through its input datasets the persistent underestimation of agb across complex terrain is supported by the literature for example both the cmip5 taylor et al 2012 see ilamb https ilamb ornl gov cmpi5 and mstmip huntzinger et al 2013 2018 model inter comparison projects that included a wide range of earth system models coupled simulations and land surface models uncoupled simulations respectively generally underestimated biomass across the western united states our analysis that links biased meteorology enhanced water stress and limitation of vegetation productivity suggests that water carbon coupling is the source of this persistent underestimation in agb 4 2 impact of spatial resolution on model performance whereas the clm simulations of agb and carbon uptake were highly sensitive to the type of meteorological dataset the hires diagnostic simulations demonstrated clm was generally insensitive to changes in spatial resolution the more spatially resolved hires simulations 1 24 meteorology and land surface provided new information about biomass distribution at fine spatial scales fig 9 however the increased spatial resolution did not significantly influence the overall magnitude of biomass and carbon flux across the domain this finding directly addresses a critical question within the tbm community fan et al 2019 to what extent abiotic and biotic processes within complex terrain need to be resolved explicitly or parameterized implicitly to provide accurate simulations at larger domains in this case we show that the effects of finely distributed biotic and abiotic processes on the water carbon cycle tend to average out at coarser scales this result is somewhat surprising given the complexity of clm 4 5 and the non linear relationships that may exist between soil moisture evapotranspiration and processes that depend upon it baker et al 2017 we speculate that this could indicate limitations in the way that clm 4 5 represents subsurface hydrology that omits subsurface transfer of water between grid cells that convey soil moisture from uphill divergent locations to downhill convergent locations within the landscape swenson et al 2019 clm 4 5 also does not consider sub grid topographical effects on solar radiation and precipitation and lacks explicit representation of plant demographics including mortality recruitment and competition finally this analysis compared coarse 1 5 and fine 1 24 spatial resolution simulations yet not all land surface characteristics were available at the fine resolution table s1 to fully resolve the land surface characteristics at the mountain slope scale 10 km it is also worth noting that the 1 24 simulations finely resolved the distribution of pfts across the land surface but adopted the default broad pft definitions in clm as discussed in sect 4 3 higher pft species specificity was likely very important for removing the negative biases in agb within buotte et al 2019 our results align with those reported by slevin et al 2017 for a different tbm jules and coarser spatial resolutions surface and meteorological forcing data at 1 2 1 2 1 1 2 2 they found their gpp simulations to be globally and regionally insensitive to the changes in spatial resolution this is likely a limitation for most tbms which are primarily developed for global applications and lack the fine scale representation of features important for watersheds within complex terrain hao et al 2021 recently showed that the implementation of sub grid topographical effects on solar radiation in the e3sm land model resulted in improved simulations of surface energy balance snow cover and surface temperature over complex terrain in the tibetan plateau they demonstrated that the sub grid topographical effects cannot be neglected even at coarse spatial scales 2 we expect improvements in the simulation of carbon dynamics with this new model scheme but such impact still remains to be investigated the recent implementation of sub grid topographical effects on precipitation in e3sm tesfa et al 2020 is also promising but its impact on model simulations also remains to be investigated 4 3 further improving biomass simulations in the western united states with structural changes in clm improved meteorology by itself substantially reduced the underestimation in biomass for utah colorado sect 4 1 however the underestimation was completely eliminated only when combined with a more advanced representation of plant processes clm5 phs fig 5 although a full attribution of how all of the structural changes between clm 4 5 and 5 0 influenced the simulation is out of scope for this analysis sect 2 1 2 the fact that clm5 sms provided almost identical total biomass as compared to clm 4 5 whereas clm5 phs overestimated the biomass fig 5 strongly suggests plant water hydraulics kennedy et al 2019 lawrence et al 2018 played a major role soil moisture stress in the clm 4 5 and clm5 sms simulations were calculated using an empirical β t function eq 5 which is a common approach used in other tbms ibis foley et al 1996 jules clark et al 2011 sib3 baker et al 2008 however the simplicity of the β t function approach is subject to known limitations to simulating water carbon coupling contributing to unrealistic large scale drought response powell et al 2013 and large uncertainty in carbon cycle simulations trugman et al 2018 the clm5 phs formulation on the other hand provides a mechanistic approach based on plant hydraulic theory that explicitly tracks water flow through the soil roots and xylem in order to calculate a dynamic leaf water potential and stomatal conductance the relatively high amount of biomass for clm5 phs is consistent with the tendency for the plant hydraulics scheme to access deep water reserves and reduce water limitation overall kennedy et al 2019 the use of clm5 phs eliminated the underestimation of biomass for utah colorado however the accuracy of the simulation did not improve and the spatial distribution was worse than for clm 4 5 with biomass overestimation at the highest elevations and underestimation at lower elevations figs 4 5 and s9 this degraded performance with a more advanced and skillful tbm lawrence et al 2018 is not surprising given that out of the box parameterizations are designed to work globally but generally require tuning to function at a specific location or sub region the sensitivity of our results to the representation of plant hydraulic stress points to the need for hydraulic trait data to better parameterize tree species across the western united states anderegg 2015 anderegg and venturas 2020 the importance of species specific trait data related to water carbon coupling is supported by the literature for example improved simulations of forested biomass across the western united states were achieved by using gridmet meteorology in combination with a version of clm 4 5 that modified the default description of forest pfts from 6 generic temperate boreal forest pfts to 13 species specific pfts representing major forest types in the western united states buotte et al 2019 an important parameter change was the use of species specific trait data to assign the soil water potential at which stomata are fully closed with values changing from a range of 2 4 to 2 5 mpa default parameterization to a range of 2 7 to 8 6 mpa across major tree species buotte et al 2019 although buotte et al 2019 included other parameter changes combined with structural changes related to increased leaf shedding and fire module adjustments within their clm 4 5 model simulation the parameter adjustment specific to stomate opening likely provided a substantial impact in alleviating the biomass underestimation we recommend a similar approach to buotte et al 2019 be applied here yet focusing on species specific hydraulic traits within clm 5 0 4 4 relevance to other models and complex terrain regions the accuracy of meteorological data was crucial to our simulated vegetation in utah and colorado and we expect this to be true across the entire western united states similar biases in precipitation and shortwave radiation that limited the accuracy of the utah and colorado simulations are also found elsewhere in the western us for example cru ncep systematically underestimated precipitation across the western us as compared to gridmet fig s27 in general tbm simulations with insufficient precipitation in regions such as the western us where vegetation productivity is water limited berner et al 2017 hashimoto et al 2019 will lead to unrealistic low productivity and biomass we speculate that tbms exhibit less sensitivity to precipitation biases in the pacific northwest region where higher annual precipitation and lower summer temperatures tend to reduce the impact of water limitation upon plant growth hashimoto et al 2019 lower sensitivity of the pacific northwest to meteorological biases is supported by a meteorology inter comparison 15 products using the tbm lpj guess ahlström et al 2017 in that analysis the western us expressed the highest amount of variation in simulated ecosystem carbon coefficient of variation 50 as compared to anywhere else in the lower 48 an indication of divergence of meteorology fields within complex topography yet this high variation was limited primarily to the interior western us and not the pacific northwest we also expect the accuracy of meteorological data to be particularly important for carbon cycle simulations in complex terrain across the globe especially where the terrain coincides with water limitation hashimoto et al 2019 for example global simulations of ecosystem carbon with lpj guess and 15 meteorological datasets indicate greater uncertainty at higher elevations compared to surrounding regions ahlström et al 2017 overall the variation in global ecosystem carbon from the ahlström et al 2017 analysis accounted for about 40 of the range reported for 18 cmip5 earth system models for 1850 2100 historical period and rcp 8 5 scenario anav et al 2013 in another global study bonan et al 2019 varied model structure clm 4 0 4 5 and 5 0 and meteorological dataset cru ncep and gswp3 and found that meteorological data contributed to 70 95 of the overall uncertainty in global vegetation carbon simulations for the historical period 1850 2014 the exact contribution of complex terrain to these global results however remains to be determined in comparison with the ahlström et al 2017 and bonan et al 2019 uncertainty analyses our results show an even stronger impact of meteorological uncertainty on terrestrial carbon simulations 6 fold variation in agb in the western us it is important to keep in mind that these prior studies were based on global simulations with coarsely resolved meteorological products our current study on the other hand provides a more complete representation of meteorological uncertainty as it includes not only global coarsely resolved products cru ncep and wang products but also regional finely resolved products narr mstmip and gridmet the gridmet dataset in particular uses an algorithm specific to complex terrain and is originally available at very fine spatial resolution 1 24 5 conclusions our results highlight the critical role meteorological forcing plays on simulations of carbon fluxes and stocks across complex terrain our simulations of vegetation biomass in the southern rockies varied substantially as a function of the meteorological dataset with almost all results exhibiting underestimations in agb we found water stress exacerbated by negative biases in precipitation and positive biases in downwelling shortwave radiation to be a critical restriction on plant growth among all the meteorological datasets gridmet had the smallest precipitation and downwelling shortwave radiation biases when compared against tower observations at a high elevation site in the colorado rockies niwot ridge and reduced the vegetation biomass biases the most when compared against site measurements niwot ridge and a regional inventory remote sensing based biomass data product nbcd2000 importantly it was both the accuracy of the meteorological dataset combined with an improved representation of water stress plant hydraulics that eliminated the biomass underestimation for utah colorado however the use of a more advanced and skillful tbm clm 5 0 actually showed degraded performance in terms of overall bias and spatial distribution of biomass with biomass overestimation at the highest elevations and underestimation at lower elevations this suggests species specific hydraulic trait data should be included in future simulations whereas it is likely that developing meteorological datasets at fine spatial scales helps improve meteorology accuracy e g gridmet the added computational expense of the implementation of model simulations at high resolution was not warranted given the minimal changes to carbon water dynamics at larger spatial domains this finding should be revisited for model representations that include higher pft species specificity and take into account other potentially important non linear processes in tbms including explicit or implicit representation of hillslope hydrology topographical effects on solar radiation and precipitation and demographic processes including mortality competition and recruitment we expect our findings to be applicable for complex terrain across the globe especially in water limited regions although the gridmet meteorological dataset is not available outside of the conterminous united states we recommend a similar approach be used in the meteorological forcing development that accounts for fine spatial features in the terrain that best removes meteorological biases software and data availability clm is the land component of cesm which is sponsored by the national science foundation nsf and the u s department of energy doe and maintained by the climate and global dynamics laboratory at the national center for atmospheric research ncar in boulder co usa clm 4 5 and 5 0 source code and input datasets are freely available at http www cesm ucar edu models cesm1 2 and https github com escomp ctsm respectively niwot ridge ameriflux site data are available in blanken 1998 present nbcd 2000 data are available in kellndorfer et al 2013 cfsr wang era wang and merra wang data are available from wang et al 2016 narr mstmip data are available in wei et al 2014b cru ncep data are available in ucar 2016 three hourly gridmet data are available in rupp and buotte 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the u s doe office of science terrestrial ecosystem science program awards de sc0010624 and de sc0010625 and by the nasa cms project awards nnx16ap33g and 80nssc20k0010 we would like to thank the center for high performance computing at the university of utah we would like to also acknowledge high performance computing support from cheyenne https doi org 10 5065 d6rx99hx provided by ncar s computational and information systems laboratory sponsored by nsf through allocation awards uusl0005 and uusl0007 funding for the ameriflux core site data us nr1 niwot ridge pi peter blanken was provided by the u s doe office of science funding for the nbcd 2000 project was provided by nasa s terrestrial ecology program award nng05g127g the development of the atmospheric forcing datasets here referred to as cfsr wang era wang and merra wang wang et al 2016 was supported by the national science foundation of china awards 41275110 and 41405087 and nsf award ags 0944101 funding for the multi scale synthesis and terrestrial model intercomparison project mstmip https nacp ornl gov mstmip shtml activity was provided through nasa roses award nnx10ag01a data management support for preparing documenting and distributing model driver and output data was performed by the modeling and synthesis thematic data center at oak ridge national laboratory ornl http nacp ornl gov with funding through nasa roses award nnh10an681 we thank peter blanken and sean burns for the us nr1 niwot ridge ameriflux data supported by the u s doe office of science through the ameriflux management project at lawrence berkeley national laboratory under award 7094866 we thank david rupp for disaggregating the daily gridmet data used here and in buotte et al 2019 we also thank anna trugman and bill anderegg for helpful discussions on this work we gratefully acknowledge the comments and suggestions of two anonymous reviewers which have greatly improved our paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105288 
