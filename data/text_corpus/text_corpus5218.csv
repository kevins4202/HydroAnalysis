index,text
26090,air pollution problems have a severe effect on the natural environment and public health the application of machine learning to air pollutant data can result in a better understanding of environmental quality of these methods the deep learning method has proven to be a very efficient and accurate method to forecast complex air quality data this paper proposes a deep learning model based on an auto encoder and bidirectional long short term memory bi lstm to forecast pm2 5 concentrations to reveal the correlation between pm2 5 and multiple climate variables the model comprises several aspects including data preprocessing auto encoder layer and bi lstm layer the performance of the proposed model was verified based on a real world air pollution dataset and the results indicated this model can improve the prediction accuracy in an experimental scenario keywords deep learning auto encoder bi lstm data preprocessing pm2 5 concentration prediction air pollution 1 introduction currently many developing countries are still suffering from air pollution problems air pollution can cause many serious health issues such as respiratory disease cardiovascular disease and reduced lung function lim et al 2012 kappos et al 2004 therefore controlling air pollution has attracted widespread public attention to reduce the damages made by air pollutants the concentration prediction method that concerns intelligent data analysis approaches has obtained increasing attention in past decades carslaw and karl 2012 johansson et al 2015 pm2 5 particulate matter with an aerodynamic diameter of less than 2 5 μm is one of the most dangerous air pollutants because it causes the most adverse effects on public health and can spread through the air kampa and elias 2008 thus the pm2 5 concentration problem is a global issue that crosses geographical boundaries an interdisciplinary approach is needed to solve it and both industries and governments should play an active role for example the prediction of pm2 5 concentrations in areas within the subway network can protect the respiratory health of subway workers and passengers from air pollutants kim et al 2008 the effective control of pm2 5 cannot only avoid social economic losses but also improve the air quality and protect people s health therefore an accurate pm2 5 prediction can significantly help in providing prompt and complete environmental quality information allowing the government to take timely action for the environment pm2 5 concentration prediction is a time series problem traditional time series forecasting methods used one dimensional time series data and analyzed data trends based on sequential variations for example the autoregressive moving average arma method used univariate time series data only pm2 5 for air pollution forecasting without considering the influence of other meteorological variables on pm2 5 hannan and kavalieris 1984 however in the field of environmental pollution prediction it is well acknowledged that the pm2 5 concentration is affected by many factors such as wind speed rainfall snow and other air pollutants all these factors also influence each other degaetano and doherty 2004 so et al 2014 the prediction process typically requires considering variables that are stochastically dependent or assessed according to different indicators therefore researchers must identify the cross influences between multiple climate variables and air pollutants to fully extract and learn their correlated features for pm2 5 prediction the above challenge motivates us to shed light on air pollution prediction difficulties in this work some traditional pm2 5 prediction methods focused on statistic models and classification techniques kiesewetter et al 2015 lu and wang 2005 however remote sensor technology has been widely used and the amount of sensor aware data has increased dramatically traditional methods lack the ability to process a large amount of multidimensional nonlinear data and analysis of the correlation features between pm2 5 concentration and other climate variables is difficult for these methods recently deep learning methods have been successfully applied in image processing natural language processing and object detection he et al 2016 collobert and weston 2008 these methods show great advantages in processing large complexity data via two ways first the deep learning method can extract and analyze the correlation between multiple variables e g auto encoder network which can improve the prediction accuracy second the deep learning method is more efficient to process a large volume of datasets as a time series problem e g long short term memory network lstm for example in huang and kuo 2018 the authors proposed an apnet model based on lstm for particulate matter forecasting in smart cities therefore the deep learning method is an appropriate approach for our work in this work our ultimate goal is to construct a pm2 5 concentration prediction model with improved accuracy over currently employed prediction models using real world meteorological data here the pollen counts traffic counts and industrial emissions are not considered given the limited data source the effectiveness of meteorological data impacting on air pollution prediction can be verified in akyüz and cabuk 2009 detersjanet al 2017 sun and sun 2017 our proposed model also aims to determine which meteorological variables have a greater impact on pm2 5 prediction by learning the correlations among meteorological and pm2 5 variables to achieve this goal we introduce a deep learning method specifically designed for pm2 5 prediction namely an auto encoder and bidirectional lstm model ae bi lstm which has the capability to extract features between the meteorological variables and pm2 5 concentration and then predict pm2 5 values in time series the model is composed of two layers the auto encoder network layer ae layer and the bi lstm layer the rationales of the proposed model are as follows 1 there are some deficiencies of implicit feature extraction in the data analysis phase in most traditional statistical methods and physical simulation models mckeen et al 2007 liu et al 2004 in these models the original data were directly used as input for prediction without considering the data features between multiple variables therefore extracting data features can promote prediction accuracy the auto encoder network exhibits good performance in extracting potential data features and reducing data volumes lange and riedmiller 2010 therefore the ae layer in our model aiming to extract the potential and implicit deep features can improve prediction performance in time series problems 2 lstm is a type of time recursive neural network that is suitable for processing and predicting important events with relatively long intervals and delays in time series data sundermeyer et al 2012 furthermore the bidirectional lstm connects two hidden layers that run in two directions between the input and output therefore the bi lstm layer is proposed to take the extracted features from the ae layer as input for predicting air pollutant concentration in a time series format the bi lstm based structure also allows the training of the prediction model to use both the future features and the past features for a specific time range efficiently which improves the prediction accuracy to a certain extent thus the bi lstm layer can effectively handle the time series problem and improve the prediction accuracy thus the ae layer of our proposed structure can effectively learn features from data and reduce the data dimension while the bi lstm layer can improve the prediction accuracy therefore our work aims at providing a novel model for pm2 5 concentration prediction namely ae bi lstm model by learning the deep features between pm2 5 concentration and climate variables the main contributions are summarized as follows 1 the deep feature extraction is considered in our work to measure the dependent relationship between climate variables and pm2 5 concentration data the proposed ae layer of our model tends to learn and compress the input data to reduce the complexity while the outputs of the ae layer are taken as input data to the bi lstm layer for training 2 both the historical and future context sequences are considered to predict the time series pm2 5 concentration value in the bi lstm layer which is capable of learning long term dependencies without keeping redundant context information 3 to improve the availability of the proposed model we propose a preprocessing phase that can analyze the correlation between air pollutant and climate variables this phase is essential for evaluating the impacts of climate variables on pm2 5 forecasting in this paper the remainder of the paper is organized as follows section 2 reviews literature that is related to this work section 3 describes a problem modeling of pm2 5 prediction section 4 introduces the proposed model based on auto encoder and bi lstm section 5 presents the experimental results section 6 summarizes the results and offers suggestions for future research 2 related work according to the characteristics of air pollutant data air pollutant prediction methods contain traditional prediction methods and deep learning methods the traditional prediction methods combine the studies of meteorology environmental science mathematics and computer science these methods can be divided into numerical prediction solazzo et al 2009 statistical prediction brooks et al 2016 empirical model based prediction and traditional machine learning based prediction xi et al 2015 karaca et al 2006 based on mathematical methods numerical prediction can establish a numerical model for the dilution and diffusion of air pollution and then utilize high speed computing power to predict the dynamic changes of air pollutant concentrations such as the urban airshed model uam and the community multiscale air quality cmaq model scheffe and morris 1993 liu et al 2010 however numerical prediction methods are computationally intensive and cannot deal with the severe pollution conditions in urban areas statistical prediction methods analyze the statistical regularity of the input and output of air pollutants and then predict the future pollution trend such as time series data analysis taylor et al 2007 multiple regression model cardelino et al 2001 and neural network model viotti et al 2002 however the accuracy of statistical prediction methods is relatively low traditional machine learning based prediction methods use only one type of contaminant as the data source and ignore the relationship between various climate variables which makes the prediction inaccurate recently deep learning methods have attracted significant attention and achieved revolutionary successes in various applications guo et al 2016 therefore many researchers have utilized deep learning methods to predict pollutant concentrations for example kuremoto combined a deep belief network with restricted boltzmann machines for time series forecasting kuremoto et al 2014 and bt ong introduced a deep recurrent neural network to predict environmental monitoring data ong et al 2014 moreover as the data dimension increases it becomes more important to describe data efficiently with machine learning and big data techniques a method proposed by hinton et al efficiently used lower dimensional data to represent high dimensional data by means of neural networks hinton and salakhutdinov 2006 bengio et al achieved better results by using auto encoder to pre train deep networks schölkopf et al 2007 all of the previous works indicate that auto encoder has the ability to extract data features and can be applied to machine learning tasks additionally auto encoder can adaptively extract images inherent features almousli and vincent 2013 thus auto encoder can be considered as an encoder this motivated auto encoder researchers to improve feature generalization and to allow auto encoder to encode data in any type of data structures krizhevsky et al used auto encoder to encode images and then utilized the obtained code as an index to retrieve images from a database achieving faster query speed krizhevsky and hinton 2011 long short term memory lstm has recently shown great promise in tackling various sequence modeling tasks in machine learning such as image segmentation stollenga et al 2015 and activity recognition qin et al 2017 however the lstm technique only takes information from the past without any information from the future an elegant effect solution as demonstrated by previous work is the bi directional lstm bi lstm method lin et al 2017 the basic idea is to present each sequence forward and backward to two separate hidden states that can capture past and future information respectively then the two hidden lstm and lstm can be considered as the lstm unit the unit is capable of learning long term dependencies without retaining redundant context information and works tremendously well on deep learning such as natural language processing xu et al 2017 and speech recognition tasks mohamed et al 2015 the research in this study builds on the existing literature focused on air pollutant prediction we proposed an auto encoder based method that can extract data features and compress data thus reducing data volume to increase the training efficiency the bi lstm method is utilized to effectively handle the time series problem predict the future data based on existing data and improve the prediction accuracy moreover we proposed a preprocessing method that can confirm the impact of weather variables on pm2 5 forecast 3 problem scenario the previous pollutant prediction studies used a single site dataset without considering the influence of surrounding sites and other climate variables which cannot predict future pollutant concentrations accurately to predict pm2 5 pollutant concentrations in regional areas this study collected pollutant data in a certain area of beijing and took into account of the impact of other climate variables the collected pollutant data mainly contain the hourly records of particulate matter pm2 5 and climate variables including dewp dew point temp air temperature rain rainfall snow snowfall and wdsp wind direction speed since the data are relatively large and noisy we need to denoise and compress it to a certain extent mainly by extracting the implicit features from the data therefore an appropriate mapping relationship must be identified to extract the features from the input data this mapping relationship is shown in eq 1 1 φ χ f where φ is defined as a transition x denotes the input data and f denotes the data after feature extraction to ensure the accuracy of feature extraction we need to construct another mapping relationship to convert f into x where the difference between x and x needs to be as small as possible then an error function such as squared errors is constructed to be minimized in the training process at the end of this process f is the extracted data feature that we need the detailed process is described as follows 2 ψ f x 3 φ ψ a r g m i n x x 2 φ ψ where ψ is defined as a transition and x denotes the reconstruction of the input x since our goal is to predict air pollutants we need to analyze the past data to predict future values after extracting data features we need to find a mapping relationship which can learn the internal relationship of data and predict the value of pollutants in the future this mapping relationship is shown in eq 4 there are some intermediate layers hidden layers between mappings which are intended to learn the inherent relationship from the dataset better 4 h x y to make a more accurate prediction as the amount of time series data increases we need to develop a function that can selectively forget some unnecessary data in such a mapping relationship to achieve a better prediction here we consider a two dimensional matrix x i j where the i th dimension represents a set of attributes including the air pollutant and climate variables and the j th dimension represents an ordered set of timestamps t t 1 t 2 t d where t 1 t 2 t d for example in the following matrix x t 1 1 represents the value of pm2 5 concentration at timestamp t 1 5 x p m 2 5 d e w p t e m p r a i n x t 1 1 x t 1 2 x t 1 3 x t 1 n x t 2 1 x t d 1 x t 2 2 x t d 2 x t 2 3 x t d 3 x t 2 n x t d n the first objective of this approach is to filter x i j to x i j which can select attributes that related to air pollution a detailed explanation of the proposed method is introduced in sect 4 1 then given x i j the next objective is to reduce data dimensions and extract implicit features y i j by the auto encoder layer finally using the data features y i j as input a prediction layer based on bi directional lstm is trained to predict the air pollutant concentrations 4 methods our proposed approach is shown in fig 1 it includes two parts the first part is to preprocess data which can build climate attributes combinations related to air pollution given the selected attributes the second part can train the prediction model of air pollutant concentration using the auto encoder and bi lstm based models auto encoders ae layer are a type of neural network that aims to perform dimensionality reduction and data denoising applying ae in pattern recognition liu et al 2017 fault diagnosis shao et al 2017 and feature learning meng et al 2017 achieved good results the ae layer can learn to compress original data from the input layer generate a representation encoding of the original data and then decompress decoding the representation into something that closely matches the original data in this research we proposed to use auto encoder to extract implicit data features from the multiple variables related to pm2 5 prediction the bi lstm neural network is well suited for processing classification and prediction tasks on time series data since information about the past and the future in time series data can play both important roles in prediction bi lstm is capable of learning long term dependencies without retaining redundant context information from both the past and the future previous research on sentiment analysis chen et al 2017 classification zhao et al 2017 speech recognition ogawa and hori 2017 and biomedicine tutubalina et al 2018 has proven that the bi lstm method can improve the time series prediction therefore we proposed to use the bi lstm based layer for air pollutant concentration prediction moreover the combination of auto encoder and bi lstm models can reduce the training time and improve the prediction accuracy 4 1 preprocessing due to the relatively small numbers of data samples the prediction model may overfit during the training process then the prediction results will be inaccurate therefore the preprocessing methods can work on data samples to solve the data imbalance problem and then improve the prediction results in this approach we proposed a preprocessing method that can select attributes from the air contaminant and climate variables related to air pollution prediction given a two dimensional array x i j this process can retrieve x i j which contains the corresponding air contaminant and climate variables the preprocessing step is calculated as follows 6 x x β β 1 β 2 β n where x denotes the input matrix the β i for each attribute is a one vector or zero vector based on whether it is related to air pollutant concentration predictions and denotes a point wise hadamard multiplication operator thus by controlling β the targeted variables can be selected and β is obtained by manually initialization we simplified the preprocessing process in fig 2 for a better presentation in this figure the left side presents the vector representations of the input data whereby each row represents one variable after the preprocessing step these vectors become vector combinations where β i is a one vector or zero vector for each combination the right side shows the time series plots of different vector combinations as the first step in our approach the result of the preprocessing step will be the input data for the following prediction models 4 2 ae bi lstm model the proposed prediction model consists of two layers the auto encoder layer and the bi lstm layer taken the preprocessed data as input the first part can reduce data dimensions and extract implicit features by using the auto encoder layer then given data features the second part can train a bi directional neural network layer based on long short term memory lstm architecture named as bi lstm to predict air pollutant concentrations the auto encoder layer can learn data structures adaptively and then represent the complex data in an efficient manner these properties make auto encoder not only well suited for large volumes of datasets but also overcome the expensive designing cost and the traditional poor generalizations moreover use of the auto encoder technique in deep learning models can extract implicit features and yield better prediction results the auto encoder layer shown in fig 3 contains two processes encoder and decoder in our approach only the output of the encoder phase is used as input in the following bi lstm layer in general the auto encoder layer can extract implicit data features and reduce data size which can accelerate the training process in the following steps and subsequently improve the prediction accuracy the auto encoder layer transforms input vector x to y via the encoder f and attempts to reconstruct x via the decoder g to produce x the reconstruction error is measured by the loss function l o s s x x in addition y can be used as input in the bi lstm layer for training in the encoder process y i j σ w x i j b is constructed where x i j is the preprocessed data σ is an relu activation function w is a weighted matrix and b is a bias vector this process can reduce the dimension of the input data in the decoder process y i j is used as input and x i j is constructed by x i j σ w y i j b where σ w and b are the relu activation function weighted matrix and bias vector respectively however σ w and b differ from σ w and b the decoder process can decompress y i j to x i j thus x and x have the same dimensions to train the auto encoder layer the reconstruction error needs to be minimized the reconstruction error is calculated as follows 7 ε x x x x 2 x σ w σ w x i j b b 2 in general the auto encoder layer can reconstruct x to x and make the difference between x and x as small as possible however in our proposed approach only the hidden layer y i j is used as the input for the bi lstm layer since it is a compressed representation of x i j in the following section a prediction layer based on long short term memory lstm is generated the lstm has a similar function as the recurrent neural network rnn model to represent dynamic temporal behavior for sequential data however the hidden layer in ltsm is replaced by a long short term memory cell the lstm cell can solve the long term dependencies and the vanishing gradients problems compared with traditional rnn models the long term dependency problem means that as the time interval increases the learned information cannot be connected to the far reaching information leading to vanishing gradients for most prediction tasks it is beneficial to have both past and future contexts for a given time range however the lstms hidden layer can only take features from the past therefore the bi directional lstm bi lstm layer is developed it can train the prediction layer using both the future features and the past features efficiently which can improve the prediction accuracy to a certain extent the bi lstm layer is shown in fig 4 one lstm unit is composed of three gates including an input gate an output gate and a forget gate these gate signals include the logistic nonlinearity σ the input gate is used to control how much it should read from its input in our model the extracted features y t t 1 t d from the ae layer in d hours before timestamp t are used as the input of bilstm and the goal is to predict the concentration of pm2 5 n hour after timestamp t both d and n are predefined time windows this input is implemented by the following formula 8 i t σ u i h t 1 w i x t b i c t f t c t 1 i t tanh u c h t 1 w c x t b c where σ is an element wise sigmoid function i f and c represent for the input gate forget gate and cell vectors respectively h t is the hidden state vector the t for the subscript of each symbol indicates the time step u i and u c denote the weight matrices for hidden state h t w i and w c denote the weight matrices of different gates for input x t b i and b c denote the bias vectors and c t denotes cell state vector the forget gate is used to control how much to forget from the current cell value in this paper the input gate is a vector that is the implicit data features extracted from the auto encoder the forget gate can selectively forget some data information about the data features in the past and is implemented by the following formula 9 f t σ u f h t 1 w f x t b f where σ is an element wise sigmoid function h t 1 is the hidden state vector the t for the subscript of each symbol indicates the time step u f denotes the weight matrices for hidden state h t w f denotes the weight matrices of different gates for input x t and b f denotes the bias vectors the output gate determines the final output information which is the predicted pm2 5 concentration value and the hidden state vector h t at the next moment the output is implemented by the following formula 10 o t σ u o h t 1 w o x t b o h t o t t a n h c t where σ is an element wise sigmoid function u o denotes the weight matrices for the hidden state h t w o denotes the weight matrices of different gates for input x t and b o denotes the bias vectors furthermore in our proposed architecture bi lstm is used instead of lstm because the bidirectional lstm can obtain a better understanding of the time series data in two directions the past time series data can influence the current prediction and the future time series data will also affect the current prediction to a certain extent learning features from both the past and future data can make a more accurate prediction of air pollution concentration then the trained parameters of the bi lstm model in the training process can be used in prediction for the information recorded in the last forward vector lstm is enhanced from front to back for the information recorded in the last backward vector lstm is enhanced from back to front the combination of the two specific records can complete the information therefore more accurate results can be predicted fig 5 shows an easy way to understand this prediction here a period of pm2 5 time series data is selected as a demonstration where the forward lstm inputs are from t 1 to t 2 t and the backward lstm inputs are from t 2 t to t 1 which will determine the prediction output y t as shown in the example the combination of forward lstm and backward lstm enables more accurate prediction and will cause less prediction error than using the one way lstm 5 experimental results 5 1 dataset to demonstrate the efficacy of the prediction model proposed in this paper experiments were performed to predict pm2 5 concentration in beijing the dataset consisted of a series of hourly values from january 2 2010 to december 31 2014 and contained a number of variables including pm2 5 concentration pm2 5 air temperature temp dew point temperature dwep wind direction speed wdsp rainfall rain and snowfall snow table 1 provides detailed information about the input variables and their units fig 6 shows the time series representation of the input variables the x axis in these figures represents the hourly timestamp from january 2 2010 to december 31 2014 with 43825 h in total and the y axis represents the value of these variables camx hsu and prather 2010 cmaq chen et al 2014 naqpms wang et al 2001 and wrfchem saide et al 2011 models were used for comparison with traditional models these methods are classical air quality models that predict air pollutants by simulating air pollutants svm rnn lstm ae with rnn and ae with lstm were implemented for comparison with machine learning methods all of these methods used data from the previous 72 h to predict the pm2 5 concentration in the future and the time windows were set in the model parameters the model parameter n timesteps represents using the data of the past n hours and n outputs represents predicting air pollution concentration values of the following n hours in the experiment 80 of the samples in the dataset were used for training 10 for validation and 10 for testing the distribution of the dataset is shown in table 2 the training dataset is used to train the model parameters the model continually trains and tunes and then optimizes the performance of the model gradually by reducing the errors the validation dataset is used to adjust the parameters and to evaluate the generalization capability of the model if the model performs worse on the validation set compared with the training set then the over fitting phenomenon occurs the ae layer prevents over fitting through weight penalty while the bi lstm layer prevents over fitting through the dropout parameter the testing set is used to measure the effectiveness of the model 6 results the objective of the experiments presented in this paper is to evaluate the proposed prediction model with this in mind first we compared the proposed model with four traditional prediction models namely camx cmaq naqpms and wrfchem second the svm rnn lstm ae with rnn ae with lstm models were chosen for comparative experiments with some machine learning method in svm x t d t 2 t 1 t was used as the model input and x t 1 was the output root mean squared error rmse is used as an evaluation measurement which is defined as 11 r m s e i 1 d y i p i 2 d where y i is the true value of pm2 5 and p i is the predicted value therefore smaller rmse values indicate increased accuracy of the prediction model before training the prediction models preprocessing was performed to select climate variables as explained in section 3 3 1 the selected variables were pm2 5 with temp pm2 5 with dwsp pm2 5 with wdsp pm2 5 with rain pm2 5 with snow and pm2 5 with wdsp rain pm2 5 wdsp and snow then the selected dataset was normalized to 0 1 by eq 12 finally the normalized dataset was used as input to the prediction model 12 n e w v a r i a b l e v a r i a b l e m i n v a r i a b l e m a x v a r i a b l e m i n v a r i a b l e in the training process we performed 300 iterations for the auto encoder and bi lstm model each for the auto encoder layer the stochastic gradient descent technique was used for training while the adam training technique was used in the bi lstm layer furthermore the number of batches was set to 200 the number of lstm unit was set to 50 and the learning rate value was 1 e 3 in the bi lstm layer table 3 shows the detailed model parameter settings table 4 shows the average rmse results for the four traditional models including camx cmaq naqpms and wrfchem based on the air pollution data from beijing the cmaq model had the best performance compared with camx naqpms and wrfchem models however the rmse of cmaq was as great as 36 fig 7 shows the comparisons between the machine learning models the rmse result is shown on the upper left side of each sub figure for all the machine learning methods the predicted values shown in scattered dots were around the straight line which represented actual values rmse results from all machine learning models were less than 20 the n timesteps is set to 72 and n outputs is set to 24 in general the machine learning models performed better than the traditional models however the svm method had the worst results among the six machine learning methods and the rmse was 19 529 rmse results of the rnn lstm ae and rnn ae and lstm and our proposed model were relatively reduced 8 641 6 248 7 539 4 744 and 3 091 respectively it was evident that our proposed model performed the best and the predicted value matched well with the actual value to further compare the prediction effect of the proposed method with other machine learning methods in time series data fig 8 shows the predicted value of each model and the actual observed pm2 5 value over continuous 1000 h from the testing dataset the x axis in the figure represents the timestamp and the y axis represents the pm2 5 concentration the color bar is shown on the left and the red line indicates the real observed value similar to fig 7 the predicted data of the svm method indicated as the light blue line reveals larger differences compared with the real data our proposed ae and bi lstm model indicated as the dark blue line is still the closest to the real data in addition to the above experimental results we also verified the correlation between pm2 5 and other climate variables in this experiment we trained the prediction models using pm2 5 alone pm2 5 with air temperature pm2 5 with dew point temperature pm2 5 with wind direction speed pm2 5 with rainfall pm 2 5 with snowfall pm2 5 with rainfall and wind speed and pm 2 5 with snowfall and wind speed according to the results in fig 7 the most relevant four models namely rnn lstm ae lstm and ae bilstm were chosen for performance comparisons the comparison results that demonstrate the rmse trends are shown in fig 9 in fig 9 the x axis indicates the time range to predict the air pollutant concentration increasing from 24 to 168 h which is the n outputs in the parameter setting in addition the y axis means the root mean squared error rmse the rnn model performs the worst whereas the ae bilstm model has the best prediction result the addition of an ae layer that extracts data features can improve the performance prominently when comparing the lstm model with the ae lstm model among these four models the prediction error using pm2 5 with temp and pm2 5 with dewp shown in blue and green lines respectively were relatively increased compared with the use of pm2 5 alone at any time however when the prediction time range was less than 120 h the combinations that include wdsp rain snow wdsp with rain and wdsp with snow can produce better results than using pm2 5 alone in our proposed ae bilstm model using pm2 5 with rain snow wdsp and rain as well as wdsp with snow perform best when predicting 24 48 and 72 h values however when predicting 96 120 and 144 h concentration values the errors of pm2 5 with rain and snow were similar and both were less than the other combinations in general our proposed model can achieve the best performance compared with the other three models to verify the correlation between pm2 5 and other climate variables more specifically rmse results using different climate variable combinations of our proposed model that increase the prediction time range from 24 to 168 are shown in table 5 along with another measurement namely correlation coefficient the value of correlation coefficient ranges from 1 to 1 this measurement can represent a statistical relationship between two variables specifically 1 indicates the strongest positive correlation and 1 indicates the strongest negative correlation correlation coefficient results show that all of the climate variables in our experiment are related to the pm2 5 prediction in a positive manner in addition the rainfall and snowfall are the most relevant single variables combining rainfall with wind speed and combing snowfall with wind speed have the greatest effects on pm2 5 prediction thus the use of pm2 5 with rain snow wdsp and rain as well as wdsp with snow can achieve the best prediction results in addition to predict 72 h pm2 5 concentration values the best result is obtained by combining pm2 5 with rainfall and wind speed the prediction using pm2 5 with wdsp rain snow wdsp and rain wdsp and snow were better than using pm2 5 alone however to predict 168 h pm2 5 concentration values it was found that using pm2 5 alone was better than combining pm2 5 with any other meteorological variable the reason is that the meteorological variables themselves are relatively accurate in 72 h which can enhance the pm2 5 prediction accuracy however the meteorological variables generate more uncertain effects over 72 h which increases the pm2 5 prediction difficulty and reduces the accuracy as a result the meteorological variables become interference factors when predicting more than 72 h pm2 5 concentration values 7 conclusion in this paper we proposed a novel prediction model to forecast pm2 5 concentrations in this model a preprocessing method is introduced to select appropriate attributes from the air containment and climate variables related to air pollution prediction such as the combination of pm2 5 and weather variables this phase is proved to be essential for evaluating the impacts of climate variables on pm2 5 forecasting then the preprocessed data are taken as input to an auto encoder layer which can extract implicit data features from a large volume dataset and improve the training efficiency the results shown in figs 7 9 proved that using the ae layer can achieve a better result finally the output of auto encoder is used as data input to train the bi lstm layer the bi lstm layer can effectively handle the time series data and improve prediction accuracy the experimental results showed our proposed model can achieve better prediction accuracy than the camx naopms and mrfchem models and several machine learning methods in addition we found a strong positive correlation between the climate variable rainfall and wind speed with pm2 5 concentration for future work there are a number of ways in which this research could be extended first the dataset in this study is limited by the data source given that pollution and meteorological data are exclusively considered data on pollen counts traffic counts vehicle emissions and industrial emissions that are more directly linked to pm2 5 could be considered in the future second the model can be extended to add datasets from multiple sites which can consider air pollutant interactions among different locations finally one important extension is to use images to predict air pollutant and to trace contamination diffusion paths declaration of competing interest the authors declared that they have no conflicts of interest to this work entitled constructing a pm2 5 concentration prediction model by combining auto encoder with bi lstm neural networks acknowledgements this work is funded by national natural science foundation of china 61572326 61802258 61702333 natural science foundation of shanghai 18zr1428300 the shanghai committee of science and technology 17070502800 and the shanghai sailing program grant no 19yf1436900 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104600 
26090,air pollution problems have a severe effect on the natural environment and public health the application of machine learning to air pollutant data can result in a better understanding of environmental quality of these methods the deep learning method has proven to be a very efficient and accurate method to forecast complex air quality data this paper proposes a deep learning model based on an auto encoder and bidirectional long short term memory bi lstm to forecast pm2 5 concentrations to reveal the correlation between pm2 5 and multiple climate variables the model comprises several aspects including data preprocessing auto encoder layer and bi lstm layer the performance of the proposed model was verified based on a real world air pollution dataset and the results indicated this model can improve the prediction accuracy in an experimental scenario keywords deep learning auto encoder bi lstm data preprocessing pm2 5 concentration prediction air pollution 1 introduction currently many developing countries are still suffering from air pollution problems air pollution can cause many serious health issues such as respiratory disease cardiovascular disease and reduced lung function lim et al 2012 kappos et al 2004 therefore controlling air pollution has attracted widespread public attention to reduce the damages made by air pollutants the concentration prediction method that concerns intelligent data analysis approaches has obtained increasing attention in past decades carslaw and karl 2012 johansson et al 2015 pm2 5 particulate matter with an aerodynamic diameter of less than 2 5 μm is one of the most dangerous air pollutants because it causes the most adverse effects on public health and can spread through the air kampa and elias 2008 thus the pm2 5 concentration problem is a global issue that crosses geographical boundaries an interdisciplinary approach is needed to solve it and both industries and governments should play an active role for example the prediction of pm2 5 concentrations in areas within the subway network can protect the respiratory health of subway workers and passengers from air pollutants kim et al 2008 the effective control of pm2 5 cannot only avoid social economic losses but also improve the air quality and protect people s health therefore an accurate pm2 5 prediction can significantly help in providing prompt and complete environmental quality information allowing the government to take timely action for the environment pm2 5 concentration prediction is a time series problem traditional time series forecasting methods used one dimensional time series data and analyzed data trends based on sequential variations for example the autoregressive moving average arma method used univariate time series data only pm2 5 for air pollution forecasting without considering the influence of other meteorological variables on pm2 5 hannan and kavalieris 1984 however in the field of environmental pollution prediction it is well acknowledged that the pm2 5 concentration is affected by many factors such as wind speed rainfall snow and other air pollutants all these factors also influence each other degaetano and doherty 2004 so et al 2014 the prediction process typically requires considering variables that are stochastically dependent or assessed according to different indicators therefore researchers must identify the cross influences between multiple climate variables and air pollutants to fully extract and learn their correlated features for pm2 5 prediction the above challenge motivates us to shed light on air pollution prediction difficulties in this work some traditional pm2 5 prediction methods focused on statistic models and classification techniques kiesewetter et al 2015 lu and wang 2005 however remote sensor technology has been widely used and the amount of sensor aware data has increased dramatically traditional methods lack the ability to process a large amount of multidimensional nonlinear data and analysis of the correlation features between pm2 5 concentration and other climate variables is difficult for these methods recently deep learning methods have been successfully applied in image processing natural language processing and object detection he et al 2016 collobert and weston 2008 these methods show great advantages in processing large complexity data via two ways first the deep learning method can extract and analyze the correlation between multiple variables e g auto encoder network which can improve the prediction accuracy second the deep learning method is more efficient to process a large volume of datasets as a time series problem e g long short term memory network lstm for example in huang and kuo 2018 the authors proposed an apnet model based on lstm for particulate matter forecasting in smart cities therefore the deep learning method is an appropriate approach for our work in this work our ultimate goal is to construct a pm2 5 concentration prediction model with improved accuracy over currently employed prediction models using real world meteorological data here the pollen counts traffic counts and industrial emissions are not considered given the limited data source the effectiveness of meteorological data impacting on air pollution prediction can be verified in akyüz and cabuk 2009 detersjanet al 2017 sun and sun 2017 our proposed model also aims to determine which meteorological variables have a greater impact on pm2 5 prediction by learning the correlations among meteorological and pm2 5 variables to achieve this goal we introduce a deep learning method specifically designed for pm2 5 prediction namely an auto encoder and bidirectional lstm model ae bi lstm which has the capability to extract features between the meteorological variables and pm2 5 concentration and then predict pm2 5 values in time series the model is composed of two layers the auto encoder network layer ae layer and the bi lstm layer the rationales of the proposed model are as follows 1 there are some deficiencies of implicit feature extraction in the data analysis phase in most traditional statistical methods and physical simulation models mckeen et al 2007 liu et al 2004 in these models the original data were directly used as input for prediction without considering the data features between multiple variables therefore extracting data features can promote prediction accuracy the auto encoder network exhibits good performance in extracting potential data features and reducing data volumes lange and riedmiller 2010 therefore the ae layer in our model aiming to extract the potential and implicit deep features can improve prediction performance in time series problems 2 lstm is a type of time recursive neural network that is suitable for processing and predicting important events with relatively long intervals and delays in time series data sundermeyer et al 2012 furthermore the bidirectional lstm connects two hidden layers that run in two directions between the input and output therefore the bi lstm layer is proposed to take the extracted features from the ae layer as input for predicting air pollutant concentration in a time series format the bi lstm based structure also allows the training of the prediction model to use both the future features and the past features for a specific time range efficiently which improves the prediction accuracy to a certain extent thus the bi lstm layer can effectively handle the time series problem and improve the prediction accuracy thus the ae layer of our proposed structure can effectively learn features from data and reduce the data dimension while the bi lstm layer can improve the prediction accuracy therefore our work aims at providing a novel model for pm2 5 concentration prediction namely ae bi lstm model by learning the deep features between pm2 5 concentration and climate variables the main contributions are summarized as follows 1 the deep feature extraction is considered in our work to measure the dependent relationship between climate variables and pm2 5 concentration data the proposed ae layer of our model tends to learn and compress the input data to reduce the complexity while the outputs of the ae layer are taken as input data to the bi lstm layer for training 2 both the historical and future context sequences are considered to predict the time series pm2 5 concentration value in the bi lstm layer which is capable of learning long term dependencies without keeping redundant context information 3 to improve the availability of the proposed model we propose a preprocessing phase that can analyze the correlation between air pollutant and climate variables this phase is essential for evaluating the impacts of climate variables on pm2 5 forecasting in this paper the remainder of the paper is organized as follows section 2 reviews literature that is related to this work section 3 describes a problem modeling of pm2 5 prediction section 4 introduces the proposed model based on auto encoder and bi lstm section 5 presents the experimental results section 6 summarizes the results and offers suggestions for future research 2 related work according to the characteristics of air pollutant data air pollutant prediction methods contain traditional prediction methods and deep learning methods the traditional prediction methods combine the studies of meteorology environmental science mathematics and computer science these methods can be divided into numerical prediction solazzo et al 2009 statistical prediction brooks et al 2016 empirical model based prediction and traditional machine learning based prediction xi et al 2015 karaca et al 2006 based on mathematical methods numerical prediction can establish a numerical model for the dilution and diffusion of air pollution and then utilize high speed computing power to predict the dynamic changes of air pollutant concentrations such as the urban airshed model uam and the community multiscale air quality cmaq model scheffe and morris 1993 liu et al 2010 however numerical prediction methods are computationally intensive and cannot deal with the severe pollution conditions in urban areas statistical prediction methods analyze the statistical regularity of the input and output of air pollutants and then predict the future pollution trend such as time series data analysis taylor et al 2007 multiple regression model cardelino et al 2001 and neural network model viotti et al 2002 however the accuracy of statistical prediction methods is relatively low traditional machine learning based prediction methods use only one type of contaminant as the data source and ignore the relationship between various climate variables which makes the prediction inaccurate recently deep learning methods have attracted significant attention and achieved revolutionary successes in various applications guo et al 2016 therefore many researchers have utilized deep learning methods to predict pollutant concentrations for example kuremoto combined a deep belief network with restricted boltzmann machines for time series forecasting kuremoto et al 2014 and bt ong introduced a deep recurrent neural network to predict environmental monitoring data ong et al 2014 moreover as the data dimension increases it becomes more important to describe data efficiently with machine learning and big data techniques a method proposed by hinton et al efficiently used lower dimensional data to represent high dimensional data by means of neural networks hinton and salakhutdinov 2006 bengio et al achieved better results by using auto encoder to pre train deep networks schölkopf et al 2007 all of the previous works indicate that auto encoder has the ability to extract data features and can be applied to machine learning tasks additionally auto encoder can adaptively extract images inherent features almousli and vincent 2013 thus auto encoder can be considered as an encoder this motivated auto encoder researchers to improve feature generalization and to allow auto encoder to encode data in any type of data structures krizhevsky et al used auto encoder to encode images and then utilized the obtained code as an index to retrieve images from a database achieving faster query speed krizhevsky and hinton 2011 long short term memory lstm has recently shown great promise in tackling various sequence modeling tasks in machine learning such as image segmentation stollenga et al 2015 and activity recognition qin et al 2017 however the lstm technique only takes information from the past without any information from the future an elegant effect solution as demonstrated by previous work is the bi directional lstm bi lstm method lin et al 2017 the basic idea is to present each sequence forward and backward to two separate hidden states that can capture past and future information respectively then the two hidden lstm and lstm can be considered as the lstm unit the unit is capable of learning long term dependencies without retaining redundant context information and works tremendously well on deep learning such as natural language processing xu et al 2017 and speech recognition tasks mohamed et al 2015 the research in this study builds on the existing literature focused on air pollutant prediction we proposed an auto encoder based method that can extract data features and compress data thus reducing data volume to increase the training efficiency the bi lstm method is utilized to effectively handle the time series problem predict the future data based on existing data and improve the prediction accuracy moreover we proposed a preprocessing method that can confirm the impact of weather variables on pm2 5 forecast 3 problem scenario the previous pollutant prediction studies used a single site dataset without considering the influence of surrounding sites and other climate variables which cannot predict future pollutant concentrations accurately to predict pm2 5 pollutant concentrations in regional areas this study collected pollutant data in a certain area of beijing and took into account of the impact of other climate variables the collected pollutant data mainly contain the hourly records of particulate matter pm2 5 and climate variables including dewp dew point temp air temperature rain rainfall snow snowfall and wdsp wind direction speed since the data are relatively large and noisy we need to denoise and compress it to a certain extent mainly by extracting the implicit features from the data therefore an appropriate mapping relationship must be identified to extract the features from the input data this mapping relationship is shown in eq 1 1 φ χ f where φ is defined as a transition x denotes the input data and f denotes the data after feature extraction to ensure the accuracy of feature extraction we need to construct another mapping relationship to convert f into x where the difference between x and x needs to be as small as possible then an error function such as squared errors is constructed to be minimized in the training process at the end of this process f is the extracted data feature that we need the detailed process is described as follows 2 ψ f x 3 φ ψ a r g m i n x x 2 φ ψ where ψ is defined as a transition and x denotes the reconstruction of the input x since our goal is to predict air pollutants we need to analyze the past data to predict future values after extracting data features we need to find a mapping relationship which can learn the internal relationship of data and predict the value of pollutants in the future this mapping relationship is shown in eq 4 there are some intermediate layers hidden layers between mappings which are intended to learn the inherent relationship from the dataset better 4 h x y to make a more accurate prediction as the amount of time series data increases we need to develop a function that can selectively forget some unnecessary data in such a mapping relationship to achieve a better prediction here we consider a two dimensional matrix x i j where the i th dimension represents a set of attributes including the air pollutant and climate variables and the j th dimension represents an ordered set of timestamps t t 1 t 2 t d where t 1 t 2 t d for example in the following matrix x t 1 1 represents the value of pm2 5 concentration at timestamp t 1 5 x p m 2 5 d e w p t e m p r a i n x t 1 1 x t 1 2 x t 1 3 x t 1 n x t 2 1 x t d 1 x t 2 2 x t d 2 x t 2 3 x t d 3 x t 2 n x t d n the first objective of this approach is to filter x i j to x i j which can select attributes that related to air pollution a detailed explanation of the proposed method is introduced in sect 4 1 then given x i j the next objective is to reduce data dimensions and extract implicit features y i j by the auto encoder layer finally using the data features y i j as input a prediction layer based on bi directional lstm is trained to predict the air pollutant concentrations 4 methods our proposed approach is shown in fig 1 it includes two parts the first part is to preprocess data which can build climate attributes combinations related to air pollution given the selected attributes the second part can train the prediction model of air pollutant concentration using the auto encoder and bi lstm based models auto encoders ae layer are a type of neural network that aims to perform dimensionality reduction and data denoising applying ae in pattern recognition liu et al 2017 fault diagnosis shao et al 2017 and feature learning meng et al 2017 achieved good results the ae layer can learn to compress original data from the input layer generate a representation encoding of the original data and then decompress decoding the representation into something that closely matches the original data in this research we proposed to use auto encoder to extract implicit data features from the multiple variables related to pm2 5 prediction the bi lstm neural network is well suited for processing classification and prediction tasks on time series data since information about the past and the future in time series data can play both important roles in prediction bi lstm is capable of learning long term dependencies without retaining redundant context information from both the past and the future previous research on sentiment analysis chen et al 2017 classification zhao et al 2017 speech recognition ogawa and hori 2017 and biomedicine tutubalina et al 2018 has proven that the bi lstm method can improve the time series prediction therefore we proposed to use the bi lstm based layer for air pollutant concentration prediction moreover the combination of auto encoder and bi lstm models can reduce the training time and improve the prediction accuracy 4 1 preprocessing due to the relatively small numbers of data samples the prediction model may overfit during the training process then the prediction results will be inaccurate therefore the preprocessing methods can work on data samples to solve the data imbalance problem and then improve the prediction results in this approach we proposed a preprocessing method that can select attributes from the air contaminant and climate variables related to air pollution prediction given a two dimensional array x i j this process can retrieve x i j which contains the corresponding air contaminant and climate variables the preprocessing step is calculated as follows 6 x x β β 1 β 2 β n where x denotes the input matrix the β i for each attribute is a one vector or zero vector based on whether it is related to air pollutant concentration predictions and denotes a point wise hadamard multiplication operator thus by controlling β the targeted variables can be selected and β is obtained by manually initialization we simplified the preprocessing process in fig 2 for a better presentation in this figure the left side presents the vector representations of the input data whereby each row represents one variable after the preprocessing step these vectors become vector combinations where β i is a one vector or zero vector for each combination the right side shows the time series plots of different vector combinations as the first step in our approach the result of the preprocessing step will be the input data for the following prediction models 4 2 ae bi lstm model the proposed prediction model consists of two layers the auto encoder layer and the bi lstm layer taken the preprocessed data as input the first part can reduce data dimensions and extract implicit features by using the auto encoder layer then given data features the second part can train a bi directional neural network layer based on long short term memory lstm architecture named as bi lstm to predict air pollutant concentrations the auto encoder layer can learn data structures adaptively and then represent the complex data in an efficient manner these properties make auto encoder not only well suited for large volumes of datasets but also overcome the expensive designing cost and the traditional poor generalizations moreover use of the auto encoder technique in deep learning models can extract implicit features and yield better prediction results the auto encoder layer shown in fig 3 contains two processes encoder and decoder in our approach only the output of the encoder phase is used as input in the following bi lstm layer in general the auto encoder layer can extract implicit data features and reduce data size which can accelerate the training process in the following steps and subsequently improve the prediction accuracy the auto encoder layer transforms input vector x to y via the encoder f and attempts to reconstruct x via the decoder g to produce x the reconstruction error is measured by the loss function l o s s x x in addition y can be used as input in the bi lstm layer for training in the encoder process y i j σ w x i j b is constructed where x i j is the preprocessed data σ is an relu activation function w is a weighted matrix and b is a bias vector this process can reduce the dimension of the input data in the decoder process y i j is used as input and x i j is constructed by x i j σ w y i j b where σ w and b are the relu activation function weighted matrix and bias vector respectively however σ w and b differ from σ w and b the decoder process can decompress y i j to x i j thus x and x have the same dimensions to train the auto encoder layer the reconstruction error needs to be minimized the reconstruction error is calculated as follows 7 ε x x x x 2 x σ w σ w x i j b b 2 in general the auto encoder layer can reconstruct x to x and make the difference between x and x as small as possible however in our proposed approach only the hidden layer y i j is used as the input for the bi lstm layer since it is a compressed representation of x i j in the following section a prediction layer based on long short term memory lstm is generated the lstm has a similar function as the recurrent neural network rnn model to represent dynamic temporal behavior for sequential data however the hidden layer in ltsm is replaced by a long short term memory cell the lstm cell can solve the long term dependencies and the vanishing gradients problems compared with traditional rnn models the long term dependency problem means that as the time interval increases the learned information cannot be connected to the far reaching information leading to vanishing gradients for most prediction tasks it is beneficial to have both past and future contexts for a given time range however the lstms hidden layer can only take features from the past therefore the bi directional lstm bi lstm layer is developed it can train the prediction layer using both the future features and the past features efficiently which can improve the prediction accuracy to a certain extent the bi lstm layer is shown in fig 4 one lstm unit is composed of three gates including an input gate an output gate and a forget gate these gate signals include the logistic nonlinearity σ the input gate is used to control how much it should read from its input in our model the extracted features y t t 1 t d from the ae layer in d hours before timestamp t are used as the input of bilstm and the goal is to predict the concentration of pm2 5 n hour after timestamp t both d and n are predefined time windows this input is implemented by the following formula 8 i t σ u i h t 1 w i x t b i c t f t c t 1 i t tanh u c h t 1 w c x t b c where σ is an element wise sigmoid function i f and c represent for the input gate forget gate and cell vectors respectively h t is the hidden state vector the t for the subscript of each symbol indicates the time step u i and u c denote the weight matrices for hidden state h t w i and w c denote the weight matrices of different gates for input x t b i and b c denote the bias vectors and c t denotes cell state vector the forget gate is used to control how much to forget from the current cell value in this paper the input gate is a vector that is the implicit data features extracted from the auto encoder the forget gate can selectively forget some data information about the data features in the past and is implemented by the following formula 9 f t σ u f h t 1 w f x t b f where σ is an element wise sigmoid function h t 1 is the hidden state vector the t for the subscript of each symbol indicates the time step u f denotes the weight matrices for hidden state h t w f denotes the weight matrices of different gates for input x t and b f denotes the bias vectors the output gate determines the final output information which is the predicted pm2 5 concentration value and the hidden state vector h t at the next moment the output is implemented by the following formula 10 o t σ u o h t 1 w o x t b o h t o t t a n h c t where σ is an element wise sigmoid function u o denotes the weight matrices for the hidden state h t w o denotes the weight matrices of different gates for input x t and b o denotes the bias vectors furthermore in our proposed architecture bi lstm is used instead of lstm because the bidirectional lstm can obtain a better understanding of the time series data in two directions the past time series data can influence the current prediction and the future time series data will also affect the current prediction to a certain extent learning features from both the past and future data can make a more accurate prediction of air pollution concentration then the trained parameters of the bi lstm model in the training process can be used in prediction for the information recorded in the last forward vector lstm is enhanced from front to back for the information recorded in the last backward vector lstm is enhanced from back to front the combination of the two specific records can complete the information therefore more accurate results can be predicted fig 5 shows an easy way to understand this prediction here a period of pm2 5 time series data is selected as a demonstration where the forward lstm inputs are from t 1 to t 2 t and the backward lstm inputs are from t 2 t to t 1 which will determine the prediction output y t as shown in the example the combination of forward lstm and backward lstm enables more accurate prediction and will cause less prediction error than using the one way lstm 5 experimental results 5 1 dataset to demonstrate the efficacy of the prediction model proposed in this paper experiments were performed to predict pm2 5 concentration in beijing the dataset consisted of a series of hourly values from january 2 2010 to december 31 2014 and contained a number of variables including pm2 5 concentration pm2 5 air temperature temp dew point temperature dwep wind direction speed wdsp rainfall rain and snowfall snow table 1 provides detailed information about the input variables and their units fig 6 shows the time series representation of the input variables the x axis in these figures represents the hourly timestamp from january 2 2010 to december 31 2014 with 43825 h in total and the y axis represents the value of these variables camx hsu and prather 2010 cmaq chen et al 2014 naqpms wang et al 2001 and wrfchem saide et al 2011 models were used for comparison with traditional models these methods are classical air quality models that predict air pollutants by simulating air pollutants svm rnn lstm ae with rnn and ae with lstm were implemented for comparison with machine learning methods all of these methods used data from the previous 72 h to predict the pm2 5 concentration in the future and the time windows were set in the model parameters the model parameter n timesteps represents using the data of the past n hours and n outputs represents predicting air pollution concentration values of the following n hours in the experiment 80 of the samples in the dataset were used for training 10 for validation and 10 for testing the distribution of the dataset is shown in table 2 the training dataset is used to train the model parameters the model continually trains and tunes and then optimizes the performance of the model gradually by reducing the errors the validation dataset is used to adjust the parameters and to evaluate the generalization capability of the model if the model performs worse on the validation set compared with the training set then the over fitting phenomenon occurs the ae layer prevents over fitting through weight penalty while the bi lstm layer prevents over fitting through the dropout parameter the testing set is used to measure the effectiveness of the model 6 results the objective of the experiments presented in this paper is to evaluate the proposed prediction model with this in mind first we compared the proposed model with four traditional prediction models namely camx cmaq naqpms and wrfchem second the svm rnn lstm ae with rnn ae with lstm models were chosen for comparative experiments with some machine learning method in svm x t d t 2 t 1 t was used as the model input and x t 1 was the output root mean squared error rmse is used as an evaluation measurement which is defined as 11 r m s e i 1 d y i p i 2 d where y i is the true value of pm2 5 and p i is the predicted value therefore smaller rmse values indicate increased accuracy of the prediction model before training the prediction models preprocessing was performed to select climate variables as explained in section 3 3 1 the selected variables were pm2 5 with temp pm2 5 with dwsp pm2 5 with wdsp pm2 5 with rain pm2 5 with snow and pm2 5 with wdsp rain pm2 5 wdsp and snow then the selected dataset was normalized to 0 1 by eq 12 finally the normalized dataset was used as input to the prediction model 12 n e w v a r i a b l e v a r i a b l e m i n v a r i a b l e m a x v a r i a b l e m i n v a r i a b l e in the training process we performed 300 iterations for the auto encoder and bi lstm model each for the auto encoder layer the stochastic gradient descent technique was used for training while the adam training technique was used in the bi lstm layer furthermore the number of batches was set to 200 the number of lstm unit was set to 50 and the learning rate value was 1 e 3 in the bi lstm layer table 3 shows the detailed model parameter settings table 4 shows the average rmse results for the four traditional models including camx cmaq naqpms and wrfchem based on the air pollution data from beijing the cmaq model had the best performance compared with camx naqpms and wrfchem models however the rmse of cmaq was as great as 36 fig 7 shows the comparisons between the machine learning models the rmse result is shown on the upper left side of each sub figure for all the machine learning methods the predicted values shown in scattered dots were around the straight line which represented actual values rmse results from all machine learning models were less than 20 the n timesteps is set to 72 and n outputs is set to 24 in general the machine learning models performed better than the traditional models however the svm method had the worst results among the six machine learning methods and the rmse was 19 529 rmse results of the rnn lstm ae and rnn ae and lstm and our proposed model were relatively reduced 8 641 6 248 7 539 4 744 and 3 091 respectively it was evident that our proposed model performed the best and the predicted value matched well with the actual value to further compare the prediction effect of the proposed method with other machine learning methods in time series data fig 8 shows the predicted value of each model and the actual observed pm2 5 value over continuous 1000 h from the testing dataset the x axis in the figure represents the timestamp and the y axis represents the pm2 5 concentration the color bar is shown on the left and the red line indicates the real observed value similar to fig 7 the predicted data of the svm method indicated as the light blue line reveals larger differences compared with the real data our proposed ae and bi lstm model indicated as the dark blue line is still the closest to the real data in addition to the above experimental results we also verified the correlation between pm2 5 and other climate variables in this experiment we trained the prediction models using pm2 5 alone pm2 5 with air temperature pm2 5 with dew point temperature pm2 5 with wind direction speed pm2 5 with rainfall pm 2 5 with snowfall pm2 5 with rainfall and wind speed and pm 2 5 with snowfall and wind speed according to the results in fig 7 the most relevant four models namely rnn lstm ae lstm and ae bilstm were chosen for performance comparisons the comparison results that demonstrate the rmse trends are shown in fig 9 in fig 9 the x axis indicates the time range to predict the air pollutant concentration increasing from 24 to 168 h which is the n outputs in the parameter setting in addition the y axis means the root mean squared error rmse the rnn model performs the worst whereas the ae bilstm model has the best prediction result the addition of an ae layer that extracts data features can improve the performance prominently when comparing the lstm model with the ae lstm model among these four models the prediction error using pm2 5 with temp and pm2 5 with dewp shown in blue and green lines respectively were relatively increased compared with the use of pm2 5 alone at any time however when the prediction time range was less than 120 h the combinations that include wdsp rain snow wdsp with rain and wdsp with snow can produce better results than using pm2 5 alone in our proposed ae bilstm model using pm2 5 with rain snow wdsp and rain as well as wdsp with snow perform best when predicting 24 48 and 72 h values however when predicting 96 120 and 144 h concentration values the errors of pm2 5 with rain and snow were similar and both were less than the other combinations in general our proposed model can achieve the best performance compared with the other three models to verify the correlation between pm2 5 and other climate variables more specifically rmse results using different climate variable combinations of our proposed model that increase the prediction time range from 24 to 168 are shown in table 5 along with another measurement namely correlation coefficient the value of correlation coefficient ranges from 1 to 1 this measurement can represent a statistical relationship between two variables specifically 1 indicates the strongest positive correlation and 1 indicates the strongest negative correlation correlation coefficient results show that all of the climate variables in our experiment are related to the pm2 5 prediction in a positive manner in addition the rainfall and snowfall are the most relevant single variables combining rainfall with wind speed and combing snowfall with wind speed have the greatest effects on pm2 5 prediction thus the use of pm2 5 with rain snow wdsp and rain as well as wdsp with snow can achieve the best prediction results in addition to predict 72 h pm2 5 concentration values the best result is obtained by combining pm2 5 with rainfall and wind speed the prediction using pm2 5 with wdsp rain snow wdsp and rain wdsp and snow were better than using pm2 5 alone however to predict 168 h pm2 5 concentration values it was found that using pm2 5 alone was better than combining pm2 5 with any other meteorological variable the reason is that the meteorological variables themselves are relatively accurate in 72 h which can enhance the pm2 5 prediction accuracy however the meteorological variables generate more uncertain effects over 72 h which increases the pm2 5 prediction difficulty and reduces the accuracy as a result the meteorological variables become interference factors when predicting more than 72 h pm2 5 concentration values 7 conclusion in this paper we proposed a novel prediction model to forecast pm2 5 concentrations in this model a preprocessing method is introduced to select appropriate attributes from the air containment and climate variables related to air pollution prediction such as the combination of pm2 5 and weather variables this phase is proved to be essential for evaluating the impacts of climate variables on pm2 5 forecasting then the preprocessed data are taken as input to an auto encoder layer which can extract implicit data features from a large volume dataset and improve the training efficiency the results shown in figs 7 9 proved that using the ae layer can achieve a better result finally the output of auto encoder is used as data input to train the bi lstm layer the bi lstm layer can effectively handle the time series data and improve prediction accuracy the experimental results showed our proposed model can achieve better prediction accuracy than the camx naopms and mrfchem models and several machine learning methods in addition we found a strong positive correlation between the climate variable rainfall and wind speed with pm2 5 concentration for future work there are a number of ways in which this research could be extended first the dataset in this study is limited by the data source given that pollution and meteorological data are exclusively considered data on pollen counts traffic counts vehicle emissions and industrial emissions that are more directly linked to pm2 5 could be considered in the future second the model can be extended to add datasets from multiple sites which can consider air pollutant interactions among different locations finally one important extension is to use images to predict air pollutant and to trace contamination diffusion paths declaration of competing interest the authors declared that they have no conflicts of interest to this work entitled constructing a pm2 5 concentration prediction model by combining auto encoder with bi lstm neural networks acknowledgements this work is funded by national natural science foundation of china 61572326 61802258 61702333 natural science foundation of shanghai 18zr1428300 the shanghai committee of science and technology 17070502800 and the shanghai sailing program grant no 19yf1436900 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104600 
26091,nitrate contamination in groundwater was evaluated using the concept of integrated aquifer assessment by combining groundwater characterization and risk analysis with tiered approaches for land and surface runoff contamination by soil chemicals and leaching of contamination to groundwater in the upper white river watershed uwrw in indiana integrated aquifer vulnerability assessment was conducted using an integration of a distributed watershed model soil and water assessment tool swat and a machine learning technique geospatial artificial neural network geo ann the results indicate that integrated aquifer vulnerability assessment performed well based on the model performance nse r2 pbias 0 66 0 70 0 07 thus the overall assessment of aquifer vulnerability can be performed using the integrated aquifer vulnerability assessment technique provided in this study moreover this approach provides an efficient guide for managing groundwater resources for policy makers and groundwater related researchers keywords geo ann groundwater protection integrated aquifer vulnerability assessment nitrate contamination swat 1 introduction groundwater is an important water resource for many people in the world and is also a primary resource in indiana with 60 of the state s drinking water coming from groundwater alley et al 2002 solley et al 1998 moreover groundwater is a vital component of the local regional and global environment with groundwater feeding ecosystems as well as providing baseflow in rivers morris et al 2003 anthropogenic activities affect the quantity and quality of water resources including groundwater which offers human populations a number of services such as water for drinking and irrigation mukate et al 2018 contaminated groundwater has been identified in both urban and rural areas agriculture can cause groundwater degradation due to application of chemicals in agriculture areas if chemicals used in agriculture are slightly soluble in water they are less likely to result in groundwater contamination however ammonium which is a major fertilizer and manure which contains nitrogen is highly soluble and mobile in water and nitrate concentrations generated by nitrification of ammonium are widespread in both surface water and groundwater high nitrate concentrations are detected in some areas of midwest states mukate et al 2018 li et al 2019 kellogg et al 1994 to alleviate the negative effects of agriculture on groundwater resources and to maintain clean groundwater groundwater management has been facilitated by groundwater monitoring and modeling allouche et al 2017 jang et al 2017 unland et al 2015 groundwater monitoring can identify groundwater quality and quantity directly in real time and groundwater monitoring data can help enhance the planning sustainable development and management of groundwater resources however groundwater monitoring is complicated and an expensive process therefore many areas have sparse groundwater monitoring data alcalá et al 2015 compared with groundwater monitoring groundwater modeling is less complicated and less expensive also groundwater modeling allows assessment of broad areas both groundwater monitoring and modeling should be mutual and complementary for efficient evaluation of groundwater quality and quantity in broad areas i e regional scale or continental if the two systems are utilized simultaneously various hydrologic and water quality models have been developed based on conceptual statistical stochastic analytic physical and numerical models of surface water and groundwater systems which provide a means for predicting surface water and groundwater system responses to future conditions thus each model has its own purpose and characteristics a particular model is usually selected and used as a suitable tool depending on a research or project goal however various research efforts require interdisciplinary fields for example if a research effort or project considers analysis planning and management of a wide range of water resources and environmental problems related to surface water and groundwater the model or models not only should address surface water but should also deal with groundwater thus this often results in the demand for an integrated approach and coupled models for different systems kamp and savenije 2007 chen et al 2018 therefore sometimes when a model cannot address a research or project problem two or more models can be used or a combined model can be utilized to enhance the physical representation of hydrologic processes for better estimation many studies related to hydrologic and water quality have been conducted to determine efficient water management using a coupled model gao et al 2019 shin et al 2019 maxwell et al 2015 for efficient groundwater resources management integrated aquifer vulnerability assessments are required integrated aquifer vulnerability assessments are incorporated into a groundwater characterization and risk analysis with tiered approaches for land and surface runoff contamination by soil chemicals and leaching of contamination to groundwater the machine learning technique ann is a data driven model based on data experienced in the real world phenomena of a specific system in contrast to analytical or numerical models data driven models can be used to solve problems in the field of hydrology and water resources engineering where knowledge of the physical behavior of the system and data are limited solomatine and ostfeld 2008 thus for the simulation of complex systems data driven models are useful to define the patterns within the behavior of the system araghinejad 2014 ann is a black box model not a real black box that does not require detailed knowledge of the internal functions of a system to identify the complex dynamic and non linear relationships from given patterns by input and output however as a black box character model users have no control except providing input data and initial parameters such as learning rate maximum number of training cycle and target error ha and stenstrom 2003 machine learning is an artificial intelligence approach for improving the computer s ability to learn without being explicitly programmed the main goal of machine learning is to find relationships between the system state variables and to replace time consuming manual processes with automatic techniques that improve accuracy or efficiency by exploring and determining regularities in training data worland et al 2018 solomatine and shrestha 2009 as time passes nonstationary trends of hydrology and water resources time series have been exhibited more frequently yu and lin 2015 coulibaly and baldwin 2005 thus for accurate estimation of hydrology and water resources systems techniques which can simulate the nonstationary patterns of variables of hydrology and water resources are required many studies have already proven that machine learning is a suitable technique in predicting nonstationary behavior of hydrology and water resources systems nourani et al 2009 worland et al 2018 the objectives of this study were 1 to generate meaningful data related to groundwater pollution by swat as significant input variables for integrated aquifer vulnerability assessment 2 to develop geo spatial ann geo ann which is compatible with gis rs data formats for flexible hydrology and water quality modeling and 3 to develop and provide a model system for integrated aquifer vulnerability assessment by combining swat and geo ann for efficient groundwater management 2 materials and methods 2 1 study area the upper white river watershed uwrw latitude 39 29 51 n longitude 86 24 02 w is located in central indiana fig 1 the drainage area of the uwrw is 6944 km2 and the most dominant land use is agriculture 3160 km2 the uwrw is important for public drinking water supply because the uwrw includes more than 3508 km of streams numerous artificial lakes and 4 reservoirs the uwrw serves as a drinking water supply for part of the city of indianapolis which is indiana s largest city the water sources in the uwrw traditionally are individual wells to provide groundwater for residential commercial and industrial purposes tedesco et al 2011 tedesco et al 2005 fleming et al 1993 the uwrw was selected to identify hydrologic water quality and aquifer risk assessment because the uwrw has available streamflow u s geological survey usgs streamflow stations water quality environmental protection agency epa fixed stations and usgs monitoring well stations data for both streamflow and groundwater fig 1 and table 1 2 2 integrated aquifer vulnerability assessment 2 2 1 overview of integrated aquifer vulnerability assessment for overall aquifer vulnerability assessment integrated aquifer vulnerability assessment should be carried out in this study overall aquifer vulnerability assessment is called integrated aquifer vulnerability assessment which includes a groundwater characterization and risk analysis with tiered approaches for land and surface runoff contamination by soil chemicals and leaching of contaminates to groundwater aquifer intrinsic and transport properties are analyzed by integrated aquifer vulnerability assessment and integrated aquifer vulnerability is a function of land and surface runoff contamination and groundwater contamination fig 2 aquifer vulnerability assessment is usually conducted by overlay and index gis model or is implemented by numerical models jang et al 2017 akhavan et al 2010 2011 thirumalaivasan et al 2003 in this study integrated aquifer vulnerability assessment was conducted for efficient aquifer vulnerability assessment using a distributed watershed model swat and machine learning technique geo ann 2 2 2 configuration of integrated aquifer vulnerability assessment 2 2 2 1 overview of hydrologic and water quality model swat swat is a physically based distributed deterministic and long term continuous time model which is used to predict impact of management practices land use and land cover lulc change and climate change on hydrology and water quality on a watershed scale at a daily time step arnold et al 1998 neitsch et al 2011 swat represents the large scale spatial heterogeneity of the study area by dividing a watershed into subbasins the subbasin is the first level of subdivision of the watershed subbasins include geographic position information within the watershed and are spatially connected to one another neitsch et al 2011 the land area in a subbasin may be divided into hydrologic response units hrus which are the smallest computational units in swat hrus are parts of a subbasin that possess unique land use management and soil attributes hrus are created by one or more unique land use and soil combinations for each subbasin surface runoff soil water content crop growth nutrient cycles and erosion are simulated for each hru and then hrus are combined and calculated for the subbasin by a weighted value neitsch et al 2011 williams et al 1984 in this study variables related to nitrate movement were extracted for each hru and hru maps were created as input data for aquifer risk assessment using geo ann 2 2 2 2 swat input data the period of swat simulation is from 1990 to 2010 because the swat format climate data i e precipitation and temperature were available in this period provided by national climate data center ncdc and processed by the agricultural research service ars also there are sufficient water quality data from 1990 to 2010 for the uwrw as described in table 2 the primary input data for the swat simulation are digital elevation model dem soil lulc and weather data additionally for nitrate simulation the scheduled management operation data were prepared to consider application of fertilizer pesticides and manure table 3 a general strategy of management practices for corn soybean rotation is described in table 3 her et al 2016 the dem from u s geological survey usgs was used for watershed delineation and soil and lulc data from natural resources conservation service nrcs and natural agricultural statistics service nass were used with respect to the hydrological response in the watershed weather data include minimum and maximum temperature daily precipitation mean wind speed relative humidity and solar radiation which were obtained from national climate data center ncdc table 2 the spatial dem fig 3 a and soil maps in the uwrw fig 3 b were obtained from usgs elevation of the uwrw varies from 162 9 to 371 7 m twenty eight soil types are distributed in the study area such as 33 5 in013 crosby 24 in040 miami 6 3 in029 sawmill 6 in026 fox 5 2 in054 miamian and 24 9 other soil types lulc in the uwrw fig 3 c includes 45 5 agricultural fields 22 5 soybeans 22 2 corn and 0 8 others the mean annual precipitation is 1093 mm and the highest the lowest daily temperature and mean daily temperatures are 36 1 c 31 3 c and 10 8 c respectively fig 3 d shows well locations in the uwrw 678 well nitrate data were utilized for the training validation and testing of geo ann using swat outputs the well nitrate data were obtained from heidelberg university and the water quality portal wqp sponsored by the u s geological survey usgs the u s environmental protection agency epa and the national water quality monitoring council nwqmc min max mean values of well nitrate data are 0 1 18 29 1 22 mg l respectively with the input data table 2 hydrology surface water groundwater hydrology and water quality were simulated in the uwrw by swat then observed streamflow and estimated baseflow or observed baseflow if available from the usgs were used to calibrate and validate swat tile drainage was applied in areas where the land use is corn or soybean and the soil drainage condition is poorly drained boles et al 2015 jiang et al 2014 based on the previous studies for indiana watersheds boles et al 2015 jiang et al 2014 green et al 2006 and the technical report about tile drainage usda nrcs 2011 parameters related to the tile drainage in swat were used annual average 2008 2015 mass of point source pollutants related to no3 showed less than 1 ton year from the national pollutant discharge elimination system npdes in the uwrw observed no3 loads estimated no3 loads using loadest were approximately 2333 ton year the small amount of point source pollutants 0 04 of observed no3 loads would not significantly affect results of this study further point source data years were not matched for the periods of this study 1990 2010 thus point sources were not considered in the uwrw 2 2 2 3 hydrologic and water quality modeling using swat the land phase of the hydrologic cycle controls the amount of water sediment nutrient and pesticide loadings to the main channel and to the aquifers in each subbasin the hydrologic cycle simulated in swat is based on the water balance equation eq 1 also nitrate leaching from the soil profile to the aquifer and nitrate loading to streamflow contributed by baseflow are estimated in swat fig 4 neitsch et al 2011 1 s w t s w 0 i 1 t r d a y i q s u r f i e t a i q t i l e i q l a t i q g w i where s w t is the final soil water content mm at time t t is the time day s w 0 is the initial soil water content mm h2o r d a y i is the amount of precipitation on day i mm h2o q s u r f i is the amount of surface runoff on day i mm h2o e t a i is the amount of evapotranspiration on day i mm h2o q t i l e i is the amount of water removed from the layer by tile drainage on day i q l a t i is the amount of lateral flow released to the main channel on day i mm h2o and q g w i is the amount of return flow on day i mm h2o surface runoff q s u r f i was estimated using the soil conservation service scs curve number cn method eq 2 scs 1972 2 q s u r f i r d a y i 0 2 s 2 r d a y i 0 8 s i a 0 2 s s 25 400 c n 254 where i a is the initial abstraction mm h2o s is the retention parameter mm h2o and cn is the curve number dimensionless the penman monteith method was used to calculate the rate of evapotranspiration e t a i eq 3 monteith 1965 3 e t a i δ h n e t g ρ a i r c p e z 0 e z r a δ γ 1 r c r a λ where δ is the slope of the saturation vapor pressure temperature curve de dt kpa c h n e t is the net radiation mj m 2d 1 g is the heat flux density to the ground mj m 2d 1 ρ a i r is the air density kg m 3 c p is the specific heat at constant pressure mj kg 1 c 1 e z 0 is the saturation vapor pressure of air height z kpa e z is the water vapor pressure of air at height z kpa γ is the psychrometric constant kpa c 1 r c is the plant canopy resistance s m 1 r a is the diffusion resistance of the air layer aerodynamic resistance s m 1 and λ is the volumetric latent heat of vaporization energy required per water volume vaporized mj m 3 lateral flow was calculated with eq 4 neitsch et al 2011 4 q l a t i 0 0224 2 s w l y e x c e s s k s a t s l p φ d l h i l l where s w l y e x c e s s is the drainage volume of water stored in the saturated zone of the hillslope per unit area mm h2o k s a t is the saturated hydraulic conductivity mm h s l p is the average slope of the subbasin m m φ d is the drainage porosity of soil mm mm and l h i l l is the hillslope length m both shallow aquifer and deep aquifers are considered as groundwater storage total baseflow groundwater flow was calculated by total amount of water in the shallow aquifer and deep aquifer eqs 5 10 neitsch et al 2011 5 q g w i q g w s h i q g w d p i where q g w i is the total groundwater flow on day i mm h2o q g w s h i is the total groundwater flow in the shallow aquifer on day i mm h2o and q g w d p i is total groundwater flow in the deep aquifer on day i mm h2o 6 q g w s h i q g w s h i 1 exp α g w s h δ t w r c h r g s h i 1 exp α g w s h δ t where q g w s h i 1 is the groundwater flow from the shallow aquifer on day i 1 mm h2o α g w s h is the baseflow recession constant δ t is the time step day and w r c h r g s h i is the amount of recharge entering the shallow aquifer on a day i mm h2o 7 q g w d p i q g w d p i 1 exp α g w d p δ t w r c h r g d p i 1 exp α g w d p δ t where q g w d p i 1 is the groundwater flow from the deep aquifer on day i 1 mm h2o α g w d p is the baseflow recession constant δ t is the time step day and w r c h r g d p i is the amount of recharge entering the deep aquifer on a day i mm h2o 8 w r c h r g i 1 exp 1 δ g w w s e e p i exp 1 δ g w w r c h r g i 1 9 w r c h r g d p i β d p w r c h r g i 10 w r c h r g s h i w r c h r g i w r c h r g d p i where w r c h r g i is the amount of recharge entering the both aquifers on day i mm h2o δ g w is the drainage time of the overlaying geologic formations days w s e e p i is the total amount of water exiting the bottom of the soil profile on day i mm h2o w r c h r g i 1 is the amount of recharge entering the aquifers on day i 1 mm h2o β d p is the aquifer percolation coefficient dimensionless nitrate in the shallow aquifer may remain in the aquifer move with recharge to the deep aquifer move with groundwater flow into the main channel or be transported out of the shallow aquifer with water moving in the soil zone in response to water deficiencies nitrate in recharge to the shallow aquifer and deep aquifer on a given day was calculated with eqs 11 and 12 neitsch et al 2011 11 n o 3 r c h r g i 1 exp 1 δ g w n o 3 p e r c exp 1 δ g w n o 3 r c h r g i 1 where n o 3 r c h r g i is the amount of nitrate in recharge entering the aquifers on day i kg n ha δ g w is the delay time drainage time of the overlying geologic formation day n o 3 p e r c is the total amount of nitrate exiting the bottom of the soil profile on day i kg n ha n o 3 r c h r g i 1 is the amount of nitrate in recharge entering the aquifers on day i 1 mm h2o 12 n o 3 g w i n o 3 s h i 1 n o 3 r c h r g i q g w i a q s h i q g w i w r e v a p i w r c h r g d p i where n o 3 g w i is the amount of nitrate in groundwater flow from the shallow aquifer on day i kg n ha n o 3 s h i 1 is the amount of nitrate in the shallow aquifer at the end of day i 1 kg n ha q g w i is the groundwater flow into the main channel on day i mm h2o a q s h i is the amount of water stored in the shallow aquifer at the end of day i mm h2o w r e v a p i is the amount of water moving into the soil zone in response to water deficiencies on day i mm h2o w r c h r g d p i is the amount of recharge entering the deep aquifer on day i mm h2o 2 2 2 4 streamflow baseflow and nitrate calibration validation streamflow and baseflow were calibrated and validated simultaneously based on usgs streamflow and simulated baseflow by modified swat 2012 code jang et al 2018 and the web gis based hydrograph analysis tool what system lim et al 2005 which can separate baseflow from streamflow for calibration and validation of nitrate loads in streamflow daily or monthly nitrate loads are necessary however nitrate concentration data were available only for a few days each year thus load estimator loadest runkel et al 2004 was used to estimate mean monthly nitrate loads using estimated nitrate loads by loadest calibration and validation of nitrate loads at the main outlet were conducted for accurate estimation of nitrate concentrations in the aquifer calibration and validation of nitrate loads in streamflow are necessary because nitrate transport to streamflow and nitrate leaching to the aquifer are interdependent sullivan et al 2019 sequential uncertainty fitting algorithm version 2 sufi 2 abbaspour 2011 abbaspour et al 2004 was used to perform sensitivity and uncertainty analysis as well as calibration and validation because sufi 2 is an efficient autocalibration algorithm and provides a user friendly environment for swat kundu et al 2016 sloboda and swayne 2013 for evaluation of the model performance nash sutcliff efficiency nse eq 13 coefficient of determination r2 eq 14 and percent bias pbias eq 15 were selected as the objective function for calibration and validation of streamflow baseflow and nitrate loads krause et al 2005 gupta et al 1999 nash and sutcliffe 1970 the total simulation period was from 1990 to 2010 21 years with the first 4 years as the model warm up period calibration for streamflow and baseflow streamflow calibration 10 outlets and baseflow calibration 1 main outlet was implemented with the parameters related to water balance subsurface water surface runoff physical properties of soil and physical properties of channel calibration for nitrate loads nitrate loads calibration 5 outlets was conducted with the parameters associated with the nitrogen cycle calibration parameter ranges for streamflow baseflow and nitrate loads were defined based on the results of sensitivity analysis and previous studies yeo et al 2014 arnold et al 2012 zhang et al 2011 lam et al 2010 du et al 2006 after model calibration validation for streamflow baseflow and nitrate loads were performed with the calibrated parameters model performance ratings for streamflow baseflow and nitrate loads were evaluated with three quantitative statistics shown in table 4 moriasi et al 2007 engel et al 2007 singh et al 2004 van liew et al 2003 13 nse 1 i 1 n y i o b s y i s i m 2 i 1 n y i o b s y m e a n 2 where y i o b s is the ith observed data y i s i m is the ith simulated data y m e a n is the mean of observed data and n is the total number of observed data 14 r 2 i 1 n y i o b s y o b s y i s i m y s i m 2 i 1 n y i o b s y o b s 2 i 1 n y i s i m y s i m 2 2 where y i o b s is the ith observed data y o b s is the mean of observed data y i s i m is the ith simulated data y s i m is the mean of simulated data and n is the total number of observed data 15 pbias 1 i 1 n y i o b s y i s i m 100 i 1 n y i o b s where y i o b s is the ith observed data y i s i m is the ith simulated data and n is the total number of observed data 2 2 2 5 retrieval of variables of aquifer vulnerability using swat as mentioned earlier two variables in eqs 11 and 12 are related to nitrate contamination in aquifers in swat the first variable no3l n o 3 r c h r g i is the nitrate leached from the soil profile and the second variable no3gw n o 3 g w i is the nitrate transported into the main stream from the groundwater loading those two variables were selected from the swat outputs to estimate aquifer vulnerability the two variables were retrieved at hru levels by an hru extractor developed for this study then spatial input variable maps for geo ann were created to identify potential aquifer vulnerable areas 2 2 3 integrated aquifer vulnerability assessment 2 2 3 1 development of geo ann geo ann was developed to train validate and predict geospatial data as well as tabular data using artificial neural networks ann geo ann would be suitable for studies of hydrology and water quality modeling because those studies typically use geospatial data as an input or output to conduct spatial analysis and prediction geo ann provides a user friendly graphical user interface gui for optimizing the parameters of the ann architecture and training the network for the prediction the shapefile and tabular format can be directly used as input without converting to a new format and can be mapped with predicted values by ann various metrics for water resources and environmental management studies such as nse r2 and pbias are implemented to evaluate the performance of the user configured ann models cross validation techniques are included in the geo ann such as k fold and leave one out cross validation loocv cross validation is an effective validation method when the amount of calibration and validation data are limited shao and er 2016 wong 2015 because there are limited data for groundwater hydrology and water quality cross validation would be a supportive technique in groundwater hydrology studies the optimal neural network parameters were designed based on the nitrate observation data used in this study because the nitrate observation data are limited in most cases training and validation have to proceed with limited data however validation using limited data often fails to accurately estimate the performance of the designed prediction model causing overfitting or underfitting problems thus cross validation was used which partitions a sample data into different subsets of training validation and testing data a k fold cross validation was used as the cross validation technique which randomly divides the k number of subset of sample data in each run heaton 2008 as a rule of thumb the division ratios of training and validation are set as 70 and 30 individually rai et al 2019 once the sample data are split into these three sets the network performance is evaluated the performance of the geo ann is evaluated using the three metrics nse r2 and pbias before assigning the input and output data into the geo ann all data are normalized to fall in 0 1 0 9 using eq 16 because the normalization improves accuracy performance and speed of geo ann kalin et al 2010 sethi et al 2010 in this study the number of inputs and output are 2 and 1 and the number of neurons ranges from 2 to 4 for determining the number of layers 1 or 2 is are a typical number in a small or moderate size of the network for this study the whole application including training validation algorithms and gui for geo ann was developed using the neural network toolbox in matlab many matlab users mainly use matlab version 2016 2017 and 2018 therefore compatibility test for geo ann was conducted and successfully completed 16 x n o r m 0 1 x 0 x min x max x min where x n o r m is the normalized value x 0 is the observed value x min is the minimum value and x max is the maximum value the functionalities of the geo ann are summarized as follows 1 design of the ann layers number of hidden layers and number of neurons 2 selection of training optimization algorithm levenberg marquardt gradient descent or bayesian regularization beale et al 2016 3 selection of normalization methods pre conditioning normalization of the user defined range e g 0 1 0 9 0 1 or 1 1 4 functions for importing and exporting a raw shapefile or tabular format for training validation and prediction 5 various metrics for performance measurements for hydrology and water quality modeling nse r2 and pbias moriasi et al 2007 6 two cross validation techniques loocv and k fold cross validation heaton 2008 7 stopping criteria design maximum number of validation increases minimum performance value maximum number of training epochs iterations 8 enable gpu and or parallel computing capability for large ann training 2 2 3 2 analysis of aquifer vulnerability using geo ann for aquifer vulnerability assessment fig 5 two predefined input variables no3l and no3gw were retrieved from the swat simulation maps for nitrate leached from the soil profile and nitrate in groundwater were obtained through surface and groundwater hydrology and water quality simulation by swat no3l and no3gw were utilized as input data to geo ann that was developed in this study nitrate concentration data from 678 monitoring wells fig 6 were used for training validation testing of geo ann 562 data points were less than 2 ppm and 116 data points were greater than 2 ppm nitrate levels over 2 ppm were assumed to be caused by human activities because nitrate levels in aquifers under natural conditions are typically less than 2 ppm in indiana navulur 1996 thus a threshold value of background concentration was set at 2 ppm in this study integrated vulnerability assessment was conducted using nitrate detections 2 ppm as elevated n levels in order to fill the data gap and validate spatially distributed nitrate concentrations k fold cross validation was used for training and validation of geo ann many studies revealed that k fold cross validation is a reliable method when the number of training validation data are small shao and er 2016 wong 2015 kalin et al 2010 recommended model performance criteria for ann with two metrics i e nse and pbias in watershed modeling at a monthly time scale kalin et al 2010 model performance criteria for geo ann were modified by adding one more metric i e coefficient of determination r2 based on moriasi et al 2015 and kalin et al 2010 as shown in table 5 geo ann performance was evaluated using the three metrics i e nse r2 and pbias eqs 13 15 3 results and discussion 3 1 calibration and validation of streamflow baseflow and nitrate loads for the simulation period of 21 years 1990 2010 monthly mean streamflow is 85 7 m3 s and monthly mean baseflow is 35 9 m3 s according to the what system lim et al 2005 monthly mean baseflow accounted for 41 9 of monthly mean streamflow the comparison between the loadest estimated and usgs observed nitrate loads indicate nse of 0 84 r2 of 0 89 and pbias of 0 92 these values indicate that loadest estimated nitrate loads quite well model calibration and validation of streamflow baseflow and nitrate loads at the main outlet were conducted by adjusting twenty four parameters calibrated values were estimated with simultaneous streamflow and baseflow calibration and nitrate loads calibration with multi site flow and water quality stations hydrographs reproduced by swat between the observed and simulated monthly streamflow and baseflow at the main outlet flow 28 and wq 28 during the calibration and validation periods are shown in fig 7 a and b as shown in fig 7 a even though some peak and low streamflow values were underestimated the calibrated model performed well for monthly streamflow simulation the calibrated model underestimated during peak streamflow periods and the model slightly overestimated during some low streamflow periods streamflow during the validation period was better estimated than during the calibration period fig 7 b shows results of monthly baseflow simulation during the calibration and validation periods baseflow simulated with the calibrated model was estimated satisfactorily thus the result shows that the calibrated model simulates both streamflow and baseflow well it is noted that simultaneous streamflow and baseflow calibration are necessary for robust estimation of hydrological parameters based on the calibrated parameters from the main outlet of the uwrw cross validation for streamflow was conducted at 9 additional usgs streamflow stations even though some peak flows were underestimated all simulations replicated the observed streamflow well the calibrated parameters explained hydrology characteristics for the entire watershed after hydrology calibration and validation using 10 stations calibration and validation for nitrate loads were conducted fig 8 shows that even though the calibrated model simulated nitrate loads reasonably well the model underestimated nitrate loads for some points at wq 10 for 1993 wq 17 for 1998 and wq 28 for 1992 and 1993 there was underestimation because timing of fertilizer application was unknown and simulated streamflow was underestimated by swat for those periods the other reason for underestimated nitrate loads could be the uncertainties of nitrate monitoring data and loadest model performance for streamflow baseflow and nitrate load simulation at the main outlet was evaluated by nse r2 and pbias table 6 for monthly streamflow calibration and validation at the main outlet nse values range from 0 85 to 0 88 r2 values vary from 0 87 to 0 92 and pbias values range from 1 36 to 3 90 based on table 4 all simulation periods for streamflow total period calibration and validation are within the very good range indicating all simulation periods are acceptable for the monthly baseflow calibration and validation nse values total period calibration and validation vary from 0 63 to 0 65 r2 values range from 0 70 to 0 73 and pbias values vary from 13 8 to 16 7 the results of the baseflow calibration and validation show very good good and satisfactory ranges which mean all simulation periods for baseflow total period calibration and validation are acceptable the results of simultaneous streamflow and baseflow calibration indicate good performance for baseflow calibration as well as streamflow calibration for monthly nitrate load calibration and validation at the main outlet nse values of all simulation periods range from 0 51 to 0 72 r2 values vary from 0 58 to 0 77 and pbias values range from 13 9 to 20 1 based on table 4 all simulation periods are within the very good good and satisfactory ranges the calibrated model is acceptable for all simulation periods of streamflow baseflow and nitrate loads as shown in table 6 simulated streamflow using calibrated parameters at the main outlet replicated observed streamflow at the 9 usgs streamflow stations well based on table 4 all simulated streamflow at the 9 usgs streamflow stations are within the very good good and satisfactory ranges which mean all 9 flow simulations are acceptable nse values vary from 0 75 to 0 91 and 90 of nse are very good r2 values range from 0 77 to 0 92 and 90 of r2 are also very good pbias values vary from 16 4 to 15 9 and 50 of pbias are very good table 7 for accurate nitrate load estimation multi site calibration was performed to satisfy all nitrate load simulations at the 4 epa fixed stations and 1 usgs water quality station all nitrate load simulations are better than satisfactory nse values vary from 0 58 to 0 77 and 40 of nse are very good r2 values range from 0 60 to 0 79 and 40 of r2 are also very good pbias values vary from 3 5 to 29 7 and 80 of pbias are very good table 8 even though all simulations for nitrate loads are within acceptable ranges based on table 4 most pbias values indicate most simulated nitrate were underestimated 3 2 development and application of geo ann geo ann was developed using the neural network toolbox in matlab to build a new model to predict nitrate concentrations for integrated aquifer vulnerability assessment the new geo ann based model was created through the process of training validation and testing using two predefined input variables nitrate leached from the soil profile no3l and nitrate transported into streams from groundwater loading no3gw with the shapefile format as well as tabular format calculated and generated by swat the coupled modeling framework using swat and geo ann was utilized to find the relationship between the nitrates leached from the soil profile and nitrate concentrations in aquifers and to identify potential vulnerable areas in aquifers the network performance was validated using different combinations of the number of neurons and hidden layers within the ranges i e neuron ranges 2 to 4 and layer ranges 1 to 2 in all simulations the levenberg marquardt algorithm was used for training the network and a hyperbolic tangent sigmoid was implemented as the transfer function for hidden layers and the output layer the training process stops if the maximum number of incremental validations reaches more than 30 with two input variables no3l kg ha no3gw kg ha calculated by eqs 11 and 12 and observed well nitrate data the two input variables and one observed variable were trained validated tested with the parameters and structures of the geo ann to create the new model which can predict nitrate concentrations in wells then three performance metrics were produced as shown in table 9 final outcomes of model performance are calculated using the median value of all outcomes by the number of k fold cross validation among different combinations of the number of neurons and hidden layers 2 hidden layers and 2 neurons produced the optimal solutions of nse r2 and pbias during the testing nse r2 pbias for the testing shows 0 66 0 70 0 07 each value is median out of 1000 simulation results according to table 5 all model performance for the testing of the geo ann indicates better than satisfactory nitrate prediction for hrus which do not have observed nitrate data was conducted using the structure of 2 hidden layers and 2 neurons for each hidden layer hl nr nr 2 2 2 3 3 analysis of integrated aquifer vulnerability according to akhavan et al 2010 de paz et al 2009 and pohlert et al 2007 it is not easy to find correlation between the nitrate leached from the soil profile and the nitrate concentrations in aquifers because other factors such as groundwater age groundwater depth lateral flow and denitrification in the unsaturated zone also play a role in this study in order to reduce complexities in estimation of nitrate concentrations in aquifers swat and geo ann were utilized to find a relationship between the nitrates leached from the soil profile and nitrate concentrations in aquifers and to identify aquifer vulnerability areas no3l the nitrate leached from the soil profile mean value 3 86 kg ha yr and no3gw the nitrate transported into main stream from the groundwater loading mean value 0 18 kg ha yr were estimated for hrus by swat simulation then average 1990 2010 estimated annual nitrate concentrations in aquifers were predicted after training validation testing using the geo ann estimated nitrate concentration ranges from 0 to 12 25 mg l and these nitrate concentration values were also normalized from 0 to 1 by eq 16 for creating the integrated aquifer vulnerability map integrated aquifer vulnerability indices were divided into five classes 0 0 2 very low 0 2 0 4 low 0 4 0 6 moderate 0 6 0 8 high and 0 8 1 0 very high as shown in fig 9 and table 10 25 0 of the aquifer systems in the uwrw was within the very low vulnerability class and 41 2 of the area was estimated as low 22 7 within the moderate vulnerability class 8 8 within the high vulnerability class and 2 3 within the very high vulnerability class the integrated aquifer vulnerability results table 10 from swat and geo ann were validated with the well database the results indicated that approximately 79 3 of nitrate detections 2 ppm are within high and very high vulnerability classes represent 11 2 of the aquifer system areas in the uwrw as predicted by swat and geo ann moreover 9 5 of the nitrate detections were within the moderate vulnerability class 22 7 of the aquifer system area in the uwrw 8 6 of the nitrate detections were within the low vulnerability class 41 2 of the area in the uwrw and 2 6 of the nitrate detections were included in the very low vulnerability class 25 0 of the area in the uwrw table 10 for integrated aquifer vulnerability assessment 11 2 of nitrate detections 2 ppm were within the very low and low vulnerability areas the nitrates detected in the very low and low vulnerability areas might be caused by point sources application timing and excessive fertilizer application among other reasons which were not considered in this study these factors i e point sources application timing and excessive fertilizer application should be considered by adding additional data and modifying the models used in this study the number of nitrate observations 2 ppm are 116 17 1 out of 678 the small number of well samples 2 ppm would increase uncertainties when models detect nitrates 2 ppm in high classes high very high also if there are more well observations with nitrate 2 ppm better prediction would be expected in the integrated aquifer vulnerability map high and very high vulnerability classes fig 8 include crop areas mainly corn soybean areas a primary cause of nitrate contamination in aquifers results from anthropogenic fertilization lin et al 2001 behm 1989 because most farmers consider nitrogen fertilizer to be cheap insurance against a crop failure farmers would obviously rather add too much nitrogen to their crops for increasing profits looker 1991 burkart and kolpin 1993 found that water samples from wells nearby corn soybean have a significantly larger frequency of excess nitrate than wells near other crops similarly corn soybean acreage is responsible for 11 times more nitrate contamination than acreage used as rangeland puckett 1994 department of commerce 1993 therefore fig 8 would explain the previous studies and this study for nitrate contamination in aquifers in terms of the nitrate contamination in aquifers the most effective approach to avoid health risks e g methemoglobinemia blue baby syndrome is to reduce application rate of fertilization and to check wells frequently further fertilizer application timing is a critical factor to cause groundwater degradation sullivan et al 2000 various best management practices bmps to prevent aquifers from nitrate contamination should be implemented and they would help reduce nitrate leaching from the soil profile into aquifers however bmps such as restriction of fertilizer application rate would help alleviate nitrate concentrations to biologically safe levels in previous studies in indiana various models were combined to overcome model limitations in an attempt to improve prediction lim 2001 used two models to predict pollutant losses to shallow groundwater in the white river basin from crop land pasture urban and forest the results of napra national agricultural pesticide risk analysis for pasture and crop land and l thia for urban forest and water were combined to reduce limitations of each model however there were still limitations for watershed scale modeling napra uses the gleams groundwater loading effects of agricultural management systems model a field scale model and thus does not represent some important watershed processes lim 2001 in the work presented herein the watershed model swat was used to estimate nitrate leaching to shallow aquifers to provide input data for geo ann which was utilized to predict nitrate concentrations in wells navulur 1996 used two models to estimate aquifer vulnerability of groundwater systems in indiana using a gis environment at a 1 250 000 scale navulur 1996 combined drastic with nleap nitrate leaching and economic analysis to predict well nitrate detections for aquifer vulnerability assessment the data scale used in navulur 1996 study was coarse 1 250 000 for field scale simulations the use of a single rainfall event and coarse data scale may result in underestimated low vulnerability areas and overestimated high vulnerability areas in this study high resolution data 1 24 000 were used further estimated nitrate leaching results from swat were used to predict nitrate concentrations in wells using geo ann as shown in navulur 1996 results approximately 91 8 of nitrate detections in wells 2 ppm are within high and very high vulnerability areas represent 56 9 of area as predicted by the combined model compared with navulur 1996 study the results presented herein had approximately 79 3 of nitrate detections in wells 2 ppm within high and very high represent 11 1 of area vulnerability areas as predicted by incorporating swat and geo ann 4 summary and conclusions streamflow and baseflow were calibrated and validated simultaneously by modified swat 2012 code and sufi 2 specifically simultaneous streamflow and baseflow calibration was implemented automatically using the modified swat 2012 code then calibration and validation of nitrate loads in streamflow were conducted using swat and estimated nitrate load data generated by observed nitrate load data and loadest for this reason data gaps of baseflow and nitrate loads at the each subbasin s outlet were filled by the what system and loadest based on the calibrated parameters for the main outlet at 9 additional usgs streamflow stations cross validation was conducted for more accurate estimation of streamflow in the uwrw after hydrology calibration and validation at the 10 usgs streamflow stations calibration and validation for nitrate loads was conducted at 4 epa fixed stations and 1 usgs water quality station model performances for streamflow and baseflow were satisfactory and model performance for nitrate loads was satisfactory as well for integrated aquifer vulnerability assessment geo ann was developed for training validation testing and predicting nitrate concentrations with the shapefile format as well as tabular format using ann geo ann provides a user friendly gui for optimizing the parameters of the ann architecture and training the network for the prediction two predefined input variables no3l and no3gw which are concerned with aquifer vulnerability that reflects pollution potential transport from the land surface to aquifers were retrieved by the calibrated swat swat and geo ann were utilized to find the relationship between the nitrates leached from the soil profile and nitrate concentrations in aquifers and to identify potential aquifer vulnerable areas through these simulation processes no3l the nitrate leached from the soil profile and no3gw the nitrate transported into streams from groundwater loading were estimated at the hru level using swat then with those two variables from swat and observed nitrate concentration data in wells nitrate concentrations in aquifers were estimated after training validation and testing nse r2 pbias 0 66 0 70 0 7 using the geo ann there was also a limitation with lack of data on nitrate concentrations in wells for training and validation to deal with lack of training and validation data the k fold cross validation method was implemented for the simulation of complex systems such as groundwater dynamics in aquifers this study indicates that machine learning is a suitable technique in predicting behavior of groundwater quality also with lack of detailed knowledge of the internal functions of complex systems and insufficient data for calibration and validation machine learning techniques would be an efficient method to identify the nonstationary patterns of variables of groundwater quality in the analysis approximately 79 3 of nitrate detections in wells 2 ppm were within high and very high vulnerability areas represent 11 2 of area as predicted by incorporating swat and geo ann moreover 9 5 of the nitrate detections were within moderate vulnerability class 22 7 of area and 8 6 of the nitrate detections were within low vulnerability class 41 2 of area 2 6 of nitrates in wells 2 ppm were detected within the very low vulnerability class 25 0 of area the results show that integrated aquifer vulnerability assessment performed well integrated aquifer vulnerability assessment considers groundwater characterization and risk analysis with tiered approaches for land and surface runoff contamination by soil chemicals and leaching of contamination to groundwater thus the overall assessment of aquifer vulnerability can be performed using the integrated aquifer vulnerability assessment technique provided in this study moreover this approach is expected to be an efficient guide for managing groundwater resources for policy makers and groundwater related researchers the models used in this study are data driven models therefore if more data i e nitrate concentration data in well point sources application timing and fertilizer application are available the approach suggested in this study would be improved and more accurate potential next steps in extending this work are to 1 conduct comparison of various machine learning algorithms e g convolution neural network bayesian linear regression and decision forest regression to better predict nitrate contamination in aquifers and 2 evaluate the application of bmps to reduce nitrate leaching from the soil profile into aquifers declaration of competing interest the authors declare no conflict of interest appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104602 
26091,nitrate contamination in groundwater was evaluated using the concept of integrated aquifer assessment by combining groundwater characterization and risk analysis with tiered approaches for land and surface runoff contamination by soil chemicals and leaching of contamination to groundwater in the upper white river watershed uwrw in indiana integrated aquifer vulnerability assessment was conducted using an integration of a distributed watershed model soil and water assessment tool swat and a machine learning technique geospatial artificial neural network geo ann the results indicate that integrated aquifer vulnerability assessment performed well based on the model performance nse r2 pbias 0 66 0 70 0 07 thus the overall assessment of aquifer vulnerability can be performed using the integrated aquifer vulnerability assessment technique provided in this study moreover this approach provides an efficient guide for managing groundwater resources for policy makers and groundwater related researchers keywords geo ann groundwater protection integrated aquifer vulnerability assessment nitrate contamination swat 1 introduction groundwater is an important water resource for many people in the world and is also a primary resource in indiana with 60 of the state s drinking water coming from groundwater alley et al 2002 solley et al 1998 moreover groundwater is a vital component of the local regional and global environment with groundwater feeding ecosystems as well as providing baseflow in rivers morris et al 2003 anthropogenic activities affect the quantity and quality of water resources including groundwater which offers human populations a number of services such as water for drinking and irrigation mukate et al 2018 contaminated groundwater has been identified in both urban and rural areas agriculture can cause groundwater degradation due to application of chemicals in agriculture areas if chemicals used in agriculture are slightly soluble in water they are less likely to result in groundwater contamination however ammonium which is a major fertilizer and manure which contains nitrogen is highly soluble and mobile in water and nitrate concentrations generated by nitrification of ammonium are widespread in both surface water and groundwater high nitrate concentrations are detected in some areas of midwest states mukate et al 2018 li et al 2019 kellogg et al 1994 to alleviate the negative effects of agriculture on groundwater resources and to maintain clean groundwater groundwater management has been facilitated by groundwater monitoring and modeling allouche et al 2017 jang et al 2017 unland et al 2015 groundwater monitoring can identify groundwater quality and quantity directly in real time and groundwater monitoring data can help enhance the planning sustainable development and management of groundwater resources however groundwater monitoring is complicated and an expensive process therefore many areas have sparse groundwater monitoring data alcalá et al 2015 compared with groundwater monitoring groundwater modeling is less complicated and less expensive also groundwater modeling allows assessment of broad areas both groundwater monitoring and modeling should be mutual and complementary for efficient evaluation of groundwater quality and quantity in broad areas i e regional scale or continental if the two systems are utilized simultaneously various hydrologic and water quality models have been developed based on conceptual statistical stochastic analytic physical and numerical models of surface water and groundwater systems which provide a means for predicting surface water and groundwater system responses to future conditions thus each model has its own purpose and characteristics a particular model is usually selected and used as a suitable tool depending on a research or project goal however various research efforts require interdisciplinary fields for example if a research effort or project considers analysis planning and management of a wide range of water resources and environmental problems related to surface water and groundwater the model or models not only should address surface water but should also deal with groundwater thus this often results in the demand for an integrated approach and coupled models for different systems kamp and savenije 2007 chen et al 2018 therefore sometimes when a model cannot address a research or project problem two or more models can be used or a combined model can be utilized to enhance the physical representation of hydrologic processes for better estimation many studies related to hydrologic and water quality have been conducted to determine efficient water management using a coupled model gao et al 2019 shin et al 2019 maxwell et al 2015 for efficient groundwater resources management integrated aquifer vulnerability assessments are required integrated aquifer vulnerability assessments are incorporated into a groundwater characterization and risk analysis with tiered approaches for land and surface runoff contamination by soil chemicals and leaching of contamination to groundwater the machine learning technique ann is a data driven model based on data experienced in the real world phenomena of a specific system in contrast to analytical or numerical models data driven models can be used to solve problems in the field of hydrology and water resources engineering where knowledge of the physical behavior of the system and data are limited solomatine and ostfeld 2008 thus for the simulation of complex systems data driven models are useful to define the patterns within the behavior of the system araghinejad 2014 ann is a black box model not a real black box that does not require detailed knowledge of the internal functions of a system to identify the complex dynamic and non linear relationships from given patterns by input and output however as a black box character model users have no control except providing input data and initial parameters such as learning rate maximum number of training cycle and target error ha and stenstrom 2003 machine learning is an artificial intelligence approach for improving the computer s ability to learn without being explicitly programmed the main goal of machine learning is to find relationships between the system state variables and to replace time consuming manual processes with automatic techniques that improve accuracy or efficiency by exploring and determining regularities in training data worland et al 2018 solomatine and shrestha 2009 as time passes nonstationary trends of hydrology and water resources time series have been exhibited more frequently yu and lin 2015 coulibaly and baldwin 2005 thus for accurate estimation of hydrology and water resources systems techniques which can simulate the nonstationary patterns of variables of hydrology and water resources are required many studies have already proven that machine learning is a suitable technique in predicting nonstationary behavior of hydrology and water resources systems nourani et al 2009 worland et al 2018 the objectives of this study were 1 to generate meaningful data related to groundwater pollution by swat as significant input variables for integrated aquifer vulnerability assessment 2 to develop geo spatial ann geo ann which is compatible with gis rs data formats for flexible hydrology and water quality modeling and 3 to develop and provide a model system for integrated aquifer vulnerability assessment by combining swat and geo ann for efficient groundwater management 2 materials and methods 2 1 study area the upper white river watershed uwrw latitude 39 29 51 n longitude 86 24 02 w is located in central indiana fig 1 the drainage area of the uwrw is 6944 km2 and the most dominant land use is agriculture 3160 km2 the uwrw is important for public drinking water supply because the uwrw includes more than 3508 km of streams numerous artificial lakes and 4 reservoirs the uwrw serves as a drinking water supply for part of the city of indianapolis which is indiana s largest city the water sources in the uwrw traditionally are individual wells to provide groundwater for residential commercial and industrial purposes tedesco et al 2011 tedesco et al 2005 fleming et al 1993 the uwrw was selected to identify hydrologic water quality and aquifer risk assessment because the uwrw has available streamflow u s geological survey usgs streamflow stations water quality environmental protection agency epa fixed stations and usgs monitoring well stations data for both streamflow and groundwater fig 1 and table 1 2 2 integrated aquifer vulnerability assessment 2 2 1 overview of integrated aquifer vulnerability assessment for overall aquifer vulnerability assessment integrated aquifer vulnerability assessment should be carried out in this study overall aquifer vulnerability assessment is called integrated aquifer vulnerability assessment which includes a groundwater characterization and risk analysis with tiered approaches for land and surface runoff contamination by soil chemicals and leaching of contaminates to groundwater aquifer intrinsic and transport properties are analyzed by integrated aquifer vulnerability assessment and integrated aquifer vulnerability is a function of land and surface runoff contamination and groundwater contamination fig 2 aquifer vulnerability assessment is usually conducted by overlay and index gis model or is implemented by numerical models jang et al 2017 akhavan et al 2010 2011 thirumalaivasan et al 2003 in this study integrated aquifer vulnerability assessment was conducted for efficient aquifer vulnerability assessment using a distributed watershed model swat and machine learning technique geo ann 2 2 2 configuration of integrated aquifer vulnerability assessment 2 2 2 1 overview of hydrologic and water quality model swat swat is a physically based distributed deterministic and long term continuous time model which is used to predict impact of management practices land use and land cover lulc change and climate change on hydrology and water quality on a watershed scale at a daily time step arnold et al 1998 neitsch et al 2011 swat represents the large scale spatial heterogeneity of the study area by dividing a watershed into subbasins the subbasin is the first level of subdivision of the watershed subbasins include geographic position information within the watershed and are spatially connected to one another neitsch et al 2011 the land area in a subbasin may be divided into hydrologic response units hrus which are the smallest computational units in swat hrus are parts of a subbasin that possess unique land use management and soil attributes hrus are created by one or more unique land use and soil combinations for each subbasin surface runoff soil water content crop growth nutrient cycles and erosion are simulated for each hru and then hrus are combined and calculated for the subbasin by a weighted value neitsch et al 2011 williams et al 1984 in this study variables related to nitrate movement were extracted for each hru and hru maps were created as input data for aquifer risk assessment using geo ann 2 2 2 2 swat input data the period of swat simulation is from 1990 to 2010 because the swat format climate data i e precipitation and temperature were available in this period provided by national climate data center ncdc and processed by the agricultural research service ars also there are sufficient water quality data from 1990 to 2010 for the uwrw as described in table 2 the primary input data for the swat simulation are digital elevation model dem soil lulc and weather data additionally for nitrate simulation the scheduled management operation data were prepared to consider application of fertilizer pesticides and manure table 3 a general strategy of management practices for corn soybean rotation is described in table 3 her et al 2016 the dem from u s geological survey usgs was used for watershed delineation and soil and lulc data from natural resources conservation service nrcs and natural agricultural statistics service nass were used with respect to the hydrological response in the watershed weather data include minimum and maximum temperature daily precipitation mean wind speed relative humidity and solar radiation which were obtained from national climate data center ncdc table 2 the spatial dem fig 3 a and soil maps in the uwrw fig 3 b were obtained from usgs elevation of the uwrw varies from 162 9 to 371 7 m twenty eight soil types are distributed in the study area such as 33 5 in013 crosby 24 in040 miami 6 3 in029 sawmill 6 in026 fox 5 2 in054 miamian and 24 9 other soil types lulc in the uwrw fig 3 c includes 45 5 agricultural fields 22 5 soybeans 22 2 corn and 0 8 others the mean annual precipitation is 1093 mm and the highest the lowest daily temperature and mean daily temperatures are 36 1 c 31 3 c and 10 8 c respectively fig 3 d shows well locations in the uwrw 678 well nitrate data were utilized for the training validation and testing of geo ann using swat outputs the well nitrate data were obtained from heidelberg university and the water quality portal wqp sponsored by the u s geological survey usgs the u s environmental protection agency epa and the national water quality monitoring council nwqmc min max mean values of well nitrate data are 0 1 18 29 1 22 mg l respectively with the input data table 2 hydrology surface water groundwater hydrology and water quality were simulated in the uwrw by swat then observed streamflow and estimated baseflow or observed baseflow if available from the usgs were used to calibrate and validate swat tile drainage was applied in areas where the land use is corn or soybean and the soil drainage condition is poorly drained boles et al 2015 jiang et al 2014 based on the previous studies for indiana watersheds boles et al 2015 jiang et al 2014 green et al 2006 and the technical report about tile drainage usda nrcs 2011 parameters related to the tile drainage in swat were used annual average 2008 2015 mass of point source pollutants related to no3 showed less than 1 ton year from the national pollutant discharge elimination system npdes in the uwrw observed no3 loads estimated no3 loads using loadest were approximately 2333 ton year the small amount of point source pollutants 0 04 of observed no3 loads would not significantly affect results of this study further point source data years were not matched for the periods of this study 1990 2010 thus point sources were not considered in the uwrw 2 2 2 3 hydrologic and water quality modeling using swat the land phase of the hydrologic cycle controls the amount of water sediment nutrient and pesticide loadings to the main channel and to the aquifers in each subbasin the hydrologic cycle simulated in swat is based on the water balance equation eq 1 also nitrate leaching from the soil profile to the aquifer and nitrate loading to streamflow contributed by baseflow are estimated in swat fig 4 neitsch et al 2011 1 s w t s w 0 i 1 t r d a y i q s u r f i e t a i q t i l e i q l a t i q g w i where s w t is the final soil water content mm at time t t is the time day s w 0 is the initial soil water content mm h2o r d a y i is the amount of precipitation on day i mm h2o q s u r f i is the amount of surface runoff on day i mm h2o e t a i is the amount of evapotranspiration on day i mm h2o q t i l e i is the amount of water removed from the layer by tile drainage on day i q l a t i is the amount of lateral flow released to the main channel on day i mm h2o and q g w i is the amount of return flow on day i mm h2o surface runoff q s u r f i was estimated using the soil conservation service scs curve number cn method eq 2 scs 1972 2 q s u r f i r d a y i 0 2 s 2 r d a y i 0 8 s i a 0 2 s s 25 400 c n 254 where i a is the initial abstraction mm h2o s is the retention parameter mm h2o and cn is the curve number dimensionless the penman monteith method was used to calculate the rate of evapotranspiration e t a i eq 3 monteith 1965 3 e t a i δ h n e t g ρ a i r c p e z 0 e z r a δ γ 1 r c r a λ where δ is the slope of the saturation vapor pressure temperature curve de dt kpa c h n e t is the net radiation mj m 2d 1 g is the heat flux density to the ground mj m 2d 1 ρ a i r is the air density kg m 3 c p is the specific heat at constant pressure mj kg 1 c 1 e z 0 is the saturation vapor pressure of air height z kpa e z is the water vapor pressure of air at height z kpa γ is the psychrometric constant kpa c 1 r c is the plant canopy resistance s m 1 r a is the diffusion resistance of the air layer aerodynamic resistance s m 1 and λ is the volumetric latent heat of vaporization energy required per water volume vaporized mj m 3 lateral flow was calculated with eq 4 neitsch et al 2011 4 q l a t i 0 0224 2 s w l y e x c e s s k s a t s l p φ d l h i l l where s w l y e x c e s s is the drainage volume of water stored in the saturated zone of the hillslope per unit area mm h2o k s a t is the saturated hydraulic conductivity mm h s l p is the average slope of the subbasin m m φ d is the drainage porosity of soil mm mm and l h i l l is the hillslope length m both shallow aquifer and deep aquifers are considered as groundwater storage total baseflow groundwater flow was calculated by total amount of water in the shallow aquifer and deep aquifer eqs 5 10 neitsch et al 2011 5 q g w i q g w s h i q g w d p i where q g w i is the total groundwater flow on day i mm h2o q g w s h i is the total groundwater flow in the shallow aquifer on day i mm h2o and q g w d p i is total groundwater flow in the deep aquifer on day i mm h2o 6 q g w s h i q g w s h i 1 exp α g w s h δ t w r c h r g s h i 1 exp α g w s h δ t where q g w s h i 1 is the groundwater flow from the shallow aquifer on day i 1 mm h2o α g w s h is the baseflow recession constant δ t is the time step day and w r c h r g s h i is the amount of recharge entering the shallow aquifer on a day i mm h2o 7 q g w d p i q g w d p i 1 exp α g w d p δ t w r c h r g d p i 1 exp α g w d p δ t where q g w d p i 1 is the groundwater flow from the deep aquifer on day i 1 mm h2o α g w d p is the baseflow recession constant δ t is the time step day and w r c h r g d p i is the amount of recharge entering the deep aquifer on a day i mm h2o 8 w r c h r g i 1 exp 1 δ g w w s e e p i exp 1 δ g w w r c h r g i 1 9 w r c h r g d p i β d p w r c h r g i 10 w r c h r g s h i w r c h r g i w r c h r g d p i where w r c h r g i is the amount of recharge entering the both aquifers on day i mm h2o δ g w is the drainage time of the overlaying geologic formations days w s e e p i is the total amount of water exiting the bottom of the soil profile on day i mm h2o w r c h r g i 1 is the amount of recharge entering the aquifers on day i 1 mm h2o β d p is the aquifer percolation coefficient dimensionless nitrate in the shallow aquifer may remain in the aquifer move with recharge to the deep aquifer move with groundwater flow into the main channel or be transported out of the shallow aquifer with water moving in the soil zone in response to water deficiencies nitrate in recharge to the shallow aquifer and deep aquifer on a given day was calculated with eqs 11 and 12 neitsch et al 2011 11 n o 3 r c h r g i 1 exp 1 δ g w n o 3 p e r c exp 1 δ g w n o 3 r c h r g i 1 where n o 3 r c h r g i is the amount of nitrate in recharge entering the aquifers on day i kg n ha δ g w is the delay time drainage time of the overlying geologic formation day n o 3 p e r c is the total amount of nitrate exiting the bottom of the soil profile on day i kg n ha n o 3 r c h r g i 1 is the amount of nitrate in recharge entering the aquifers on day i 1 mm h2o 12 n o 3 g w i n o 3 s h i 1 n o 3 r c h r g i q g w i a q s h i q g w i w r e v a p i w r c h r g d p i where n o 3 g w i is the amount of nitrate in groundwater flow from the shallow aquifer on day i kg n ha n o 3 s h i 1 is the amount of nitrate in the shallow aquifer at the end of day i 1 kg n ha q g w i is the groundwater flow into the main channel on day i mm h2o a q s h i is the amount of water stored in the shallow aquifer at the end of day i mm h2o w r e v a p i is the amount of water moving into the soil zone in response to water deficiencies on day i mm h2o w r c h r g d p i is the amount of recharge entering the deep aquifer on day i mm h2o 2 2 2 4 streamflow baseflow and nitrate calibration validation streamflow and baseflow were calibrated and validated simultaneously based on usgs streamflow and simulated baseflow by modified swat 2012 code jang et al 2018 and the web gis based hydrograph analysis tool what system lim et al 2005 which can separate baseflow from streamflow for calibration and validation of nitrate loads in streamflow daily or monthly nitrate loads are necessary however nitrate concentration data were available only for a few days each year thus load estimator loadest runkel et al 2004 was used to estimate mean monthly nitrate loads using estimated nitrate loads by loadest calibration and validation of nitrate loads at the main outlet were conducted for accurate estimation of nitrate concentrations in the aquifer calibration and validation of nitrate loads in streamflow are necessary because nitrate transport to streamflow and nitrate leaching to the aquifer are interdependent sullivan et al 2019 sequential uncertainty fitting algorithm version 2 sufi 2 abbaspour 2011 abbaspour et al 2004 was used to perform sensitivity and uncertainty analysis as well as calibration and validation because sufi 2 is an efficient autocalibration algorithm and provides a user friendly environment for swat kundu et al 2016 sloboda and swayne 2013 for evaluation of the model performance nash sutcliff efficiency nse eq 13 coefficient of determination r2 eq 14 and percent bias pbias eq 15 were selected as the objective function for calibration and validation of streamflow baseflow and nitrate loads krause et al 2005 gupta et al 1999 nash and sutcliffe 1970 the total simulation period was from 1990 to 2010 21 years with the first 4 years as the model warm up period calibration for streamflow and baseflow streamflow calibration 10 outlets and baseflow calibration 1 main outlet was implemented with the parameters related to water balance subsurface water surface runoff physical properties of soil and physical properties of channel calibration for nitrate loads nitrate loads calibration 5 outlets was conducted with the parameters associated with the nitrogen cycle calibration parameter ranges for streamflow baseflow and nitrate loads were defined based on the results of sensitivity analysis and previous studies yeo et al 2014 arnold et al 2012 zhang et al 2011 lam et al 2010 du et al 2006 after model calibration validation for streamflow baseflow and nitrate loads were performed with the calibrated parameters model performance ratings for streamflow baseflow and nitrate loads were evaluated with three quantitative statistics shown in table 4 moriasi et al 2007 engel et al 2007 singh et al 2004 van liew et al 2003 13 nse 1 i 1 n y i o b s y i s i m 2 i 1 n y i o b s y m e a n 2 where y i o b s is the ith observed data y i s i m is the ith simulated data y m e a n is the mean of observed data and n is the total number of observed data 14 r 2 i 1 n y i o b s y o b s y i s i m y s i m 2 i 1 n y i o b s y o b s 2 i 1 n y i s i m y s i m 2 2 where y i o b s is the ith observed data y o b s is the mean of observed data y i s i m is the ith simulated data y s i m is the mean of simulated data and n is the total number of observed data 15 pbias 1 i 1 n y i o b s y i s i m 100 i 1 n y i o b s where y i o b s is the ith observed data y i s i m is the ith simulated data and n is the total number of observed data 2 2 2 5 retrieval of variables of aquifer vulnerability using swat as mentioned earlier two variables in eqs 11 and 12 are related to nitrate contamination in aquifers in swat the first variable no3l n o 3 r c h r g i is the nitrate leached from the soil profile and the second variable no3gw n o 3 g w i is the nitrate transported into the main stream from the groundwater loading those two variables were selected from the swat outputs to estimate aquifer vulnerability the two variables were retrieved at hru levels by an hru extractor developed for this study then spatial input variable maps for geo ann were created to identify potential aquifer vulnerable areas 2 2 3 integrated aquifer vulnerability assessment 2 2 3 1 development of geo ann geo ann was developed to train validate and predict geospatial data as well as tabular data using artificial neural networks ann geo ann would be suitable for studies of hydrology and water quality modeling because those studies typically use geospatial data as an input or output to conduct spatial analysis and prediction geo ann provides a user friendly graphical user interface gui for optimizing the parameters of the ann architecture and training the network for the prediction the shapefile and tabular format can be directly used as input without converting to a new format and can be mapped with predicted values by ann various metrics for water resources and environmental management studies such as nse r2 and pbias are implemented to evaluate the performance of the user configured ann models cross validation techniques are included in the geo ann such as k fold and leave one out cross validation loocv cross validation is an effective validation method when the amount of calibration and validation data are limited shao and er 2016 wong 2015 because there are limited data for groundwater hydrology and water quality cross validation would be a supportive technique in groundwater hydrology studies the optimal neural network parameters were designed based on the nitrate observation data used in this study because the nitrate observation data are limited in most cases training and validation have to proceed with limited data however validation using limited data often fails to accurately estimate the performance of the designed prediction model causing overfitting or underfitting problems thus cross validation was used which partitions a sample data into different subsets of training validation and testing data a k fold cross validation was used as the cross validation technique which randomly divides the k number of subset of sample data in each run heaton 2008 as a rule of thumb the division ratios of training and validation are set as 70 and 30 individually rai et al 2019 once the sample data are split into these three sets the network performance is evaluated the performance of the geo ann is evaluated using the three metrics nse r2 and pbias before assigning the input and output data into the geo ann all data are normalized to fall in 0 1 0 9 using eq 16 because the normalization improves accuracy performance and speed of geo ann kalin et al 2010 sethi et al 2010 in this study the number of inputs and output are 2 and 1 and the number of neurons ranges from 2 to 4 for determining the number of layers 1 or 2 is are a typical number in a small or moderate size of the network for this study the whole application including training validation algorithms and gui for geo ann was developed using the neural network toolbox in matlab many matlab users mainly use matlab version 2016 2017 and 2018 therefore compatibility test for geo ann was conducted and successfully completed 16 x n o r m 0 1 x 0 x min x max x min where x n o r m is the normalized value x 0 is the observed value x min is the minimum value and x max is the maximum value the functionalities of the geo ann are summarized as follows 1 design of the ann layers number of hidden layers and number of neurons 2 selection of training optimization algorithm levenberg marquardt gradient descent or bayesian regularization beale et al 2016 3 selection of normalization methods pre conditioning normalization of the user defined range e g 0 1 0 9 0 1 or 1 1 4 functions for importing and exporting a raw shapefile or tabular format for training validation and prediction 5 various metrics for performance measurements for hydrology and water quality modeling nse r2 and pbias moriasi et al 2007 6 two cross validation techniques loocv and k fold cross validation heaton 2008 7 stopping criteria design maximum number of validation increases minimum performance value maximum number of training epochs iterations 8 enable gpu and or parallel computing capability for large ann training 2 2 3 2 analysis of aquifer vulnerability using geo ann for aquifer vulnerability assessment fig 5 two predefined input variables no3l and no3gw were retrieved from the swat simulation maps for nitrate leached from the soil profile and nitrate in groundwater were obtained through surface and groundwater hydrology and water quality simulation by swat no3l and no3gw were utilized as input data to geo ann that was developed in this study nitrate concentration data from 678 monitoring wells fig 6 were used for training validation testing of geo ann 562 data points were less than 2 ppm and 116 data points were greater than 2 ppm nitrate levels over 2 ppm were assumed to be caused by human activities because nitrate levels in aquifers under natural conditions are typically less than 2 ppm in indiana navulur 1996 thus a threshold value of background concentration was set at 2 ppm in this study integrated vulnerability assessment was conducted using nitrate detections 2 ppm as elevated n levels in order to fill the data gap and validate spatially distributed nitrate concentrations k fold cross validation was used for training and validation of geo ann many studies revealed that k fold cross validation is a reliable method when the number of training validation data are small shao and er 2016 wong 2015 kalin et al 2010 recommended model performance criteria for ann with two metrics i e nse and pbias in watershed modeling at a monthly time scale kalin et al 2010 model performance criteria for geo ann were modified by adding one more metric i e coefficient of determination r2 based on moriasi et al 2015 and kalin et al 2010 as shown in table 5 geo ann performance was evaluated using the three metrics i e nse r2 and pbias eqs 13 15 3 results and discussion 3 1 calibration and validation of streamflow baseflow and nitrate loads for the simulation period of 21 years 1990 2010 monthly mean streamflow is 85 7 m3 s and monthly mean baseflow is 35 9 m3 s according to the what system lim et al 2005 monthly mean baseflow accounted for 41 9 of monthly mean streamflow the comparison between the loadest estimated and usgs observed nitrate loads indicate nse of 0 84 r2 of 0 89 and pbias of 0 92 these values indicate that loadest estimated nitrate loads quite well model calibration and validation of streamflow baseflow and nitrate loads at the main outlet were conducted by adjusting twenty four parameters calibrated values were estimated with simultaneous streamflow and baseflow calibration and nitrate loads calibration with multi site flow and water quality stations hydrographs reproduced by swat between the observed and simulated monthly streamflow and baseflow at the main outlet flow 28 and wq 28 during the calibration and validation periods are shown in fig 7 a and b as shown in fig 7 a even though some peak and low streamflow values were underestimated the calibrated model performed well for monthly streamflow simulation the calibrated model underestimated during peak streamflow periods and the model slightly overestimated during some low streamflow periods streamflow during the validation period was better estimated than during the calibration period fig 7 b shows results of monthly baseflow simulation during the calibration and validation periods baseflow simulated with the calibrated model was estimated satisfactorily thus the result shows that the calibrated model simulates both streamflow and baseflow well it is noted that simultaneous streamflow and baseflow calibration are necessary for robust estimation of hydrological parameters based on the calibrated parameters from the main outlet of the uwrw cross validation for streamflow was conducted at 9 additional usgs streamflow stations even though some peak flows were underestimated all simulations replicated the observed streamflow well the calibrated parameters explained hydrology characteristics for the entire watershed after hydrology calibration and validation using 10 stations calibration and validation for nitrate loads were conducted fig 8 shows that even though the calibrated model simulated nitrate loads reasonably well the model underestimated nitrate loads for some points at wq 10 for 1993 wq 17 for 1998 and wq 28 for 1992 and 1993 there was underestimation because timing of fertilizer application was unknown and simulated streamflow was underestimated by swat for those periods the other reason for underestimated nitrate loads could be the uncertainties of nitrate monitoring data and loadest model performance for streamflow baseflow and nitrate load simulation at the main outlet was evaluated by nse r2 and pbias table 6 for monthly streamflow calibration and validation at the main outlet nse values range from 0 85 to 0 88 r2 values vary from 0 87 to 0 92 and pbias values range from 1 36 to 3 90 based on table 4 all simulation periods for streamflow total period calibration and validation are within the very good range indicating all simulation periods are acceptable for the monthly baseflow calibration and validation nse values total period calibration and validation vary from 0 63 to 0 65 r2 values range from 0 70 to 0 73 and pbias values vary from 13 8 to 16 7 the results of the baseflow calibration and validation show very good good and satisfactory ranges which mean all simulation periods for baseflow total period calibration and validation are acceptable the results of simultaneous streamflow and baseflow calibration indicate good performance for baseflow calibration as well as streamflow calibration for monthly nitrate load calibration and validation at the main outlet nse values of all simulation periods range from 0 51 to 0 72 r2 values vary from 0 58 to 0 77 and pbias values range from 13 9 to 20 1 based on table 4 all simulation periods are within the very good good and satisfactory ranges the calibrated model is acceptable for all simulation periods of streamflow baseflow and nitrate loads as shown in table 6 simulated streamflow using calibrated parameters at the main outlet replicated observed streamflow at the 9 usgs streamflow stations well based on table 4 all simulated streamflow at the 9 usgs streamflow stations are within the very good good and satisfactory ranges which mean all 9 flow simulations are acceptable nse values vary from 0 75 to 0 91 and 90 of nse are very good r2 values range from 0 77 to 0 92 and 90 of r2 are also very good pbias values vary from 16 4 to 15 9 and 50 of pbias are very good table 7 for accurate nitrate load estimation multi site calibration was performed to satisfy all nitrate load simulations at the 4 epa fixed stations and 1 usgs water quality station all nitrate load simulations are better than satisfactory nse values vary from 0 58 to 0 77 and 40 of nse are very good r2 values range from 0 60 to 0 79 and 40 of r2 are also very good pbias values vary from 3 5 to 29 7 and 80 of pbias are very good table 8 even though all simulations for nitrate loads are within acceptable ranges based on table 4 most pbias values indicate most simulated nitrate were underestimated 3 2 development and application of geo ann geo ann was developed using the neural network toolbox in matlab to build a new model to predict nitrate concentrations for integrated aquifer vulnerability assessment the new geo ann based model was created through the process of training validation and testing using two predefined input variables nitrate leached from the soil profile no3l and nitrate transported into streams from groundwater loading no3gw with the shapefile format as well as tabular format calculated and generated by swat the coupled modeling framework using swat and geo ann was utilized to find the relationship between the nitrates leached from the soil profile and nitrate concentrations in aquifers and to identify potential vulnerable areas in aquifers the network performance was validated using different combinations of the number of neurons and hidden layers within the ranges i e neuron ranges 2 to 4 and layer ranges 1 to 2 in all simulations the levenberg marquardt algorithm was used for training the network and a hyperbolic tangent sigmoid was implemented as the transfer function for hidden layers and the output layer the training process stops if the maximum number of incremental validations reaches more than 30 with two input variables no3l kg ha no3gw kg ha calculated by eqs 11 and 12 and observed well nitrate data the two input variables and one observed variable were trained validated tested with the parameters and structures of the geo ann to create the new model which can predict nitrate concentrations in wells then three performance metrics were produced as shown in table 9 final outcomes of model performance are calculated using the median value of all outcomes by the number of k fold cross validation among different combinations of the number of neurons and hidden layers 2 hidden layers and 2 neurons produced the optimal solutions of nse r2 and pbias during the testing nse r2 pbias for the testing shows 0 66 0 70 0 07 each value is median out of 1000 simulation results according to table 5 all model performance for the testing of the geo ann indicates better than satisfactory nitrate prediction for hrus which do not have observed nitrate data was conducted using the structure of 2 hidden layers and 2 neurons for each hidden layer hl nr nr 2 2 2 3 3 analysis of integrated aquifer vulnerability according to akhavan et al 2010 de paz et al 2009 and pohlert et al 2007 it is not easy to find correlation between the nitrate leached from the soil profile and the nitrate concentrations in aquifers because other factors such as groundwater age groundwater depth lateral flow and denitrification in the unsaturated zone also play a role in this study in order to reduce complexities in estimation of nitrate concentrations in aquifers swat and geo ann were utilized to find a relationship between the nitrates leached from the soil profile and nitrate concentrations in aquifers and to identify aquifer vulnerability areas no3l the nitrate leached from the soil profile mean value 3 86 kg ha yr and no3gw the nitrate transported into main stream from the groundwater loading mean value 0 18 kg ha yr were estimated for hrus by swat simulation then average 1990 2010 estimated annual nitrate concentrations in aquifers were predicted after training validation testing using the geo ann estimated nitrate concentration ranges from 0 to 12 25 mg l and these nitrate concentration values were also normalized from 0 to 1 by eq 16 for creating the integrated aquifer vulnerability map integrated aquifer vulnerability indices were divided into five classes 0 0 2 very low 0 2 0 4 low 0 4 0 6 moderate 0 6 0 8 high and 0 8 1 0 very high as shown in fig 9 and table 10 25 0 of the aquifer systems in the uwrw was within the very low vulnerability class and 41 2 of the area was estimated as low 22 7 within the moderate vulnerability class 8 8 within the high vulnerability class and 2 3 within the very high vulnerability class the integrated aquifer vulnerability results table 10 from swat and geo ann were validated with the well database the results indicated that approximately 79 3 of nitrate detections 2 ppm are within high and very high vulnerability classes represent 11 2 of the aquifer system areas in the uwrw as predicted by swat and geo ann moreover 9 5 of the nitrate detections were within the moderate vulnerability class 22 7 of the aquifer system area in the uwrw 8 6 of the nitrate detections were within the low vulnerability class 41 2 of the area in the uwrw and 2 6 of the nitrate detections were included in the very low vulnerability class 25 0 of the area in the uwrw table 10 for integrated aquifer vulnerability assessment 11 2 of nitrate detections 2 ppm were within the very low and low vulnerability areas the nitrates detected in the very low and low vulnerability areas might be caused by point sources application timing and excessive fertilizer application among other reasons which were not considered in this study these factors i e point sources application timing and excessive fertilizer application should be considered by adding additional data and modifying the models used in this study the number of nitrate observations 2 ppm are 116 17 1 out of 678 the small number of well samples 2 ppm would increase uncertainties when models detect nitrates 2 ppm in high classes high very high also if there are more well observations with nitrate 2 ppm better prediction would be expected in the integrated aquifer vulnerability map high and very high vulnerability classes fig 8 include crop areas mainly corn soybean areas a primary cause of nitrate contamination in aquifers results from anthropogenic fertilization lin et al 2001 behm 1989 because most farmers consider nitrogen fertilizer to be cheap insurance against a crop failure farmers would obviously rather add too much nitrogen to their crops for increasing profits looker 1991 burkart and kolpin 1993 found that water samples from wells nearby corn soybean have a significantly larger frequency of excess nitrate than wells near other crops similarly corn soybean acreage is responsible for 11 times more nitrate contamination than acreage used as rangeland puckett 1994 department of commerce 1993 therefore fig 8 would explain the previous studies and this study for nitrate contamination in aquifers in terms of the nitrate contamination in aquifers the most effective approach to avoid health risks e g methemoglobinemia blue baby syndrome is to reduce application rate of fertilization and to check wells frequently further fertilizer application timing is a critical factor to cause groundwater degradation sullivan et al 2000 various best management practices bmps to prevent aquifers from nitrate contamination should be implemented and they would help reduce nitrate leaching from the soil profile into aquifers however bmps such as restriction of fertilizer application rate would help alleviate nitrate concentrations to biologically safe levels in previous studies in indiana various models were combined to overcome model limitations in an attempt to improve prediction lim 2001 used two models to predict pollutant losses to shallow groundwater in the white river basin from crop land pasture urban and forest the results of napra national agricultural pesticide risk analysis for pasture and crop land and l thia for urban forest and water were combined to reduce limitations of each model however there were still limitations for watershed scale modeling napra uses the gleams groundwater loading effects of agricultural management systems model a field scale model and thus does not represent some important watershed processes lim 2001 in the work presented herein the watershed model swat was used to estimate nitrate leaching to shallow aquifers to provide input data for geo ann which was utilized to predict nitrate concentrations in wells navulur 1996 used two models to estimate aquifer vulnerability of groundwater systems in indiana using a gis environment at a 1 250 000 scale navulur 1996 combined drastic with nleap nitrate leaching and economic analysis to predict well nitrate detections for aquifer vulnerability assessment the data scale used in navulur 1996 study was coarse 1 250 000 for field scale simulations the use of a single rainfall event and coarse data scale may result in underestimated low vulnerability areas and overestimated high vulnerability areas in this study high resolution data 1 24 000 were used further estimated nitrate leaching results from swat were used to predict nitrate concentrations in wells using geo ann as shown in navulur 1996 results approximately 91 8 of nitrate detections in wells 2 ppm are within high and very high vulnerability areas represent 56 9 of area as predicted by the combined model compared with navulur 1996 study the results presented herein had approximately 79 3 of nitrate detections in wells 2 ppm within high and very high represent 11 1 of area vulnerability areas as predicted by incorporating swat and geo ann 4 summary and conclusions streamflow and baseflow were calibrated and validated simultaneously by modified swat 2012 code and sufi 2 specifically simultaneous streamflow and baseflow calibration was implemented automatically using the modified swat 2012 code then calibration and validation of nitrate loads in streamflow were conducted using swat and estimated nitrate load data generated by observed nitrate load data and loadest for this reason data gaps of baseflow and nitrate loads at the each subbasin s outlet were filled by the what system and loadest based on the calibrated parameters for the main outlet at 9 additional usgs streamflow stations cross validation was conducted for more accurate estimation of streamflow in the uwrw after hydrology calibration and validation at the 10 usgs streamflow stations calibration and validation for nitrate loads was conducted at 4 epa fixed stations and 1 usgs water quality station model performances for streamflow and baseflow were satisfactory and model performance for nitrate loads was satisfactory as well for integrated aquifer vulnerability assessment geo ann was developed for training validation testing and predicting nitrate concentrations with the shapefile format as well as tabular format using ann geo ann provides a user friendly gui for optimizing the parameters of the ann architecture and training the network for the prediction two predefined input variables no3l and no3gw which are concerned with aquifer vulnerability that reflects pollution potential transport from the land surface to aquifers were retrieved by the calibrated swat swat and geo ann were utilized to find the relationship between the nitrates leached from the soil profile and nitrate concentrations in aquifers and to identify potential aquifer vulnerable areas through these simulation processes no3l the nitrate leached from the soil profile and no3gw the nitrate transported into streams from groundwater loading were estimated at the hru level using swat then with those two variables from swat and observed nitrate concentration data in wells nitrate concentrations in aquifers were estimated after training validation and testing nse r2 pbias 0 66 0 70 0 7 using the geo ann there was also a limitation with lack of data on nitrate concentrations in wells for training and validation to deal with lack of training and validation data the k fold cross validation method was implemented for the simulation of complex systems such as groundwater dynamics in aquifers this study indicates that machine learning is a suitable technique in predicting behavior of groundwater quality also with lack of detailed knowledge of the internal functions of complex systems and insufficient data for calibration and validation machine learning techniques would be an efficient method to identify the nonstationary patterns of variables of groundwater quality in the analysis approximately 79 3 of nitrate detections in wells 2 ppm were within high and very high vulnerability areas represent 11 2 of area as predicted by incorporating swat and geo ann moreover 9 5 of the nitrate detections were within moderate vulnerability class 22 7 of area and 8 6 of the nitrate detections were within low vulnerability class 41 2 of area 2 6 of nitrates in wells 2 ppm were detected within the very low vulnerability class 25 0 of area the results show that integrated aquifer vulnerability assessment performed well integrated aquifer vulnerability assessment considers groundwater characterization and risk analysis with tiered approaches for land and surface runoff contamination by soil chemicals and leaching of contamination to groundwater thus the overall assessment of aquifer vulnerability can be performed using the integrated aquifer vulnerability assessment technique provided in this study moreover this approach is expected to be an efficient guide for managing groundwater resources for policy makers and groundwater related researchers the models used in this study are data driven models therefore if more data i e nitrate concentration data in well point sources application timing and fertilizer application are available the approach suggested in this study would be improved and more accurate potential next steps in extending this work are to 1 conduct comparison of various machine learning algorithms e g convolution neural network bayesian linear regression and decision forest regression to better predict nitrate contamination in aquifers and 2 evaluate the application of bmps to reduce nitrate leaching from the soil profile into aquifers declaration of competing interest the authors declare no conflict of interest appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104602 
26092,the coupled carbon c cycle across terrestrial and aquatic environments at the watershed scale has been identified as an important but poorly constrained component of the global carbon budget here we extended soil and water assessment tool swat with coupled riverine particulate organic carbon poc and dissolved organic carbon doc modules referred to as swat c hereafter results show that swat c reproduced daily poc and doc fluxes well in two watersheds in the northeastern united states we found that swat c tended to underestimate high flows and peak doc and poc fluxes uncertainty analysis indicated flux uncertainties associated with poc and doc simulations were larger than those for flow simulation sensitive parameters controlling poc and doc biogeochemical processes were identified along with how these parameters influence mechanisms underlying c cycling we anticipate that the tool developed and applied here will inform c related ecosystem services in watershed assessment and planning keywords dissolved organic carbon particulate organic carbon aquatic carbon cycle terrestrial aquatic ecosystem coupling watershed 1 introduction carbon c cycling across terrestrial and aquatic environments has been identified as an important but poorly constrained component of the global c budget butman et al 2016 regnier et al 2013 in addition organic carbon oc in aquatic ecosystems impacts the health of human and aquatic life for example dissolved organic carbon doc can complex with heavy metals and influence their solubility and mobility lawlor and tipping 2003 weng et al 2002 impact drinking water safety through forming toxic disinfection byproducts dbp in the process of water treatment chowdhury et al 2009 evans et al 2005 and alter water chemistry e g acidity and nutrient supply eshleman and hemond 1985 stewart and wetzel 1981 thus the increase of oc concentrations in aquatic ecosystems has been a growing concern for its wide environmental impacts evans et al 2005 as such the world health organization who 2011 and u s environmental protection agency epa 2018 require monitoring oc in surface waters to ensure water quality and security numerical models such as those c cycle models coupled with general circulation models brovkin et al 2002 friedlingstein et al 2006 have been used to explore the magnitude of c fluxes and make predictions as affected by human activities mcguire et al 2001 and climate changes at the global scale berthelot et al 2005 cramer et al 2001 ito 2005 prentice et al 2001 those global scale studies provide insights on future climate change scenarios but they tend to lack detailed information on local c fate and environmental impacts and as a result they are not informative for local policy making and scientific research ecosystem models can also be used to simulate the c balance for a single homogeneous ecosystem e g a forest stand or grassland churkina et al 2003 clivot et al 2019 running and coughlan 1988 zhu et al 2003 or mosaic ecosystems at the regional scale huang et al 2017 parton et al 1992 sitch et al 2003 but they often omit c exchange between terrestrial and aquatic ecosystems butman et al 2016 therefore watershed scale models that explicitly represent regional local hydrology and nutrient cycle processes across both terrestrial and aquatic ecosystems are needed to quantify regional c cycling most watershed scale models can simulate terrestrial c fluxes with different complexity of biogeochemical processes however the aquatic c cycle is often oversimplified in most watershed scale models arnold et al 1998 bicknell et al 2001 laflen et al 1991 leonard et al 1987 smith et al 1997 young et al 1989 on the other hand many water quality models were designed specifically to simulate hydrodynamics and biochemical and physical conversion processes in streams and lakes adiyanti et al 2016 rauch et al 1998 wang et al 2013 wei et al 2018 but they require inputs from terrestrial ecosystems to drive their simulations of aquatic processes chapra et al 2003 cole and wells 2006 hamrick 1996 knightes et al 2019 wool et al 2006 clearly there is a need to improve watershed scale models with respect to hydrological and biogeochemical processes across terrestrial and aquatic ecosystems particularly quantification of c fluxes between terrestrial and aquatic environments requires that 1 the model should simulate terrestrial c cycle processes including c uptake by photosynthesis release by plant and soil respiration as well as disturbance processes e g human activities from the perspective of simulation the model can track the speed and magnitude of c cycling through different reservoirs including organic molecules in living and dead organisms soil organic matter and co2 in the atmosphere inclusion of land use change and management practices is necessary for a complete assessment of c cycle under the influence of human activities and 2 the model should also represent aquatic c cycle processes including generation and transportation of inorganic c ic dissolved organic c doc and particulate organic c poc from land to water bodies through runoff leaching erosion processes and biogeochemical transformation processes between those different forms of c in freshwater chapra et al 2003 yang et al 2016 2017 the soil and water assessment tool swat model arnold et al 1998 is a continuous physically based and widely used watershed scale model and has been successfully tested for simulating watershed hydrology land surface water and heat exchange and nutrient cycles across terrestrial and aquatic environments in a wide range of watersheds across the globe abbaspour et al 2017 qi et al 2017a 2017b srinivasan et al 2010 zhang et al 2008 swat has also been widely used to assess effectiveness of best management practices bmps as affected by land use and climate change betrie et al 2011 bracmort et al 2006 ficklin et al 2009 li et al 2014 liang et al 2019 qi et al 2018a zhang et al 2017 it analyzes small or large watersheds by discretizing them into subbasins which are then further subdivided into hydrological response units hrus with homogeneous land use soil properties and slope the water balance i e surface and subsurface runoff percolation and base flow and evapotranspiration and transmission losses crop growth and nutrient cycling processes are simulated for each individual hru water flow sediment and nutrient loadings are then routed through channels ponds and reservoirs to the watershed outlet sonka et al 1986 sonka et al 1986 recent development of the swat model included a c module i e swat c to simulate c cycle processes in terrestrial environments yang and zhang 2016 zhang 2018 zhang et al 2013 and doc cycling in river networks du et al 2019 however the representation of poc cycling in aquatic environments has not been examined within the swat c modeling framework in this study we aimed to 1 develop a poc module within the frame work of swat 2 integrate the poc module with the previously developed doc module and terrestrial c module 3 evaluate performance of the integrated model on poc and doc simulation in two watersheds in the northeastern united states and 4 conduct parameter sensitivity and model uncertainty analyses with the intent to identify important factors regulating coupled terrestrial aquatic c cycling and identify avenues for future application 2 methods and materials 2 1 description of swat c the swat c model yang and zhang 2016 zhang 2018 zhang et al 2013 following the century model parton et al 1994 separates the soil organic matter som and residue into five pools swat c simulates addition decomposition transformation and removal of each som residue pool present in surface and subsurface soil layers as well as soil organic c soc loss by multiplying the amount of soil erosion with concentration of soc in mineral soils and an enrichment ratio a recent study developed and tested the doc module in the forested cannonsville watershed in upper new york du et al 2019 which estimates doc mass production in soils controlled by a liquid soil partition coefficient soc content and other environmental factors izaurralde et al 2006 leonard et al 1987 soil doc can be removed from soil layers by surface runoff lateral flow and percolation to shallow aquifers a doc pool in the shallow aquifer is also added in that study which receives doc recharge from the bottom soil layer and transports doc to streams via baseflow the algorithms depicting riverine doc processes are obtained by adapting and combining the kinetics from qual2k chapra and pelletier 2003 and ce qual w2 cole and wells 2006 in general riverine doc is represented in two doc pools i e refractory and labile doc rdoc and ldoc respectively which undergo complex reactions as regulated by multiple biotic and abiotic factors along river networks the doc module allows swat c to satisfactorily estimate overall soil doc production and transport to streams and captures daily doc fluxes at the watershed outlet du et al 2019 for poc modeling swat c simulates soil erosion using the modified universal soil loss equation musle neitsch et al 2011 williams and berndt 1977 which is further coupled with a poc enrichment method to estimate the amount of lateral soc through soil erosion zhang 2018 however the poc cycling processes in swat c has not been described and tested 2 2 incorporating riverine poc cycling processes into the swat c model we have incorporated an in stream poc module into swat c to depict the fate of poc discharged into rivers and routed through the stream network the riverine poc module simulates the transport and transformation of terrestrially and internally through algae growth and die off processes derived poc in river networks both phytoplankton and bottom algae growth are represented in the module together the poc and doc processes modeled in both terrestrial and aquatic ecosystems are illustrated in fig 1 the riverine poc module receives poc loadings from local upland and upstream reaches and routes the poc to downstream reaches after calculating the poc transport and reactions in the water the algorithms of riverine poc processes are derived from the kinetics of poc as described in the ce qual w2 cole and wells 2006 and qual2k chapra et al 2003 models within the module riverine poc is represented in refractory and labile poc i e rpoc and lpoc and the kinetic changes of lpoc and rpoc in water column with respect to time are computed as follows 2 2 1 lpoc kinetics mortality of floating and bottom algae increases lpoc while settling to the bed sediments causes losses the transformation kinetics decreasing lpoc stocks include lpoc dissolution to labile dissolved organic c ldoc lpoc decay to ic and lpoc hydrolysis to rpoc the kinetic changes of lpoc are computed as follows 1 lpoc t f lpocp k dp t r ca a p floating algal mortality ap lpoc 1 h f lpocb k db t r cb a b bottom algal mortality ab lpoc k l p o c t l p o c lpoc dissolution to ldoc lpoc ldoc k l p o c d e c t l p o c lpoc decay to ic lpoc ic k l r p o c t l p o c lpoc hydrolysis to rpoc lpoc rpoc v l p o c h l p o c lpoc settling lpoc bed sediment where lpoc is labile particular organic c concentration mg c l 1 f lpocp is the fraction of algal mortality into lpoc 0 1 0 r ca is the c to algae biomass ratio mg c mg d a p is the floating algal biomass mg d l 1 k dp is the algae mortality rate d 1 v lpoc is the lpoc settling velocity m d 1 k lpoc is the dissolution rate of lpoc to ldoc d 1 k rpoc dec is the decay rate of lpoc to ic d 1 k lr poc is the decay rate of lpoc to rpoc d 1 f lpocb is the fraction of bottom algal mortality into lpoc 0 1 0 h is the water depth m calculated by swat s routing algorithm k db is the bottom algae mortality rate d 1 r cb is the c to biomass ratio in benthic algae mg c mg d and a b is the bottom algae biomass g d m 2 2 2 2 rpoc kinetics rpoc increases from the mortality of floating and bottom algae and loses via settling to the bed sediment layer other rpoc transformation processes include lpoc hydrolysis to rpoc rpoc dissolution to ldoc and rpoc decay to ic the kinetic changes of rpoc are computed as follows 2 rpoc t f rpocp k dp t r ca a p floating algal mortality ap rpoc 1 h f rpocb k db t r cb a b bottom algal mortality ab rpoc k l r p o c t l p o c lpoc hydrolysis to rpoc lpoc rpoc k r p o c t r p o c rpoc dissolution to ldoc rpoc ldoc k r p o c d e c t r p o c rpoc decay to ic rpoc ic v r p o c h r p o c rpoc settling rpoc bed sediment where rpoc is the refractory particular organic c concentration mg c l 1 f rpocp is the fraction of algal mortality into rpoc 0 1 0 v rpoc is the rpoc settling velocity m d 1 k rpoc is the dissolution rate of rpoc to ldoc d 1 k rpoc dec is the decay rate of rpoc to ic d 1 and f rpocb is the fraction of benthic algal mortality into rpoc 0 1 0 2 2 3 bottom algae processes we used the bottom algae photosynthesis respiration and death processes from the qual2k model to simulate the mass balance of bottom algae which can influence the dynamics of poc and doc in the water column more details of these processs are described by chapra and pelletier 2003 2 3 swat c parameterization we integrated poc cycling processes into the swat c model du et al 2019 zhang et al 2013 to achieve the capability of simulating watershed poc fluxes from terrestrial to aquatic ecosystems within the swat c model du et al 2019 identified six sensitive parameters controlling doc cycling as shown in table 1 these parameters are doc percolation coefficient β doc which specifies the concentration of doc in surface runoff as a fraction of the concentration in percolation the liquid solid partition coefficient k oc which determines the production of doc in soil solution the decay rate of ldoc to rdoc k lr doc which controls the transformation rate from ldoc to rdoc the doc half life in a shallow aquifer hl gw which governs decay of doc in the shallow aquifer and the mineralization rates of ldoc and rdoc k ldoc and k rdoc respectively the poc module developed in the present study has eight calibration parameters table 1 the poc enrichment ratio er poc which is defined as the ratio of the concentration of poc in eroded soils to the concentration of soc in the soil surface layer the lpoc and rpoc settling velocity v lpoc and v rpoc respectively which control the deposition of poc the dissolution rates for lpoc to ldoc and rpoc to ldoc k lpoc and k rpoc respectively which determine the transformation from poc to doc the decay rates of rpoc to ic and lpoc to ic k rpoc dec and k rpoc dec respectively which determine the transformation rate from poc to ic and the decay rate for lpoc to rpoc k lr poc which control the transformation between lpoc and rpoc 2 4 study area and data collection we used two adjacent watersheds defined by us geological survey usgs gauging stations at the tuckahoe creak near ruthsburg md usgs 01491500 and the choptank river near greensboro md usgs 01491000 these two watersheds are referred to as the tuckahoe watershed tw 220 7 km2 and greensboro watershed gw 290 1 km2 respectively fig 2 they are located at the headwaters of the choptank river watershed crw in the coastal plain of the chesapeake bay watershed fig 2 in tw the major land uses are agriculture 54 0 and forestry 32 8 while gw has a lower percentage of agricultural area 36 1 and a higher percentage of forest 48 3 tw is dominated by well drained soils 56 1 hydrologic soil group hsg a b of which 69 5 are on croplands lee et al 2016 in contrast a large portion of soils 74 5 in gw are poorly drained hsg c d and most of those soils 67 2 are on croplands ator and denver 2015 the major cropland soil types in the crw are the othello soil series fine silty and the mattapex soil series fine silty othello soils are poorly drained with moderately slow permeability while mattapex soils are moderately well drained with moderate or moderately slow permeability hively et al 2009 in addition the land in the crw is relatively flat with most areas having less than 2 slopes the study area is characterized by a temperate and humid climate with an average annual temperature of 15 4 c and precipitation is evenly distributed throughout the year with an average annual amount of ca 1200 mm ator et al 2005 yeo et al 2014 high quality climate soil and land use data are essential for credible watershed modeling we used a soil map based on the usda natural resources conservation service nrcs soil survey geographic database ssurgo a 10 m light detection and ranging lidar based digital elevation model provided topographic information the land use map and the scheduling of crop rotations were generated using 2008 2012 data from the usda national agriculture statistics service nass cropland data layer cdl daily weather inputs for swat including precipitation temperature solar radiation relative humidity and wind speed were derived from high resolution 1 8 nasa north american land data assimilation system 2 nldas2 climate forcing data xia et al 2012 that have been proven to help improve swat hydrologic modeling as compared with using sparsely distributed rain gauges qi et al 2019b for more information regarding swat model setup for these two watersheds please refer to lee et al 2016 coincident with the ruthsburg and greensboro usgs gauging stations we conducted monitoring of riverine total organic c toc and doc concentrations for tw and gw respectively fig 2 we employed in situ instrument packages containing full spectrum 200 700 nm spectrophotometer probes s can instruments vienna austria which perform water quality sampling at 30 min intervals we used multiwavelength calibrations for toc and doc in stream water avagyan et al 2014 the fine time resolution toc and doc data were aggregated to a daily time scale for swat c calibration and validation the concentrations of toc and doc were further multiplied with observed flow rate to estimate fluxes in units of kg c day 1 poc was derived by subtracting doc from toc 2 5 model calibration sensitivity and uncertainty analysis the sampling period jan 2014 oct 2015 was divided into calibration 2014 and validation 2015 periods before the calibration period we used a two year 2012 2013 warm up period to initialize the swat c model we chose the most frequently calibrated parameters for daily flow rate based on the swat calibration literature abbaspour et al 2007b arnold et al 2012 and previous studies conducted in the study watersheds lee et al 2016 2017 we employed a multi step procedure to calibrate poc and doc loads the first step was to calibrate streamflow which is the key for the following water quality calibration sediment load needs to be calibrated before poc load calibration because poc load is closely associated with soil erosion on land surfaces and transport in streams following poc calibration doc load is calibrated in the present study we did not have sediment data to directly calibrate sediment load as a result when calibrating poc we adjusted sediment related parameters together with poc controlling parameters table 1 lists all calibration parameters for daily flow rate sediment poc and doc loads as well as their value ranges the sequential uncertainty fitting algorithm version 2 sufi 2 method in swat cup abbaspour et al 2007a was used to conduct calibration for daily flow rate and poc and doc loads it was also used to conduct parameter sensitivity and uncertainty analysis we used the global sensitivity analysis method for sensitivity analysis for different sets of parameters corresponding to daily flow rate and poc and doc loads specifically we considered four scenarios regarding parameter sensitivity for poc and doc loads 1 sensitivity analysis of poc parameters for poc load 2 sensitivity analysis of sediment and poc parameters for poc load 3 sensitivity analysis of doc parameters for doc load 4 sensitivity analysis of poc and doc parameters for doc load scenario 2 is aimed to investigate the impact of erosion and sediment transport resuspension and deposition processes on poc fate and scenario 4 attempts to analyze the impact of poc processes on doc fate the global sensitivity analysis approach used was a multiple regression system with latin hypercube samples by means of objective function values used in calculating the response parameter sensitivities given as 3 g α i 1 m β i b i where g is the objective function value α and β i are regression coefficients b i is the calibrated value of the ith parameter and m is the number of parameters considered for example when conducting sensitivity analysis of poc parameters for poc load scenario 1 eight m 8 parameters were selected table 1 to calculate the objective function value with different combinations of sampled parameter values we employed the nash sutcliffe coefficient ns nash and sutcliffe 1970 as the objective function g ns because it is a commonly used goodness of fit coefficient in hydrologic modeling studies further model performance evaluation criteria have been established by moriasi et al 2007 for ns a student t test was used to quantify the statistical significance of each parameter with a p value 0 05 indicating a parameter as sensitive in the present study the global sensitivity analysis approach estimates the change in the objective function resulting from changes in each parameter while all other parameters are changing abbaspour et al 2007a and as a result it does not provide an absolute measure of the sensitivity but rather the relative sensitivity thus we also provided parameter sensitivity rankings based on p value for additional sensitivity analysis in sufi 2 the uncertainties in model structure parameters and input data are not separately estimated but are attributed as total model uncertainty to the parameters abbaspour et al 2007a the latin hypercube sampling method is used in sufi 2 for vast parameter value combinations and resultant model simulations are used to calculate the percentage of measured data bracketed by the 95 percent prediction uncertainty often referred to as 95ppu which is measured by the p factor the range of the p factor varies from 0 to 1 0 with values close to 1 0 all observations bracketed by the prediction uncertainty indicating very strong model performance and small prediction uncertainty the r factor is the average thickness of the 95ppu bands divided by the standard deviation of the observed data an r factor varying between 0 and 1 0 indicates acceptable prediction uncertainty estimation abbaspour et al 2007a abbaspour 2013 in general a trade off between p factor and r factor exists for model uncertainty evaluation a larger p factor can be achieved at the expense of a larger r factor and vice versa a model with a balance between the two factors can provide acceptable prediction uncertainty qi et al 2019a 2 6 model performance evaluation for model evaluation we used a combination of different efficiency criteria complemented by the assessment of the absolute or relative error as recommended by krause et al 2005 specifically three well known statistical criteria i e percent bias pbias coefficient of determination r2 and ns are used here and given as 4 p b i a s 100 o a v g p a v g o a v g 5 n s 1 i 1 n o i p i 2 i 1 n o i o a v g 2 6 r 2 i 1 n o i o a v g p i p a v g i 1 n o i o a v g 2 i 1 n p i p a v g 2 0 5 where o i and p i are the observed and predicted values and o avg and p avg are the average of the observed and predicted values respectively the r2 ranges from 0 to 1 and quantifies the proportion of explained variance in observed data with higher values indicating less error variance the ns is a normalized statistic and estimates the relative magnitude of the residual variance as compared to the observed nash and sutcliffe 1970 and demonstrates how well the plot of observed versus simulated data fits the 1 1 line and pbias has the ability to indicate poor model performance and measures the average tendency of the simulated data to be larger or smaller than observed data low magnitude values of pbias indicate accurate model simulation positive values indicate model underestimation bias and negative values indicate model overestimation bias gupta et al 1999 3 results and discussion 3 1 model performance during calibration and validation model performance on daily flow rate and poc and doc loads as indicated by r2 ns and pbias during calibration and validation are shown in table 2 during calibration we tried to ensure that the model performance was satisfactory according to model performance evaluation criteria for watershed models summarized by moriasi et al 2007 model simulation can be judged as satisfactory if ns 0 50 at a monthly time step while 25 pbias 25 for streamflow and 70 pbias 70 for nitrogen n and phosphorus p here we assumed the criteria for n and p are applicable to poc and doc simulation assessment as the criteria for evaluating model performance for c fluxes have not been established due to few available studies on watershed scale c cycling model performance criteria based on ns moriasi et al 2007 was also assumed to be applicable to r2 because both statistics are based on squared differences between observation and prediction krause et al 2005 overall the calibration results demonstrated satisfactory model performance for three water quantity and quality variables based on all three statistics table 2 one exception was pbias for simulated streamflow in gw which was 37 3 greater than the 25 threshold during validation model performance on poc and doc fluxes in the tw and streamflow in the gw was satisfactory in terms of all three statistics except for pbias for streamflow in gw which was 32 9 ns and r2 values during validation were less than 0 5 threshold for gw it is worth noting that this criterion was recommended for model evaluation at a monthly time step while the present study evaluated model performance at a daily time step because increasing temporal resolution of simulations from the monthly time step to the daily time step often results in poorer model performance the model performance criteria for monthly evaluation should be relaxed slightly for the daily time step for example the satisfactory criteria for n and p may be ns 0 4 or even ns 0 3 moriasi et al 2007 simulated vs observed daily flow rate and poc and doc loads are shown in figs 3 and 4 for the tw and gw respectively in general simulated hydrographs of streamflow and poc and doc fluxes matched observations well the model tended to underestimate streamflow during both calibration and validation periods in the two watersheds indicated by positive pbias values in table 2 that is the model underestimated many high flows which were dominated by surface runoff it has been reported that the swat model tended to retain less soil water leading to underestimated surface runoff especially under wet conditions mapfumo et al 2004 qi et al 2018b as a result peak doc fluxes were also underestimated figs 3c and 4c because of the close relationship between streamflow and doc fluxes du et al 2019 poc load is not only related to streamflow but also associated with soil erosion and sediment transport during poc calibration we also considered parameters controlling sediment processes on land and in streams table 3 simulated poc and doc loads in general followed observed data patterns and the model can capture seasonal variations well as with streamflow simulations poc fluxes during some high flow events were underestimated as our observations span a two year period we used only one year of data for calibration and less than a year data for validation despite continuous poc and doc observations the relatively short observation for model evaluation may not allow us to derive conclusions that can be extrapolated to longer time periods this is because longer time periods will likely include more complex hydrologic conditions calibrated parameter values with respect to different water quantity and quality variables in the two study watersheds are shown in table 3 we will discuss more about parameter calibration in conjunction with parameter sensitivity analyses in section 3 3 3 2 model uncertainty analysis the 95ppu bands for daily flow rate and poc and doc loads are shown in figs 5 and 6 for tw and gw respectively the figures show that many observed peak flows were out of the 95ppu bands further confirming the difficulty in simulating peak flows since poc and doc cycling are closely coupled with hydrological processes we observed similar underestimation patterns in doc and poc load simulations during peak flow events table 4 lists the p factor and r factor values for estimated 95ppu bands for streamflow and poc and doc fluxes respectively the p factor values indicate that the 95ppu bands include 81 93 observed daily flow rates 55 91 observed poc loads and 31 68 observed doc loads during the periods in the two study watersheds in general decreases in p factor values from streamflow to c results indicate that the uncertainty associated with poc and doc modeling is higher than that for streamflow simulation this is not a surprise because streamflow simulation is mainly controlled by hydrologic processes but poc and doc simulations are governed by both hydrological and biogeochemical processes the increase in complexity is a source of additional difficulty in simulating poc and doc accounting for the decrease in p factor values as compared with streamflow simulation performance according to abbaspour 2013 an r factor less than 1 0 indicates a model has an acceptable prediction uncertainty estimation in this study r factor values were all less than 1 0 for daily flow and poc and doc fluxes in calibration and validation periods except for streamflow during calibration in tw and doc during validation in gw table 4 3 3 model parameter sensitivity analysis table 5 lists the p value and sensitivity ranking of different parameters for simulating different water quantity and quality variables the most sensitive parameters for daily flow rate were cnop1 surlag cnop2 and ch k2 which controlled the surface runoff yield and routing in the two study watersheds see table 1 for variable explanation calibrated values of cnop1 and cnop2 were greater for gw than those for tw table 3 respectively reflecting the fact that a large portion of soils in gw are poorly drained compared with soils in tw the calibrated value of surlag was less for gw than that for tw table 3 indicating larger portion of surface runoff lagged in the gw which reflects gw has higher percentage of forest mainly forest wetlands than tw snowmelt parameters such as sftmp were also among the top sensitive parameters for daily flow rate in both watersheds which indicates snow affects hydrological processes in the study area especially during snowmelt seasons groundwater parameters alpha bf and gw delay were sensitive for flow rate in tw and gw respectively indicating the importance of groundwater contribution to streamflow we found that calibrating cnop curve number for operation separately for different management practices instead of only calibrating cn2 improves seasonal flow rate simulation qi et al 2017b calibrated cnop1 curve number for planting operation values were less than calibrated cnop2 curve number for harvesting operation values in both watersheds table 3 which reflects the transition of land surface hydrological conditions from planting to harvesting sensitivity of ch k2 in both study watersheds suggested that stream water may have a close relationship with underlaying groundwater system in the study areas the calibrated value of ch k2 was greater for tw than that for gw which may be explained by well drained condition in tw for poc load v rpoc and er poc were the most sensitive parameters among the eight poc controlling parameters in the two study watersheds table 5 v lpoc was also relatively sensitive in gw er poc controls the magnitude of poc transported from soil surface and v rpoc and v lpoc affects the settling velocity for rpoc and lpoc in streams respectively the calibrated value of er poc was greater for gw than that for tw table 3 which is consistent with the measurement mean daily poc load at the outlet of the gw is 0 075 kg ha 0 068 kg ha for tw note that the calibrated values of v rpoc were very close for tw and gw table 3 suggesting similar topographical climatic and vegetational conditions between the two study watersheds when combined with parameters related to sediment load v rpoc and er poc remained the most sensitive parameters for poc load in the two study watersheds table 5 this result indicates that poc load in the two watersheds are controlled by the poc production and transport in both terrestrial and aquatic systems soil erosion related parameters usle k and usle p were relatively sensitive compared with other sediment parameters table 5 these results imply that sediment parameters had secondary influences on poc load simulation in the two study watersheds when applied to other watersheds sediment parameters may be also considered for calibration of poc load for doc load β doc was the most sensitive parameter in both study watersheds table 5 parameter k ldoc k oc and hl gw were also sensitive either in tw or gw parameters k oc and β doc control the total magnitude of doc generated and transported from upland soils parameters k ldoc and hl gw affect doc reactions in the stream and groundwater respectively these results indicate that doc loads in the two study watersheds are controlled by the doc production and transport in both terrestrial and aquatic environments the calibrated value of β doc was greater for tw than that for gw table 3 this is because the model was calibrated to simulate comparable doc load at the outlet of tw to gw mean daily doc load at the outlet of tw and gw is 0 125 and 0 13 kg ha respectively considering that soils in tw are well drained and less surface runoff is generated compared with gw when poc parameters were included the most sensitive parameters for doc load was still β doc in both study watersheds table 5 poc parameters v rpoc and er poc were sensitive in tw indicating that poc processes had impacts on doc load this effect was not significant since poc parameters were not relatively sensitive in the gw p value 0 05 however we suggest that when calibrating doc poc relevant parameters such as v rpoc and er poc may also be considered 4 conclusion in this study we expanded the swat c model with the capability of simulating riverine particulate organic carbon poc cycling and tested the new model for simulating both poc and dissolved organic carbon doc fluxes against continuous observations in the tuckahoe and greensboro watersheds in the northeastern united states we conducted sensitivity analysis for parameters that regulate flow sediment poc and doc processes the results indicate that poc modeling is influenced by parameters controlling flow sediment and poc cycling processes for doc modeling as poc can be transformed into doc fig 1 it is also necessary to calibrate those sensitive parameters for poc processes based on these findings the calibration of swat c for modeling river c fluxes should involve a four step procedure with flow calibrated first then followed by sediment calibration then poc calibration and finally doc calibration evaluated with the three statistical metrics i e percent bias coefficient of determination and nash sutcliffe coefficient the swat c model simulated daily poc and doc loads well during calibration and validation and replicated seasonal variations we also found that the swat c model underestimated high flows and peak poc and doc fluxes our uncertainty analyses showed that the 95 predictive uncertainty bands include more flow observations than poc and doc observations indicating larger uncertainties associated with poc and doc simulations this is because poc and doc modeling involves more data and parameters and is more complex than streamflow modeling given the importance of aquatic c cycling in accounting for c budgets and its relevance to drinking water quality and ecosystem health the inclusion of new c components into the swat model will help inform c related ecosystem services for watershed assessment and planning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the funding support for this project was provided by national aeronautics and space administration nnh13zda001n nnx17ae66g and 18 cms18 0052 and united states department of agriculture 2017 67003 26485 and 2017 67003 26484 and national science foundation 1639327 funding was also provided in part by the united states department of agriculture natural resources conservation service conservation effects assessment project nrcs ceap appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104601 
26092,the coupled carbon c cycle across terrestrial and aquatic environments at the watershed scale has been identified as an important but poorly constrained component of the global carbon budget here we extended soil and water assessment tool swat with coupled riverine particulate organic carbon poc and dissolved organic carbon doc modules referred to as swat c hereafter results show that swat c reproduced daily poc and doc fluxes well in two watersheds in the northeastern united states we found that swat c tended to underestimate high flows and peak doc and poc fluxes uncertainty analysis indicated flux uncertainties associated with poc and doc simulations were larger than those for flow simulation sensitive parameters controlling poc and doc biogeochemical processes were identified along with how these parameters influence mechanisms underlying c cycling we anticipate that the tool developed and applied here will inform c related ecosystem services in watershed assessment and planning keywords dissolved organic carbon particulate organic carbon aquatic carbon cycle terrestrial aquatic ecosystem coupling watershed 1 introduction carbon c cycling across terrestrial and aquatic environments has been identified as an important but poorly constrained component of the global c budget butman et al 2016 regnier et al 2013 in addition organic carbon oc in aquatic ecosystems impacts the health of human and aquatic life for example dissolved organic carbon doc can complex with heavy metals and influence their solubility and mobility lawlor and tipping 2003 weng et al 2002 impact drinking water safety through forming toxic disinfection byproducts dbp in the process of water treatment chowdhury et al 2009 evans et al 2005 and alter water chemistry e g acidity and nutrient supply eshleman and hemond 1985 stewart and wetzel 1981 thus the increase of oc concentrations in aquatic ecosystems has been a growing concern for its wide environmental impacts evans et al 2005 as such the world health organization who 2011 and u s environmental protection agency epa 2018 require monitoring oc in surface waters to ensure water quality and security numerical models such as those c cycle models coupled with general circulation models brovkin et al 2002 friedlingstein et al 2006 have been used to explore the magnitude of c fluxes and make predictions as affected by human activities mcguire et al 2001 and climate changes at the global scale berthelot et al 2005 cramer et al 2001 ito 2005 prentice et al 2001 those global scale studies provide insights on future climate change scenarios but they tend to lack detailed information on local c fate and environmental impacts and as a result they are not informative for local policy making and scientific research ecosystem models can also be used to simulate the c balance for a single homogeneous ecosystem e g a forest stand or grassland churkina et al 2003 clivot et al 2019 running and coughlan 1988 zhu et al 2003 or mosaic ecosystems at the regional scale huang et al 2017 parton et al 1992 sitch et al 2003 but they often omit c exchange between terrestrial and aquatic ecosystems butman et al 2016 therefore watershed scale models that explicitly represent regional local hydrology and nutrient cycle processes across both terrestrial and aquatic ecosystems are needed to quantify regional c cycling most watershed scale models can simulate terrestrial c fluxes with different complexity of biogeochemical processes however the aquatic c cycle is often oversimplified in most watershed scale models arnold et al 1998 bicknell et al 2001 laflen et al 1991 leonard et al 1987 smith et al 1997 young et al 1989 on the other hand many water quality models were designed specifically to simulate hydrodynamics and biochemical and physical conversion processes in streams and lakes adiyanti et al 2016 rauch et al 1998 wang et al 2013 wei et al 2018 but they require inputs from terrestrial ecosystems to drive their simulations of aquatic processes chapra et al 2003 cole and wells 2006 hamrick 1996 knightes et al 2019 wool et al 2006 clearly there is a need to improve watershed scale models with respect to hydrological and biogeochemical processes across terrestrial and aquatic ecosystems particularly quantification of c fluxes between terrestrial and aquatic environments requires that 1 the model should simulate terrestrial c cycle processes including c uptake by photosynthesis release by plant and soil respiration as well as disturbance processes e g human activities from the perspective of simulation the model can track the speed and magnitude of c cycling through different reservoirs including organic molecules in living and dead organisms soil organic matter and co2 in the atmosphere inclusion of land use change and management practices is necessary for a complete assessment of c cycle under the influence of human activities and 2 the model should also represent aquatic c cycle processes including generation and transportation of inorganic c ic dissolved organic c doc and particulate organic c poc from land to water bodies through runoff leaching erosion processes and biogeochemical transformation processes between those different forms of c in freshwater chapra et al 2003 yang et al 2016 2017 the soil and water assessment tool swat model arnold et al 1998 is a continuous physically based and widely used watershed scale model and has been successfully tested for simulating watershed hydrology land surface water and heat exchange and nutrient cycles across terrestrial and aquatic environments in a wide range of watersheds across the globe abbaspour et al 2017 qi et al 2017a 2017b srinivasan et al 2010 zhang et al 2008 swat has also been widely used to assess effectiveness of best management practices bmps as affected by land use and climate change betrie et al 2011 bracmort et al 2006 ficklin et al 2009 li et al 2014 liang et al 2019 qi et al 2018a zhang et al 2017 it analyzes small or large watersheds by discretizing them into subbasins which are then further subdivided into hydrological response units hrus with homogeneous land use soil properties and slope the water balance i e surface and subsurface runoff percolation and base flow and evapotranspiration and transmission losses crop growth and nutrient cycling processes are simulated for each individual hru water flow sediment and nutrient loadings are then routed through channels ponds and reservoirs to the watershed outlet sonka et al 1986 sonka et al 1986 recent development of the swat model included a c module i e swat c to simulate c cycle processes in terrestrial environments yang and zhang 2016 zhang 2018 zhang et al 2013 and doc cycling in river networks du et al 2019 however the representation of poc cycling in aquatic environments has not been examined within the swat c modeling framework in this study we aimed to 1 develop a poc module within the frame work of swat 2 integrate the poc module with the previously developed doc module and terrestrial c module 3 evaluate performance of the integrated model on poc and doc simulation in two watersheds in the northeastern united states and 4 conduct parameter sensitivity and model uncertainty analyses with the intent to identify important factors regulating coupled terrestrial aquatic c cycling and identify avenues for future application 2 methods and materials 2 1 description of swat c the swat c model yang and zhang 2016 zhang 2018 zhang et al 2013 following the century model parton et al 1994 separates the soil organic matter som and residue into five pools swat c simulates addition decomposition transformation and removal of each som residue pool present in surface and subsurface soil layers as well as soil organic c soc loss by multiplying the amount of soil erosion with concentration of soc in mineral soils and an enrichment ratio a recent study developed and tested the doc module in the forested cannonsville watershed in upper new york du et al 2019 which estimates doc mass production in soils controlled by a liquid soil partition coefficient soc content and other environmental factors izaurralde et al 2006 leonard et al 1987 soil doc can be removed from soil layers by surface runoff lateral flow and percolation to shallow aquifers a doc pool in the shallow aquifer is also added in that study which receives doc recharge from the bottom soil layer and transports doc to streams via baseflow the algorithms depicting riverine doc processes are obtained by adapting and combining the kinetics from qual2k chapra and pelletier 2003 and ce qual w2 cole and wells 2006 in general riverine doc is represented in two doc pools i e refractory and labile doc rdoc and ldoc respectively which undergo complex reactions as regulated by multiple biotic and abiotic factors along river networks the doc module allows swat c to satisfactorily estimate overall soil doc production and transport to streams and captures daily doc fluxes at the watershed outlet du et al 2019 for poc modeling swat c simulates soil erosion using the modified universal soil loss equation musle neitsch et al 2011 williams and berndt 1977 which is further coupled with a poc enrichment method to estimate the amount of lateral soc through soil erosion zhang 2018 however the poc cycling processes in swat c has not been described and tested 2 2 incorporating riverine poc cycling processes into the swat c model we have incorporated an in stream poc module into swat c to depict the fate of poc discharged into rivers and routed through the stream network the riverine poc module simulates the transport and transformation of terrestrially and internally through algae growth and die off processes derived poc in river networks both phytoplankton and bottom algae growth are represented in the module together the poc and doc processes modeled in both terrestrial and aquatic ecosystems are illustrated in fig 1 the riverine poc module receives poc loadings from local upland and upstream reaches and routes the poc to downstream reaches after calculating the poc transport and reactions in the water the algorithms of riverine poc processes are derived from the kinetics of poc as described in the ce qual w2 cole and wells 2006 and qual2k chapra et al 2003 models within the module riverine poc is represented in refractory and labile poc i e rpoc and lpoc and the kinetic changes of lpoc and rpoc in water column with respect to time are computed as follows 2 2 1 lpoc kinetics mortality of floating and bottom algae increases lpoc while settling to the bed sediments causes losses the transformation kinetics decreasing lpoc stocks include lpoc dissolution to labile dissolved organic c ldoc lpoc decay to ic and lpoc hydrolysis to rpoc the kinetic changes of lpoc are computed as follows 1 lpoc t f lpocp k dp t r ca a p floating algal mortality ap lpoc 1 h f lpocb k db t r cb a b bottom algal mortality ab lpoc k l p o c t l p o c lpoc dissolution to ldoc lpoc ldoc k l p o c d e c t l p o c lpoc decay to ic lpoc ic k l r p o c t l p o c lpoc hydrolysis to rpoc lpoc rpoc v l p o c h l p o c lpoc settling lpoc bed sediment where lpoc is labile particular organic c concentration mg c l 1 f lpocp is the fraction of algal mortality into lpoc 0 1 0 r ca is the c to algae biomass ratio mg c mg d a p is the floating algal biomass mg d l 1 k dp is the algae mortality rate d 1 v lpoc is the lpoc settling velocity m d 1 k lpoc is the dissolution rate of lpoc to ldoc d 1 k rpoc dec is the decay rate of lpoc to ic d 1 k lr poc is the decay rate of lpoc to rpoc d 1 f lpocb is the fraction of bottom algal mortality into lpoc 0 1 0 h is the water depth m calculated by swat s routing algorithm k db is the bottom algae mortality rate d 1 r cb is the c to biomass ratio in benthic algae mg c mg d and a b is the bottom algae biomass g d m 2 2 2 2 rpoc kinetics rpoc increases from the mortality of floating and bottom algae and loses via settling to the bed sediment layer other rpoc transformation processes include lpoc hydrolysis to rpoc rpoc dissolution to ldoc and rpoc decay to ic the kinetic changes of rpoc are computed as follows 2 rpoc t f rpocp k dp t r ca a p floating algal mortality ap rpoc 1 h f rpocb k db t r cb a b bottom algal mortality ab rpoc k l r p o c t l p o c lpoc hydrolysis to rpoc lpoc rpoc k r p o c t r p o c rpoc dissolution to ldoc rpoc ldoc k r p o c d e c t r p o c rpoc decay to ic rpoc ic v r p o c h r p o c rpoc settling rpoc bed sediment where rpoc is the refractory particular organic c concentration mg c l 1 f rpocp is the fraction of algal mortality into rpoc 0 1 0 v rpoc is the rpoc settling velocity m d 1 k rpoc is the dissolution rate of rpoc to ldoc d 1 k rpoc dec is the decay rate of rpoc to ic d 1 and f rpocb is the fraction of benthic algal mortality into rpoc 0 1 0 2 2 3 bottom algae processes we used the bottom algae photosynthesis respiration and death processes from the qual2k model to simulate the mass balance of bottom algae which can influence the dynamics of poc and doc in the water column more details of these processs are described by chapra and pelletier 2003 2 3 swat c parameterization we integrated poc cycling processes into the swat c model du et al 2019 zhang et al 2013 to achieve the capability of simulating watershed poc fluxes from terrestrial to aquatic ecosystems within the swat c model du et al 2019 identified six sensitive parameters controlling doc cycling as shown in table 1 these parameters are doc percolation coefficient β doc which specifies the concentration of doc in surface runoff as a fraction of the concentration in percolation the liquid solid partition coefficient k oc which determines the production of doc in soil solution the decay rate of ldoc to rdoc k lr doc which controls the transformation rate from ldoc to rdoc the doc half life in a shallow aquifer hl gw which governs decay of doc in the shallow aquifer and the mineralization rates of ldoc and rdoc k ldoc and k rdoc respectively the poc module developed in the present study has eight calibration parameters table 1 the poc enrichment ratio er poc which is defined as the ratio of the concentration of poc in eroded soils to the concentration of soc in the soil surface layer the lpoc and rpoc settling velocity v lpoc and v rpoc respectively which control the deposition of poc the dissolution rates for lpoc to ldoc and rpoc to ldoc k lpoc and k rpoc respectively which determine the transformation from poc to doc the decay rates of rpoc to ic and lpoc to ic k rpoc dec and k rpoc dec respectively which determine the transformation rate from poc to ic and the decay rate for lpoc to rpoc k lr poc which control the transformation between lpoc and rpoc 2 4 study area and data collection we used two adjacent watersheds defined by us geological survey usgs gauging stations at the tuckahoe creak near ruthsburg md usgs 01491500 and the choptank river near greensboro md usgs 01491000 these two watersheds are referred to as the tuckahoe watershed tw 220 7 km2 and greensboro watershed gw 290 1 km2 respectively fig 2 they are located at the headwaters of the choptank river watershed crw in the coastal plain of the chesapeake bay watershed fig 2 in tw the major land uses are agriculture 54 0 and forestry 32 8 while gw has a lower percentage of agricultural area 36 1 and a higher percentage of forest 48 3 tw is dominated by well drained soils 56 1 hydrologic soil group hsg a b of which 69 5 are on croplands lee et al 2016 in contrast a large portion of soils 74 5 in gw are poorly drained hsg c d and most of those soils 67 2 are on croplands ator and denver 2015 the major cropland soil types in the crw are the othello soil series fine silty and the mattapex soil series fine silty othello soils are poorly drained with moderately slow permeability while mattapex soils are moderately well drained with moderate or moderately slow permeability hively et al 2009 in addition the land in the crw is relatively flat with most areas having less than 2 slopes the study area is characterized by a temperate and humid climate with an average annual temperature of 15 4 c and precipitation is evenly distributed throughout the year with an average annual amount of ca 1200 mm ator et al 2005 yeo et al 2014 high quality climate soil and land use data are essential for credible watershed modeling we used a soil map based on the usda natural resources conservation service nrcs soil survey geographic database ssurgo a 10 m light detection and ranging lidar based digital elevation model provided topographic information the land use map and the scheduling of crop rotations were generated using 2008 2012 data from the usda national agriculture statistics service nass cropland data layer cdl daily weather inputs for swat including precipitation temperature solar radiation relative humidity and wind speed were derived from high resolution 1 8 nasa north american land data assimilation system 2 nldas2 climate forcing data xia et al 2012 that have been proven to help improve swat hydrologic modeling as compared with using sparsely distributed rain gauges qi et al 2019b for more information regarding swat model setup for these two watersheds please refer to lee et al 2016 coincident with the ruthsburg and greensboro usgs gauging stations we conducted monitoring of riverine total organic c toc and doc concentrations for tw and gw respectively fig 2 we employed in situ instrument packages containing full spectrum 200 700 nm spectrophotometer probes s can instruments vienna austria which perform water quality sampling at 30 min intervals we used multiwavelength calibrations for toc and doc in stream water avagyan et al 2014 the fine time resolution toc and doc data were aggregated to a daily time scale for swat c calibration and validation the concentrations of toc and doc were further multiplied with observed flow rate to estimate fluxes in units of kg c day 1 poc was derived by subtracting doc from toc 2 5 model calibration sensitivity and uncertainty analysis the sampling period jan 2014 oct 2015 was divided into calibration 2014 and validation 2015 periods before the calibration period we used a two year 2012 2013 warm up period to initialize the swat c model we chose the most frequently calibrated parameters for daily flow rate based on the swat calibration literature abbaspour et al 2007b arnold et al 2012 and previous studies conducted in the study watersheds lee et al 2016 2017 we employed a multi step procedure to calibrate poc and doc loads the first step was to calibrate streamflow which is the key for the following water quality calibration sediment load needs to be calibrated before poc load calibration because poc load is closely associated with soil erosion on land surfaces and transport in streams following poc calibration doc load is calibrated in the present study we did not have sediment data to directly calibrate sediment load as a result when calibrating poc we adjusted sediment related parameters together with poc controlling parameters table 1 lists all calibration parameters for daily flow rate sediment poc and doc loads as well as their value ranges the sequential uncertainty fitting algorithm version 2 sufi 2 method in swat cup abbaspour et al 2007a was used to conduct calibration for daily flow rate and poc and doc loads it was also used to conduct parameter sensitivity and uncertainty analysis we used the global sensitivity analysis method for sensitivity analysis for different sets of parameters corresponding to daily flow rate and poc and doc loads specifically we considered four scenarios regarding parameter sensitivity for poc and doc loads 1 sensitivity analysis of poc parameters for poc load 2 sensitivity analysis of sediment and poc parameters for poc load 3 sensitivity analysis of doc parameters for doc load 4 sensitivity analysis of poc and doc parameters for doc load scenario 2 is aimed to investigate the impact of erosion and sediment transport resuspension and deposition processes on poc fate and scenario 4 attempts to analyze the impact of poc processes on doc fate the global sensitivity analysis approach used was a multiple regression system with latin hypercube samples by means of objective function values used in calculating the response parameter sensitivities given as 3 g α i 1 m β i b i where g is the objective function value α and β i are regression coefficients b i is the calibrated value of the ith parameter and m is the number of parameters considered for example when conducting sensitivity analysis of poc parameters for poc load scenario 1 eight m 8 parameters were selected table 1 to calculate the objective function value with different combinations of sampled parameter values we employed the nash sutcliffe coefficient ns nash and sutcliffe 1970 as the objective function g ns because it is a commonly used goodness of fit coefficient in hydrologic modeling studies further model performance evaluation criteria have been established by moriasi et al 2007 for ns a student t test was used to quantify the statistical significance of each parameter with a p value 0 05 indicating a parameter as sensitive in the present study the global sensitivity analysis approach estimates the change in the objective function resulting from changes in each parameter while all other parameters are changing abbaspour et al 2007a and as a result it does not provide an absolute measure of the sensitivity but rather the relative sensitivity thus we also provided parameter sensitivity rankings based on p value for additional sensitivity analysis in sufi 2 the uncertainties in model structure parameters and input data are not separately estimated but are attributed as total model uncertainty to the parameters abbaspour et al 2007a the latin hypercube sampling method is used in sufi 2 for vast parameter value combinations and resultant model simulations are used to calculate the percentage of measured data bracketed by the 95 percent prediction uncertainty often referred to as 95ppu which is measured by the p factor the range of the p factor varies from 0 to 1 0 with values close to 1 0 all observations bracketed by the prediction uncertainty indicating very strong model performance and small prediction uncertainty the r factor is the average thickness of the 95ppu bands divided by the standard deviation of the observed data an r factor varying between 0 and 1 0 indicates acceptable prediction uncertainty estimation abbaspour et al 2007a abbaspour 2013 in general a trade off between p factor and r factor exists for model uncertainty evaluation a larger p factor can be achieved at the expense of a larger r factor and vice versa a model with a balance between the two factors can provide acceptable prediction uncertainty qi et al 2019a 2 6 model performance evaluation for model evaluation we used a combination of different efficiency criteria complemented by the assessment of the absolute or relative error as recommended by krause et al 2005 specifically three well known statistical criteria i e percent bias pbias coefficient of determination r2 and ns are used here and given as 4 p b i a s 100 o a v g p a v g o a v g 5 n s 1 i 1 n o i p i 2 i 1 n o i o a v g 2 6 r 2 i 1 n o i o a v g p i p a v g i 1 n o i o a v g 2 i 1 n p i p a v g 2 0 5 where o i and p i are the observed and predicted values and o avg and p avg are the average of the observed and predicted values respectively the r2 ranges from 0 to 1 and quantifies the proportion of explained variance in observed data with higher values indicating less error variance the ns is a normalized statistic and estimates the relative magnitude of the residual variance as compared to the observed nash and sutcliffe 1970 and demonstrates how well the plot of observed versus simulated data fits the 1 1 line and pbias has the ability to indicate poor model performance and measures the average tendency of the simulated data to be larger or smaller than observed data low magnitude values of pbias indicate accurate model simulation positive values indicate model underestimation bias and negative values indicate model overestimation bias gupta et al 1999 3 results and discussion 3 1 model performance during calibration and validation model performance on daily flow rate and poc and doc loads as indicated by r2 ns and pbias during calibration and validation are shown in table 2 during calibration we tried to ensure that the model performance was satisfactory according to model performance evaluation criteria for watershed models summarized by moriasi et al 2007 model simulation can be judged as satisfactory if ns 0 50 at a monthly time step while 25 pbias 25 for streamflow and 70 pbias 70 for nitrogen n and phosphorus p here we assumed the criteria for n and p are applicable to poc and doc simulation assessment as the criteria for evaluating model performance for c fluxes have not been established due to few available studies on watershed scale c cycling model performance criteria based on ns moriasi et al 2007 was also assumed to be applicable to r2 because both statistics are based on squared differences between observation and prediction krause et al 2005 overall the calibration results demonstrated satisfactory model performance for three water quantity and quality variables based on all three statistics table 2 one exception was pbias for simulated streamflow in gw which was 37 3 greater than the 25 threshold during validation model performance on poc and doc fluxes in the tw and streamflow in the gw was satisfactory in terms of all three statistics except for pbias for streamflow in gw which was 32 9 ns and r2 values during validation were less than 0 5 threshold for gw it is worth noting that this criterion was recommended for model evaluation at a monthly time step while the present study evaluated model performance at a daily time step because increasing temporal resolution of simulations from the monthly time step to the daily time step often results in poorer model performance the model performance criteria for monthly evaluation should be relaxed slightly for the daily time step for example the satisfactory criteria for n and p may be ns 0 4 or even ns 0 3 moriasi et al 2007 simulated vs observed daily flow rate and poc and doc loads are shown in figs 3 and 4 for the tw and gw respectively in general simulated hydrographs of streamflow and poc and doc fluxes matched observations well the model tended to underestimate streamflow during both calibration and validation periods in the two watersheds indicated by positive pbias values in table 2 that is the model underestimated many high flows which were dominated by surface runoff it has been reported that the swat model tended to retain less soil water leading to underestimated surface runoff especially under wet conditions mapfumo et al 2004 qi et al 2018b as a result peak doc fluxes were also underestimated figs 3c and 4c because of the close relationship between streamflow and doc fluxes du et al 2019 poc load is not only related to streamflow but also associated with soil erosion and sediment transport during poc calibration we also considered parameters controlling sediment processes on land and in streams table 3 simulated poc and doc loads in general followed observed data patterns and the model can capture seasonal variations well as with streamflow simulations poc fluxes during some high flow events were underestimated as our observations span a two year period we used only one year of data for calibration and less than a year data for validation despite continuous poc and doc observations the relatively short observation for model evaluation may not allow us to derive conclusions that can be extrapolated to longer time periods this is because longer time periods will likely include more complex hydrologic conditions calibrated parameter values with respect to different water quantity and quality variables in the two study watersheds are shown in table 3 we will discuss more about parameter calibration in conjunction with parameter sensitivity analyses in section 3 3 3 2 model uncertainty analysis the 95ppu bands for daily flow rate and poc and doc loads are shown in figs 5 and 6 for tw and gw respectively the figures show that many observed peak flows were out of the 95ppu bands further confirming the difficulty in simulating peak flows since poc and doc cycling are closely coupled with hydrological processes we observed similar underestimation patterns in doc and poc load simulations during peak flow events table 4 lists the p factor and r factor values for estimated 95ppu bands for streamflow and poc and doc fluxes respectively the p factor values indicate that the 95ppu bands include 81 93 observed daily flow rates 55 91 observed poc loads and 31 68 observed doc loads during the periods in the two study watersheds in general decreases in p factor values from streamflow to c results indicate that the uncertainty associated with poc and doc modeling is higher than that for streamflow simulation this is not a surprise because streamflow simulation is mainly controlled by hydrologic processes but poc and doc simulations are governed by both hydrological and biogeochemical processes the increase in complexity is a source of additional difficulty in simulating poc and doc accounting for the decrease in p factor values as compared with streamflow simulation performance according to abbaspour 2013 an r factor less than 1 0 indicates a model has an acceptable prediction uncertainty estimation in this study r factor values were all less than 1 0 for daily flow and poc and doc fluxes in calibration and validation periods except for streamflow during calibration in tw and doc during validation in gw table 4 3 3 model parameter sensitivity analysis table 5 lists the p value and sensitivity ranking of different parameters for simulating different water quantity and quality variables the most sensitive parameters for daily flow rate were cnop1 surlag cnop2 and ch k2 which controlled the surface runoff yield and routing in the two study watersheds see table 1 for variable explanation calibrated values of cnop1 and cnop2 were greater for gw than those for tw table 3 respectively reflecting the fact that a large portion of soils in gw are poorly drained compared with soils in tw the calibrated value of surlag was less for gw than that for tw table 3 indicating larger portion of surface runoff lagged in the gw which reflects gw has higher percentage of forest mainly forest wetlands than tw snowmelt parameters such as sftmp were also among the top sensitive parameters for daily flow rate in both watersheds which indicates snow affects hydrological processes in the study area especially during snowmelt seasons groundwater parameters alpha bf and gw delay were sensitive for flow rate in tw and gw respectively indicating the importance of groundwater contribution to streamflow we found that calibrating cnop curve number for operation separately for different management practices instead of only calibrating cn2 improves seasonal flow rate simulation qi et al 2017b calibrated cnop1 curve number for planting operation values were less than calibrated cnop2 curve number for harvesting operation values in both watersheds table 3 which reflects the transition of land surface hydrological conditions from planting to harvesting sensitivity of ch k2 in both study watersheds suggested that stream water may have a close relationship with underlaying groundwater system in the study areas the calibrated value of ch k2 was greater for tw than that for gw which may be explained by well drained condition in tw for poc load v rpoc and er poc were the most sensitive parameters among the eight poc controlling parameters in the two study watersheds table 5 v lpoc was also relatively sensitive in gw er poc controls the magnitude of poc transported from soil surface and v rpoc and v lpoc affects the settling velocity for rpoc and lpoc in streams respectively the calibrated value of er poc was greater for gw than that for tw table 3 which is consistent with the measurement mean daily poc load at the outlet of the gw is 0 075 kg ha 0 068 kg ha for tw note that the calibrated values of v rpoc were very close for tw and gw table 3 suggesting similar topographical climatic and vegetational conditions between the two study watersheds when combined with parameters related to sediment load v rpoc and er poc remained the most sensitive parameters for poc load in the two study watersheds table 5 this result indicates that poc load in the two watersheds are controlled by the poc production and transport in both terrestrial and aquatic systems soil erosion related parameters usle k and usle p were relatively sensitive compared with other sediment parameters table 5 these results imply that sediment parameters had secondary influences on poc load simulation in the two study watersheds when applied to other watersheds sediment parameters may be also considered for calibration of poc load for doc load β doc was the most sensitive parameter in both study watersheds table 5 parameter k ldoc k oc and hl gw were also sensitive either in tw or gw parameters k oc and β doc control the total magnitude of doc generated and transported from upland soils parameters k ldoc and hl gw affect doc reactions in the stream and groundwater respectively these results indicate that doc loads in the two study watersheds are controlled by the doc production and transport in both terrestrial and aquatic environments the calibrated value of β doc was greater for tw than that for gw table 3 this is because the model was calibrated to simulate comparable doc load at the outlet of tw to gw mean daily doc load at the outlet of tw and gw is 0 125 and 0 13 kg ha respectively considering that soils in tw are well drained and less surface runoff is generated compared with gw when poc parameters were included the most sensitive parameters for doc load was still β doc in both study watersheds table 5 poc parameters v rpoc and er poc were sensitive in tw indicating that poc processes had impacts on doc load this effect was not significant since poc parameters were not relatively sensitive in the gw p value 0 05 however we suggest that when calibrating doc poc relevant parameters such as v rpoc and er poc may also be considered 4 conclusion in this study we expanded the swat c model with the capability of simulating riverine particulate organic carbon poc cycling and tested the new model for simulating both poc and dissolved organic carbon doc fluxes against continuous observations in the tuckahoe and greensboro watersheds in the northeastern united states we conducted sensitivity analysis for parameters that regulate flow sediment poc and doc processes the results indicate that poc modeling is influenced by parameters controlling flow sediment and poc cycling processes for doc modeling as poc can be transformed into doc fig 1 it is also necessary to calibrate those sensitive parameters for poc processes based on these findings the calibration of swat c for modeling river c fluxes should involve a four step procedure with flow calibrated first then followed by sediment calibration then poc calibration and finally doc calibration evaluated with the three statistical metrics i e percent bias coefficient of determination and nash sutcliffe coefficient the swat c model simulated daily poc and doc loads well during calibration and validation and replicated seasonal variations we also found that the swat c model underestimated high flows and peak poc and doc fluxes our uncertainty analyses showed that the 95 predictive uncertainty bands include more flow observations than poc and doc observations indicating larger uncertainties associated with poc and doc simulations this is because poc and doc modeling involves more data and parameters and is more complex than streamflow modeling given the importance of aquatic c cycling in accounting for c budgets and its relevance to drinking water quality and ecosystem health the inclusion of new c components into the swat model will help inform c related ecosystem services for watershed assessment and planning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the funding support for this project was provided by national aeronautics and space administration nnh13zda001n nnx17ae66g and 18 cms18 0052 and united states department of agriculture 2017 67003 26485 and 2017 67003 26484 and national science foundation 1639327 funding was also provided in part by the united states department of agriculture natural resources conservation service conservation effects assessment project nrcs ceap appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104601 
26093,the paper demonstrates the use of bayesian networks in multicriteria decision analysis mcda of environmental design alternatives for environmental flows eflows and physical habitat remediation measures in the mandalselva river in norway we demonstrate how mcda using multi attribute value functions can be implemented in a bayesian network with decision and utility nodes an object oriented bayesian network is used to integrate impacts computed in quantitative sub models of hydropower revenues and atlantic salmon smolt production and qualitative judgement models of mesohabitat fishability and riverscape aesthetics we show how conditional probability tables are useful for modelling uncertainty in value scaling functions and variance in criteria weights due to different stakeholder preferences while the paper demonstrates the technical feasibility of mcda in a bn we also discuss the challenges of providing decision support to a real world habitat remediation process graphical abstract image 1 keywords valuation bayesian network bn multi attribute valuation theory mavt atlantic salmon water framework directive wfd disproportionate cost good ecological potential river aesthetics angling 1 introduction environmental flow eflow has been defined as the hydrological regime required to sustain freshwater and estuarine ecosystems and the human livelihoods and well being that depend on them acreman et al 2014 concessions to regulate rivers for hydropower production in norway have historically evaluated eflow requirements and physical habitat mitigation measures separately it is unusual to explicitly compare costs against environmental objectives in order to determine eflow despite the assessment of disproportionate cost being required by the water framework directive wfd finstad et al 2007 recently norwegian authorities conducted a national screening project to prioritize the renewal of existing concessions where the most negative environmental impacts could be reduced at the lowest possible foregone power production sørensen et al 2013 they prioritized concession renewals based on an evaluation of foregone hydropower revenue and the magnitude of environmental improvements on important fish populations biodiversity and landscape impacts and recreational use one of the shortcomings of the study was not evaluating the potential for physical habitat restoration measures to partly substitute for stricter environmental flow requirements that follow from the application of the wfd in the revision process not considering the synergistic interactions between environmental compensation measures means that the costs of achieving good ecological potential under the wfd are likely to be overestimated barton et al 2016b more broadly habitat restoration on site combined with offsetting measures can offer net gains to biodiversity bbop 2009 as well as to other user interests such as recreation aas and onstad 2013 the central handbook on environmental design in regulated salmon rivers in norway forseth and harby 2014 outlines steps for water negotiations as input to action plans for rivers but proposes no formal method for assessing trade offs across management objectives in this study we propose such a formalized multi criteria decision analysis mcda to evaluating the trade offs between hydropower production and cultural ecosystem services of recreational salmon fishing and riparian landscape aesthetics and the supporting service of habitat quality for salmon smolt we demonstrate how different quantitative and qualitative impact assessments can be combined in an object oriented bayesian network oobn with nodes for valuation functions criteria weights and decision nodes in order to consistently integrate uncertainty from different impact model domains within mcda the model is deployed in an online interface to ease communication with a wider community to our knowledge this is the first example in norway of mcda integrating assessment of eflows and physical habitat restoration measures through the application of a bayesian network approach 1 1 environmental design in hydropower river regulation research linking the economics of hydropower operation and physical river restoration measures to environmental outcomes has been limited due to the computational complexities of linking biophysical and economic models of varying resolution and data sources barton et al 2010 charmasson and zinke 2011 niu and insley 2013 person et al 2014 bustos et al 2017 jager and smith 2008 found that earlier studies have optimized hydropower production against simple fish habitat objectives or ignored hydropower objectives altogether in optimizing hydraulic and fish stocking objectives forseth and harby 2014 propose an environmental design approach to regulated salmon rivers that brings together the state of knowledge on flow requirements of hydropower environmental flows and morphological river restoration measures their handbook proposes that trade off appraisals be made through water negotiations based on appraisal of multiple environmental design objectives recently bustos et al 2017 demonstrated cost effectiveness comparing hydropower production economics and salmon habitat restoration costs against productivity of atlantic salmon they integrated a series of models including hydropower production river hydraulics channel wetted area and salmon smolt productivity while their study used historical hydrological data the cost effectiveness of alternatives was assessed in terms of mean smolt production per norwegian kroner nok in foregone power and remediation costs in the present study we extend the bustos et al 2017 cost effectiveness analysis to include recreational fishing and river aesthetics we assess whether the conclusions from their cost effectiveness analysis are modified by taking a multi criteria decision approach that includes cultural ecosystem services which may have non linear relationships to eflows 1 2 modelling uncertainty in environmental flows design acreman et al 2014 reviewed the changing role of ecohydrological science in guiding eflows the hydrological regime required to sustain freshwater and estuarine ecosystems and the livelihoods and well being that depend on them recent advances include modelling of dynamics in hydrological analysis of flow regimes and addressing more than one stressor flow channel morphology because all methods and data have a degree of uncertainty acreman and colleagues stress the importance of risk communication between scientists practitioners policy makers and the public the present study contributes to the broader literature on modelling river restoration in the context of environmental flows and approaches to explicit calculation of uncertainty in decision support models the present study builds on prior experiences in using bayesian networks bns to model river management decisions for example uusitalo et al 2005 estimated atlantic salmon smolt carrying capacity of rivers using expert knowledge in a bn in hugin software identifying large disagreement in experts judgement they conclude that operational management objectives other than those based on maximal smolt production levels should be considered in order to decrease the uncertainty connected with evaluation of management success chan et al 2012 used bayesian network models in netica software to model relationships between dry season flows in an australian river affected by agricultural irrigation and key aspects of biology of barramundi and sooty grunter fish species bns were largely based on expert knowledge due to the lack of quantified biological knowledge but could later be verification against field observations sometimes also called conditional probability networks bns have been used to formalize expert ecological knowledge of hydrological regimes on fish habitat shenton et al 2011 horne et al 2018 similarly gawne et al 2012 demonstrated the use of a bn as a decision support tool for water supply and quality of wetlands in australia to maximize outcomes on population health of four native fish shenton et al 2014 integrated dynamic simulations from a hydrological sub model with an established expert based ecological model of grayling fish habitat spawning larvae transport and recruitment in the present study we contribute to this literature by demonstrating a combined impact modelling of morphological measures and eflow marsili libelli et al 2013 implement an instream flow assessment method modelling uncertain habitat suitability in large scale river modelling in tuscany italy they use fuzzy logic to implement uncertainty in expert judgement about habitat suitability curves under variable hydrological conditions in the present study we show how uncertainty and non linearity in expert assessment of habitat suitability curves can be implemented in conditional probability tables 1 3 multi criteria decision analysis in a bayesian setting landis et al 2017 argue that bayesian networks could also be used as the computational background for mcda of risk based adaptive environmental management bayesian networks has been used in multi criteria decision analyses mcda to support decisions in different fields in the transport and safety field fenton and neil 2001 decommissioning of offshore oil and gas platforms henrion et al 2015 urban planning langemeyer et al 2020 in the diagnosis of significant adverse effect of the eu water framework directive on hydropower production in norway barton et al 2016b and in the evaluation of which nutrient abatement measure optimally improves the condition of a costal ecosystem lehikoinen et al 2014 huang et al 2011 conducted a review of environmental applications of mcda in the period 2000 to 2009 vassoney et al 2017 analyzed all peer reviewed articles on the use of multicriteria analysis for sustainable hydropower planning and management from 2000 to 2015 in either review no papers were cited that used bayesian network applications of mcda to environmental management of freshwater ecosystems however there is literature that supports the use of bayesian network as mcda tool ins this field varis 1997 documented and discussed the use of influence diagrams and belief networks and their relation to decision trees the examples provided were focused on environmental and resource management of freshwater and fisheries studies in addition varis and kuikka 1997 showed the appropriateness of bayesian networks for handling problems in environmental and resource management such as to asses and safeguard the wild salmon stocks in the baltic sea under the pressure of extensive commercial fishery of reared salmon they showed that bayesian networks enhance the communication among experts using analytically based arguments allowed to find inconsistencies in information and to calculate joint predictions supporting decision making based on more balanced information in recent years the use of bayesian network for mcda has increased for the field of environmental management in freshwater systems bayesian networks has been applied by stewart koster et al 2010 to support the prioritization of flow and catchment restoration alternatives including the cost effectiveness of the interventions holmes et al 2018 used bayesian networks as a support tool for stream fisheries management with the objective of diagnose the factors limiting stream trout fisheries related to impacts on fish migrations from the hydropower structure and operations in the northern hemisphere a bayesian network was used to combine and show the effectiveness of possible fishpass structures and the mortality rate from the hydropower turbines wilkes et al 2018 reichert et al 2015 discuss requirements for environmental decision support arguing that a combination of probability theory and scenario planning with multi attribute utility theory fulfills requirements for representation and quantification of scientific knowledge elicitation of societal preferences and communication with authorities politicians and the public they argue for explicit modelling of ecological state as separate from other ecosystem services in order to better account for complexity in valuation they present a multi criteria decision making process in which uncertainty is considered explicitly in multi attribute valuation functions beinat 1997 demonstrated how to model uncertainty in marginal value functions in multi attribute utility models for mcda in the present study we show how uncertainty in value functions can be implemented using conditional probability tables reichert et al 2015 conclude that more research is needed on value aggregation in environmental decision support and in particular non additive approaches that consider joint fulfillment of management goals mcda has been used extensively in evaluating river regulation in finland testing a variety of additive and non additive value elicitation techniques discovering differences in weighting of river restoration criteria across multiple stakeholders marttunen and suomalainen 2005 marttunen and hamalainen 2008 marttunen et al 2008 variation in stakeholder weighting of criteria is addressed through sensitivity analysis convertino et al 2013 demonstrated non linear mcda aggregation in sustainable river restoration alternatives using their model promaa which can use probability distributions of criteria utilities and weight coefficients for assessing probabilities of likely rank events based on pairwise comparison of alternatives in an integrated scale they demonstrated probability distribution of weights by assigning an arbitrary fixed standard deviation to all criteria weights instead of point values in the present study we demonstrate a criteria weight probability distribution generated by variation in stakeholder perspectives convertino et al 2013 emphasised the information costs involved in obtaining expert judgements of value in mcda with many criteria weight combinations considering information costs and value researchers have advised caution in modelling uncertainty without evaluating the decision support of additional probabilistic information in otherwise deterministic models uusitalo et al 2015 recommends using information value analysis to evaluate increasing model complexity linkov et al 2015 also argue for a weight of evidence approach to quantitative data integration in multi criteria decision analysis specifically advocating bayesian methods for value of information analysis in the present study we discuss the scope for conducting value of information analysis in object oriented bayesian networks oobn oobn make it possible to subdivide a network into submodels that can be independently specified by domain experts facilitating analysis and communication of complex nested model structure barton et al 2012 the paper is laid out as follows in section 2 materials and methods we present the mcda framework combining the chain of models employed by bustos et al 2017 to assess costs and smolt productivity we extend the impact assessment to the cultural ecosystem services of river aesthetics and angling experience we present how we use oobns to implement multi criteria decision analysis supplementary material further documentation of sub networks and assessments that were needed to generate them an online demonstration version of the network is available at http demo hugin com example mcda oobn in section 3 our results show how the features of bayesian networks can be used to appraise environmental design decisions from different perspectives section 4 discusses the differences between our findings for preferred management alternatives based on mcda and those of bustos et al 2017 based on cost effectiveness analysis we discuss the advantages and limitations of our application of mcda in an oobn 2 materials and methods in this section we first provide an introduction to the study area the management problem and define the scenarios for eflows and habitat restoration that were modelled we provide a summary of the chain of models employed by bustos et al 2017 to assess costs and smolt productivity we explain how this work forms the basis for our extension of the impact assessment to the cultural ecosystem services of river aesthetics and fishing experience supporting material provides additional details on sub models we then present the multi criteria decision analysis framework and how we use a bayesian network to implement it finally we discuss the diagnostic tools we use to assess the model 2 1 study area the mandalselva river basin is located in southern norway 58 n 7 e it originates in setesdalsheiene at 600 800 m a s l the river is 115 km long and has a catchment area of 1800 km2 it flows south to the town of mandal where the majority of the people 13 000 inhabiting the catchment live the remaining 5000 people live in rural areas of the catchment l abée lund et al 2009 the mandalselva river varies from low to high gradients over short distances allowing a combination of fast currents waterfalls slow deep pools wide and calm stretches and lakes the mandalselva was among the top 10 of norway s most productive salmon rivers before the acidification period appr from 1920 to 2000 which eradicated the atlantic salmon population in the river in the first part of the twentieth century hesthagen and hansen 1991 during this period several hydropower facilities were constructed including the lowest laudal hydropower plant located within the river reach available to atlantic salmon the final migration barrier is located 47 km from the sea at the kavfossen waterfall fig 1 this power station involved a tunnel bypass of the bulk of the flow and produced a 5 km long minimum flow river stretch atlantic salmon production has been restored after liming and re stocking hesthagen and johnsen 2004 in order to mitigate the aesthetic effects of the very low flow originally 250 l s and maintain a continuous water surface in the laudal minimum flow stretch 11 stone weirs and one concrete weir were constructed in the 1980s in this study we evaluate the potential effects of removing the stone weirs 3 5 and the kleveland concrete weir 8 fig 1 in order to improve salmon habitat and the river s suitability for salmon fishing fishability and aesthetics compared to costs of eflow and restorations measures 2 2 definition of management scenarios the overall aim of the mcda was to demonstrate how to model the optimal combinations of weir removal and discharge regimes measured by stakeholder preferences for hydropower production smolt production recreational fishability and river aesthetics we aimed to test alternative scenarios including a proposal by the norwegian water resources and energy directorate nve for a trial minimum eflow regime which more than doubled the previous voluntary spill regime at laudal in place since 1995 if salmon productivity could be improved by the trial eflow regime the hydropower producer s 1995 concession would be open to revision by the authorities the baseline scenario a represents the bypass flow prior to the introduction of nve s trial minimum flow regime implemented in 2013 nve s trial regime from 2013 was as follows labelled scenario p4 during the spring salmon smolt migration period set to last 14 days approximately 50 of the inflow released in the river to avoid smolt turbine entrainment during summer 8 25 m3 s are released depending on inflow to ensure both smolt production and upstream migration of adult salmon during winter 6 m3 s are released to ensure salmon juvenile winter survival scenario a is identical to bustos et al 2017 while the label p represents scenarios with photo manipulated representation of river aesthetics under different flow conditions intermediate scenarios p1 and p2 involved summer minimum flow releases at 3 and 4 6 m3 s respectively 1 1 scenario p3 was removed from the analysis because it lacked completely consistent photo scenario information weir removal decreases wetted river surface area but typically increases river flow speed thus potentially increasing salmon spawning area for any given flow rate fjeldstad et al 2014 increased flow speed can also increase recreational fishability in certain river segments while the impact on river aesthetics of decreased water surface area depends on recreational preferences pflueger et al 2010 the habitat modification tested in this paper concerns removal of weirs and the addition of spawning gravel to the river reach weirs 3 5 and 8 in the laudal residual flow reach were selected for further photo and mesohabitat analysis because of their recreational accessibility and hence potential impact on river aesthetics and perceived fishability of the river an overview of scenarios modelled in this paper is provided in table 1 where p denotes the photo scenarios weir removal and addition of spawning gravel were modelled in all p1 p2 and p4 flow scenario combinations a total of 16 4 2 2 combinations of measures across all models are assessed in this paper table 1 bustos et al 2017 simulated a number of other intermediate scenarios with only marginal differences to p1 p2 p4 see supplementary material s1 note summer low flow rules were defined by a step function conditional on a range of inflows in p2 and p4 aesthetic effects were evaluated at fixed flows 3 0 6 0 and 15 0 m3 s intermediate scenario p3 was removed because it lacked a complete photo scenario in the rest of the paper flow scenarios are labelled wp if weirs are removed and g if spawning gravel is added 2 3 ecosystem service sub models the ecosystem service cascade framework links ecosystem structure to function services benefits and values haines young and potschin 2010 later developments have emphasised the stepwise intervention of policy and management adding feedback loops and stepwise updating spangenberg et al 2014 barton et al 2017 hausknost et al 2017 fig 2 provides a conceptual overview of the cascade of ecosystem service models and the link to the mcda as used in this paper it shows how biophysical and economic submodels are integrated explicitly in the mcda using object oriented bayesian network functionality limited stepwise feedback was part of our study through i adjustment of environmental design scenarios modelled in bustos et al 2017 based on feedback from stakeholders to the outcomes for hydropower production wetted area and smolt production ii adjustment of impact weights based on updating of perspectives by hydropower company stakeholders and iii scaling of impacts through consultation with domain experts an overview of the sub models s1 s7 is presented below with further details available in the supplementary materials hydropower energy foregone relative to the voluntary baseline scenario a was calculated using a power production potential function considering turbine efficiency net head of water and average water flow through the turbine annual foregone energy output was scaled with an average price for may 2012 may 2013 in order to be comparable to the study by bustos et al 2017 fixed costs of weir removal and artificial establishment of spawning gravel were annualized and added to annual foregone revenues in each scenario resulting in an incremental annual cost for each scenario relative to scenario a see supplementary material s1 for further documentation the flow regime in the bypass section discussed above determines the wetted area wetted area was determined using hydraulics calculations in hec ras 1d hec ras 2008 using a model developed by fjeldstad et al 2014 the hydraulic model estimates changes in wetted area water velocity and water depth which are subsequently inputs to mesohabitat quality classification and mapping see supplementary material s2 for further documentation the effect of the hydrological regime on salmon production was determined using the individual based salmon population model ib salmon hedger et al 2013a hedger et al 2013 2018 this model simulates salmon population abundances across salmon life stages from juvenile fry parr smolt to adult adult at sea returning spawner using functions that govern how individual salmon respond to the environment such as temperature dependent growth and density dependent mortality the model is 1 dimensionally spatially explicit with the watercourse being compartmentalised into a series of sections 50 m in length smolt production the number of individuals migrating to sea was used as the metric of population condition as a supporting ecosystem service smolt production is a commonly used metric because it represents the stage at which the population has experienced all sources of juvenile mortality in the river and is relatively easy to measure empirically see supplementary material s3 for further documentation the hydraulic model hec ras uses mapped river cross sections and flows to simulate spatial distribution of water velocities and water depths we used these model data combined with judgement of salmon fishers in the research team pers com fjeldstad to classify physical conditions for fishing under different scenarios using the mesohabitat classification system by borsanyi et al 2004 different mesohabitat types were plotted by experts in the bypass area and one map was produced for each scenario and for each reach above weirs 3 5 8 maps of the extent of each habitat type were generated for specific river summer discharges 3 m3 s 6 m3 s 15 m3 s with and without weirs we generated a matrix of changes in mesohabitat area relative to the baseline scenario a summer situation minimum flow of 3 m3 s each mesohabitat class was then valued independently by 3 salmon fishers in the project team on a scale from 0 to 1 for their relative fishability mesohabitat mapping and expert scoring were combined in a sub network embedded in the oobn fig 2 see supplementary material s4 for further documentation of the sub model habitat remediation through weir removal affects riverscape aesthetics as wide and calm weir reservoir water surfaces are converted to a narrower but more variable river in fact accessibility of river reaches and hence potential aesthetic impacts was the determining factor for which weirs were selected for scenario modelling in this study we used photo scenario and photo simulation methods tress and tress 2003 junker and buchecker 2008 to assess visual effects of combinations of different low flow regimes with and without weir removal baseline photos at 6 m3 s were taken for river reaches around the selected weirs photoshop scenario illustrations were based on hec ras 1d hydraulic modelling data for the parameters wetted area water velocity and water depth s2 expert knowledge and systematic judgement were used to illustrate the specific water surface structures resulting from varying water velocities and water depths including light and color of the water surface and shadows additionally we used a 3 d visual modelling technique for the site with the largest weir structure at kleveland bridge pers com 3dsmia b dervo this resulted in 3 4 photoscenarios which were evaluated by 6 experts familiar with the laudal river reach for whether subjective aesthetics were better or worse than the situation actually observed with a discharge of 6 m3 s these responses were then rescaled relative to the reference scenario a in the mcda see supplementary material s5 for further documentation of the sub network 2 4 mcda using an object oriented bayesian network oobn an oobn is a hierarchical representation of a joint probability distribution over a set of random variables it consists of a graphical structure describing dependence and independence relations between variables in the model represented as the nodes in the graph the dependence relations are quantified using conditional probability distributions the hierarchical structure is created through the use of instance nodes which are realizations of self contained sub networks within a network class an instance node is connected to nodes in the encapsulating network class through its interface nodes an oobn can be augmented with decision and utility nodes and used to find decisions with optimal utility considering joint probabilities fig 3 shows the oobn for the mcda of environmental design in the laudal river the model structure shows the multi attribute value theory mavt approach to mcda that was implemented including from top to bottom in the figure design alternatives design characteristics of the alternatives impacts scaling valuation of impacts and weighting of scaled impacts weighting of impacts is conditional on interests from different stakeholder perspectives using oobn as a meta model to integrate different disciplinary sub models koller and pfeffer 1997 barton et al 2016b requires i converting model simulations into conditional probability tables in the main network and ii linking the expert belief based sub models to comparable assessment criteria for mcda mavt further steps include iii scaling normalizing valuing impacts so they can be compared iv eliciting relative importance of impacts expressed as relative weights and v comparing alternatives based on the summed weighted impacts these are discussed in turn below i learning conditional probabilities from model simulations increase in smolt productivity and opportunity costs of foregone power in scenarios p1 p4 relative to scenario a were processed in stata and then imported to hugin expert s learning wizard we used the learning wizard to discretize probability distributions of smolt and hydropower generation then to generate conditional probability tables for all combinations of minimum flow and habitat restoration measures in table 1 the necessary path condition npc algorithm hugin 2014 was used to learn the conditional probability distributions of the smolt hydropower impact combinations in the mcda network fig 3 discretization of high resolution near continuous input data to interval distributions is necessary in order to combine these different data sets with discrete expert judgements on mesohabitat fishability and riverscape aesthetics discussed below differences between scenario a and p1 4 were calculated in stata outside hugin to avoid information loss in discretizing probability distributions for input data coming from algorithms and models using high resolution data daily time step probability distributions were discretized as equal intervals in the likely impact range of each node to ease visual interpretation of differences between scenarios ii integrating bayesian belief sub networks sub networks are linked to the main network through input output nodes sub networks for individual impacts can easily be updated without changes needing to be made to the upper level network johnson et al 2013 iii scaling impacts the different units of impact for cost smolt fishability and aesthetics are scaled normalized in separate nodes yellow fig 3 criteria weighting in mcda using multi attribute valuation theory is often linear and deterministic belton and stewart 2002 on the other hand conditional probability tables in bayesian networks allow for non parametric non linear scaling valuation of impacts see supplementary material s6 for further documentation of the scaling of impacts on cost smolt fishability and aesthetics iv eliciting preferences using relative weights six stakeholder members of the reference group for environmental design in the mandalselva river volunteered to provide information on the relative importance of impacts in an anonymous questionnaire while the six participating stakeholders did not represent all user groups they represent a variety of institutions and user types serving to demonstrate preference variation in an mcda stakeholders granted permission to publish their preference weighting without reference to institutional affiliation with the exception of actor a who allowed the identification of hydropower interests fig 4 stakeholders were asked to distribute 100 points across the different criteria relative to their importance in our bayesian network application of mcda we model these importance weights as probability distributions generated by variance in preferences across stakeholders fig 5 shows how the relative importance of weights is represented by probability distributions conditional on different actor interests anonymized for this paper see supplementary material s7 for further details on stakeholder s relative weighting of impacts 3 results we focus the presentation of results on how inference and diagnostic features of bayesian belief network can be used to see the outcomes of mcda with different types of evidence default model without evidence fig 6 actor interest evidence fig 7 impact criteria weighting evidence fig 8 fig 6 illustrates the default model with no evidence inserted in which all decision alternatives are equally likely indicated by equal probabilities in the left hand side of the node monitor each node monitor shows the expected marginal utility of each state of that node variable in right hand side of the node monitor inserting evidence in different parts of the model provides different diagnostic perspectives once the network has been compiled evidence in any part of the network can be assessed with no further computing time we used hugins web deployment of probabilistic graphical models madsen et al 2013 to deploy the model online where users can experiment with different scenarios and inferences http demo hugin com example mcda oobn the decision node in fig 6 indicates the expected utility of each environmental design scenario see also fig 5 for a zoomed in version scenario p2 wp g with 6 m3 s winter and summer flow removal of weirs 3 5 8 and use of spawning gravel generates the highest expected utility of all scenarios assuming all stakeholders have equal weight scenario p2 wp g has the highest expected utility when all actors preferences have equal probability weight notably streamflow measures alone scenario p1 p2 p4 have roughly about half the utility of measures with habitat improvements including weir removal and spawning gravel this total expected utility perspective on the mcda does not reflect the marginal contribution of individual design features fig 7 illustrates the use of bayesian network in inference mode for diagnostic reasoning it illustrates an actor interest perspective on the mcda in which a specific actor s perspective chosen as evidence lower panel with the chosen actor s preference weights middle panel the bayesian network is used to infer utilities of the environmental design alternatives for that specific actor upper panel this is a unique model analysis feature of implementing mcda in a bn actor a left hand panels has a small negative expected utility 0 04 if no further evidence is available on design alternatives looking more closely at the design alternatives upper left hand panel p4 scenarios have unequivocally negative utility both p1 and p2 are positive if habitat remediation measures are included with a slight preference for p1 scenarios costs have the highest weight for actor a p cost 53 p smolt 30 p fishability 10 p aesthetics 7 for actor c all alternatives are positive with a preference for p2 wp g e utility 0 67 preferences are dominated by p smolt 70 actor d has a preference for p4 wp g with weir removal alternatives influencing choices heavily preferences are dominated by p aesthetics 85 fig 8 shows an impact criteria weighting perspective on the mcda demonstrating another form of diagnostic reasoning using bn in inference mode what alternatives are preferred when an extreme positions is taken with all preference weight given to a specific criteria the network infers the utility of each environmental design scenario when a specific criteria is allocated 100 importance middle panel the relative importance of the selected criteria for each actor is shown in the lower panel for example if costs are the only decision criteria p1 scenarios have the lowest foregone hydropower production hence the lowest negative utility and are preferred cost has greatest relative importance for actor a 49 scenarios with more flow weir removal and spawning gravel are preferred if smolt is the only decision criterion notably spawning gravel has a relatively large impact on utility for the fishability criteria once weirs are removed changes in flow have little impact on utility this is because of the low variability in salmon habitat extent for the flow ranges of a restored riverbed finally riverscape aesthetics require a combination of weir removal combined with greater flow weir removal more than doubles utility if flow is increased from 3 m3 s p1 to 6 m3 s p2 4 discussion 4 1 implications for environmental design decisions in the mandalselva river as in bustos et al 2017 we find that habitat remediation measures have a larger impact on utility than changes in environmental flow did our addition of fishability and aesthetics impacts modify conclusions about preferred flow scenarios weir removal had a large positive impact on fishability but assuming weir removal is in place increases in flow had small or no net effects across different mesohabitats that are attractive for fishing weir removal had a relatively small impact on aesthetic value of the riverscape which was reinforced by increased eflow we note that these considerations are the result of scaling of impacts by an expert panel while stakeholders provided the relative weighting of the importance of each impact different decision alternatives could have been possible if stakeholders had also been asked to evaluate the scaling of fishability and aesthetics 4 2 potential improvements in sub models the results of the mcda are sensitive to assumptions in the sub models a more detailed discussion of potential sub model improvements can be found in supplementary materials here we summarise the challenges of identifying uncertainty in the submodels assessment of impact criteria in mcda often assumes that criteria are independent while this is often not the case in environmental management beinat 1997 in our case several interdependencies are modelled outside the oobn fig 1 and as such are not represented explicitly in the bayesian network fig 3 change in wetted area assessed using the hydraulic model determines the availability of salmon habitat simulated in the salmon population model mesohabitat mapping and photo scenarios water velocity and depth determine mesohabitat quality for fishing water velocity was used by experts to adjust illustrations in the photo scenarios ib salmon was simulated for changes in wetted area but water velocity and depth do not determine smolt productivity directly size of wetted area is a proxy for combined effect of flow and depth bayesian networks are ideal for specifying the conditional probabilities due to ecosystem functional relationships between impacts we could not make full use of this feature in this study due to limitations in jointly simulating sub models in the mcda appraisal process management options under investigation should ideally be iteratively updated by repeated model simulation for this to be possible simulation models need to have faster run times to allow for updating of management assumptions in our case the hydraulic modelling of wetted area and ib salmon is the keystone in the model chain at present the capabilities of ib salmon for uncertainty analysis are limited to sensitivity analysis due to long run times of dynamic individual based population simulation models this limited the combinations of environmental flow and habitat restoration measures that we could simulate and emulate in a sub network within a oobn in future applications hugin s machine learning functionality could be used emulate the computationally demanding dynamic models improvements would include monte carlo markov chain capabilities in ib salmon and hydraulic models combined with greater computing power furthermore linked simulation is important in capturing the covariation across impacts power loss and smolt production without which joint uncertainty across all impacts may be overestimated barton et al 2016a better modelling of covariance for qualitative assessments such as river aesthetics and mesohabitat fishability could also be achieved with more resources in future applications in our simple example we only modelled these impacts for summer conditions assuming no impacts in winter or spring 4 3 uncertainty analysis in mcda using oobns our mcda application in oobn demonstrates how bayesian inference can be used for rapid sensitivity analysis from different perspectives providing evidence to different nodes in the network the oobn hugin computes and displays marginal utilities of all states in the model marginal utilities can be interpreted directly as the relative influence of any particular node state on the utility of alternative decisions the deployment of the model online makes experimentation with different conditioning factors accessible without technical expertise madsen et al 2013 while more participatory appraisal of uncertainty in mcda is facilitated by a bayesian network approach the explicit quantification of uncertainty imposes additional assumptions of its own simulation results from hydropower production have high temporal resolution we discretized the daily cost simulation output into a limited number of intervals representing annual totals which leads to some information loss uusitalo 2007 this is an example of an information cost of mcda integrating across precise quantitative and less precise qualitative impacts structural robustness analysis can be carried out by implementing bayesian network with nodes with high low resolution in discretization of continuous variables nodes with more states increase the complexity of a network exponentially marcot et al 2006 value of information voi analysis can be used to compare how much entropy is reduced in a specified decision node from obtaining additional evidence in different nodes in a bayesian network hugin 2014 unfortunately voi comparing variables in all submodels in an oobn is not possible to carry out in hugin software only the variables in the main model are assessed for voi to cover all variables a non hierarchical model could be specified this could have been manageable in our simple network however in mcda with more criteria and with sub models with many nodes this can result in a complex network structure which does not communicate the model purpose very well these are all examples of increasing information costs with increasing diverse and plural valuation barton et al 2017 4 4 valuation biases in mcda in many circumstances allowing experts to provide uncertainty judgements and avoiding forcing precise estimates from them is fundamental to a fair and trustworthy representation of value functions however it complicates the computational requirements of the model beinat 1997 in this paper we show that conditional probability tables in bayesian networks provide an easy implementation of the approach to value function uncertainty in mcda advocated by beinat 1997 the approach splits preference elicitation into i scaling of value functions and ii direct weight rating while it allows impacts to be compared using multi attribute utility functions it splits the preference system in two parts and is prone to error accumulation beinat 1997 for the convenience of demonstrating mcda using bayesian networks we assigned the role of value scaling to experts it could have been carried out by stakeholders or users in a more participatory implementation for example the hydropower company s willingness to forego power production could have been implemented as a non linear scaling function with a threshold for some level of cost considered disproportionate similarly specific user groups could have been asked to assess and scale fishability fishers and aesthetics visitors and residents aas and onstad 2013 consistent weighting of impacts depends on stakeholders being familiar with the full range of each impact for the best and worst alternatives weighting is jointly determined with value scaling beinat 1997 stakeholders involved were member of the environmental design project reference group and generally familiar with alternatives under consideration they had little difficulty in providing quantitative weights through a questionnaire and in comparing their weights to guesses about other stakeholders weights however the range of impacts was not presented to them in the questionnaire itself and we do not know whether availability of this additional information would have changed their relative weights these limitations could have been addressed by a different participatory process in our study and we believe could be easily accommodated by an object oriented modelling approach to mcda 4 5 stakeholder assessment mcda during the final year of our study 2016 the hydropower operator agder energy removed all weirs and conducted other habitat remediation measures in the laudal stretch the mcda and stakeholder preference survey were conducted prior to removal of the weirs in 2015 2016 final discussions with stakeholders regarding the relevance of the mcda was conducted after weir removal january 2017 stakeholder assessed advantages and disadvantages of multi criteria decision analysis as support for environmental design in the mandalselva stakeholders found bayesian networks to be a transparent and systematic documentation of multiple impacts and differences between their interests however stakeholders also found the mcda challenging because of their unfamiliarity with the two new criteria we introduced in our study quantitative impact assessments of fishability and aesthetics some stakeholders found the choice of mesohabitat quality as an excessively narrow proxy for fishability although the stated purpose of the mcda was to evaluate alternatives across initially incommensurable impact indicators some stakeholders found the analysis to be incomplete in not having valued all ecosystem services monetarily some stakeholders admitted that their preferences expressed in terms of weights were tentative due to their lacking a formal mandate and or a formal hearing process not being integrated in the study finally stakeholders were surprised at what they considered to be a low sensitivity of decision alternative rankings to changes in preference weights further discussion of the stakeholder evaluation can be found in supplementary material s8 the limitations they point out are in part due to the focus of the research in demonstrating mcda implementation in a bayesian network rather than a contracted decision support consultancy we think that mcda modelled in bayesian networks could go some way to formalizing preference uncertainty and stakeholder representativity using conditional probability distributions in the current model impact criteria weights are represented as conditional probability distributions while each stakeholder s preferences have equal weight a utility function could also be defined in which stakeholders preferences are assigned probabilities depending on how representative they are in the affected population the insensitivity of outcomes to stakeholder preferences may in part be a feature of bns modelling of impacts of different river regulation alternatives as probability distributions our analysis shows that the more uncertain modelled environmental impacts are the less the ranking of alternatives is sensitive to stakeholder preference assessment under reasonable assumptions about preferences such as excluding lexicographic preferences assigning 100 weight to a particular criterion while this initially looks like a barrier to applying mcda the oobn helps us to formulate critical questions about the value of information in collecting i stakeholders preferences versus spending limited research resources on improving ii the accuracy of impact assessments or iii conducting pilot projects and experiments to reduce uncertainty effectively a bayesian network helps researchers ask diagnostic questions of an mcda regarding the value of information across different types of environmental appraisal activities including stakeholder preference assessment 5 conclusions the main aim of our paper was to demonstrate the use of bayesian networks in mcda for environmental management we demonstrate how mcda using multi attribute value functions could be implemented in an object oriented bayesian network with decision and utility nodes we show how the oobn can be used to integrate quantitative model simulation with qualitative expert judgement models of impacts we integrated available models and data hydropower and smolt production and created new expert based models fishability and aesthetics the way each of the submodels deals with uncertainty in parameters and causal factors is different and at first glance inconsistent however this is the messy reality of integrating available knowledge in decision support the strength of the object oriented modelling approach is that different sub models with their assumptions can be integrated in a single mcda using bayesian networks to diagnose their impacts on decision options the paper also demonstrates how conditional probability tables used in bayesian networks are useful for modelling uncertainty in value scaling functions and for formalizing the variation in criteria weight rating due to the diversity of stakeholder preferences to our knowledge this is the first example of mcda in norway integrating assessment of eflow and morphological restoration measures in regulated rivers our impact assessment spans the provisioning ecosystem services of hydropower production cultural ecosystem services of recreational salmon fishing and riparian landscape aesthetics and the supporting services of habitat quality for salmon smolt it is also the first environmental application of mcda using a bayesian network approach that we know of in the literature the study also provided practical recommendations on environmental design and hydro power regulation of the laudal stretch of the mandalselva river in norway our analysis reinforces and elaborates on an earlier cost effectiveness assessment by bustos et al 2017 which pointed out that morphological river restoration measures can compensate for lower eflows our analysis extends the scope of this previous study with an assessment of fishability and riverscape aesthetics we find that eflows have a smaller effect on fishability and aesthetics than if they are combined with physical weir removal the study provides further support for the argument that physical river restoration can reduce eflow requirements and thereby reduce the loss in hydropower production while at the same time gaining satisfactory conditions for angling and landscape aesthetic interests funding ecomanage funded by the research council of norway contract no 215934 e20 ecomanage is organized under the research centre cedren centre for environmental design of renewable energy www cedren no software and or data availability software https www hugin com online model http demo hugin com example mcda oobn methodsx paper with supplementary material https doi org 10 1016 j envsoft 2019 104604 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank jannicke moe for useful comments on the manuscript we thank svein haugland agder energi produksjon jo halleraker and svein skogen in the norwegian environment agency for participation in a number of user group meetings and continued feedback on the modelling results throughout the ecomanage project improved development and management of energy and water resources we also want to thank members of the mandal river miljødesign reference group for participating in the stakeholder survey and focus group to assessing the model outcomes all errors and omissions in this article are the responsibility of the authors appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104604 
26093,the paper demonstrates the use of bayesian networks in multicriteria decision analysis mcda of environmental design alternatives for environmental flows eflows and physical habitat remediation measures in the mandalselva river in norway we demonstrate how mcda using multi attribute value functions can be implemented in a bayesian network with decision and utility nodes an object oriented bayesian network is used to integrate impacts computed in quantitative sub models of hydropower revenues and atlantic salmon smolt production and qualitative judgement models of mesohabitat fishability and riverscape aesthetics we show how conditional probability tables are useful for modelling uncertainty in value scaling functions and variance in criteria weights due to different stakeholder preferences while the paper demonstrates the technical feasibility of mcda in a bn we also discuss the challenges of providing decision support to a real world habitat remediation process graphical abstract image 1 keywords valuation bayesian network bn multi attribute valuation theory mavt atlantic salmon water framework directive wfd disproportionate cost good ecological potential river aesthetics angling 1 introduction environmental flow eflow has been defined as the hydrological regime required to sustain freshwater and estuarine ecosystems and the human livelihoods and well being that depend on them acreman et al 2014 concessions to regulate rivers for hydropower production in norway have historically evaluated eflow requirements and physical habitat mitigation measures separately it is unusual to explicitly compare costs against environmental objectives in order to determine eflow despite the assessment of disproportionate cost being required by the water framework directive wfd finstad et al 2007 recently norwegian authorities conducted a national screening project to prioritize the renewal of existing concessions where the most negative environmental impacts could be reduced at the lowest possible foregone power production sørensen et al 2013 they prioritized concession renewals based on an evaluation of foregone hydropower revenue and the magnitude of environmental improvements on important fish populations biodiversity and landscape impacts and recreational use one of the shortcomings of the study was not evaluating the potential for physical habitat restoration measures to partly substitute for stricter environmental flow requirements that follow from the application of the wfd in the revision process not considering the synergistic interactions between environmental compensation measures means that the costs of achieving good ecological potential under the wfd are likely to be overestimated barton et al 2016b more broadly habitat restoration on site combined with offsetting measures can offer net gains to biodiversity bbop 2009 as well as to other user interests such as recreation aas and onstad 2013 the central handbook on environmental design in regulated salmon rivers in norway forseth and harby 2014 outlines steps for water negotiations as input to action plans for rivers but proposes no formal method for assessing trade offs across management objectives in this study we propose such a formalized multi criteria decision analysis mcda to evaluating the trade offs between hydropower production and cultural ecosystem services of recreational salmon fishing and riparian landscape aesthetics and the supporting service of habitat quality for salmon smolt we demonstrate how different quantitative and qualitative impact assessments can be combined in an object oriented bayesian network oobn with nodes for valuation functions criteria weights and decision nodes in order to consistently integrate uncertainty from different impact model domains within mcda the model is deployed in an online interface to ease communication with a wider community to our knowledge this is the first example in norway of mcda integrating assessment of eflows and physical habitat restoration measures through the application of a bayesian network approach 1 1 environmental design in hydropower river regulation research linking the economics of hydropower operation and physical river restoration measures to environmental outcomes has been limited due to the computational complexities of linking biophysical and economic models of varying resolution and data sources barton et al 2010 charmasson and zinke 2011 niu and insley 2013 person et al 2014 bustos et al 2017 jager and smith 2008 found that earlier studies have optimized hydropower production against simple fish habitat objectives or ignored hydropower objectives altogether in optimizing hydraulic and fish stocking objectives forseth and harby 2014 propose an environmental design approach to regulated salmon rivers that brings together the state of knowledge on flow requirements of hydropower environmental flows and morphological river restoration measures their handbook proposes that trade off appraisals be made through water negotiations based on appraisal of multiple environmental design objectives recently bustos et al 2017 demonstrated cost effectiveness comparing hydropower production economics and salmon habitat restoration costs against productivity of atlantic salmon they integrated a series of models including hydropower production river hydraulics channel wetted area and salmon smolt productivity while their study used historical hydrological data the cost effectiveness of alternatives was assessed in terms of mean smolt production per norwegian kroner nok in foregone power and remediation costs in the present study we extend the bustos et al 2017 cost effectiveness analysis to include recreational fishing and river aesthetics we assess whether the conclusions from their cost effectiveness analysis are modified by taking a multi criteria decision approach that includes cultural ecosystem services which may have non linear relationships to eflows 1 2 modelling uncertainty in environmental flows design acreman et al 2014 reviewed the changing role of ecohydrological science in guiding eflows the hydrological regime required to sustain freshwater and estuarine ecosystems and the livelihoods and well being that depend on them recent advances include modelling of dynamics in hydrological analysis of flow regimes and addressing more than one stressor flow channel morphology because all methods and data have a degree of uncertainty acreman and colleagues stress the importance of risk communication between scientists practitioners policy makers and the public the present study contributes to the broader literature on modelling river restoration in the context of environmental flows and approaches to explicit calculation of uncertainty in decision support models the present study builds on prior experiences in using bayesian networks bns to model river management decisions for example uusitalo et al 2005 estimated atlantic salmon smolt carrying capacity of rivers using expert knowledge in a bn in hugin software identifying large disagreement in experts judgement they conclude that operational management objectives other than those based on maximal smolt production levels should be considered in order to decrease the uncertainty connected with evaluation of management success chan et al 2012 used bayesian network models in netica software to model relationships between dry season flows in an australian river affected by agricultural irrigation and key aspects of biology of barramundi and sooty grunter fish species bns were largely based on expert knowledge due to the lack of quantified biological knowledge but could later be verification against field observations sometimes also called conditional probability networks bns have been used to formalize expert ecological knowledge of hydrological regimes on fish habitat shenton et al 2011 horne et al 2018 similarly gawne et al 2012 demonstrated the use of a bn as a decision support tool for water supply and quality of wetlands in australia to maximize outcomes on population health of four native fish shenton et al 2014 integrated dynamic simulations from a hydrological sub model with an established expert based ecological model of grayling fish habitat spawning larvae transport and recruitment in the present study we contribute to this literature by demonstrating a combined impact modelling of morphological measures and eflow marsili libelli et al 2013 implement an instream flow assessment method modelling uncertain habitat suitability in large scale river modelling in tuscany italy they use fuzzy logic to implement uncertainty in expert judgement about habitat suitability curves under variable hydrological conditions in the present study we show how uncertainty and non linearity in expert assessment of habitat suitability curves can be implemented in conditional probability tables 1 3 multi criteria decision analysis in a bayesian setting landis et al 2017 argue that bayesian networks could also be used as the computational background for mcda of risk based adaptive environmental management bayesian networks has been used in multi criteria decision analyses mcda to support decisions in different fields in the transport and safety field fenton and neil 2001 decommissioning of offshore oil and gas platforms henrion et al 2015 urban planning langemeyer et al 2020 in the diagnosis of significant adverse effect of the eu water framework directive on hydropower production in norway barton et al 2016b and in the evaluation of which nutrient abatement measure optimally improves the condition of a costal ecosystem lehikoinen et al 2014 huang et al 2011 conducted a review of environmental applications of mcda in the period 2000 to 2009 vassoney et al 2017 analyzed all peer reviewed articles on the use of multicriteria analysis for sustainable hydropower planning and management from 2000 to 2015 in either review no papers were cited that used bayesian network applications of mcda to environmental management of freshwater ecosystems however there is literature that supports the use of bayesian network as mcda tool ins this field varis 1997 documented and discussed the use of influence diagrams and belief networks and their relation to decision trees the examples provided were focused on environmental and resource management of freshwater and fisheries studies in addition varis and kuikka 1997 showed the appropriateness of bayesian networks for handling problems in environmental and resource management such as to asses and safeguard the wild salmon stocks in the baltic sea under the pressure of extensive commercial fishery of reared salmon they showed that bayesian networks enhance the communication among experts using analytically based arguments allowed to find inconsistencies in information and to calculate joint predictions supporting decision making based on more balanced information in recent years the use of bayesian network for mcda has increased for the field of environmental management in freshwater systems bayesian networks has been applied by stewart koster et al 2010 to support the prioritization of flow and catchment restoration alternatives including the cost effectiveness of the interventions holmes et al 2018 used bayesian networks as a support tool for stream fisheries management with the objective of diagnose the factors limiting stream trout fisheries related to impacts on fish migrations from the hydropower structure and operations in the northern hemisphere a bayesian network was used to combine and show the effectiveness of possible fishpass structures and the mortality rate from the hydropower turbines wilkes et al 2018 reichert et al 2015 discuss requirements for environmental decision support arguing that a combination of probability theory and scenario planning with multi attribute utility theory fulfills requirements for representation and quantification of scientific knowledge elicitation of societal preferences and communication with authorities politicians and the public they argue for explicit modelling of ecological state as separate from other ecosystem services in order to better account for complexity in valuation they present a multi criteria decision making process in which uncertainty is considered explicitly in multi attribute valuation functions beinat 1997 demonstrated how to model uncertainty in marginal value functions in multi attribute utility models for mcda in the present study we show how uncertainty in value functions can be implemented using conditional probability tables reichert et al 2015 conclude that more research is needed on value aggregation in environmental decision support and in particular non additive approaches that consider joint fulfillment of management goals mcda has been used extensively in evaluating river regulation in finland testing a variety of additive and non additive value elicitation techniques discovering differences in weighting of river restoration criteria across multiple stakeholders marttunen and suomalainen 2005 marttunen and hamalainen 2008 marttunen et al 2008 variation in stakeholder weighting of criteria is addressed through sensitivity analysis convertino et al 2013 demonstrated non linear mcda aggregation in sustainable river restoration alternatives using their model promaa which can use probability distributions of criteria utilities and weight coefficients for assessing probabilities of likely rank events based on pairwise comparison of alternatives in an integrated scale they demonstrated probability distribution of weights by assigning an arbitrary fixed standard deviation to all criteria weights instead of point values in the present study we demonstrate a criteria weight probability distribution generated by variation in stakeholder perspectives convertino et al 2013 emphasised the information costs involved in obtaining expert judgements of value in mcda with many criteria weight combinations considering information costs and value researchers have advised caution in modelling uncertainty without evaluating the decision support of additional probabilistic information in otherwise deterministic models uusitalo et al 2015 recommends using information value analysis to evaluate increasing model complexity linkov et al 2015 also argue for a weight of evidence approach to quantitative data integration in multi criteria decision analysis specifically advocating bayesian methods for value of information analysis in the present study we discuss the scope for conducting value of information analysis in object oriented bayesian networks oobn oobn make it possible to subdivide a network into submodels that can be independently specified by domain experts facilitating analysis and communication of complex nested model structure barton et al 2012 the paper is laid out as follows in section 2 materials and methods we present the mcda framework combining the chain of models employed by bustos et al 2017 to assess costs and smolt productivity we extend the impact assessment to the cultural ecosystem services of river aesthetics and angling experience we present how we use oobns to implement multi criteria decision analysis supplementary material further documentation of sub networks and assessments that were needed to generate them an online demonstration version of the network is available at http demo hugin com example mcda oobn in section 3 our results show how the features of bayesian networks can be used to appraise environmental design decisions from different perspectives section 4 discusses the differences between our findings for preferred management alternatives based on mcda and those of bustos et al 2017 based on cost effectiveness analysis we discuss the advantages and limitations of our application of mcda in an oobn 2 materials and methods in this section we first provide an introduction to the study area the management problem and define the scenarios for eflows and habitat restoration that were modelled we provide a summary of the chain of models employed by bustos et al 2017 to assess costs and smolt productivity we explain how this work forms the basis for our extension of the impact assessment to the cultural ecosystem services of river aesthetics and fishing experience supporting material provides additional details on sub models we then present the multi criteria decision analysis framework and how we use a bayesian network to implement it finally we discuss the diagnostic tools we use to assess the model 2 1 study area the mandalselva river basin is located in southern norway 58 n 7 e it originates in setesdalsheiene at 600 800 m a s l the river is 115 km long and has a catchment area of 1800 km2 it flows south to the town of mandal where the majority of the people 13 000 inhabiting the catchment live the remaining 5000 people live in rural areas of the catchment l abée lund et al 2009 the mandalselva river varies from low to high gradients over short distances allowing a combination of fast currents waterfalls slow deep pools wide and calm stretches and lakes the mandalselva was among the top 10 of norway s most productive salmon rivers before the acidification period appr from 1920 to 2000 which eradicated the atlantic salmon population in the river in the first part of the twentieth century hesthagen and hansen 1991 during this period several hydropower facilities were constructed including the lowest laudal hydropower plant located within the river reach available to atlantic salmon the final migration barrier is located 47 km from the sea at the kavfossen waterfall fig 1 this power station involved a tunnel bypass of the bulk of the flow and produced a 5 km long minimum flow river stretch atlantic salmon production has been restored after liming and re stocking hesthagen and johnsen 2004 in order to mitigate the aesthetic effects of the very low flow originally 250 l s and maintain a continuous water surface in the laudal minimum flow stretch 11 stone weirs and one concrete weir were constructed in the 1980s in this study we evaluate the potential effects of removing the stone weirs 3 5 and the kleveland concrete weir 8 fig 1 in order to improve salmon habitat and the river s suitability for salmon fishing fishability and aesthetics compared to costs of eflow and restorations measures 2 2 definition of management scenarios the overall aim of the mcda was to demonstrate how to model the optimal combinations of weir removal and discharge regimes measured by stakeholder preferences for hydropower production smolt production recreational fishability and river aesthetics we aimed to test alternative scenarios including a proposal by the norwegian water resources and energy directorate nve for a trial minimum eflow regime which more than doubled the previous voluntary spill regime at laudal in place since 1995 if salmon productivity could be improved by the trial eflow regime the hydropower producer s 1995 concession would be open to revision by the authorities the baseline scenario a represents the bypass flow prior to the introduction of nve s trial minimum flow regime implemented in 2013 nve s trial regime from 2013 was as follows labelled scenario p4 during the spring salmon smolt migration period set to last 14 days approximately 50 of the inflow released in the river to avoid smolt turbine entrainment during summer 8 25 m3 s are released depending on inflow to ensure both smolt production and upstream migration of adult salmon during winter 6 m3 s are released to ensure salmon juvenile winter survival scenario a is identical to bustos et al 2017 while the label p represents scenarios with photo manipulated representation of river aesthetics under different flow conditions intermediate scenarios p1 and p2 involved summer minimum flow releases at 3 and 4 6 m3 s respectively 1 1 scenario p3 was removed from the analysis because it lacked completely consistent photo scenario information weir removal decreases wetted river surface area but typically increases river flow speed thus potentially increasing salmon spawning area for any given flow rate fjeldstad et al 2014 increased flow speed can also increase recreational fishability in certain river segments while the impact on river aesthetics of decreased water surface area depends on recreational preferences pflueger et al 2010 the habitat modification tested in this paper concerns removal of weirs and the addition of spawning gravel to the river reach weirs 3 5 and 8 in the laudal residual flow reach were selected for further photo and mesohabitat analysis because of their recreational accessibility and hence potential impact on river aesthetics and perceived fishability of the river an overview of scenarios modelled in this paper is provided in table 1 where p denotes the photo scenarios weir removal and addition of spawning gravel were modelled in all p1 p2 and p4 flow scenario combinations a total of 16 4 2 2 combinations of measures across all models are assessed in this paper table 1 bustos et al 2017 simulated a number of other intermediate scenarios with only marginal differences to p1 p2 p4 see supplementary material s1 note summer low flow rules were defined by a step function conditional on a range of inflows in p2 and p4 aesthetic effects were evaluated at fixed flows 3 0 6 0 and 15 0 m3 s intermediate scenario p3 was removed because it lacked a complete photo scenario in the rest of the paper flow scenarios are labelled wp if weirs are removed and g if spawning gravel is added 2 3 ecosystem service sub models the ecosystem service cascade framework links ecosystem structure to function services benefits and values haines young and potschin 2010 later developments have emphasised the stepwise intervention of policy and management adding feedback loops and stepwise updating spangenberg et al 2014 barton et al 2017 hausknost et al 2017 fig 2 provides a conceptual overview of the cascade of ecosystem service models and the link to the mcda as used in this paper it shows how biophysical and economic submodels are integrated explicitly in the mcda using object oriented bayesian network functionality limited stepwise feedback was part of our study through i adjustment of environmental design scenarios modelled in bustos et al 2017 based on feedback from stakeholders to the outcomes for hydropower production wetted area and smolt production ii adjustment of impact weights based on updating of perspectives by hydropower company stakeholders and iii scaling of impacts through consultation with domain experts an overview of the sub models s1 s7 is presented below with further details available in the supplementary materials hydropower energy foregone relative to the voluntary baseline scenario a was calculated using a power production potential function considering turbine efficiency net head of water and average water flow through the turbine annual foregone energy output was scaled with an average price for may 2012 may 2013 in order to be comparable to the study by bustos et al 2017 fixed costs of weir removal and artificial establishment of spawning gravel were annualized and added to annual foregone revenues in each scenario resulting in an incremental annual cost for each scenario relative to scenario a see supplementary material s1 for further documentation the flow regime in the bypass section discussed above determines the wetted area wetted area was determined using hydraulics calculations in hec ras 1d hec ras 2008 using a model developed by fjeldstad et al 2014 the hydraulic model estimates changes in wetted area water velocity and water depth which are subsequently inputs to mesohabitat quality classification and mapping see supplementary material s2 for further documentation the effect of the hydrological regime on salmon production was determined using the individual based salmon population model ib salmon hedger et al 2013a hedger et al 2013 2018 this model simulates salmon population abundances across salmon life stages from juvenile fry parr smolt to adult adult at sea returning spawner using functions that govern how individual salmon respond to the environment such as temperature dependent growth and density dependent mortality the model is 1 dimensionally spatially explicit with the watercourse being compartmentalised into a series of sections 50 m in length smolt production the number of individuals migrating to sea was used as the metric of population condition as a supporting ecosystem service smolt production is a commonly used metric because it represents the stage at which the population has experienced all sources of juvenile mortality in the river and is relatively easy to measure empirically see supplementary material s3 for further documentation the hydraulic model hec ras uses mapped river cross sections and flows to simulate spatial distribution of water velocities and water depths we used these model data combined with judgement of salmon fishers in the research team pers com fjeldstad to classify physical conditions for fishing under different scenarios using the mesohabitat classification system by borsanyi et al 2004 different mesohabitat types were plotted by experts in the bypass area and one map was produced for each scenario and for each reach above weirs 3 5 8 maps of the extent of each habitat type were generated for specific river summer discharges 3 m3 s 6 m3 s 15 m3 s with and without weirs we generated a matrix of changes in mesohabitat area relative to the baseline scenario a summer situation minimum flow of 3 m3 s each mesohabitat class was then valued independently by 3 salmon fishers in the project team on a scale from 0 to 1 for their relative fishability mesohabitat mapping and expert scoring were combined in a sub network embedded in the oobn fig 2 see supplementary material s4 for further documentation of the sub model habitat remediation through weir removal affects riverscape aesthetics as wide and calm weir reservoir water surfaces are converted to a narrower but more variable river in fact accessibility of river reaches and hence potential aesthetic impacts was the determining factor for which weirs were selected for scenario modelling in this study we used photo scenario and photo simulation methods tress and tress 2003 junker and buchecker 2008 to assess visual effects of combinations of different low flow regimes with and without weir removal baseline photos at 6 m3 s were taken for river reaches around the selected weirs photoshop scenario illustrations were based on hec ras 1d hydraulic modelling data for the parameters wetted area water velocity and water depth s2 expert knowledge and systematic judgement were used to illustrate the specific water surface structures resulting from varying water velocities and water depths including light and color of the water surface and shadows additionally we used a 3 d visual modelling technique for the site with the largest weir structure at kleveland bridge pers com 3dsmia b dervo this resulted in 3 4 photoscenarios which were evaluated by 6 experts familiar with the laudal river reach for whether subjective aesthetics were better or worse than the situation actually observed with a discharge of 6 m3 s these responses were then rescaled relative to the reference scenario a in the mcda see supplementary material s5 for further documentation of the sub network 2 4 mcda using an object oriented bayesian network oobn an oobn is a hierarchical representation of a joint probability distribution over a set of random variables it consists of a graphical structure describing dependence and independence relations between variables in the model represented as the nodes in the graph the dependence relations are quantified using conditional probability distributions the hierarchical structure is created through the use of instance nodes which are realizations of self contained sub networks within a network class an instance node is connected to nodes in the encapsulating network class through its interface nodes an oobn can be augmented with decision and utility nodes and used to find decisions with optimal utility considering joint probabilities fig 3 shows the oobn for the mcda of environmental design in the laudal river the model structure shows the multi attribute value theory mavt approach to mcda that was implemented including from top to bottom in the figure design alternatives design characteristics of the alternatives impacts scaling valuation of impacts and weighting of scaled impacts weighting of impacts is conditional on interests from different stakeholder perspectives using oobn as a meta model to integrate different disciplinary sub models koller and pfeffer 1997 barton et al 2016b requires i converting model simulations into conditional probability tables in the main network and ii linking the expert belief based sub models to comparable assessment criteria for mcda mavt further steps include iii scaling normalizing valuing impacts so they can be compared iv eliciting relative importance of impacts expressed as relative weights and v comparing alternatives based on the summed weighted impacts these are discussed in turn below i learning conditional probabilities from model simulations increase in smolt productivity and opportunity costs of foregone power in scenarios p1 p4 relative to scenario a were processed in stata and then imported to hugin expert s learning wizard we used the learning wizard to discretize probability distributions of smolt and hydropower generation then to generate conditional probability tables for all combinations of minimum flow and habitat restoration measures in table 1 the necessary path condition npc algorithm hugin 2014 was used to learn the conditional probability distributions of the smolt hydropower impact combinations in the mcda network fig 3 discretization of high resolution near continuous input data to interval distributions is necessary in order to combine these different data sets with discrete expert judgements on mesohabitat fishability and riverscape aesthetics discussed below differences between scenario a and p1 4 were calculated in stata outside hugin to avoid information loss in discretizing probability distributions for input data coming from algorithms and models using high resolution data daily time step probability distributions were discretized as equal intervals in the likely impact range of each node to ease visual interpretation of differences between scenarios ii integrating bayesian belief sub networks sub networks are linked to the main network through input output nodes sub networks for individual impacts can easily be updated without changes needing to be made to the upper level network johnson et al 2013 iii scaling impacts the different units of impact for cost smolt fishability and aesthetics are scaled normalized in separate nodes yellow fig 3 criteria weighting in mcda using multi attribute valuation theory is often linear and deterministic belton and stewart 2002 on the other hand conditional probability tables in bayesian networks allow for non parametric non linear scaling valuation of impacts see supplementary material s6 for further documentation of the scaling of impacts on cost smolt fishability and aesthetics iv eliciting preferences using relative weights six stakeholder members of the reference group for environmental design in the mandalselva river volunteered to provide information on the relative importance of impacts in an anonymous questionnaire while the six participating stakeholders did not represent all user groups they represent a variety of institutions and user types serving to demonstrate preference variation in an mcda stakeholders granted permission to publish their preference weighting without reference to institutional affiliation with the exception of actor a who allowed the identification of hydropower interests fig 4 stakeholders were asked to distribute 100 points across the different criteria relative to their importance in our bayesian network application of mcda we model these importance weights as probability distributions generated by variance in preferences across stakeholders fig 5 shows how the relative importance of weights is represented by probability distributions conditional on different actor interests anonymized for this paper see supplementary material s7 for further details on stakeholder s relative weighting of impacts 3 results we focus the presentation of results on how inference and diagnostic features of bayesian belief network can be used to see the outcomes of mcda with different types of evidence default model without evidence fig 6 actor interest evidence fig 7 impact criteria weighting evidence fig 8 fig 6 illustrates the default model with no evidence inserted in which all decision alternatives are equally likely indicated by equal probabilities in the left hand side of the node monitor each node monitor shows the expected marginal utility of each state of that node variable in right hand side of the node monitor inserting evidence in different parts of the model provides different diagnostic perspectives once the network has been compiled evidence in any part of the network can be assessed with no further computing time we used hugins web deployment of probabilistic graphical models madsen et al 2013 to deploy the model online where users can experiment with different scenarios and inferences http demo hugin com example mcda oobn the decision node in fig 6 indicates the expected utility of each environmental design scenario see also fig 5 for a zoomed in version scenario p2 wp g with 6 m3 s winter and summer flow removal of weirs 3 5 8 and use of spawning gravel generates the highest expected utility of all scenarios assuming all stakeholders have equal weight scenario p2 wp g has the highest expected utility when all actors preferences have equal probability weight notably streamflow measures alone scenario p1 p2 p4 have roughly about half the utility of measures with habitat improvements including weir removal and spawning gravel this total expected utility perspective on the mcda does not reflect the marginal contribution of individual design features fig 7 illustrates the use of bayesian network in inference mode for diagnostic reasoning it illustrates an actor interest perspective on the mcda in which a specific actor s perspective chosen as evidence lower panel with the chosen actor s preference weights middle panel the bayesian network is used to infer utilities of the environmental design alternatives for that specific actor upper panel this is a unique model analysis feature of implementing mcda in a bn actor a left hand panels has a small negative expected utility 0 04 if no further evidence is available on design alternatives looking more closely at the design alternatives upper left hand panel p4 scenarios have unequivocally negative utility both p1 and p2 are positive if habitat remediation measures are included with a slight preference for p1 scenarios costs have the highest weight for actor a p cost 53 p smolt 30 p fishability 10 p aesthetics 7 for actor c all alternatives are positive with a preference for p2 wp g e utility 0 67 preferences are dominated by p smolt 70 actor d has a preference for p4 wp g with weir removal alternatives influencing choices heavily preferences are dominated by p aesthetics 85 fig 8 shows an impact criteria weighting perspective on the mcda demonstrating another form of diagnostic reasoning using bn in inference mode what alternatives are preferred when an extreme positions is taken with all preference weight given to a specific criteria the network infers the utility of each environmental design scenario when a specific criteria is allocated 100 importance middle panel the relative importance of the selected criteria for each actor is shown in the lower panel for example if costs are the only decision criteria p1 scenarios have the lowest foregone hydropower production hence the lowest negative utility and are preferred cost has greatest relative importance for actor a 49 scenarios with more flow weir removal and spawning gravel are preferred if smolt is the only decision criterion notably spawning gravel has a relatively large impact on utility for the fishability criteria once weirs are removed changes in flow have little impact on utility this is because of the low variability in salmon habitat extent for the flow ranges of a restored riverbed finally riverscape aesthetics require a combination of weir removal combined with greater flow weir removal more than doubles utility if flow is increased from 3 m3 s p1 to 6 m3 s p2 4 discussion 4 1 implications for environmental design decisions in the mandalselva river as in bustos et al 2017 we find that habitat remediation measures have a larger impact on utility than changes in environmental flow did our addition of fishability and aesthetics impacts modify conclusions about preferred flow scenarios weir removal had a large positive impact on fishability but assuming weir removal is in place increases in flow had small or no net effects across different mesohabitats that are attractive for fishing weir removal had a relatively small impact on aesthetic value of the riverscape which was reinforced by increased eflow we note that these considerations are the result of scaling of impacts by an expert panel while stakeholders provided the relative weighting of the importance of each impact different decision alternatives could have been possible if stakeholders had also been asked to evaluate the scaling of fishability and aesthetics 4 2 potential improvements in sub models the results of the mcda are sensitive to assumptions in the sub models a more detailed discussion of potential sub model improvements can be found in supplementary materials here we summarise the challenges of identifying uncertainty in the submodels assessment of impact criteria in mcda often assumes that criteria are independent while this is often not the case in environmental management beinat 1997 in our case several interdependencies are modelled outside the oobn fig 1 and as such are not represented explicitly in the bayesian network fig 3 change in wetted area assessed using the hydraulic model determines the availability of salmon habitat simulated in the salmon population model mesohabitat mapping and photo scenarios water velocity and depth determine mesohabitat quality for fishing water velocity was used by experts to adjust illustrations in the photo scenarios ib salmon was simulated for changes in wetted area but water velocity and depth do not determine smolt productivity directly size of wetted area is a proxy for combined effect of flow and depth bayesian networks are ideal for specifying the conditional probabilities due to ecosystem functional relationships between impacts we could not make full use of this feature in this study due to limitations in jointly simulating sub models in the mcda appraisal process management options under investigation should ideally be iteratively updated by repeated model simulation for this to be possible simulation models need to have faster run times to allow for updating of management assumptions in our case the hydraulic modelling of wetted area and ib salmon is the keystone in the model chain at present the capabilities of ib salmon for uncertainty analysis are limited to sensitivity analysis due to long run times of dynamic individual based population simulation models this limited the combinations of environmental flow and habitat restoration measures that we could simulate and emulate in a sub network within a oobn in future applications hugin s machine learning functionality could be used emulate the computationally demanding dynamic models improvements would include monte carlo markov chain capabilities in ib salmon and hydraulic models combined with greater computing power furthermore linked simulation is important in capturing the covariation across impacts power loss and smolt production without which joint uncertainty across all impacts may be overestimated barton et al 2016a better modelling of covariance for qualitative assessments such as river aesthetics and mesohabitat fishability could also be achieved with more resources in future applications in our simple example we only modelled these impacts for summer conditions assuming no impacts in winter or spring 4 3 uncertainty analysis in mcda using oobns our mcda application in oobn demonstrates how bayesian inference can be used for rapid sensitivity analysis from different perspectives providing evidence to different nodes in the network the oobn hugin computes and displays marginal utilities of all states in the model marginal utilities can be interpreted directly as the relative influence of any particular node state on the utility of alternative decisions the deployment of the model online makes experimentation with different conditioning factors accessible without technical expertise madsen et al 2013 while more participatory appraisal of uncertainty in mcda is facilitated by a bayesian network approach the explicit quantification of uncertainty imposes additional assumptions of its own simulation results from hydropower production have high temporal resolution we discretized the daily cost simulation output into a limited number of intervals representing annual totals which leads to some information loss uusitalo 2007 this is an example of an information cost of mcda integrating across precise quantitative and less precise qualitative impacts structural robustness analysis can be carried out by implementing bayesian network with nodes with high low resolution in discretization of continuous variables nodes with more states increase the complexity of a network exponentially marcot et al 2006 value of information voi analysis can be used to compare how much entropy is reduced in a specified decision node from obtaining additional evidence in different nodes in a bayesian network hugin 2014 unfortunately voi comparing variables in all submodels in an oobn is not possible to carry out in hugin software only the variables in the main model are assessed for voi to cover all variables a non hierarchical model could be specified this could have been manageable in our simple network however in mcda with more criteria and with sub models with many nodes this can result in a complex network structure which does not communicate the model purpose very well these are all examples of increasing information costs with increasing diverse and plural valuation barton et al 2017 4 4 valuation biases in mcda in many circumstances allowing experts to provide uncertainty judgements and avoiding forcing precise estimates from them is fundamental to a fair and trustworthy representation of value functions however it complicates the computational requirements of the model beinat 1997 in this paper we show that conditional probability tables in bayesian networks provide an easy implementation of the approach to value function uncertainty in mcda advocated by beinat 1997 the approach splits preference elicitation into i scaling of value functions and ii direct weight rating while it allows impacts to be compared using multi attribute utility functions it splits the preference system in two parts and is prone to error accumulation beinat 1997 for the convenience of demonstrating mcda using bayesian networks we assigned the role of value scaling to experts it could have been carried out by stakeholders or users in a more participatory implementation for example the hydropower company s willingness to forego power production could have been implemented as a non linear scaling function with a threshold for some level of cost considered disproportionate similarly specific user groups could have been asked to assess and scale fishability fishers and aesthetics visitors and residents aas and onstad 2013 consistent weighting of impacts depends on stakeholders being familiar with the full range of each impact for the best and worst alternatives weighting is jointly determined with value scaling beinat 1997 stakeholders involved were member of the environmental design project reference group and generally familiar with alternatives under consideration they had little difficulty in providing quantitative weights through a questionnaire and in comparing their weights to guesses about other stakeholders weights however the range of impacts was not presented to them in the questionnaire itself and we do not know whether availability of this additional information would have changed their relative weights these limitations could have been addressed by a different participatory process in our study and we believe could be easily accommodated by an object oriented modelling approach to mcda 4 5 stakeholder assessment mcda during the final year of our study 2016 the hydropower operator agder energy removed all weirs and conducted other habitat remediation measures in the laudal stretch the mcda and stakeholder preference survey were conducted prior to removal of the weirs in 2015 2016 final discussions with stakeholders regarding the relevance of the mcda was conducted after weir removal january 2017 stakeholder assessed advantages and disadvantages of multi criteria decision analysis as support for environmental design in the mandalselva stakeholders found bayesian networks to be a transparent and systematic documentation of multiple impacts and differences between their interests however stakeholders also found the mcda challenging because of their unfamiliarity with the two new criteria we introduced in our study quantitative impact assessments of fishability and aesthetics some stakeholders found the choice of mesohabitat quality as an excessively narrow proxy for fishability although the stated purpose of the mcda was to evaluate alternatives across initially incommensurable impact indicators some stakeholders found the analysis to be incomplete in not having valued all ecosystem services monetarily some stakeholders admitted that their preferences expressed in terms of weights were tentative due to their lacking a formal mandate and or a formal hearing process not being integrated in the study finally stakeholders were surprised at what they considered to be a low sensitivity of decision alternative rankings to changes in preference weights further discussion of the stakeholder evaluation can be found in supplementary material s8 the limitations they point out are in part due to the focus of the research in demonstrating mcda implementation in a bayesian network rather than a contracted decision support consultancy we think that mcda modelled in bayesian networks could go some way to formalizing preference uncertainty and stakeholder representativity using conditional probability distributions in the current model impact criteria weights are represented as conditional probability distributions while each stakeholder s preferences have equal weight a utility function could also be defined in which stakeholders preferences are assigned probabilities depending on how representative they are in the affected population the insensitivity of outcomes to stakeholder preferences may in part be a feature of bns modelling of impacts of different river regulation alternatives as probability distributions our analysis shows that the more uncertain modelled environmental impacts are the less the ranking of alternatives is sensitive to stakeholder preference assessment under reasonable assumptions about preferences such as excluding lexicographic preferences assigning 100 weight to a particular criterion while this initially looks like a barrier to applying mcda the oobn helps us to formulate critical questions about the value of information in collecting i stakeholders preferences versus spending limited research resources on improving ii the accuracy of impact assessments or iii conducting pilot projects and experiments to reduce uncertainty effectively a bayesian network helps researchers ask diagnostic questions of an mcda regarding the value of information across different types of environmental appraisal activities including stakeholder preference assessment 5 conclusions the main aim of our paper was to demonstrate the use of bayesian networks in mcda for environmental management we demonstrate how mcda using multi attribute value functions could be implemented in an object oriented bayesian network with decision and utility nodes we show how the oobn can be used to integrate quantitative model simulation with qualitative expert judgement models of impacts we integrated available models and data hydropower and smolt production and created new expert based models fishability and aesthetics the way each of the submodels deals with uncertainty in parameters and causal factors is different and at first glance inconsistent however this is the messy reality of integrating available knowledge in decision support the strength of the object oriented modelling approach is that different sub models with their assumptions can be integrated in a single mcda using bayesian networks to diagnose their impacts on decision options the paper also demonstrates how conditional probability tables used in bayesian networks are useful for modelling uncertainty in value scaling functions and for formalizing the variation in criteria weight rating due to the diversity of stakeholder preferences to our knowledge this is the first example of mcda in norway integrating assessment of eflow and morphological restoration measures in regulated rivers our impact assessment spans the provisioning ecosystem services of hydropower production cultural ecosystem services of recreational salmon fishing and riparian landscape aesthetics and the supporting services of habitat quality for salmon smolt it is also the first environmental application of mcda using a bayesian network approach that we know of in the literature the study also provided practical recommendations on environmental design and hydro power regulation of the laudal stretch of the mandalselva river in norway our analysis reinforces and elaborates on an earlier cost effectiveness assessment by bustos et al 2017 which pointed out that morphological river restoration measures can compensate for lower eflows our analysis extends the scope of this previous study with an assessment of fishability and riverscape aesthetics we find that eflows have a smaller effect on fishability and aesthetics than if they are combined with physical weir removal the study provides further support for the argument that physical river restoration can reduce eflow requirements and thereby reduce the loss in hydropower production while at the same time gaining satisfactory conditions for angling and landscape aesthetic interests funding ecomanage funded by the research council of norway contract no 215934 e20 ecomanage is organized under the research centre cedren centre for environmental design of renewable energy www cedren no software and or data availability software https www hugin com online model http demo hugin com example mcda oobn methodsx paper with supplementary material https doi org 10 1016 j envsoft 2019 104604 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank jannicke moe for useful comments on the manuscript we thank svein haugland agder energi produksjon jo halleraker and svein skogen in the norwegian environment agency for participation in a number of user group meetings and continued feedback on the modelling results throughout the ecomanage project improved development and management of energy and water resources we also want to thank members of the mandal river miljødesign reference group for participating in the stakeholder survey and focus group to assessing the model outcomes all errors and omissions in this article are the responsibility of the authors appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104604 
26094,geohazards e g floods and landslides are increasingly resulting in loss of life and economic damage process models can help the understanding of the frequency and occurrence of these hazards reducing their impact however models require significant resources to develop and ideally should be made widely available further integrated modelling demands that process models are discoverable available and ultimately linkable metadata standards are defined for data but the equivalent for process models are not yet available as an international standard or even as accepted best practice requirements have been gathered for model metadata using a questionnaire and a metadata schema has been developed the resulting schema split model code software and model instance application given that model codes are used multiple times international data metadata standards e g iso19115 have been used for the core of the standard the schema has been implemented via the pure portal www pureportal org which makes models available and forms the basis of an international standard for model metadata 1 introduction natural hazards are many and various and include floods droughts landslides earthquakes tsunamis and volcanic eruptions between 2006 and 2015 globally these events have resulted in 0 77 m deaths and injuries as well as causing 1 4 trillion usd in economic damage sanderson and sharma 2016 there is growing evidence that certain types of natural hazards have a greater economic impact related to economic growth population growth and the movement of population to vulnerable areas etkin 1999 as a result of this perceived increase in the impact of natural hazards over the last few decades decision makers have recognised that more resources are required to understand hazards and to simulate them responding to this need the uk s natural environment research council nerc instigated the probability uncertainty and risk in the environment pure programme www nerc ac uk research funded programmes pure the programme aims to both improve the understanding of natural hazards and to make the science underlying hazards much more accessible to decision makers and consists of a range of activities including large grants racer e g lickiss et al 2017 and credible e g simpson et al 2016 phd students secondments and the development of the pure portal to make hazard models more easily available to a range of users process models particularly of the environment are expensive to develop and apply requiring significant collective effort to code them and to ensure that they are working i e benchmarking them against other model codes or standard datasets and then to apply them to different areas this effort is best capitalised on by making the models available for others to use for related studies or even apply the model codes to different areas here we define model codes as the software developed to solve the governing equations with suitable boundary conditions and model instances as the application of the model codes to particular study areas however to make both model codes and their instances available for examination and to be applied to different problems then they have to be discoverable in other words enough information has to be made available for the models themselves to be found using suitable searches once found or discovered then the user has to make a decision on whether the model code is suitable for their purpose questions such as what mathematical equations have been used how have these equations been solved what boundary conditions can be applied all need to be addressed prior to a model user applying the model code or the results from the model instances further description or at least links to further information is required e g to websites with details as to how the model code works or pdfs of manuals so that the prospective user can understand in more detail what the model code actually does further the source code itself could be made available using online software repositories so that the functionality of the model code itself can be further reviewed integrated modelling is the relatively recent discipline of taking a question that is deceptively simple to articulate and to create a workflow which joins models of different origin together to address the question moore and hughes 2017 there are six main challenges model components are discoverable available and linkable they can be coupled semantics and ontologies are used to define models and their variables the linked model systems can be run as quickly as possible uncertainty is addressed and the results visualised in an appropriate form to address the question under consideration laniak et al 2013 by making models available for discovery the user can then start the process of assessing them to include them in their workflow for example does the model code and associated instance provide a solution that enables them to fully address their questions joining models or model coupling can take a number of different forms e g belete et al 2017 including passing data between models in files with defined formats joining model codes together to form a new combined code creating models as components and linking them at runtime and exposing models as a service given that these all require information about the model codes and their inputs and outputs then provision of both discovery i e basic information about the model codes and instances and technical metadata containing more details on the workings of the model code and its application is even more desirable 1 1 previous work whilst there are many metadata catalogues and repositories for spatial data sets e g data gov uk the equivalent for process models and their instances are much less common however a number of model catalogues have been created and zaslavskay et al 2014 review five existing ones which mainly contain model codes examples include the us based community surface dynamics modelling system csdms model catalogue peckham 2014 and the tess model registry funded by the eu which was created to store technical metadata for ecosystem management model instances model metadata is required to facilitate the comparison of climate models codes and their instances for international model inter comparison particularly for defining the physics of the models dunlap et al 2008 report on the earth system curator which has developed a metadata schema for describing climate models and their outputs this is important for understanding the differences in climate model behaviour based around core metadata which provides basic information on the model code developer other classes include modelling and data which describe the code and its output the modelling class is in turn populated by configuration grids and coupling which describe the physics numerical gridding and any model linkages respectively processes to automatically obtain metadata from the model instances are presented the schema itself is implemented in a website in the earth systems documentation project es doc org and contains descriptions of the main climate models currently under use e g uk s met office hadley centre model code there are no internationally recognised standards for mathematical model metadata however this is recognised as an issue by those who seek to promote the use of model metadata several initiatives e g harpham and danovaro 2015 have attempted to define de facto standards and common features required have emerged their work is based on iso19115 and involved developing a metadata schema for both discovering models as well as providing sufficient technical metadata for the user to decide whether the model is suitable for their use their approach is tied to a workflow involving p gridded or q flow data and as such limited to this application zaslavskay et al 2014 report five model catalogues each with their own metadata but that arrive at common features such as describing the model developers and code guardians a model code inventory has been built for hydrological models a collaboration between texas a m university and the us bureau of reclamation it provides a web based list of model codes which link to pdfs providing metadata in common with the work presented here it was driven using a questionnaire based approach singh et al 2006 the cuashi backed hydroshare initiative horsburgh et al 2016 has developed social objects to define entities including models with the aim of enabling both the model and their developers to be networked the aim of hydroshare is to promote not just the description and availability of model codes and their instances but also facilitate collaboration around them horsburgh et al 2016 the latter is achieved by using the concept of resources which can define a number of different entities including model program i e model code model instance as well as generic ones an example of using this schema using swat a watershed hydrological model code is presented in morsy et al 2017 who develop a metadata schema for a single model instance this is at odds with harpham and danovaro 2015 who develop a metadata approach for linking models however morsy et al 2017 separate model program code and model instance resources model instance the example they present is specific for hydrology only in the uk the fluidearth 2 catalogue holds metadata on linkable models harpham et al 2014 fluidearth is the implementation of the openmi model linking standard gregersen et al 2007 harpham et al 2019 and is designed to make the creation of openmi compliant models as straightforward as possible as part of the fluidearth initiative openmi fluidearth compliant models are described on the fluidearth portal to enable discovery of model codes that can be joined together into a composition or series of linked models however the linking of models requires that the variables that can be linked are described appropriately as a development of the csdms approach peckham 2014 a variable naming convention has been developed to describe state variables this convention based on providing a standardised list of variable equivalent to an ontology but with a strict naming convention based on a two part context descriptor followed by a variable name an example would be channel bed manning coefficient which describes the variable manning coefficient applied to the context of a channel bed defining the variable name explicitly has two advantages one is the ability to link the variable name to metadata categories within the csdms system and the other is to provide strict definition for variables which can be exchanged model components at runtime the latter is a pre requisite for model integration using a questionnaire based study funded by the nerc environmental data ned call hughes et al 2013 report that the specific challenges identified are 1 to ensure that the data used to create models are recorded and their origin known 2 the models produced are themselves available to run either by downloading or remotely via webservices 3 the results produced by these models can be obtained in a recognisable format further a number of attributes which are not typically captured by the often used metadata schemas applied to datasets e g iso 19115 are seen as being important in a model metadata scheme these are important considerations for the development of both model metadata and the model repositories that implement them whilst there are no internationally recognised standards for model metadata however there are significant examples of best practice in the climate dunlap et al 2008 hydrology horsburgh et al 2016 morsy et al 2017 singh et al 2006 and model integration harpham and danovaro 2015 peckham 2014 disciplines further model repositories have been created such as those reported by zaslavskay et al 2014 and the implementation of hydroshare described by morsy et al 2017 our work builds on that undertaken to date uses the best practice offered by preceding workers and includes the result of a questionnaire to gather the requirements of environmental modellers hughes et al 2013 to design a model metadata scheme that can be applicable for a wide range of environmental models only by separating model code from instance can the flexibility of approach adopted by morsy et al 2017 building on iso19115 as described by harpham and danovaro 2015 and producing an open repository a development of hydroshare can model codes and their instances be made available further the approach presented below is extensible so that model integration can be facilitated as espoused by harpham and danovaro 2015 this paper aims to describe the development of the metadata schema for hazard models as used for the pure portal given that there is currently no international standard for model metadata it sets out the process by which the metadata schema has been developed firstly metadata schemas for data are described the base work on which the requirements for model metadata have been gathered is then detailed and the model presented finally the implementation of the model metadata schema in the pure portal is outlined it is hoped that by setting out the model metadata schema and its reasoning behind it and its use that this can be used as the basis for an international standard for example with the open geospatial consortium ogc 2 current metadata schemes for data resources metadata is provided by publishers to describe their resource e g datasets to aid its discovery exchange and re use a metadata schema defines a set of elements where each element refers to one aspect of the resource description for example the title or the author these elements are sometimes grouped into sets of elements useful for different purposes for example discovery metadata contains the elements useful for finding a resource of interest technical or use metadata contains the elements necessary for using the resource correctly for each element the schema defines the semantics i e the meaning intended to be conveyed and the rules for the content i e the data type the domain range from which values can be used whether the element is mandatory optional or conditional the schema will also define how the elements are related to each other in terms of a hierarchy and cardinality i e how many of the elements are the same some metadata standards inherently define an encoding by which the metadata must be formatted in a data structure in other cases the method for encoding the metadata is described in a guidance document or complementary standard by using these accepted schemes differing levels of interoperability can be achieved at the simplest level the metadata can be presented using the agreed term labels and natural language definitions so a person can understand it and manually compare it to metadata records to achieve additional syntactic and schematic interoperability the metadata can be encoded according to the schema rules and appropriate data format in such that an application can consume the metadata chan and zeng 2006 mappings crosswalks or translations between alternative metadata schemes or the individual elements within a scheme can be used so that metadata can be aggregated from various schema sources for even greater interoperability the data content of the elements within a metadata record instance can be encoded using the world wide web consortium w3c rdf resource description framework 1 1 www w3 org rdf standard such that a full description of the term its meaning and its relations to other terms can be easily obtained as a web resource from a unique resource identifier uri the use of controlled vocabularies is discussed further below there are a significant number of standards for both discovery and technical metadata for data of different types there are also a range of services by which metadata can be recorded and the data stored alongside these data as previously outlined by hughes et al 2013 nerc itself puts a significant amount of effort into storing data and model results and making the metadata available for example there are seven data centres and the data catalogue service dcs to search metadata for datasets stored in the nerc data centres a useful summary of metadata standards across a range of disciplines is available on the research data alliance rda metadata directory 2 2 http rd alliance github io metadata directory standards the commonly used standards relevant to this study are described below 2 1 dublin core the dublin core schema 3 3 dublincore org is a metadata standard with widespread use the dublin core metadata element set is the original set of terms published it consists of fifteen optional metadata elements for cataloguing web resources and physical resources this set of elements has been endorsed as international organization for standardization iso standard 15836 2009 internet engineering task force request for comment ietf rfc 5013 and us national information standards organisation niso standard z39 85 the wider and full set of dublin core metadata terms including the 15 original ones are maintained by the dublin core metadata initiative dcmi this set includes spatial and temporal extent provenance and various ways of relating different resources source ispartof isformatof etc dcmi publishes a number of encoding guidelines as recommendations for example in rdf extensible markup language xml and hypertext markup language html 4 4 dublincore org resources expressions 2 2 international organization for standardization iso standards the iso standards 19115 5 5 iso 19115 2003 www iso org standard 26020 html iso19115 2014 www iso org standard 53798 html and 19119 6 6 iso 19119 2005 www iso org standard 39890 html iso19119 2016 www iso org standard 59221 html are internationally adopted schema for describing geographic information and services respectively the first edition of iso 19115 was published in 2003 but it has since been split into parts iso 19115 1 2014 contains the fundamentals of the standard iso 19115 2 2009 contains extensions for imagery and gridded data and iso ts 19115 3 2016 provides an xml schema implementation for the fundamental concepts compatible with iso ts 19138 2007 geographic metadata xml or gmd the iso19115 19119 standards are described using universal modelling language uml but a complementary iso standard 19139 7 7 iso ts 19139 2007 www iso org standard 32557 html defines how the metadata should be serialised and encoded in xml conformance against the 19115 19119 standards is therefore most easily checked by presenting the metadata in xml and validating them against the 19139 xml schema the standard is primarily aimed at geographic datasets and services but the domain based list of scopes that the metadata can be used for includes non geographic datasets models information applies to a copy of imitation of an existing or hypothetical object and software information applies to a computer program or routine though there are no additional metadata elements that specifically relate to these resource types the research data alliance rda has provided links to mappings of the iso 19115 standard from various community standards and exchange formats 8 8 rd alliance github io metadata directory standards iso 19115 html the set of elements in the iso standard is large and would unlikely to ever all be used in one metadata record but only a small number of elements are in the core mandatory set members of different discipline domains and national governments have therefore created extensions and profiles of the iso standard that enforce tighter constraints on cardinality and mandatory elements while removing many optional elements the profiles used for inspire 9 9 inspire ec europa eu infrastructure for spatial information in europe and uk gemini 10 10 www agi org uk agi groups standards committee uk gemini geo spatial metadata interoperability initiative are discussed below 2 3 inspire discovery metadata inspire is an infrastructure for spatial information within europe and is an european commission ec directive the purpose is to enable interoperability and sharing of data that are related to activities or policies having impact on the environment inspire has published a specification for metadata used for data discovery based on the iso19115 19119 application profile metadata for geographic information with a definition of core metadata elements from this required for inspire compliance some mandatory iso metadata elements are not mandatory in the inspire standard therefore need to be included as additional elements in order to make an inspire record into a valid iso record these are noted in the implementing rules technical guidance inspire maintenance and implementation group mig 2017 the inspire metadata implementing rules were revised in march 2017 to version 2 0 1 the encoding continues to be based on iso 19115 2003 not on the latest version iso 19115 1 2014 resources that are in scope of the inspire directive are those that are datasets series collection of datasets or services are digital have a geospatial component and the subject of which is one or more of the defined environmental themes process modelling code is not in scope of inspire 2 4 uk gemini within the united kingdom uk the uk gemini metadata standard association for geographic information 2012 represents the uk implementation of inspire and as such also conforms to the iso 19115 19119 standards all mandatory inspire elements in version 2 1 of the implementing rules are mandatory within uk gemini 2 2 revision 2 3 is in progress at the time of writing and will be conformant with the inspire implementing rules 2 0 1 the gemini encoding guidelines for each version ensure that the metadata is encoded in valid iso 19139 xml which will also be valid against the relevant inspire standard 2 5 dcat the data catalog dcat vocabulary 11 11 www w3 org tr vocab dcat is a recommendation of the w3c it is a resource description framework rdf vocabulary designed to facilitate interoperability and federated searches across data catalogues published on the web and also facilitate digital preservation it makes extensive use of terms from other vocabularies in particular dublin core and only defines a minimal set of classes and properties of its own an application profile dcat ap was published by the interoperability solutions administrations isa 12 12 ec europa eu archives isa programme of the eu for data portals in europe 13 13 joinup ec europa eu release dcat ap v11 dcat ap has itself been extended to geodcat ap to better handle geospatial datasets and services and to provide an rdf representation for the iso 19139 xml metadata elements required by inspire a stylesheet transformation has been made available for converting between iso19139 xml and geodcat ap rdf 14 14 github com geocat iso 19139 to dcat ap blob master iso 19139 to dcat ap xsl as stated by the authors the geodcat ap specification does not replace the inspire metadata regulation nor the inspire metadata technical guidelines based on iso 19115 and iso19119 its purpose is give owners of geospatial metadata the possibility to promote greater and more efficient use of their datasets by providing an additional rdf syntax binding 15 15 joinup ec europa eu node 154143 2 6 climate and forecast cf metadata conventions the climate and forecast cf conventions 16 16 cfconventions org define metadata that enable a file containing observation data to be self describing it was originally framed as a network common data form netcdf standard for model generated climate forecast data but can equally be used for other data formats and other observational datasets the metadata provides some basic discovery level elements such as the title author and spatial and temporal extent but primarily is designed to promote the re use and processing of datasets e g for manipulating large scale datasets for ocean circulation and atmospheric data this is achieved by providing a definition of all the variables in the dataset in terms of the observed parameter the measurement units and data quality information about each data value 3 controlled vocabularies as discussed above using a standard metadata schema and a standard encoding provides the syntactic and semantic interoperability for the metadata record to further enable the interoperability of the content of the metadata the vocabulary used in it should be taken as much as possible from controlled vocabulary lists where the term is defined and preferably from vocabularies that are in common or agreed usage most metadata schemas provide such vocabularies or code lists along with the metadata standard and the content of elements can be restricted according to that list one of the ways to make a vocabulary easily accessible to applications is to represent the vocabulary term as a web resource using w3c resource description framework standard and serve the vocabulary terms and definitions via a web application programming interface api for example nerc provides an api to a variety of vocabularies used across nerc leadbetter 2012 iugs cgi international union of geological sciences commission for the management and application of geoscience information publishes a suite of internationally agreed geoscience vocabularies 17 17 resource geosciml org the british geological survey bgs has started the publication of key stratigraphy vocabularies 18 18 data bgs ac uk api clients can also present the vocabulary in more human friendly formats within a web browser or as a text file downloads for the additional metadata elements required for this hazard metadata schema new code lists were required for parameters software code amongst others see below for this first version the vocabularies and definitions are only available internally to bgs and are not available as web resources this has been identified as a potential task for future work as discussed later 4 identifying metadata requirements in order to gain a better understanding of the metadata needs of environmental modellers a nerc funded study under the title metamodel ned ned referring to a nerc environmental data grant which funded the work hughes et al 2013 was undertaken the results from this study formed an important input to the development of the model metadata scheme proposed here and are summarised below this study was aimed at addressing issues related to ensuring that environmental models are properly recorded and documented so that the models can be retrieved and used at a later date accordingly two key aspects of model metadata were considered namely the use of discovery metadata in order to locate and access models of interest and also the range of metadata attributes required when actually using and running a model once it has been found technical metadata an on line questionnaire was circulated to three hundred and twenty stakeholders in universities commercial organisations and other research organisations including nerc data centres a total of 108 responses were received predominantly from europe but with some from the united states and australia this rate of response c 30 can be regarded as reasonably favourable in terms of what would typically be expected the recipients were well recognised specialists in environmental modelling the fact that many of the respondents held senior positions in their respective organisations and that responses were received from those working in a variety of environmental science disciplines e g climate change earth system modelling groundwater and land use modelling and hazard modelling in general was considered to also add weight to the conclusions drawn 4 1 find and locating models the need for discovery metadata one of the questions posed by the metamodel ned study investigated what metadata standards were being used by environmental modellers the results 19 19 see http nora nerc ac uk id eprint 504868 for the details on the results of the questionnaire indicated that the number of researchers making use of schemes such as iso 19115 and 19119 which are frequently used to describe metadata for datasets having a geographic component was relatively small less than 10 of those surveyed overall the questionnaire results indicated a general consensus that there is a lack of internationally accepted metadata standards available for metadata about environmental models this study showed that rather than making use of metadata to discover models a large proportion of the modellers questioned identified and located models of interest either through contact with organisations which they already collaborated with or by approaching recognised suppliers of specific models directly contact with collaborators or known producers of models could be expected to provide a good overview of models available in the researcher s own discipline but could mean that potentially useful models outside the researcher s immediate scientific discipline are overlooked there is clearly therefore a need for additional and potentially more detailed metadata available to help people find models there is also a perceived risk of researchers using models developed in other disciplines without access to comprehensive documentation and information on how to use the models respondents to the questionnaire were also asked about what metadata attributes they considered to be most important attributes such as descriptive information about the model the parameters used and the spatial extent of the model were relatively high on the list with 50 65 of respondents regarding these as of high importance followed by quality assessments it was notable the information about the type of model for example probabilistic or deterministic technical metadata about the software platform used to run the model were rated as of high importance by over 30 of respondents and the time period over which a model runs e g days months years were rated as high importance by c 20 of respondents also seen as important was some indication of the overall ease of use of models even at the discovery level so that modellers could avoid spending inordinate amounts of time and resources configuring an unfamiliar model ideally also there was interest in the models scientific pedigree e g how many peer reviewed publications did it feature in and how was it regarded by other researchers and whether the model actually answered the science questions it had been designed to address the availability of temporal information such as the time step used in the model and the relative time period being covered by the model is also information that researchers regarded as necessary both to locate and make use of a model further details on the responses and their ranking can be found in hughes et al 2013 4 2 using and running model codes descriptive metadata once models of interest have been located a researcher is potentially able to download the model code at least for a model which is in the public domain and then start to use it one of the areas highlighted by the metamodel ned study where current metadata schemas do not fully meet the requirements of modellers is in the provision of the information needed by modellers to use models of particular interest is information about how to actually configure a model once the user has located the model code e g c 60 of respondents rated the availability of assumptions made in building the model to be of high importance hence the provision of information about manuals particularly information available over the internet information on boundary conditions and assumptions made in using the model are seen as important technical metadata on how to configure the model code to run in a specific computing environment are also regarded as of high importance by over 30 of survey respondents for example the computer language in which the code is written and relevant details of the operating system on which that code is optimised together with memory or distributed computing environments for example where models are being coupled together in a linked composition series of connected models information on the input datasets and individual parameters which need to be provided to run a model and also the parameters output from the model are also regraded as of high importance by 60 of respondents 4 3 gaps in model metadata provision these indications of additional metadata requirements when describing environmental models point to a number of gaps in model metadata provision gaps in provision are present particularly in the representation of temporal information model code configuration and parameters used and exchanged between models and in technical metadata describing the computing and software environment used for modelling the major gaps in current metadata provision are described below and summarised in fig 1 temporal information metadata about temporal resolution and scale e g period of time over which measurements have been made years months weeks days or smaller time units is required particularly when linking models together also more information on dates and times when model runs were undertaken this could include start and end of calibration simulation and validation periods for example this information is considered more useful than dates when the metadata record was submitted model code configuration and parameters models codes are data hungry and require both spatial and temporal data to drive their instances therefore to describe a model instance requires the description of the data both in terms of spatially distributed but that doesn t change with time along with time variant data further information is required on the parameters used both the range of parameters which a model code can accept and also the parameters actually supplied which may be different in order to assist making use of model codes created by other researchers this information ideally also needs to include information on the units used so that for example any differences in units can be taken account of when linking models this would also include information about assumptions made in the model setup boundary conditions and the science questions which the model is aimed to address some of this information may be contained in manuals or peer reviewed papers and links to such information should also be included computing environment information on the programming language used to create the model code also details of overall operating system and computing requirements in terms of operating system and general processor memory requirements e g will the code run on a laptop or is an hpc system required 5 metadata schema development in designing a metadata schema to meet the requirements indicated above our approach was to build on those components of established metadata schemes such as iso19115 which are relevant to modelling and add additional functionality as necessary further current best practice for model metadata was used harpham and danovaro 2015 morsy et al 2017 extended using the results of the questionnaire reported in hughes et al 2013 by combining the international standards available for data metadata with current best practice for the model metadata then a comprehensive model metadata schema that is fit for purpose for environmental models as well as model integration is developed for example attributes such as title description or abstract and the geographic bounding box were clearly relevant to modelling from the user requirements gathering undertaken an important concept which became evident from our user requirements discussions was the consideration of the model code which describes the algorithm underlying the model and its encapsulation into compiled code and also the model instance or model application which is the application of the model code together with input data and relevant assumptions usually over a defined geographic area whilst it is recognised that this complicates the schema and provides model developers with more work it offers the advantage that once a model code is used many times then it only has to have one metadata entry obviously if model code variants proliferate then more entries are required it was clear from early discussions with the groundwater modelling community for example that there was a fairly restricted set of model codes or algorithms used for a wide range of different applications the usgs modflow bgs zoomq3d and dhi s feflow seem to be commonly used for a number of projects reported in the literature e g jackson et al 2005 the same seems to be the case for the seismic hazard community in terms of the workflows used for conducting the modelling process it was clear that different attributes were often related to the model code and to the model instance respectively separating the entity of a model code and relating it to zero one or possibly multiple model instances in the schema enables definitive details to be stored about each model code once only thus reducing errors which could result from slightly different information being entered about a model code each time it is used the inverse relationship from a model instance to a single model code is made mandatory which means that a model instance cannot be represented in the metadata schema until model code information has been provided which is also in line with good modelling practice the model code related metadata components include information about the computing environment for example the programming language used to write the code the compiler and the processor capabilities required this type of information allows a prospective user of the code to determine whether the code can be run on a laptop or whether for example it requires a high performance computing cluster fig 2 the model code metadata also records the model type model purpose and mathematical basis of a model these are implemented as controlled vocabularies in the data model allowing users of the metadata to query on these attributes for model instances the science question which the model is designed to answer can be recorded as a free text field as can any initial or boundary conditions required this information is very relevant to users of the model but also allows researchers to search for models from a particular environmental science discipline the modal instance metadata importantly also contains information about input and output datasets such datasets could include formal datasets which have a doi for example and therefore this information can also be recorded in other respects the model code and model instance may share certain attributes for example the model code metadata should contain information about the parameters which a model code takes as input in order for it to run whilst a model instance built using that model code would record the datasets and parameters actually used and their units similarly there is a requirement to record both details of publications and reports about both model codes and instances and both have contact details the key conceptual entities and their relationships are summarised in figs 3 and 4 5 1 model code the main object modelcode fig 3 contains the basic information of the model code software that processes the input data alongside the basic description of the code it has attributes containing information on what the model is model purpose model type and mathematical type as well as where to get further information on the model code such as a url linking to websites containing further details responsibleparty this provides information on the code guardian as well as links to an object organisation describing their organisations documentresource it is important that links to other documents are available this object stores the details of other documents such as a manual or a base paper in the peer reviewed literature modelcodeparameter this describes the parameters the model takes as either inputs or outputs it enables the user to determine whether the correct data are available as well as determining how the model codes could be linked keyphrase this is an object containing a free text field which further information describing the model code such as its uses can be stored computingenvironment information on the operating system e g windows pc or unix so that the user can decide whether they have access to the computational resources to run the model 5 2 model instance the main object modelinstance fig 4 has two groups of attributes one detailing the administration of the modelinstance instance of the object and the other describing aspects of the model itself the former has ids to link the modelinstance instances and dates of publication the latter has attributes to describe the vertical and horizontal extent of the modelinstance as well as the time stepping to help describe the model instance then documentresource and keyphrase as described for the modelcode can also be accessed using a many to many relationship for example a model code could be referenced in multiple papers and a paper could also describe more than one model code and so needs to be reflected in the data model this means that there is a pool of the instances of these objects which all model code instances can refer to further the responsibleparty and organisation objects and their instances can also be linked to the model instance sciencediscipline given that the pure portal is designed to store the model metadata for hazard models so this object stores the discipline area that the model instance was created for isogeobndbox since a model code has been applied to a geographical area to create an instance then the definition of the bounding box of the model application needs to be defined this object stores the co ordinates as bottom left and top right as well as the potential for shape objects such as shapefiles modelinstancedataset and modelinstanceparameter these describes the datasets and parameters the model instance have used as inputs and the outputs produced enables the user to determine whether the correct data are available as well as determining how the model codes could be linked modelinstancegrid given that the majority of numerical models are discretised and are grid based then information on the type of grid its characteristics are required there may be a need to extend this for non uniform grids but there is the ability to have a one to many relationship additionalinformation this is an object containing a free text field in which further information describing the model code such as its uses can be stored 6 implementation of the pure portal the basic system consists of a java based web front end oracle database and a spreadsheet data entry method the implementation was achieved using the scrum approach based on the agile development process the three main elements of the system are described in more detail below 6 1 web front end the software architecture utilised the internal guidelines laid out within the bgs and the application itself was built in house and implemented by bgs software developers the requirements gathering process consisted of on site visits to potential users and online surveys hughes et al 2013 as well as eliciting the experiences of scientists working on the pure programme and in bgs the solution arrived at is a web based application www pure org instead of a standalone desktop application fig 5 a d this enables users outside the organisation to access and submit data allowing wider contribution to the model metadata added the web application was built utilising an object orientated language java as this offered scalability reliability and security alongside the large internal pool of java developers within bgs made it a natural choice the web application was programmed to the latest version at the time of development java 8 the web application was built using an in house java framework this increased the productivity of the developers as they were working on application framework they were experienced with the application adopted a model view controller mvc architectural pattern a well established pattern that promoted maintainability and reusability of code the metadata required a database backend to provide persistent secure and backed up data storage that is scalable the metadata content needs to be constrained by various controlled vocabularies and we also required the potential for the database to be edited by multiple users should the web application be updated in the future to provide online editing and upload functionality a relational database management system rdbms is well suited to this purpose bgs already hosts an oracle 11 g rdbms database which supports many existing web based applications similarly bgs software developers have many years of experience in developing read write applications to the database backend and can rapidly build a web application by using familiar code frameworks for these reasons the database was implemented in the corporate oracle 11 g database for this application but was designed in an agnostic way such that it could easily be migrated or re created in different database software such as an open source rdbms as required we applied a map based feature to allow for location based searching and retrieval of datasets through an esri gis map based service the web application client side was built in html javascript and freemarker and the server side in java a map component was added to the application to show the location of the model metadata the gis functionality allows the researcher to quickly identify model metadata this addresses a requirement from researchers to identify models with minimal effort which would be relevant for further enquiry the application was designed primarily for desktop users but ensured the application rendered correctly for mobile and tablet devices as well given the likely use of the portal a specific app was not built for ios android or other mobile operating systems however it still can be viewed on hand held devices the metadata schema described above was implemented with an oracle 11 g relational database system since this is the corporate database platform used by the bgs however the schema could equally be implemented in another rdbms the relational implementation is built around the central concepts of model code and model instance described above key model code information is held in the table modelcode which stores information such as model type model purpose and mathematical basis of the model figs 3 and 4 and appendix 1 the primary key of this table is the field modelcodeidentifier this is a system populated numeric identifier to uniquely identify the model code the field code name allows a brief name for the model code to be captured similarly the modelinstance entity has a system populated numeric primary key field called modelinstanceidentifier the modelinstance entity also contains a field called alternative title which can be used to record a shortened name for the model instance for example for display on a web page the table modelinstance contains the title and abstract for the model instance in addition to information about the total vertical extent publication date and time step information other model instance concepts such as the science discipline and input and output datasets and individual input and output parameters are then linked to the modelinstance table fig 4 some entities have a many to many relationship between them for example a model code could have multiple reports describing it but a published report may also refer to more than one model similarly a model code may have a number of relevant key phrases and a key phrase can also be relevant to more than one model these many to many relationships are implemented by linking tables within the rdbms structure which contain the primary key fields of the two tables they link these are further described in appendix 1 where possible use was made of vocabulary or dictionary tables i e tables containing codified values together with an explanation of what those values mean in order to implement the controlled vocabularies for data items such as model purpose model type and the mathematical basis of the model the topics which are constrained by controlled vocabularies are listed in table 1 generally existing controlled vocabularies were not available for these concepts and so for example the model parameter vocabulary was developed pragmatically to reflect the actual values used by providers of the data future planned work will involve aligning the pure controlled vocabularies to any suitable vocabularies that already exist or may be created in the future information about the geographic extent of a model instance is held in the table isogeobndbox this stores the coordinates of a rectangle bounding the area represented by the model instance within the oracle implementation of the database an oracle spatial shape object has also been included as an additional attribute to provide for efficient manipulation of the spatial data and for example facilitate display within a geographic information system however this attribute is optional for example where the schema is implemented in a rdbms system which does not support spatial objects 6 2 data entry it was clear from the online survey and interviews that researchers were keen to have access to quality metadata but were reluctant to provide metadata due to time constraints as it was perceived that the filling in of forms and submission of metadata was an onerous process microsoft ms excel spreadsheets were created to allow the researchers to submit model data two ms excel forms were created one for the submission of model code and one for model instance each ms excel form was annotated and colour coded to help the researcher minimise the length of time required to fill in each section the application provided help documentation to support researchers to enable them to fill in the ms excel forms correctly survey s carried out on model researchers experience of submitting metadata to the application required supporting the model researcher on how to submit metadata correctly this ensured that model researchers would be less frustrated with metadata submission process and reduce the probability of model researchers giving up prior to submission and increasing the likelihood of metadata being submitted and be of good quality metadata submissions have come from a range of researchers the completed ms excel spreadsheets are submitted by the model researcher through the portal via e mail the metadata is then reviewed by a metadata curator to ensure data submitted is of sufficient quality or if requires further follow up with model researcher once data standards have been met the data is ingested into the metadata repository and made available through the portal 6 3 user testing the final application was tested with sample group of researchers both within bgs and externally the testing was carried out through a combination of observing participants utilising the application on line surveys and soliciting feedback through the portal the purpose was to identify areas for further improvement failure points to be immediately rectified demonstration of the application was carried out both to bgs model researchers and external participants the aim was to solicit feedback from researchers as well as to demonstrate and educate how to use the portal 7 summary and future directions the process of creating a model metadata schema and implementing it in the pure portal has been described based on previous work hughes et al 2013 along with a questionnaire response and informed by the experience of both developing and using data metadata a prototype model metadata standard has been created the main features requested from the questionnaire have been implemented including information on code guardians supporting documentation and technical metadata on timesteps model codes and model instances that is the software used to produce a model and its application to a particular area have separately defined metadata the pure portal has been created using an oracle database with a java based web front end whilst the focus of the work has been on hazard models the model metadata schema and its implementation of the pure portal can and has been applied to different types of model codes and their application the pure metadata model is only currently represented in human readable form on the pure portal website and in the data submission ms excel spreadsheet using labels for the metadata attributes that were chosen by the commissioners of the project to improve the interoperability of the data the elements of the metadata model should be mapped aligned to those in dublin core geo dcat ap and iso 19139 where possible the portal pages could be adapted so the user can choose the schema labels to display and the schema labels should be linked to their definitions to enable seamless integration with other metadata catalogues across various disciplines the pure portal could be adapted to be a web api that provides the metadata record encoded in one or more of iso 19139 xml dublin core rdf and geo dcat ap in machine readable data format note that this would be a lossy conversion as not all of the pure metadata elements are represented further mapping and alignment with these standards would quantify the extent of information loss to enable the full set of elements of the pure portal schema to be integrated in third party systems the schema itself should be published on the web as an xml schema or an rdf schema complete with its alignments to existing schemas 1 alignment with dublin core this work is yet to be done but from an initial analysis this would be relatively straightforward 2 alignment with iso19115 a comparison of the pure metadata model and the iso19115 2003 was carried out and most elements were found to have some equivalent in the iso standard with the notable exception of the input and output data parameters the assessment would need to be done in more depth and also to take into account changes implemented in the latest version iso19115 2014 3 formally extend iso metadata standard the iso 19115 standard provides a method for extending metadata to fit specialized needs harpham and danovaro 2015 the pure community could create a formal extension of the iso model to accommodate the metadata elements not already covered and publish the pure metadata standard as a profile 4 utilise linked data for controlled vocabularies the vocabularies developed for this metadata schema could be published via a web api and identified within the metadata by a uri this could be either via the new version of the bgs vocabulary service and linked data api currently in development and or the nerc vocabulary server nvs any inter relationships synonyms broader narrower terms etc between terms within these vocabularies could be identified and represented in rdf as simple knowledge organisation system 20 20 www w3 org 2009 08 skos reference skos html skos relations any relationships between these vocabulary terms and existing terms in the nvs or published from other sources could be identified and represented in rdf as skos relations such as skos closematch the pure portal is available for use by researchers model developers and model users to store and access metadata for openly available hazard models the fair principal findability accessibility interoperability and reusability wilkinson et al 2016 has been applied for datasets for a number of years the pure portal provides the elementary infrastructure for the guiding principle of fair to be extended to model codes and their instances which is a requirement if model integration is to be achieved further the metadata schema that has been developed can be fed into the process of defining an international recognised metadata standard facilitated by the ogc or another international standards body if the barriers to achieving model integration are to be lowered then model codes and their instances should be discoverable available and linkable the pure portal is another step along the way to achieving this aim software availability the pure portal is available as two instances www pureportal org as well as the natural environmental research council nerc model repository model search nerc ac uk used for nerc grant holders the database development was undertaken in common with bgs standard approach the vocab servers are available via vocab nerc ac uk acknowledgements this work is published with the permission of the executive director of the british geological survey nerc it was funded by the natural environmental research council nerc as part of the probability uncertainty and risk programme we thank two anonymous reviewers for their comments which helped to improve the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104597 appendix descriptions of the entities and attributes shown in figs 3 and 4 entity descriptions entity name description modelcode information about the model code used to run a model documentresource details of documents and papers providing information about models including manuals and peer reviewed papers modelcodedocument link table to implement many to many relationship between modelcode and documentresource modelinstancedocument link table to implement many to many relationship between modelinstance and documentresource modelcodeparameter parameters required by a model code computingenvironment computing environment under which a model runs responsibleparty responsible parties e g creators or maintainers of models organisation organisation to which the responsible party is affiliated keyphrase keywords and keyphrases to describe the model to assist searching and discovery modelcodekeyphrase link table to implement many to many relationship between modelcode and keyphrase modelinstancekeyphrase link table to implement many to many relationship between modelinstance and keyphrase modelinstance entity holding key information about a model instance i e the application of a model code to a specific geographic area or problem sciencediscipline scientific discipline to which a model relates isogeobndbox details of the geographic rectangular bounding box relating to a model instance additionalinformation supporting information about a model instance e g the science question underlying the model or boundary conditions modelinstancegrid describes the grid used where a model is gridded modelinstancedataset details of datasets input or output from a model modelinstanceparameter details of parameters used by a model instance attribute descriptions entity name attribute name data type mandatory y n description modelcode model code identifier pk number y unique identifier for each model code code name character y name by which the model code is known code description character n description of what the model code does model purpose character y vocabulary constrained code representing the purpose for which the model code is designed model type character y vocabulary constrained code representing the type of model url link character n link to a url where the code can be downloaded if available ninimum time step character n minimum time step maximum time step character n maximum time step version number character n version of the model code e g 1 01 publication date date n date of publication of the model code mathematical type character y vocabulary constrained code representing the mathematical basis of the code documentresource document identifier pk number y unique id for a document report title character y title of the paper report publication flag character n flag to indicate whether the paper report is published documnnt url character n url link to the paper report where available document doi character n digital object identifier for a published paper report authors character n list of authors year published date n year of publication document details character n details of the paper report e g details of journal or other publication modelcodedocument model code identifier pk number y model code identifier document identifier pk number y document identifier modelinstancedocument model instance identifier pk number y model instance identifier document identifier pk number y document identifier modelcodeparameter parameter identifier pk number y unique identifier for each parameter model code identifier number y provides link to modelcode entity parameter code character y vocabulary constrained code representing the parameter e g ground water level etc input output flag character n flag field to indicate whether an input or output parameter computingenvironment computing environment identifier pk number y unique identifier for each record model code identifier number y foreign key database link to modelcode table code language character y vocabulary constrained field for code language e g c fortran etc compiler character n details of the compiler used to compile the code if relevant operating system character y vocabulary constrained field for operating system cpu character n details of the cpu required to run the code system memory character n required system memory comments character n comments field responsibleparty resparty identifier pk number y unique id for each responsible party record used by database contact name character y contact name organisation identifier number y provides link to organisation country character n country email address character n email address organisation organisation identifier pk number y unique id organisation organisation name character y organisation name keyphrase keyphrase identifier pk number y unique id for keyphrase keyphrase description character y keyphrase description scope notes character n information about the scope of the keyphrase status character n whether active obsolete data source character n records the source of the keyphrase definition character n definition keyphrase type character n type code to allow different subsets of keyphrases to be held in the same database table modelcodekeyphrase model code identifier pk number y model code identifier keyphrase identifier pk number y keyphrase identifier modelinstancekeyphrase model instance identifier pk number y model instance id keyphrase identifier pk number y keyphrase identifier modelinstance model instance identifier pk number y unique id for model instance model code identifier number y unique model code record date date n date record created maximum vertical extent number n maximum vertical extent of model instance minimum vertical extent number n minimum vertical extent of model instance title character y model instance title abstract character n model instance abstract spatial reference system character y unique id for spatial reference system constrained by vocabulary vertical datum character y vertical datum e g mean sea level constrained by vocabulary vertical scope character y vertical scope constrained by vocabulary vertical units character n unit of measurement for vertical dimension publication date date n publication date of the model to which this metadata refers start date date n start date of the period which the model represents end date date n end date of the period which the model represents temporal reference system character y temporal reference system constrained by vocabulary completion status character n completion status of the metadata e g partial or complete parent citation identifier number n citation id of a metadata record of which this record is a child thus implementing the potential for hierarchical relationships alternative title character n alternative title e g used to store a shortened title for the model instance for display purposes date submitted date n date the metadata record was submitted time step interval character n the interval for time step if relevant e g hour day month etc time step value number n the frequency of that time step e g every 6 h etc sciencediscipline discipline code pk character y unique code representing each scientific discipline model instance identifier number y model instance id link to modelinstance table isogeobndbox geo bounding box identifier pk number y unique id assigned by database for each bounding box record model instance identifier number y model instance identifier north west easting number y the easting of the north west corner of the bounding rectangle north west northing number y the northing of the north west corner of the bounding rectangle south east easting number y the easting of the south east corner of the bounding rectangle south east northing number y the northing of the north east corner of the bounding rectangle shape blob n geometry object representing the bounding rectangle e g oracle spatial shape object in oracle implementation additionalinformation additional information identifier pk number y unique id assigned by database for each additional information record model instance identifier number y link to modelinstance description character n text representing the additional information e g stating the boundary conditions etc additional information type character n code representing the type of additional information e g science question boundary conditions etc modelinstancegrid grid identifier pk number y unique id assigned by database for each grid record model instance identifier number y model instance id provides link to modelinstance table grid spacing x number n grid spacing in the x dimension horizontal plane grid spacing y number n grid spacing in the y dimension horizontal plane grid spacing z number n grid spacing in the z depth dimension grid cells x number n number of grid cells in the x dimension grid cells y number n number of grid cells in the y dimension grid cells z number n number of grid cells in the z dimension grid type character y vocabulary constrained code representing the type of grid used modelinstancedataset dataset identifier pk number y unique id assigned by database for each dataset record model instance identifier number y provides link to modelinstance entity dataset name character y name of dataset dataset description character n description of dataset dataset version character n dataset version dataset type character y vocabulary constrained representing the dataset type dataset url character n a url providing more information about the dataset or from where it can be accessed temporal reference system character n temporal reference system relating to the dataset temporal resolution character n temporal resolution relating to the dataset in out flag character n flag to indicate whether the dataset is input to the model or an output from it modelinstanceparameter parameter identifier pk number y unique id for parameter record dataset identifier number y dataset id foreign key link to modelinstance table parameter code character y unique code for each parameter constrained by vocabulary parameter value number n specific parameter value parameter unit character n the units of measurement used for the parameter 
26094,geohazards e g floods and landslides are increasingly resulting in loss of life and economic damage process models can help the understanding of the frequency and occurrence of these hazards reducing their impact however models require significant resources to develop and ideally should be made widely available further integrated modelling demands that process models are discoverable available and ultimately linkable metadata standards are defined for data but the equivalent for process models are not yet available as an international standard or even as accepted best practice requirements have been gathered for model metadata using a questionnaire and a metadata schema has been developed the resulting schema split model code software and model instance application given that model codes are used multiple times international data metadata standards e g iso19115 have been used for the core of the standard the schema has been implemented via the pure portal www pureportal org which makes models available and forms the basis of an international standard for model metadata 1 introduction natural hazards are many and various and include floods droughts landslides earthquakes tsunamis and volcanic eruptions between 2006 and 2015 globally these events have resulted in 0 77 m deaths and injuries as well as causing 1 4 trillion usd in economic damage sanderson and sharma 2016 there is growing evidence that certain types of natural hazards have a greater economic impact related to economic growth population growth and the movement of population to vulnerable areas etkin 1999 as a result of this perceived increase in the impact of natural hazards over the last few decades decision makers have recognised that more resources are required to understand hazards and to simulate them responding to this need the uk s natural environment research council nerc instigated the probability uncertainty and risk in the environment pure programme www nerc ac uk research funded programmes pure the programme aims to both improve the understanding of natural hazards and to make the science underlying hazards much more accessible to decision makers and consists of a range of activities including large grants racer e g lickiss et al 2017 and credible e g simpson et al 2016 phd students secondments and the development of the pure portal to make hazard models more easily available to a range of users process models particularly of the environment are expensive to develop and apply requiring significant collective effort to code them and to ensure that they are working i e benchmarking them against other model codes or standard datasets and then to apply them to different areas this effort is best capitalised on by making the models available for others to use for related studies or even apply the model codes to different areas here we define model codes as the software developed to solve the governing equations with suitable boundary conditions and model instances as the application of the model codes to particular study areas however to make both model codes and their instances available for examination and to be applied to different problems then they have to be discoverable in other words enough information has to be made available for the models themselves to be found using suitable searches once found or discovered then the user has to make a decision on whether the model code is suitable for their purpose questions such as what mathematical equations have been used how have these equations been solved what boundary conditions can be applied all need to be addressed prior to a model user applying the model code or the results from the model instances further description or at least links to further information is required e g to websites with details as to how the model code works or pdfs of manuals so that the prospective user can understand in more detail what the model code actually does further the source code itself could be made available using online software repositories so that the functionality of the model code itself can be further reviewed integrated modelling is the relatively recent discipline of taking a question that is deceptively simple to articulate and to create a workflow which joins models of different origin together to address the question moore and hughes 2017 there are six main challenges model components are discoverable available and linkable they can be coupled semantics and ontologies are used to define models and their variables the linked model systems can be run as quickly as possible uncertainty is addressed and the results visualised in an appropriate form to address the question under consideration laniak et al 2013 by making models available for discovery the user can then start the process of assessing them to include them in their workflow for example does the model code and associated instance provide a solution that enables them to fully address their questions joining models or model coupling can take a number of different forms e g belete et al 2017 including passing data between models in files with defined formats joining model codes together to form a new combined code creating models as components and linking them at runtime and exposing models as a service given that these all require information about the model codes and their inputs and outputs then provision of both discovery i e basic information about the model codes and instances and technical metadata containing more details on the workings of the model code and its application is even more desirable 1 1 previous work whilst there are many metadata catalogues and repositories for spatial data sets e g data gov uk the equivalent for process models and their instances are much less common however a number of model catalogues have been created and zaslavskay et al 2014 review five existing ones which mainly contain model codes examples include the us based community surface dynamics modelling system csdms model catalogue peckham 2014 and the tess model registry funded by the eu which was created to store technical metadata for ecosystem management model instances model metadata is required to facilitate the comparison of climate models codes and their instances for international model inter comparison particularly for defining the physics of the models dunlap et al 2008 report on the earth system curator which has developed a metadata schema for describing climate models and their outputs this is important for understanding the differences in climate model behaviour based around core metadata which provides basic information on the model code developer other classes include modelling and data which describe the code and its output the modelling class is in turn populated by configuration grids and coupling which describe the physics numerical gridding and any model linkages respectively processes to automatically obtain metadata from the model instances are presented the schema itself is implemented in a website in the earth systems documentation project es doc org and contains descriptions of the main climate models currently under use e g uk s met office hadley centre model code there are no internationally recognised standards for mathematical model metadata however this is recognised as an issue by those who seek to promote the use of model metadata several initiatives e g harpham and danovaro 2015 have attempted to define de facto standards and common features required have emerged their work is based on iso19115 and involved developing a metadata schema for both discovering models as well as providing sufficient technical metadata for the user to decide whether the model is suitable for their use their approach is tied to a workflow involving p gridded or q flow data and as such limited to this application zaslavskay et al 2014 report five model catalogues each with their own metadata but that arrive at common features such as describing the model developers and code guardians a model code inventory has been built for hydrological models a collaboration between texas a m university and the us bureau of reclamation it provides a web based list of model codes which link to pdfs providing metadata in common with the work presented here it was driven using a questionnaire based approach singh et al 2006 the cuashi backed hydroshare initiative horsburgh et al 2016 has developed social objects to define entities including models with the aim of enabling both the model and their developers to be networked the aim of hydroshare is to promote not just the description and availability of model codes and their instances but also facilitate collaboration around them horsburgh et al 2016 the latter is achieved by using the concept of resources which can define a number of different entities including model program i e model code model instance as well as generic ones an example of using this schema using swat a watershed hydrological model code is presented in morsy et al 2017 who develop a metadata schema for a single model instance this is at odds with harpham and danovaro 2015 who develop a metadata approach for linking models however morsy et al 2017 separate model program code and model instance resources model instance the example they present is specific for hydrology only in the uk the fluidearth 2 catalogue holds metadata on linkable models harpham et al 2014 fluidearth is the implementation of the openmi model linking standard gregersen et al 2007 harpham et al 2019 and is designed to make the creation of openmi compliant models as straightforward as possible as part of the fluidearth initiative openmi fluidearth compliant models are described on the fluidearth portal to enable discovery of model codes that can be joined together into a composition or series of linked models however the linking of models requires that the variables that can be linked are described appropriately as a development of the csdms approach peckham 2014 a variable naming convention has been developed to describe state variables this convention based on providing a standardised list of variable equivalent to an ontology but with a strict naming convention based on a two part context descriptor followed by a variable name an example would be channel bed manning coefficient which describes the variable manning coefficient applied to the context of a channel bed defining the variable name explicitly has two advantages one is the ability to link the variable name to metadata categories within the csdms system and the other is to provide strict definition for variables which can be exchanged model components at runtime the latter is a pre requisite for model integration using a questionnaire based study funded by the nerc environmental data ned call hughes et al 2013 report that the specific challenges identified are 1 to ensure that the data used to create models are recorded and their origin known 2 the models produced are themselves available to run either by downloading or remotely via webservices 3 the results produced by these models can be obtained in a recognisable format further a number of attributes which are not typically captured by the often used metadata schemas applied to datasets e g iso 19115 are seen as being important in a model metadata scheme these are important considerations for the development of both model metadata and the model repositories that implement them whilst there are no internationally recognised standards for model metadata however there are significant examples of best practice in the climate dunlap et al 2008 hydrology horsburgh et al 2016 morsy et al 2017 singh et al 2006 and model integration harpham and danovaro 2015 peckham 2014 disciplines further model repositories have been created such as those reported by zaslavskay et al 2014 and the implementation of hydroshare described by morsy et al 2017 our work builds on that undertaken to date uses the best practice offered by preceding workers and includes the result of a questionnaire to gather the requirements of environmental modellers hughes et al 2013 to design a model metadata scheme that can be applicable for a wide range of environmental models only by separating model code from instance can the flexibility of approach adopted by morsy et al 2017 building on iso19115 as described by harpham and danovaro 2015 and producing an open repository a development of hydroshare can model codes and their instances be made available further the approach presented below is extensible so that model integration can be facilitated as espoused by harpham and danovaro 2015 this paper aims to describe the development of the metadata schema for hazard models as used for the pure portal given that there is currently no international standard for model metadata it sets out the process by which the metadata schema has been developed firstly metadata schemas for data are described the base work on which the requirements for model metadata have been gathered is then detailed and the model presented finally the implementation of the model metadata schema in the pure portal is outlined it is hoped that by setting out the model metadata schema and its reasoning behind it and its use that this can be used as the basis for an international standard for example with the open geospatial consortium ogc 2 current metadata schemes for data resources metadata is provided by publishers to describe their resource e g datasets to aid its discovery exchange and re use a metadata schema defines a set of elements where each element refers to one aspect of the resource description for example the title or the author these elements are sometimes grouped into sets of elements useful for different purposes for example discovery metadata contains the elements useful for finding a resource of interest technical or use metadata contains the elements necessary for using the resource correctly for each element the schema defines the semantics i e the meaning intended to be conveyed and the rules for the content i e the data type the domain range from which values can be used whether the element is mandatory optional or conditional the schema will also define how the elements are related to each other in terms of a hierarchy and cardinality i e how many of the elements are the same some metadata standards inherently define an encoding by which the metadata must be formatted in a data structure in other cases the method for encoding the metadata is described in a guidance document or complementary standard by using these accepted schemes differing levels of interoperability can be achieved at the simplest level the metadata can be presented using the agreed term labels and natural language definitions so a person can understand it and manually compare it to metadata records to achieve additional syntactic and schematic interoperability the metadata can be encoded according to the schema rules and appropriate data format in such that an application can consume the metadata chan and zeng 2006 mappings crosswalks or translations between alternative metadata schemes or the individual elements within a scheme can be used so that metadata can be aggregated from various schema sources for even greater interoperability the data content of the elements within a metadata record instance can be encoded using the world wide web consortium w3c rdf resource description framework 1 1 www w3 org rdf standard such that a full description of the term its meaning and its relations to other terms can be easily obtained as a web resource from a unique resource identifier uri the use of controlled vocabularies is discussed further below there are a significant number of standards for both discovery and technical metadata for data of different types there are also a range of services by which metadata can be recorded and the data stored alongside these data as previously outlined by hughes et al 2013 nerc itself puts a significant amount of effort into storing data and model results and making the metadata available for example there are seven data centres and the data catalogue service dcs to search metadata for datasets stored in the nerc data centres a useful summary of metadata standards across a range of disciplines is available on the research data alliance rda metadata directory 2 2 http rd alliance github io metadata directory standards the commonly used standards relevant to this study are described below 2 1 dublin core the dublin core schema 3 3 dublincore org is a metadata standard with widespread use the dublin core metadata element set is the original set of terms published it consists of fifteen optional metadata elements for cataloguing web resources and physical resources this set of elements has been endorsed as international organization for standardization iso standard 15836 2009 internet engineering task force request for comment ietf rfc 5013 and us national information standards organisation niso standard z39 85 the wider and full set of dublin core metadata terms including the 15 original ones are maintained by the dublin core metadata initiative dcmi this set includes spatial and temporal extent provenance and various ways of relating different resources source ispartof isformatof etc dcmi publishes a number of encoding guidelines as recommendations for example in rdf extensible markup language xml and hypertext markup language html 4 4 dublincore org resources expressions 2 2 international organization for standardization iso standards the iso standards 19115 5 5 iso 19115 2003 www iso org standard 26020 html iso19115 2014 www iso org standard 53798 html and 19119 6 6 iso 19119 2005 www iso org standard 39890 html iso19119 2016 www iso org standard 59221 html are internationally adopted schema for describing geographic information and services respectively the first edition of iso 19115 was published in 2003 but it has since been split into parts iso 19115 1 2014 contains the fundamentals of the standard iso 19115 2 2009 contains extensions for imagery and gridded data and iso ts 19115 3 2016 provides an xml schema implementation for the fundamental concepts compatible with iso ts 19138 2007 geographic metadata xml or gmd the iso19115 19119 standards are described using universal modelling language uml but a complementary iso standard 19139 7 7 iso ts 19139 2007 www iso org standard 32557 html defines how the metadata should be serialised and encoded in xml conformance against the 19115 19119 standards is therefore most easily checked by presenting the metadata in xml and validating them against the 19139 xml schema the standard is primarily aimed at geographic datasets and services but the domain based list of scopes that the metadata can be used for includes non geographic datasets models information applies to a copy of imitation of an existing or hypothetical object and software information applies to a computer program or routine though there are no additional metadata elements that specifically relate to these resource types the research data alliance rda has provided links to mappings of the iso 19115 standard from various community standards and exchange formats 8 8 rd alliance github io metadata directory standards iso 19115 html the set of elements in the iso standard is large and would unlikely to ever all be used in one metadata record but only a small number of elements are in the core mandatory set members of different discipline domains and national governments have therefore created extensions and profiles of the iso standard that enforce tighter constraints on cardinality and mandatory elements while removing many optional elements the profiles used for inspire 9 9 inspire ec europa eu infrastructure for spatial information in europe and uk gemini 10 10 www agi org uk agi groups standards committee uk gemini geo spatial metadata interoperability initiative are discussed below 2 3 inspire discovery metadata inspire is an infrastructure for spatial information within europe and is an european commission ec directive the purpose is to enable interoperability and sharing of data that are related to activities or policies having impact on the environment inspire has published a specification for metadata used for data discovery based on the iso19115 19119 application profile metadata for geographic information with a definition of core metadata elements from this required for inspire compliance some mandatory iso metadata elements are not mandatory in the inspire standard therefore need to be included as additional elements in order to make an inspire record into a valid iso record these are noted in the implementing rules technical guidance inspire maintenance and implementation group mig 2017 the inspire metadata implementing rules were revised in march 2017 to version 2 0 1 the encoding continues to be based on iso 19115 2003 not on the latest version iso 19115 1 2014 resources that are in scope of the inspire directive are those that are datasets series collection of datasets or services are digital have a geospatial component and the subject of which is one or more of the defined environmental themes process modelling code is not in scope of inspire 2 4 uk gemini within the united kingdom uk the uk gemini metadata standard association for geographic information 2012 represents the uk implementation of inspire and as such also conforms to the iso 19115 19119 standards all mandatory inspire elements in version 2 1 of the implementing rules are mandatory within uk gemini 2 2 revision 2 3 is in progress at the time of writing and will be conformant with the inspire implementing rules 2 0 1 the gemini encoding guidelines for each version ensure that the metadata is encoded in valid iso 19139 xml which will also be valid against the relevant inspire standard 2 5 dcat the data catalog dcat vocabulary 11 11 www w3 org tr vocab dcat is a recommendation of the w3c it is a resource description framework rdf vocabulary designed to facilitate interoperability and federated searches across data catalogues published on the web and also facilitate digital preservation it makes extensive use of terms from other vocabularies in particular dublin core and only defines a minimal set of classes and properties of its own an application profile dcat ap was published by the interoperability solutions administrations isa 12 12 ec europa eu archives isa programme of the eu for data portals in europe 13 13 joinup ec europa eu release dcat ap v11 dcat ap has itself been extended to geodcat ap to better handle geospatial datasets and services and to provide an rdf representation for the iso 19139 xml metadata elements required by inspire a stylesheet transformation has been made available for converting between iso19139 xml and geodcat ap rdf 14 14 github com geocat iso 19139 to dcat ap blob master iso 19139 to dcat ap xsl as stated by the authors the geodcat ap specification does not replace the inspire metadata regulation nor the inspire metadata technical guidelines based on iso 19115 and iso19119 its purpose is give owners of geospatial metadata the possibility to promote greater and more efficient use of their datasets by providing an additional rdf syntax binding 15 15 joinup ec europa eu node 154143 2 6 climate and forecast cf metadata conventions the climate and forecast cf conventions 16 16 cfconventions org define metadata that enable a file containing observation data to be self describing it was originally framed as a network common data form netcdf standard for model generated climate forecast data but can equally be used for other data formats and other observational datasets the metadata provides some basic discovery level elements such as the title author and spatial and temporal extent but primarily is designed to promote the re use and processing of datasets e g for manipulating large scale datasets for ocean circulation and atmospheric data this is achieved by providing a definition of all the variables in the dataset in terms of the observed parameter the measurement units and data quality information about each data value 3 controlled vocabularies as discussed above using a standard metadata schema and a standard encoding provides the syntactic and semantic interoperability for the metadata record to further enable the interoperability of the content of the metadata the vocabulary used in it should be taken as much as possible from controlled vocabulary lists where the term is defined and preferably from vocabularies that are in common or agreed usage most metadata schemas provide such vocabularies or code lists along with the metadata standard and the content of elements can be restricted according to that list one of the ways to make a vocabulary easily accessible to applications is to represent the vocabulary term as a web resource using w3c resource description framework standard and serve the vocabulary terms and definitions via a web application programming interface api for example nerc provides an api to a variety of vocabularies used across nerc leadbetter 2012 iugs cgi international union of geological sciences commission for the management and application of geoscience information publishes a suite of internationally agreed geoscience vocabularies 17 17 resource geosciml org the british geological survey bgs has started the publication of key stratigraphy vocabularies 18 18 data bgs ac uk api clients can also present the vocabulary in more human friendly formats within a web browser or as a text file downloads for the additional metadata elements required for this hazard metadata schema new code lists were required for parameters software code amongst others see below for this first version the vocabularies and definitions are only available internally to bgs and are not available as web resources this has been identified as a potential task for future work as discussed later 4 identifying metadata requirements in order to gain a better understanding of the metadata needs of environmental modellers a nerc funded study under the title metamodel ned ned referring to a nerc environmental data grant which funded the work hughes et al 2013 was undertaken the results from this study formed an important input to the development of the model metadata scheme proposed here and are summarised below this study was aimed at addressing issues related to ensuring that environmental models are properly recorded and documented so that the models can be retrieved and used at a later date accordingly two key aspects of model metadata were considered namely the use of discovery metadata in order to locate and access models of interest and also the range of metadata attributes required when actually using and running a model once it has been found technical metadata an on line questionnaire was circulated to three hundred and twenty stakeholders in universities commercial organisations and other research organisations including nerc data centres a total of 108 responses were received predominantly from europe but with some from the united states and australia this rate of response c 30 can be regarded as reasonably favourable in terms of what would typically be expected the recipients were well recognised specialists in environmental modelling the fact that many of the respondents held senior positions in their respective organisations and that responses were received from those working in a variety of environmental science disciplines e g climate change earth system modelling groundwater and land use modelling and hazard modelling in general was considered to also add weight to the conclusions drawn 4 1 find and locating models the need for discovery metadata one of the questions posed by the metamodel ned study investigated what metadata standards were being used by environmental modellers the results 19 19 see http nora nerc ac uk id eprint 504868 for the details on the results of the questionnaire indicated that the number of researchers making use of schemes such as iso 19115 and 19119 which are frequently used to describe metadata for datasets having a geographic component was relatively small less than 10 of those surveyed overall the questionnaire results indicated a general consensus that there is a lack of internationally accepted metadata standards available for metadata about environmental models this study showed that rather than making use of metadata to discover models a large proportion of the modellers questioned identified and located models of interest either through contact with organisations which they already collaborated with or by approaching recognised suppliers of specific models directly contact with collaborators or known producers of models could be expected to provide a good overview of models available in the researcher s own discipline but could mean that potentially useful models outside the researcher s immediate scientific discipline are overlooked there is clearly therefore a need for additional and potentially more detailed metadata available to help people find models there is also a perceived risk of researchers using models developed in other disciplines without access to comprehensive documentation and information on how to use the models respondents to the questionnaire were also asked about what metadata attributes they considered to be most important attributes such as descriptive information about the model the parameters used and the spatial extent of the model were relatively high on the list with 50 65 of respondents regarding these as of high importance followed by quality assessments it was notable the information about the type of model for example probabilistic or deterministic technical metadata about the software platform used to run the model were rated as of high importance by over 30 of respondents and the time period over which a model runs e g days months years were rated as high importance by c 20 of respondents also seen as important was some indication of the overall ease of use of models even at the discovery level so that modellers could avoid spending inordinate amounts of time and resources configuring an unfamiliar model ideally also there was interest in the models scientific pedigree e g how many peer reviewed publications did it feature in and how was it regarded by other researchers and whether the model actually answered the science questions it had been designed to address the availability of temporal information such as the time step used in the model and the relative time period being covered by the model is also information that researchers regarded as necessary both to locate and make use of a model further details on the responses and their ranking can be found in hughes et al 2013 4 2 using and running model codes descriptive metadata once models of interest have been located a researcher is potentially able to download the model code at least for a model which is in the public domain and then start to use it one of the areas highlighted by the metamodel ned study where current metadata schemas do not fully meet the requirements of modellers is in the provision of the information needed by modellers to use models of particular interest is information about how to actually configure a model once the user has located the model code e g c 60 of respondents rated the availability of assumptions made in building the model to be of high importance hence the provision of information about manuals particularly information available over the internet information on boundary conditions and assumptions made in using the model are seen as important technical metadata on how to configure the model code to run in a specific computing environment are also regarded as of high importance by over 30 of survey respondents for example the computer language in which the code is written and relevant details of the operating system on which that code is optimised together with memory or distributed computing environments for example where models are being coupled together in a linked composition series of connected models information on the input datasets and individual parameters which need to be provided to run a model and also the parameters output from the model are also regraded as of high importance by 60 of respondents 4 3 gaps in model metadata provision these indications of additional metadata requirements when describing environmental models point to a number of gaps in model metadata provision gaps in provision are present particularly in the representation of temporal information model code configuration and parameters used and exchanged between models and in technical metadata describing the computing and software environment used for modelling the major gaps in current metadata provision are described below and summarised in fig 1 temporal information metadata about temporal resolution and scale e g period of time over which measurements have been made years months weeks days or smaller time units is required particularly when linking models together also more information on dates and times when model runs were undertaken this could include start and end of calibration simulation and validation periods for example this information is considered more useful than dates when the metadata record was submitted model code configuration and parameters models codes are data hungry and require both spatial and temporal data to drive their instances therefore to describe a model instance requires the description of the data both in terms of spatially distributed but that doesn t change with time along with time variant data further information is required on the parameters used both the range of parameters which a model code can accept and also the parameters actually supplied which may be different in order to assist making use of model codes created by other researchers this information ideally also needs to include information on the units used so that for example any differences in units can be taken account of when linking models this would also include information about assumptions made in the model setup boundary conditions and the science questions which the model is aimed to address some of this information may be contained in manuals or peer reviewed papers and links to such information should also be included computing environment information on the programming language used to create the model code also details of overall operating system and computing requirements in terms of operating system and general processor memory requirements e g will the code run on a laptop or is an hpc system required 5 metadata schema development in designing a metadata schema to meet the requirements indicated above our approach was to build on those components of established metadata schemes such as iso19115 which are relevant to modelling and add additional functionality as necessary further current best practice for model metadata was used harpham and danovaro 2015 morsy et al 2017 extended using the results of the questionnaire reported in hughes et al 2013 by combining the international standards available for data metadata with current best practice for the model metadata then a comprehensive model metadata schema that is fit for purpose for environmental models as well as model integration is developed for example attributes such as title description or abstract and the geographic bounding box were clearly relevant to modelling from the user requirements gathering undertaken an important concept which became evident from our user requirements discussions was the consideration of the model code which describes the algorithm underlying the model and its encapsulation into compiled code and also the model instance or model application which is the application of the model code together with input data and relevant assumptions usually over a defined geographic area whilst it is recognised that this complicates the schema and provides model developers with more work it offers the advantage that once a model code is used many times then it only has to have one metadata entry obviously if model code variants proliferate then more entries are required it was clear from early discussions with the groundwater modelling community for example that there was a fairly restricted set of model codes or algorithms used for a wide range of different applications the usgs modflow bgs zoomq3d and dhi s feflow seem to be commonly used for a number of projects reported in the literature e g jackson et al 2005 the same seems to be the case for the seismic hazard community in terms of the workflows used for conducting the modelling process it was clear that different attributes were often related to the model code and to the model instance respectively separating the entity of a model code and relating it to zero one or possibly multiple model instances in the schema enables definitive details to be stored about each model code once only thus reducing errors which could result from slightly different information being entered about a model code each time it is used the inverse relationship from a model instance to a single model code is made mandatory which means that a model instance cannot be represented in the metadata schema until model code information has been provided which is also in line with good modelling practice the model code related metadata components include information about the computing environment for example the programming language used to write the code the compiler and the processor capabilities required this type of information allows a prospective user of the code to determine whether the code can be run on a laptop or whether for example it requires a high performance computing cluster fig 2 the model code metadata also records the model type model purpose and mathematical basis of a model these are implemented as controlled vocabularies in the data model allowing users of the metadata to query on these attributes for model instances the science question which the model is designed to answer can be recorded as a free text field as can any initial or boundary conditions required this information is very relevant to users of the model but also allows researchers to search for models from a particular environmental science discipline the modal instance metadata importantly also contains information about input and output datasets such datasets could include formal datasets which have a doi for example and therefore this information can also be recorded in other respects the model code and model instance may share certain attributes for example the model code metadata should contain information about the parameters which a model code takes as input in order for it to run whilst a model instance built using that model code would record the datasets and parameters actually used and their units similarly there is a requirement to record both details of publications and reports about both model codes and instances and both have contact details the key conceptual entities and their relationships are summarised in figs 3 and 4 5 1 model code the main object modelcode fig 3 contains the basic information of the model code software that processes the input data alongside the basic description of the code it has attributes containing information on what the model is model purpose model type and mathematical type as well as where to get further information on the model code such as a url linking to websites containing further details responsibleparty this provides information on the code guardian as well as links to an object organisation describing their organisations documentresource it is important that links to other documents are available this object stores the details of other documents such as a manual or a base paper in the peer reviewed literature modelcodeparameter this describes the parameters the model takes as either inputs or outputs it enables the user to determine whether the correct data are available as well as determining how the model codes could be linked keyphrase this is an object containing a free text field which further information describing the model code such as its uses can be stored computingenvironment information on the operating system e g windows pc or unix so that the user can decide whether they have access to the computational resources to run the model 5 2 model instance the main object modelinstance fig 4 has two groups of attributes one detailing the administration of the modelinstance instance of the object and the other describing aspects of the model itself the former has ids to link the modelinstance instances and dates of publication the latter has attributes to describe the vertical and horizontal extent of the modelinstance as well as the time stepping to help describe the model instance then documentresource and keyphrase as described for the modelcode can also be accessed using a many to many relationship for example a model code could be referenced in multiple papers and a paper could also describe more than one model code and so needs to be reflected in the data model this means that there is a pool of the instances of these objects which all model code instances can refer to further the responsibleparty and organisation objects and their instances can also be linked to the model instance sciencediscipline given that the pure portal is designed to store the model metadata for hazard models so this object stores the discipline area that the model instance was created for isogeobndbox since a model code has been applied to a geographical area to create an instance then the definition of the bounding box of the model application needs to be defined this object stores the co ordinates as bottom left and top right as well as the potential for shape objects such as shapefiles modelinstancedataset and modelinstanceparameter these describes the datasets and parameters the model instance have used as inputs and the outputs produced enables the user to determine whether the correct data are available as well as determining how the model codes could be linked modelinstancegrid given that the majority of numerical models are discretised and are grid based then information on the type of grid its characteristics are required there may be a need to extend this for non uniform grids but there is the ability to have a one to many relationship additionalinformation this is an object containing a free text field in which further information describing the model code such as its uses can be stored 6 implementation of the pure portal the basic system consists of a java based web front end oracle database and a spreadsheet data entry method the implementation was achieved using the scrum approach based on the agile development process the three main elements of the system are described in more detail below 6 1 web front end the software architecture utilised the internal guidelines laid out within the bgs and the application itself was built in house and implemented by bgs software developers the requirements gathering process consisted of on site visits to potential users and online surveys hughes et al 2013 as well as eliciting the experiences of scientists working on the pure programme and in bgs the solution arrived at is a web based application www pure org instead of a standalone desktop application fig 5 a d this enables users outside the organisation to access and submit data allowing wider contribution to the model metadata added the web application was built utilising an object orientated language java as this offered scalability reliability and security alongside the large internal pool of java developers within bgs made it a natural choice the web application was programmed to the latest version at the time of development java 8 the web application was built using an in house java framework this increased the productivity of the developers as they were working on application framework they were experienced with the application adopted a model view controller mvc architectural pattern a well established pattern that promoted maintainability and reusability of code the metadata required a database backend to provide persistent secure and backed up data storage that is scalable the metadata content needs to be constrained by various controlled vocabularies and we also required the potential for the database to be edited by multiple users should the web application be updated in the future to provide online editing and upload functionality a relational database management system rdbms is well suited to this purpose bgs already hosts an oracle 11 g rdbms database which supports many existing web based applications similarly bgs software developers have many years of experience in developing read write applications to the database backend and can rapidly build a web application by using familiar code frameworks for these reasons the database was implemented in the corporate oracle 11 g database for this application but was designed in an agnostic way such that it could easily be migrated or re created in different database software such as an open source rdbms as required we applied a map based feature to allow for location based searching and retrieval of datasets through an esri gis map based service the web application client side was built in html javascript and freemarker and the server side in java a map component was added to the application to show the location of the model metadata the gis functionality allows the researcher to quickly identify model metadata this addresses a requirement from researchers to identify models with minimal effort which would be relevant for further enquiry the application was designed primarily for desktop users but ensured the application rendered correctly for mobile and tablet devices as well given the likely use of the portal a specific app was not built for ios android or other mobile operating systems however it still can be viewed on hand held devices the metadata schema described above was implemented with an oracle 11 g relational database system since this is the corporate database platform used by the bgs however the schema could equally be implemented in another rdbms the relational implementation is built around the central concepts of model code and model instance described above key model code information is held in the table modelcode which stores information such as model type model purpose and mathematical basis of the model figs 3 and 4 and appendix 1 the primary key of this table is the field modelcodeidentifier this is a system populated numeric identifier to uniquely identify the model code the field code name allows a brief name for the model code to be captured similarly the modelinstance entity has a system populated numeric primary key field called modelinstanceidentifier the modelinstance entity also contains a field called alternative title which can be used to record a shortened name for the model instance for example for display on a web page the table modelinstance contains the title and abstract for the model instance in addition to information about the total vertical extent publication date and time step information other model instance concepts such as the science discipline and input and output datasets and individual input and output parameters are then linked to the modelinstance table fig 4 some entities have a many to many relationship between them for example a model code could have multiple reports describing it but a published report may also refer to more than one model similarly a model code may have a number of relevant key phrases and a key phrase can also be relevant to more than one model these many to many relationships are implemented by linking tables within the rdbms structure which contain the primary key fields of the two tables they link these are further described in appendix 1 where possible use was made of vocabulary or dictionary tables i e tables containing codified values together with an explanation of what those values mean in order to implement the controlled vocabularies for data items such as model purpose model type and the mathematical basis of the model the topics which are constrained by controlled vocabularies are listed in table 1 generally existing controlled vocabularies were not available for these concepts and so for example the model parameter vocabulary was developed pragmatically to reflect the actual values used by providers of the data future planned work will involve aligning the pure controlled vocabularies to any suitable vocabularies that already exist or may be created in the future information about the geographic extent of a model instance is held in the table isogeobndbox this stores the coordinates of a rectangle bounding the area represented by the model instance within the oracle implementation of the database an oracle spatial shape object has also been included as an additional attribute to provide for efficient manipulation of the spatial data and for example facilitate display within a geographic information system however this attribute is optional for example where the schema is implemented in a rdbms system which does not support spatial objects 6 2 data entry it was clear from the online survey and interviews that researchers were keen to have access to quality metadata but were reluctant to provide metadata due to time constraints as it was perceived that the filling in of forms and submission of metadata was an onerous process microsoft ms excel spreadsheets were created to allow the researchers to submit model data two ms excel forms were created one for the submission of model code and one for model instance each ms excel form was annotated and colour coded to help the researcher minimise the length of time required to fill in each section the application provided help documentation to support researchers to enable them to fill in the ms excel forms correctly survey s carried out on model researchers experience of submitting metadata to the application required supporting the model researcher on how to submit metadata correctly this ensured that model researchers would be less frustrated with metadata submission process and reduce the probability of model researchers giving up prior to submission and increasing the likelihood of metadata being submitted and be of good quality metadata submissions have come from a range of researchers the completed ms excel spreadsheets are submitted by the model researcher through the portal via e mail the metadata is then reviewed by a metadata curator to ensure data submitted is of sufficient quality or if requires further follow up with model researcher once data standards have been met the data is ingested into the metadata repository and made available through the portal 6 3 user testing the final application was tested with sample group of researchers both within bgs and externally the testing was carried out through a combination of observing participants utilising the application on line surveys and soliciting feedback through the portal the purpose was to identify areas for further improvement failure points to be immediately rectified demonstration of the application was carried out both to bgs model researchers and external participants the aim was to solicit feedback from researchers as well as to demonstrate and educate how to use the portal 7 summary and future directions the process of creating a model metadata schema and implementing it in the pure portal has been described based on previous work hughes et al 2013 along with a questionnaire response and informed by the experience of both developing and using data metadata a prototype model metadata standard has been created the main features requested from the questionnaire have been implemented including information on code guardians supporting documentation and technical metadata on timesteps model codes and model instances that is the software used to produce a model and its application to a particular area have separately defined metadata the pure portal has been created using an oracle database with a java based web front end whilst the focus of the work has been on hazard models the model metadata schema and its implementation of the pure portal can and has been applied to different types of model codes and their application the pure metadata model is only currently represented in human readable form on the pure portal website and in the data submission ms excel spreadsheet using labels for the metadata attributes that were chosen by the commissioners of the project to improve the interoperability of the data the elements of the metadata model should be mapped aligned to those in dublin core geo dcat ap and iso 19139 where possible the portal pages could be adapted so the user can choose the schema labels to display and the schema labels should be linked to their definitions to enable seamless integration with other metadata catalogues across various disciplines the pure portal could be adapted to be a web api that provides the metadata record encoded in one or more of iso 19139 xml dublin core rdf and geo dcat ap in machine readable data format note that this would be a lossy conversion as not all of the pure metadata elements are represented further mapping and alignment with these standards would quantify the extent of information loss to enable the full set of elements of the pure portal schema to be integrated in third party systems the schema itself should be published on the web as an xml schema or an rdf schema complete with its alignments to existing schemas 1 alignment with dublin core this work is yet to be done but from an initial analysis this would be relatively straightforward 2 alignment with iso19115 a comparison of the pure metadata model and the iso19115 2003 was carried out and most elements were found to have some equivalent in the iso standard with the notable exception of the input and output data parameters the assessment would need to be done in more depth and also to take into account changes implemented in the latest version iso19115 2014 3 formally extend iso metadata standard the iso 19115 standard provides a method for extending metadata to fit specialized needs harpham and danovaro 2015 the pure community could create a formal extension of the iso model to accommodate the metadata elements not already covered and publish the pure metadata standard as a profile 4 utilise linked data for controlled vocabularies the vocabularies developed for this metadata schema could be published via a web api and identified within the metadata by a uri this could be either via the new version of the bgs vocabulary service and linked data api currently in development and or the nerc vocabulary server nvs any inter relationships synonyms broader narrower terms etc between terms within these vocabularies could be identified and represented in rdf as simple knowledge organisation system 20 20 www w3 org 2009 08 skos reference skos html skos relations any relationships between these vocabulary terms and existing terms in the nvs or published from other sources could be identified and represented in rdf as skos relations such as skos closematch the pure portal is available for use by researchers model developers and model users to store and access metadata for openly available hazard models the fair principal findability accessibility interoperability and reusability wilkinson et al 2016 has been applied for datasets for a number of years the pure portal provides the elementary infrastructure for the guiding principle of fair to be extended to model codes and their instances which is a requirement if model integration is to be achieved further the metadata schema that has been developed can be fed into the process of defining an international recognised metadata standard facilitated by the ogc or another international standards body if the barriers to achieving model integration are to be lowered then model codes and their instances should be discoverable available and linkable the pure portal is another step along the way to achieving this aim software availability the pure portal is available as two instances www pureportal org as well as the natural environmental research council nerc model repository model search nerc ac uk used for nerc grant holders the database development was undertaken in common with bgs standard approach the vocab servers are available via vocab nerc ac uk acknowledgements this work is published with the permission of the executive director of the british geological survey nerc it was funded by the natural environmental research council nerc as part of the probability uncertainty and risk programme we thank two anonymous reviewers for their comments which helped to improve the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104597 appendix descriptions of the entities and attributes shown in figs 3 and 4 entity descriptions entity name description modelcode information about the model code used to run a model documentresource details of documents and papers providing information about models including manuals and peer reviewed papers modelcodedocument link table to implement many to many relationship between modelcode and documentresource modelinstancedocument link table to implement many to many relationship between modelinstance and documentresource modelcodeparameter parameters required by a model code computingenvironment computing environment under which a model runs responsibleparty responsible parties e g creators or maintainers of models organisation organisation to which the responsible party is affiliated keyphrase keywords and keyphrases to describe the model to assist searching and discovery modelcodekeyphrase link table to implement many to many relationship between modelcode and keyphrase modelinstancekeyphrase link table to implement many to many relationship between modelinstance and keyphrase modelinstance entity holding key information about a model instance i e the application of a model code to a specific geographic area or problem sciencediscipline scientific discipline to which a model relates isogeobndbox details of the geographic rectangular bounding box relating to a model instance additionalinformation supporting information about a model instance e g the science question underlying the model or boundary conditions modelinstancegrid describes the grid used where a model is gridded modelinstancedataset details of datasets input or output from a model modelinstanceparameter details of parameters used by a model instance attribute descriptions entity name attribute name data type mandatory y n description modelcode model code identifier pk number y unique identifier for each model code code name character y name by which the model code is known code description character n description of what the model code does model purpose character y vocabulary constrained code representing the purpose for which the model code is designed model type character y vocabulary constrained code representing the type of model url link character n link to a url where the code can be downloaded if available ninimum time step character n minimum time step maximum time step character n maximum time step version number character n version of the model code e g 1 01 publication date date n date of publication of the model code mathematical type character y vocabulary constrained code representing the mathematical basis of the code documentresource document identifier pk number y unique id for a document report title character y title of the paper report publication flag character n flag to indicate whether the paper report is published documnnt url character n url link to the paper report where available document doi character n digital object identifier for a published paper report authors character n list of authors year published date n year of publication document details character n details of the paper report e g details of journal or other publication modelcodedocument model code identifier pk number y model code identifier document identifier pk number y document identifier modelinstancedocument model instance identifier pk number y model instance identifier document identifier pk number y document identifier modelcodeparameter parameter identifier pk number y unique identifier for each parameter model code identifier number y provides link to modelcode entity parameter code character y vocabulary constrained code representing the parameter e g ground water level etc input output flag character n flag field to indicate whether an input or output parameter computingenvironment computing environment identifier pk number y unique identifier for each record model code identifier number y foreign key database link to modelcode table code language character y vocabulary constrained field for code language e g c fortran etc compiler character n details of the compiler used to compile the code if relevant operating system character y vocabulary constrained field for operating system cpu character n details of the cpu required to run the code system memory character n required system memory comments character n comments field responsibleparty resparty identifier pk number y unique id for each responsible party record used by database contact name character y contact name organisation identifier number y provides link to organisation country character n country email address character n email address organisation organisation identifier pk number y unique id organisation organisation name character y organisation name keyphrase keyphrase identifier pk number y unique id for keyphrase keyphrase description character y keyphrase description scope notes character n information about the scope of the keyphrase status character n whether active obsolete data source character n records the source of the keyphrase definition character n definition keyphrase type character n type code to allow different subsets of keyphrases to be held in the same database table modelcodekeyphrase model code identifier pk number y model code identifier keyphrase identifier pk number y keyphrase identifier modelinstancekeyphrase model instance identifier pk number y model instance id keyphrase identifier pk number y keyphrase identifier modelinstance model instance identifier pk number y unique id for model instance model code identifier number y unique model code record date date n date record created maximum vertical extent number n maximum vertical extent of model instance minimum vertical extent number n minimum vertical extent of model instance title character y model instance title abstract character n model instance abstract spatial reference system character y unique id for spatial reference system constrained by vocabulary vertical datum character y vertical datum e g mean sea level constrained by vocabulary vertical scope character y vertical scope constrained by vocabulary vertical units character n unit of measurement for vertical dimension publication date date n publication date of the model to which this metadata refers start date date n start date of the period which the model represents end date date n end date of the period which the model represents temporal reference system character y temporal reference system constrained by vocabulary completion status character n completion status of the metadata e g partial or complete parent citation identifier number n citation id of a metadata record of which this record is a child thus implementing the potential for hierarchical relationships alternative title character n alternative title e g used to store a shortened title for the model instance for display purposes date submitted date n date the metadata record was submitted time step interval character n the interval for time step if relevant e g hour day month etc time step value number n the frequency of that time step e g every 6 h etc sciencediscipline discipline code pk character y unique code representing each scientific discipline model instance identifier number y model instance id link to modelinstance table isogeobndbox geo bounding box identifier pk number y unique id assigned by database for each bounding box record model instance identifier number y model instance identifier north west easting number y the easting of the north west corner of the bounding rectangle north west northing number y the northing of the north west corner of the bounding rectangle south east easting number y the easting of the south east corner of the bounding rectangle south east northing number y the northing of the north east corner of the bounding rectangle shape blob n geometry object representing the bounding rectangle e g oracle spatial shape object in oracle implementation additionalinformation additional information identifier pk number y unique id assigned by database for each additional information record model instance identifier number y link to modelinstance description character n text representing the additional information e g stating the boundary conditions etc additional information type character n code representing the type of additional information e g science question boundary conditions etc modelinstancegrid grid identifier pk number y unique id assigned by database for each grid record model instance identifier number y model instance id provides link to modelinstance table grid spacing x number n grid spacing in the x dimension horizontal plane grid spacing y number n grid spacing in the y dimension horizontal plane grid spacing z number n grid spacing in the z depth dimension grid cells x number n number of grid cells in the x dimension grid cells y number n number of grid cells in the y dimension grid cells z number n number of grid cells in the z dimension grid type character y vocabulary constrained code representing the type of grid used modelinstancedataset dataset identifier pk number y unique id assigned by database for each dataset record model instance identifier number y provides link to modelinstance entity dataset name character y name of dataset dataset description character n description of dataset dataset version character n dataset version dataset type character y vocabulary constrained representing the dataset type dataset url character n a url providing more information about the dataset or from where it can be accessed temporal reference system character n temporal reference system relating to the dataset temporal resolution character n temporal resolution relating to the dataset in out flag character n flag to indicate whether the dataset is input to the model or an output from it modelinstanceparameter parameter identifier pk number y unique id for parameter record dataset identifier number y dataset id foreign key link to modelinstance table parameter code character y unique code for each parameter constrained by vocabulary parameter value number n specific parameter value parameter unit character n the units of measurement used for the parameter 
