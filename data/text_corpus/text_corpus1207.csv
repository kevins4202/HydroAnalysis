index,text
6035,dielectric sensors are a popular choice for soil moisture determination however the output of many of these devices especially those operating at frequencies in the mhz range can be significantly affected by the influence of soil salinity and texture the aim of this paper is to examine the impact of salinity texture and measurement frequency on dielectric permittivity soil moisture calibration curves for ten soils of medium texture in the frequency range from 20 mhz to 3 ghz based on the complex dielectric permittivity spectra measured with the use of a coaxial transmission line cell connected to a vector network analyzer the use of laboratory measured spectra ensures that at all frequencies dielectric permittivity is measured at the same conditions with no influence of factors such as different sensitivity zone or various installation conditions as in the case of comparing individual sensors operating at various frequencies the obtained relations were comparable to the calibration curves of several capacitance impedance sensors operating at corresponding frequencies calibration curves for apparent permittivity which were obtained with the use of simulations in the time domain based on the measured spectra were also determined for the tested soils even though the tested soils did not include soils with high clay content the impact of soil texture on the calibration curves was observed results confirmed that the influence of soil salinity and texture on dielectric permittivity was the most significant in the lower part of the frequency spectrum while the smallest impact of these factors was observed at frequencies of several hundred mhz and above keywords soil moisture soil salinity complex dielectric permittivity spectrum microwave measurements vector network analyzer calibration functions 1 introduction accurate monitoring of soil water content can contribute to improvement of water efficiency in irrigation levidow et al 2014 however one of the factors limiting the use of soil moisture sensors in agriculture is concern over their measurement accuracy inadequate for application in automated irrigation systems levidow et al 2014 soulis et al 2015 the performance of various kinds of commercial moisture sensors in soils of various water content salinity and texture has been extensively examined varble and chávez 2011 vaz et al 2013 schwartz et al 2013 singh et al 2018 kargas and soulis 2019 nevertheless there is no widely accepted standard for the assessment and comparison of the sensors performance jones et al 2018 dielectric sensors are a popular choice for in situ soil moisture measurement robinson et al 2008 jones et al 2018 most of these devices belong into one of the following groups differing in operating principle i time domain reflectometry tdr and time domain transmissometry tdt sensors which utilize a step or a needle pulse reflection or transmission analysis and ii frequency domain reflectometry fdr devices which operate at a single or several specific frequencies usually measuring directly capacitance or impedance among the sensors utilizing the frequency domain measurements there are also resonant sensors whose frequency of operation depends on soil dielectric properties stacheder et al 2009 soil moisture dielectric sensors generally enable easy fast and minimally invasive measurement many of these devices can be used for continuous monitoring in the field bogena et al 2007 dorigo et al 2011 skierucha et al 2012 most fdr sensors operate at frequencies below 300 mhz vaz et al 2013 soil complex dielectric permittivity ε ε j ε at such frequencies is affected by multiple dielectric relaxation mechanisms stemming from the existence of bound water electrical double layer and other interfacial phenomena kelleners et al 2005 wagner et al 2011 these phenomena can be especially strong in soils rich in clay and organic matter also the presence of dissolved ions has impact not only on the imaginary part of dielectric permittivity ε but also on the real permittivity ε at low frequencies e g through the maxwell wagner effect campbell 1990 chen and or 2006 therefore the relation between θ and ε at frequencies in the mhz range can be much more strongly affected by the soil salinity and other soil properties than in the tdr technique where the apparent dielectric permittivity ε a is obtained from the waveform whose input pulse spectrum stretches often into the ghz frequencies kelleners et al 2005 indeed strong influence of soil salinity on the output of many soil moisture sensors has been observed rosenbaum et al 2011 kargas et al 2014 kargas and soulis 2019 among the dielectric sensors tdr and tdt devices are generally recognized as the most accurate however some studies demonstrated the impact of soil salinity and clay content on the tdr readout and thus on volumetric water content θ measurement accuracy bittelli et al 2008 topp et al 2000 despite the concerns regarding moisture measurement accuracy most fdr devices have been generally less expensive than the tdr instruments making them more attractive for popular use in order to improve the performance of the frequency domain devices by extending their operational frequency range or enabling measurement of some part of dielectric spectrum several new techniques were investigated logsdon 2008 skierucha and wilczek 2010 xu et al 2012 pelletier et al 2012 kitić and crnojević bengin 2013 szypłowska et al 2016 lewandowski et al 2018 lin et al 2018 the determination of broadband complex dielectric permittivity spectra of various soils and the impact of salinity on the θ ε calibration curves at various frequencies is beneficial for the development of new soil moisture meters that operate in the frequency domain in szypłowska et al 2018 the θ ε relations in the 20 mhz 3 ghz frequency range for three soils of different texture and varied bulk electrical conductivity were examined the aim of the present study is to extend this analysis to ten more soils with the application of an increased frequency resolution and the inclusion of apparent permittivity analysis the use of laboratory measured dielectric spectra ensures that at all examined frequencies soil sample properties are the same and there is no influence of sensor sensitivity zone various installation conditions or raw permittivity measurement accuracy as in the case of testing individual sensors operating at specific frequencies the soil tested in the present study belong to loamy sand sandy loam and silt loam textures the obtained functions were compared to the established calibration curves from the literature based on the results an optimal frequency range with minimal influence of soil salinity and texture was determined 2 materials and methods 2 1 dielectric spectrum measurement complex dielectric permittivity spectra of porous materials can be measured with the use of coaxial transmission line cells sabouroux and ba 2011 lauer et al 2012 bobrov et al 2015 lewandowski et al 2017 neves et al 2018 here a two port coaxial transmission line system connected to a vectorstar ms4642a anritsu corporation japan vector network analyzer vna presented in lewandowski et al 2017 was used the sample cell consisted of a section of a coaxial transmission line of 38 8mm inner diameter made of acid resistant steel 316l the outer diameter of the inner conductor was 16 9mm and the total length of the sample cell was 73 4mm material under test mut was confined with the use of 7 5mm thick plastic supports made from polyoxymethylene and equipped with o rings to prevent evaporation and leakage the position of the plastic supports was adjustable to accommodate the length of the loaded sample from about 3 to 4 6cm the sample cell was connected to the vna with the use of commercial type n eia 1 5 8 adapters spinner gmbh germany the cell and adapters were placed in a vertical position the lower type n eia 1 5 8 adapter was connected to the vna via a low loss armored test cable type pe3c0231 from pasternack that was secured in a fixed position and via appropriate adapters the upper type n eia 1 5 8 adapter was movable it was connected by a cascade of two phase stable cables lu1 005 1000 from rosenberger and 3671kfk50 60 from anritsu with appropriate adapters the dimensions of the coaxial cell and the use of the type n eia 1 5 8 adapters were modeled on the measurement system presented in lauer et al 2012 the measurement cell connected to the type n eia 1 5 8 adapters is presented in fig 1 the system was calibrated with the use of the multiline thru reflect line trl scheme utilizing five coaxial transmission lines of various lengths an open standard and a 50 ω matched load complex dielectric permittivity was extracted from the transmission parameters with the use of a nonlinear least squares optimization algorithm the dielectric spectra processed in the present paper were also recently used for the salinity index model evaluation at various frequencies szypłowska et al 2019 further details of the measurement system calibration procedure and permittivity extraction algorithm were presented in lewandowski et al 2017 lewandowski et al 2017 lewandowski et al 2019 2 2 soil material and sample preparation ten soils with properties presented in tables 1 and 2 were tested before the experiment the soils were air dried and sieved particle size distribution was determined with the use of a laser diffractometer mastersizer 2000 with a hydro g dispersion unit uk in accordance with the procedure presented in bieganowski et al 2018 specific surface area was determined using the brunauer emmett teller nitrogen gas adsorption method by a surface characterization analyzer 3flex micromeritics usa total organic carbon content was measured with the use of a toc v cph analyzer shimadzu corporation japan equipped with a solid sample combustion unit ssm 5000a solid phase density was determined with the use of a helium pycnometer ultrapyc 1200e v4 01 quantachrome instruments usa the samples were prepared by mixing the air dry soil material with three potassium chloride kcl solutions of electrical conductivity σ s 0 52 1 02 and 1 50 s m 1 and distilled water σ s 2 10 4 s m 1 for each sample a predefined amount of liquid was measured in order to achieve target water content by mass w calculated on a dry mass basis according to the following formula 1 w m w m s where m w is the mass of the added water and m s is the mass of dry soil material five water contents up to near the point of saturation and four salinity levels were achieved which gave 20 samples for each tested soil after mixing the soil material was loaded into the sample cell and closed by the upper plastic bead the positions of the beads were measured before the cell installation in the system for the purpose of correcting for the transmission line section between the mut and the calibration plane at the end of the sample cell and in order to obtain the volume of a given sample from the target water content by mass and the determined bulk soil sample density d b volumetric water content of each sample was calculated according to the formula 2 θ w 1 w d b d w where d w is the density of water in the present study density of the samples varied from 1 37 to 2 31 g cm 3 with the mean value of about 1 82 g cm 3 all measurements were done at room temperature 21 1 5 c due to experimental protocol errors four samples from the set of 200 measured samples in total were excluded from further analysis 2 3 processing of the measured spectra the obtained dielectric spectra were modeled with the use of a three pole debye model with a direct current bulk electrical conductivity term σ b 3 ε ε i 1 3 δ ε i 1 j ω τ i j σ b ω ε 0 where ε is the high frequency limit dielectric permittivity ω 2 π f with f as the electric field frequency δ ε i and τ i stand for the relaxation amplitude and time of an i th debye pole respectively and ε 0 8 85 10 12 f m 1 is the value of vacuum permittivity the fitted functions effectively smoothed the spectra in order to diminish the impact of measurement errors on the subsequent analysis the number of poles i e three was chosen in order to accurately model the spectra while avoiding excessive number of parameters to fit among all of the analyzed spectra the mean value of rmse of fitting of eq 3 was 0 08 with the minimum and maximum values of 0 02 and 0 23 respectively the obtained functions were then used to obtain the real part of dielectric permittivity at frequencies from 20 mhz to 3 ghz with 10 mhz resolution for each measured soil sample next the θ ε calibration curves were fitted in the form of a linear function of the square root of dielectric permittivity as given by the following equation 4 θ a ε b for each soil separately and for all soils combined at each examined frequency an equation of the above form is frequently used to describe the relation between soil volumetric water content and dielectric permittivity vaz et al 2013 skierucha and wilczek 2010 delta t devices ltd 2017 the rmse was calculated for each fitted curve according to the eq 5 the values of θ i represent the experimental data and the values obtained from the fitted model are represented by θ mi n is the total number of observations of θ i the sum of the mean squared difference between the experimental data and the modeled values was divided by the number of the degrees of freedom equal to the number of observations of a given soil minus the number of fitted parameters which was 2 for the linear function 5 rmse i 1 n θ i θ mi 2 n 2 in order to compare the obtained calibration curves at specific frequencies to the calibration curves that could be obtained in the time domain apparent dielectric permittivity was simulated for each measured dielectric spectrum according to a procedure described in the appendix for the data simulated in the time domain eq 4 was also used in order to determine calibration functions but with ε a substituting ε all numerical operations on the measured spectra were performed in matlab software mathworks inc usa with the exception of the determination of apparent permittivity in the time domain performed in advanced design system ads software keysight technologies formerly agilent usa described in the appendix 3 results and discussion 3 1 complex dielectric permittivity spectra sample spectra of ε of soil no 613 moistened with kcl solutions of σ s 0 52 and 1 5 s m 1 are presented in fig 2 eq 3 was fitted to the spectra and the resultant functions are shown in the plots bulk electrical conductivity of the samples depended on moistening solution and water content the ranges of σ b in dsm 1 for each tested soil and each solution are presented in table 3 at all frequencies water content was the major influence on complex dielectric permittivity at frequencies below 0 2 0 5 mhz a rise in ε with a decrease in frequency is noticeable as seen in fig 2 a b and fig 3 this can be attributed to the impact of low frequency dielectric dispersion mechanisms associated with interfacial phenomena such behavior of ε agrees with the literature data kelleners et al 2005 chen and or 2006 wagner et al 2011 lauer et al 2012 fig 3 presents spectra of the real part of dielectric permittivity of four samples of the same soil of one water content level but moistened with various solutions the rise in ε at low frequencies depends on salinity i e the more saline solution was used and thus the higher σ b was the higher ε was observed the relations between σ b and ε in the 20 mhz 3 ghz frequency range have been examined in szypłowska et al 2019 in the aspect of extraction of soil solution electrical conductivity with the use of the salinity index model the measured spectra were used to simulate the apparent dielectric permittivity that can be measured using time domain techniques the waveforms of samples of soil no 613 moistened with the kcl solution of σ s 0 52 s m 1 whose dielectric spectra are presented in fig 2 a and b are shown in fig 4 with the increase in water content the round trip propagation time of the simulated pulse increased as expected 3 2 relations between moisture and permittivity in fig 5 volumetric water content was plotted as a function of the real part of dielectric permittivity at several chosen frequencies 20 50 100 and 450 mhz and 1 and 3 ghz for all measured samples prepared with all used moistening solutions with σ b ranges as presented in table 3 these specific frequencies were chosen in order to illustrate the frequency behavior of the θ ε relations in the plots all measurement points from all soils are shown but only four fitted functions for selected soils are included for clarity also some reference calibration curves from the literature including the functions from three widely used and frequency specific water content sensors were shown for clarity 1 wet2 sensor calibrations for mineral and clay soils at 20 mhz vaz et al 2013 6 θ 0 099 ε 0 178 mineral θ 0 091 ε 0 182 clay 2 hydra probe hp calibration function for mineral and loam soils at 50 mhz vaz et al 2013 7 θ 0 109 ε 0 179 3 ml3 thetaprobe calibration function for mineral soils at 100 mhz delta t devices ltd 2017 8 θ 0 119 ε 0 190 4 calibration function for an fdr sensor in the frequency range 390 480 mhz skierucha and wilczek 2010 9 θ 0 113 ε 0 167 5 topp s equation obtained for tdr measurements topp et al 1980 10 θ 5 3 10 2 2 92 10 2 ε a 5 5 10 4 ε a 2 4 3 10 6 ε a 3 the spread in the measured points stemming from the influence of salinity and texture on ε depends on the measurement frequency which is especially pronounced at 20 and 50 mhz the grt soil with the highest clay fraction among the tested soils causes the greatest enhancement of the real permittivity as a function of water content for the lower frequencies with an increase in frequency differences among the tested soils become smaller at 450 mhz and above differences among data points for the various soils and salinity levels diminish for the range of water contents examined the fitted parameters of eq 4 their standard errors as well as r 2 and rmse obtained at six selected frequencies for each soil and for all soils combined are presented in table 4 in order to assess the goodness of fit between measured values and eq 4 in a quantitative way the rmse was computed for each soil and for all soils combined and at all examined frequencies from 20 mhz to 3 ghz with 10 mhz interval the frequency dependence of the rmse is presented in fig 6 where for each case the maximum value of rmse was obtained at 20 mhz for each tested soil the rmse decreased with an increase in frequency especially at frequencies below 200 500 mhz depending on a given soil the largest difference between the maximum and minimum values of the rmse among the soils was observed for soil no 617 also the smallest minimal value of the rmse of 0 00302 was observed for soil no 617 at frequency 940 mhz for some soils the rmse started to rise slightly with frequency above 2 ghz though this effect did not impact the calibration for all soils combined for all soils combined the smallest rmse rmse min 0 0098 was observed at 3 ghz for frequencies above about 500 mhz the rmse changes little and is around 0 01 a quantitative cut off condition for acceptable values of the rmse could also be taken as 2 rmse min which corresponds to the case of twice the variance than for the cases with rmse min 1 1 a similar condition is used in the filter theory to determine the 3 db cut off frequency at this frequency the filter amplitude response drops by 2 with respect to the asymptotic value here for all soils combined 2 rmse min 0 0139 the values of the rmse smaller than 2 rmse min were achieved at the frequency of 250 mhz and above the values of rmse min and 2 rmse min are depicted in fig 6 the obtained results indicate that at frequencies above several hundred mhz the influence of soil salinity and texture is minimized with respect to the lower frequency range with the smallest rmse obtained at the highest analyzed frequency the results also indicate the need of performing soil specific calibration especially at lower frequencies szypłowska et al 2018 demonstrated that the frequency range 150 500 mhz was optimal for the moisture calibration functions for three tested soils the results obtained for 10 more soils in the present study did not exhibit any clear minimum pointing to frequencies of several hundred mhz and higher as the most optimal instead the results obtained in the present study with the use of apparent permittivity determined from simulated tdr waveforms are presented in fig 7 and table 5 the obtained relations for each soil as well as for all soils combined are generally similar to relations obtained for frequency domain data at 1 ghz also in terms of rmse generally the obtained data and corresponding soil moisture dielectric permittivity calibration functions are slightly shifted towards higher permittivity values in comparison to reference calibration curves at all frequencies and also in the case of ε a this would result in overestimation of volumetric water content while using factory calibrations this issue has also been reported in other studies for several commercial soil moisture sensors singh et al 2018 4 conclusions in the case of the measured soils the impact of soil salinity on the θ ε relation was the smallest at frequencies of several hundred mhz and above thus the most accurate soil moisture determination based on ε measurement with the use of a general calibration function is possible at frequencies above at least 250 mhz at low frequencies the impact of soil texture is visible even though the tested soils are of medium texture and do not include soils of high clay content for the most accurate results and especially at low frequencies soil specific calibration is advised the obtained rmse in this optimal frequency range is comparable to the rmse of the calibration function obtained with the use of apparent permittivity calculated in the time domain it is important also to note that soil in the field differs from the laboratory prepared material in terms of density heterogeneity the presence of plant roots soil organisms stones etc therefore the relations provided in the paper should not be used directly for any device meant for field measurements without appropriate verification further research will explore the applicability of the developed relations for field measurements declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding the project is financed by the polish national agency for academic exchange soil complex dielectric permittivity spectra used for the analysis were obtained in the scope of the project no 2014 15 d st10 04000 financed by the polish national science centre ncn appendix a time domain analysis for a tem transmission line filled with a dielectric with permittivity ε one can write the scattering parameters as baker jarvis 1990 baker jarvis et al 2005 11 s 11 s 22 γ 1 z 2 1 γ 2 z 2 12 s 21 s 12 z 1 γ 2 1 γ 2 z 2 where 13 γ 1 ε 1 ε 14 z e j 2 π f c ε l and f is the frequency c is the speed of light in vacuum and l is the length of the transmission line here arbitrarily taken as 10 cm assuming that the line is terminated with an open circuit one can easily show that the input reflection coefficient γ in will be given by 15 γ in z 2 γ 1 γ z 2 this value was used to describe the sample in ads transient state simulations see fig 8 in these simulation a pulse with 150ps rise and fall time and 10ps width was sent into a load with reflection coefficient given by 15 and the delay of the reflected pulse was determined and used as t p a screenshot of the simulation circuit is shown in fig 8 while some simulation results are depicted in fig 4 from the obtained t p values for each measured spectrum apparent permittivity was directly calculated with the use of the following formula 16 ε a ct p 2 l 2 
6035,dielectric sensors are a popular choice for soil moisture determination however the output of many of these devices especially those operating at frequencies in the mhz range can be significantly affected by the influence of soil salinity and texture the aim of this paper is to examine the impact of salinity texture and measurement frequency on dielectric permittivity soil moisture calibration curves for ten soils of medium texture in the frequency range from 20 mhz to 3 ghz based on the complex dielectric permittivity spectra measured with the use of a coaxial transmission line cell connected to a vector network analyzer the use of laboratory measured spectra ensures that at all frequencies dielectric permittivity is measured at the same conditions with no influence of factors such as different sensitivity zone or various installation conditions as in the case of comparing individual sensors operating at various frequencies the obtained relations were comparable to the calibration curves of several capacitance impedance sensors operating at corresponding frequencies calibration curves for apparent permittivity which were obtained with the use of simulations in the time domain based on the measured spectra were also determined for the tested soils even though the tested soils did not include soils with high clay content the impact of soil texture on the calibration curves was observed results confirmed that the influence of soil salinity and texture on dielectric permittivity was the most significant in the lower part of the frequency spectrum while the smallest impact of these factors was observed at frequencies of several hundred mhz and above keywords soil moisture soil salinity complex dielectric permittivity spectrum microwave measurements vector network analyzer calibration functions 1 introduction accurate monitoring of soil water content can contribute to improvement of water efficiency in irrigation levidow et al 2014 however one of the factors limiting the use of soil moisture sensors in agriculture is concern over their measurement accuracy inadequate for application in automated irrigation systems levidow et al 2014 soulis et al 2015 the performance of various kinds of commercial moisture sensors in soils of various water content salinity and texture has been extensively examined varble and chávez 2011 vaz et al 2013 schwartz et al 2013 singh et al 2018 kargas and soulis 2019 nevertheless there is no widely accepted standard for the assessment and comparison of the sensors performance jones et al 2018 dielectric sensors are a popular choice for in situ soil moisture measurement robinson et al 2008 jones et al 2018 most of these devices belong into one of the following groups differing in operating principle i time domain reflectometry tdr and time domain transmissometry tdt sensors which utilize a step or a needle pulse reflection or transmission analysis and ii frequency domain reflectometry fdr devices which operate at a single or several specific frequencies usually measuring directly capacitance or impedance among the sensors utilizing the frequency domain measurements there are also resonant sensors whose frequency of operation depends on soil dielectric properties stacheder et al 2009 soil moisture dielectric sensors generally enable easy fast and minimally invasive measurement many of these devices can be used for continuous monitoring in the field bogena et al 2007 dorigo et al 2011 skierucha et al 2012 most fdr sensors operate at frequencies below 300 mhz vaz et al 2013 soil complex dielectric permittivity ε ε j ε at such frequencies is affected by multiple dielectric relaxation mechanisms stemming from the existence of bound water electrical double layer and other interfacial phenomena kelleners et al 2005 wagner et al 2011 these phenomena can be especially strong in soils rich in clay and organic matter also the presence of dissolved ions has impact not only on the imaginary part of dielectric permittivity ε but also on the real permittivity ε at low frequencies e g through the maxwell wagner effect campbell 1990 chen and or 2006 therefore the relation between θ and ε at frequencies in the mhz range can be much more strongly affected by the soil salinity and other soil properties than in the tdr technique where the apparent dielectric permittivity ε a is obtained from the waveform whose input pulse spectrum stretches often into the ghz frequencies kelleners et al 2005 indeed strong influence of soil salinity on the output of many soil moisture sensors has been observed rosenbaum et al 2011 kargas et al 2014 kargas and soulis 2019 among the dielectric sensors tdr and tdt devices are generally recognized as the most accurate however some studies demonstrated the impact of soil salinity and clay content on the tdr readout and thus on volumetric water content θ measurement accuracy bittelli et al 2008 topp et al 2000 despite the concerns regarding moisture measurement accuracy most fdr devices have been generally less expensive than the tdr instruments making them more attractive for popular use in order to improve the performance of the frequency domain devices by extending their operational frequency range or enabling measurement of some part of dielectric spectrum several new techniques were investigated logsdon 2008 skierucha and wilczek 2010 xu et al 2012 pelletier et al 2012 kitić and crnojević bengin 2013 szypłowska et al 2016 lewandowski et al 2018 lin et al 2018 the determination of broadband complex dielectric permittivity spectra of various soils and the impact of salinity on the θ ε calibration curves at various frequencies is beneficial for the development of new soil moisture meters that operate in the frequency domain in szypłowska et al 2018 the θ ε relations in the 20 mhz 3 ghz frequency range for three soils of different texture and varied bulk electrical conductivity were examined the aim of the present study is to extend this analysis to ten more soils with the application of an increased frequency resolution and the inclusion of apparent permittivity analysis the use of laboratory measured dielectric spectra ensures that at all examined frequencies soil sample properties are the same and there is no influence of sensor sensitivity zone various installation conditions or raw permittivity measurement accuracy as in the case of testing individual sensors operating at specific frequencies the soil tested in the present study belong to loamy sand sandy loam and silt loam textures the obtained functions were compared to the established calibration curves from the literature based on the results an optimal frequency range with minimal influence of soil salinity and texture was determined 2 materials and methods 2 1 dielectric spectrum measurement complex dielectric permittivity spectra of porous materials can be measured with the use of coaxial transmission line cells sabouroux and ba 2011 lauer et al 2012 bobrov et al 2015 lewandowski et al 2017 neves et al 2018 here a two port coaxial transmission line system connected to a vectorstar ms4642a anritsu corporation japan vector network analyzer vna presented in lewandowski et al 2017 was used the sample cell consisted of a section of a coaxial transmission line of 38 8mm inner diameter made of acid resistant steel 316l the outer diameter of the inner conductor was 16 9mm and the total length of the sample cell was 73 4mm material under test mut was confined with the use of 7 5mm thick plastic supports made from polyoxymethylene and equipped with o rings to prevent evaporation and leakage the position of the plastic supports was adjustable to accommodate the length of the loaded sample from about 3 to 4 6cm the sample cell was connected to the vna with the use of commercial type n eia 1 5 8 adapters spinner gmbh germany the cell and adapters were placed in a vertical position the lower type n eia 1 5 8 adapter was connected to the vna via a low loss armored test cable type pe3c0231 from pasternack that was secured in a fixed position and via appropriate adapters the upper type n eia 1 5 8 adapter was movable it was connected by a cascade of two phase stable cables lu1 005 1000 from rosenberger and 3671kfk50 60 from anritsu with appropriate adapters the dimensions of the coaxial cell and the use of the type n eia 1 5 8 adapters were modeled on the measurement system presented in lauer et al 2012 the measurement cell connected to the type n eia 1 5 8 adapters is presented in fig 1 the system was calibrated with the use of the multiline thru reflect line trl scheme utilizing five coaxial transmission lines of various lengths an open standard and a 50 ω matched load complex dielectric permittivity was extracted from the transmission parameters with the use of a nonlinear least squares optimization algorithm the dielectric spectra processed in the present paper were also recently used for the salinity index model evaluation at various frequencies szypłowska et al 2019 further details of the measurement system calibration procedure and permittivity extraction algorithm were presented in lewandowski et al 2017 lewandowski et al 2017 lewandowski et al 2019 2 2 soil material and sample preparation ten soils with properties presented in tables 1 and 2 were tested before the experiment the soils were air dried and sieved particle size distribution was determined with the use of a laser diffractometer mastersizer 2000 with a hydro g dispersion unit uk in accordance with the procedure presented in bieganowski et al 2018 specific surface area was determined using the brunauer emmett teller nitrogen gas adsorption method by a surface characterization analyzer 3flex micromeritics usa total organic carbon content was measured with the use of a toc v cph analyzer shimadzu corporation japan equipped with a solid sample combustion unit ssm 5000a solid phase density was determined with the use of a helium pycnometer ultrapyc 1200e v4 01 quantachrome instruments usa the samples were prepared by mixing the air dry soil material with three potassium chloride kcl solutions of electrical conductivity σ s 0 52 1 02 and 1 50 s m 1 and distilled water σ s 2 10 4 s m 1 for each sample a predefined amount of liquid was measured in order to achieve target water content by mass w calculated on a dry mass basis according to the following formula 1 w m w m s where m w is the mass of the added water and m s is the mass of dry soil material five water contents up to near the point of saturation and four salinity levels were achieved which gave 20 samples for each tested soil after mixing the soil material was loaded into the sample cell and closed by the upper plastic bead the positions of the beads were measured before the cell installation in the system for the purpose of correcting for the transmission line section between the mut and the calibration plane at the end of the sample cell and in order to obtain the volume of a given sample from the target water content by mass and the determined bulk soil sample density d b volumetric water content of each sample was calculated according to the formula 2 θ w 1 w d b d w where d w is the density of water in the present study density of the samples varied from 1 37 to 2 31 g cm 3 with the mean value of about 1 82 g cm 3 all measurements were done at room temperature 21 1 5 c due to experimental protocol errors four samples from the set of 200 measured samples in total were excluded from further analysis 2 3 processing of the measured spectra the obtained dielectric spectra were modeled with the use of a three pole debye model with a direct current bulk electrical conductivity term σ b 3 ε ε i 1 3 δ ε i 1 j ω τ i j σ b ω ε 0 where ε is the high frequency limit dielectric permittivity ω 2 π f with f as the electric field frequency δ ε i and τ i stand for the relaxation amplitude and time of an i th debye pole respectively and ε 0 8 85 10 12 f m 1 is the value of vacuum permittivity the fitted functions effectively smoothed the spectra in order to diminish the impact of measurement errors on the subsequent analysis the number of poles i e three was chosen in order to accurately model the spectra while avoiding excessive number of parameters to fit among all of the analyzed spectra the mean value of rmse of fitting of eq 3 was 0 08 with the minimum and maximum values of 0 02 and 0 23 respectively the obtained functions were then used to obtain the real part of dielectric permittivity at frequencies from 20 mhz to 3 ghz with 10 mhz resolution for each measured soil sample next the θ ε calibration curves were fitted in the form of a linear function of the square root of dielectric permittivity as given by the following equation 4 θ a ε b for each soil separately and for all soils combined at each examined frequency an equation of the above form is frequently used to describe the relation between soil volumetric water content and dielectric permittivity vaz et al 2013 skierucha and wilczek 2010 delta t devices ltd 2017 the rmse was calculated for each fitted curve according to the eq 5 the values of θ i represent the experimental data and the values obtained from the fitted model are represented by θ mi n is the total number of observations of θ i the sum of the mean squared difference between the experimental data and the modeled values was divided by the number of the degrees of freedom equal to the number of observations of a given soil minus the number of fitted parameters which was 2 for the linear function 5 rmse i 1 n θ i θ mi 2 n 2 in order to compare the obtained calibration curves at specific frequencies to the calibration curves that could be obtained in the time domain apparent dielectric permittivity was simulated for each measured dielectric spectrum according to a procedure described in the appendix for the data simulated in the time domain eq 4 was also used in order to determine calibration functions but with ε a substituting ε all numerical operations on the measured spectra were performed in matlab software mathworks inc usa with the exception of the determination of apparent permittivity in the time domain performed in advanced design system ads software keysight technologies formerly agilent usa described in the appendix 3 results and discussion 3 1 complex dielectric permittivity spectra sample spectra of ε of soil no 613 moistened with kcl solutions of σ s 0 52 and 1 5 s m 1 are presented in fig 2 eq 3 was fitted to the spectra and the resultant functions are shown in the plots bulk electrical conductivity of the samples depended on moistening solution and water content the ranges of σ b in dsm 1 for each tested soil and each solution are presented in table 3 at all frequencies water content was the major influence on complex dielectric permittivity at frequencies below 0 2 0 5 mhz a rise in ε with a decrease in frequency is noticeable as seen in fig 2 a b and fig 3 this can be attributed to the impact of low frequency dielectric dispersion mechanisms associated with interfacial phenomena such behavior of ε agrees with the literature data kelleners et al 2005 chen and or 2006 wagner et al 2011 lauer et al 2012 fig 3 presents spectra of the real part of dielectric permittivity of four samples of the same soil of one water content level but moistened with various solutions the rise in ε at low frequencies depends on salinity i e the more saline solution was used and thus the higher σ b was the higher ε was observed the relations between σ b and ε in the 20 mhz 3 ghz frequency range have been examined in szypłowska et al 2019 in the aspect of extraction of soil solution electrical conductivity with the use of the salinity index model the measured spectra were used to simulate the apparent dielectric permittivity that can be measured using time domain techniques the waveforms of samples of soil no 613 moistened with the kcl solution of σ s 0 52 s m 1 whose dielectric spectra are presented in fig 2 a and b are shown in fig 4 with the increase in water content the round trip propagation time of the simulated pulse increased as expected 3 2 relations between moisture and permittivity in fig 5 volumetric water content was plotted as a function of the real part of dielectric permittivity at several chosen frequencies 20 50 100 and 450 mhz and 1 and 3 ghz for all measured samples prepared with all used moistening solutions with σ b ranges as presented in table 3 these specific frequencies were chosen in order to illustrate the frequency behavior of the θ ε relations in the plots all measurement points from all soils are shown but only four fitted functions for selected soils are included for clarity also some reference calibration curves from the literature including the functions from three widely used and frequency specific water content sensors were shown for clarity 1 wet2 sensor calibrations for mineral and clay soils at 20 mhz vaz et al 2013 6 θ 0 099 ε 0 178 mineral θ 0 091 ε 0 182 clay 2 hydra probe hp calibration function for mineral and loam soils at 50 mhz vaz et al 2013 7 θ 0 109 ε 0 179 3 ml3 thetaprobe calibration function for mineral soils at 100 mhz delta t devices ltd 2017 8 θ 0 119 ε 0 190 4 calibration function for an fdr sensor in the frequency range 390 480 mhz skierucha and wilczek 2010 9 θ 0 113 ε 0 167 5 topp s equation obtained for tdr measurements topp et al 1980 10 θ 5 3 10 2 2 92 10 2 ε a 5 5 10 4 ε a 2 4 3 10 6 ε a 3 the spread in the measured points stemming from the influence of salinity and texture on ε depends on the measurement frequency which is especially pronounced at 20 and 50 mhz the grt soil with the highest clay fraction among the tested soils causes the greatest enhancement of the real permittivity as a function of water content for the lower frequencies with an increase in frequency differences among the tested soils become smaller at 450 mhz and above differences among data points for the various soils and salinity levels diminish for the range of water contents examined the fitted parameters of eq 4 their standard errors as well as r 2 and rmse obtained at six selected frequencies for each soil and for all soils combined are presented in table 4 in order to assess the goodness of fit between measured values and eq 4 in a quantitative way the rmse was computed for each soil and for all soils combined and at all examined frequencies from 20 mhz to 3 ghz with 10 mhz interval the frequency dependence of the rmse is presented in fig 6 where for each case the maximum value of rmse was obtained at 20 mhz for each tested soil the rmse decreased with an increase in frequency especially at frequencies below 200 500 mhz depending on a given soil the largest difference between the maximum and minimum values of the rmse among the soils was observed for soil no 617 also the smallest minimal value of the rmse of 0 00302 was observed for soil no 617 at frequency 940 mhz for some soils the rmse started to rise slightly with frequency above 2 ghz though this effect did not impact the calibration for all soils combined for all soils combined the smallest rmse rmse min 0 0098 was observed at 3 ghz for frequencies above about 500 mhz the rmse changes little and is around 0 01 a quantitative cut off condition for acceptable values of the rmse could also be taken as 2 rmse min which corresponds to the case of twice the variance than for the cases with rmse min 1 1 a similar condition is used in the filter theory to determine the 3 db cut off frequency at this frequency the filter amplitude response drops by 2 with respect to the asymptotic value here for all soils combined 2 rmse min 0 0139 the values of the rmse smaller than 2 rmse min were achieved at the frequency of 250 mhz and above the values of rmse min and 2 rmse min are depicted in fig 6 the obtained results indicate that at frequencies above several hundred mhz the influence of soil salinity and texture is minimized with respect to the lower frequency range with the smallest rmse obtained at the highest analyzed frequency the results also indicate the need of performing soil specific calibration especially at lower frequencies szypłowska et al 2018 demonstrated that the frequency range 150 500 mhz was optimal for the moisture calibration functions for three tested soils the results obtained for 10 more soils in the present study did not exhibit any clear minimum pointing to frequencies of several hundred mhz and higher as the most optimal instead the results obtained in the present study with the use of apparent permittivity determined from simulated tdr waveforms are presented in fig 7 and table 5 the obtained relations for each soil as well as for all soils combined are generally similar to relations obtained for frequency domain data at 1 ghz also in terms of rmse generally the obtained data and corresponding soil moisture dielectric permittivity calibration functions are slightly shifted towards higher permittivity values in comparison to reference calibration curves at all frequencies and also in the case of ε a this would result in overestimation of volumetric water content while using factory calibrations this issue has also been reported in other studies for several commercial soil moisture sensors singh et al 2018 4 conclusions in the case of the measured soils the impact of soil salinity on the θ ε relation was the smallest at frequencies of several hundred mhz and above thus the most accurate soil moisture determination based on ε measurement with the use of a general calibration function is possible at frequencies above at least 250 mhz at low frequencies the impact of soil texture is visible even though the tested soils are of medium texture and do not include soils of high clay content for the most accurate results and especially at low frequencies soil specific calibration is advised the obtained rmse in this optimal frequency range is comparable to the rmse of the calibration function obtained with the use of apparent permittivity calculated in the time domain it is important also to note that soil in the field differs from the laboratory prepared material in terms of density heterogeneity the presence of plant roots soil organisms stones etc therefore the relations provided in the paper should not be used directly for any device meant for field measurements without appropriate verification further research will explore the applicability of the developed relations for field measurements declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding the project is financed by the polish national agency for academic exchange soil complex dielectric permittivity spectra used for the analysis were obtained in the scope of the project no 2014 15 d st10 04000 financed by the polish national science centre ncn appendix a time domain analysis for a tem transmission line filled with a dielectric with permittivity ε one can write the scattering parameters as baker jarvis 1990 baker jarvis et al 2005 11 s 11 s 22 γ 1 z 2 1 γ 2 z 2 12 s 21 s 12 z 1 γ 2 1 γ 2 z 2 where 13 γ 1 ε 1 ε 14 z e j 2 π f c ε l and f is the frequency c is the speed of light in vacuum and l is the length of the transmission line here arbitrarily taken as 10 cm assuming that the line is terminated with an open circuit one can easily show that the input reflection coefficient γ in will be given by 15 γ in z 2 γ 1 γ z 2 this value was used to describe the sample in ads transient state simulations see fig 8 in these simulation a pulse with 150ps rise and fall time and 10ps width was sent into a load with reflection coefficient given by 15 and the delay of the reflected pulse was determined and used as t p a screenshot of the simulation circuit is shown in fig 8 while some simulation results are depicted in fig 4 from the obtained t p values for each measured spectrum apparent permittivity was directly calculated with the use of the following formula 16 ε a ct p 2 l 2 
6036,effects of spatial and temporal resolutions chen yang a b fei zheng c yuanyuan liu d you kuan zhang a b wei liu e qiang zhang a b xiaofan yang f a guangdong provincial key laboratory of soil and groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china guangdong provincial key laboratory of soil and groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china guangdong provincial key laboratory of soil and groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china b state environmental protection key laboratory of integrated surface water groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china state environmental protection key laboratory of integrated surface water groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china state environmental protection key laboratory of integrated surface water groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china c mcc water environment technology research institute mcc huatian engineering technology corporation nanjing 210019 china mcc water environment technology research institute mcc huatian engineering technology corporation nanjing 210019 china mcc water environment technology research institute mcc huatian engineering technology corporation nanjing 210019 china d school of earth sciences and engineering nanjing university nanjing 210023 china school of earth sciences and engineering nanjing university nanjing 210023 china school of earth sciences and engineering nanjing university nanjing 210023 china e institute of geological survey china university of geosciences wuhan 430074 china institute of geological survey china university of geosciences wuhan 430074 china institute of geological survey china university of geosciences wuhan 430074 china f state key laboratory of earth surface processes and resource ecology faculty of geographical science beijing normal university beijing 100875 china state key laboratory of earth surface processes and resource ecology faculty of geographical science beijing normal university beijing 100875 china state key laboratory of earth surface processes and resource ecology faculty of geographical science beijing normal university beijing 100875 china corresponding author this manuscript was handled by huaming guo editor in chief effects of spatial and temporal resolutions sr and tr on modeling hydro biogeochemical transformation of chromium cr are important in simulating reactive transport processes the current study was conducted in the hyporheic zone hz at the hanford site of the u s department of energy which has been known for its highly heterogeneous sediments and transient hydrodynamics distributions of hydraulic conductivity and sediment associated fe concentration were averaged at a group of srs while measured hourly water levels were further moving averaged at daily and monthly trs fe concentration is selected for assembling geochemical heterogeneity due to its important role in redox transformation of cr at the site three fe distributions with fe concentrated on small medium and large sediment grains respectively were also considered a series of flow and reactive transport simulations configured with different combinations of srs trs and fe distributions were conducted simulated results revealed that cr vi discharged to river is underestimated if sr decreases for both the hydrodynamics and the discharge of cr vi to river difference caused by sr can be amplified with the decrease of tr biogeochemical transformation of cr is more dependent on sr while hydrodynamics is on tr the stronger control of sr than tr on biogeochemical transformation of cr is resulted from more sensitive increase of fe ii with decreasing sr than with decreasing tr effect of sr is highly sensitive to variations of sr with fe on medium size grains while is persistent to a much smaller sr with fe on small size grains results from the current study are expected to benefit modelers on the selections of the spatial temporal resolutions for in house modeling and field sampling which may also have implications for upscaling keywords discretization scale chromium reactive transport modeling hydro biogeochemical processes hyporheic zone 1 introduction physical and geochemical heterogeneities in the subsurface such as spatial variations of the hydraulic conductivities chemical species and concentrations significantly regulate the reactive transport of contaminants li et al 2006 liu et al 2014 perujo et al 2017 salamon et al 2007 wen and li 2017 for instance the reaction rates which are largely dependent on properties of sediments liu et al 2014 li et al 2010 in relatively homogeneous batch reactors can be several orders of magnitude larger than those in highly heterogeneous natural systems li et al 2008 salehikhoo et al 2013 white and brantley 2003 moreover with the rapid development of hydraulic facilities e g dams and regulations temporal variabilities of regional hydrodynamics become more complex which further affect the migration and the transformation of contaminants in the riverine system shuai et al 2019 zachara et al 2016 as such the spatial and temporal resolutions scales in discretization used in the reactive transport modeling are critical because small scale heterogeneities i e smaller than the discretization scale may have huge impact on macroscopic system behaviors nevertheless are unavoidably neglected li et al 2006 selected studies have focused on the effect of model resolution on the simulation results vieux and needham 1993 indicated that the sediment yield increased as much as 32 due to the shortened stream lengths caused by large cell size used in the agriculture nonpoint source pollution model molnar and julien 2000 tested a group of grid cell sizes varying from 127 m to 914 m using the casc2d rainfall runoff model and suggested that a proper application of a coarser grid under certain circumstances requires subsequent compensation such as using more sophisticated descriptions of the parameters in the model yu et al 2014 examined the results of a hydrological model using different spatial 15 200 m and temporal 5 60 min resolutions and found that the spatio temporal resolution significantly affected the hydrological response of the watershed because the resolutions were correlated to the elevation and precipitation which were important in the simulations stenta et al 2017 pointed out that in order to obtain the same hydrological response when the grid cell size was increased other parameters in the hydrological model needed to be re scaled unfortunately these studies are limited in hydrological modeling without reactive transport to our best knowledge the combined effects of spatial and temporal resolutions on modeling biogeochemical transformation of the redox sensitive chemicals were rarely studied in addition due to the excessive computational and fieldwork costs in reactive transport modeling li et al 2010 2011 the conduction of such a study is also expected to help modelers to choose the most appropriate resolution in their simulations to balance the model precision and the computational cost last but not the least although previous studies have tried to establish upscaling laws of the reaction rates to bridge the gaps between scales wen and li 2017 either theoretical or numerical upscaling coarse graining is difficult to generalize for modeling in practice an intuitive knowledge of the effects of spatio temporal resolutions on reactive transport modeling may provide implications for such efforts on upscaling in this study we investigated the role of spatial and temporal resolutions sr and tr in reactive transport modeling based on a representative hyporheic zone hz located in the 100 bc area at the hanford site of the u s department of energy us doe the subsurface environment of the hanford 100 bc area is composed of heterogeneous aquifers with transient hydrodynamics due to the poorly sorted sediments hammond et al 2011 and the upstream dam operations yabusaki et al 2008 chromium cr contamination is serious at the site due to the release of radioactive material williams et al 2008 and the hz is the last barrier determining the fate of cr discharged to river as cr vi or retarded in the hz as cr iii yang et al 2018 a reaction network for biogeochemical transformation of cr liu et al 2017 yang et al 2018 specially established for cr retardation in the hz at the hanford site was selected for reactive transport modeling in the reaction network cr vi can be reduced to immobilized cr iii in the hz by sediment associated fe ii while the dissolved oxygen do intrusion from the river inhibits the immobilization of cr vi distribution of sediment associated fe ii is largely dependent on the heterogeneities of subsurface materials li et al 2011 while do intrusion is mainly controlled by hydrodynamics yang et al 2018 in addition previous study yang et al 2018 on the effect of tr promoted the further exploration on sr and combined tr and sr in the following sections the study area is briefly introduced in section 2 details of the reactive transport modeling of cr are shown in section 3 recent numerical models in yang et al 2018 were adopted in this study with additional configurations as follows spatial distributions of the physical and geochemical parameters were obtained with a stochastic approach based on measurements in previous studies e g williams et al 2008 then the stochastically generated or calculated spatial parameters and the measured water levels were successively upscaled and moving averaged from baseline scales to larger scales respectively a group of scenarios with different srs and trs were simulated using a community groundwater solver for water flow coupled with an in house simulator for reactive transport in section 4 effects of trs and srs on modeling cr transport and transformation in the hz are discussed based on the simulation results from section 3 finally conclusions are summarized in section 5 2 study area the study site is located at the hanford 100 bc area in the southeastern washington state of the u s detailed descriptions of this site were given in yang et al 2018 and here only a brief introduction is summarized a two dimensional 2d transect is selected as the modeling domain indicated by the yellow dash line in fig 1 cr vi contamination is critical in the hanford 100 bc area which has potential environmental risks due to the discharge of cr vi to the nearby columbia river and its surrounding ecosystems truex et al 2015 particularly river water discharged from upstream dam operations induces extra diurnal and weekly fluctuations of the river stage yabusaki et al 2008 thus the enhanced exchange between river water rw and groundwater gw in the hz could cause additional environmental risks yang et al 2018 the selected transect starts at well 199 b2 14 and ends at the center of the river with a height of 45 m and a length of 314 m fig 2 sediments in the transect are categorized as the hanford formation with hydraulic conductivities over 2000 m d and ringold e formation with typical values of 40 120 m d ma et al 2010 specifically a 0 3 m thick alluvium layer with low permeability 4 m d along the river bank is found to be an active biogeochemical zone hot spots where sediments have the redox potential to reduce cr vi to cr iii herzog et al 2015 liu et al 2017 moser et al 2003 williams et al 2008 3 models setup 3 1 stochastic simulation and analysis of the soil properties stochastic simulation of heterogeneities is prerequisite in flow and reactive transport modeling in this study for physical heterogeneity hydraulic conductivity k was selected li et al 2010 salamon et al 2007 zhang 2002 for geochemical heterogeneity concentration of sediment associated fe cfe was selected due to its important role in the immobilization of cr vi in the hz liu et al 2017 yang et al 2018 distributions of both k and cfe are closely related to grain size distribution of the sediments for example sediments composed of larger size grains usually have higher permeability and less cfe than smaller size grains li et al 2010 hence in this study the grain size distribution was determined based on the stochastically generated gamma log data gg borehole geophysical logging data and then the k and cfe were calculated based on grain size distribution using their corresponding relationships 3 1 1 gamma log data geo statistical analysis has not been performed documented in hanford 100 bc area so the stochastic distribution of gg in the hanford 300 area was utilized because both sites have similar geological properties shuai et al 2019 williams et al 2008 the variogram of gg is described by a spherical model with three nested structures as follows williams et al 2008 1 γ h c 0 i 1 3 c i 1 5 h a i 0 5 h a i 3 where γ is the variogram value h is the lag separation distance c 0 is the nugget and ci and ai are the sills and actual ranges respectively values of these parameters are listed in table 1 williams et al 2008 the gg field was generated in the modeling domain using the sgsim program sequential gaussian simulation program in the geostatistical software library gslib deutsch and journel 1998 generated gg was further scaled to the mean and the standard deviation of the ringold formation which are 215 832 and of 33 418 respectively williams et al 2008 3 1 2 grain size and saturated hydraulic conductivity a correlation between the grain size and the gg was established as eq 2 williams et al 2008 2 d g p 2 p 1 p 2 1 p 3 g g p 4 p 5 where dg mm is the geometric mean diameter as the representative grain size metrics and p 1 p 5 are constants as 28 0 00001 0 006 20 9152 and 0 9522 thus dg was calculated based on the gg generated in section 3 1 1 the 20 60 and 77 quantiles of the cumulative distribution function for the calculated diameters are 0 08 mm 2 3566 mm and 8 2494 mm respectively which are comparable to the results of grain size analysis at the hanford site in liu et al 2017 saturated hydraulic conductivity ks of the sediment was estimated using the kozeny carmen equation bear 1972 williams et al 2008 as follows 3 k s ρ w g μ n 3 1 n 2 d g 2 180 where ρ w and μ are the density and viscosity of the water respectively g is the gravitational constant and n is the porosity of the ringold formation 3 1 3 fe concentration cfe was calculated based on the grain size which is considered as an improvement from previous studies that the chemical content was determined by a negative correlation with k li et al 2010 2011 three fe distributions were considered in this study to cover a wide range of grain size which are listed in table 2 1 distribution 1 fe was concentrated on small size grains which is commonly found in previous studies brook and moore 1988 lin et al 2003 martincic et al 1990 okweye et al 2016 salomons and förstner 1984 yao et al 2015 thus a 100 measured concentration mc liu et al 2017 was set for dg 0 08 mm and 1 mc for dg 0 08 mm 2 distribution 2 fe was concentrated on medium size grains shang et al 2011 which is a common distribution of redox sensitive uranium at the hanford site thus 100 mc was set for 0 08 mm dg 2 36 mm and 1 mc for dg 0 08 mm and dg 2 36 mm 3 distribution 3 fe was concentrated on small and large size grains i e two sides since previous studies reported similar or even higher heavy metal concentrations on coarser grains brook and moore 1988 lin et al 2003 singh et al 1999 tessier et al 1982 tsai et al 2003 thus 100 mc was set for dg 0 08 mm and dg 2 36 mm and 1 mc for 0 08 mm dg 2 36 mm there was no fe distributed on grains with size larger than 8 25 mm since only a 8 mm size fraction was sieve collected in liu et al 2017 3 2 spatial and temporal resolutions gamma log data and grain size were generated and calculated respectively on a grid with 5 cm 5 cm resolution then the ks and cfe calculated on 5 cm 5 cm grid cells were up scaled to grid cells of 10 cm 10 cm 50 cm 50 cm and 200 cm horizontal 100 cm vertical respectively they are referred to as 10 50 and 200 cm grid cells in the rest of the paper representing srs from the laboratory to the field ks were up scaled using the geometric mean wen and gomezhernandez 1996 while cfe were up scaled using the volume averaging williams et al 2008 4 c fec j 1 n c fef j v f j j 1 n v f j where v is the volume of a fine grid cell the subscripts c f and j refer to the coarse grid cell 10 50 or 200 cm the fine grid cell 5 cm and the index of the fine grid cell respectively the summations were taken over all the fine grid cells n belonging to the same coarse grid cell distributions of ks and cfe at different srs are shown in figs 3 and 4 and s1 and s2 in the supporting information ks shown in fig 3 are those adjusted in the flow simulations it should be noted that a 50 cm grid cell was further divided into a group of 10 cm sub grid cells and the parameters applied on these sub grid cells were kept the same to those on that 50 cm grid cell fig s3 such treatment was also done for the grid of 200 cm resolution therefore all simulations were actually conducted on the same grid of 10 cm resolution as such difference induced by sr could be retained while additional numerical deviations could be avoided for all three srs the model domain was divided into 207 480 grid cells with 62 218 inactive cells to accommodate the irregular river boundary hereinafter increasing sr means the increase of spatial resolution i e the increase of parameter resolution and vice versa time series of hourly water levels in well 199 b2 14 and at the river gauge were moving averaged at daily 24 h and monthly 720 h temporal scales fig 5 these trs represent scales from the level dominated by human activities to the level controlled mainly by natural regulations similarly a hypothetical time scale was used meaning that the hourly time step was actually used in all scenarios hereinafter increasing tr means the increase of temporal resolution i e the increase of temporal variability and vice versa therefore a total of 9 velocity fields 10 cm hourly 10 cm daily 10 cm monthly 50 cm hourly 50 cm daily 50 cm monthly 200 cm hourly 200 cm daily and 200 cm monthly and 27 reactive transport fields each velocity field was used to calculate reactive transport with three cfe distributions were simulated in this study 3 3 flow and reactive transport modeling 3 3 1 modeling flow a massively parallel subsurface flow and reactive transport model pflotran hammond et al 2014 was used to simulate variably saturated flow in the modeling domain the water levels in well 199 b2 14 and at the river gauge were used as the specified heads on the left side and on the right slope under the river stage respectively the river bank above the river stage was treated as the seepage boundary the top and bottom boundaries were no flow due to the low precipitation and the impermeable underlying sediments respectively the center of the river at the right boundary was assumed to be no flow as well the characteristic curve of the soil moisture was based on the brooks corey function and the related parameters are listed in table s1 in each scenario steady state flow field based on annual means of the well level and the river stage was first calculated as the initial condition for the corresponding transient flow field after spin up of the transient flow fields to reach quasi steady state one more year was simulated the results of which were repeatedly used for 10 years of simulation in the reactive transport 3 3 2 reaction network of cr in the hz the biogeochemical reaction network in this study was first established by liu et al 2017 and further improved by yang et al 2018 details table s2 of the reaction network included 1 multi rate cr vi sorption and reduction by sediment associated fe ii not in aqueous phase with the corresponding fe ii oxidation to fe iii 2 microbially mediated regeneration of fe ii with dissolved organic carbon doc as the electron donor and carbon source 3 abiotic oxidation of fe ii by do that competes with cr vi 4 do consumption by microorganisms as the electron acceptor with doc as the electron donor and carbon source and 5 doc production from the particulate organic carbon poc transformation the reactive fe ii for cr reduction was assumed to be only present in the alluvium so that in other regions cr was only subject to sorption shown in the blue and purple areas in fig 2 specific parameters of the reactions are provided in table s3 3 3 3 modeling reactive transport reactive transport simulations of 10 years for cr vi with other relevant chemical species i e organic carbon fe ii fe iii and do in the alluvium and cr vi sorption to sediments outside the alluvium were performed based on the flow fields obtained a priori initial conditions based on measurements liu et al 2017 for these reactions are provided in table s4 cr vi with a concentration of 2000 μg l dresel et al 2008 johnson 2016 liu et al 2017 smoot et al 2011 truex et al 2015 yang et al 2018 was injected as a plug at the fifth year shown as the 10 m 5 m red box in fig 2 for all the mobile species the top bottom and right sides of the model domain were assumed to be no flux boundaries and the left side was treated as constant concentration when the water flow was into the model domain and as zero concentration gradient when the water flow was out of the domain species concentrations in the rw were assumed to be constant the reactive transport was solved by an in house developed simulator as an alternative of the reaction sandbox provided in pflotran which has been proved to be feasible for the current study site yang et al 2018 then the cr vi mass dissolved in the inland area adsorbed to the sediments and discharged to river the cr iii mass precipitated in the hz and the do mass transported into the model domain and dissolved in the inland area yang et al 2018 were calculated for the following analysis 4 results and discussion 4 1 physical and chemical distributions at different srs and trs in fig 3 with decreasing sr preferential flow paths are gradually reduced which can be observed by the shrinkage of the red area the reason is that large ks are removed to compensate for extremely low values during upscaling at places where the color becomes warmer hydraulic connections are strengthened due to the replacement of small ks with larger values flow fields in the domain would become smoother with fewer sudden changes of velocity which will be discussed in section 4 2 similar patterns are observed for cfe in fig 4 cfe are more evenly distributed at lower sr since some parts with lower cfe are supplemented in up scaling which could sacrifice the strong redox ability of the nearby places in fig 5 with the moving average from hourly to monthly tr crossovers between gw level and river stage in the middle of a year are gradually filtered out this can cause decrease of the flow reversal frequency in the modeling area yang et al 2018 flow reversal here means rw intrusion to subsurface instead of the general gw discharge to river it has importance on redox transformation of cr in the hz yang et al 2018 this effect might be magnified when superimposed on the spatial scaling effect in this study 4 2 hydrodynamics and cr transformation at different srs fig 6 shows horizontal velocities u at three observation points p1 p3 located from the well side well 199 b2 14 to riverside first of all simulated horizontal velocities are consistent with results from previous studies conducted at the hanford site e g 25 to 25 m d in yabusaki et al 2008 indicating the accuracy of the model in this study in this section the first and second rows are focused since here we only discuss the effect of sr at p1 the water is almost stagnant at 10 cm sr 0 m d and becomes dynamic with decreasing sr fig 6a and d e g over 0 2 m d at 200 cm sr in contrast u at p2 where it is extremely high decreases to about one third 15 m d to 5 m d at peak values from 10 cm to 200 cm sr leading to a more moderate state fig 6b and e this indicates that upscaling tends to result in a more uniform flow field in space hence variations of hydrodynamics with sr discussed here are consistent with the prior judgement in section 4 1 what s more the effect of spatial upscaling is not obvious in the middle of the year when interactions between gw and rw are especially intense e g little difference in three srs at about 4000 h in fig 6a in the first column of fig 7 cr vi mass to river is minimum at 200 cm sr for all three fe distributions with a lower sr fe ii is more averaged to larger size grains where there is no or less fe ii at a higher sr thus the retardation capability of the hz to cr vi becomes stronger and cr vi is more prevented to river this overestimation of retardation capability in the hz means the underestimation of environmental risk about the cr vi discharge to river this phenomenon was drawn in fig 8 by taking a scenario with fe distributed on medium size grains cr vi enter the river through corridors with a small amount of fe ii at 10 cm sr while cr vi are retarded in the hz at 200 cm sr similar phenomena have been reported in previous studies elfeki et al 2012 showed that higher heterogeneity which corresponds to higher sr here might lead to larger variance of the contaminant plume i e more dispersion of the plume zheng and jiao 1998 described the small patches connected to the main body of the plume by a narrow neck and the fringy feature on the iso surface of the plume at the made site where the sediments are extremely heterogenous these phenomena are mainly caused by the preferential flow paths due to high heterogeneity elfeki et al 2012 zheng and jiao 1998 in summary with higher sr cr vi is more easily discharged to river along paths with larger grain sizes where k is larger and cfe is lower when fe is on small size grains cr vi enters the river at a maximum value and cr iii precipitated in the hz is the minimum regardless of the sr this can be observed by that the grey line group is always the highest in the first column while the lowest in the second column in fig 7 this indicates that the highest risk to river occurs when fe is on small size grains when fe is on small and large size grains higher cr iii mass is observed fig 7j meaning more retarded cr vi in the hz since large size grains are always the preferential flow paths for cr vi this is consistent with li et al 2011 in which the largest amount of u iv was generated at the edge of the preferential flow paths interfacing with fe iii rich areas dissolved and adsorbed cr vi in the inland area is independent of fe distribution and is of slight difference at different srs the third and fourth columns in fig 7 this difference is due to the hydrodynamic discrepancies induced by different physical heterogeneities at different srs moderate hydrodynamics at a lower sr result in more dissolved and adsorbed cr vi in the inland area this is shown by the fact that red lines are always higher than blue and black lines with a maximum discrepancy of 1 g this also indicates that the effect of geochemical heterogeneity is more important than that of the physical heterogeneity which was rarely discussed in previous studies for two sides fe distribution cr vi mass to river is not sensitive to sr fig 7i for medium size distribution it is the most sensitive case to sr due to the large discrepancy among three srs hence with the same decrease of sr retardation capability of the hz is the most overestimated when fe is on the medium size grains and thus the environmental risk to river is the most underestimated with medium size distribution effect of upscaling lasts to about 200 cm sr since cr vi discharge to river is totally prevented at this sr fig 7e this means that a maximum underestimation of environmental risk to river has been achieved at 200 cm sr and the underestimation would not increase any more with the continued decrease of sr for small size distribution fig 7a cr vi mass to river is less sensitive to sr than that with medium size distribution however upscaling has the most persistent effect on cr vi mass to river since there is still a large amount of cr vi discharged to river at 200 cm sr effect of upscaling should disappear at a sr lower than 200 cm at which cr discharge to river becomes almost zero as that in fig 7e as such the selection of sr deserves particular attention when environmental risk assessment is conducted at places where redox sensitive chemicals are distributed on small size grains due to the persistent effect of upscaling attention should also be paid when fe is distributed on medium size grains which might be the dominating distribution pattern of fe at the hanford site 4 3 hydrodynamics and cr transformation at different combinations of sr and tr with decreasing tr fluctuations of u at higher tr are smoothed out and hydrodynamics become gentle more importantly both frequency and magnitude of the reversed flow decrease with decreasing tr which can be observed for instance by comparing fig 6f with l difference in hydrodynamics of different srs become more obvious with decreasing tr in general for hydrodynamics either the discrepancy induced by tr is much more obvious than that induced by sr or hydrodynamics are more controlled by tr thus at places with intense anthropogenic activities inducing extra water flow fluctuations at small time scales it is necessary to use higher tr to capture such hydrodynamics while the discrepancy induced by srs is insignificant under such transient conditions with lower tr more cr iii precipitated in the hz the second column in fig 9 regardless of the sr this means an overestimated retardation capability of the hz therefore the risks seem to be further underestimated when compared to that caused by a lower sr for example from 10 cm hourly to 200 cm hourly then to 200 cm monthly fig 9j cr iii precipitation continuously increases however cr vi to river the first column in fig 9 also increase with decreasing tr which in fact neutralize a small portion of the underestimated risks from the individual decrease of sr e g from 10 cm hourly to 50 cm hourly then to 50 cm monthly fig 9i the higher the sr the more the neutralization at lower tr taking fig 9i for example difference between black line 10 cm sr and corresponding grey line is larger than that for red line 200 cm sr then increase of both cr vi discharge and cr iii precipitation are balanced by decrease of dissolved and adsorbed cr vi in the inland area at lower tr this can be observed by comparing grey line group with other line groups in the third and fourth columns of fig 9 decrease of dissolved and adsorbed cr vi are observed at lower tr which is consistent with yang et al 2018 in addition difference of cr vi mass to river among different srs are increased with decreasing tr fig 9i this can be observed by comparing the grey line group with other line groups in the first column of fig 9 in contrast to hydrodynamics redox transformation of cr is more dependent on the sr than the tr e g 10 cm hourly 200 cm hourly vs 10 cm hourly 10 cm monthly 4 4 mechanisms for effects of sr and combined sr and tr on cr transformation in fig 10 a and 10e do concentration has insignificant dependence on sr at p4 this is shown as the slight increase of do with decreasing sr however the difference of do among different srs is much more obvious at the regional scale i e the whole modeling domain in fig 11 and its content is the highest at 200 cm sr the concentration of organic carbon oc is generally lower at 200 cm sr fig 10b and f because it is more consumed by biological activities at the corresponding higher do level in contrast fe ii is higher at a lower sr triple concentration at 200 cm sr to that at 50 cm sr fig 10h and that is why more cr iii is generated at 200 cm sr fig 9b f and j therefore more oc particularly the poc is consumed to maintain this relatively high fe ii level it is clearly shown in fig 10g that poc concentration is about 10 μmol g lower at 200 cm sr than that at 10 cm sr in fig 10a flow reversals at daily and monthly trs carry less do from rw to gw which can be observed due to smaller peaks 80 μmol l relative to those at hourly tr 300 μmol l thus less oc is consumed as shown in the second and third columns in fig 10 therefore more fe ii is left and more cr iii is precipitated in the hz at lower trs which is the explanation for the results in fig 9 however fe ii content increases moderately with the decrease of tr when compared to that with the decrease of sr in fig 10h concentration of fe ii increase from 0 to 0 01 then to 0 04 μmol g with decreasing sr while from 0 01 to 0 02 μmol g from hourly to daily and monthly therefore the redox transformation of fe ii and thus cr is more sensitive to sr than tr besides concentrations of oc fig 10b f c and g decrease with time as the gradual intrusion of do from rw to gw when a large rw intrusion occurs in the middle and at the end of the year do has obvious concentration peaks fig 10e while oc has clear valleys fig 10f and g fe ii also gradually decreases with time due to the do intrusion the above findings are also consistent with those in yang et al 2018 5 conclusions in this study taking the redox sensitive cr as an example numerical simulations were conducted to illustrate effect of spatio temporal resolution on modeling biogeochemical transformation of contaminants in a hyporheic zone hz at the us doe hanford site the srs are 10 50 and 200 cm representing scales from the laboratory to the field while trs are hourly daily and monthly representing scales from the level dominated by human activities to that controlled mainly by natural regulations generated stochastic physical and geochemical fields and measured water levels are successively upscaled to larger spatial and temporal scales simulation results are of particular importance for help choosing the proper srs and trs when conducting monitoring and or modeling of reactive contaminants and for improving accuracy of environmental risk assessment on contaminants discharge to river or other water bodies results also have general implications for riverine areas that have been polluted by other redox sensitive pollutants such as as or sb which have high mobility at a low valence state and low mobility at a high valence state several important conclusions of this study can be drawn as follows 1 lower srs and trs tend to generate more uniform hydrodynamics hydrodynamics are more controlled by tr and decrease of tr could amplify the effect of sr thus at places where the human activities are intensive a higher tr should be preferentially considered 2 environmental risk of cr vi to river is underestimated with decreasing sr decrease of tr could neutralize a small portion of this underestimation difference of cr vi mass to river among different srs are amplified with the decrease of tr redox transformation of the cr is more dependent on sr than tr 3 with the same decrease of sr risk is underestimated the most for medium size distribution for small size distribution upscaling has the most persistent effect on cr vi mass to river since there is still a large amount of cr vi discharged to river at 200 cm sr in this study 4 do content is higher at lower sr and more dependent on sr at the regional scale than at the local scale such as at p4 more sensitive increase of fe ii with decreasing sr than with decreasing tr results in the stronger control of sr than tr on generation of cr iii 5 in general for reactive transport the effect of geochemical heterogeneity is more important than that of physical heterogeneity which in turn is also more critical than that of the temporal variability of the hydrodynamics geochemical heterogeneity could amplify the risk caused by physical heterogeneity declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the national natural science foundation of china china grant no 41807198 41877183 and by the guangdong provincial key laboratory of soil and groundwater pollution control china grant no 2017b030301012 we thank the tremendous help from dr glenn hammond and other developers for using pflotran in the hydrodynamics simulations appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124152 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6036,effects of spatial and temporal resolutions chen yang a b fei zheng c yuanyuan liu d you kuan zhang a b wei liu e qiang zhang a b xiaofan yang f a guangdong provincial key laboratory of soil and groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china guangdong provincial key laboratory of soil and groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china guangdong provincial key laboratory of soil and groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china b state environmental protection key laboratory of integrated surface water groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china state environmental protection key laboratory of integrated surface water groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china state environmental protection key laboratory of integrated surface water groundwater pollution control school of environmental science and engineering southern university of science and technology shenzhen 518055 china c mcc water environment technology research institute mcc huatian engineering technology corporation nanjing 210019 china mcc water environment technology research institute mcc huatian engineering technology corporation nanjing 210019 china mcc water environment technology research institute mcc huatian engineering technology corporation nanjing 210019 china d school of earth sciences and engineering nanjing university nanjing 210023 china school of earth sciences and engineering nanjing university nanjing 210023 china school of earth sciences and engineering nanjing university nanjing 210023 china e institute of geological survey china university of geosciences wuhan 430074 china institute of geological survey china university of geosciences wuhan 430074 china institute of geological survey china university of geosciences wuhan 430074 china f state key laboratory of earth surface processes and resource ecology faculty of geographical science beijing normal university beijing 100875 china state key laboratory of earth surface processes and resource ecology faculty of geographical science beijing normal university beijing 100875 china state key laboratory of earth surface processes and resource ecology faculty of geographical science beijing normal university beijing 100875 china corresponding author this manuscript was handled by huaming guo editor in chief effects of spatial and temporal resolutions sr and tr on modeling hydro biogeochemical transformation of chromium cr are important in simulating reactive transport processes the current study was conducted in the hyporheic zone hz at the hanford site of the u s department of energy which has been known for its highly heterogeneous sediments and transient hydrodynamics distributions of hydraulic conductivity and sediment associated fe concentration were averaged at a group of srs while measured hourly water levels were further moving averaged at daily and monthly trs fe concentration is selected for assembling geochemical heterogeneity due to its important role in redox transformation of cr at the site three fe distributions with fe concentrated on small medium and large sediment grains respectively were also considered a series of flow and reactive transport simulations configured with different combinations of srs trs and fe distributions were conducted simulated results revealed that cr vi discharged to river is underestimated if sr decreases for both the hydrodynamics and the discharge of cr vi to river difference caused by sr can be amplified with the decrease of tr biogeochemical transformation of cr is more dependent on sr while hydrodynamics is on tr the stronger control of sr than tr on biogeochemical transformation of cr is resulted from more sensitive increase of fe ii with decreasing sr than with decreasing tr effect of sr is highly sensitive to variations of sr with fe on medium size grains while is persistent to a much smaller sr with fe on small size grains results from the current study are expected to benefit modelers on the selections of the spatial temporal resolutions for in house modeling and field sampling which may also have implications for upscaling keywords discretization scale chromium reactive transport modeling hydro biogeochemical processes hyporheic zone 1 introduction physical and geochemical heterogeneities in the subsurface such as spatial variations of the hydraulic conductivities chemical species and concentrations significantly regulate the reactive transport of contaminants li et al 2006 liu et al 2014 perujo et al 2017 salamon et al 2007 wen and li 2017 for instance the reaction rates which are largely dependent on properties of sediments liu et al 2014 li et al 2010 in relatively homogeneous batch reactors can be several orders of magnitude larger than those in highly heterogeneous natural systems li et al 2008 salehikhoo et al 2013 white and brantley 2003 moreover with the rapid development of hydraulic facilities e g dams and regulations temporal variabilities of regional hydrodynamics become more complex which further affect the migration and the transformation of contaminants in the riverine system shuai et al 2019 zachara et al 2016 as such the spatial and temporal resolutions scales in discretization used in the reactive transport modeling are critical because small scale heterogeneities i e smaller than the discretization scale may have huge impact on macroscopic system behaviors nevertheless are unavoidably neglected li et al 2006 selected studies have focused on the effect of model resolution on the simulation results vieux and needham 1993 indicated that the sediment yield increased as much as 32 due to the shortened stream lengths caused by large cell size used in the agriculture nonpoint source pollution model molnar and julien 2000 tested a group of grid cell sizes varying from 127 m to 914 m using the casc2d rainfall runoff model and suggested that a proper application of a coarser grid under certain circumstances requires subsequent compensation such as using more sophisticated descriptions of the parameters in the model yu et al 2014 examined the results of a hydrological model using different spatial 15 200 m and temporal 5 60 min resolutions and found that the spatio temporal resolution significantly affected the hydrological response of the watershed because the resolutions were correlated to the elevation and precipitation which were important in the simulations stenta et al 2017 pointed out that in order to obtain the same hydrological response when the grid cell size was increased other parameters in the hydrological model needed to be re scaled unfortunately these studies are limited in hydrological modeling without reactive transport to our best knowledge the combined effects of spatial and temporal resolutions on modeling biogeochemical transformation of the redox sensitive chemicals were rarely studied in addition due to the excessive computational and fieldwork costs in reactive transport modeling li et al 2010 2011 the conduction of such a study is also expected to help modelers to choose the most appropriate resolution in their simulations to balance the model precision and the computational cost last but not the least although previous studies have tried to establish upscaling laws of the reaction rates to bridge the gaps between scales wen and li 2017 either theoretical or numerical upscaling coarse graining is difficult to generalize for modeling in practice an intuitive knowledge of the effects of spatio temporal resolutions on reactive transport modeling may provide implications for such efforts on upscaling in this study we investigated the role of spatial and temporal resolutions sr and tr in reactive transport modeling based on a representative hyporheic zone hz located in the 100 bc area at the hanford site of the u s department of energy us doe the subsurface environment of the hanford 100 bc area is composed of heterogeneous aquifers with transient hydrodynamics due to the poorly sorted sediments hammond et al 2011 and the upstream dam operations yabusaki et al 2008 chromium cr contamination is serious at the site due to the release of radioactive material williams et al 2008 and the hz is the last barrier determining the fate of cr discharged to river as cr vi or retarded in the hz as cr iii yang et al 2018 a reaction network for biogeochemical transformation of cr liu et al 2017 yang et al 2018 specially established for cr retardation in the hz at the hanford site was selected for reactive transport modeling in the reaction network cr vi can be reduced to immobilized cr iii in the hz by sediment associated fe ii while the dissolved oxygen do intrusion from the river inhibits the immobilization of cr vi distribution of sediment associated fe ii is largely dependent on the heterogeneities of subsurface materials li et al 2011 while do intrusion is mainly controlled by hydrodynamics yang et al 2018 in addition previous study yang et al 2018 on the effect of tr promoted the further exploration on sr and combined tr and sr in the following sections the study area is briefly introduced in section 2 details of the reactive transport modeling of cr are shown in section 3 recent numerical models in yang et al 2018 were adopted in this study with additional configurations as follows spatial distributions of the physical and geochemical parameters were obtained with a stochastic approach based on measurements in previous studies e g williams et al 2008 then the stochastically generated or calculated spatial parameters and the measured water levels were successively upscaled and moving averaged from baseline scales to larger scales respectively a group of scenarios with different srs and trs were simulated using a community groundwater solver for water flow coupled with an in house simulator for reactive transport in section 4 effects of trs and srs on modeling cr transport and transformation in the hz are discussed based on the simulation results from section 3 finally conclusions are summarized in section 5 2 study area the study site is located at the hanford 100 bc area in the southeastern washington state of the u s detailed descriptions of this site were given in yang et al 2018 and here only a brief introduction is summarized a two dimensional 2d transect is selected as the modeling domain indicated by the yellow dash line in fig 1 cr vi contamination is critical in the hanford 100 bc area which has potential environmental risks due to the discharge of cr vi to the nearby columbia river and its surrounding ecosystems truex et al 2015 particularly river water discharged from upstream dam operations induces extra diurnal and weekly fluctuations of the river stage yabusaki et al 2008 thus the enhanced exchange between river water rw and groundwater gw in the hz could cause additional environmental risks yang et al 2018 the selected transect starts at well 199 b2 14 and ends at the center of the river with a height of 45 m and a length of 314 m fig 2 sediments in the transect are categorized as the hanford formation with hydraulic conductivities over 2000 m d and ringold e formation with typical values of 40 120 m d ma et al 2010 specifically a 0 3 m thick alluvium layer with low permeability 4 m d along the river bank is found to be an active biogeochemical zone hot spots where sediments have the redox potential to reduce cr vi to cr iii herzog et al 2015 liu et al 2017 moser et al 2003 williams et al 2008 3 models setup 3 1 stochastic simulation and analysis of the soil properties stochastic simulation of heterogeneities is prerequisite in flow and reactive transport modeling in this study for physical heterogeneity hydraulic conductivity k was selected li et al 2010 salamon et al 2007 zhang 2002 for geochemical heterogeneity concentration of sediment associated fe cfe was selected due to its important role in the immobilization of cr vi in the hz liu et al 2017 yang et al 2018 distributions of both k and cfe are closely related to grain size distribution of the sediments for example sediments composed of larger size grains usually have higher permeability and less cfe than smaller size grains li et al 2010 hence in this study the grain size distribution was determined based on the stochastically generated gamma log data gg borehole geophysical logging data and then the k and cfe were calculated based on grain size distribution using their corresponding relationships 3 1 1 gamma log data geo statistical analysis has not been performed documented in hanford 100 bc area so the stochastic distribution of gg in the hanford 300 area was utilized because both sites have similar geological properties shuai et al 2019 williams et al 2008 the variogram of gg is described by a spherical model with three nested structures as follows williams et al 2008 1 γ h c 0 i 1 3 c i 1 5 h a i 0 5 h a i 3 where γ is the variogram value h is the lag separation distance c 0 is the nugget and ci and ai are the sills and actual ranges respectively values of these parameters are listed in table 1 williams et al 2008 the gg field was generated in the modeling domain using the sgsim program sequential gaussian simulation program in the geostatistical software library gslib deutsch and journel 1998 generated gg was further scaled to the mean and the standard deviation of the ringold formation which are 215 832 and of 33 418 respectively williams et al 2008 3 1 2 grain size and saturated hydraulic conductivity a correlation between the grain size and the gg was established as eq 2 williams et al 2008 2 d g p 2 p 1 p 2 1 p 3 g g p 4 p 5 where dg mm is the geometric mean diameter as the representative grain size metrics and p 1 p 5 are constants as 28 0 00001 0 006 20 9152 and 0 9522 thus dg was calculated based on the gg generated in section 3 1 1 the 20 60 and 77 quantiles of the cumulative distribution function for the calculated diameters are 0 08 mm 2 3566 mm and 8 2494 mm respectively which are comparable to the results of grain size analysis at the hanford site in liu et al 2017 saturated hydraulic conductivity ks of the sediment was estimated using the kozeny carmen equation bear 1972 williams et al 2008 as follows 3 k s ρ w g μ n 3 1 n 2 d g 2 180 where ρ w and μ are the density and viscosity of the water respectively g is the gravitational constant and n is the porosity of the ringold formation 3 1 3 fe concentration cfe was calculated based on the grain size which is considered as an improvement from previous studies that the chemical content was determined by a negative correlation with k li et al 2010 2011 three fe distributions were considered in this study to cover a wide range of grain size which are listed in table 2 1 distribution 1 fe was concentrated on small size grains which is commonly found in previous studies brook and moore 1988 lin et al 2003 martincic et al 1990 okweye et al 2016 salomons and förstner 1984 yao et al 2015 thus a 100 measured concentration mc liu et al 2017 was set for dg 0 08 mm and 1 mc for dg 0 08 mm 2 distribution 2 fe was concentrated on medium size grains shang et al 2011 which is a common distribution of redox sensitive uranium at the hanford site thus 100 mc was set for 0 08 mm dg 2 36 mm and 1 mc for dg 0 08 mm and dg 2 36 mm 3 distribution 3 fe was concentrated on small and large size grains i e two sides since previous studies reported similar or even higher heavy metal concentrations on coarser grains brook and moore 1988 lin et al 2003 singh et al 1999 tessier et al 1982 tsai et al 2003 thus 100 mc was set for dg 0 08 mm and dg 2 36 mm and 1 mc for 0 08 mm dg 2 36 mm there was no fe distributed on grains with size larger than 8 25 mm since only a 8 mm size fraction was sieve collected in liu et al 2017 3 2 spatial and temporal resolutions gamma log data and grain size were generated and calculated respectively on a grid with 5 cm 5 cm resolution then the ks and cfe calculated on 5 cm 5 cm grid cells were up scaled to grid cells of 10 cm 10 cm 50 cm 50 cm and 200 cm horizontal 100 cm vertical respectively they are referred to as 10 50 and 200 cm grid cells in the rest of the paper representing srs from the laboratory to the field ks were up scaled using the geometric mean wen and gomezhernandez 1996 while cfe were up scaled using the volume averaging williams et al 2008 4 c fec j 1 n c fef j v f j j 1 n v f j where v is the volume of a fine grid cell the subscripts c f and j refer to the coarse grid cell 10 50 or 200 cm the fine grid cell 5 cm and the index of the fine grid cell respectively the summations were taken over all the fine grid cells n belonging to the same coarse grid cell distributions of ks and cfe at different srs are shown in figs 3 and 4 and s1 and s2 in the supporting information ks shown in fig 3 are those adjusted in the flow simulations it should be noted that a 50 cm grid cell was further divided into a group of 10 cm sub grid cells and the parameters applied on these sub grid cells were kept the same to those on that 50 cm grid cell fig s3 such treatment was also done for the grid of 200 cm resolution therefore all simulations were actually conducted on the same grid of 10 cm resolution as such difference induced by sr could be retained while additional numerical deviations could be avoided for all three srs the model domain was divided into 207 480 grid cells with 62 218 inactive cells to accommodate the irregular river boundary hereinafter increasing sr means the increase of spatial resolution i e the increase of parameter resolution and vice versa time series of hourly water levels in well 199 b2 14 and at the river gauge were moving averaged at daily 24 h and monthly 720 h temporal scales fig 5 these trs represent scales from the level dominated by human activities to the level controlled mainly by natural regulations similarly a hypothetical time scale was used meaning that the hourly time step was actually used in all scenarios hereinafter increasing tr means the increase of temporal resolution i e the increase of temporal variability and vice versa therefore a total of 9 velocity fields 10 cm hourly 10 cm daily 10 cm monthly 50 cm hourly 50 cm daily 50 cm monthly 200 cm hourly 200 cm daily and 200 cm monthly and 27 reactive transport fields each velocity field was used to calculate reactive transport with three cfe distributions were simulated in this study 3 3 flow and reactive transport modeling 3 3 1 modeling flow a massively parallel subsurface flow and reactive transport model pflotran hammond et al 2014 was used to simulate variably saturated flow in the modeling domain the water levels in well 199 b2 14 and at the river gauge were used as the specified heads on the left side and on the right slope under the river stage respectively the river bank above the river stage was treated as the seepage boundary the top and bottom boundaries were no flow due to the low precipitation and the impermeable underlying sediments respectively the center of the river at the right boundary was assumed to be no flow as well the characteristic curve of the soil moisture was based on the brooks corey function and the related parameters are listed in table s1 in each scenario steady state flow field based on annual means of the well level and the river stage was first calculated as the initial condition for the corresponding transient flow field after spin up of the transient flow fields to reach quasi steady state one more year was simulated the results of which were repeatedly used for 10 years of simulation in the reactive transport 3 3 2 reaction network of cr in the hz the biogeochemical reaction network in this study was first established by liu et al 2017 and further improved by yang et al 2018 details table s2 of the reaction network included 1 multi rate cr vi sorption and reduction by sediment associated fe ii not in aqueous phase with the corresponding fe ii oxidation to fe iii 2 microbially mediated regeneration of fe ii with dissolved organic carbon doc as the electron donor and carbon source 3 abiotic oxidation of fe ii by do that competes with cr vi 4 do consumption by microorganisms as the electron acceptor with doc as the electron donor and carbon source and 5 doc production from the particulate organic carbon poc transformation the reactive fe ii for cr reduction was assumed to be only present in the alluvium so that in other regions cr was only subject to sorption shown in the blue and purple areas in fig 2 specific parameters of the reactions are provided in table s3 3 3 3 modeling reactive transport reactive transport simulations of 10 years for cr vi with other relevant chemical species i e organic carbon fe ii fe iii and do in the alluvium and cr vi sorption to sediments outside the alluvium were performed based on the flow fields obtained a priori initial conditions based on measurements liu et al 2017 for these reactions are provided in table s4 cr vi with a concentration of 2000 μg l dresel et al 2008 johnson 2016 liu et al 2017 smoot et al 2011 truex et al 2015 yang et al 2018 was injected as a plug at the fifth year shown as the 10 m 5 m red box in fig 2 for all the mobile species the top bottom and right sides of the model domain were assumed to be no flux boundaries and the left side was treated as constant concentration when the water flow was into the model domain and as zero concentration gradient when the water flow was out of the domain species concentrations in the rw were assumed to be constant the reactive transport was solved by an in house developed simulator as an alternative of the reaction sandbox provided in pflotran which has been proved to be feasible for the current study site yang et al 2018 then the cr vi mass dissolved in the inland area adsorbed to the sediments and discharged to river the cr iii mass precipitated in the hz and the do mass transported into the model domain and dissolved in the inland area yang et al 2018 were calculated for the following analysis 4 results and discussion 4 1 physical and chemical distributions at different srs and trs in fig 3 with decreasing sr preferential flow paths are gradually reduced which can be observed by the shrinkage of the red area the reason is that large ks are removed to compensate for extremely low values during upscaling at places where the color becomes warmer hydraulic connections are strengthened due to the replacement of small ks with larger values flow fields in the domain would become smoother with fewer sudden changes of velocity which will be discussed in section 4 2 similar patterns are observed for cfe in fig 4 cfe are more evenly distributed at lower sr since some parts with lower cfe are supplemented in up scaling which could sacrifice the strong redox ability of the nearby places in fig 5 with the moving average from hourly to monthly tr crossovers between gw level and river stage in the middle of a year are gradually filtered out this can cause decrease of the flow reversal frequency in the modeling area yang et al 2018 flow reversal here means rw intrusion to subsurface instead of the general gw discharge to river it has importance on redox transformation of cr in the hz yang et al 2018 this effect might be magnified when superimposed on the spatial scaling effect in this study 4 2 hydrodynamics and cr transformation at different srs fig 6 shows horizontal velocities u at three observation points p1 p3 located from the well side well 199 b2 14 to riverside first of all simulated horizontal velocities are consistent with results from previous studies conducted at the hanford site e g 25 to 25 m d in yabusaki et al 2008 indicating the accuracy of the model in this study in this section the first and second rows are focused since here we only discuss the effect of sr at p1 the water is almost stagnant at 10 cm sr 0 m d and becomes dynamic with decreasing sr fig 6a and d e g over 0 2 m d at 200 cm sr in contrast u at p2 where it is extremely high decreases to about one third 15 m d to 5 m d at peak values from 10 cm to 200 cm sr leading to a more moderate state fig 6b and e this indicates that upscaling tends to result in a more uniform flow field in space hence variations of hydrodynamics with sr discussed here are consistent with the prior judgement in section 4 1 what s more the effect of spatial upscaling is not obvious in the middle of the year when interactions between gw and rw are especially intense e g little difference in three srs at about 4000 h in fig 6a in the first column of fig 7 cr vi mass to river is minimum at 200 cm sr for all three fe distributions with a lower sr fe ii is more averaged to larger size grains where there is no or less fe ii at a higher sr thus the retardation capability of the hz to cr vi becomes stronger and cr vi is more prevented to river this overestimation of retardation capability in the hz means the underestimation of environmental risk about the cr vi discharge to river this phenomenon was drawn in fig 8 by taking a scenario with fe distributed on medium size grains cr vi enter the river through corridors with a small amount of fe ii at 10 cm sr while cr vi are retarded in the hz at 200 cm sr similar phenomena have been reported in previous studies elfeki et al 2012 showed that higher heterogeneity which corresponds to higher sr here might lead to larger variance of the contaminant plume i e more dispersion of the plume zheng and jiao 1998 described the small patches connected to the main body of the plume by a narrow neck and the fringy feature on the iso surface of the plume at the made site where the sediments are extremely heterogenous these phenomena are mainly caused by the preferential flow paths due to high heterogeneity elfeki et al 2012 zheng and jiao 1998 in summary with higher sr cr vi is more easily discharged to river along paths with larger grain sizes where k is larger and cfe is lower when fe is on small size grains cr vi enters the river at a maximum value and cr iii precipitated in the hz is the minimum regardless of the sr this can be observed by that the grey line group is always the highest in the first column while the lowest in the second column in fig 7 this indicates that the highest risk to river occurs when fe is on small size grains when fe is on small and large size grains higher cr iii mass is observed fig 7j meaning more retarded cr vi in the hz since large size grains are always the preferential flow paths for cr vi this is consistent with li et al 2011 in which the largest amount of u iv was generated at the edge of the preferential flow paths interfacing with fe iii rich areas dissolved and adsorbed cr vi in the inland area is independent of fe distribution and is of slight difference at different srs the third and fourth columns in fig 7 this difference is due to the hydrodynamic discrepancies induced by different physical heterogeneities at different srs moderate hydrodynamics at a lower sr result in more dissolved and adsorbed cr vi in the inland area this is shown by the fact that red lines are always higher than blue and black lines with a maximum discrepancy of 1 g this also indicates that the effect of geochemical heterogeneity is more important than that of the physical heterogeneity which was rarely discussed in previous studies for two sides fe distribution cr vi mass to river is not sensitive to sr fig 7i for medium size distribution it is the most sensitive case to sr due to the large discrepancy among three srs hence with the same decrease of sr retardation capability of the hz is the most overestimated when fe is on the medium size grains and thus the environmental risk to river is the most underestimated with medium size distribution effect of upscaling lasts to about 200 cm sr since cr vi discharge to river is totally prevented at this sr fig 7e this means that a maximum underestimation of environmental risk to river has been achieved at 200 cm sr and the underestimation would not increase any more with the continued decrease of sr for small size distribution fig 7a cr vi mass to river is less sensitive to sr than that with medium size distribution however upscaling has the most persistent effect on cr vi mass to river since there is still a large amount of cr vi discharged to river at 200 cm sr effect of upscaling should disappear at a sr lower than 200 cm at which cr discharge to river becomes almost zero as that in fig 7e as such the selection of sr deserves particular attention when environmental risk assessment is conducted at places where redox sensitive chemicals are distributed on small size grains due to the persistent effect of upscaling attention should also be paid when fe is distributed on medium size grains which might be the dominating distribution pattern of fe at the hanford site 4 3 hydrodynamics and cr transformation at different combinations of sr and tr with decreasing tr fluctuations of u at higher tr are smoothed out and hydrodynamics become gentle more importantly both frequency and magnitude of the reversed flow decrease with decreasing tr which can be observed for instance by comparing fig 6f with l difference in hydrodynamics of different srs become more obvious with decreasing tr in general for hydrodynamics either the discrepancy induced by tr is much more obvious than that induced by sr or hydrodynamics are more controlled by tr thus at places with intense anthropogenic activities inducing extra water flow fluctuations at small time scales it is necessary to use higher tr to capture such hydrodynamics while the discrepancy induced by srs is insignificant under such transient conditions with lower tr more cr iii precipitated in the hz the second column in fig 9 regardless of the sr this means an overestimated retardation capability of the hz therefore the risks seem to be further underestimated when compared to that caused by a lower sr for example from 10 cm hourly to 200 cm hourly then to 200 cm monthly fig 9j cr iii precipitation continuously increases however cr vi to river the first column in fig 9 also increase with decreasing tr which in fact neutralize a small portion of the underestimated risks from the individual decrease of sr e g from 10 cm hourly to 50 cm hourly then to 50 cm monthly fig 9i the higher the sr the more the neutralization at lower tr taking fig 9i for example difference between black line 10 cm sr and corresponding grey line is larger than that for red line 200 cm sr then increase of both cr vi discharge and cr iii precipitation are balanced by decrease of dissolved and adsorbed cr vi in the inland area at lower tr this can be observed by comparing grey line group with other line groups in the third and fourth columns of fig 9 decrease of dissolved and adsorbed cr vi are observed at lower tr which is consistent with yang et al 2018 in addition difference of cr vi mass to river among different srs are increased with decreasing tr fig 9i this can be observed by comparing the grey line group with other line groups in the first column of fig 9 in contrast to hydrodynamics redox transformation of cr is more dependent on the sr than the tr e g 10 cm hourly 200 cm hourly vs 10 cm hourly 10 cm monthly 4 4 mechanisms for effects of sr and combined sr and tr on cr transformation in fig 10 a and 10e do concentration has insignificant dependence on sr at p4 this is shown as the slight increase of do with decreasing sr however the difference of do among different srs is much more obvious at the regional scale i e the whole modeling domain in fig 11 and its content is the highest at 200 cm sr the concentration of organic carbon oc is generally lower at 200 cm sr fig 10b and f because it is more consumed by biological activities at the corresponding higher do level in contrast fe ii is higher at a lower sr triple concentration at 200 cm sr to that at 50 cm sr fig 10h and that is why more cr iii is generated at 200 cm sr fig 9b f and j therefore more oc particularly the poc is consumed to maintain this relatively high fe ii level it is clearly shown in fig 10g that poc concentration is about 10 μmol g lower at 200 cm sr than that at 10 cm sr in fig 10a flow reversals at daily and monthly trs carry less do from rw to gw which can be observed due to smaller peaks 80 μmol l relative to those at hourly tr 300 μmol l thus less oc is consumed as shown in the second and third columns in fig 10 therefore more fe ii is left and more cr iii is precipitated in the hz at lower trs which is the explanation for the results in fig 9 however fe ii content increases moderately with the decrease of tr when compared to that with the decrease of sr in fig 10h concentration of fe ii increase from 0 to 0 01 then to 0 04 μmol g with decreasing sr while from 0 01 to 0 02 μmol g from hourly to daily and monthly therefore the redox transformation of fe ii and thus cr is more sensitive to sr than tr besides concentrations of oc fig 10b f c and g decrease with time as the gradual intrusion of do from rw to gw when a large rw intrusion occurs in the middle and at the end of the year do has obvious concentration peaks fig 10e while oc has clear valleys fig 10f and g fe ii also gradually decreases with time due to the do intrusion the above findings are also consistent with those in yang et al 2018 5 conclusions in this study taking the redox sensitive cr as an example numerical simulations were conducted to illustrate effect of spatio temporal resolution on modeling biogeochemical transformation of contaminants in a hyporheic zone hz at the us doe hanford site the srs are 10 50 and 200 cm representing scales from the laboratory to the field while trs are hourly daily and monthly representing scales from the level dominated by human activities to that controlled mainly by natural regulations generated stochastic physical and geochemical fields and measured water levels are successively upscaled to larger spatial and temporal scales simulation results are of particular importance for help choosing the proper srs and trs when conducting monitoring and or modeling of reactive contaminants and for improving accuracy of environmental risk assessment on contaminants discharge to river or other water bodies results also have general implications for riverine areas that have been polluted by other redox sensitive pollutants such as as or sb which have high mobility at a low valence state and low mobility at a high valence state several important conclusions of this study can be drawn as follows 1 lower srs and trs tend to generate more uniform hydrodynamics hydrodynamics are more controlled by tr and decrease of tr could amplify the effect of sr thus at places where the human activities are intensive a higher tr should be preferentially considered 2 environmental risk of cr vi to river is underestimated with decreasing sr decrease of tr could neutralize a small portion of this underestimation difference of cr vi mass to river among different srs are amplified with the decrease of tr redox transformation of the cr is more dependent on sr than tr 3 with the same decrease of sr risk is underestimated the most for medium size distribution for small size distribution upscaling has the most persistent effect on cr vi mass to river since there is still a large amount of cr vi discharged to river at 200 cm sr in this study 4 do content is higher at lower sr and more dependent on sr at the regional scale than at the local scale such as at p4 more sensitive increase of fe ii with decreasing sr than with decreasing tr results in the stronger control of sr than tr on generation of cr iii 5 in general for reactive transport the effect of geochemical heterogeneity is more important than that of physical heterogeneity which in turn is also more critical than that of the temporal variability of the hydrodynamics geochemical heterogeneity could amplify the risk caused by physical heterogeneity declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the national natural science foundation of china china grant no 41807198 41877183 and by the guangdong provincial key laboratory of soil and groundwater pollution control china grant no 2017b030301012 we thank the tremendous help from dr glenn hammond and other developers for using pflotran in the hydrodynamics simulations appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124152 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6037,hydrological and climatological extreme events are characterized by several correlated random variables for a better associated risk assessment the dependence structure between these variables must be taken into account by considering copulas multiparameter copulas m copulas play an important role by their flexibility and ability to capture more than one type of dependence since model misspecification can lead to underestimation or overestimation of the associated risk it is of high importance to select the appropriate copula to this end several goodness of fit gof tests have been proposed however these tests are applied validated and evaluated for the usual one parameter copulas nevertheless there is no specific gof test for m copulas that takes into account the specificities of hydrometeorological series hence the aim of the present paper is to introduce new gof tests specifically for m copulas and adapted to hydrometeorological context more precisely the proposed gof tests are based on multivariate l moments a simulation study is conducted to evaluate and compare the performances of the proposed tests the results confirm the usefulness of the new gof tests in comparison with some well established ones finally the newly introduced tests are illustrated on hydrometeorological data series keywords multivariate l moments flood precipitation parameter estimation goodness of fit power multiparameter copula testing 1 introduction and literature review hydrometeorological events are characterized by a number of correlated random variables e g chebana and ouarda 2009 hao and singh 2016 therefore a multivariate frequency analysis that takes into account the dependence between variables is of fundamental importance copulas have been proposed to model the dependence structure these variables e g salvadori et al 2007 chebana 2013 genest and chebana 2017 in hydrometeorology copulas have gained substantial and increasing attention e g salvadori and de michele 2004 salvadori et al 2007 kao and govindaraju 2008 vandenberghe et al 2010 hao and aghakouchak 2013 the usual one parameter copulas have been largely developed in statistics and widely used in applications however multiparameter copula denoted m copula is a developing topic with several open issues recently sadegh et al 2017 et de michele et al 2013 used m copulas to model meteorological drought events in regional hfa requena et al 2016 used two parameter archimedean copulas to generate synthetic homogenous regions with more flexibility as shown by salvadori and de michele 2010 m copulas are likely to better model dependence between multivariate data than classical one parameter copulas in recent years formal goodness of fit gof tests have been introduced to select appropriate copula e g wang and wells 2000 fermanian 2005 genest et al 2006 mesfioui et al 2009 and references therein these gof tests could be conceptually valid for any copula structure however they are evaluated and validated only for one parameter copula furthermore as pointed out by berg 2009 based on simulation results conducted for existing gof tests those based on empirical copula and kendall process are recommended even for one parameter copulas these two recommended tests have some important drawbacks as detailed in section 2 2 in addition even though the importance and usefulness of general gof tests specific ones for given classes of copulas are also required this is in analogy to gof tests for distributions in the univariate framework such as for normal distribution e g chowdhury et al 1991 choulakian and stephens 2001 zardasht et al 2015 regarding copulas genest et al 2011 proposed a specific gof test for extreme value copulas whereas durocher and quessy 2017 focused on spatial copulas when dealing with m copula one can find the test proposed by kojadinovic and yan 2011 the latter focuses only on gaussian and student copula as m copulas in addition this test is exactly the empirical copula gof test by genest et al 2006 where only the p value evaluation is based on multiplier approach they showed that the test is more appropriate for high dimensional and large sample size series which is generally not the case of hydrometeorological series the main objective of the present paper is to introduce and evaluate a new and specific gof tests for m copulas see table 1 based on multivariate l moments an extensive simulation study involving a large number of m copulas and dependence conditions is conducted to evaluate performance of the newly proposed tests and to compare with classical tests the paper is organized as follows in section 2 a short discussion of the theoretical background is presented m copulas existing gof tests and multivariate l moments the proposed multivariate l moments based gof tests are presented in section 3 whereas section 4 deals with the simulation study application to real world hydrometeorological data is represented in section 5 concluding remarks are presented in the last section 2 m copula in modeling hydrological variables in this section a brief overview of m copulas is presented for more details see e g joe 2014 in the following for the sake of brevity and simplicity only the bivariate case is considered 2 1 m copulas extreme value and archimedean m copulas are of special interest in hydrology and climatology e g zhang and singh 2007 salvadori and de michele 2010 aghakouchak 2014 indeed m copulas can cover a range of dependence from perfect positive dependence to independence and may be extended to perfect negative dependence m copulas introduce additional flexibility to the model since they include other subfamilies of classical copulas as especial cases e g salvadori and de michele 2010 de michele et al 2013 for instance the bb5 as 2 copula includes gumbel and galambos ones consequently m copulas provide a better way to capture different mutual dependencies e g chen and khashanah 2014 hence this may improve the modelling features of the dependence structure a number of archimedean m copulas are available in the literature such as bb1 bb6 and bb7 e g joe 2014 besides extreme value copulas which include bb5 as an example in the m copula category have been extensively used in recent years given their importance in modeling catastrophic events e g salvadori and michele 2011 zhang and singh 2012 for more details about properties of m copulas including those used in this study see table 2 the reader can refer to joe 2014 2 2 existing copula gof tests as indicate above several gof tests have been already proposed for copula selection in the following we focus on the tests described by genest et al 2009 as omnibus tests and recommended by berg 2009 especially those based on the empirical copula process and the kendall process in fact the other existing tests entail many shortcomings berg 2009 for example they involve many arbitrary choices e g kernel type window length weight function that make their application cumbersome furthermore they are computationally expensive for high dimensional cases for the omnibus tests their performance has not been evaluated when dealing with m copulas to the best of our knowledge this study is the first one to present a critical review and to evaluate compare their power in the case of m copulas table 1 the empirical copula based test proposed by genest et al 2009 consists in comparing the distance between the empirical copula c n and an estimation c θ n of c obtained under the null hypothesis genest and rivest 1993 proposed the kendall based test based on comparing the distance between the empirical kendall s function k n and an estimation k θ n of k obtained under the null hypothesis when dealing with one parameter copula these two gof tests have some drawbacks for instance their application is not suitable for small samples or low dependence levels genest et al 2006 berg and quessy 2009 in addition as pointed out by genest et al 2009 the kendall based test is not consistent for extreme value ev copulas since two different ev copulas have the same kendall s function barbe et al 1996 therefore the test is not appropriate to distinguish between different ev copulas e g fermanian 2013 in addition as pointed out in genest et al 2009 the empirical copula based gof test is not appropriate for small sized samples which is the case of the most of hydrological series e g laio et al 2009 2 3 multivariate l moments the multivariate l moment was introduced by serfling and xiao 2007 they provide a summary and a description of the properties and shapes of a multivariate distribution this makes them particularly useful in parameter estimation and hypothesis testing since multivariate also univariate l moments are much less biased than classical moments they are used as meaningful replacements of classical moments in a wide variety of applications mainly in hydrology climatology and meteorology analysis e g hosking and wallis 1993 chebana and ouarda 2007 kysely and picek 2007 brahimi et al 2015 the multivariate l moments are defined as bellow 1 λ k ij c o v x i p k 1 f j x j i j 1 2 and k 2 where cov is the covariance k is the order of the multivariate l moment p k 1 is the shifted legendre polynomial chang and wang 1983 x i is a random variable with marginal distribution f i for i 1 2 brahimi et al 2015 introduced the copula s multivariate l moments as 2 λ k c 12 0 1 0 1 c u 1 u 2 u 1 u 2 d u 1 d p k u 2 where c is the associated copula function the sample version of the kth copula multivariate l moments is defined in term of pseudo observations as brahimi et al 2015 3 λ k 12 c 1 n i 1 n r i n 1 p k 1 s i n 1 where r i is the rank of x i 1 among x i 1 x n 1 et s i is the rank of the concomitant x i n 12 among x i 2 x n 2 x 12 is formed by sorting x 2 in the ascending order and in turn shuffling x 1 by the order of x 2 serfling and xiao 2007 the copula multivariate l moments coefficient ratios are defined as 4 τ k 12 cop λ k 12 cop λ 2 2 for k 3 and τ 2 12 cop λ 2 12 cop λ 1 2 according to brahimi et al 2015 the vector of the multivariate copula l moments converges in distribution to the multivariate normal distribution with variance covariance matrix σ σ ij 1 i j 4 defined as 5 σ ij b i b j 1 n i j 2 1 k l n k 1 i l i 2 j k 1 j l j 2 i x k n 12 x l n 12 where b k 1 n k 1 i 1 n i 1 k x i n 12 k 0 3 3 the proposed gof tests for m copulas in this section we present the new proposed gof tests for m copulas based on multivariate copula l moments two nonparametric approaches of tests are developed see table 3 the first one is based on the distance between the multivariate l moments of the fitted copulas and the sample multivariate l moments it is inspired by the hosking and wallis 1993 gof test proposed in the univariate setting the second approach consists in generalizing the gof test developed by shih 1998 it is based on the deviation between two estimators of the parameters vector of the m copula using the multivariate l moments estimation method brahimi et al 2015 and maximum pseudo likelihood mpl estimation method kojadinovic and yan 2010 3 1 proposed gof test based on multivariate copula l moments in this section a gof test for m copulas based on multivariate copula l moments ratios is presented the main idea of the new developed gof test consists on evaluating a distance between the empirical values of τ 3 12 and τ 4 12 and their theoretical counterparts 4 associated to the m copula c θ n under the null hypothesis h 0 let ω τ 3 12 τ 4 12 be the vector of the empirical multivariate l moments ratios and ω cop τ 3 12 cop τ 4 12 cop be their theoretical counterparts from the underlying m copula the associated statistic is defined as 6 z n χ n ω ω cop t ω 1 ω ω cop where ω σ ij n λ 2 2 i j 3 4 and σ ij defined by eq 5 it is worth mentioning that for traditional gof tests the asymptotic distributions of their statistics depend on the unknown copula c θ genest et al 2009 and their p values could only be obtained via bootstrap procedures genest and rémillard 2008 hence for convenience and comparison purpose the bootstrap procedure is used to estimate p values of the proposed test asymptotic results are out of the scope of the present paper and could be the object of future work the use of the proposed statistic in 6 has several advantages including a simple formula available for the proposed statistic in terms of the ranks of the observations b the statistic involves no subjective choices such as the choice of a kernel and associated smoothing parameters c compared to conventional moments multivariate l moments are less subject to bias in estimation d multivariate l moments are able to characterize a wide range of m copulas 3 2 extension of the shih s gof test to m copulas when dealing with the one parameter clayton copula shih 1998 proposed a moment based gof test the basic idea of the test consists on the comparison between two estimators of the scalar parameter θ via kendall s τ and a weighted rank based estimator berg and quessy 2009 extended this test for any one parameter copula based on classical copula moments kendall tau and spearman rho however the case of m copula has not been investigated in this section we propose an extension of the moment based gof test to deal with m copulas the proposed gof test is then based on the comparison of two parameters vector estimates namely the multivariate l moments estimator θ lmom and the maximum pseudo likelihood estimator θ mpl as in the tests by shih 1998 and berg and quessy 2009 under the null hypothesis that the unknown copula of a population belongs to a specific family both estimators converge to the true parameters however if the assumed copula is invalid the two estimators generally do not converge to the same value white 1981 in their simplest form assume a vector of p parameters θ θ 1 θ p t let l 12 λ 1 12 λ 2 12 λ p 12 t be the vector of the first p variate l moments of the fitted copula c n θ the estimator θ lmom a 1 l 12 t is a consistent estimator of θ where a is the coefficient of the shifted legendre polynomial p k 1 brahimi et al 2015 we define the statistic z n lm as 7 z n lm n i 1 p θ lmom i θ mpl i 2 note that the statistic z n lm is defined for any d variate copulas indeed as pointed out in joe 2014 for a d variate copula the number of parameters is of the order of the dimension d up to the square of the dimension as above the corresponding p value is estimated by bootstrap the proposed gof test based on parameter estimation methods has many advantages a its statistic is free of any subjective choices which could affect the performance of the tests b compared to conventional moments multivariate l moments are less subject to bias in estimation c parameter estimates obtained from multivariate l moments are usually more accurate in small samples than the estimates obtained by other methods 4 simulation study 4 1 simulation design the objective of the simulation study is to evaluate the performance of the proposed gof tests as well as the already existing ones when the latter are able to test m copulas typically the performance of a gof test is evaluated through the estimation of first type error α level of the test as well as the test power the rejection rates of the null hypothesis in this study α is fixed to 5 as a usual value in several previous studies e g berg 2009 genest et al 2009 it was pointed out in the literature e g genest et al 2006 berg 2009 fermanian 2013 in the case of one parameter copulas that the performance of a gof test is influenced by several factors especially sample size and dependence level hence a sensitivity analysis is performed to identify the effect of these factors on the behavior of each gof test since for most flood events kendall s τ is between 0 3 and 0 8 e g zhang and singh 2007 requena et al 2013 and for comparison purpose we considered τ 0 2 0 4 0 6 as in previous gof testing studies e g berg and quessy 2009 regarding the effect of sample size n 30 50 100 are investigated the values of n are selected on the basis of situations commonly encountered in flood frequency analysis indeed barth et al 2017 reported that data examined from 1375 long term u s geological survey usgs streamgage sites have at least 30 years annual peak flow furthermore santhosh and srinivas 2013 indicated that the minimum sample size drawn from five typical peak flow records from different parts of the world is equal to 40 for all dependence scenarios m 10 000 samples are generated the latter is typically used in gof testing simulations e g genest et al 2009 data are generated through monte carlo simulations from representative and most used copulas in hydrometeorology analyses e g salvadori et al 2007 sadegh et al 2017 therefore two classes of copula family are used 1 archimedean m copulas including bb1 bb6 and bb7 and 2 extreme value bb5 copula to generate data from a given m copula we applied the procedure proposed by nelsen 2006 based on the conditional distribution method 4 2 simulation results in this section simulation results obtained by the application of different gof tests to the considered copulas are presented comparisons are given in the last part of this section 4 2 1 first type error estimates the first desirable property of a gof test is the ability to maintain the first type error close to the significance level probabilities of rejection are estimated under h0 and compared to the theoretical nominal level the effect of sample size on the level of considered gof tests is shown in table 4 for dependence level τ 0 6 for n 50 and n 100 it is found that the new developed gof tests perform well in terms of first type error since its estimation is very close to α 5 between 4 and 5 2 for sample size n 30 the two proposed tests z n lm and z n χ have slightly large first type errors between 5 7 and 7 1 this is expected since the sample l coskewness and l cokurtosis ratios τ 3 12 and τ 4 12 respectively can be slightly biased for small n serfling and xiao 2007 in addition this might be explained also by a tendency for σ 33 and σ 44 estimated by equation to underestimate the true variance of l coskewness and l cokurtosis for small n as in the univariate case kjeldsen and prosdocimi 2015 table 5 provides results of application of the proposed tests for sample size n 100 and different dependence levels from table 5 one can see that the first type errors of the proposed tests z n lm and z n χ appear in general to be closer to 5 as the dependence level increases which is in agreement with other studies e g genest et al 2009 durocher and quessy 2017 indeed for low dependence level τ 0 2 first type errors are between 6 and 10 this is expected since for low dependence level the distinctive features of the models are fuzzier e g genest et al 2009 as a consequence different tests failure to distinguish between different copulas e g berg 2009 berg and quessy 2009 mesfioui et al 2009 however the higher dependence level is τ 0 6 the better the first type errors are between 4 and 5 3 for moderate dependence level τ 0 4 the z n χ test 5 fails to hold the nominal level estimated levels above 6 this is likely due to the fact that the copula l moments are approximatively equal to zero for near independently data serfling and xiao 2007 brahimi et al 2015 4 2 2 power evaluation in order to be more informative the powers of the considered tests are presented as boxplots in figs 1 and 2 from fig 1 one can see that in most cases the proposed test z n lm shows almost the highest power for several m copulas the power of the z n lm is between 85 and 100 for sample size n 100 and dependence level τ 0 6 even for a moderate sample size n 50 z n lm has the highest power on the other hand the power of the test statistic z n χ increases with the sample size indeed large sample size not only helps to distinguish between copulas but also plays a role in the reliability of the bootstrap procedures used to approximate the associated p values as it is the case for one parameter copula berg 2009 berg and quessy 2009 moreover as pointed out in the literature of gof testing when sample size is small it is more difficult to distinguish between copulas since the dependence level affects the power of a test in a similar way as the sample size the following discussion will be brief from fig 2 for low dependence level τ 0 2 it is found that results of the developed gof tests are satisfactory indeed z n lm has the highest power between 51 and 85 and z n χ power estimates are between 40 and 69 from fig 2 it is found that the power of the proposed gof tests appear to improve as the dependence level increases the same pattern is observed in gof testing literature this can be explained by the fact that when dependence level increases the copula structure is more distinguishable e g berg 2009 however when the dependence level is too small the dependence structures are hardly distinguishable 4 2 3 comparisons a comparison of the performance between the proposed tests and the classical ones is made through boxplots figs 1 and 2 according to fig 1 the power estimates of all statistics increase when n becomes larger this agrees with similar findings by berg 2009 and genest et al 2009 made for one parameter copula gof tests however z n χ is less influenced by n with only a difference about 2 when n increases from 30 to 100 this can be explained by the fact that multivariate l moments are robust and suffer less from the effects of sampling variability brahimi et al 2015 nevertheless figs 1 and 2 show that the two classical statistics s n c and s n k are not able to distinguish the different m copulas the associated powers of these tests are between 9 and 27 for n 30 and between 20 and 69 for n 100 for s n k this is unsurprising since the kendall function could be the same for two different copulas e g nelsen 2006 on the other hand s n c behaves in the same way as in the case of one parameter copula for m copula low power estimates may be caused by the small sample size n 1 5 0 genest et al 2006 the most striking result from fig 2 is that the tests s n c and s n k have lower powers even for strong dependence τ 0 6 when dealing with m copulas a possible explanation in the case of the statistic s n k might be that as the number of parameters increases the univariate summary represented by the probability integral transformation and its distribution function k is less representative of the multivariate dependence structure in the case of s n c it might be related to the fact that the sample size is relatively small even n 100 and as justified by genest et al 2006 this test has high power for n 150 from fig 1 different gof tests can be ranked in term of power as z n lm z n χ s n k s n c over all for a given n and τ the proposed tests have higher powers than those of the classical tests in fact since the shape of copula structure can be described by their multivariate l moment matrices serfling and xiao 2007 the proposed statistics allow a better differentiation of various copulas note that the gof test z n lm based on parameters estimation method has all times the best performance this may be due to inconsistency of the test in fact the parameter estimates by the mpl and multivariate l moments may be identical as supported by berg and quessy 2009 in the case of moments based gof test and so the test will accept more often the null hypothesis one wants to study the performance of the proposed gof tests when dealing with one parameter copulas table 6 provides corresponding results obtained from the proposed tests and the classical ones the proposed tests z n lm and z n χ perform well in terms of power compared to the classical tests the power estimates for the proposed tests are between 45 and 87 for classical gof tests power estimates are between 33 4 and 84 3 noticeable exception is made in the case of z n χ statistic which is outperformed by the classical tests when dealing with the first type errors 5 illustrative case studies after evaluating and comparing the proposed gof tests on simulated data in this section we apply these tests on real world hydrometeorological data a comparison is also made with the classical kendall and empirical gof tests the purpose of these applications is to illustrate the practical use of the proposed tests in order to select an appropriate copula family the first application deals with flood events and the second with meteorological drought note that these two application are characterized by different record lengths dependence level and dependence shape 5 1 bivariate flood frequency analysis to scrutinize the impact of length of data on the performance of the gof tests we use two flood peak q m3 s and volume v m3 data series the first one from the romaine station located at the province of quebec canada for the period 1984 2010 the second data set corresponds to the ankang hydrological station at the upper hanjiang river china the same series is used in xiong et al 2015 for the period 1950 2011 since the focus of the study is on the dependence structure the choice of marginal distributions is performed but not presented first the dependence analysis between variables is carried out by dependence measures in order to discard inappropriate copulas the kendall s τ is adopted to measure a quantitative value of the dependence structure kendall s τ is 0 30 and 0 8 respectively for romaine and ankang stations for ankang station this value supports the positive and strong dependence between q and v however for romaine station the dependence is relatively low hence copulas with dependence range not matching with those of the series are discarded and four copulas are retained for subsequent analysis m copulas bb1 and bb5 and one parameter copulas gumbel and galambos besides quantitative measures of the dependence graphical analysis of the dependence is conducted scatter plots and kendall s function plots are used to evaluate if a chosen copula fit properly the observed data a set of 10 000 synthetic pairs are generated from each one of the considered copulas then the synthetic pairs are transformed by using univariate marginal distributions from fig 3 a presenting scatter plots for romaine station we can see that extreme value copulas i e bb5 and galambos are sharper in the upper right corner than archimedean copulas bb1 and gumbel which are more scattered this figure shows also the positive lower tail dependence of the bb1 copula we can conclude that extreme value copulas discard the largest observations whereas the bb1 copula covers these observations in the generated sample as its dependence structure is more spread in the upper tail as expected we can see from fig 4 a that the distance between the empirical and theoretical kendall s function is greater for extreme value copula than for archimedean copula this analysis shows that extreme value copulas are not suitable for fitting romaine data consequently bb1 copula seems to be the best fitting copula however a further analysis is needed to confirm this preliminary conclusion to select the best fitting copula in a formal way the above described gof tests are applied gof test statistics and associated p values are summarized in table 7 it can be seen that bb1 copula is accepted by the proposed tests z n lm and z n χ but rejected by the classical tests s n c and s n k the gumbel copula is accepted by proposed tests as well as s n k however s n c rejects all copulas except the galambos one indeed as indicated above this test is affected by small sample size n 26 it is apparent from table 7 that s n k is unable to distinguish between different extreme value copulas gumbel bb5 and galambos indeed regarding s n k all considered extreme value copulas are accepted as a good candidate for fitting the romaine data as they have the same kendall s function genest and boies 2003 for z n lm all copulas are also accepted this can be explained by the inconsistency of the test therefore it is important to bear in mind that the parameter estimates by the mpl and multivariate l moments may be identical even if the copula not fit well the data e g berg and quessy 2009 in order to select the best fitting copula among those accepted by different gof tests the model selection criterion aic is considered sadegh et al 2017 accordingly the bb1 copula is selected as the most appropriate copula for q v of the romaine station as a reminder the bb1 copula is accepted by new proposed tests whereas it is rejected by the classical tests in the case of the ankang data as shown in fig 3b one parameter copulas gumbel and galambos copula and the two parameter bb5 copula contains the gumbel and galambos copulas display rather similar behaviour in terms of scatter plots these copulas reproduce suitably the dependence in the upper extremes moreover bb1 copula reproduces suitably the dependence in the upper and lower tails since its structure is more spread from fig 4b theoretical kendall s function of the bb1 and gumbel copulas are closer to the empirical kendall s function than for extreme value copulas then bb1 and gumbel copulas could better fit the ankang data as showed in table 7 bb1 copula is accepted by the proposed tests z n lm and z n χ while it is rejected by the classical tests s n k and s n c bb5 and galambos copulas are accepted by all gof tests however gumbel copula is accepted by z n lm and s n k while it is rejected by z n χ and s n c note that for ankang station compared to romaine station the sample size is large and dependence level is high hence it would be easier to distinguish between the different copulas according to aic the bb1 copula could be selected as the best one to fit data these applications show in a practical and complimentary way to the simulation results the advantage of using the new proposed tests especially when dealing with small sample size and low dependence level in addition these applications confirm the ability of the proposed tests to distinguish between different extreme value copulas with respect to a kendall approach which necessarily shrinks into the parameter all the information about the ev copula the main objective of this study is to deal with hydrological series characterized by small sample size as pointed out through the simulation study and the application to real world data this objective is reached with the proposed tests 5 2 bivariate drought analysis in this section we focus our attention on drought analysis with an emphasis on the use of gof tests for the best fitting copula selection since drought is one of the most damaging natural hazards its prediction is of high importance for risk assessment and decision making droughts can be of meteorological agricultural hydrological or socio economical nature in this study we focus on meteorological droughts caused by a deficit in precipitation and a lack of soil moisture hence we employ the multivariate standardized drought index msdi proposed by hao and aghakouchak 2013 where a sequence of negative msdis indicates a drought event in this application we used monthly precipitation and soil moisture data from 1948 to 2017 obtained from the climate prediction center cpc for climatic division 1 in northern california different copulas are used to construct the joint probability distribution of precipitation mm day and soil moisture mm anomalies defined as deviations from the 1971 to 2000 monthly climatology to scrutinize the impact of length of data on the dependence structure and the performance of the underlying gof tests we use a subset of recent 30 years 1988 2017 of annual data in addition to the original 70 years 1948 2017 of record fig 5 shows the scatter plots and different fitted copula to describe the dependence structure between precipitation and soil moisture anomalies using 70 years of annual fig 5a and 30 years of annual fig 5b observed data it is worth noting that the two data sets have relatively small dependence strength the kendall τ is 0 48 and 0 35 for data set of length 70 and 30 years respectively copulas with dependence strength not matching with those of the series are discarded five copulas are retained for subsequent analysis m copulas bb1 and bb5 and one parameter copulas gumbel frank and clayton from fig 5a in the case n 70 a visual inspection of different copulas fitted to the data shows that only the clayton and frank copulas are able to characterize the dependence structure of this data set however from fig 6 a theoretical kendall s function of the bb5 and clayton copulas are closer to the empirical kendall s function than other copulas then bb5 and clayton copulas could better model the dependence between precipitation and soil moisture anomalies when the original data record is considered n 70 results regarding the application of the proposed gof tests as well as classical tests are summarized in table 8 for the 70 years data set neither clayton frank nor gumbel copulas could be rejected considering the p values by z n lm and s n c it is also notable from table 8 that the proposed statistic z n χ accept only the bb5 and clayton copulas as good candidates to fit data it is also important to highlight that the bb5 copula is rejected by the classical statistics s n k and s n c recall that bb5 and clayton copulas provide a very good fit to the data regarding the kendall plot fig 6a since p values cannot be used to rank copulas but only to accept or reject them e g salvadori and michele 2011 we used the criterion aic for ranking copula that passed gof tests consequently both clayton and bb5 copulas could be chosen as the best fitting copula since they have very closer aic in the case of small sample size the graphical diagnostics of the dependence structure fig 5b and 7b in order to select the appropriate copulas become difficult from fig 5b we can see that all fitted copula could be a good candidate this behavior is replicated in fig 6b where different theoretical and empirical kendall s functions are close to each other this highlights the importance of using formal gof tests when dealing with small sample size n 30 based on z n χ and z n lm statistics bb5 gumbel and frank copulas seem to fit properly the data tables 8 based on s n k all considered copulas are able to represent the dependence structure except the bb1 copula whereas none these copulas is accepted by s n c this might be explained by the inconsistency of these two tests for small sized sample genest et al 2006 according to aic criterion bb5 copula is selected as the best copula followed by gumbel copula 6 conclusion the proposed gof tests are the first ones designed specifically to deal with m copulas a novelty of this paper lies in the introduction of multivariate l moments in gof copula testing simulations results show that the proposed tests are good candidates to control first type errors and overall they have the highest performance in terms of power even for small samples and weak dependence furthermore the obtained results from two different case studies confirm the advantages of the proposed tests especially when dealing with small sample sizes as well as when considering several extreme value copulas declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
6037,hydrological and climatological extreme events are characterized by several correlated random variables for a better associated risk assessment the dependence structure between these variables must be taken into account by considering copulas multiparameter copulas m copulas play an important role by their flexibility and ability to capture more than one type of dependence since model misspecification can lead to underestimation or overestimation of the associated risk it is of high importance to select the appropriate copula to this end several goodness of fit gof tests have been proposed however these tests are applied validated and evaluated for the usual one parameter copulas nevertheless there is no specific gof test for m copulas that takes into account the specificities of hydrometeorological series hence the aim of the present paper is to introduce new gof tests specifically for m copulas and adapted to hydrometeorological context more precisely the proposed gof tests are based on multivariate l moments a simulation study is conducted to evaluate and compare the performances of the proposed tests the results confirm the usefulness of the new gof tests in comparison with some well established ones finally the newly introduced tests are illustrated on hydrometeorological data series keywords multivariate l moments flood precipitation parameter estimation goodness of fit power multiparameter copula testing 1 introduction and literature review hydrometeorological events are characterized by a number of correlated random variables e g chebana and ouarda 2009 hao and singh 2016 therefore a multivariate frequency analysis that takes into account the dependence between variables is of fundamental importance copulas have been proposed to model the dependence structure these variables e g salvadori et al 2007 chebana 2013 genest and chebana 2017 in hydrometeorology copulas have gained substantial and increasing attention e g salvadori and de michele 2004 salvadori et al 2007 kao and govindaraju 2008 vandenberghe et al 2010 hao and aghakouchak 2013 the usual one parameter copulas have been largely developed in statistics and widely used in applications however multiparameter copula denoted m copula is a developing topic with several open issues recently sadegh et al 2017 et de michele et al 2013 used m copulas to model meteorological drought events in regional hfa requena et al 2016 used two parameter archimedean copulas to generate synthetic homogenous regions with more flexibility as shown by salvadori and de michele 2010 m copulas are likely to better model dependence between multivariate data than classical one parameter copulas in recent years formal goodness of fit gof tests have been introduced to select appropriate copula e g wang and wells 2000 fermanian 2005 genest et al 2006 mesfioui et al 2009 and references therein these gof tests could be conceptually valid for any copula structure however they are evaluated and validated only for one parameter copula furthermore as pointed out by berg 2009 based on simulation results conducted for existing gof tests those based on empirical copula and kendall process are recommended even for one parameter copulas these two recommended tests have some important drawbacks as detailed in section 2 2 in addition even though the importance and usefulness of general gof tests specific ones for given classes of copulas are also required this is in analogy to gof tests for distributions in the univariate framework such as for normal distribution e g chowdhury et al 1991 choulakian and stephens 2001 zardasht et al 2015 regarding copulas genest et al 2011 proposed a specific gof test for extreme value copulas whereas durocher and quessy 2017 focused on spatial copulas when dealing with m copula one can find the test proposed by kojadinovic and yan 2011 the latter focuses only on gaussian and student copula as m copulas in addition this test is exactly the empirical copula gof test by genest et al 2006 where only the p value evaluation is based on multiplier approach they showed that the test is more appropriate for high dimensional and large sample size series which is generally not the case of hydrometeorological series the main objective of the present paper is to introduce and evaluate a new and specific gof tests for m copulas see table 1 based on multivariate l moments an extensive simulation study involving a large number of m copulas and dependence conditions is conducted to evaluate performance of the newly proposed tests and to compare with classical tests the paper is organized as follows in section 2 a short discussion of the theoretical background is presented m copulas existing gof tests and multivariate l moments the proposed multivariate l moments based gof tests are presented in section 3 whereas section 4 deals with the simulation study application to real world hydrometeorological data is represented in section 5 concluding remarks are presented in the last section 2 m copula in modeling hydrological variables in this section a brief overview of m copulas is presented for more details see e g joe 2014 in the following for the sake of brevity and simplicity only the bivariate case is considered 2 1 m copulas extreme value and archimedean m copulas are of special interest in hydrology and climatology e g zhang and singh 2007 salvadori and de michele 2010 aghakouchak 2014 indeed m copulas can cover a range of dependence from perfect positive dependence to independence and may be extended to perfect negative dependence m copulas introduce additional flexibility to the model since they include other subfamilies of classical copulas as especial cases e g salvadori and de michele 2010 de michele et al 2013 for instance the bb5 as 2 copula includes gumbel and galambos ones consequently m copulas provide a better way to capture different mutual dependencies e g chen and khashanah 2014 hence this may improve the modelling features of the dependence structure a number of archimedean m copulas are available in the literature such as bb1 bb6 and bb7 e g joe 2014 besides extreme value copulas which include bb5 as an example in the m copula category have been extensively used in recent years given their importance in modeling catastrophic events e g salvadori and michele 2011 zhang and singh 2012 for more details about properties of m copulas including those used in this study see table 2 the reader can refer to joe 2014 2 2 existing copula gof tests as indicate above several gof tests have been already proposed for copula selection in the following we focus on the tests described by genest et al 2009 as omnibus tests and recommended by berg 2009 especially those based on the empirical copula process and the kendall process in fact the other existing tests entail many shortcomings berg 2009 for example they involve many arbitrary choices e g kernel type window length weight function that make their application cumbersome furthermore they are computationally expensive for high dimensional cases for the omnibus tests their performance has not been evaluated when dealing with m copulas to the best of our knowledge this study is the first one to present a critical review and to evaluate compare their power in the case of m copulas table 1 the empirical copula based test proposed by genest et al 2009 consists in comparing the distance between the empirical copula c n and an estimation c θ n of c obtained under the null hypothesis genest and rivest 1993 proposed the kendall based test based on comparing the distance between the empirical kendall s function k n and an estimation k θ n of k obtained under the null hypothesis when dealing with one parameter copula these two gof tests have some drawbacks for instance their application is not suitable for small samples or low dependence levels genest et al 2006 berg and quessy 2009 in addition as pointed out by genest et al 2009 the kendall based test is not consistent for extreme value ev copulas since two different ev copulas have the same kendall s function barbe et al 1996 therefore the test is not appropriate to distinguish between different ev copulas e g fermanian 2013 in addition as pointed out in genest et al 2009 the empirical copula based gof test is not appropriate for small sized samples which is the case of the most of hydrological series e g laio et al 2009 2 3 multivariate l moments the multivariate l moment was introduced by serfling and xiao 2007 they provide a summary and a description of the properties and shapes of a multivariate distribution this makes them particularly useful in parameter estimation and hypothesis testing since multivariate also univariate l moments are much less biased than classical moments they are used as meaningful replacements of classical moments in a wide variety of applications mainly in hydrology climatology and meteorology analysis e g hosking and wallis 1993 chebana and ouarda 2007 kysely and picek 2007 brahimi et al 2015 the multivariate l moments are defined as bellow 1 λ k ij c o v x i p k 1 f j x j i j 1 2 and k 2 where cov is the covariance k is the order of the multivariate l moment p k 1 is the shifted legendre polynomial chang and wang 1983 x i is a random variable with marginal distribution f i for i 1 2 brahimi et al 2015 introduced the copula s multivariate l moments as 2 λ k c 12 0 1 0 1 c u 1 u 2 u 1 u 2 d u 1 d p k u 2 where c is the associated copula function the sample version of the kth copula multivariate l moments is defined in term of pseudo observations as brahimi et al 2015 3 λ k 12 c 1 n i 1 n r i n 1 p k 1 s i n 1 where r i is the rank of x i 1 among x i 1 x n 1 et s i is the rank of the concomitant x i n 12 among x i 2 x n 2 x 12 is formed by sorting x 2 in the ascending order and in turn shuffling x 1 by the order of x 2 serfling and xiao 2007 the copula multivariate l moments coefficient ratios are defined as 4 τ k 12 cop λ k 12 cop λ 2 2 for k 3 and τ 2 12 cop λ 2 12 cop λ 1 2 according to brahimi et al 2015 the vector of the multivariate copula l moments converges in distribution to the multivariate normal distribution with variance covariance matrix σ σ ij 1 i j 4 defined as 5 σ ij b i b j 1 n i j 2 1 k l n k 1 i l i 2 j k 1 j l j 2 i x k n 12 x l n 12 where b k 1 n k 1 i 1 n i 1 k x i n 12 k 0 3 3 the proposed gof tests for m copulas in this section we present the new proposed gof tests for m copulas based on multivariate copula l moments two nonparametric approaches of tests are developed see table 3 the first one is based on the distance between the multivariate l moments of the fitted copulas and the sample multivariate l moments it is inspired by the hosking and wallis 1993 gof test proposed in the univariate setting the second approach consists in generalizing the gof test developed by shih 1998 it is based on the deviation between two estimators of the parameters vector of the m copula using the multivariate l moments estimation method brahimi et al 2015 and maximum pseudo likelihood mpl estimation method kojadinovic and yan 2010 3 1 proposed gof test based on multivariate copula l moments in this section a gof test for m copulas based on multivariate copula l moments ratios is presented the main idea of the new developed gof test consists on evaluating a distance between the empirical values of τ 3 12 and τ 4 12 and their theoretical counterparts 4 associated to the m copula c θ n under the null hypothesis h 0 let ω τ 3 12 τ 4 12 be the vector of the empirical multivariate l moments ratios and ω cop τ 3 12 cop τ 4 12 cop be their theoretical counterparts from the underlying m copula the associated statistic is defined as 6 z n χ n ω ω cop t ω 1 ω ω cop where ω σ ij n λ 2 2 i j 3 4 and σ ij defined by eq 5 it is worth mentioning that for traditional gof tests the asymptotic distributions of their statistics depend on the unknown copula c θ genest et al 2009 and their p values could only be obtained via bootstrap procedures genest and rémillard 2008 hence for convenience and comparison purpose the bootstrap procedure is used to estimate p values of the proposed test asymptotic results are out of the scope of the present paper and could be the object of future work the use of the proposed statistic in 6 has several advantages including a simple formula available for the proposed statistic in terms of the ranks of the observations b the statistic involves no subjective choices such as the choice of a kernel and associated smoothing parameters c compared to conventional moments multivariate l moments are less subject to bias in estimation d multivariate l moments are able to characterize a wide range of m copulas 3 2 extension of the shih s gof test to m copulas when dealing with the one parameter clayton copula shih 1998 proposed a moment based gof test the basic idea of the test consists on the comparison between two estimators of the scalar parameter θ via kendall s τ and a weighted rank based estimator berg and quessy 2009 extended this test for any one parameter copula based on classical copula moments kendall tau and spearman rho however the case of m copula has not been investigated in this section we propose an extension of the moment based gof test to deal with m copulas the proposed gof test is then based on the comparison of two parameters vector estimates namely the multivariate l moments estimator θ lmom and the maximum pseudo likelihood estimator θ mpl as in the tests by shih 1998 and berg and quessy 2009 under the null hypothesis that the unknown copula of a population belongs to a specific family both estimators converge to the true parameters however if the assumed copula is invalid the two estimators generally do not converge to the same value white 1981 in their simplest form assume a vector of p parameters θ θ 1 θ p t let l 12 λ 1 12 λ 2 12 λ p 12 t be the vector of the first p variate l moments of the fitted copula c n θ the estimator θ lmom a 1 l 12 t is a consistent estimator of θ where a is the coefficient of the shifted legendre polynomial p k 1 brahimi et al 2015 we define the statistic z n lm as 7 z n lm n i 1 p θ lmom i θ mpl i 2 note that the statistic z n lm is defined for any d variate copulas indeed as pointed out in joe 2014 for a d variate copula the number of parameters is of the order of the dimension d up to the square of the dimension as above the corresponding p value is estimated by bootstrap the proposed gof test based on parameter estimation methods has many advantages a its statistic is free of any subjective choices which could affect the performance of the tests b compared to conventional moments multivariate l moments are less subject to bias in estimation c parameter estimates obtained from multivariate l moments are usually more accurate in small samples than the estimates obtained by other methods 4 simulation study 4 1 simulation design the objective of the simulation study is to evaluate the performance of the proposed gof tests as well as the already existing ones when the latter are able to test m copulas typically the performance of a gof test is evaluated through the estimation of first type error α level of the test as well as the test power the rejection rates of the null hypothesis in this study α is fixed to 5 as a usual value in several previous studies e g berg 2009 genest et al 2009 it was pointed out in the literature e g genest et al 2006 berg 2009 fermanian 2013 in the case of one parameter copulas that the performance of a gof test is influenced by several factors especially sample size and dependence level hence a sensitivity analysis is performed to identify the effect of these factors on the behavior of each gof test since for most flood events kendall s τ is between 0 3 and 0 8 e g zhang and singh 2007 requena et al 2013 and for comparison purpose we considered τ 0 2 0 4 0 6 as in previous gof testing studies e g berg and quessy 2009 regarding the effect of sample size n 30 50 100 are investigated the values of n are selected on the basis of situations commonly encountered in flood frequency analysis indeed barth et al 2017 reported that data examined from 1375 long term u s geological survey usgs streamgage sites have at least 30 years annual peak flow furthermore santhosh and srinivas 2013 indicated that the minimum sample size drawn from five typical peak flow records from different parts of the world is equal to 40 for all dependence scenarios m 10 000 samples are generated the latter is typically used in gof testing simulations e g genest et al 2009 data are generated through monte carlo simulations from representative and most used copulas in hydrometeorology analyses e g salvadori et al 2007 sadegh et al 2017 therefore two classes of copula family are used 1 archimedean m copulas including bb1 bb6 and bb7 and 2 extreme value bb5 copula to generate data from a given m copula we applied the procedure proposed by nelsen 2006 based on the conditional distribution method 4 2 simulation results in this section simulation results obtained by the application of different gof tests to the considered copulas are presented comparisons are given in the last part of this section 4 2 1 first type error estimates the first desirable property of a gof test is the ability to maintain the first type error close to the significance level probabilities of rejection are estimated under h0 and compared to the theoretical nominal level the effect of sample size on the level of considered gof tests is shown in table 4 for dependence level τ 0 6 for n 50 and n 100 it is found that the new developed gof tests perform well in terms of first type error since its estimation is very close to α 5 between 4 and 5 2 for sample size n 30 the two proposed tests z n lm and z n χ have slightly large first type errors between 5 7 and 7 1 this is expected since the sample l coskewness and l cokurtosis ratios τ 3 12 and τ 4 12 respectively can be slightly biased for small n serfling and xiao 2007 in addition this might be explained also by a tendency for σ 33 and σ 44 estimated by equation to underestimate the true variance of l coskewness and l cokurtosis for small n as in the univariate case kjeldsen and prosdocimi 2015 table 5 provides results of application of the proposed tests for sample size n 100 and different dependence levels from table 5 one can see that the first type errors of the proposed tests z n lm and z n χ appear in general to be closer to 5 as the dependence level increases which is in agreement with other studies e g genest et al 2009 durocher and quessy 2017 indeed for low dependence level τ 0 2 first type errors are between 6 and 10 this is expected since for low dependence level the distinctive features of the models are fuzzier e g genest et al 2009 as a consequence different tests failure to distinguish between different copulas e g berg 2009 berg and quessy 2009 mesfioui et al 2009 however the higher dependence level is τ 0 6 the better the first type errors are between 4 and 5 3 for moderate dependence level τ 0 4 the z n χ test 5 fails to hold the nominal level estimated levels above 6 this is likely due to the fact that the copula l moments are approximatively equal to zero for near independently data serfling and xiao 2007 brahimi et al 2015 4 2 2 power evaluation in order to be more informative the powers of the considered tests are presented as boxplots in figs 1 and 2 from fig 1 one can see that in most cases the proposed test z n lm shows almost the highest power for several m copulas the power of the z n lm is between 85 and 100 for sample size n 100 and dependence level τ 0 6 even for a moderate sample size n 50 z n lm has the highest power on the other hand the power of the test statistic z n χ increases with the sample size indeed large sample size not only helps to distinguish between copulas but also plays a role in the reliability of the bootstrap procedures used to approximate the associated p values as it is the case for one parameter copula berg 2009 berg and quessy 2009 moreover as pointed out in the literature of gof testing when sample size is small it is more difficult to distinguish between copulas since the dependence level affects the power of a test in a similar way as the sample size the following discussion will be brief from fig 2 for low dependence level τ 0 2 it is found that results of the developed gof tests are satisfactory indeed z n lm has the highest power between 51 and 85 and z n χ power estimates are between 40 and 69 from fig 2 it is found that the power of the proposed gof tests appear to improve as the dependence level increases the same pattern is observed in gof testing literature this can be explained by the fact that when dependence level increases the copula structure is more distinguishable e g berg 2009 however when the dependence level is too small the dependence structures are hardly distinguishable 4 2 3 comparisons a comparison of the performance between the proposed tests and the classical ones is made through boxplots figs 1 and 2 according to fig 1 the power estimates of all statistics increase when n becomes larger this agrees with similar findings by berg 2009 and genest et al 2009 made for one parameter copula gof tests however z n χ is less influenced by n with only a difference about 2 when n increases from 30 to 100 this can be explained by the fact that multivariate l moments are robust and suffer less from the effects of sampling variability brahimi et al 2015 nevertheless figs 1 and 2 show that the two classical statistics s n c and s n k are not able to distinguish the different m copulas the associated powers of these tests are between 9 and 27 for n 30 and between 20 and 69 for n 100 for s n k this is unsurprising since the kendall function could be the same for two different copulas e g nelsen 2006 on the other hand s n c behaves in the same way as in the case of one parameter copula for m copula low power estimates may be caused by the small sample size n 1 5 0 genest et al 2006 the most striking result from fig 2 is that the tests s n c and s n k have lower powers even for strong dependence τ 0 6 when dealing with m copulas a possible explanation in the case of the statistic s n k might be that as the number of parameters increases the univariate summary represented by the probability integral transformation and its distribution function k is less representative of the multivariate dependence structure in the case of s n c it might be related to the fact that the sample size is relatively small even n 100 and as justified by genest et al 2006 this test has high power for n 150 from fig 1 different gof tests can be ranked in term of power as z n lm z n χ s n k s n c over all for a given n and τ the proposed tests have higher powers than those of the classical tests in fact since the shape of copula structure can be described by their multivariate l moment matrices serfling and xiao 2007 the proposed statistics allow a better differentiation of various copulas note that the gof test z n lm based on parameters estimation method has all times the best performance this may be due to inconsistency of the test in fact the parameter estimates by the mpl and multivariate l moments may be identical as supported by berg and quessy 2009 in the case of moments based gof test and so the test will accept more often the null hypothesis one wants to study the performance of the proposed gof tests when dealing with one parameter copulas table 6 provides corresponding results obtained from the proposed tests and the classical ones the proposed tests z n lm and z n χ perform well in terms of power compared to the classical tests the power estimates for the proposed tests are between 45 and 87 for classical gof tests power estimates are between 33 4 and 84 3 noticeable exception is made in the case of z n χ statistic which is outperformed by the classical tests when dealing with the first type errors 5 illustrative case studies after evaluating and comparing the proposed gof tests on simulated data in this section we apply these tests on real world hydrometeorological data a comparison is also made with the classical kendall and empirical gof tests the purpose of these applications is to illustrate the practical use of the proposed tests in order to select an appropriate copula family the first application deals with flood events and the second with meteorological drought note that these two application are characterized by different record lengths dependence level and dependence shape 5 1 bivariate flood frequency analysis to scrutinize the impact of length of data on the performance of the gof tests we use two flood peak q m3 s and volume v m3 data series the first one from the romaine station located at the province of quebec canada for the period 1984 2010 the second data set corresponds to the ankang hydrological station at the upper hanjiang river china the same series is used in xiong et al 2015 for the period 1950 2011 since the focus of the study is on the dependence structure the choice of marginal distributions is performed but not presented first the dependence analysis between variables is carried out by dependence measures in order to discard inappropriate copulas the kendall s τ is adopted to measure a quantitative value of the dependence structure kendall s τ is 0 30 and 0 8 respectively for romaine and ankang stations for ankang station this value supports the positive and strong dependence between q and v however for romaine station the dependence is relatively low hence copulas with dependence range not matching with those of the series are discarded and four copulas are retained for subsequent analysis m copulas bb1 and bb5 and one parameter copulas gumbel and galambos besides quantitative measures of the dependence graphical analysis of the dependence is conducted scatter plots and kendall s function plots are used to evaluate if a chosen copula fit properly the observed data a set of 10 000 synthetic pairs are generated from each one of the considered copulas then the synthetic pairs are transformed by using univariate marginal distributions from fig 3 a presenting scatter plots for romaine station we can see that extreme value copulas i e bb5 and galambos are sharper in the upper right corner than archimedean copulas bb1 and gumbel which are more scattered this figure shows also the positive lower tail dependence of the bb1 copula we can conclude that extreme value copulas discard the largest observations whereas the bb1 copula covers these observations in the generated sample as its dependence structure is more spread in the upper tail as expected we can see from fig 4 a that the distance between the empirical and theoretical kendall s function is greater for extreme value copula than for archimedean copula this analysis shows that extreme value copulas are not suitable for fitting romaine data consequently bb1 copula seems to be the best fitting copula however a further analysis is needed to confirm this preliminary conclusion to select the best fitting copula in a formal way the above described gof tests are applied gof test statistics and associated p values are summarized in table 7 it can be seen that bb1 copula is accepted by the proposed tests z n lm and z n χ but rejected by the classical tests s n c and s n k the gumbel copula is accepted by proposed tests as well as s n k however s n c rejects all copulas except the galambos one indeed as indicated above this test is affected by small sample size n 26 it is apparent from table 7 that s n k is unable to distinguish between different extreme value copulas gumbel bb5 and galambos indeed regarding s n k all considered extreme value copulas are accepted as a good candidate for fitting the romaine data as they have the same kendall s function genest and boies 2003 for z n lm all copulas are also accepted this can be explained by the inconsistency of the test therefore it is important to bear in mind that the parameter estimates by the mpl and multivariate l moments may be identical even if the copula not fit well the data e g berg and quessy 2009 in order to select the best fitting copula among those accepted by different gof tests the model selection criterion aic is considered sadegh et al 2017 accordingly the bb1 copula is selected as the most appropriate copula for q v of the romaine station as a reminder the bb1 copula is accepted by new proposed tests whereas it is rejected by the classical tests in the case of the ankang data as shown in fig 3b one parameter copulas gumbel and galambos copula and the two parameter bb5 copula contains the gumbel and galambos copulas display rather similar behaviour in terms of scatter plots these copulas reproduce suitably the dependence in the upper extremes moreover bb1 copula reproduces suitably the dependence in the upper and lower tails since its structure is more spread from fig 4b theoretical kendall s function of the bb1 and gumbel copulas are closer to the empirical kendall s function than for extreme value copulas then bb1 and gumbel copulas could better fit the ankang data as showed in table 7 bb1 copula is accepted by the proposed tests z n lm and z n χ while it is rejected by the classical tests s n k and s n c bb5 and galambos copulas are accepted by all gof tests however gumbel copula is accepted by z n lm and s n k while it is rejected by z n χ and s n c note that for ankang station compared to romaine station the sample size is large and dependence level is high hence it would be easier to distinguish between the different copulas according to aic the bb1 copula could be selected as the best one to fit data these applications show in a practical and complimentary way to the simulation results the advantage of using the new proposed tests especially when dealing with small sample size and low dependence level in addition these applications confirm the ability of the proposed tests to distinguish between different extreme value copulas with respect to a kendall approach which necessarily shrinks into the parameter all the information about the ev copula the main objective of this study is to deal with hydrological series characterized by small sample size as pointed out through the simulation study and the application to real world data this objective is reached with the proposed tests 5 2 bivariate drought analysis in this section we focus our attention on drought analysis with an emphasis on the use of gof tests for the best fitting copula selection since drought is one of the most damaging natural hazards its prediction is of high importance for risk assessment and decision making droughts can be of meteorological agricultural hydrological or socio economical nature in this study we focus on meteorological droughts caused by a deficit in precipitation and a lack of soil moisture hence we employ the multivariate standardized drought index msdi proposed by hao and aghakouchak 2013 where a sequence of negative msdis indicates a drought event in this application we used monthly precipitation and soil moisture data from 1948 to 2017 obtained from the climate prediction center cpc for climatic division 1 in northern california different copulas are used to construct the joint probability distribution of precipitation mm day and soil moisture mm anomalies defined as deviations from the 1971 to 2000 monthly climatology to scrutinize the impact of length of data on the dependence structure and the performance of the underlying gof tests we use a subset of recent 30 years 1988 2017 of annual data in addition to the original 70 years 1948 2017 of record fig 5 shows the scatter plots and different fitted copula to describe the dependence structure between precipitation and soil moisture anomalies using 70 years of annual fig 5a and 30 years of annual fig 5b observed data it is worth noting that the two data sets have relatively small dependence strength the kendall τ is 0 48 and 0 35 for data set of length 70 and 30 years respectively copulas with dependence strength not matching with those of the series are discarded five copulas are retained for subsequent analysis m copulas bb1 and bb5 and one parameter copulas gumbel frank and clayton from fig 5a in the case n 70 a visual inspection of different copulas fitted to the data shows that only the clayton and frank copulas are able to characterize the dependence structure of this data set however from fig 6 a theoretical kendall s function of the bb5 and clayton copulas are closer to the empirical kendall s function than other copulas then bb5 and clayton copulas could better model the dependence between precipitation and soil moisture anomalies when the original data record is considered n 70 results regarding the application of the proposed gof tests as well as classical tests are summarized in table 8 for the 70 years data set neither clayton frank nor gumbel copulas could be rejected considering the p values by z n lm and s n c it is also notable from table 8 that the proposed statistic z n χ accept only the bb5 and clayton copulas as good candidates to fit data it is also important to highlight that the bb5 copula is rejected by the classical statistics s n k and s n c recall that bb5 and clayton copulas provide a very good fit to the data regarding the kendall plot fig 6a since p values cannot be used to rank copulas but only to accept or reject them e g salvadori and michele 2011 we used the criterion aic for ranking copula that passed gof tests consequently both clayton and bb5 copulas could be chosen as the best fitting copula since they have very closer aic in the case of small sample size the graphical diagnostics of the dependence structure fig 5b and 7b in order to select the appropriate copulas become difficult from fig 5b we can see that all fitted copula could be a good candidate this behavior is replicated in fig 6b where different theoretical and empirical kendall s functions are close to each other this highlights the importance of using formal gof tests when dealing with small sample size n 30 based on z n χ and z n lm statistics bb5 gumbel and frank copulas seem to fit properly the data tables 8 based on s n k all considered copulas are able to represent the dependence structure except the bb1 copula whereas none these copulas is accepted by s n c this might be explained by the inconsistency of these two tests for small sized sample genest et al 2006 according to aic criterion bb5 copula is selected as the best copula followed by gumbel copula 6 conclusion the proposed gof tests are the first ones designed specifically to deal with m copulas a novelty of this paper lies in the introduction of multivariate l moments in gof copula testing simulations results show that the proposed tests are good candidates to control first type errors and overall they have the highest performance in terms of power even for small samples and weak dependence furthermore the obtained results from two different case studies confirm the advantages of the proposed tests especially when dealing with small sample sizes as well as when considering several extreme value copulas declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
6038,drought detection is an essential process for drought risk management and watershed sustainability describing a reliable predictive model for drought events has always been the motivation of the meteorology scientists the current research proposes a functional time series analysis for the construction of a reliable predictive strategy of drought interval occurrences the proposed model is validated with respect to its performance based on the palmer drought severity index pdsi the applied methodology is conducted to produce point and interval forecasts for the drought time series pattern the modeling procedure is inspected on the konya watershed central anatolia turkey the study is incorporated a second phase of modeling where entertained for heteroscedastic and seasonal hydro climatic data streamflow several statistical metrics and graphical presentation are computed for modeling evaluation two benchmark models are developed for modeling evaluation including linear exponential smoothing les and autoregressive integrated moving average arima in quantitative terms for instance the proposed functional principal component analysis fpca model exhibited a remarkable drought interval prediction enhancement over the les and arima models by approximately 29 5 and 28 3 using the root mean square error rmse metric at aksaray station in accordance with the attained results the proposed methodology demonstrated a reliable predictive model for capturing future pdsi intervals and streamflow magnitudes keywords drought functional data konya basin pdsi time series 1 introduction history has indicated that no matter the size or social status of human settlement drought is inevitable as a natural occurrence drought can be experienced in all climatic zones asadi et al 2015 drought is experienced whenever there is a negative deviation from the normal level of precipitation in a given region over a given period however drought has been defined in many modes in meteorology drought is specifically referred to a deficit in the level of precipitation and this is the focus of our predictions pasho et al 2015 in agriculture drought occurs when there is insufficient level of soil moisture to support plant growth farooq et al 2009 in hydrology drought simply means the unavailability of bulk water hao et al 2016 changes in the precipitation regime have been attributed as the major cause of drought but other climatic factors such as high winds and temperatures can equally increase drought severity hazard is reported to be naturally happened when its event occurs without any human input and negatively impacts people or the environment wilhite 2000 drought as a form of naturally occurring hazard is further intensified by some human factors such as the increased demand for water due to the increasing global population mishra and singh 2010 several atmospheric and hydrologic processes that contribute to atmospheric moisture have an influence on the occurrence and severity of the drought whenever there are dry hydrologic conditions drought positive feedback mechanisms set in due to the depletion of the upper soil layer moisture and lower levels of evapotranspiration kim et al 2015 these processes contribute to the reduction in the relative atmospheric humidity the chance of a low level of precipitation is increased at lower levels of relative humidity drought conditions can only be relieved by events that can transfer enough moisture from outside to the dry region such moisture levels must be strong enough to cause sufficient precipitation the impact of drought conditions can be sensed in many areas of society ranging from regional to global economic impacts it can even be felt in areas that are not directly affected by the drought itself being that almost half of the earth s terrestrial surface is susceptible to drought it can be considered as a widespread phenomenon similarly the location of most of the major agricultural lands made them susceptible to drought events drought is a transient and recurrent meteorological phenomenon arising from the absence of long term rainfall and causes severe socio economic problems in the regions where it has occurred mishra and singh 2010 it has widespread and significant impacts on many areas such as economy environment industry agriculture health and human life keyantash and dracup 2002 drought monitoring and forecasting play an important role to mitigate its impacts and consequences for instance decision makers may plan and manage the water resource systems and long term economic and social planning wilhite et al 2007 the severity and spatial extent of the drought are generally measured by drought indexes examples are the palmer drought severity index pdsi standard precipitation index spi palmer hydrological drought index and palmer crop moisture index which are common and frequently used in the literature guttman 1999 vicente serrano et al 2010 analyzing and predicting drought event is an important problem and thus has received great attention over the past two decades drought analysis is done with various statistical and data driven models and the severity intensity and duration of the drought are tried to be characterized for example bonaccorso et al 2003 studied drought analysis using principal component analysis pca another research conducted by dinpashoh et al 2004 where drought analyzed with several climatic and geographical variables by procrustes analysis and l moment based models paulo et al 2005 developed markov chain modeling strategy to estimate the probability of different drought severity classes the expected time in each class of severity the recurrence time to a particular drought class and the expected time for the spi to change from a particular class to another moreira et al 2006 investigated a log linear model to investigate differences relative to drought class transitions among time periods mishra and desai 2006 evaluated various data intelligence models including linear stochastic recursive multi step neural network and direct multi step neural network for drought forecasting another attempted on the data intelligence implementation conducted by mishra et al 2007 the authors proposed a new hybrid intelligence model by combining a linear stochastic model and a non linear artificial neural network model for drought forecasting bacanli et al 2009 investigated the feasibility of adaptive neuro fuzzy inference system for drought forecasting and quantitative value of spi drought indices marj and meijering 2011 proposed an artificial neural network based model for agricultural drought forecasting belayneh et al 2014 established different data driven models for long term drought forecasting most recently a residual based bootstrap model was constructed to predict drought indices intervals beyaztas et al 2018 the potential effects of drought have made it necessary to accurately predict their occurrence with sufficient lead time in order to avoid its consequences both physical and conceptual models can be used to predict drought the conceptual models are mainly deployed in hydrologic forecasting because they require less information accurate in various hydrological applications and easy to build ali et al 2018 mouatadid et al 2018 due to recent technological advances the process of collecting data is becoming ambidextrous causing high dimensional and complex data structure such data type sampled over time space or other continuum measures can be characterized as functional data and can be visualized analyzed modeled and predicted by functional data analysis fda which is a relatively new area in statistics fda considers the data in the form of curves which eliminates the problem of the high number of variables and focuses on temporal dependence between these curves when analyzing the functional data stadtmler and zampiceni 2015 it has several advantages over the classical multivariate statistical analysis models ferraty and vieu 2006 for example fda allows us to look at functional data as a whole rather than a single dependent variable provides additional information about the data such as smoothness and derivatives bypasses the missing data problem by interpolation and smoothing techniques effectively reduces the data noise by smoothing techniques can be used for the irregularly sampled time series data also contrary to classical time series models the fda does not suffer from the high correlation problem between the repeated measurements since it considers the whole curve as a single entity modeling functional time series using fda techniques has received increasing attention in the literature some selected examples of useful fda implementations analyzing of handwriting in chinese ramsay 2000 medical applications pfeiffer et al 2007 modeling and forecasting of breast cancer mortality rate erbas et al 2007 call volume forecasting shen and huang 2008 modeling and forecasting of fertility rates hyndman and shang 2009 forecasting of sea surface temperature shang and hyndman 2011 modeling internet users behaviours kosiorowski 2014 and several other related science and engineering applications ferraty 2011 provided an excellent overview of research and application on functional data analysis an extensive review application for the fda has been conducted by ramsay and silverman 2002 recently within the field of hydrology several successful investigations have been conducted on the feasibility of the fda approach suhaila et al 2011 converted daily and monthly rainfall time series data into a smooth curve where it represents a continuous rainfall pattern of peninsular malaysian region using the fda approach chebana et al 2012 explored the concept of the frequency analysis to inspect the hydrograph curves in which represented as a functional data set the examined functional data approach provided an effective and efficient tool for estimating the risk associated with the flood extreme events adham et al 2014 assessed the potentiality of watershed runoff based on the identification of the curve number soil conservation service and functional data analysis the aim of the study was to convert the runoff data into a smooth curve for better watershed analysis and assessment masselot et al 2016 forecasted streamflow pattern using functional data analysis approach the proposed model attained a more informative understanding of the streamflow trend in comparison with the artificial neural network model ternynck et al 2016 introduced the feasibility of the statistical fda approach for watershed streamflow pattern classification the proposed model provided a better understanding of flood hazard and hydrological phenomena most recently curceac et al 2019 forecasted the air temperature hydrological process using the capacity of the fda approach the forecasting demonstrated a superior result over the seasonal autoregressive moving average model based on the exhibited literature functional data analysis approach evidenced its potential in the field of hydrology engineering however the viability of the fda yet to be explored for drought interval simulation in the current research and for the best knowledge of the authors i fda based model is developed to visualize and analyze the drought interval observations ii the study is adopted to produce point and interval forecasts for the drought series iii finally with the aim to investigate the generalization capability of the proposed fda model the streamflow pattern of the inspected case study is simulated for numerical analysis the modeling performance of the proposed method was evaluated using pdsi based drought observations among others pdsi which is created by palmer 1965 provides standardized measurements of moisture conditions to measure the drought of a region it is based on a supply and demand model of water balance equation and calculated based on the precipitation temperature and the local water content of the soil for more information about the pdsi readers might refer to the following reference alley 1984 2 methodology functional data consists a random functions and each function represents a sample element which is recorded at discrete time points let t t 1 t 2 t k denote the time points where the discrete data x t 1 x t 2 x t k observed then a functional data consisting of n functions x 1 t x 2 t x n t where x t t t 1 t k can be represented as in table 1 the functions or curves are constructed by discrete data points observed at t 1 t 2 t k and it is assumed that each function x t is continuous and smooth functional time series fts is a special case of functional data and arises when the functional data objects are collected sequentially over time in other words let x t t 1 2 n be a sequence of functional observations then it is called as fts if each x t is a random function x t τ where τ is a continuous variable defined in the interval a b τ a b contrary to r n where multivariate analysis works on fda considers that the functions are the elements of a separable hilbert space metric space semi metric space or the banach space which is the space of all continuous functions throughout this study it is assumed that the functional random variable x t is a second order stochastic process with finite second order moment please see ramsay and dalzell 1991 for the theoretical details on fda several models including fpca have been proposed to analyze fts fpca is used for two main purposes i dimension reduction and ii capturing the principal mode of variations it reduces the dimensions of the data by transforming the data to uncorrelated principal components and the first few of them keep most of the variations presented in the data while doing so fpca decomposes the covariance operator into an orthogonal basis of eigenfunctions which are used to describe the variations in the data in more detail let x i t for i 1 2 n and t t be a sample of fts let also ν j and λ j for j 1 2 represent the jth eigenfunction and jth eigenvalue respectively the covariance operator is decomposed by the following orthogonal expansion 1 c t s j 1 λ j ν j t ν j s t s t eigenfunctions are obtained by maximizing the variance max ν var t x i t ν t dt with ν 2 1 for i 1 2 n the jth principal component scores with zero mean and variance λ j are defined as 2 ξ ij t x i t ν j t dt the karhunen loeve expansion is used to express the random curves as follow 3 x i t j 1 ξ ij ν j t t t as a summary fpca reduces the dimension by decomposing x i into orthogonal components ξ ij ν j the variation in x i in the direction of ν j is measured by the eigenvalue λ j and only the first few components retain most of the variations in x i for more information about fpca and its practical demonstration please see ramsay and silverman 2002 and ramsay et al 2009 2 1 forecasting functional time series throughout this study the functional principal component regression fpcr proposed by hyndman and shang 2009 is considered for forecasting fts to start with for i 1 2 n and t t let x i t denote a function 4 x t f t σ t ε t where ε t is an independent and identically distributed iid random variable with zero mean and unit variance σ t is allowing the amount of error to wary with t and f t is assumed a continuous smooth function if μ ν j and ξ j represent the mean function principal component function and principal component score of the jth principal component function respectively then the function f can be decomposed as follows 5 f μ j 1 ξ j ν j e where e is an iid random function let f t f 1 t f 2 t f n t represent a sample of functional data which can be decomposed as 6 f i t f t j 1 j ξ i j ν j t e i t σ i t ε t where f t n 1 i 1 n f i t is the estimate of μ ν j is the jth estimated eigenfunction ξ i j t f i t f t ν j t dt is the jth principal component score for year i ε t is the residual and j where j n is the optimal number of components determined by cross validation please see hyndman and booth 2008 to see the effect of j on the forecast accuracy hyndman and shang 2009 emphasize that each ξ i j for j 1 2 j can be considered as a univariate time series and can be forecasted independently since they are uncorrelated the unobserved future curves can be estimated by multiplying the forecasted principal component scores with the principle components let h 1 2 denote the forecast horizon then the h step ahead forecast of function x i t conditional on the observed smoothed functions f t f 1 t f 2 t f n t and the estimated functional principal components ν t ν 1 t ν 2 t ν j t are obtained as follows 7 x n h n t e x n h t f t ν t f t j 1 j ξ n h n j ν j t where the h step ahead forecast of ξ n h j ξ n h n j are calculated using univariate time series forecasting models such as linear exponential smoothing les auto regressive ar model auto regressive integrated moving average arima or random walk hyndman and khandakar 2008 the forecasts variance are approximated as follows 8 ζ n h t var x n h t f t ν t μ 2 t j 1 j s n h j ν j 2 t ψ t σ n h 2 t where s n h j var ξ n h j ξ 1 j ξ 2 j ξ n j are calculated from the time series model the model error variance ψ t is estimated as n 1 i 1 n ε i 2 t for t t and μ 2 t and σ n h 2 t can be obtained using a non parametric smoothing technique out numerical results which are discussed in detail in section 4 revealed that the arima and les models performed significantly better compared to ar and random walk therefore throughout this study the forecasts are obtained using arima and les methods only briefly arima transforms the time series data into a stationary form and uses them to construct the time series model then based on the constructed model it forecasts the unobservable future observations using a recursion linear exponential smoothing on the other hand gives weights to all observations larger weights are assigned to recent observations and weights decrease exponentially as the observations become older it uses the following recursion to calculate the forecasts x t h t ℓ t b t h s t m h m where m ℓ t b t and s t denote the seasonality length level of the univariate time series growth and the seasonal component respectively for more information about arima and les models please see hyndman and khandakar 2008 2 2 bootstrap prediction intervals as pointed by hyndman and shang 2009 there are four errors sources that must be taken into account when using functional principal component regression the first two sources of errors occur in estimating f i t and μ t the other two sources occur due to forecasting principal component scores ξ i j and predicting model residuals ε i t hence the point forecasts may not adequately capable of high forecasting accuracy since the uncertainty associated with each point forecast is not known in general on the other hand prediction intervals can provide reasonable inferences by taking into account the uncertainty associated with each forecast once the forecast variance ζ n h t obtained the 100 1 α prediction interval for x n h t can be calculated as x n h n t z α 2 ζ n h t where z α 2 is the α 2 th quantile of the standard normal distribution however this procedure may still not be enough to obtain accurate prediction intervals since it is based on the normality assumption which is generally unknown in practice thus such prediction intervals can be affected due to departure from the normality assumption and may lead us to unreliable results as an alternative the bootstrap method can be used to construct prediction intervals without considering distributional assumptions in the context of forecasting the main idea behind this method is to approximate the distribution of future observations by drawing the resamples conditionally on the available data in this study the non parametric bootstrap procedure proposed by hyndman and shang 2009 is used to obtain prediction intervals for a sample of functional data f t f 1 t f 2 t f n t let ξ i j i 1 2 n and j 1 2 j denote the fitted principal component scores also let ξ i i h j and ψ i j h ξ i j ξ i i h j for i h 1 n denote the h step ahead forecasts of ξ i j and the corresponding forecast errors respectively then the following describes the bootstrap algorithm considered in this study step 1 obtain the bootstrap sample of ξ n h j as ξ n h n j ξ n h n j ψ i j h where ψ i j h is a random sample from ψ i j h step 2 assuming the data is well approximated by the first j principal components obtain the bootstrap sample of model errors e t defined in eq 6 e t by sampling with replacement from e 1 t e 2 t e n t also obtain the bootstrap sample of smoothing errors ε n h by sampling with replacement from the set ε 1 ε 2 ε n step 3 obtain the h step ahead forecast of function x t as follows x n h n t f t j 1 j ξ n h n j ν j t e n h n t σ n h t ε n h step 4 obtain bootstrap replicates x n h n 1 t x n h n 2 t x n h n b t by repeating steps 1 3 b times for each h note that b denotes the number of bootstrap replications let g ℓ p x n h n t ℓ be the bootstrap distribution function of unknown distribution function of x n h t then the 100 1 α bootstrap prediction interval for x n h t is obtained as g 1 α 2 g 1 1 α 2 2 3 prediction evaluation metrics the applied models fpcr arima and les are validated statistically using several performance metrics including root mean squared errors rmse determination coefficient r 2 percent bias p bias nash sutcliffe efficiency nse and modified index of agreement mia yaseen et al 2018 yaseen et al 2018 krause et al 2005 larabi et al 2018 the mathematical formulation can be expressed as follows 9 rmse i 1 n x i t x i t 2 n 10 r 2 i 1 n x i t x i t x i t x i t 2 i 1 n x i t x i t 2 i 1 n x i t x i t 2 11 p bias i 1 n x i t x i t i 1 n x i t 100 12 nse 1 i 1 n x i t x i t 2 i 1 n x i t x t 2 13 mia 1 i 1 n x i t x i t i 1 n x i t x t x i t x t where x i t and x i t respectively denote the observed and predicted functions for i 1 n and x t 1 n i 1 n x i t and x t 1 n i 1 n x i t denote the means of the observed and predicted functions respectively 3 case study and data description the coordinates of konya watershed basin lie between 36 51 and 39 29 north latitudes and 31 36 and 34 52 east longitudes in the central anatolia region of turkey the basin has a surface area of about 54000 km2 covering almost 7 of turkey as illustrated in fig 1 the altitude of the basin ranges from 900 to 3534 m to the north of the basin lies sakarya and kizilirmak basins while kizilirmak and seyhan form the east border of the basin to the south of the basin lies the eastern mediterranean basin while antalya and akarcay basins lie in the west of the basin surrounding the basin are the city centers of aksaray nigde konya karaman isparta antalya ankara and nevsehir the basin is positioned at the region with the least annual rainfall in the whole of turkey its total annual precipitation ranged between 286 740 mm the span of the investigated dataset covered the period jan 2007 dec 2015 the occurrence of drought in this arid semi arid region has been observed to cause serious socio economic problems some of the notable problems of drought in this area include irrigational agricultural practices the existence of undocumented water wells loss of natural wetlands functions as a result of prolonged conservation strategies groundwater level reduction as well as loss of soil and habitat salinization table 2 presents a detailed information about the konya basin stations used in this study 4 application and results discussion the main enthusiasm of the present study is to develop a reliable mathematical model where drought interval can be predicted accurately in this section the application findings of the fpcr model are discussed and validated with those of traditional arima and les models the applied modeling strategy is constructed over the mediterranean region where drought events are a highly stochastic pattern hoerling et al 2012 based on the available datasets at each investigated station there are 46 years of observations n 46 x i t k i 1 2 46 and the number of months is k 12 k 1 2 12 the ith observation x i t k represents the monthly pdsi values for the ith year at first the data set is converted to the smooth curves x i t t t 1 12 by a smoothing spline method where the smoothing parameter controlled by generalized cross validation the plots of the raw and functional pdsi series are drawn in figs 2 and 3 respectively here the smoothed pdsi series were plotted by rainbow r package proposed by shang 2011 in these figures the pdsi series from distant past years are shown in red while the most recent years are shown in violet the figures presented the general behaviors of the pdsi series over the studied period span note that aksaray and karaman stations are demonstrated a limited pdsi magnitude between 5 with obvious increment reached 7 during the summer season july and august on the other hand seydisehir and karapinar stations are behaved differently in accordance with their coordinates with limited pdsi ranged between 4 5 with noticeable augmentation during july and august for karapinar station the highest densities magnitudes of the pdsi series illustrated in fig 4 where the pdsi values are ordered by the highest density regions in this figure the red curves which gathered in the center of the data are denoted the most common series whereas the violet curves are presented the most outlying ones in order to obtain out of sample bootstrap prediction intervals the monthly pdsi series split into the following two parts i the model is constructed based on the first 45 years 1970 2014 of data observations to compute the 12 steps ahead predictions for the targeted year i e 2015 ii the result is validated against the observed values the results of the fpca are indicated that the first j 2 principal components explain more than 80 of total variability for each station the actual values are 95 94 82 and 92 for aksaray karaman seydisehir and karapinar stations respectively thus for the fpcr only the first j 2 principal components are used to construct the model the scatter plots of the observed vs fitted pdsi functions observations are given in fig 5 this figure is shown that the observed pdsi series are better predicted by the fpcr compared to both arima and les models several statistical metrics are computed to validate the potential of the applied fpcr and the comparable arima and les models table 3 is reported the numerical results of the rmse r 2 p bias nse and mia based on the exhibited results fpcr model has attained the minimal magnitudes of the rmse and the highest determination coefficient and nash sutcliffe efficiency metrics the r 2 and nse metrics are ranged between 0 77 and 0 93 for all the inspected stations the proposed fpca model is demonstrated a noticeable drought interval prediction enhancement over the les and arima models by approximately 29 5 and 28 3 27 8 and 26 6 22 8 and 19 2 28 1 and 26 2 at aksaray karaman seydisehir and karapinar stations respectively using the rmse metric the p bias values for the inspected stations e g karaman seydisehir and karapinar using fpcr model are 0 0004 0 0065 and 0 0001 whereas at aksaray station p bias 0 0047 using the same concept of correlation graphical scatter plots of the inspected stations are displayed in fig 5 the variance results toward the fitted line observed vs fitted pdsi values are showed that the fpcr model is provided better approximations compared to arima and les models compared to other stations the pdsi series at seydisehir station has the smoothest shape see fig 3 thus any deflation in the predicted series produces the highest error for the pdsi series of seydisehir station among others consequently all the methods tend to produce their worst performances for this station the experienced low prediction performance at the seydisehir station might be due to the location of this station that is influenced by the nearby climatic region hence more insightful climate information can be incorporated for modeling the drought interval at this station in the future research the out of sample prediction intervals are constructed based on the b 1000 bootstrap replications and the nominal level α is set to 0 05 to construct a 95 prediction interval the results are presented in fig 6 this figure is demonstrated that the bootstrap prediction intervals obtained using both fpcr based model and traditional arima and les are comparable and included all of the observations of the pdsi values only the intervals obtained by traditional les fails to cover two pdsi observations points 7 and 8 the results are exhibited the bootstrap method based on arima has narrower prediction intervals than those obtained using the les model since the latter produces more variable results also the results showed that the traditional models produced less uncertain forecast intervals for short term forecasts compared to those of fprc while this is vice versa for long term forecasts moreover fig 6 is suggested that the point forecasts are not consistent with the observed values in all cases the second phase investigation of the current study is adopted for the monthly streamflow pattern detection the data consisted of a period of 45 years jan 1967 dec 2011 of observations obtained from the black sea basin in turkey compared to the pdsi data it is very much difficult to obtain a reliable prediction interval for the streamflow pattern since it presents heteroscedasticity and seasonality characteristics the time series of the functional data and highest density plots for the streamflow data are plotted in fig 7 the figure is exhibited that the raw data are well smoothed by the smoothing spline method similar to the pdsi data the modeling is performed using the first 44 years of observations while the obtained 12 steps ahead predictions belong to the year of 2011 for this data the results of fpca is suggested to use the first j 3 principal components which explain 90 of the total variation to construct the fpcr model the scatter plots of the observed vs fitted streamflow observations are given in fig 8 this figure is demonstrated that the streamflow data are better predicted by the fpcr compared to traditional arima and les methods again the implementation of the streamflow pattern prediction is validated using the reported statistical performance metrics table 4 is reported the numerical results fpcr model is demonstrated the superior over the traditional models the 95 bootstrap prediction intervals for the streamflow data are given by fig 9 this figure is indicated that the bootstrap methodology is also capable of producing valid prediction intervals for the heteroscedastic as well as seasonal hydro climatic variables however the same is not true for the traditional models as is shown by the left panel of fig 9 5 conclusion the current study was adopted on the drought interval observation prediction using the potential of functional data analysis time series models to visualize pdsi component the model was constructed in parallel with bootstrap prediction intervals for future magnitude the proposed model was verified the feasibility of the bootstrap method for attaining an acceptable prediction for the future pdsi interval values the investigation was extended to cover the streamflow forecasting of the same region for validation purpose two classical models i e les and arima were developed based on the attained statistical analysis the proposed fpca based model provided reliable predictability for the drought and streamflow pattern simulation with an acceptable determination coefficient and absolute error metrics values for the drought prediction fpca lse and arima models attained r 2 0 9349 0 8715 0 8702 0 9285 0 8677 0 8676 0 7773 0 6755 0 6763 0 8913 0 8052 0 8056 for aksaray karaman seydisehir karapinar stations respectively whereas the best prediction of streamflow was achieved r 2 0 9861 0 4197 and 0 6389 using fpca les and arima models respectively with minimum value of p bias 0 0001 for fpca model as a future research devotion the performances of the functional time series methodology can be studied for other drought indices such as standard precipitation index palmer hydrological drought index and palmer crop moisture index in addition the model further can be used in other hydro climatic variables such as surface air temperature evaporation evapotranspiration and precipitation declaration of competing interest the authors have no conflict of interest acknowledgement authors are very much thankful for the hydrological data provider turkish state meteorological service mgm 
6038,drought detection is an essential process for drought risk management and watershed sustainability describing a reliable predictive model for drought events has always been the motivation of the meteorology scientists the current research proposes a functional time series analysis for the construction of a reliable predictive strategy of drought interval occurrences the proposed model is validated with respect to its performance based on the palmer drought severity index pdsi the applied methodology is conducted to produce point and interval forecasts for the drought time series pattern the modeling procedure is inspected on the konya watershed central anatolia turkey the study is incorporated a second phase of modeling where entertained for heteroscedastic and seasonal hydro climatic data streamflow several statistical metrics and graphical presentation are computed for modeling evaluation two benchmark models are developed for modeling evaluation including linear exponential smoothing les and autoregressive integrated moving average arima in quantitative terms for instance the proposed functional principal component analysis fpca model exhibited a remarkable drought interval prediction enhancement over the les and arima models by approximately 29 5 and 28 3 using the root mean square error rmse metric at aksaray station in accordance with the attained results the proposed methodology demonstrated a reliable predictive model for capturing future pdsi intervals and streamflow magnitudes keywords drought functional data konya basin pdsi time series 1 introduction history has indicated that no matter the size or social status of human settlement drought is inevitable as a natural occurrence drought can be experienced in all climatic zones asadi et al 2015 drought is experienced whenever there is a negative deviation from the normal level of precipitation in a given region over a given period however drought has been defined in many modes in meteorology drought is specifically referred to a deficit in the level of precipitation and this is the focus of our predictions pasho et al 2015 in agriculture drought occurs when there is insufficient level of soil moisture to support plant growth farooq et al 2009 in hydrology drought simply means the unavailability of bulk water hao et al 2016 changes in the precipitation regime have been attributed as the major cause of drought but other climatic factors such as high winds and temperatures can equally increase drought severity hazard is reported to be naturally happened when its event occurs without any human input and negatively impacts people or the environment wilhite 2000 drought as a form of naturally occurring hazard is further intensified by some human factors such as the increased demand for water due to the increasing global population mishra and singh 2010 several atmospheric and hydrologic processes that contribute to atmospheric moisture have an influence on the occurrence and severity of the drought whenever there are dry hydrologic conditions drought positive feedback mechanisms set in due to the depletion of the upper soil layer moisture and lower levels of evapotranspiration kim et al 2015 these processes contribute to the reduction in the relative atmospheric humidity the chance of a low level of precipitation is increased at lower levels of relative humidity drought conditions can only be relieved by events that can transfer enough moisture from outside to the dry region such moisture levels must be strong enough to cause sufficient precipitation the impact of drought conditions can be sensed in many areas of society ranging from regional to global economic impacts it can even be felt in areas that are not directly affected by the drought itself being that almost half of the earth s terrestrial surface is susceptible to drought it can be considered as a widespread phenomenon similarly the location of most of the major agricultural lands made them susceptible to drought events drought is a transient and recurrent meteorological phenomenon arising from the absence of long term rainfall and causes severe socio economic problems in the regions where it has occurred mishra and singh 2010 it has widespread and significant impacts on many areas such as economy environment industry agriculture health and human life keyantash and dracup 2002 drought monitoring and forecasting play an important role to mitigate its impacts and consequences for instance decision makers may plan and manage the water resource systems and long term economic and social planning wilhite et al 2007 the severity and spatial extent of the drought are generally measured by drought indexes examples are the palmer drought severity index pdsi standard precipitation index spi palmer hydrological drought index and palmer crop moisture index which are common and frequently used in the literature guttman 1999 vicente serrano et al 2010 analyzing and predicting drought event is an important problem and thus has received great attention over the past two decades drought analysis is done with various statistical and data driven models and the severity intensity and duration of the drought are tried to be characterized for example bonaccorso et al 2003 studied drought analysis using principal component analysis pca another research conducted by dinpashoh et al 2004 where drought analyzed with several climatic and geographical variables by procrustes analysis and l moment based models paulo et al 2005 developed markov chain modeling strategy to estimate the probability of different drought severity classes the expected time in each class of severity the recurrence time to a particular drought class and the expected time for the spi to change from a particular class to another moreira et al 2006 investigated a log linear model to investigate differences relative to drought class transitions among time periods mishra and desai 2006 evaluated various data intelligence models including linear stochastic recursive multi step neural network and direct multi step neural network for drought forecasting another attempted on the data intelligence implementation conducted by mishra et al 2007 the authors proposed a new hybrid intelligence model by combining a linear stochastic model and a non linear artificial neural network model for drought forecasting bacanli et al 2009 investigated the feasibility of adaptive neuro fuzzy inference system for drought forecasting and quantitative value of spi drought indices marj and meijering 2011 proposed an artificial neural network based model for agricultural drought forecasting belayneh et al 2014 established different data driven models for long term drought forecasting most recently a residual based bootstrap model was constructed to predict drought indices intervals beyaztas et al 2018 the potential effects of drought have made it necessary to accurately predict their occurrence with sufficient lead time in order to avoid its consequences both physical and conceptual models can be used to predict drought the conceptual models are mainly deployed in hydrologic forecasting because they require less information accurate in various hydrological applications and easy to build ali et al 2018 mouatadid et al 2018 due to recent technological advances the process of collecting data is becoming ambidextrous causing high dimensional and complex data structure such data type sampled over time space or other continuum measures can be characterized as functional data and can be visualized analyzed modeled and predicted by functional data analysis fda which is a relatively new area in statistics fda considers the data in the form of curves which eliminates the problem of the high number of variables and focuses on temporal dependence between these curves when analyzing the functional data stadtmler and zampiceni 2015 it has several advantages over the classical multivariate statistical analysis models ferraty and vieu 2006 for example fda allows us to look at functional data as a whole rather than a single dependent variable provides additional information about the data such as smoothness and derivatives bypasses the missing data problem by interpolation and smoothing techniques effectively reduces the data noise by smoothing techniques can be used for the irregularly sampled time series data also contrary to classical time series models the fda does not suffer from the high correlation problem between the repeated measurements since it considers the whole curve as a single entity modeling functional time series using fda techniques has received increasing attention in the literature some selected examples of useful fda implementations analyzing of handwriting in chinese ramsay 2000 medical applications pfeiffer et al 2007 modeling and forecasting of breast cancer mortality rate erbas et al 2007 call volume forecasting shen and huang 2008 modeling and forecasting of fertility rates hyndman and shang 2009 forecasting of sea surface temperature shang and hyndman 2011 modeling internet users behaviours kosiorowski 2014 and several other related science and engineering applications ferraty 2011 provided an excellent overview of research and application on functional data analysis an extensive review application for the fda has been conducted by ramsay and silverman 2002 recently within the field of hydrology several successful investigations have been conducted on the feasibility of the fda approach suhaila et al 2011 converted daily and monthly rainfall time series data into a smooth curve where it represents a continuous rainfall pattern of peninsular malaysian region using the fda approach chebana et al 2012 explored the concept of the frequency analysis to inspect the hydrograph curves in which represented as a functional data set the examined functional data approach provided an effective and efficient tool for estimating the risk associated with the flood extreme events adham et al 2014 assessed the potentiality of watershed runoff based on the identification of the curve number soil conservation service and functional data analysis the aim of the study was to convert the runoff data into a smooth curve for better watershed analysis and assessment masselot et al 2016 forecasted streamflow pattern using functional data analysis approach the proposed model attained a more informative understanding of the streamflow trend in comparison with the artificial neural network model ternynck et al 2016 introduced the feasibility of the statistical fda approach for watershed streamflow pattern classification the proposed model provided a better understanding of flood hazard and hydrological phenomena most recently curceac et al 2019 forecasted the air temperature hydrological process using the capacity of the fda approach the forecasting demonstrated a superior result over the seasonal autoregressive moving average model based on the exhibited literature functional data analysis approach evidenced its potential in the field of hydrology engineering however the viability of the fda yet to be explored for drought interval simulation in the current research and for the best knowledge of the authors i fda based model is developed to visualize and analyze the drought interval observations ii the study is adopted to produce point and interval forecasts for the drought series iii finally with the aim to investigate the generalization capability of the proposed fda model the streamflow pattern of the inspected case study is simulated for numerical analysis the modeling performance of the proposed method was evaluated using pdsi based drought observations among others pdsi which is created by palmer 1965 provides standardized measurements of moisture conditions to measure the drought of a region it is based on a supply and demand model of water balance equation and calculated based on the precipitation temperature and the local water content of the soil for more information about the pdsi readers might refer to the following reference alley 1984 2 methodology functional data consists a random functions and each function represents a sample element which is recorded at discrete time points let t t 1 t 2 t k denote the time points where the discrete data x t 1 x t 2 x t k observed then a functional data consisting of n functions x 1 t x 2 t x n t where x t t t 1 t k can be represented as in table 1 the functions or curves are constructed by discrete data points observed at t 1 t 2 t k and it is assumed that each function x t is continuous and smooth functional time series fts is a special case of functional data and arises when the functional data objects are collected sequentially over time in other words let x t t 1 2 n be a sequence of functional observations then it is called as fts if each x t is a random function x t τ where τ is a continuous variable defined in the interval a b τ a b contrary to r n where multivariate analysis works on fda considers that the functions are the elements of a separable hilbert space metric space semi metric space or the banach space which is the space of all continuous functions throughout this study it is assumed that the functional random variable x t is a second order stochastic process with finite second order moment please see ramsay and dalzell 1991 for the theoretical details on fda several models including fpca have been proposed to analyze fts fpca is used for two main purposes i dimension reduction and ii capturing the principal mode of variations it reduces the dimensions of the data by transforming the data to uncorrelated principal components and the first few of them keep most of the variations presented in the data while doing so fpca decomposes the covariance operator into an orthogonal basis of eigenfunctions which are used to describe the variations in the data in more detail let x i t for i 1 2 n and t t be a sample of fts let also ν j and λ j for j 1 2 represent the jth eigenfunction and jth eigenvalue respectively the covariance operator is decomposed by the following orthogonal expansion 1 c t s j 1 λ j ν j t ν j s t s t eigenfunctions are obtained by maximizing the variance max ν var t x i t ν t dt with ν 2 1 for i 1 2 n the jth principal component scores with zero mean and variance λ j are defined as 2 ξ ij t x i t ν j t dt the karhunen loeve expansion is used to express the random curves as follow 3 x i t j 1 ξ ij ν j t t t as a summary fpca reduces the dimension by decomposing x i into orthogonal components ξ ij ν j the variation in x i in the direction of ν j is measured by the eigenvalue λ j and only the first few components retain most of the variations in x i for more information about fpca and its practical demonstration please see ramsay and silverman 2002 and ramsay et al 2009 2 1 forecasting functional time series throughout this study the functional principal component regression fpcr proposed by hyndman and shang 2009 is considered for forecasting fts to start with for i 1 2 n and t t let x i t denote a function 4 x t f t σ t ε t where ε t is an independent and identically distributed iid random variable with zero mean and unit variance σ t is allowing the amount of error to wary with t and f t is assumed a continuous smooth function if μ ν j and ξ j represent the mean function principal component function and principal component score of the jth principal component function respectively then the function f can be decomposed as follows 5 f μ j 1 ξ j ν j e where e is an iid random function let f t f 1 t f 2 t f n t represent a sample of functional data which can be decomposed as 6 f i t f t j 1 j ξ i j ν j t e i t σ i t ε t where f t n 1 i 1 n f i t is the estimate of μ ν j is the jth estimated eigenfunction ξ i j t f i t f t ν j t dt is the jth principal component score for year i ε t is the residual and j where j n is the optimal number of components determined by cross validation please see hyndman and booth 2008 to see the effect of j on the forecast accuracy hyndman and shang 2009 emphasize that each ξ i j for j 1 2 j can be considered as a univariate time series and can be forecasted independently since they are uncorrelated the unobserved future curves can be estimated by multiplying the forecasted principal component scores with the principle components let h 1 2 denote the forecast horizon then the h step ahead forecast of function x i t conditional on the observed smoothed functions f t f 1 t f 2 t f n t and the estimated functional principal components ν t ν 1 t ν 2 t ν j t are obtained as follows 7 x n h n t e x n h t f t ν t f t j 1 j ξ n h n j ν j t where the h step ahead forecast of ξ n h j ξ n h n j are calculated using univariate time series forecasting models such as linear exponential smoothing les auto regressive ar model auto regressive integrated moving average arima or random walk hyndman and khandakar 2008 the forecasts variance are approximated as follows 8 ζ n h t var x n h t f t ν t μ 2 t j 1 j s n h j ν j 2 t ψ t σ n h 2 t where s n h j var ξ n h j ξ 1 j ξ 2 j ξ n j are calculated from the time series model the model error variance ψ t is estimated as n 1 i 1 n ε i 2 t for t t and μ 2 t and σ n h 2 t can be obtained using a non parametric smoothing technique out numerical results which are discussed in detail in section 4 revealed that the arima and les models performed significantly better compared to ar and random walk therefore throughout this study the forecasts are obtained using arima and les methods only briefly arima transforms the time series data into a stationary form and uses them to construct the time series model then based on the constructed model it forecasts the unobservable future observations using a recursion linear exponential smoothing on the other hand gives weights to all observations larger weights are assigned to recent observations and weights decrease exponentially as the observations become older it uses the following recursion to calculate the forecasts x t h t ℓ t b t h s t m h m where m ℓ t b t and s t denote the seasonality length level of the univariate time series growth and the seasonal component respectively for more information about arima and les models please see hyndman and khandakar 2008 2 2 bootstrap prediction intervals as pointed by hyndman and shang 2009 there are four errors sources that must be taken into account when using functional principal component regression the first two sources of errors occur in estimating f i t and μ t the other two sources occur due to forecasting principal component scores ξ i j and predicting model residuals ε i t hence the point forecasts may not adequately capable of high forecasting accuracy since the uncertainty associated with each point forecast is not known in general on the other hand prediction intervals can provide reasonable inferences by taking into account the uncertainty associated with each forecast once the forecast variance ζ n h t obtained the 100 1 α prediction interval for x n h t can be calculated as x n h n t z α 2 ζ n h t where z α 2 is the α 2 th quantile of the standard normal distribution however this procedure may still not be enough to obtain accurate prediction intervals since it is based on the normality assumption which is generally unknown in practice thus such prediction intervals can be affected due to departure from the normality assumption and may lead us to unreliable results as an alternative the bootstrap method can be used to construct prediction intervals without considering distributional assumptions in the context of forecasting the main idea behind this method is to approximate the distribution of future observations by drawing the resamples conditionally on the available data in this study the non parametric bootstrap procedure proposed by hyndman and shang 2009 is used to obtain prediction intervals for a sample of functional data f t f 1 t f 2 t f n t let ξ i j i 1 2 n and j 1 2 j denote the fitted principal component scores also let ξ i i h j and ψ i j h ξ i j ξ i i h j for i h 1 n denote the h step ahead forecasts of ξ i j and the corresponding forecast errors respectively then the following describes the bootstrap algorithm considered in this study step 1 obtain the bootstrap sample of ξ n h j as ξ n h n j ξ n h n j ψ i j h where ψ i j h is a random sample from ψ i j h step 2 assuming the data is well approximated by the first j principal components obtain the bootstrap sample of model errors e t defined in eq 6 e t by sampling with replacement from e 1 t e 2 t e n t also obtain the bootstrap sample of smoothing errors ε n h by sampling with replacement from the set ε 1 ε 2 ε n step 3 obtain the h step ahead forecast of function x t as follows x n h n t f t j 1 j ξ n h n j ν j t e n h n t σ n h t ε n h step 4 obtain bootstrap replicates x n h n 1 t x n h n 2 t x n h n b t by repeating steps 1 3 b times for each h note that b denotes the number of bootstrap replications let g ℓ p x n h n t ℓ be the bootstrap distribution function of unknown distribution function of x n h t then the 100 1 α bootstrap prediction interval for x n h t is obtained as g 1 α 2 g 1 1 α 2 2 3 prediction evaluation metrics the applied models fpcr arima and les are validated statistically using several performance metrics including root mean squared errors rmse determination coefficient r 2 percent bias p bias nash sutcliffe efficiency nse and modified index of agreement mia yaseen et al 2018 yaseen et al 2018 krause et al 2005 larabi et al 2018 the mathematical formulation can be expressed as follows 9 rmse i 1 n x i t x i t 2 n 10 r 2 i 1 n x i t x i t x i t x i t 2 i 1 n x i t x i t 2 i 1 n x i t x i t 2 11 p bias i 1 n x i t x i t i 1 n x i t 100 12 nse 1 i 1 n x i t x i t 2 i 1 n x i t x t 2 13 mia 1 i 1 n x i t x i t i 1 n x i t x t x i t x t where x i t and x i t respectively denote the observed and predicted functions for i 1 n and x t 1 n i 1 n x i t and x t 1 n i 1 n x i t denote the means of the observed and predicted functions respectively 3 case study and data description the coordinates of konya watershed basin lie between 36 51 and 39 29 north latitudes and 31 36 and 34 52 east longitudes in the central anatolia region of turkey the basin has a surface area of about 54000 km2 covering almost 7 of turkey as illustrated in fig 1 the altitude of the basin ranges from 900 to 3534 m to the north of the basin lies sakarya and kizilirmak basins while kizilirmak and seyhan form the east border of the basin to the south of the basin lies the eastern mediterranean basin while antalya and akarcay basins lie in the west of the basin surrounding the basin are the city centers of aksaray nigde konya karaman isparta antalya ankara and nevsehir the basin is positioned at the region with the least annual rainfall in the whole of turkey its total annual precipitation ranged between 286 740 mm the span of the investigated dataset covered the period jan 2007 dec 2015 the occurrence of drought in this arid semi arid region has been observed to cause serious socio economic problems some of the notable problems of drought in this area include irrigational agricultural practices the existence of undocumented water wells loss of natural wetlands functions as a result of prolonged conservation strategies groundwater level reduction as well as loss of soil and habitat salinization table 2 presents a detailed information about the konya basin stations used in this study 4 application and results discussion the main enthusiasm of the present study is to develop a reliable mathematical model where drought interval can be predicted accurately in this section the application findings of the fpcr model are discussed and validated with those of traditional arima and les models the applied modeling strategy is constructed over the mediterranean region where drought events are a highly stochastic pattern hoerling et al 2012 based on the available datasets at each investigated station there are 46 years of observations n 46 x i t k i 1 2 46 and the number of months is k 12 k 1 2 12 the ith observation x i t k represents the monthly pdsi values for the ith year at first the data set is converted to the smooth curves x i t t t 1 12 by a smoothing spline method where the smoothing parameter controlled by generalized cross validation the plots of the raw and functional pdsi series are drawn in figs 2 and 3 respectively here the smoothed pdsi series were plotted by rainbow r package proposed by shang 2011 in these figures the pdsi series from distant past years are shown in red while the most recent years are shown in violet the figures presented the general behaviors of the pdsi series over the studied period span note that aksaray and karaman stations are demonstrated a limited pdsi magnitude between 5 with obvious increment reached 7 during the summer season july and august on the other hand seydisehir and karapinar stations are behaved differently in accordance with their coordinates with limited pdsi ranged between 4 5 with noticeable augmentation during july and august for karapinar station the highest densities magnitudes of the pdsi series illustrated in fig 4 where the pdsi values are ordered by the highest density regions in this figure the red curves which gathered in the center of the data are denoted the most common series whereas the violet curves are presented the most outlying ones in order to obtain out of sample bootstrap prediction intervals the monthly pdsi series split into the following two parts i the model is constructed based on the first 45 years 1970 2014 of data observations to compute the 12 steps ahead predictions for the targeted year i e 2015 ii the result is validated against the observed values the results of the fpca are indicated that the first j 2 principal components explain more than 80 of total variability for each station the actual values are 95 94 82 and 92 for aksaray karaman seydisehir and karapinar stations respectively thus for the fpcr only the first j 2 principal components are used to construct the model the scatter plots of the observed vs fitted pdsi functions observations are given in fig 5 this figure is shown that the observed pdsi series are better predicted by the fpcr compared to both arima and les models several statistical metrics are computed to validate the potential of the applied fpcr and the comparable arima and les models table 3 is reported the numerical results of the rmse r 2 p bias nse and mia based on the exhibited results fpcr model has attained the minimal magnitudes of the rmse and the highest determination coefficient and nash sutcliffe efficiency metrics the r 2 and nse metrics are ranged between 0 77 and 0 93 for all the inspected stations the proposed fpca model is demonstrated a noticeable drought interval prediction enhancement over the les and arima models by approximately 29 5 and 28 3 27 8 and 26 6 22 8 and 19 2 28 1 and 26 2 at aksaray karaman seydisehir and karapinar stations respectively using the rmse metric the p bias values for the inspected stations e g karaman seydisehir and karapinar using fpcr model are 0 0004 0 0065 and 0 0001 whereas at aksaray station p bias 0 0047 using the same concept of correlation graphical scatter plots of the inspected stations are displayed in fig 5 the variance results toward the fitted line observed vs fitted pdsi values are showed that the fpcr model is provided better approximations compared to arima and les models compared to other stations the pdsi series at seydisehir station has the smoothest shape see fig 3 thus any deflation in the predicted series produces the highest error for the pdsi series of seydisehir station among others consequently all the methods tend to produce their worst performances for this station the experienced low prediction performance at the seydisehir station might be due to the location of this station that is influenced by the nearby climatic region hence more insightful climate information can be incorporated for modeling the drought interval at this station in the future research the out of sample prediction intervals are constructed based on the b 1000 bootstrap replications and the nominal level α is set to 0 05 to construct a 95 prediction interval the results are presented in fig 6 this figure is demonstrated that the bootstrap prediction intervals obtained using both fpcr based model and traditional arima and les are comparable and included all of the observations of the pdsi values only the intervals obtained by traditional les fails to cover two pdsi observations points 7 and 8 the results are exhibited the bootstrap method based on arima has narrower prediction intervals than those obtained using the les model since the latter produces more variable results also the results showed that the traditional models produced less uncertain forecast intervals for short term forecasts compared to those of fprc while this is vice versa for long term forecasts moreover fig 6 is suggested that the point forecasts are not consistent with the observed values in all cases the second phase investigation of the current study is adopted for the monthly streamflow pattern detection the data consisted of a period of 45 years jan 1967 dec 2011 of observations obtained from the black sea basin in turkey compared to the pdsi data it is very much difficult to obtain a reliable prediction interval for the streamflow pattern since it presents heteroscedasticity and seasonality characteristics the time series of the functional data and highest density plots for the streamflow data are plotted in fig 7 the figure is exhibited that the raw data are well smoothed by the smoothing spline method similar to the pdsi data the modeling is performed using the first 44 years of observations while the obtained 12 steps ahead predictions belong to the year of 2011 for this data the results of fpca is suggested to use the first j 3 principal components which explain 90 of the total variation to construct the fpcr model the scatter plots of the observed vs fitted streamflow observations are given in fig 8 this figure is demonstrated that the streamflow data are better predicted by the fpcr compared to traditional arima and les methods again the implementation of the streamflow pattern prediction is validated using the reported statistical performance metrics table 4 is reported the numerical results fpcr model is demonstrated the superior over the traditional models the 95 bootstrap prediction intervals for the streamflow data are given by fig 9 this figure is indicated that the bootstrap methodology is also capable of producing valid prediction intervals for the heteroscedastic as well as seasonal hydro climatic variables however the same is not true for the traditional models as is shown by the left panel of fig 9 5 conclusion the current study was adopted on the drought interval observation prediction using the potential of functional data analysis time series models to visualize pdsi component the model was constructed in parallel with bootstrap prediction intervals for future magnitude the proposed model was verified the feasibility of the bootstrap method for attaining an acceptable prediction for the future pdsi interval values the investigation was extended to cover the streamflow forecasting of the same region for validation purpose two classical models i e les and arima were developed based on the attained statistical analysis the proposed fpca based model provided reliable predictability for the drought and streamflow pattern simulation with an acceptable determination coefficient and absolute error metrics values for the drought prediction fpca lse and arima models attained r 2 0 9349 0 8715 0 8702 0 9285 0 8677 0 8676 0 7773 0 6755 0 6763 0 8913 0 8052 0 8056 for aksaray karaman seydisehir karapinar stations respectively whereas the best prediction of streamflow was achieved r 2 0 9861 0 4197 and 0 6389 using fpca les and arima models respectively with minimum value of p bias 0 0001 for fpca model as a future research devotion the performances of the functional time series methodology can be studied for other drought indices such as standard precipitation index palmer hydrological drought index and palmer crop moisture index in addition the model further can be used in other hydro climatic variables such as surface air temperature evaporation evapotranspiration and precipitation declaration of competing interest the authors have no conflict of interest acknowledgement authors are very much thankful for the hydrological data provider turkish state meteorological service mgm 
6039,hydraulic conductivity plays an important role in groundwater systems but the estimation of hydraulic conductivity has many difficulties the costs feasibility and the scale dependent results in this study we compared the hydraulic conductivity inferred from the conventional method slug test and the natural perturbation methods tidal response and post seismic water level recovery methods at two confined wells djp well and mp well the results show that the hydraulic conductivities estimated by the conventional method and the natural perturbation methods deviate somewhat from each other the reason for the difference in hydraulic conductivities inferred from three methods is the variations of wellbore skin by analyzing the uncertainty of estimation we conclude that the different assumptions of the models cause inherent uncertainty and that the parameter uncertainty is secondary the calculation results of these methods represent the different scales of the aquifer properties 5 10 m for the slug test and 10 25 m for the tidal response and the post seismic water level recovery methods averaged by the hydraulic properties of the fault zone the calculation errors of the three methods are 25 for the slug test less than 10 for the tidal analysis and larger than 10 for the third method based on these comparisons we conclude that the tidal response method is the most favorable way to infer aquifer parameters in this study both the tidal response and post seismic water level recovery method provide a way to estimate aquifer properties as well as the mechanism of the co seismic water level responses keywords aquifer parameter water level earth tide earthquake slug test 1 introduction aquifer parameters such as hydraulic conductivity and specific storage are the key parameters that control groundwater flow subsurface temperature and solute transport jiang et al 2012 jiang et al 2009 manga et al 2012 wang et al 2012 thus understanding aquifer hydrological properties is important in evaluating many engineering applications such as groundwater resource management jiang et al 2009 underground waste storage carrigan et al 1991 slope stability assessment sterrett and edil 2010 seismic activity monitoring shi and wang 2016 yan et al 2016 etc traditionally pumping tests have been the most widely used methods for estimating aquifer properties such as hydraulic conductivity and specific storage hvorslev 1951 jacob 1940 however such methods are costly and produces large distribution to the well aquifer system thus they are not suitable in a system that requires continuous observation e g groundwater monitoring networks for earthquake monitoring furthermore no time dependent aquifer properties could be obtained from the tradition aquifer test although many studies have documented that the aquifer properties will be changed under earthquakes or anthropogenic activities such as mining excessive exploitation etc allègre et al 2016 elkhoury et al 2006 kitagawa and kano 2016 xue et al 2013 an alternative way to probe aquifer properties is to measure the water level response to natural perturbations such as barometric pressure earth tide etc rojstaczer 1988 such methods have been widely adopted to obtain real time aquifer property variations shi and wang 2014 xue et al 2013 yan et al 2016 moreover the aquifer parameters can also be inferred from the postseismic water level recovery process wang et al 2017b zhang et al 2017 although several methods can be used to infer aquifer parameters differences exist among these methods and each method may be hindered by its application conditions for example the advantage of the tidal response method is that it is strictly passive it can be used for the long term monitoring of aquifer property variations however this method must be used in wells that can record clear tidal signals allègre et al 2016 shi et al 2015 the limitation of the post seismic water level recovery method is that it can only be used following earthquake induced water level changes it is important to compare the aquifer parameters inferred from these different methods to evaluate the reliability of the results in future field work although allègre et al 2016 conducted a comparison of the permeability inferred from the tidal response method and the pump test no comparison has been performed between the conventional method e g slug test and natural perturbations methods e g tidal response post seismic water level recovery in this study we present a comparison of the hydraulic conductivity inferred from slug test tidal response and post seismic water level recovery methods in two wells in the three gorges area tga china such a comparison is very important because it can expand our choices for permeability calculation and will help us choose suitable ways to explore the change of aquifer property caused by earthquake 2 case study the three gorges dam which is located in hubei province china is the largest dam in the world although the crust is stable with inactive fault activity four earthquakes with magnitudes 6 0 occurred in the past decades che 2002 and seismicity increased sharply after reservoir impoundment in 2003 yi 2012 we chose two confined wells djp well and mp well as the cases in this study the two wells belong to the three gorges groundwater observational network that has been used for reservoir triggered seismic activity monitoring since 2001 both of these faults are situated at the head area of the three gorges area and the changmutuo and gaojiachong faults are the main fault zones extending north south in this area fig 1 the wells are situated near the fault junctions or at the ends of the faults and penetrate confined aquifers clear tidal signals can be recorded in both wells and coseismic water level responses following the may 12 2008 ms 8 0 wenchuan earthquakes are also recoded the epicenter of the ms 8 0 wenchuan earthquake is located in the longmenshan fault zone which is located in the eastern margin of the tibet plateau the longmenshan fault zone strikes ne to sw with a width of 40 50 km and a length of about 500 km the ms 8 0 wenchuan earthquake produced a length of about 340 km rupture striking ne sw with thrust and right lateral components on a high angle fault dipping to the nw shi et al 2014 wang et al 2011 the djp well is situated 100 m west of the gaojiachong fault which dips ne at 60 80 on the north side of the yangtze river the sse nnw trending gaojiachong fault is close to the ssw nne trending changmutuo fault without intersecting it the fault is cemented with well developed tectonite mylonite and cataclastic rock on either side shi et al 2013 the length and width of the gaojiachong fault are 16 km and 16 23 m respectively fig 1 as listed in table 1 the djp well is 153 1 m deep and screened at a depth interval of 80 153 1 m fig 2 a djp well which penetrate the entire thickness of confined aquifer is situated in a biotite bearing quartz diorite fracture aquifer with an aquifer thickness of 30 m the casing radius and the well radius are 133 mm and 114 mm respectively liu et al 2009 shi et al 2013 the water level sensor is located 4 m below the ground surface table 1 the hydraulic conductivity inferred from hydraulic test after construction was 0 1 m d the mp well is situated south of the yangtze river and east of the maoping river with a distance of 50 m from the central section of the changmutuo fault the fault s trajectory varies along its length the southern section strikes nnw sse the middle is n s and the northern section in nne ssw the fault which is in a well cemented state dips ne at 60 80 over a length of 10 km with a width of 20 25 m the mp well is 200 m deep and screened at a depth interval of 80 200 5 m fig 2b mp well penetrate the entire confined aquifer and the aquifer thickness is 45 m table 1 the well has the same casing radius and radius as the djp well the water level sensor is located 4 m below the surface a hydraulic test of the well aquifer system was conducted in 2001 the hydraulic conductivity was 0 01 m d and the specific storage was 1 7 10 5 1 m liu et al 2009 shi et al 2013 zhang et al 2017 the dsw 01 digital instrument which was developed by the institute of seismology china earthquake administration cea has been used for water level measurement since 2001 zhang et al 2017 it has a range of 0 10 m with a resolution of 1 mm and a sampling rate of 1 min water level data are sampled every minute stored in a mainframe computer and sent to the hubei provincial earthquake administration wirelessly we collected the water level data from february to june in 2008 to estimate the hydraulic parameters of the aquifers 3 analysis 3 1 slug test the slug test is a particular aquifer test in which a known volume of water is suddenly injected into or withdrawn from a well the change in the water level will recover as water is exchanged between the well and the formation aquifer hydraulic properties can be estimated from the recovery of the water level which is monitored over time bjerg et al 1992 bouwer 1989 butler 2002 hussein et al 2013 hyder et al 1994 analysis solutions of the slug test exist for various geometries and boundary conditions both of djp and mp well penetrate the entire confined aquifer and we make the assumptions that the hydraulic conditions are homogeneous and isotropic which are same as the one considered by cooper et al 1967 several studies have successfully used cooper s method to estimate the aquifer parameters in fractured aquifer with similar hydrogeological setting of our wells raj et al 1996 singh et al 1999 spane and thorne 1985 thus we also try to use cooper s method in our analysis to make a comparison we select the cooper method and the bouwer and rice method to calculate the hydraulic conductivity in slug test although the bouwer and rice method is proposed as unconfined aquifer it has been demonstrated useful in confined aquifer and fractured confined aquifer by previous studies avci et al 2010 bouwer 1989 maréchal et al 2008 we analyze the experimental data by the cooper et al 1967 method hereafter referred to as the c b p method and the bouwer and rice 1976 method hereafter referred to as the b r method to estimate the hydraulic conductivity of the aquifer in the c b p method the ratio of the measured head to the head after injection is equal to a defined function 1 h h 0 f η μ 2 η t t r c 2 3 μ r s 2 s r c 2 the curve of the experimental data is matched to the type curve which has the same curvature on same scale semilogarithmic paper the vertical time axis t1 which overlays the vertical axis for t t r c 2 1 0 is selected the transmissivity is found from 4 t 1 0 r c 2 t 1 another useful method was developed by bouwer and rice 1976 the hydraulic conductivity k around the aquifer can be calculated by the following equation 5 k r c 2 ln r e r w 2 l e 1 t ln h 0 h t where rc is the radius of the well casing r is the radius measured from the center of the well to the undisturbed aquifer material re is the effective radial distance over which the head is dissipated this is also the distance away from the well over which the average value of k is being measured le is the length of the screen or open section of the well through which water can enter h is the drawdown at time t 0 and ht is the drawdown at time t t ht h0 must always be less than one i e the water level must always approach the static water level as time increases during the slug test we used two buckets with diameters of 31 3 cm and 31 cm and heights of 26 5 cm and 26 cm respectively to quickly add a certain volume of water into the djp well this caused the water level to rise to 1 625 m above the initial water level fig 3 a bucket with a diameter of 30 5 cm and a height of 22 6 cm was used to quickly add water into the well which caused a water level increase of 1 175 m in the mp well fig 3 to reduce the calculation errors the water level recovery records were both fitted with the c b p method and the b r method the hydraulic conductivities estimated by the two methods are 0 006 m d and 0 001 m d for the djp well and 0 0018 m d and 0 0016 m d for the mp well respectively the results are listed in table 2 and the fitting results are shown in fig 4 3 2 tidal analysis the periodic earth tide fluctuation causes a change in the water level in the confined aquifer under the effect of wellbore storage there is a lag between the water level response and the earth tide fluctuation the tidal factor is defined as the ratio of the amplitude of the observed water level changes and the amplitude of the theoretical volumetric strain tide the phase shift and tidal factor are two key parameters in tidal analysis which can be used to infer the transmissivity and storage coefficient of the aquifer elkhoury et al 2006 hsieh et al 1987 lai et al 2014 shi et al 2013 hsieh et al 1987 developed an ideal model with a single homogeneous isotropic laterally extensive and horizontal confined aquifer to estimate the hydraulic parameters under the force of earth tides the relationship between the phase shift and the transmissivity can be written as follows 6 a e 2 f 2 1 2 7 η tan 1 f e where 8 e 1 ω r c 2 2 t ψ k e r α w φ k e i α w 9 f ω r c 2 2 t φ k e r α w ψ k e i α w 10 α w ω s t 1 2 r w where a is the amplitude response η is the phase shift in degrees t is the transmissivity m2 d s is the dimensionless storage coefficient ker and kei are the zeroth order kelvin functions rw is the radius of the well m rc is the inner radius of the well casing m and ω is the frequency of the tide 1 s for earth tide analysis and for realistic values of rw t and s the value of αw as computed by 10 will usually be small since a η ω rw and rc are all known values the transmissivity and the storage coefficient can be obtained by substituting 8 9 and 10 into 6 and 7 the djp and mp wells are constructed in confined aquifers and can record clear tidal signals water level data from february to june 2008 were chosen for tidal analysis before performing the analysis the steps and spikes caused by instrument malfunctions or maintenance work were removed o1 m2 k1 and s2 are the main waves that account for at least 95 of all tidal waves that influence the groundwater level fluctuation we chose the m2 wave for further analysis because it is more stable and less affected by external influences than the other waves tidal analysis was performed via the baytap g program tamura et al 2010 which is a widely used program in tidal analysis a time interval of 10 days with an overlap of 5 days was used to extract the tidal parameters from the time series the reasons why the time interval of 10 days was used to extract the tidal parameters from the time series is that we have the limited data length and want to get as much as data points of the tidal response the results by 30 days window size figs 5 c 5d 6 c and 6d and 10 days window size figs 5a 5b 6a and 6b are compared and they provide similar results as shown in figs 5 and 6 both the phase shift and the amplitude increased suddenly after the earthquake and the red error bars indicate the root mean square error rmse a phase shift increase implies a transmissivity or permeability increase elkhoury et al 2006 hsieh et al 1987 the hydraulic conductivity obtained from the tidal analysis was 0 023 0 0017 m d before the wenchuan earthquake and 0 047 0 0037 m d after the earthquake in the djp well the results for the mp well were 0 003 0 0001 m d and 0 005 0 0003 m d the aquifer parameters inferred from the tidal analysis are listed in table 2 3 3 postseismic water level recovery method the water level changes induced by earthquakes are similar to the changes in the water level in the slug test in both responses the groundwater level in the well changes abruptly followed by a gradual recovery process hvorslev 1951 shi et al 2015 in a previous study there were several types of coseismic water level responses 1 step like change 2 sustainable change and 3 oscillations the water level changes following the earthquake are shown in fig 7 the wenchuan earthquake caused sustained changes in the water level in the djp and mp wells as shown in fig 7 the water level of the two wells continuously dropped and recovered to the initial level for a period of more than 15 days according to global observations the change in the water level in the well induced by an earthquake will be triggered if the seismic energy density is more than 10 3 j m3 seismic energy density could be estimated from log r 0 48 m 0 33 log e 0 4 we calculated the seismic energy density of the two wells and the result is 61 j m3 which is within the seismic energy density range of a trigging water level change induced by an earthquake wang 2007 the hydraulic parameters of aquifers around wells can be estimated by different models under suitable assumptions manga and rowland 2010 roelofts 1998 wang et al 2017a zhang et al 2017 we assumed that the confined aquifer is a one dimensional homogeneous aquifer and that the storage properties are not changed by an earthquake jang et al 2008 based on these assumptions we used the permanent permeability change model and the transient permeability change model see text in supporting information to simulate the earthquake induced water level change and to estimate the hydraulic conductivity of the aquifers these models have been commonly used to interpret the mechanism of the water level discharge in the well spring responses to earthquake manga and rowland 2010 shi et al 2018 zhang et al 2017 in these two wells the hydraulic conductivity after an earthquake inferred from the two models with the nonlinear least squares marquardt levenberg algorithm method was larger than that before an earthquake for the permanent permeability change model the best fitting results are 0 05 0 007 m d for the djp well and 0 016 0 001 m d for the mp well for the transient permeability change model the hydraulic conductivity before the earthquake is 0 01 0 0012 m d for the djp well and 0 005 0 0007 m d for the mp well and the hydraulic conductivity after the earthquake is 0 05 0 0062 m d for the djp well and 0 011 0 0013 m d for the mp well the results are listed in table 2 and the fitting lines are shown in fig 7 with different color lines 4 discussion for the aquifer parameters inferred from the above methods all methods are based on the ideal model which assumes that the aquifer is single homogeneous and isotropic all the methods are based on the fluctuation of the water level induced by human or natural perturbations the parameters estimated by the conventional method and the natural perturbation methods deviate somewhat from one other and there may be several reasons for the different results inferred from the different methods 4 1 the difference in hydraulic conductivities estimated by three methods from table 2 the hydraulic conductivities estimated by slug test is smallest while the value inferred from the postseismic water level response method is largest the difference in three models may be caused by the different wellbore skin effect used in the estimation the largest hydraulic conductivity is obtained from the coseismic water level response method which is a based on a one dimensional homogeneous aquifer model that ignoring the effect of skin but the wellbore skin is a part of the model in slug test and tidal response method according to hussein et al 2013 skin effect is wide spread occurrence in the aquifer and whether skin effect would be disrupted during the slug test is depend on the initial displacement of the well water level if the initial displacement is not enough large to disrupt the skin effect the hydraulic conductivity inferred from the slug test would be smaller than the disrupted case butler and healey 2010 yang and gates 2010 used numerical modeling to explore wellbore skin effect on the result of hydraulic conductivity and found that the hydraulic conductivity estimated with the presence of wellbore skin are approximately five to seven times lower than that without considering the skin effect by slug test from table 2 the hydraulic conductivity estimated by tidal response is four to seven times larger than the result from the slug test in djp and two to three times larger for mp well thus the hydraulic conductivity estimated from the slug test will be similar with the hydraulic conductivity inferred from the tidal response after earthquake if we assume that there is no skin effect existed for the slug test thus the exist of skin effect during the slug test leads to the smaller hydraulic conductivity obtained from the slug test for the tidal response method hydrogeological property around the well such as transmissivity storage coefficient and wellbore skin may have impacts on the tidal response of djp and mp well hsieh et al 1987 suggested that the change of transmissivity and storage coefficient induced by earthquake can cause the variations of phase shift and amplitude moreover changes in wellbore skin and lateral boundary may also lead to the changes of phase shift and amplitude change matsumoto and roeloffs 2003 the well water level in response to earth tides or other periodic loading with frequency ω can be calculated by 11 h w ω 2 t f 2 i ω r w h tide ω 2 t r w f 2 i ω i ω r c 2 f 1 i ω i ω r w r c 2 s w f 2 i ω 12 f 1 p k 0 r w sp t 1 2 2 m 1 k 0 mw sp t 1 2 13 f 2 p sp t 1 2 k 1 r w sp t 1 2 where p is the laplace transform variable s w is the skin factor t is transmissivity s is the storage coefficient w is the width of aquifer h tide ω is the hydraulic head variation far from the well and k0 and k1 are modified bessel functions of orders 0 and 1 respectively because h w ω and h tide ω are complex they can be determined as matsumoto and roeloffs 2003 14 a ω h w ω h tide ω 15 η ω tan 1 i h w ω h tide ω r h w ω h tide ω r f is the real part of a function f and i f is the imaginary part since s t w rw rc a and η are all known values the wellbore skin before earthquake of the two wells can be inferred by substituting 12 15 into 11 which is s w 8 for djp well and sw 10 for mp well we assumed that the change of phase shift and amplitude are only caused by the damage of wellbore skin during earthquake post seismic sw of the two wells could be calculated from the eq 11 assuming r c 0 0665 m r w 0 057 m transmissivity t 0 0008 m2d 1 for djp and t 0 00007 m2d 1 for mp and width of aquifer w 20 m for djp and w 23 m for mp well post seismic phase shift of 23 8 for the djp well and 55 5 for the mp well as such the calculated post seismic sw is 2 5 for djp well and 3 for the mp well and the calculated tidal amplitude is 2 8 cm and 2 4 cm for the two wells table 3 larger than the observed tidal response amplitude as proposed by matsumoto and roeloffs 2003 the smaller observed tidal amplitude may be mainly caused by the changes of wellbore storage changes of hydraulic conductivity as wellbore skin only has small impact on it although changes of lateral boundaries of aquifer may also change the amplitude of water level such changes have not been observed before and we did not consider such factor in this study and if there is gas existed in the water the increase of fluid compressibility is possible which may also decrease of the amplitude but this is not the case of our two wells thus the different hydraulic conductivities inferred from the tidal response before and after the earthquakes are major affected by the permeability changes during the earthquake although the reducing of skin effect will also cause the increase of permeability thus we considered that the skin effect is the major reason for the difference between slug test and tidal response method 4 2 uncertainty of the different methods here we analyze the uncertainty caused by the three methods for the slug test the errors may come from the factors of the skin effect and the effective radius in section 4 1 we have discussed the influence of wellbore skin brown et al 1995 found that the effective radius of the b r method varies with time the effective radius which is the time averaged mean value deviates from the real value consequently the hydraulic conductivity that describes the hydraulic characteristics of the aquifer within the effective radius is also time dependent as shown in fig 8 we calculated the hydraulic conductivity of different time scales and the results show that the value inferred from early in the test is larger than the long term drawdown estimate by almost an order of magnitude for the djp well however for the mp well the results of the slug test vary within an order of magnitude the reason for the difference between the djp and mp wells may be due to the different lengths of the screen brown et al 1995 compared the hydraulic conductivity inferred from the different screen lengths and the results showed that the errors of hydraulic conductivity increased 10 with a 2 5 m decrease in screen length the theoretical basis of the b r method essentially originates from the thiem equation which describes the radial flow system when the screen length has a small value the type of groundwater flow around the well deviates from a radial geometry which causes larger errors in tidal analysis the hydraulic conductivity and specific storage are inverse to the amplitude and phase lag before performing the tidal analysis the suitable window length and steps should be selected different steps may lead to errors in the results and we calculate hydraulic conductivity inferred from different steps in fig 9 the results show that the errors of results are small and both the pre seismic and post seismic hydraulic conductivity vary within an order of magnitude in addition the shortening of the model can also cause errors in the results although the ideal model is widely used to calculate the hydraulic conductivity elkhoury et al 2006 hsieh et al 1987 lai et al 2014 rui et al 2016 shi et al 2013 hsieh s model only considers the horizontal flow and ignores the vertical flow if the vertical groundwater flow is considered the errors of the tidal analysis may be reduced the parameter errors of the postseismic water level recovery method are listed in table 1 and the uncertainty of the parameter estimates is more than 10 the uncertainty of the results also originates from the model assumption this method is based on the assumption that the specific storage does not change during an earthquake but both hydraulic conductivity and specific storage may change following an earthquake jang et al 2008 ward 2015 if specific storage changed after earthquake the hydraulic conductivity is calculated under the different change ratios of specific storage as shown in fig 10 the change of hydraulic conductivity is positively correlated with the variation of specific storage the hydraulic conductivity of djp well vary within an order of magnitude for mp well when the decrease of specific storage is larger than 20 the hydraulic conductivity will be reduced by an order of magnitude we believe that the different assumptions between the models provide inherent uncertainties and that the uncertainties of the parameters are secondary 4 3 scale of investigation the hydraulic conductivity inferred from the different methods represents the different scale ranges of aquifer characteristics for instance the hydraulic conductivity of the core sample test only represents the aquifer characteristics on a small scale and the scale of the pumping test is the distance between the pumping well and the monitoring well allègre et al 2016 elkhoury et al 2006 kitagawa and kano 2016 xue et al 2013 in this study we quantitatively calculate the scale range of methods and compare the results which can fully represent the average properties of the aquifer for completely or partially penetrating confined wells the characteristic parameters of the aquifers and well structures are known the empirical equation of the influence radius which is indicative of the scale range of the aquifer parameters calculated by the slug test is provided by bouwer and rice 1976 when d lw 16 ln r e r w 1 1 ln l w r w a b ln d l w r w l r w 1 in this equation a and b are dimensionless coefficients that are given in bouwer and rice 1976 d is the saturated thickness of the aquifer and lw is the distance from the water table to the bottom of the borehole if d lw the term in d lw rw cannot be used the equation transfers to 17 ln r e r w 1 1 ln l w r w c l r w 1 where c is the dimensionless parameter also given in bouwer and rice 1976 for the djp well and mp well the latter equation is used to calculate the measurement scale based on the equations the scale measurements of the slug tests range from 5 to 10 m the drawdown expression developed by hsieh et al 1987 is used to calculate the tidal analysis scale under tidal forcing the periodic water in and out of the well induces drawdown around the well the analytic solution for drawdown in a well with periodic discharge is 18 s r t q 0 2 π t ϕ k e r α ω ψ k e i α ω i ψ k e r α ω ϕ k e i α ω exp i ω t with 19 ϕ k e r 1 α ω k e i 1 α ω 2 1 2 α ω k e r 1 2 α ω k e i 1 2 α ω 20 ψ k e r 1 α ω k e i 1 α ω 2 1 2 α ω k e r 1 2 α ω k e i 1 2 α ω 21 α ω ω s t 1 2 r w where ker α w and kei α w are the kelvin functions of order zero and ker1 α w and kei1 α w are the kelvin functions of order one q0 exp i ω t is the discharge from the aquifer to the well for the tidal analysis small values of rw t and s lead to φ 1 and ψ 0 following the method applied in xue et al 2013 and allègre et al 2016 the distance from the center of the well to the place where the drawdown decays to 5 of its maximum is the most sensitive region that represents the influence scale of the tidal analysis based on the analysis solution we conclude that the range from 10 to 25 m around well is the most sensitive region fig 11 there is no method to quantitatively estimate the measurement scale of the postseismic water level response method in this method we use the average hydraulic conductivity to represent the whole aquifer characteristics to fit the coseismic water level change based on the hypothesis the scale measurement is the length of the fault zone connected with the aquifer the results imply that the scale range of these different designed purpose methods are different table 4 so we should choose the suitable way to estimate the hydraulic conductivity 4 4 results errors in addition to the uncertainty of the estimate and the scale range the errors of the results are also important values by which to compare the suitability of the models the hydraulic conductivities inferred from the different models have errors and we calculate their uncertainties for the slug test brown et al 1995 concluded that the error between the k values estimated by the b r method and the numerical simulation method range from 10 to 25 in this study the hydraulic conductivity estimated by the slug test is at least 25 lower than that inferred from natural perturbation methods for tidal analysis shi et al 2013 used tidal analysis to estimate the hydraulic conductivity of the aquifer around the djp and mp wells but did not calculate the error of the results we calculated the uncertainty of the hydraulic parameters and the results show that the uncertainty of the parameter estimates is less than 10 for the postseismic water level recovery method zhang et al 2017 used this method to estimate the hydraulic conductivity around the mp well in this study we also used the same method to calculate the hydraulic parameters around the djp well both results show that the errors of hydraulic conductivity are larger than 10 based on the calculation of errors we conclude that the tidal analysis is more accurate than the postseismic water level recovery method and the slug test is not a good way to estimate the aquifer parameters after earthquake 4 5 comparative analysis the hydrogeological settings around well are complex the calculation of aquifer parameters especially after earthquake relies heavily on the knowledge of the conceptual model of the site although the conceptual model can help us solve the hydrology problem the different model choices provide the errors of results thus method chosen for estimation of aquifer parameters should be appropriate for site conditions in the previous part of this study we compared three methods of estimating aquifer parameters in uncertainty of estimation scale of investigation and results errors table 4 the results of these methods deviate somewhat from each other in the investigations of engineering field we may need to estimate the hydraulic conductivity of low permeability materials but hydraulic conductivity in these areas is too low to conduct a pumping test the slug test can be performed quickly in the well but the result of the slug test may be influenced by the amount of water injected to improve the reliability of the slug test for the determination of aquifer hydraulic conductivity more times of slug tests should be performed at a given well the different volume of water should be injected into wellbore so that the uncertainty of estimation can be reduced if the same volume of water is injected into the wellbore at the first and last tests the skin effect can be detected in coseismic hydrology we always want to know whether aquifer property is changes following earthquakes although the postseismic water level recovery method can be used to calculate the hydraulic conductivity after earthquake there is a difference between the ideal model and a geological structure in the real world compared with two other methods the tide analysis is a passive method and it can be used to estimate aquifer hydraulic conductivity around the wells with typical distance of decades of meters it also provides more reliable way to explore the effect on aquifer caused by earthquake 5 conclusion the conventional hydrogeology method and the natural perturbation method are available to obtain aquifer parameters in this study we used three different methods to estimate the aquifer parameters around the djp well and the mp well we obtained the hydraulic conductivity of a small scale range from the slug test the continuous change of hydraulic conductivity from the tidal analysis and the transient and permanent change of hydraulic conductivity from the postseismic water level recovery method the different methods have application conditions by analyzing the model application conditions we believe that the different assumptions between the models provide the inherent uncertainties and that the uncertainty of parameters is secondary to further compare the accuracy of those methods we calculated the scale range of the different methods which is on the order of dozens of meters the results show that the scale range of the slug test is the smallest and the scale range of the postseismic water level recovery method is largest the errors of the results are also calculated they imply that the hydraulic conductivity inferred from tidal analysis has smaller errors than the other methods because these methods are designed for different purpose the tidal analysis is suitable for inferring the variation of aquifer parameters caused by earthquake future work with numerical modeling may be used to verify the scale of the model influence and the errors of results in detail declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank mei jiangchang for the helping in the filed and zhang weihua for providing the water level data this work is supported by the national natural science foundation of china 41972251 u1602233 and the fok ying tung education foundation 161014 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124169 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6039,hydraulic conductivity plays an important role in groundwater systems but the estimation of hydraulic conductivity has many difficulties the costs feasibility and the scale dependent results in this study we compared the hydraulic conductivity inferred from the conventional method slug test and the natural perturbation methods tidal response and post seismic water level recovery methods at two confined wells djp well and mp well the results show that the hydraulic conductivities estimated by the conventional method and the natural perturbation methods deviate somewhat from each other the reason for the difference in hydraulic conductivities inferred from three methods is the variations of wellbore skin by analyzing the uncertainty of estimation we conclude that the different assumptions of the models cause inherent uncertainty and that the parameter uncertainty is secondary the calculation results of these methods represent the different scales of the aquifer properties 5 10 m for the slug test and 10 25 m for the tidal response and the post seismic water level recovery methods averaged by the hydraulic properties of the fault zone the calculation errors of the three methods are 25 for the slug test less than 10 for the tidal analysis and larger than 10 for the third method based on these comparisons we conclude that the tidal response method is the most favorable way to infer aquifer parameters in this study both the tidal response and post seismic water level recovery method provide a way to estimate aquifer properties as well as the mechanism of the co seismic water level responses keywords aquifer parameter water level earth tide earthquake slug test 1 introduction aquifer parameters such as hydraulic conductivity and specific storage are the key parameters that control groundwater flow subsurface temperature and solute transport jiang et al 2012 jiang et al 2009 manga et al 2012 wang et al 2012 thus understanding aquifer hydrological properties is important in evaluating many engineering applications such as groundwater resource management jiang et al 2009 underground waste storage carrigan et al 1991 slope stability assessment sterrett and edil 2010 seismic activity monitoring shi and wang 2016 yan et al 2016 etc traditionally pumping tests have been the most widely used methods for estimating aquifer properties such as hydraulic conductivity and specific storage hvorslev 1951 jacob 1940 however such methods are costly and produces large distribution to the well aquifer system thus they are not suitable in a system that requires continuous observation e g groundwater monitoring networks for earthquake monitoring furthermore no time dependent aquifer properties could be obtained from the tradition aquifer test although many studies have documented that the aquifer properties will be changed under earthquakes or anthropogenic activities such as mining excessive exploitation etc allègre et al 2016 elkhoury et al 2006 kitagawa and kano 2016 xue et al 2013 an alternative way to probe aquifer properties is to measure the water level response to natural perturbations such as barometric pressure earth tide etc rojstaczer 1988 such methods have been widely adopted to obtain real time aquifer property variations shi and wang 2014 xue et al 2013 yan et al 2016 moreover the aquifer parameters can also be inferred from the postseismic water level recovery process wang et al 2017b zhang et al 2017 although several methods can be used to infer aquifer parameters differences exist among these methods and each method may be hindered by its application conditions for example the advantage of the tidal response method is that it is strictly passive it can be used for the long term monitoring of aquifer property variations however this method must be used in wells that can record clear tidal signals allègre et al 2016 shi et al 2015 the limitation of the post seismic water level recovery method is that it can only be used following earthquake induced water level changes it is important to compare the aquifer parameters inferred from these different methods to evaluate the reliability of the results in future field work although allègre et al 2016 conducted a comparison of the permeability inferred from the tidal response method and the pump test no comparison has been performed between the conventional method e g slug test and natural perturbations methods e g tidal response post seismic water level recovery in this study we present a comparison of the hydraulic conductivity inferred from slug test tidal response and post seismic water level recovery methods in two wells in the three gorges area tga china such a comparison is very important because it can expand our choices for permeability calculation and will help us choose suitable ways to explore the change of aquifer property caused by earthquake 2 case study the three gorges dam which is located in hubei province china is the largest dam in the world although the crust is stable with inactive fault activity four earthquakes with magnitudes 6 0 occurred in the past decades che 2002 and seismicity increased sharply after reservoir impoundment in 2003 yi 2012 we chose two confined wells djp well and mp well as the cases in this study the two wells belong to the three gorges groundwater observational network that has been used for reservoir triggered seismic activity monitoring since 2001 both of these faults are situated at the head area of the three gorges area and the changmutuo and gaojiachong faults are the main fault zones extending north south in this area fig 1 the wells are situated near the fault junctions or at the ends of the faults and penetrate confined aquifers clear tidal signals can be recorded in both wells and coseismic water level responses following the may 12 2008 ms 8 0 wenchuan earthquakes are also recoded the epicenter of the ms 8 0 wenchuan earthquake is located in the longmenshan fault zone which is located in the eastern margin of the tibet plateau the longmenshan fault zone strikes ne to sw with a width of 40 50 km and a length of about 500 km the ms 8 0 wenchuan earthquake produced a length of about 340 km rupture striking ne sw with thrust and right lateral components on a high angle fault dipping to the nw shi et al 2014 wang et al 2011 the djp well is situated 100 m west of the gaojiachong fault which dips ne at 60 80 on the north side of the yangtze river the sse nnw trending gaojiachong fault is close to the ssw nne trending changmutuo fault without intersecting it the fault is cemented with well developed tectonite mylonite and cataclastic rock on either side shi et al 2013 the length and width of the gaojiachong fault are 16 km and 16 23 m respectively fig 1 as listed in table 1 the djp well is 153 1 m deep and screened at a depth interval of 80 153 1 m fig 2 a djp well which penetrate the entire thickness of confined aquifer is situated in a biotite bearing quartz diorite fracture aquifer with an aquifer thickness of 30 m the casing radius and the well radius are 133 mm and 114 mm respectively liu et al 2009 shi et al 2013 the water level sensor is located 4 m below the ground surface table 1 the hydraulic conductivity inferred from hydraulic test after construction was 0 1 m d the mp well is situated south of the yangtze river and east of the maoping river with a distance of 50 m from the central section of the changmutuo fault the fault s trajectory varies along its length the southern section strikes nnw sse the middle is n s and the northern section in nne ssw the fault which is in a well cemented state dips ne at 60 80 over a length of 10 km with a width of 20 25 m the mp well is 200 m deep and screened at a depth interval of 80 200 5 m fig 2b mp well penetrate the entire confined aquifer and the aquifer thickness is 45 m table 1 the well has the same casing radius and radius as the djp well the water level sensor is located 4 m below the surface a hydraulic test of the well aquifer system was conducted in 2001 the hydraulic conductivity was 0 01 m d and the specific storage was 1 7 10 5 1 m liu et al 2009 shi et al 2013 zhang et al 2017 the dsw 01 digital instrument which was developed by the institute of seismology china earthquake administration cea has been used for water level measurement since 2001 zhang et al 2017 it has a range of 0 10 m with a resolution of 1 mm and a sampling rate of 1 min water level data are sampled every minute stored in a mainframe computer and sent to the hubei provincial earthquake administration wirelessly we collected the water level data from february to june in 2008 to estimate the hydraulic parameters of the aquifers 3 analysis 3 1 slug test the slug test is a particular aquifer test in which a known volume of water is suddenly injected into or withdrawn from a well the change in the water level will recover as water is exchanged between the well and the formation aquifer hydraulic properties can be estimated from the recovery of the water level which is monitored over time bjerg et al 1992 bouwer 1989 butler 2002 hussein et al 2013 hyder et al 1994 analysis solutions of the slug test exist for various geometries and boundary conditions both of djp and mp well penetrate the entire confined aquifer and we make the assumptions that the hydraulic conditions are homogeneous and isotropic which are same as the one considered by cooper et al 1967 several studies have successfully used cooper s method to estimate the aquifer parameters in fractured aquifer with similar hydrogeological setting of our wells raj et al 1996 singh et al 1999 spane and thorne 1985 thus we also try to use cooper s method in our analysis to make a comparison we select the cooper method and the bouwer and rice method to calculate the hydraulic conductivity in slug test although the bouwer and rice method is proposed as unconfined aquifer it has been demonstrated useful in confined aquifer and fractured confined aquifer by previous studies avci et al 2010 bouwer 1989 maréchal et al 2008 we analyze the experimental data by the cooper et al 1967 method hereafter referred to as the c b p method and the bouwer and rice 1976 method hereafter referred to as the b r method to estimate the hydraulic conductivity of the aquifer in the c b p method the ratio of the measured head to the head after injection is equal to a defined function 1 h h 0 f η μ 2 η t t r c 2 3 μ r s 2 s r c 2 the curve of the experimental data is matched to the type curve which has the same curvature on same scale semilogarithmic paper the vertical time axis t1 which overlays the vertical axis for t t r c 2 1 0 is selected the transmissivity is found from 4 t 1 0 r c 2 t 1 another useful method was developed by bouwer and rice 1976 the hydraulic conductivity k around the aquifer can be calculated by the following equation 5 k r c 2 ln r e r w 2 l e 1 t ln h 0 h t where rc is the radius of the well casing r is the radius measured from the center of the well to the undisturbed aquifer material re is the effective radial distance over which the head is dissipated this is also the distance away from the well over which the average value of k is being measured le is the length of the screen or open section of the well through which water can enter h is the drawdown at time t 0 and ht is the drawdown at time t t ht h0 must always be less than one i e the water level must always approach the static water level as time increases during the slug test we used two buckets with diameters of 31 3 cm and 31 cm and heights of 26 5 cm and 26 cm respectively to quickly add a certain volume of water into the djp well this caused the water level to rise to 1 625 m above the initial water level fig 3 a bucket with a diameter of 30 5 cm and a height of 22 6 cm was used to quickly add water into the well which caused a water level increase of 1 175 m in the mp well fig 3 to reduce the calculation errors the water level recovery records were both fitted with the c b p method and the b r method the hydraulic conductivities estimated by the two methods are 0 006 m d and 0 001 m d for the djp well and 0 0018 m d and 0 0016 m d for the mp well respectively the results are listed in table 2 and the fitting results are shown in fig 4 3 2 tidal analysis the periodic earth tide fluctuation causes a change in the water level in the confined aquifer under the effect of wellbore storage there is a lag between the water level response and the earth tide fluctuation the tidal factor is defined as the ratio of the amplitude of the observed water level changes and the amplitude of the theoretical volumetric strain tide the phase shift and tidal factor are two key parameters in tidal analysis which can be used to infer the transmissivity and storage coefficient of the aquifer elkhoury et al 2006 hsieh et al 1987 lai et al 2014 shi et al 2013 hsieh et al 1987 developed an ideal model with a single homogeneous isotropic laterally extensive and horizontal confined aquifer to estimate the hydraulic parameters under the force of earth tides the relationship between the phase shift and the transmissivity can be written as follows 6 a e 2 f 2 1 2 7 η tan 1 f e where 8 e 1 ω r c 2 2 t ψ k e r α w φ k e i α w 9 f ω r c 2 2 t φ k e r α w ψ k e i α w 10 α w ω s t 1 2 r w where a is the amplitude response η is the phase shift in degrees t is the transmissivity m2 d s is the dimensionless storage coefficient ker and kei are the zeroth order kelvin functions rw is the radius of the well m rc is the inner radius of the well casing m and ω is the frequency of the tide 1 s for earth tide analysis and for realistic values of rw t and s the value of αw as computed by 10 will usually be small since a η ω rw and rc are all known values the transmissivity and the storage coefficient can be obtained by substituting 8 9 and 10 into 6 and 7 the djp and mp wells are constructed in confined aquifers and can record clear tidal signals water level data from february to june 2008 were chosen for tidal analysis before performing the analysis the steps and spikes caused by instrument malfunctions or maintenance work were removed o1 m2 k1 and s2 are the main waves that account for at least 95 of all tidal waves that influence the groundwater level fluctuation we chose the m2 wave for further analysis because it is more stable and less affected by external influences than the other waves tidal analysis was performed via the baytap g program tamura et al 2010 which is a widely used program in tidal analysis a time interval of 10 days with an overlap of 5 days was used to extract the tidal parameters from the time series the reasons why the time interval of 10 days was used to extract the tidal parameters from the time series is that we have the limited data length and want to get as much as data points of the tidal response the results by 30 days window size figs 5 c 5d 6 c and 6d and 10 days window size figs 5a 5b 6a and 6b are compared and they provide similar results as shown in figs 5 and 6 both the phase shift and the amplitude increased suddenly after the earthquake and the red error bars indicate the root mean square error rmse a phase shift increase implies a transmissivity or permeability increase elkhoury et al 2006 hsieh et al 1987 the hydraulic conductivity obtained from the tidal analysis was 0 023 0 0017 m d before the wenchuan earthquake and 0 047 0 0037 m d after the earthquake in the djp well the results for the mp well were 0 003 0 0001 m d and 0 005 0 0003 m d the aquifer parameters inferred from the tidal analysis are listed in table 2 3 3 postseismic water level recovery method the water level changes induced by earthquakes are similar to the changes in the water level in the slug test in both responses the groundwater level in the well changes abruptly followed by a gradual recovery process hvorslev 1951 shi et al 2015 in a previous study there were several types of coseismic water level responses 1 step like change 2 sustainable change and 3 oscillations the water level changes following the earthquake are shown in fig 7 the wenchuan earthquake caused sustained changes in the water level in the djp and mp wells as shown in fig 7 the water level of the two wells continuously dropped and recovered to the initial level for a period of more than 15 days according to global observations the change in the water level in the well induced by an earthquake will be triggered if the seismic energy density is more than 10 3 j m3 seismic energy density could be estimated from log r 0 48 m 0 33 log e 0 4 we calculated the seismic energy density of the two wells and the result is 61 j m3 which is within the seismic energy density range of a trigging water level change induced by an earthquake wang 2007 the hydraulic parameters of aquifers around wells can be estimated by different models under suitable assumptions manga and rowland 2010 roelofts 1998 wang et al 2017a zhang et al 2017 we assumed that the confined aquifer is a one dimensional homogeneous aquifer and that the storage properties are not changed by an earthquake jang et al 2008 based on these assumptions we used the permanent permeability change model and the transient permeability change model see text in supporting information to simulate the earthquake induced water level change and to estimate the hydraulic conductivity of the aquifers these models have been commonly used to interpret the mechanism of the water level discharge in the well spring responses to earthquake manga and rowland 2010 shi et al 2018 zhang et al 2017 in these two wells the hydraulic conductivity after an earthquake inferred from the two models with the nonlinear least squares marquardt levenberg algorithm method was larger than that before an earthquake for the permanent permeability change model the best fitting results are 0 05 0 007 m d for the djp well and 0 016 0 001 m d for the mp well for the transient permeability change model the hydraulic conductivity before the earthquake is 0 01 0 0012 m d for the djp well and 0 005 0 0007 m d for the mp well and the hydraulic conductivity after the earthquake is 0 05 0 0062 m d for the djp well and 0 011 0 0013 m d for the mp well the results are listed in table 2 and the fitting lines are shown in fig 7 with different color lines 4 discussion for the aquifer parameters inferred from the above methods all methods are based on the ideal model which assumes that the aquifer is single homogeneous and isotropic all the methods are based on the fluctuation of the water level induced by human or natural perturbations the parameters estimated by the conventional method and the natural perturbation methods deviate somewhat from one other and there may be several reasons for the different results inferred from the different methods 4 1 the difference in hydraulic conductivities estimated by three methods from table 2 the hydraulic conductivities estimated by slug test is smallest while the value inferred from the postseismic water level response method is largest the difference in three models may be caused by the different wellbore skin effect used in the estimation the largest hydraulic conductivity is obtained from the coseismic water level response method which is a based on a one dimensional homogeneous aquifer model that ignoring the effect of skin but the wellbore skin is a part of the model in slug test and tidal response method according to hussein et al 2013 skin effect is wide spread occurrence in the aquifer and whether skin effect would be disrupted during the slug test is depend on the initial displacement of the well water level if the initial displacement is not enough large to disrupt the skin effect the hydraulic conductivity inferred from the slug test would be smaller than the disrupted case butler and healey 2010 yang and gates 2010 used numerical modeling to explore wellbore skin effect on the result of hydraulic conductivity and found that the hydraulic conductivity estimated with the presence of wellbore skin are approximately five to seven times lower than that without considering the skin effect by slug test from table 2 the hydraulic conductivity estimated by tidal response is four to seven times larger than the result from the slug test in djp and two to three times larger for mp well thus the hydraulic conductivity estimated from the slug test will be similar with the hydraulic conductivity inferred from the tidal response after earthquake if we assume that there is no skin effect existed for the slug test thus the exist of skin effect during the slug test leads to the smaller hydraulic conductivity obtained from the slug test for the tidal response method hydrogeological property around the well such as transmissivity storage coefficient and wellbore skin may have impacts on the tidal response of djp and mp well hsieh et al 1987 suggested that the change of transmissivity and storage coefficient induced by earthquake can cause the variations of phase shift and amplitude moreover changes in wellbore skin and lateral boundary may also lead to the changes of phase shift and amplitude change matsumoto and roeloffs 2003 the well water level in response to earth tides or other periodic loading with frequency ω can be calculated by 11 h w ω 2 t f 2 i ω r w h tide ω 2 t r w f 2 i ω i ω r c 2 f 1 i ω i ω r w r c 2 s w f 2 i ω 12 f 1 p k 0 r w sp t 1 2 2 m 1 k 0 mw sp t 1 2 13 f 2 p sp t 1 2 k 1 r w sp t 1 2 where p is the laplace transform variable s w is the skin factor t is transmissivity s is the storage coefficient w is the width of aquifer h tide ω is the hydraulic head variation far from the well and k0 and k1 are modified bessel functions of orders 0 and 1 respectively because h w ω and h tide ω are complex they can be determined as matsumoto and roeloffs 2003 14 a ω h w ω h tide ω 15 η ω tan 1 i h w ω h tide ω r h w ω h tide ω r f is the real part of a function f and i f is the imaginary part since s t w rw rc a and η are all known values the wellbore skin before earthquake of the two wells can be inferred by substituting 12 15 into 11 which is s w 8 for djp well and sw 10 for mp well we assumed that the change of phase shift and amplitude are only caused by the damage of wellbore skin during earthquake post seismic sw of the two wells could be calculated from the eq 11 assuming r c 0 0665 m r w 0 057 m transmissivity t 0 0008 m2d 1 for djp and t 0 00007 m2d 1 for mp and width of aquifer w 20 m for djp and w 23 m for mp well post seismic phase shift of 23 8 for the djp well and 55 5 for the mp well as such the calculated post seismic sw is 2 5 for djp well and 3 for the mp well and the calculated tidal amplitude is 2 8 cm and 2 4 cm for the two wells table 3 larger than the observed tidal response amplitude as proposed by matsumoto and roeloffs 2003 the smaller observed tidal amplitude may be mainly caused by the changes of wellbore storage changes of hydraulic conductivity as wellbore skin only has small impact on it although changes of lateral boundaries of aquifer may also change the amplitude of water level such changes have not been observed before and we did not consider such factor in this study and if there is gas existed in the water the increase of fluid compressibility is possible which may also decrease of the amplitude but this is not the case of our two wells thus the different hydraulic conductivities inferred from the tidal response before and after the earthquakes are major affected by the permeability changes during the earthquake although the reducing of skin effect will also cause the increase of permeability thus we considered that the skin effect is the major reason for the difference between slug test and tidal response method 4 2 uncertainty of the different methods here we analyze the uncertainty caused by the three methods for the slug test the errors may come from the factors of the skin effect and the effective radius in section 4 1 we have discussed the influence of wellbore skin brown et al 1995 found that the effective radius of the b r method varies with time the effective radius which is the time averaged mean value deviates from the real value consequently the hydraulic conductivity that describes the hydraulic characteristics of the aquifer within the effective radius is also time dependent as shown in fig 8 we calculated the hydraulic conductivity of different time scales and the results show that the value inferred from early in the test is larger than the long term drawdown estimate by almost an order of magnitude for the djp well however for the mp well the results of the slug test vary within an order of magnitude the reason for the difference between the djp and mp wells may be due to the different lengths of the screen brown et al 1995 compared the hydraulic conductivity inferred from the different screen lengths and the results showed that the errors of hydraulic conductivity increased 10 with a 2 5 m decrease in screen length the theoretical basis of the b r method essentially originates from the thiem equation which describes the radial flow system when the screen length has a small value the type of groundwater flow around the well deviates from a radial geometry which causes larger errors in tidal analysis the hydraulic conductivity and specific storage are inverse to the amplitude and phase lag before performing the tidal analysis the suitable window length and steps should be selected different steps may lead to errors in the results and we calculate hydraulic conductivity inferred from different steps in fig 9 the results show that the errors of results are small and both the pre seismic and post seismic hydraulic conductivity vary within an order of magnitude in addition the shortening of the model can also cause errors in the results although the ideal model is widely used to calculate the hydraulic conductivity elkhoury et al 2006 hsieh et al 1987 lai et al 2014 rui et al 2016 shi et al 2013 hsieh s model only considers the horizontal flow and ignores the vertical flow if the vertical groundwater flow is considered the errors of the tidal analysis may be reduced the parameter errors of the postseismic water level recovery method are listed in table 1 and the uncertainty of the parameter estimates is more than 10 the uncertainty of the results also originates from the model assumption this method is based on the assumption that the specific storage does not change during an earthquake but both hydraulic conductivity and specific storage may change following an earthquake jang et al 2008 ward 2015 if specific storage changed after earthquake the hydraulic conductivity is calculated under the different change ratios of specific storage as shown in fig 10 the change of hydraulic conductivity is positively correlated with the variation of specific storage the hydraulic conductivity of djp well vary within an order of magnitude for mp well when the decrease of specific storage is larger than 20 the hydraulic conductivity will be reduced by an order of magnitude we believe that the different assumptions between the models provide inherent uncertainties and that the uncertainties of the parameters are secondary 4 3 scale of investigation the hydraulic conductivity inferred from the different methods represents the different scale ranges of aquifer characteristics for instance the hydraulic conductivity of the core sample test only represents the aquifer characteristics on a small scale and the scale of the pumping test is the distance between the pumping well and the monitoring well allègre et al 2016 elkhoury et al 2006 kitagawa and kano 2016 xue et al 2013 in this study we quantitatively calculate the scale range of methods and compare the results which can fully represent the average properties of the aquifer for completely or partially penetrating confined wells the characteristic parameters of the aquifers and well structures are known the empirical equation of the influence radius which is indicative of the scale range of the aquifer parameters calculated by the slug test is provided by bouwer and rice 1976 when d lw 16 ln r e r w 1 1 ln l w r w a b ln d l w r w l r w 1 in this equation a and b are dimensionless coefficients that are given in bouwer and rice 1976 d is the saturated thickness of the aquifer and lw is the distance from the water table to the bottom of the borehole if d lw the term in d lw rw cannot be used the equation transfers to 17 ln r e r w 1 1 ln l w r w c l r w 1 where c is the dimensionless parameter also given in bouwer and rice 1976 for the djp well and mp well the latter equation is used to calculate the measurement scale based on the equations the scale measurements of the slug tests range from 5 to 10 m the drawdown expression developed by hsieh et al 1987 is used to calculate the tidal analysis scale under tidal forcing the periodic water in and out of the well induces drawdown around the well the analytic solution for drawdown in a well with periodic discharge is 18 s r t q 0 2 π t ϕ k e r α ω ψ k e i α ω i ψ k e r α ω ϕ k e i α ω exp i ω t with 19 ϕ k e r 1 α ω k e i 1 α ω 2 1 2 α ω k e r 1 2 α ω k e i 1 2 α ω 20 ψ k e r 1 α ω k e i 1 α ω 2 1 2 α ω k e r 1 2 α ω k e i 1 2 α ω 21 α ω ω s t 1 2 r w where ker α w and kei α w are the kelvin functions of order zero and ker1 α w and kei1 α w are the kelvin functions of order one q0 exp i ω t is the discharge from the aquifer to the well for the tidal analysis small values of rw t and s lead to φ 1 and ψ 0 following the method applied in xue et al 2013 and allègre et al 2016 the distance from the center of the well to the place where the drawdown decays to 5 of its maximum is the most sensitive region that represents the influence scale of the tidal analysis based on the analysis solution we conclude that the range from 10 to 25 m around well is the most sensitive region fig 11 there is no method to quantitatively estimate the measurement scale of the postseismic water level response method in this method we use the average hydraulic conductivity to represent the whole aquifer characteristics to fit the coseismic water level change based on the hypothesis the scale measurement is the length of the fault zone connected with the aquifer the results imply that the scale range of these different designed purpose methods are different table 4 so we should choose the suitable way to estimate the hydraulic conductivity 4 4 results errors in addition to the uncertainty of the estimate and the scale range the errors of the results are also important values by which to compare the suitability of the models the hydraulic conductivities inferred from the different models have errors and we calculate their uncertainties for the slug test brown et al 1995 concluded that the error between the k values estimated by the b r method and the numerical simulation method range from 10 to 25 in this study the hydraulic conductivity estimated by the slug test is at least 25 lower than that inferred from natural perturbation methods for tidal analysis shi et al 2013 used tidal analysis to estimate the hydraulic conductivity of the aquifer around the djp and mp wells but did not calculate the error of the results we calculated the uncertainty of the hydraulic parameters and the results show that the uncertainty of the parameter estimates is less than 10 for the postseismic water level recovery method zhang et al 2017 used this method to estimate the hydraulic conductivity around the mp well in this study we also used the same method to calculate the hydraulic parameters around the djp well both results show that the errors of hydraulic conductivity are larger than 10 based on the calculation of errors we conclude that the tidal analysis is more accurate than the postseismic water level recovery method and the slug test is not a good way to estimate the aquifer parameters after earthquake 4 5 comparative analysis the hydrogeological settings around well are complex the calculation of aquifer parameters especially after earthquake relies heavily on the knowledge of the conceptual model of the site although the conceptual model can help us solve the hydrology problem the different model choices provide the errors of results thus method chosen for estimation of aquifer parameters should be appropriate for site conditions in the previous part of this study we compared three methods of estimating aquifer parameters in uncertainty of estimation scale of investigation and results errors table 4 the results of these methods deviate somewhat from each other in the investigations of engineering field we may need to estimate the hydraulic conductivity of low permeability materials but hydraulic conductivity in these areas is too low to conduct a pumping test the slug test can be performed quickly in the well but the result of the slug test may be influenced by the amount of water injected to improve the reliability of the slug test for the determination of aquifer hydraulic conductivity more times of slug tests should be performed at a given well the different volume of water should be injected into wellbore so that the uncertainty of estimation can be reduced if the same volume of water is injected into the wellbore at the first and last tests the skin effect can be detected in coseismic hydrology we always want to know whether aquifer property is changes following earthquakes although the postseismic water level recovery method can be used to calculate the hydraulic conductivity after earthquake there is a difference between the ideal model and a geological structure in the real world compared with two other methods the tide analysis is a passive method and it can be used to estimate aquifer hydraulic conductivity around the wells with typical distance of decades of meters it also provides more reliable way to explore the effect on aquifer caused by earthquake 5 conclusion the conventional hydrogeology method and the natural perturbation method are available to obtain aquifer parameters in this study we used three different methods to estimate the aquifer parameters around the djp well and the mp well we obtained the hydraulic conductivity of a small scale range from the slug test the continuous change of hydraulic conductivity from the tidal analysis and the transient and permanent change of hydraulic conductivity from the postseismic water level recovery method the different methods have application conditions by analyzing the model application conditions we believe that the different assumptions between the models provide the inherent uncertainties and that the uncertainty of parameters is secondary to further compare the accuracy of those methods we calculated the scale range of the different methods which is on the order of dozens of meters the results show that the scale range of the slug test is the smallest and the scale range of the postseismic water level recovery method is largest the errors of the results are also calculated they imply that the hydraulic conductivity inferred from tidal analysis has smaller errors than the other methods because these methods are designed for different purpose the tidal analysis is suitable for inferring the variation of aquifer parameters caused by earthquake future work with numerical modeling may be used to verify the scale of the model influence and the errors of results in detail declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank mei jiangchang for the helping in the filed and zhang weihua for providing the water level data this work is supported by the national natural science foundation of china 41972251 u1602233 and the fok ying tung education foundation 161014 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124169 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
