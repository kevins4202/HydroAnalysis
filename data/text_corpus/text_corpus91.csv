index,text
455,current computing capabilities in combination with available theoretical frameworks allow for the petrophysical evaluation of porous rocks from simple images this procedure represents a less expensive alternative and it complements generally expensive laboratory measurements a porosity permeability and forchheimer tensors estimation is reported here through a numerical solution of associated closure problems within digital images of porous rock thin sections the solution of these steady state boundary value problems allows direct computation of all permeability elements and forchheimer tensors the digital images were obtained from a computer procedure that identifies the network of pores in thin sections the results of the numerical estimation of permeability and porosity and its scopes were compared with those obtained via experimental measurements in four samples representing three distinct lithologies travertine sandstone and limestone and were found to be in acceptable agreement once the permeability was computed the forchheimer tensor was calculated as function of the pore scale reynolds number the forchheimer coefficient varied with fluid velocity at an exponent close to 2 range from 1 7 up to 4 2 depending on the lithology and local microstructure the critical reynolds number at which the forchheimer coefficient became relevant was approximately 0 25 we found that the forchheimer tensor exhibited anisotropy not only according to the local microstructure but also according to the flow path keywords digital images petrophysics closure problems forchheimer coefficient porous rocks 1 introduction several theoretical works have addressed the development of a momentum equation governing porous media on a macroscale according to the physics originally considered on a microscale different macroscale equations have been formally derived or empirically postulated such as darcy s law and darcy brinkman darcy forchheimer darcy brinkman forchheimer and buckingham darcy equations e g wu et al 2015 kou et al 2016 2019 das et al 2018 the darcy forchheimer model is employed when inertial effects appear rodríguez de castro and radilla 2017 when a porous medium is adjacent to a macroscale boundary or a clear fluid region the darcy brinkman model recovers the viscous drag and predicts the development of the associated boundary layers givler and altobelli 1994 however the effective viscosity to be employed still deserves investigation valdés parada et al 2007 the darcy brinkman forchheimer model encompasses both the inertial and viscous drag effects kou et al 2016 darcy s law is valid for single phase creeping flow inside the interstices of porous media and when the no slip condition is valid at the solid fluid interface whitaker 1986 this means that darcy s law may not be valid if it is employed for gas flow because the slip gathers relevancy such a problem has been tackled by means of klinkenberg type models klinkenberg 1941 the extent of darcy s law for multiphase flow has been routinely done by introducing the concept of relative permeabilities but the complex physics between mobile phases and the solid miscibility wettability interfacial forces has complicated the unicity of theoretical and experimental justifications see for instance delshad and pope 1989 as mentioned the darcy forchheimer model accounts for the inertial effects inside the pores but some researchers go beyond this model and classify the inertial flow regime as follows skjetne and auriault 1999 1 the weak inertia regime for δ1 2 reynolds number 1 2 the strong inertia regime 1 reynolds number 10 range for the quadratic form of the darcy forchheimer model and 3 the turbulent regime reynolds number of order 100 commonly the formal derivation of macroscale equations starts by setting up microscale governing equations and then applying for instance mathematical techniques and or thermodynamics laws two of the most frequently used mathematical techniques for such upscaling are the spatial averaging and homogenisation methods based on these techniques several researchers have developed upscaled equations and have consequently obtained valuable analyses and physical interpretations of the dependent variables effective coefficients and validity of resulting equations one of the first works was carried out by irmay 1958 who based on the pore scale velocity profile and performing appropriate space averaging found that the forchheimer equation becomes more relevant for higher reynolds numbers and steady flow the spatial averaging of the navier stokes equation has been widely used in the literature gray and o neill 1976 presented a general flow equation for a slowly deforming solid matrix which can be reduced to darcy s law with spatially independent density the authors defined five constants that need to be experimentally obtained whitaker and coworkers whitaker 1986 barrere et al 1992 whitaker 1996 applied the volume averaging method to pore scale equations and obtained the associated upscaled equation considering no slip at the fluid solid interface one substantial difference with respect to gray and o neill s 1976 approach was the introduction of closure problems which allows the computation of effective coefficients as well as the permeability tensor k originally the closure problems were written as integro differential boundary value problems whitaker 1986 this feature was tackled later by barrere et al 1992 who developing an approximation based on the stokes equation which is numerically more tractable a similar formulation of closure problems was extended later by whitaker 1996 to derive darcy s law including the forchheimer correction recently lasseux et al 2019 developed closure problems for unsteady flow deriving a dynamic apparent permeability tensor the application of the volume averaging method has been extended to other flow conditions within porous media 1 large scale heterogeneous systems quintard and whitaker 1998 2 two phase flow chen et al 2019 lasseux et al 2008 valdés parada and espinosa paredes 2005 3 non newtonian fluids wang et al 2014 ochoa tapia et al 2007 4 viscous drag force near to macroscale boundaries valdés parada et al 2007 5 reduced porous matrix golfier et al 2002 6 slip and inertial effects lasseux et al 2014 2016 7 turbulent flow wood et al 2020 and 8 thermodynamically constrained momentum transport of chemical species and fluids gray and miller 2009 the homogenisation method sánchez palencia 1980 has also been used to derive upscaled equations for flow in porous media arbogast 1993 developed two scale homogenisation in naturally fractured porous media for single and immiscible and miscible two phase cases marušić paloka and mikelić 2000 rigorously justified a nonlinear filtration law that does not necessarily encompass the forchheimer model arbogast and lehr 2006 developed the homogenised darcy s law equation for a porous system with vugs embedded they considered darcy s law within the porous rock and the stokes equation in the vugs santos and sheen 2008 analysed a porous system composed by two solid phases saturated with a single flowing phase the derived darcy s law contained three permeability tensors whose explicit computation can be carried out by numerical methods darabi et al 2012 upscaled from local to large scale gas flow processes in ultra tight porous media and formulated a theoretical pressure dependent permeability function recently rohan et al 2018 homogenised a hierarchical porous medium composed of two characteristic scales in this case the derived equation encompassed the darcy brinkman model containing six homogenised coefficients whose definitions depend on physics at lower scales the authors reported that this approach contains one effective viscosity accounting for the drag force as derived previously from the volume averaging method valdés parada et al 2007 the deviations of flow from the classical darcy s law formulation have been theoretically numerically and or experimentally recognized in several works lasseux and valdés parada 2017 reviewed the historical developments for non darcy flow under inertial and slip conditions hassanizadeh and gray 1987 demonstrated by using estimates of orders of magnitude and spatial averaging that at the onset of nonlinear flow the macroscale viscous and inertial forces are smaller than microscale viscous forces based on the homogenisation method firdaouss et al 1997 found that deviations behaved quadratically with the reynolds number strong nonlinear up to five orders of magnitude behavior of forchheimer coefficient was experimentally observed by xiong et al 2018 the authors suggested a critical reynolds number to quantify the onset of nonlinear flow such a nonlinear behaviour was also verified by means of pore scale numerical simulations e g agnaou et al 2017 costa et al 1999 nissan and berkowitz 2018 previous comparisons of using synthetical cells against experimental data indicated that analytical techniques such as closure problems represent an accurate and less expensive alternative for computing petrophysical parameters one of the simplest and most well grounded analytical techniques is the closure scheme proposed by whitaker 1996 whitaker s closure remains valid for one incompressible fluid and a rigid porous matrix that shows a clear separation of scales with these premises the main objective of this work is to demonstrate the applicability of closure problems for estimating the petrophysics of sedimentary rocks and to assess its precision the specific goals are 1 to generate representative micrographs from thin sections of sedimentary rocks 2d images the selected lithologies were sandstone and carbonate rocks which commonly serve as repositories for fluid reservoirs 2 to cut cylindrical cores from the rock samples to measure the permeability and porosity experimentally 3 to solve the closure problems of whitaker 1996 and then compute the permeability and forchheimer tensors this work is organised as follows section 2 describes the sedimentary rocks used the experimental procedure to obtain the petrophysics and the computer routine to obtain digital images from the thin sections section 3 presents the basics of the closure scheme to be solved section 4 summarises the numerical estimations of petrophysical parameters and compares them with experimental data a discussion about the dependency of the forchheimer coefficient on the reynolds number and the critical reynolds number for the onset of darcy s law deviation is provided in the last part of section 4 section 5 presents the concluding remarks 2 material and experimental methods 2 1 description of rock samples the geological material used in this study included a carbonate core from the austin chalk a sandstone from the laredo formation and a travertine sample the austin chalk was used the standard cb 100 distributed by kocurek industries inc caldwell tx this material corresponds to upper cretaceous coniacian santonian fine grained argillaceous compacted limestone calcite 50 it is composed primarily of the skeletal remains of calcareous nannoplankton i e coccoliths the austin chalk was deposited in the gulf coast region of south central texas during a sea level highstand as a transgressive unit wang et al 2004 applying a laser polarised noble gas nmr imaging technique established the austin chalk as one rock characterised by high porosity but very small and poorly connected pores the reported effective porosity 18 4 was a half value with respect to the primary parameter 29 7 a consequence of its low permeability 2 6 3 6 md however the supplier reported www kocurekindustries com austin chalk cores a primary porosity range of 25 27 although with a higher permeability of 8 0 15 0 md and an unconfined compressive strength of 2000 3600 psi a sample was collected from a small outcrop of the laredo formation bartonian eocene located behind the monterrey reynosa highway coordinates 14r477922e 2845295n google earth 2019 geodetic datum wgs84 this rock corresponds to fine grained brown sandstone thickness 0 2 1 0 m showing planar stratification and ripple mark evidence interbedded with shale the sandstone arkose was characterised as a moderately sorted anhedral and subhedral quartz 70 plagioclase and k feldspar arrangement size crystal 1 mm accompanied by a clay groundmass previous reports about the laredo formation as a part of the claiborne group were published by eargle 1968 and westgate 1988 however in subsequent works e g eguiluz de antuñano 2011 the laredo formation term was deprecated this sequence was instead considered part of the cook mountain formation lutetian bartonian eocene the fine sandy strandplain barrier bar cook mountain deposits have been interpreted as a prodelta sequence small petroleum reservoirs linked to the cook mountain formation have been reported in south texas although some of them could be in fact lower yegua formation sands hackley 2012 to the best of our knowledge there is no petrophysical information available for the laredo cook mountain sandstone in northeastern mexico travertine from a stream elongate crust thickness 1 5 m outcropping in the la huasteca canyon rompepicos dam coordinates 14r359701e 2827312n google earth 2019 geodetic datum wgs84 17 km southwest from monterrey n l was collected it is considered to be meteogene travertine that developed from cold water springs and build up along the joints and faults in the cupido platform limestones aptian lehmann et al 1999 recently torres de la cruz et al 2019 reported δ13c 5 59 0 00 and δ18o 8 07 0 00 based on pdb standard for a travertine sample from la huasteca canyon meteogene travertines are characterised by stable carbon isotopic compositions from 0 0 to 11 00 0 00 pentecost and viles 1994 the sample collected in this work showed several porosity morphologies although could be classified as intergranular type according to pentecost 2005 however many pores and conducts were filled with clay and carbonate groundmass porosity information for travertine is relatively scarce most estimates were from water absorption capacity wac experiments expressed as percent void volume pentecost 2005 a mode value of 52 5 with a maximum of 80 for meteogene travertines make them among the most porous limestones however because of a lack of hydraulic connectivity these wac values invariably underestimate the true porosity diagenetic phenomena e g cementation recrystallisation dolomitisation etc can significantly decrease this parameter fig 1 shows the core samples for each one of the lithologies described 2 2 measuring experimental petrophysics here we present the experimental methodology used to measure the porosity and permeability of the rock samples depicted in fig 1 by definition porosity represents the void space in a porous medium per unit bulk volume in sedimentary rocks different types of porosity according to its origin can be found in this part we recognise two types interconnected effective for fluid movement and isolated porosities the sum of both porosities yields the absolute porosity in this sense we stress that the experimental setup and the computer routines described below measured different porosities providing apparent discrepancies on the one hand the experimental apparatus only measured the interconnected porosity because it was based on the measurement of gas entering a given sample on the other hand digital processing can quantify all the voids in the cutting plane of a thin section thus digital processing measured the absolute porosity permeability refers to the ability of rocks to permit the flow of fluids through their pores this is an intrinsic and tensorial property of rocks and it must be independent of the fluids flowing and the operating conditions most experimental setups measure the permeability from the volumetric flow rate and the pressure drop along the rock sample then an equation relating the flow rate pressure drop and permeability is employed the most used is the darcy s law nevertheless its application implies assumptions that need to be fulfilled in order to yield physically coherent outcomes the porosity of the rock samples was determined with the experimental setup depicted in fig 2 that equipment included 1 a manual pump 2 a rock core port sleeve 3 a stainless steel high pressure vessel 4 a gas chamber 5 an air compressor 6 a vacuum pump and 7 a data acquisition system the pressure instrument transmitter pit of fig 2 allows the processing of data in a computer machine interface all the rock samples were cylindrical cores 5 0 cm long and 3 8 cm in diameter see fig 1 the porosity was determined by measuring the quantity of air entering to the cores recording the pressure drop in the vessel supplying the air and using the boyle s method the permeability was obtained with a core holder whose operation is schematised in fig 3 using water we set up a known flow rate entering the rock core with this information and the pressure differential the permeability was obtained with software based on the darcy s law equation 2 3 digital processing a thin section was elaborated for each rock which was stained with methylene blue to facilitate the porous microstructure identification in this way the blue color indicates the existence of pore spaces pixels are the smallest unit to represent digital images and a great number of them create an image in this work we applied morphological operations to binary images where each pixel contained two accepted elements max and min in our implementation 0 and 255 such images were interpreted further with a mathematical set of black and white pixels the porosity evaluation was carried out by analysing thin section images 4x and 10x objectives obtained with a leica icc50 hd photographic camera coupled to a leica dm750p polarising light microscope the maximum resolution of the microscope with acceptable quality of images was 10x the informatic image treatment was carried out using r studio software and the following protocols briones carrillo et al 2016 1 using the r studio software biops the image was converted to black and white pixels 2 border detection subroutines were applied and then dilatation and erosion procedures were carried out to render the images 3 the r algorithm was implemented for porosity identification and calculation 4 the imagej software was applied to highlight the blue colors on the original images 5 gimp software was applied to section images and their sizes were homogenised 6 the images were vectorised and saved in a readable format dxf for the comsol software the numerical solver in this process the inkscape software was used 3 the closure problems and their numerical solutions 3 1 the macroscale formulation for momentum transport using the volume averaging method whitaker 1996 developed a macroscopic equation for momentum transport following the darcy forchheimer formulation in single phase β phase incompressible flow in a rigid porous medium 1 v β k μ β p β β ρ β g f v β forchheimer term with k being the permeability tensor and f the forchheimer correction tensor the last term of the above equation is called the forchheimer term other parameters are the fluid viscosity μβ the seepage velocity field v β the pressure p β the density ρβ and the gravity acceleration g the tensors k and f are defined in terms of the so called closure variables d and m which solve the following boundary value problems closure problems closure problem 1 in β phase 2 0 d 2 d i in β phase 3 d 0 at a βσ 4 d 0 periodicity 5 d r l i d r d r l i d r i 1 2 3 definition of permeability 6 d k closure problem 2 in β phase 7 ρ β v β μ β m m 2 m i in β phase 8 m 0 at a βσ 9 m 0 periodicity 10 m r l i m r m r l i m r i 1 2 3 11 m h definition of forchheimer coefficient 12 f k h 1 i we note that additional conditions are needed for the vectors d and m to avoid infinity of solutions these conditions can be made by imposing average constraints over such variables but we note that only the gradients are needed and furthermore just enforcing point like constraints is enough to achieve particular solutions of the closure problems in the closure problems the variables ψβ and ψβ β represent the superficial and phase average operators defined as 13 ψ β 1 v v β ψ β d v 14 ψ β β 1 v β v β ψ β d v which are related through ψβ εβ ψβ β with εβ being the volume fraction of β phase inside the region enclosed by the volume v v is the averaging volume and v β is the volume of β phase inside v thus it is clear then that v β and p β β are the superficial and phase averages of velocity v β and pressure p β on a pore scale the velocity field v β in eq 7 was calculated with the steady state incompressible navier stokes flow problem in β phase 15 ρ β v β v β p β μ β 2 v β continuity equation 16 v β 0 no slip condition at the walls 17 v β 0 at outlet boundaries 18 p β 1 atm at inlet boundaries 19 p β p inlet the known p inlet value allows us to vary the flow intensity within the cell to investigate the forchheimer coefficient for variable reynolds numbers defined below we recognise that solving eqs 15 19 in the 2d micrographs represents significant computing efforts and thus other novel approaches can be explored in future investigations e g see eq 26b in valdés parada et al 2016 the mathematical structure of closure problems 1 and 2 is quite similar to the flow problem eqs 15 17 but it is not a general rule as demonstrated in other closure problems developed for different transport problems whitaker 1999 in terms of computational effort the closure problems can be equally challenging as dns problems in principle the closure problems are applicable only to periodic unit cells without the need of finding a solution in the entire macroscale domain however the identification of such unit cells can be cumbersome in highly heterogeneous systems and where a hierarchy of scales is not recognisable the closure problems are developed to compute directly the effective transport coefficients whereas other approaches first solve the dns problem compute average quantities and then calculate the coefficients of the upscaled model by using parameter optimisation routines this is one of the advantages of closure problems other feature that closure problems possess is that they clearly capture dominant sources over the macroscale phenomenon for instance closure problem 1 captures the effects of the geometry on a microscale for the no slip condition it is quantified in tensor k closure problem 2 captures essentially the inertial effects on microscale it is quantified in the tensor h defined in eq 11 in the following section we discuss the numerical solution of closure problems 1 and 2 and how from their solutions the permeability and forchheimer coefficients are computed 3 2 numerical solution of closure problems fig 4 shows the working sequence using as example a sandstone laredo 1 thin section using the 10x resolution in this work as mentioned above we analysed a number of samples using 4x and 10x resolutions of microphotographs we found that the resolution influenced mainly the permeability estimations but the estimations remained under the same order of magnitude a larger resolution decreased the permeability estimation while the porosity and forchheimer tensor presented acceptable consistency with the resolution employed a brief comparison of the numerical computations of porosity permeability and forchheimer coefficients employing 4x and 10x resolutions of micrographs are presented in appendix b here we present just the results and images using 10x resolution for the sake of brevity the upper left image of fig 4 depicts the micrograph when methylene blue was employed during the thin sectioning process with the aim to identify the porous microstructure the upper right image shows the micrograph after the computer image processing initially we attempted to use this image within the numerical solver to solve the closure and flow problems however the computational resources became prohibitive and furthermore the image was divided into nine equal parts to facilitate the simulations we verified that the number of subdivisions did not substantially affect the numerical estimations of permeability and the forchheimer tensor additionally the agreement between the numerical solution and the experimental petrophysics was investigated and is presented in the following sections for illustrative purposes the lower graphs of fig 4 present the velocity profiles obtained when the navier stokes equations were numerically solved inside the geometries depicted in the upper figures in these cases the fluid was forced to flow horizontally it should be noted that the velocity profiles were not the primary variables to compute the permeability and the forchheimer tensor but they were the input data needed to solve closure problem 2 see eq 7 after the thin sections microphotographs were digitally processed and divided into nine parts each one was imported into the software comsol multiphysics 5 2a comsol is a commercial software based on finite element procedures and it was employed to solve the boundary values presented in appendix a and the flow problem to compute the velocity field comsol can handle periodic boundary conditions such as the ones included in the closure problems within comsol some numerical aspects were kept by default other relevant configurations were the following 1 the grid mesh was composed by triangles which are adequate for drawing irregular boundaries 2 the solver employed was the multifrontal massively parallel sparse direct solver mumps method which is recommended for large sparse systems of equations and 3 the termination of iterations was based on the newton method for nonlinear equations we observed one moderate dependency of results with the grid mesh density and thus for all the calculations we ensured the independency of the numerical results table 1 summarises the mesh dependency of xx permeability for one thin section of the laredo 1 rock sample we ran the comsol program using a 16 gb ram memory amd fx 8150 eight core processor 3 61 ghz desktop computer as complementary information in table 1 the cpu time is reported in order to illustrate the computational effort the computation time exponentially increases when more than 180 000 triangular elements are employed the usage of more than 510 000 domain elements became prohibitive in light of the great number of rock images that needed to be processed and simulated and furthermore the results presented in this work were calculated employing around 150 000 500 000 domain elements depending on the particular rock sample as one example fig 5 shows the triangular grid mesh employed on one section of the laredo 1 rock sample in fig 6 we plot the closure variables dxx dyx dxy dyy dx and dy we recall that these closure variables are not properly flow variables but their behaviour is quite similar the diagonal components the xx and yy elements of tensor d have similar orders of magnitude to those off the diagonal i e the xy and yx elements this is an interesting feature because one would expect that dxy and dyx would be close to zero and consequently the crossed permeabilities would vanish i e kxy kyx 0 the variables dx and dy present periodicity in one direction and linear behaviour towards the cell centre the corresponding graphics for the components of the closure variables m and m present similar profiles as for d and d respectively with slightly variations due to the convection caused by the pore scale velocity v β for this reason the plotting of those variables is omitted here the closure problems have been solved previously for regular arrays of fibre geometries luminari et al 2018 yielding closure variables with larger orders of magnitude in comparison to those obtained in this work 4 results and discussion 4 1 analysis of permeability and porosity for the four lithologies studied we computed the diagonal elements of the permeability tensor k xx and yy elements after solving closure problem 1 inside the processed images additionally the porosity was computed using the comsol software and the r studio subroutines both methods provided similar results but in what follows only the porosity computed by comsol is reported the relation between porosity and permeability is plotted in fig 7 separately for each lithology the petrophysics measured in the laboratory are included for the purpose of comparison it is observed that the travertine rock presents the larger variations of petrophysics whereas the sandstone laredo rocks show smaller data dispersion broadly speaking the estimations of permeability from the numerical solution of the closure problems for the digital images provided acceptable agreement with experimental measurements only for two lithologies laredo 1 and travertine this statement is supported by the data plotted in fig 7 where the experimental data lie in the centre of the scattered data for austin chalk and sandstone laredo 2 we found that the numerical procedure overestimated the experimental data from a quantitative point of view we computed several average values of data plotted in fig 7 and they are collected in table 2 we calculated the median and geometric averages as they could be more representative for quantities that vary logarithmically from this perspective the sandstone laredo 1 and the austin chalk samples presented better estimations as their orders of magnitude were the same as those of the lab data thus the numerical estimations of petrophysics using digital images and the numerical solution of closure problems are at least adequate for rocks presenting final anisotropy discussed later and irregular sorting of particles during the lithification process however such an asseveration needs to be verified for other lithologies in fig 8 we plotted the ratio between the xx and yy elements of k as a function of porosity the travertine sample shows a relative high anisotropy in comparison to the rest of the samples because its permeability ratio can be larger than 3 in contrast sandstone laredo 2 is characterised by near isotropic behaviour as its xx and yy permeability ratio approaches the isotropy line this behaviour of permeability is inherently linked to the rock fabric of each rock it is important to note that travertine is characterised by grains of heterogeneous size and pores with different morphologies e g micro interpeloidal and interlaminar porosity that are also randomly filled mainly by clays this condition is reflected in the markedly anisotropic behaviour of the porosity and permeability as already reported in the literature soete et al 2015 in comparison to travertine sandstone laredo 2 shows a more homogeneous grain distribution and most of its pores are interconnected intergranular type loucks et al 2012 consequently it petrophysical behaviour tends to be isotropic clearly a change in orientation of the laredo sandstone core samples 1 and 2 has yielded a variation in the permeability the cementation compaction and authigenic clay content are other factors controlling the pore volume in the sandstone 4 2 analysis of the forchheimer tensor dependency on the reynolds number once the closure problems are numerically solved in each section of micrographs then all the components of permeability k and tensor h can be computed according to eqs 6 and 11 respectively thus the forchheimer tensor can be calculated according to the eq 12 now to generalise the numerical computation of the forchheimer tensor as function of the fluid velocity we introduce the reynolds number as follows 20 re ρ β u l c μ β where lc is a characteristic length at the pore scale we adopted a characteristic length based on the grain size as follows 21 l c volume of grains surface area of grains which is numerically computed inside each digital image in practice we did not know the depth of the grains and pores in the 2d images we therefore approximated the characteristic length in the following manner 22 l c surface area of grains perimeter of grains this assumes that grains and pores topologically extended to the orthogonal direction of 2d images the characteristic velocity u in eq 20 was computed within the comsol environment as the norm of the pore scale velocity vector i e 23 u 1 v β v β v β x 2 v β y 2 d v with v β x and v β y being the x and y elements of the fluid velocity in fig 9 we plotted all the xx and yy elements of tensor f for the different lithologies and varying the reynolds number the reynolds number include horizontal x direction flow and vertical y direction flow in the 2d digital images an increasing tendency is observed for all the rocks with different data dispersions depending on the rock heterogeneity we note that the travertine rock shows significant randomness of data indicating a highly heterogeneous microstructure that impacts the interstitial flow the laredo 1 and the austin chalk standard rocks present behaviour similar to the darcy s law deviation when the flow increases at this point it is necessary to discuss why for a given reynolds number and using the same rock sample the forchheimer coefficient can vary up to two orders of magnitude in the numerical calculations of f we kept fixed the fluid parameters ρβ and μβ which according to eq 20 resulted in a particular reynolds number directing our attention to the fluid velocity effects we notice from the closure problem 2 and specifically eq 7 that the closure variable m tends to decrease inside the computational cell because of the external boundary condition used m 0 in boundary value problems very convective terms yield a field solution that tends to acquire the dirichlet condition used at the injection boundary this eventually leads to smaller values of the tensor h see eq 11 with direct consequences for the tensor f we must be clear that up to this point we have not considered turbulent flow within the pore interstices which is also commonly associated with deviations from darcy s law however we emphasise that closure problem 2 is not limited to only laminar flow cases turbulent models can also be used to solve the velocity v β and in this way the effect of inertia and turbulence would be directly quantified in the tensor f wood et al 2020 nevertheless the use of turbulent flow models violates the supporting theory here employed as they would require great pressure drops that could modify the microstructure of rocks thus we stress that closure problems 1 and 2 are supported on the rigid porous medium assumption whitaker 1986 1996 and to the best of our knowledge closure schemes linked to rock mechanics have not been developed yet and thus they are deserving of future research within the discussion here stated the role of the characteristic length lc is not clear yet increasing f values indicate that the inertia has increased i e v β and the length lc must decrease in order to preserve the product ulc as well as the reynolds number as mentioned in the above paragraph however the inverse relation between u and lc is not linear and it is highly dependent on the local microstructure for the same lithology large variations of the forchheimer tensor indicate part of the history that matrix grains have experienced until they are lithified for instance 1 the degree of rework as they are transported abrasion physical mixing from other clast sources etc in the case of clastic rocks 2 the sedimentation environment and 3 post sedimentation processes to collapse all the data plotted in fig 9 we used the following expression to obtain a more tractable equation 24 f a re re c p although empirical this equation allows us to identify two important features concerning the deviation from darcy s law 1 the critical reynolds number from which the forchheimer coefficient is non zero and 2 the exponent at which the coefficient f depends on the reynolds number the parameters involved in eq 24 estimated to fit the data of fig 9 are collected in table 3 the fitting parameter r 2 is also included to illustrate how the data adjust to eq 24 the best fittings are for sandstone laredo rocks whereas the curves with the worst adjustments are for the travertine rock in general great dispersion of data originated with poor r 2values to complement the analysis in fig 10 the fitted curves for each lithology are graphed analysing such curves we observe that the forchheimer tensor not only exhibits intrinsic anisotropy between its diagonal elements say fyy and fxx but also different behaviour of a given coefficient say fxx for instance according to the flow direction i e fxx presents one tendency according to the reynolds number when the flow is oriented in the x direction and another tendency when the fluid flows in the y direction the effect of the flow angle orientation over the forchheimer tensor was corroborated by additional simulations they are presented in the appendix b the discussion here induces us to propose the following expression 25 f b i j k l v β k β v β l β v β β f a re c p ρ β l c μ β p this proposal has an empirical basis and tries to mimic the formula used to estimate the hydrodynamic dispersion coefficient bear 1961 the functional f a re c p ρ β l c μ β p is defined below on the basis of the reynolds number and the empirical eq 24 v β β represents the magnitude of the average velocity and the ratio v β k β v β l β v β β tries to capture the effect of transverse or longitudinal flow over a specific component bijkl of the fourth rank tensor b under assumptions such as isotropic porous medium and invariance to the permutation of indices ijkl some researchers have suggested that tensor b can be determined using the approach described by bear and cheng 2010 eq 7 1 43 26 b b t δ i j k l b l b t 2 δ i k δ j l δ i l δ j k analogously with the mass transport problem bt and bl would be referred to as the transverse and longitudinal coefficients respectively associated to the forchheimer tensor f one could interpret the tensor f in light of the proposal eq 25 and estimate the associated coefficients bt and bl but the absence of a theoretical background at this stage would diminish the relevancy of the results developing such a theory is beyond the scope of this paper but the findings of this work indicate the need to derive new theoretical approaches able to encompass the anisotropy of f associated with the flow direction with easy application for practical engineering problems thus one of the approaches employed to account for non darcy effects in porous media encompasses the form for 1d linear flow and neglecting gravity lee and wattenbarger 1996 eq 1 106 27 d p β β d x μ β v β k β ρ β v β 2 where β is the inertial coefficient or turbulence factor it is clear that the coefficient β is not able to account for the anisotropy of flow discussed in this section similarly in the literature other expressions for non darcy models can be found for instance a general momentum balance in a single fluid porous medium can be written as kou et al 2016 wu et al 2015 28 ρ ϕ u t ρ ϕ 2 uu p μ ϕ 2 u μ k u ρ f k u u ρ g usually the coefficient f is calculated as f 1 75 150 ϕ 3 indeed we can re write eq 28 as follows 29 u k μ p ρ g ρ f k μ u u k ϕ 2 u ρ ϕ k μ u t ρ ϕ 2 k μ uu the unique way that eqs 1 and 29 can be compared is by discarding the accumulation and convective terms i e 30 u k μ p ρ g ρ f k μ u u k ϕ 2 u comparing eqs 1 and 30 we conclude that 31 f v β ρ f k μ u u if we consider one isotropic tensor f say f fxx i then we have that 32 f v β f x x i v β f x x v β ρ f k μ u u and now it is clear that the coefficient we calculated with the closure problems fxx relates with the coefficient f in eq 28 as follows 33 f x x ρ f k μ u also we observe that a reynolds number can be defined including terms of eq 28 as follows 34 re ρ u μ f k characteristic length in other words the forchheimer term in the eq 28 has an associated reynolds number where the characteristic length depends intrinsically on the properties of the porous medium the permeability and porosity remembering that f f φ one disadvantage of the form of eq 28 is related to the tensorial nature of the permeability and forchheimer terms which cannot be properly handled and or analysed in detail because isotropy has been assumed a priori however the closure scheme developed by whitaker 1996 offers the possibility to compute all the components of such tensors as functions of the microstructure and pore scale velocity but at the expense of solving two coupled boundary value problems the closure problems as demonstrated so far at some point eqs 1 27 and 28 present similarities in their mathematical structure and some pros and cons can be drawn eqs 27 and 28 present better applicability to engineering problems if empirical or fitted data are provided for the involved coefficients eq 1 presents a strong theoretical framework behind the tensors k and f allowing us to obtain insights into their physical nature and relations with the pore scale phenomena revising eq 24 and the definition of the reynolds number in eq 20 the forchheimer coefficient in one scalar form can be written as 35 f u p f a re c p ρ β l c μ β p here we must recall that the velocity in the above equation is related to the intrinsic average velocity v β βaccording to eq 23 now following the momentum eq 1 it is plausible to write the forchheimer term as 36 forchheimer term v β β p 1 this expression sheds light on the discussion found in the literature about the deviation of darcy s law from a linear velocity pressure relationship and how such deviation behaves with the reynolds number according to eq 25 and the empirical eq 24 the onset of darcy s law deviation takes place for reynolds numbers greater than re c in as much the dependency of this deviation with respect to the velocity is of order p 1 as observed in eq 36 the summary of all the results computed in this work see data collected in tables 4 and 5 supports the following statements at least for sedimentary rocks in a broad sense the forchheimer coefficient depends with the square of the reynolds number but such dependency presents variations with the local microstructure and the lithology and it can be as much as four orders of magnitude larger even so and globally the dependency with the reynolds number found in this work agrees well with results reported in the literature the onset of darcy s law deviation occurs at a reynolds number of around 0 15 but once again such a value depends on the local microstructure and the lithology and it may appear even for reynolds number of 0 4 these critical reynolds numbers agree well with others numerical investigations in the literature see table 4 in muljadi et al 2016 5 concluding remarks we computed the porosity and permeability as well as the forchheimer tensors from the numerical solution of closure problems boundary value problems from digital images of thin sections of rock samples the accuracy of this methodology was tested by proper comparison with experimental data of porosity and permeability performed using the same rock samples where the thin sections were manufactured the results showed acceptable agreement for those rocks exhibiting major anisotropy of permeability once the permeability calculation was performed it allowed computation of the forchheimer tensor which was analysed in terms of the fluid velocity a non linear dependency with the flow velocity was found for pore scale reynolds numbers larger than approximately 0 25 the forchheimer coefficient increased at an exponent around 2 from 1 7 up to 4 2 of average fluid velocity depending on lithology and local microstructure additionally we report that the forchheimer tensor exhibits anisotropy not only intrinsically with the local microstructure but also with respect to the flow direction credit authorship contribution statement c g aguilar madera conceptualization visualization j v flores cano supervision investigation supervision v matías pérez methodology validation methodology j a briones carrillo resources software resources f velasco tapia formal analysis writing review editing formal analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103616 appendix c supplementary materials image application 1 appendix a algebraic manipulation of closure problems for a 2d domain we calculate all the components of tensor d performing the dot product of eqs 2 to 6 by the x and or y basis vectors e x and e y by the right left or both sides as required which yields boundary value problem 1 in β phase 37 0 x d x x x d x x y y d x x 1 0 y d x x x d y x y y d y x 0 y d y x x d y y y y d y y 1 0 x d y x x d x y y y d x y in β phase 38 x d x x y d y x 0 x d x y y d y y 0 at a βσ 39 d x x 0 d y x 0 d x y 0 d y y 0 d x 0 d y 0 periodicity 40 d x r l i d x r d y r l i d y r d x x r l i d x x r d y y r l i d y y r d y x r l i d y x r d x y r l i d x y r i 1 2 3 permeability components 41 d x x k x x d y y k y y d y x k y x d x y k x y notice that eqs 37 38 include six dependent variables dxx dyy dyx dxy dx and dy that need to be solved simultaneously once the six dependent variables are solved then each component of the permeability tensor can be computed according to eq 41 following the above procedure for the boundary value problem 2 we have boundary value problem 2 in β phase 42 ρ β μ β v β x x m x x v β y y m x x x m x x x m x x y y m x x 1 ρ β μ β v β x x m y x v β y y m y x y m x x x m y x y y m y x ρ β μ β v β x x m y y v β y y m y y y m y x x m y y y y m y y 1 ρ β μ β v β x x m x y v β y y m x y x m y x x m x y y y m x y in β phase 43 x m x x y m y x 0 x m x y y m y y 0 at a βσ 44 m x x 0 m y x 0 m x y 0 m y y 0 m x 0 m y 0 periodicity 45 m x r l i d x r m y r l i d y r m x x r l i m x x r m y y r l i m y y r m y x r l i m y x r m x y r l i m x y r i 1 2 3 46 m x x h x x m y y h y y m x y h x y m y x h y x in this case we observe that as input data the pore scale velocity field must be provided which can be solved independently of closure problem 2 this dependency of tensor m with the pore scale velocity dictates the way the forchheimer coefficient will depend on the reynolds number appendix b effect of thin section resolution and flow angle orientation firstly in this part we provide a comparison of the porosity permeability and forchheimer coefficient computed in the sandstone laredo 1 rock sample using 10x and 4x resolutions according to results of fig b 1 the permeability is overestimated when the 4x resolution was used to photograph the thin section but however the results still present the same order of magnitude the porosity computation was similar for both cases of 4x and 10x resolutions whereas the forchheimer coefficient developed the same tendency with respect to the pore scale reynolds number nevertheless one contraction of scattered data was observed for reynolds numbers larger than 1 the results of fig b1 suggest that the permeability is the property most dependent on the photograph resolution the fig b2 presents the numerical estimation of the xx and yy elements of the forchheimer tensor while varying the flow angle orientation and reynolds number we modify the flow angle orientation by changing the pressure gradient direction in this case the sandstone laredo 1 rock was employed using a 10x resolution we note major effects at low reynolds number for reynolds numbers close to 10 the forchheimer coefficient tends to collapse and the effect of the flow angle can not be separately observed it is hard to elucidate one clear tendency of the flow angle effect over the forchheimer tensor due to the high heterogeneity of the local microstructure of porous rocks in general we found that flow at orientation 30 yields smaller values of the forchheimer tensor while flow at 60 maximizes the tensor however we recognize that such asseverations could depend on the orientation taken during the drilling of rock sample and manufacturing of thin sections in simple words we found that flow at given direction increases the corresponding forhcheimer coefficient for instance the flow in x direction increases the coefficient fxx 
455,current computing capabilities in combination with available theoretical frameworks allow for the petrophysical evaluation of porous rocks from simple images this procedure represents a less expensive alternative and it complements generally expensive laboratory measurements a porosity permeability and forchheimer tensors estimation is reported here through a numerical solution of associated closure problems within digital images of porous rock thin sections the solution of these steady state boundary value problems allows direct computation of all permeability elements and forchheimer tensors the digital images were obtained from a computer procedure that identifies the network of pores in thin sections the results of the numerical estimation of permeability and porosity and its scopes were compared with those obtained via experimental measurements in four samples representing three distinct lithologies travertine sandstone and limestone and were found to be in acceptable agreement once the permeability was computed the forchheimer tensor was calculated as function of the pore scale reynolds number the forchheimer coefficient varied with fluid velocity at an exponent close to 2 range from 1 7 up to 4 2 depending on the lithology and local microstructure the critical reynolds number at which the forchheimer coefficient became relevant was approximately 0 25 we found that the forchheimer tensor exhibited anisotropy not only according to the local microstructure but also according to the flow path keywords digital images petrophysics closure problems forchheimer coefficient porous rocks 1 introduction several theoretical works have addressed the development of a momentum equation governing porous media on a macroscale according to the physics originally considered on a microscale different macroscale equations have been formally derived or empirically postulated such as darcy s law and darcy brinkman darcy forchheimer darcy brinkman forchheimer and buckingham darcy equations e g wu et al 2015 kou et al 2016 2019 das et al 2018 the darcy forchheimer model is employed when inertial effects appear rodríguez de castro and radilla 2017 when a porous medium is adjacent to a macroscale boundary or a clear fluid region the darcy brinkman model recovers the viscous drag and predicts the development of the associated boundary layers givler and altobelli 1994 however the effective viscosity to be employed still deserves investigation valdés parada et al 2007 the darcy brinkman forchheimer model encompasses both the inertial and viscous drag effects kou et al 2016 darcy s law is valid for single phase creeping flow inside the interstices of porous media and when the no slip condition is valid at the solid fluid interface whitaker 1986 this means that darcy s law may not be valid if it is employed for gas flow because the slip gathers relevancy such a problem has been tackled by means of klinkenberg type models klinkenberg 1941 the extent of darcy s law for multiphase flow has been routinely done by introducing the concept of relative permeabilities but the complex physics between mobile phases and the solid miscibility wettability interfacial forces has complicated the unicity of theoretical and experimental justifications see for instance delshad and pope 1989 as mentioned the darcy forchheimer model accounts for the inertial effects inside the pores but some researchers go beyond this model and classify the inertial flow regime as follows skjetne and auriault 1999 1 the weak inertia regime for δ1 2 reynolds number 1 2 the strong inertia regime 1 reynolds number 10 range for the quadratic form of the darcy forchheimer model and 3 the turbulent regime reynolds number of order 100 commonly the formal derivation of macroscale equations starts by setting up microscale governing equations and then applying for instance mathematical techniques and or thermodynamics laws two of the most frequently used mathematical techniques for such upscaling are the spatial averaging and homogenisation methods based on these techniques several researchers have developed upscaled equations and have consequently obtained valuable analyses and physical interpretations of the dependent variables effective coefficients and validity of resulting equations one of the first works was carried out by irmay 1958 who based on the pore scale velocity profile and performing appropriate space averaging found that the forchheimer equation becomes more relevant for higher reynolds numbers and steady flow the spatial averaging of the navier stokes equation has been widely used in the literature gray and o neill 1976 presented a general flow equation for a slowly deforming solid matrix which can be reduced to darcy s law with spatially independent density the authors defined five constants that need to be experimentally obtained whitaker and coworkers whitaker 1986 barrere et al 1992 whitaker 1996 applied the volume averaging method to pore scale equations and obtained the associated upscaled equation considering no slip at the fluid solid interface one substantial difference with respect to gray and o neill s 1976 approach was the introduction of closure problems which allows the computation of effective coefficients as well as the permeability tensor k originally the closure problems were written as integro differential boundary value problems whitaker 1986 this feature was tackled later by barrere et al 1992 who developing an approximation based on the stokes equation which is numerically more tractable a similar formulation of closure problems was extended later by whitaker 1996 to derive darcy s law including the forchheimer correction recently lasseux et al 2019 developed closure problems for unsteady flow deriving a dynamic apparent permeability tensor the application of the volume averaging method has been extended to other flow conditions within porous media 1 large scale heterogeneous systems quintard and whitaker 1998 2 two phase flow chen et al 2019 lasseux et al 2008 valdés parada and espinosa paredes 2005 3 non newtonian fluids wang et al 2014 ochoa tapia et al 2007 4 viscous drag force near to macroscale boundaries valdés parada et al 2007 5 reduced porous matrix golfier et al 2002 6 slip and inertial effects lasseux et al 2014 2016 7 turbulent flow wood et al 2020 and 8 thermodynamically constrained momentum transport of chemical species and fluids gray and miller 2009 the homogenisation method sánchez palencia 1980 has also been used to derive upscaled equations for flow in porous media arbogast 1993 developed two scale homogenisation in naturally fractured porous media for single and immiscible and miscible two phase cases marušić paloka and mikelić 2000 rigorously justified a nonlinear filtration law that does not necessarily encompass the forchheimer model arbogast and lehr 2006 developed the homogenised darcy s law equation for a porous system with vugs embedded they considered darcy s law within the porous rock and the stokes equation in the vugs santos and sheen 2008 analysed a porous system composed by two solid phases saturated with a single flowing phase the derived darcy s law contained three permeability tensors whose explicit computation can be carried out by numerical methods darabi et al 2012 upscaled from local to large scale gas flow processes in ultra tight porous media and formulated a theoretical pressure dependent permeability function recently rohan et al 2018 homogenised a hierarchical porous medium composed of two characteristic scales in this case the derived equation encompassed the darcy brinkman model containing six homogenised coefficients whose definitions depend on physics at lower scales the authors reported that this approach contains one effective viscosity accounting for the drag force as derived previously from the volume averaging method valdés parada et al 2007 the deviations of flow from the classical darcy s law formulation have been theoretically numerically and or experimentally recognized in several works lasseux and valdés parada 2017 reviewed the historical developments for non darcy flow under inertial and slip conditions hassanizadeh and gray 1987 demonstrated by using estimates of orders of magnitude and spatial averaging that at the onset of nonlinear flow the macroscale viscous and inertial forces are smaller than microscale viscous forces based on the homogenisation method firdaouss et al 1997 found that deviations behaved quadratically with the reynolds number strong nonlinear up to five orders of magnitude behavior of forchheimer coefficient was experimentally observed by xiong et al 2018 the authors suggested a critical reynolds number to quantify the onset of nonlinear flow such a nonlinear behaviour was also verified by means of pore scale numerical simulations e g agnaou et al 2017 costa et al 1999 nissan and berkowitz 2018 previous comparisons of using synthetical cells against experimental data indicated that analytical techniques such as closure problems represent an accurate and less expensive alternative for computing petrophysical parameters one of the simplest and most well grounded analytical techniques is the closure scheme proposed by whitaker 1996 whitaker s closure remains valid for one incompressible fluid and a rigid porous matrix that shows a clear separation of scales with these premises the main objective of this work is to demonstrate the applicability of closure problems for estimating the petrophysics of sedimentary rocks and to assess its precision the specific goals are 1 to generate representative micrographs from thin sections of sedimentary rocks 2d images the selected lithologies were sandstone and carbonate rocks which commonly serve as repositories for fluid reservoirs 2 to cut cylindrical cores from the rock samples to measure the permeability and porosity experimentally 3 to solve the closure problems of whitaker 1996 and then compute the permeability and forchheimer tensors this work is organised as follows section 2 describes the sedimentary rocks used the experimental procedure to obtain the petrophysics and the computer routine to obtain digital images from the thin sections section 3 presents the basics of the closure scheme to be solved section 4 summarises the numerical estimations of petrophysical parameters and compares them with experimental data a discussion about the dependency of the forchheimer coefficient on the reynolds number and the critical reynolds number for the onset of darcy s law deviation is provided in the last part of section 4 section 5 presents the concluding remarks 2 material and experimental methods 2 1 description of rock samples the geological material used in this study included a carbonate core from the austin chalk a sandstone from the laredo formation and a travertine sample the austin chalk was used the standard cb 100 distributed by kocurek industries inc caldwell tx this material corresponds to upper cretaceous coniacian santonian fine grained argillaceous compacted limestone calcite 50 it is composed primarily of the skeletal remains of calcareous nannoplankton i e coccoliths the austin chalk was deposited in the gulf coast region of south central texas during a sea level highstand as a transgressive unit wang et al 2004 applying a laser polarised noble gas nmr imaging technique established the austin chalk as one rock characterised by high porosity but very small and poorly connected pores the reported effective porosity 18 4 was a half value with respect to the primary parameter 29 7 a consequence of its low permeability 2 6 3 6 md however the supplier reported www kocurekindustries com austin chalk cores a primary porosity range of 25 27 although with a higher permeability of 8 0 15 0 md and an unconfined compressive strength of 2000 3600 psi a sample was collected from a small outcrop of the laredo formation bartonian eocene located behind the monterrey reynosa highway coordinates 14r477922e 2845295n google earth 2019 geodetic datum wgs84 this rock corresponds to fine grained brown sandstone thickness 0 2 1 0 m showing planar stratification and ripple mark evidence interbedded with shale the sandstone arkose was characterised as a moderately sorted anhedral and subhedral quartz 70 plagioclase and k feldspar arrangement size crystal 1 mm accompanied by a clay groundmass previous reports about the laredo formation as a part of the claiborne group were published by eargle 1968 and westgate 1988 however in subsequent works e g eguiluz de antuñano 2011 the laredo formation term was deprecated this sequence was instead considered part of the cook mountain formation lutetian bartonian eocene the fine sandy strandplain barrier bar cook mountain deposits have been interpreted as a prodelta sequence small petroleum reservoirs linked to the cook mountain formation have been reported in south texas although some of them could be in fact lower yegua formation sands hackley 2012 to the best of our knowledge there is no petrophysical information available for the laredo cook mountain sandstone in northeastern mexico travertine from a stream elongate crust thickness 1 5 m outcropping in the la huasteca canyon rompepicos dam coordinates 14r359701e 2827312n google earth 2019 geodetic datum wgs84 17 km southwest from monterrey n l was collected it is considered to be meteogene travertine that developed from cold water springs and build up along the joints and faults in the cupido platform limestones aptian lehmann et al 1999 recently torres de la cruz et al 2019 reported δ13c 5 59 0 00 and δ18o 8 07 0 00 based on pdb standard for a travertine sample from la huasteca canyon meteogene travertines are characterised by stable carbon isotopic compositions from 0 0 to 11 00 0 00 pentecost and viles 1994 the sample collected in this work showed several porosity morphologies although could be classified as intergranular type according to pentecost 2005 however many pores and conducts were filled with clay and carbonate groundmass porosity information for travertine is relatively scarce most estimates were from water absorption capacity wac experiments expressed as percent void volume pentecost 2005 a mode value of 52 5 with a maximum of 80 for meteogene travertines make them among the most porous limestones however because of a lack of hydraulic connectivity these wac values invariably underestimate the true porosity diagenetic phenomena e g cementation recrystallisation dolomitisation etc can significantly decrease this parameter fig 1 shows the core samples for each one of the lithologies described 2 2 measuring experimental petrophysics here we present the experimental methodology used to measure the porosity and permeability of the rock samples depicted in fig 1 by definition porosity represents the void space in a porous medium per unit bulk volume in sedimentary rocks different types of porosity according to its origin can be found in this part we recognise two types interconnected effective for fluid movement and isolated porosities the sum of both porosities yields the absolute porosity in this sense we stress that the experimental setup and the computer routines described below measured different porosities providing apparent discrepancies on the one hand the experimental apparatus only measured the interconnected porosity because it was based on the measurement of gas entering a given sample on the other hand digital processing can quantify all the voids in the cutting plane of a thin section thus digital processing measured the absolute porosity permeability refers to the ability of rocks to permit the flow of fluids through their pores this is an intrinsic and tensorial property of rocks and it must be independent of the fluids flowing and the operating conditions most experimental setups measure the permeability from the volumetric flow rate and the pressure drop along the rock sample then an equation relating the flow rate pressure drop and permeability is employed the most used is the darcy s law nevertheless its application implies assumptions that need to be fulfilled in order to yield physically coherent outcomes the porosity of the rock samples was determined with the experimental setup depicted in fig 2 that equipment included 1 a manual pump 2 a rock core port sleeve 3 a stainless steel high pressure vessel 4 a gas chamber 5 an air compressor 6 a vacuum pump and 7 a data acquisition system the pressure instrument transmitter pit of fig 2 allows the processing of data in a computer machine interface all the rock samples were cylindrical cores 5 0 cm long and 3 8 cm in diameter see fig 1 the porosity was determined by measuring the quantity of air entering to the cores recording the pressure drop in the vessel supplying the air and using the boyle s method the permeability was obtained with a core holder whose operation is schematised in fig 3 using water we set up a known flow rate entering the rock core with this information and the pressure differential the permeability was obtained with software based on the darcy s law equation 2 3 digital processing a thin section was elaborated for each rock which was stained with methylene blue to facilitate the porous microstructure identification in this way the blue color indicates the existence of pore spaces pixels are the smallest unit to represent digital images and a great number of them create an image in this work we applied morphological operations to binary images where each pixel contained two accepted elements max and min in our implementation 0 and 255 such images were interpreted further with a mathematical set of black and white pixels the porosity evaluation was carried out by analysing thin section images 4x and 10x objectives obtained with a leica icc50 hd photographic camera coupled to a leica dm750p polarising light microscope the maximum resolution of the microscope with acceptable quality of images was 10x the informatic image treatment was carried out using r studio software and the following protocols briones carrillo et al 2016 1 using the r studio software biops the image was converted to black and white pixels 2 border detection subroutines were applied and then dilatation and erosion procedures were carried out to render the images 3 the r algorithm was implemented for porosity identification and calculation 4 the imagej software was applied to highlight the blue colors on the original images 5 gimp software was applied to section images and their sizes were homogenised 6 the images were vectorised and saved in a readable format dxf for the comsol software the numerical solver in this process the inkscape software was used 3 the closure problems and their numerical solutions 3 1 the macroscale formulation for momentum transport using the volume averaging method whitaker 1996 developed a macroscopic equation for momentum transport following the darcy forchheimer formulation in single phase β phase incompressible flow in a rigid porous medium 1 v β k μ β p β β ρ β g f v β forchheimer term with k being the permeability tensor and f the forchheimer correction tensor the last term of the above equation is called the forchheimer term other parameters are the fluid viscosity μβ the seepage velocity field v β the pressure p β the density ρβ and the gravity acceleration g the tensors k and f are defined in terms of the so called closure variables d and m which solve the following boundary value problems closure problems closure problem 1 in β phase 2 0 d 2 d i in β phase 3 d 0 at a βσ 4 d 0 periodicity 5 d r l i d r d r l i d r i 1 2 3 definition of permeability 6 d k closure problem 2 in β phase 7 ρ β v β μ β m m 2 m i in β phase 8 m 0 at a βσ 9 m 0 periodicity 10 m r l i m r m r l i m r i 1 2 3 11 m h definition of forchheimer coefficient 12 f k h 1 i we note that additional conditions are needed for the vectors d and m to avoid infinity of solutions these conditions can be made by imposing average constraints over such variables but we note that only the gradients are needed and furthermore just enforcing point like constraints is enough to achieve particular solutions of the closure problems in the closure problems the variables ψβ and ψβ β represent the superficial and phase average operators defined as 13 ψ β 1 v v β ψ β d v 14 ψ β β 1 v β v β ψ β d v which are related through ψβ εβ ψβ β with εβ being the volume fraction of β phase inside the region enclosed by the volume v v is the averaging volume and v β is the volume of β phase inside v thus it is clear then that v β and p β β are the superficial and phase averages of velocity v β and pressure p β on a pore scale the velocity field v β in eq 7 was calculated with the steady state incompressible navier stokes flow problem in β phase 15 ρ β v β v β p β μ β 2 v β continuity equation 16 v β 0 no slip condition at the walls 17 v β 0 at outlet boundaries 18 p β 1 atm at inlet boundaries 19 p β p inlet the known p inlet value allows us to vary the flow intensity within the cell to investigate the forchheimer coefficient for variable reynolds numbers defined below we recognise that solving eqs 15 19 in the 2d micrographs represents significant computing efforts and thus other novel approaches can be explored in future investigations e g see eq 26b in valdés parada et al 2016 the mathematical structure of closure problems 1 and 2 is quite similar to the flow problem eqs 15 17 but it is not a general rule as demonstrated in other closure problems developed for different transport problems whitaker 1999 in terms of computational effort the closure problems can be equally challenging as dns problems in principle the closure problems are applicable only to periodic unit cells without the need of finding a solution in the entire macroscale domain however the identification of such unit cells can be cumbersome in highly heterogeneous systems and where a hierarchy of scales is not recognisable the closure problems are developed to compute directly the effective transport coefficients whereas other approaches first solve the dns problem compute average quantities and then calculate the coefficients of the upscaled model by using parameter optimisation routines this is one of the advantages of closure problems other feature that closure problems possess is that they clearly capture dominant sources over the macroscale phenomenon for instance closure problem 1 captures the effects of the geometry on a microscale for the no slip condition it is quantified in tensor k closure problem 2 captures essentially the inertial effects on microscale it is quantified in the tensor h defined in eq 11 in the following section we discuss the numerical solution of closure problems 1 and 2 and how from their solutions the permeability and forchheimer coefficients are computed 3 2 numerical solution of closure problems fig 4 shows the working sequence using as example a sandstone laredo 1 thin section using the 10x resolution in this work as mentioned above we analysed a number of samples using 4x and 10x resolutions of microphotographs we found that the resolution influenced mainly the permeability estimations but the estimations remained under the same order of magnitude a larger resolution decreased the permeability estimation while the porosity and forchheimer tensor presented acceptable consistency with the resolution employed a brief comparison of the numerical computations of porosity permeability and forchheimer coefficients employing 4x and 10x resolutions of micrographs are presented in appendix b here we present just the results and images using 10x resolution for the sake of brevity the upper left image of fig 4 depicts the micrograph when methylene blue was employed during the thin sectioning process with the aim to identify the porous microstructure the upper right image shows the micrograph after the computer image processing initially we attempted to use this image within the numerical solver to solve the closure and flow problems however the computational resources became prohibitive and furthermore the image was divided into nine equal parts to facilitate the simulations we verified that the number of subdivisions did not substantially affect the numerical estimations of permeability and the forchheimer tensor additionally the agreement between the numerical solution and the experimental petrophysics was investigated and is presented in the following sections for illustrative purposes the lower graphs of fig 4 present the velocity profiles obtained when the navier stokes equations were numerically solved inside the geometries depicted in the upper figures in these cases the fluid was forced to flow horizontally it should be noted that the velocity profiles were not the primary variables to compute the permeability and the forchheimer tensor but they were the input data needed to solve closure problem 2 see eq 7 after the thin sections microphotographs were digitally processed and divided into nine parts each one was imported into the software comsol multiphysics 5 2a comsol is a commercial software based on finite element procedures and it was employed to solve the boundary values presented in appendix a and the flow problem to compute the velocity field comsol can handle periodic boundary conditions such as the ones included in the closure problems within comsol some numerical aspects were kept by default other relevant configurations were the following 1 the grid mesh was composed by triangles which are adequate for drawing irregular boundaries 2 the solver employed was the multifrontal massively parallel sparse direct solver mumps method which is recommended for large sparse systems of equations and 3 the termination of iterations was based on the newton method for nonlinear equations we observed one moderate dependency of results with the grid mesh density and thus for all the calculations we ensured the independency of the numerical results table 1 summarises the mesh dependency of xx permeability for one thin section of the laredo 1 rock sample we ran the comsol program using a 16 gb ram memory amd fx 8150 eight core processor 3 61 ghz desktop computer as complementary information in table 1 the cpu time is reported in order to illustrate the computational effort the computation time exponentially increases when more than 180 000 triangular elements are employed the usage of more than 510 000 domain elements became prohibitive in light of the great number of rock images that needed to be processed and simulated and furthermore the results presented in this work were calculated employing around 150 000 500 000 domain elements depending on the particular rock sample as one example fig 5 shows the triangular grid mesh employed on one section of the laredo 1 rock sample in fig 6 we plot the closure variables dxx dyx dxy dyy dx and dy we recall that these closure variables are not properly flow variables but their behaviour is quite similar the diagonal components the xx and yy elements of tensor d have similar orders of magnitude to those off the diagonal i e the xy and yx elements this is an interesting feature because one would expect that dxy and dyx would be close to zero and consequently the crossed permeabilities would vanish i e kxy kyx 0 the variables dx and dy present periodicity in one direction and linear behaviour towards the cell centre the corresponding graphics for the components of the closure variables m and m present similar profiles as for d and d respectively with slightly variations due to the convection caused by the pore scale velocity v β for this reason the plotting of those variables is omitted here the closure problems have been solved previously for regular arrays of fibre geometries luminari et al 2018 yielding closure variables with larger orders of magnitude in comparison to those obtained in this work 4 results and discussion 4 1 analysis of permeability and porosity for the four lithologies studied we computed the diagonal elements of the permeability tensor k xx and yy elements after solving closure problem 1 inside the processed images additionally the porosity was computed using the comsol software and the r studio subroutines both methods provided similar results but in what follows only the porosity computed by comsol is reported the relation between porosity and permeability is plotted in fig 7 separately for each lithology the petrophysics measured in the laboratory are included for the purpose of comparison it is observed that the travertine rock presents the larger variations of petrophysics whereas the sandstone laredo rocks show smaller data dispersion broadly speaking the estimations of permeability from the numerical solution of the closure problems for the digital images provided acceptable agreement with experimental measurements only for two lithologies laredo 1 and travertine this statement is supported by the data plotted in fig 7 where the experimental data lie in the centre of the scattered data for austin chalk and sandstone laredo 2 we found that the numerical procedure overestimated the experimental data from a quantitative point of view we computed several average values of data plotted in fig 7 and they are collected in table 2 we calculated the median and geometric averages as they could be more representative for quantities that vary logarithmically from this perspective the sandstone laredo 1 and the austin chalk samples presented better estimations as their orders of magnitude were the same as those of the lab data thus the numerical estimations of petrophysics using digital images and the numerical solution of closure problems are at least adequate for rocks presenting final anisotropy discussed later and irregular sorting of particles during the lithification process however such an asseveration needs to be verified for other lithologies in fig 8 we plotted the ratio between the xx and yy elements of k as a function of porosity the travertine sample shows a relative high anisotropy in comparison to the rest of the samples because its permeability ratio can be larger than 3 in contrast sandstone laredo 2 is characterised by near isotropic behaviour as its xx and yy permeability ratio approaches the isotropy line this behaviour of permeability is inherently linked to the rock fabric of each rock it is important to note that travertine is characterised by grains of heterogeneous size and pores with different morphologies e g micro interpeloidal and interlaminar porosity that are also randomly filled mainly by clays this condition is reflected in the markedly anisotropic behaviour of the porosity and permeability as already reported in the literature soete et al 2015 in comparison to travertine sandstone laredo 2 shows a more homogeneous grain distribution and most of its pores are interconnected intergranular type loucks et al 2012 consequently it petrophysical behaviour tends to be isotropic clearly a change in orientation of the laredo sandstone core samples 1 and 2 has yielded a variation in the permeability the cementation compaction and authigenic clay content are other factors controlling the pore volume in the sandstone 4 2 analysis of the forchheimer tensor dependency on the reynolds number once the closure problems are numerically solved in each section of micrographs then all the components of permeability k and tensor h can be computed according to eqs 6 and 11 respectively thus the forchheimer tensor can be calculated according to the eq 12 now to generalise the numerical computation of the forchheimer tensor as function of the fluid velocity we introduce the reynolds number as follows 20 re ρ β u l c μ β where lc is a characteristic length at the pore scale we adopted a characteristic length based on the grain size as follows 21 l c volume of grains surface area of grains which is numerically computed inside each digital image in practice we did not know the depth of the grains and pores in the 2d images we therefore approximated the characteristic length in the following manner 22 l c surface area of grains perimeter of grains this assumes that grains and pores topologically extended to the orthogonal direction of 2d images the characteristic velocity u in eq 20 was computed within the comsol environment as the norm of the pore scale velocity vector i e 23 u 1 v β v β v β x 2 v β y 2 d v with v β x and v β y being the x and y elements of the fluid velocity in fig 9 we plotted all the xx and yy elements of tensor f for the different lithologies and varying the reynolds number the reynolds number include horizontal x direction flow and vertical y direction flow in the 2d digital images an increasing tendency is observed for all the rocks with different data dispersions depending on the rock heterogeneity we note that the travertine rock shows significant randomness of data indicating a highly heterogeneous microstructure that impacts the interstitial flow the laredo 1 and the austin chalk standard rocks present behaviour similar to the darcy s law deviation when the flow increases at this point it is necessary to discuss why for a given reynolds number and using the same rock sample the forchheimer coefficient can vary up to two orders of magnitude in the numerical calculations of f we kept fixed the fluid parameters ρβ and μβ which according to eq 20 resulted in a particular reynolds number directing our attention to the fluid velocity effects we notice from the closure problem 2 and specifically eq 7 that the closure variable m tends to decrease inside the computational cell because of the external boundary condition used m 0 in boundary value problems very convective terms yield a field solution that tends to acquire the dirichlet condition used at the injection boundary this eventually leads to smaller values of the tensor h see eq 11 with direct consequences for the tensor f we must be clear that up to this point we have not considered turbulent flow within the pore interstices which is also commonly associated with deviations from darcy s law however we emphasise that closure problem 2 is not limited to only laminar flow cases turbulent models can also be used to solve the velocity v β and in this way the effect of inertia and turbulence would be directly quantified in the tensor f wood et al 2020 nevertheless the use of turbulent flow models violates the supporting theory here employed as they would require great pressure drops that could modify the microstructure of rocks thus we stress that closure problems 1 and 2 are supported on the rigid porous medium assumption whitaker 1986 1996 and to the best of our knowledge closure schemes linked to rock mechanics have not been developed yet and thus they are deserving of future research within the discussion here stated the role of the characteristic length lc is not clear yet increasing f values indicate that the inertia has increased i e v β and the length lc must decrease in order to preserve the product ulc as well as the reynolds number as mentioned in the above paragraph however the inverse relation between u and lc is not linear and it is highly dependent on the local microstructure for the same lithology large variations of the forchheimer tensor indicate part of the history that matrix grains have experienced until they are lithified for instance 1 the degree of rework as they are transported abrasion physical mixing from other clast sources etc in the case of clastic rocks 2 the sedimentation environment and 3 post sedimentation processes to collapse all the data plotted in fig 9 we used the following expression to obtain a more tractable equation 24 f a re re c p although empirical this equation allows us to identify two important features concerning the deviation from darcy s law 1 the critical reynolds number from which the forchheimer coefficient is non zero and 2 the exponent at which the coefficient f depends on the reynolds number the parameters involved in eq 24 estimated to fit the data of fig 9 are collected in table 3 the fitting parameter r 2 is also included to illustrate how the data adjust to eq 24 the best fittings are for sandstone laredo rocks whereas the curves with the worst adjustments are for the travertine rock in general great dispersion of data originated with poor r 2values to complement the analysis in fig 10 the fitted curves for each lithology are graphed analysing such curves we observe that the forchheimer tensor not only exhibits intrinsic anisotropy between its diagonal elements say fyy and fxx but also different behaviour of a given coefficient say fxx for instance according to the flow direction i e fxx presents one tendency according to the reynolds number when the flow is oriented in the x direction and another tendency when the fluid flows in the y direction the effect of the flow angle orientation over the forchheimer tensor was corroborated by additional simulations they are presented in the appendix b the discussion here induces us to propose the following expression 25 f b i j k l v β k β v β l β v β β f a re c p ρ β l c μ β p this proposal has an empirical basis and tries to mimic the formula used to estimate the hydrodynamic dispersion coefficient bear 1961 the functional f a re c p ρ β l c μ β p is defined below on the basis of the reynolds number and the empirical eq 24 v β β represents the magnitude of the average velocity and the ratio v β k β v β l β v β β tries to capture the effect of transverse or longitudinal flow over a specific component bijkl of the fourth rank tensor b under assumptions such as isotropic porous medium and invariance to the permutation of indices ijkl some researchers have suggested that tensor b can be determined using the approach described by bear and cheng 2010 eq 7 1 43 26 b b t δ i j k l b l b t 2 δ i k δ j l δ i l δ j k analogously with the mass transport problem bt and bl would be referred to as the transverse and longitudinal coefficients respectively associated to the forchheimer tensor f one could interpret the tensor f in light of the proposal eq 25 and estimate the associated coefficients bt and bl but the absence of a theoretical background at this stage would diminish the relevancy of the results developing such a theory is beyond the scope of this paper but the findings of this work indicate the need to derive new theoretical approaches able to encompass the anisotropy of f associated with the flow direction with easy application for practical engineering problems thus one of the approaches employed to account for non darcy effects in porous media encompasses the form for 1d linear flow and neglecting gravity lee and wattenbarger 1996 eq 1 106 27 d p β β d x μ β v β k β ρ β v β 2 where β is the inertial coefficient or turbulence factor it is clear that the coefficient β is not able to account for the anisotropy of flow discussed in this section similarly in the literature other expressions for non darcy models can be found for instance a general momentum balance in a single fluid porous medium can be written as kou et al 2016 wu et al 2015 28 ρ ϕ u t ρ ϕ 2 uu p μ ϕ 2 u μ k u ρ f k u u ρ g usually the coefficient f is calculated as f 1 75 150 ϕ 3 indeed we can re write eq 28 as follows 29 u k μ p ρ g ρ f k μ u u k ϕ 2 u ρ ϕ k μ u t ρ ϕ 2 k μ uu the unique way that eqs 1 and 29 can be compared is by discarding the accumulation and convective terms i e 30 u k μ p ρ g ρ f k μ u u k ϕ 2 u comparing eqs 1 and 30 we conclude that 31 f v β ρ f k μ u u if we consider one isotropic tensor f say f fxx i then we have that 32 f v β f x x i v β f x x v β ρ f k μ u u and now it is clear that the coefficient we calculated with the closure problems fxx relates with the coefficient f in eq 28 as follows 33 f x x ρ f k μ u also we observe that a reynolds number can be defined including terms of eq 28 as follows 34 re ρ u μ f k characteristic length in other words the forchheimer term in the eq 28 has an associated reynolds number where the characteristic length depends intrinsically on the properties of the porous medium the permeability and porosity remembering that f f φ one disadvantage of the form of eq 28 is related to the tensorial nature of the permeability and forchheimer terms which cannot be properly handled and or analysed in detail because isotropy has been assumed a priori however the closure scheme developed by whitaker 1996 offers the possibility to compute all the components of such tensors as functions of the microstructure and pore scale velocity but at the expense of solving two coupled boundary value problems the closure problems as demonstrated so far at some point eqs 1 27 and 28 present similarities in their mathematical structure and some pros and cons can be drawn eqs 27 and 28 present better applicability to engineering problems if empirical or fitted data are provided for the involved coefficients eq 1 presents a strong theoretical framework behind the tensors k and f allowing us to obtain insights into their physical nature and relations with the pore scale phenomena revising eq 24 and the definition of the reynolds number in eq 20 the forchheimer coefficient in one scalar form can be written as 35 f u p f a re c p ρ β l c μ β p here we must recall that the velocity in the above equation is related to the intrinsic average velocity v β βaccording to eq 23 now following the momentum eq 1 it is plausible to write the forchheimer term as 36 forchheimer term v β β p 1 this expression sheds light on the discussion found in the literature about the deviation of darcy s law from a linear velocity pressure relationship and how such deviation behaves with the reynolds number according to eq 25 and the empirical eq 24 the onset of darcy s law deviation takes place for reynolds numbers greater than re c in as much the dependency of this deviation with respect to the velocity is of order p 1 as observed in eq 36 the summary of all the results computed in this work see data collected in tables 4 and 5 supports the following statements at least for sedimentary rocks in a broad sense the forchheimer coefficient depends with the square of the reynolds number but such dependency presents variations with the local microstructure and the lithology and it can be as much as four orders of magnitude larger even so and globally the dependency with the reynolds number found in this work agrees well with results reported in the literature the onset of darcy s law deviation occurs at a reynolds number of around 0 15 but once again such a value depends on the local microstructure and the lithology and it may appear even for reynolds number of 0 4 these critical reynolds numbers agree well with others numerical investigations in the literature see table 4 in muljadi et al 2016 5 concluding remarks we computed the porosity and permeability as well as the forchheimer tensors from the numerical solution of closure problems boundary value problems from digital images of thin sections of rock samples the accuracy of this methodology was tested by proper comparison with experimental data of porosity and permeability performed using the same rock samples where the thin sections were manufactured the results showed acceptable agreement for those rocks exhibiting major anisotropy of permeability once the permeability calculation was performed it allowed computation of the forchheimer tensor which was analysed in terms of the fluid velocity a non linear dependency with the flow velocity was found for pore scale reynolds numbers larger than approximately 0 25 the forchheimer coefficient increased at an exponent around 2 from 1 7 up to 4 2 of average fluid velocity depending on lithology and local microstructure additionally we report that the forchheimer tensor exhibits anisotropy not only intrinsically with the local microstructure but also with respect to the flow direction credit authorship contribution statement c g aguilar madera conceptualization visualization j v flores cano supervision investigation supervision v matías pérez methodology validation methodology j a briones carrillo resources software resources f velasco tapia formal analysis writing review editing formal analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103616 appendix c supplementary materials image application 1 appendix a algebraic manipulation of closure problems for a 2d domain we calculate all the components of tensor d performing the dot product of eqs 2 to 6 by the x and or y basis vectors e x and e y by the right left or both sides as required which yields boundary value problem 1 in β phase 37 0 x d x x x d x x y y d x x 1 0 y d x x x d y x y y d y x 0 y d y x x d y y y y d y y 1 0 x d y x x d x y y y d x y in β phase 38 x d x x y d y x 0 x d x y y d y y 0 at a βσ 39 d x x 0 d y x 0 d x y 0 d y y 0 d x 0 d y 0 periodicity 40 d x r l i d x r d y r l i d y r d x x r l i d x x r d y y r l i d y y r d y x r l i d y x r d x y r l i d x y r i 1 2 3 permeability components 41 d x x k x x d y y k y y d y x k y x d x y k x y notice that eqs 37 38 include six dependent variables dxx dyy dyx dxy dx and dy that need to be solved simultaneously once the six dependent variables are solved then each component of the permeability tensor can be computed according to eq 41 following the above procedure for the boundary value problem 2 we have boundary value problem 2 in β phase 42 ρ β μ β v β x x m x x v β y y m x x x m x x x m x x y y m x x 1 ρ β μ β v β x x m y x v β y y m y x y m x x x m y x y y m y x ρ β μ β v β x x m y y v β y y m y y y m y x x m y y y y m y y 1 ρ β μ β v β x x m x y v β y y m x y x m y x x m x y y y m x y in β phase 43 x m x x y m y x 0 x m x y y m y y 0 at a βσ 44 m x x 0 m y x 0 m x y 0 m y y 0 m x 0 m y 0 periodicity 45 m x r l i d x r m y r l i d y r m x x r l i m x x r m y y r l i m y y r m y x r l i m y x r m x y r l i m x y r i 1 2 3 46 m x x h x x m y y h y y m x y h x y m y x h y x in this case we observe that as input data the pore scale velocity field must be provided which can be solved independently of closure problem 2 this dependency of tensor m with the pore scale velocity dictates the way the forchheimer coefficient will depend on the reynolds number appendix b effect of thin section resolution and flow angle orientation firstly in this part we provide a comparison of the porosity permeability and forchheimer coefficient computed in the sandstone laredo 1 rock sample using 10x and 4x resolutions according to results of fig b 1 the permeability is overestimated when the 4x resolution was used to photograph the thin section but however the results still present the same order of magnitude the porosity computation was similar for both cases of 4x and 10x resolutions whereas the forchheimer coefficient developed the same tendency with respect to the pore scale reynolds number nevertheless one contraction of scattered data was observed for reynolds numbers larger than 1 the results of fig b1 suggest that the permeability is the property most dependent on the photograph resolution the fig b2 presents the numerical estimation of the xx and yy elements of the forchheimer tensor while varying the flow angle orientation and reynolds number we modify the flow angle orientation by changing the pressure gradient direction in this case the sandstone laredo 1 rock was employed using a 10x resolution we note major effects at low reynolds number for reynolds numbers close to 10 the forchheimer coefficient tends to collapse and the effect of the flow angle can not be separately observed it is hard to elucidate one clear tendency of the flow angle effect over the forchheimer tensor due to the high heterogeneity of the local microstructure of porous rocks in general we found that flow at orientation 30 yields smaller values of the forchheimer tensor while flow at 60 maximizes the tensor however we recognize that such asseverations could depend on the orientation taken during the drilling of rock sample and manufacturing of thin sections in simple words we found that flow at given direction increases the corresponding forhcheimer coefficient for instance the flow in x direction increases the coefficient fxx 
456,in recent years significant breakthroughs in exploring big data recognition of complex patterns and predicting intricate variables have been made one efficient way of analyzing big data recognizing complex patterns and extracting trends is through machine learning ml algorithms the field of porous media and more generally geoscience have also witnessed much progress and recent progress in developing various ml techniques have benefitted various problems in porous media and geoscience across disparate scales thus it is becoming increasingly clear that it is imperative to adopt advanced ml methods for the problems in porous media and geoscience because they enable researchers to solve many difficult problems at the same time one can use the already existing extensive knowledge of porous media to endow ml algorithms and develop novel physics guided methods the goal of this review paper is to provide the first comprehensive review of the recently developed methods in the ml algorithms and describe their application to porous media and geoscience thus we review the basic concept of the ml and describe more advanced methods known as deep learning algorithms then the application of such methods to various problems in porous media and geoscience such as hydrological modeling fluid flow in porous media and sub surface characterization are reviewed we also provide a discussion of future directions in this rapidly developing field keywords machine learning deep learning data analytics subsurface systems geomedia physics guided artificial intelligence 1 introduction the world is facing momentous challenges regarding climate change natural hazards water resources and energy consumption facing such challenges and addressing them requires development of solutions to some very difficult problem problems among such problems are those related to geosciences and in particular characterization and modeling of porous media and understanding fluid flow transport reaction adsorption and deformation there both at small and large scales although tremendous progress has been made as the so called big data which are needed for the task have been available historically and are being produced more than ever faghmous and kumar 2014 gupta and nearing 2014 mohaghegh 2018 2017 monteleoni et al 2013 sahimi 1995 tahmasebi et al 2018 vandal et al 2017 much remains to be done technological advances in data gathering computational power and demand for making high resolution and accurate models and cloud systems have made it possible to leverage big data and complex computations some of such information e g remote sensing images are for example collected daily which results in producing a large volume of data unlike other fields however most of the data in porous media and geosciences are often available publicly and thus by accessing them one can develop more efficient techniques traditionally computational approaches and experiments have been used to characterize and model small scale porous media ranging from membranes to adsorbents catalysts and core scale porous materials such methods can however be quite expensive and time demanding and their use at larger scales pose significant difficulties or when the number of samples to be studied is very large for example extensive experiments are conducted in very long workflows in order to characterize and describe the heterogeneity of porous materials and compute their important properties such as permeability electrical conductivity and sorption capacity furthermore carrying out such experiments requires extensive and often expensive equipment and instrumentation and expertise as well as an environment that yields reliable results on the other hand computational methods have also made substantial advances and they now can produce with acceptable error data that are comparable with the experimental results moreover compared to the experiments computational methods provide a more controlled environment and produce accurate results over a smaller time scale and often more economically but these methods can still be expensive and more importantly the acquired experience from the previous calculations cannot easily be used for future computations unless the problems in the past and future are closely linked therefore ideally a combination of experimental and computational modeling may be the most efficient way of characterization of porous materials at a small scale both experimental studies particularly for large scale problems and computational methods require a deep understanding of the physics of the system and the effect of various variables on the process of interest if one is to make accurate predictions such methods may not be able to address accurate predictions for multiphysics problems if the necessary workflows have not been developed for example it is currently very difficult to study by experiments the effect of morphology fluid temperature and mineralogy on the deformation of porous media and materials due to the necessity of calibrating various parameters in a similar manner coupling the physics flow reaction and deformation in a consistent and seamless computational framework remains very challenging one important approach to analyzing big data and using them in the modeling of porous media related problems is based on the use of artificial intelligence ai large datasets are almost always necessary for proper evaluation comparing ml algorithms and assessing their performance the need for big datasets in the ml contexts stems from the fact that the analytical examination and comparison of the most flexible ml algorithms can range from very difficult to nearly impossible thus big data is not only necessary for developing a successful application of ml but is also critical in verifying which ml performs better unlike the standard statistical methods that often do not perform well in dealing with big data the ml methods are more effective when they use big data significant progress has recently been made in the use of ai machine learning ml and more recently the so called deep learning dl for modeling of small scale porous media briefly dl is a subset of ml and ml itself is a subclass of ai ai is defined for most of those algorithms that try to solve a problem by integrating an algorithm with intelligence thus they can either perform work better than humans or they can intelligently solve a problem while ml refers to those methods that can use the computer power to learn a specific job and make decisions the dl algorithms on the other hand are developed to mimic the pattern recognition of the human brain moreover due to the aforementioned issues the ml techniques have emerged as promising alternatives for dealing with multi physics processes in particular discovering latent patterns extracting important features and identifying the connections between various variables when one deals with a large amount of data is not straightforward if one relies on the more traditional approaches whereas the use of the ml approaches exhibits much potential for helping one to make more informed and accurate decisions porous media problems from those at the smallest scale the nanoscale to the largest at field scales are often endowed with big data that are spatially distributed and can dynamically vary as well which add another level of complexity mohaghegh 2018 sahimi 1995 tahmasebi et al 2018 the growing availability of big data offers fertile grounds for the use of various ml and data mining techniques kim 2016 mohaghegh 2018 several key attributes distinguish the data in porous media and subsurface systems from the common data in other fields for example unlike the regular image recognition field in computer science the data in porous media are based on physical laws as such the ml techniques cannot sometimes be applied to such data directly implying that such techniques should be reformulated and tailored to the existing data unlocking the big data in geosciences and taking adaptable decisions require developing more advanced techniques for identification of complex dependencies and patterns which are otherwise either very difficult or nearly impossible all such issues along with the explosion of data sensing tools as well as the continuing development of advanced computational algorithms have provided the impetus for developing ml and automated methods that can address the aforementioned issues thus as we hope this review will demonstrate the ml techniques can be used as a very efficient alternative for identifying the patterns in large and multidimensional data such that linear and nonlinear correlations among various physical parameters can be discovered and extracted the results obtained so far by the application of the ml in engineering and science have indicated the discovery of new physics and patterns that could not be discerned before or required such a significant amount of time and resources as to render them practically impossible to use such abilities of the ml techniques have aided the researchers to uncover the potential of big data and enhance our understanding of complex phenomena related to porous media from small to the largest scale in geomedia based on such considerations the purpose of this paper is to provide a comprehensive review of the state of the art methods in the ml and advanced data analytics methods although a few reviews have already been published abrahart et al 2012 karpatne et al 2019 maier et al 2010 shen 2018 tyralis et al 2019 they are however specific to narrow issues whereas the present paper aims to describe and discuss the key studies in the field of porous media at various scales in table 1 we provide a brief overview of the main applications of the ml each of which will be discussed in this paper such methods include both supervised and unsupervised classes of the ml algorithms in fact the supervised methods are built based on true outputs in such methods one knows exactly what the response should be whereas the true outputs are not available in unsupervised algorithms in other words the supervised algorithms link two known datasets input and output by best estimating the parameters in the ml which allows one to quickly optimize the training this issue can be problematic in unsupervised methods as the network should discover the responses at the same time this freedom can allow the ml methods to identify the latent features more broadly as the output is not restricted to a specific response see fig 1 the rest of this paper is organized as follows first the basic concepts of artificial intelligence are reviewed then some of the popular data analytics and statistical methods are summarized the applications of ml and data analytics methods for porous media are discussed in the next section finally our conclusions and vision for future developments are summarized 2 artificial intelligence machine learning and data analytics artificial neural networks anns are computing systems that imitate the working mechanism of human brains they can complete their tasks without being explicitly programmed to follow some specific rules chen et al 2019 generally speaking the anns consist of parallelly operated elements named neurons that resemble the basic unit of the nervous system mathematically speaking a neuron in an ann serves as a nonlinear parameterized and bounded function which receives input data from the outside world and produces or outputs its predictions such as the value of a target variable in this context two different methods usually are applied to parameterize the function the first method parameterizes the input variable and forms a global input as a linear combination of the input variable xi weighted by a parameter wi named the weights that signifies the relative significance of the input the output y of the neuron is obtained as a nonlinear function of the global input such that 1 y f b i 1 n w i x i where b is the bias which is initially a random number eq 1 is shown graphically in fig 2 a for a simple model and in fig 2 b for a more complex and multilayer network a nonlinear transform function can also be parameterized given fas a gaussian radial basis function the output is calculated by 2 y exp i 1 n x i w i 2 2 b 0 2 where the wi are the coordinates of the multivariate normal distribution and a bias parameter b 0 is assumed along with all directions a neural network is represented mathematically as a nonlinear function of two or more neurons that can be connected through various ways and the structure of the network describes exactly the procedure for generating the output by combing and weighting the inputs the most common and widely used ann is the multilayer feedforward neural network the feedforward architecture allows data to be transferred only from the inputs to outputs with no loop appearing in the network as shown in fig 2 b a multilayer feedforward neural network contains three types of layers namely the input the hidden and the output layers the number of neurons in the input and output layer depends on the number of independent and dependent variables respectively for the hidden layer the number of neurons is a hyperparameter and can be optimized in order to achieve better performance this type of network is a so called supervised learning algorithm sla and is especially suitable for classification and regression the task of the sla is learning the relation that maps the input data that produces the output such input output pairs are usually referred to as the training data which consist of a set of training examples that are pairs of an input object and the desired output value based on which a function is inferred after analyzing the training data the sla produces a function that is used for mapping new examples generally a loss function l y y is first defined to measure the difference between the predicted value y and the true value y then an optimization algorithm is used to minimize the predicted error the difference between the two by iteratively updating the weight and bias parameters the most commonly used and effective algorithm to train an ann is backpropagation introduced by rumelhart et al 1985 based on the chain rule of differentiation the backpropagation algorithm computes the derivative of the loss function l with respect to all the variables and parameters in the network then the derivatives are utilized to update the weight and bias parameters according to a gradient descent algorithm ruder 2016 generally speaking the backpropagation process usually involves three stages the feedforward pass to calculate output based on the input data backpropagation of the associated error and finally updating of the weight and bias parameters mathematically a training example x is selected at random from the dataset and is fed into the network the value of each node in the hidden layers and the output layer is denoted by vj and y k respectively to this end the loss function l y k y k is calculated in order to measure the difference between the network output and real value for each output node k for which one calculates 3 δ k l y k y k y k l k a k where lk is the activation function for kth output node and ak is the weighted sum or the input for the activation function given the value for each node in the intermediate prior layer one calculates 4 δ j l j a j k δ k w k j where wkj is the weight from node j to k such a procedure is repeated to produce δ j for the jth node until the input layer each δ j computes the derivative of the loss function with respect to the jth node s activation function input given the values vj calculated in the forward pass and the values δ j calculated in the backward pass the derivative of the loss function l with respect to w jj is computed by 5 l w j j δ j v j the parameter is then updated based on gradient descent 6 w j j t w j j t 1 η l w j j t 1 where η is the learning rate that controls how much we adjust the weight with respect to the calculated loss gradient currently the most widely used optimization algorithm is the mini batch gradient descent which attempts to combine the strength of stochastic gradient descent sgd and batch gradient descent bgd the sgd updates the parameters after the computation on each training example for robustness whereas the bgd updates the parameters after the evaluation of all the training examples for efficiency there are also many variants of the sgd such as adagrad duchi et al 2011 adadelta zeiler 2012 rmsprop tieleman and hinton 2012 and adam kingma and ba 2015 the anns are reliable techniques when dealing with problems with an incomplete dataset or with highly complex and ill posed problems where decisions are usually made based on intuition they have been applied successfully to many problems in a number of fields bishop 1995 and can also be used for functional approximation haque and sudhakar 2002 prasad et al 2009 the anns are actually nonlinear parametric function approximators that establish a mapping between multiple inputs and a single output they can also be used effectively to solve problems in pattern recognition such as for instance in sound george et al 2013 olmez and dokur 2003 image sharma et al 2008 zhou et al 2002 or video recognition bhandarkar and chen 2005 karkanis et al 2001 such tasks can be completed without a priori definition of a pattern in such cases the anns learn to identify new patterns and have the ability to associate memories borders et al 2017 michel and farrell 1990 which means they can recall a pattern when given only a subset clue the network structure for such applications is usually complex with many interacting dynamic neurons 2 1 boosting algorithms boosting derives from the computational learning theory alpaydin n d freund 1995 hastie et al 2009 james et al 2000 schapire 1990 and usually performs a classification task it is one of the ensemble methods that aims to improve the prediction accuracy of a classifier by converting multiple weak learners into a strong one hence boosting it the main principle is that a single weak learner has limited capabilities and is difficult to improve but it can be combined with others to build a powerful and stable learner adaboost freund and schapire 1997 is the first successful boosting algorithm developed for binary classification the weak learner or the base learner used in adaboost is a decision tree with one level referred to as the decision stump generally speaking adaboost algorithm contains the following steps initially a weak learner is trained with an equally weighted training dataset then the distribution of the training data is adjusted based on the predicted results of the trained weak learner specifically the misclassified training data are associated with larger weights and then are used to train the next weak learner such steps are repeated until the number of weak classifiers recaches a predefined value finally the weak learners are weighed and combined to form a strong learner gradient boosting friedman et al 2000 extended the method of regression it works similar to a numerical optimization algorithm and iteratively searches for a new additive model that reduces the loss function at each step specifically an initial guess is made with a decision tree to maximally reduce the loss function then at each step a new decision tree is fitted to the residual between the data and the predicted results and added to the previous model to update the residual this step is repeated until a predefined maximum number of iterations is reached note that the decision trees added to the model at prior stages are not modified the added decision tree at each step is usually rescaled with a shrinkage parameter between 0 and 1 it is believed that a higher number of small steps provide higher accuracy than a lower number of large steps touzani et al 2018 in each iteration instead of using the complete training dataset a randomly selected subsample is used to fit the decision tree which can efficiently reduce the computational cost xgboost chen and guestrin 2016 which stands for extreme gradient boosting is a version of the gradient boosting machine but focuses on improving the computational speed and model performance xgboost uses a second order taylor s expansion for the objective function rather than a full objective function for optimization which allows faster computation moreover it introduces some regularization terms to prevent overfitting 2 2 principal component analysis principal component analysis pca is a nonparametric approach used for reducing the dimensionality of a dataset while preserving as much variability statistical information as possible it was invented by pearson pearson 1901 and later developed independently by hotelling hotelling 1933 the pca applies an orthogonal linear transformation to reframe the original correlated data in a new coordinate framework called principal components which are linearly uncorrelated jolliffe and cadima 2016 the projected data on each principal component conserves the maximum variance generally speaking the components are ordered according to the variance of the projected data which means the data projected onto the first component has the largest variance while the variance for the last one is the smallest huang et al 2017 there are two main approaches for the pca eigenvalue decomposition of the data covariance matrix and singular value decomposition svd of the centered data matrix a data matrix x of size m n with m and n being respectively the number of samples and the number of variables is centered by subtracting the column wise mean of x from the corresponding column the covariance matrix c of x a n n symmetrical matrix is defined by c x t x m 1 with t denoting the transpose operation the eigen decomposition diagonalizes c to produce vλv t where the columns of v and the diagonal elements λ i i 1 n of λ are the eigenvectors and eigenvalues of c respectively note that the eigenvalues of c are arranged in decreasing order along the main diagonal of λ and are paired with the columns of v the principal components are the columns of xv alternatively the svd of x yields usv t where the diagonal elements si i 1 n of s are related to the eigenvalues by λ i s i 2 n 1 the principal components are the columns of us generally one selects the first few significant principal components and then projects the data onto them jolliffe 2006 there are many applications of the pca that due to space limitation cannot all be described here it has for example been applied to exploratory analysis and data visualization hibbs et al 2005 pei et al 2007 in this case the pca projects high dimensional data onto a small number of principal components then one visualizes the transformed data in a projected 2d or 3d space the pca based clustering analysis is also very common metsalu and vilo 2015 yeung and ruzzo 2001 the variation trends of the data are mainly represented by the first few principal components while the residual noise is extracted by other components then it is possible to cluster the data by projecting the data onto the first few components regression analysis can also benefit from the dimension reduction provided by the pca abdul wahab et al 2005 camdevyren et al 2005 as sometimes direct regression analysis with high dimensional data could result in saturated models and unreasonable estimates furthermore it is possible to first perform the pca and then use the first few principal components as the inputs to facilitate regression analysis in the literature several variants of the pca have been developed with better numerical performance and desired properties functional pca aguilera et al 1999 allows predicting a time series beyond the discrete time observation times in the past sparse pca zou et al 2006 adds sparsity constrains to the inputs which searches for a linear combination of a few input variables instead of all of them multilinear pca lu et al 2006 can capture useful information from tensor representation and utilizes them to recognize faces kernel pca hoffmann 2007 as a nonlinear feature extractor is capable of preprocessing the dataset to simplify the classification problem robust pca candès et al 2009 is a modification of the standard pca that works well for grossly corrupted observations through decomposition in low rank and sparse matrices 2 3 multidimensional scaling multidimensional scaling mds is one of the widely used methods for visualizing the similarity between several datasets machado and lopes 2020 it calculates the pairwise distances between the datasets and represents them as n points in a cartesian space where n is the number of data points compared to the pca the mds can capture nonlinear dimensionality reduction and provide a more accurate representation of large datasets generally speaking given a distance matrix for representing the dissimilarity between each pair of point object and based on the desired number of dimensions n the mds inserts each data point in an n dimensional space in a way that the distances between the objects points are preserved after which the final distribution is visualized mathematically speaking let us assume that the distances between several points objects are stored in d dij where dij indicates the distance between points i and j d i j x i x j 2 y i y j 2 then the matrix b 1 2 j d 2 j formed where j is a centering matrix next the largest m eigenvalues and their corresponding eigenvectors e m are calculated based on which x e m λ m 1 2 is computed λ m is the diagonal matrix of b chen et al 2008 2 4 support vector machine support vector machine svm is a supervised learning algorithm that does not require any parameterization and hence no assumption about the true distribution of the dataset cortes and vapnik 1995 developed the current basic svm algorithm which due to its robustness and efficiency has been widely used for solving classification and regression problems the key idea of the svm is to project the dataset into another feature space where the dimensionality is higher compared to that of input space which makes the classification task much easier krell 2018 a simplistic but effective way of understanding how the svm works is by viewing it as a sort of a machine that separates the left and right side cars buildings and people on the sidewalks in order to make the widest lane the support vector classification aims to build a decision boundary within the feature space so that each data point is inserted into its corresponding class noble 2006 unlike other classification methods an optimal hyperplane is generated by the svm to maximize the margin between two classes and reduce the generalization error see fig 3 the distances between the hyperplane and the nearest point in each class are summed and maximized by the svm in practice if a dataset belongs to two different classes it is not always separable into two classes in this case the svm still produces a hyperplane that maximizes the margin and minimizes the classification error with a predefined loss function chapelle 2007 the svm is more appropriate for the classification problem where the classes can be separated with a linear straight boundary ben hur and ong 2008 however a curved decision boundary is also very common in the real world kernel functions as nonlinear terms can help transform the data and project onto a new space in which the differences are more visible many kernel functions have been utilized with the svms such as linear polynomial and radial basis function and sigmoid typical svm classifiers include hard margin svm pan et al 2005 soft margin svm wu and zhou 2005 k nearest neighbor svm zhang et al 2006 radial basis function svm prajapati and patle 2010 and translational invariant kernels svm decoste and scholkopf 2002 the support vector regression assumes that we have no knowledge about the joint distribution of the input and output variables and that a correlation between them can be found thus it tries to infer a function y f x according to the training dataset x xi yi i 1 n where xi is the ith input sample yi is the corresponding target value and n is the number of training samples this goal is equivalent to a regression function of the form 7 f x i 1 n a i a i k x i x b where k is a positive definite kernel function and a a and b are the parameters of the model a and a are calculated by minimizing the following objective function 8 1 2 i j 1 a i a i a j a j k x i x j ε i 1 n a i a i y i i 1 n a i a i which is subjected to the following constraint 9 i 1 n a i a i 0 and a i a i 0 c where parameter the c governs the smoothness of the approximation function and ε governs the precision of the approximation function there is an interaction between the parameters and hence increasing or decreasing one of them may change the values of the others both linear and nonlinear kernel functions have been used for support vector regression but the results provided by a nonlinear function have higher precision the optimization problem defined by eqs 8 and 9 is a general frame for the support vector regression problem 2 5 k means clustering k means clustering kmc forgy 1965 lloyd 1982 is used to separate n data points into k clusters specifically each dataset is classified into a cluster that has the smallest difference between its mean value and the value of the current data point the kmc can deal with a dataset with high dimensionality to identify the representative data which are known as the cluster centers for a training dataset x xi i 1 n the cluster centers μ μ j j 1 k are first randomly picked from the dataset then the following procedure is repeated until convergence is achieved for each data point xi one calculates the euclidean distance between xi and each cluster center and assigns the current data point to the nearest cluster s center c i argmin j x i μ j 2 for each cluster one recomputes its mean value to update the cluster s center i e one computes μ j i 1 n c i x i i 1 n c i the kmc stems from signal processing where it was originally used for vector quantization kovesi et al 2001 lee et al 1997 which means reducing the color palette of an image to a fixed number of colors currently it is used for cluster analysis kaur et al 2019 oyelade et al 2010 and feature learning coates et al 2011 lin and wu 2009 note that the number of clusters k is usually difficult to choose considering the fact that no external constraints are given moreover compared to other more sophisticated feature learning approaches such as autoencoder and the restricted boltzmann machine the kmc usually needs more data to achieve comparable performance since each data point only contributes to one feature coates and ng 2012 2 6 k nearest neighbor k nearest neighbor knn is a non parametric algorithm designed to solve regression and classification problems altman 1992 it evaluates the distance similarity between the test and training examples based on a predefined metric function thus the class of a test example depends on the majority vote from the selected neighbors specifically the distance between the test sample and all the stored training samples is calculated using a certain distance function after which k nearest neighbors of the test sample are selected then the test sample is assigned to the class that is most common among the selected neighbors the optimum k value is solved either by using all the examples and taking the inverted indexes jirina and jirina jr 2008 or by using ensemble learning hassanat et al 2014 since one must calculate the distance between each test sample to the selected training examples and store all training samples the computational cost and memory requirements are usually unaffordable various solutions have been proposed to overcome the two problems such as reducing the size of the training dataset gates 1972 hart 1968 and using approximate knn classification arya and mount 1993 zheng et al 2016 the knn preserves the consistency of the calculated results as the number of samples approaches becomes very large the error rate for a two class classification problem is not worse than the bayes error rate which is the lowest possible error rate for the given data distribution 2 7 random forest an algorithm for ensemble learning used for classification and regression is the random forest rf algorithm the rf constructs several decision trees in the training process it then produces the class that is the mode of the classes for the classification problem and the mean prediction value for the regression problem barandiaran 1998 breiman 2001 ho 1995 see fig 4 the rf in a broader view is constructed based on the idea of bagging or bootstrap aggregating proposed by breiman breiman 1996 to improve the classification methods in essence the bagging uses a noisy fitted function and based on its average it reduces the variance the decision trees are constructed by randomly picking a subset of training examples with replacement i e some examples are selected two or more times while others will not appear in that subset at all each decision tree is independently produced without any trimming and each node is split using a predefined number of randomly selected features by growing the forest up to a predefined number of trees it creates trees that have high variance and low bias belgiu and dragu 2016 given a training dataset x xi i 1 n with targets y yi i 1 n the original bagging algorithm eliminates a sample from the training data b times and fits a tree which results in new training and target data x b and y b this goal is achieved by training fitting a regression tree fb on x b and y b then the final estimate for a new data x is obtained by f 1 b b 1 b f b x based on this algorithm the rf method selects a random set of m features where m p or m log 2 p from each split location out of p original number of features during the learning process this helps with reducing the variance and improving accuracy thus the final classification decision is taken by averaging the class assignment probabilities calculated by all the produced trees the rf has been successfully used in several fields including classification of hyperspectral data ham et al 2005 species immitzer et al 2012 cancer statnikov et al 2008 traffic sign zaklouta et al 2011 and land cover gislason et al 2006 2 8 convolutional neural networks convolutional neural networks cnns developed by lecun lecun 1989 lecun and yoshua 1998 are feedforward neural networks since the data are passed from the input to the output only along the forward direction the input data for the cnn usually have a grid like topology including time series data 1d data grayscale images one channel with 2d inputs color images three channel data with 2d inputs and multidimensional time varying data 3d e g seismic data various architectures of cnn have been proposed in the literature with most of them containing convolutional and pooling layers see fig 5 generally speaking one or more fully connected layers are connected after the convolutional and pooling layers convolutional layers extract the features representing the input data specifically the convolutional operation moves a filter i e kernel across the input layer in a certain order and generates a feature map each movement produces one unit in the feature map by calculating a dot product between the filter and a small local region of the input typically there is more than one filter and each of them is assigned to detect a specific feature the dimension of the filter is usually much smaller than that of the input the mathematical formulation for the output of a convolutional layer expressed by 10 z i j σ k 1 k m l 1 l n w k l x i k 1 j l 1 b where w is the filter with a size of m n x is the input data or the output from previous layers b is the bias for the current convolutional filter and z is the output for the convolution operation in each convolutional layer a stride parameter s is defined to control the skip distance between consecutive filter moves the input data are sometimes padded with zeros around the border in order to control the spatial size of the output the size of zero padding p is a hyperparameter meaning that it should be optimized for a 2d input with dimensions of w h and a filter of size f f the size of the output from the convolutional operation or the size of the feature map is w f 2p s 1 h f 2p s 1 a nonlinear activation function σ is used to conduct an element wise transformation after the convolution operation traditionally the sigmoid and hyperbolic tangent functions were used to add nonlinearity but they bring about a vanishing gradient problem to retard the training process kolen and kremer 2010 glorot et al 2010 found that a piecewise linear activation function namely a rectified linear unit relu can relieve that problem relu defined as max 0 x where x is the input is faster to compute and outputs a sparse representation i e only about half of the hidden units are active and have non zero outputs the pooling layers aim at reducing the spatial resolution of the feature maps and canceling the influence caused by distorting and translating the input data ranzato et al 2007 average pooling is one of the popular functions that computes the mean value of a small local region in the feature maps and then passes it to the next operation recently max pooling has also been used for sending the maximum value of a small region to the next layer scherer et al 2010 found that max pooling technique is skilled in extracting the invariant features from image like data improving the generalization performance and converging much faster hence outperforming the traditional subsampling operation to extract the latent features with a high degree of abstraction multiple convolutional and pooling layers are connected where the simple features extracted by lower convolutional layers are further processed by the following layers to produce more complex feature representations the fully connected layers at the final layers aim to understand and utilize the extracted high level features and complete the reasoning tasks such as regression or classification zeiler and fergus 2014 the key idea of convolution operation namely sparse interactions parameter sharing and equivariant representations is to reform the world of machine learning and make them closer to human s abilities goodfellow et al 2016 for example the sparse interactions are achieved by reducing the size of the filter as they are smaller than that of input data and result in less memory volume and higher efficiency of the computations parameter sharing indicates that instead of studying a different set of parameters for each location only one set needs to be studied and therefore it further relieves the storage demanding for model parameters equivariant representations also imply that the way the output changes is the same as that of input data the capability of cnn to learn hierarchical representations of context invariant features enables its application to image classification and recognition there are many cnn based deep architectures in the literature lenet lecun et al 1998 lecun et al 1989 a pioneering work by lecun is used for handwritten digit recognition and later for reading zip codes digits alexnet krizhevsky et al 2012 is similar to lenet but much bigger and deeper with all the convolutional layers stacked together which won the ilsvrc 2012 competition award googlenet szegedy et al 2015 introduced a new architecture called inception in which the depth of the network is increased but with fewer parameters than the alexnet vggnet wang et al 2015 does a thorough analysis of the depth factor in a convnet and efficiently controls the number of parameters by using very small 3 3 convolution filters in all the layers resnet he et al 2016 is a learning framework in which the layers learn residual functions with respect to the inputs instead of learning unreferenced functions it was demonstrated that the residual networks are easier to optimize to gain much better accuracy the aforementioned cnns utilizes typically one or more fully connected layers between the last convolutional layer and the classifier in order to enable classification into a small number of classes currently end to end fully cnns such as the u net ronneberger et al 2015 and segnet badrinarayanan et al 2015 are preferred both of which have been used for image segmentation 2 9 autoencoder autoencoder is a widely used unsupervised learning algorithm which attempts to accurately reconstruct or reproduce the input data autoencoder was first introduced by hinton and the pdp group to address the problem of backpropagation without a teacher by using the input data as the teacher rumelhart and mcclelland 1987 a typical autoencoder consists of an encoder a decoder and a latent layer see fig 6 given the input data x r n where n is the data dimension the encoder function first projects it onto a latent space h rl with a weight matrix w n bias b n and an activation function σ r 0 1 11 h σ w n x b n the decoder function then transforms the latent representation into the output z r n through an inverse mode with a weight matrix w h bias b h and an activation function δ r 0 1 12 z δ w h h b h the tied weight strategy w n w h w has usually been applied to simplify the network structure therefore the parameters to be solved for are w b n b h given the training samples the objective function to train an autoencoder is to minimize the cost function 13 arg mi n w b n b h l x z which measures the difference between the input and reconstructed data by for example the mean square error the autoencoder is mostly used for reducing the dimensionality of input data and generating a latent representation of them while neglecting the effect of noise that may appear in the dataset specifically the implicit probability distribution of the data is estimated based on the training dataset with new samples drawn from the inferred data distribution the training data with high dimensionality is compressed to a low dimensional latent representation based on the assumption that the real data could be reconstructed from them with the help of the decoder this assumption is based on the relative characteristics of the patterns of the output that are stored in the latent layer and interpreted by the model there are different types of variants for autoencoder sparse autoencoder makhzani and frey 2013 adds regularization the sparsity constraint on the hidden neurons or the latent representation a neuron is defined as active when its output is close to one and inactive when it is close to zero a sparse autoencoder reduces the number of neurons being used by disabling some of them the constraint is realized by a penalty named kullback leibler kl divergence kullback and leibler 1951 14 d k l ρ ρ j ρ log ρ ρ j 1 ρ log 1 ρ 1 ρ j where ρ j is the average activation of the jth hidden unit with the average taken over the training set and ρ is the desired activation value close to zero a denoising autoencoder vincent et al 2008 was proposed for dealing with the small variations of the input data which can generate the same result even if the input data are partially destroyed in the training process we first corrupt the input data x with one of the two main kinds of corruption methods binary noise and gaussian noise in order to obtain a destroyed version x for binary noise several data points are randomly picked and their values are set to zero gaussian noise generates a set of random values following the gaussian distribution and adds them to the original data the destroyed input is sent to the vanilla autoencoder different from the archetypal autoencoder the outputs from encoder and decoder of variational autoencoder represent a sample drawn from a parameterized probability density function doersch 2016 given an example x the probabilistic encoder produces a distribution q φ h x e g gaussian over the possible values of latent representation h rather than a single point where the model parameters and latent states sample this statistical distribution h qϕ h x the probabilistic decoder p θ x h is a conditional generative model that computes the probability of generating x given the latent variable h a variational autoencoder then tries to approximate p θ x h by the given distribution q φ h x it is also of high significance to keep the balance between the reconstruction accuracy and matching the gaussian distribution therefore the loss function consists of these items the reconstruction accuracy is measured by the mean square error and the dissimilarity between the distribution of the latent representation while the gaussian distribution is represented by the kl divergence variation autoencoder could produce samples that do not appear in the real training dataset while the fully connected autoencoder commonly ignores the spatial structure within the image a convolutional autoencoder masci et al 2011 is proposed to handle this problem the convolutional autoencoder is a combination of the vanilla autoencoder and convolutional layers some redundancy of the model parameters is introduced to force the learned representation to be global i e covering the entire input data moreover since the weights and bias are shared as the filters are moved over the input data the spatial locality can be conserved on the other hand the basic autoencoders can be concatenated on top of each other in order to construct a deep stacked autoencoder qi et al 2014 which consists of an input layer several hidden layers and an output layer a stacked autoencoder inherits the benefit of deep neural networks and can still learn a deep representation of the input the first hidden layer tends to learn the low level features of the input data the hidden layer that follows it typically produces the high level features extracted from the pattern that appeared in the previous levels therefore a hierarchical representation of input data is extracted 2 10 generative adversarial networks generative adversarial networks gans are deep generative models that utilize various training methods in order to infer the probability density function of the real data based on the training dataset the standard gan consists of two adversarial networks a generator and a discriminator to compete with each other see fig 7 the goal of the generator is to produce realistic images that can deceive the discriminator while the discriminator tends to identify the fake images generated by the generator network and classifies the images sampled from the given image dataset as real goodfellow et al 2014 i j 2015 this minimax game is an alternative and creative means of inferring the probability density function of the original image dataset the gans have been used in many fields such as image to image translation ma et al 2018 improving image resolution wu et al 2017 medical image processing armanious et al 2018 and video frame prediction a x a x lee et al 2018 in all such applications the gans have shown to be superior to the other traditional methods in terms of producing images with high resolution and fidelity the vanilla gans draw samples directly from the assumed data distribution and do not explicitly model the underlying probability density function the input z for the generator g is simply random noise drawn from a prior distribution p z typically a gaussian or a uniform distribution the output xg from the generator is expected to be visually similar to the real sample xr which is sampled from the real data distribution pr x thus the generator tries to learn a nonlinear mapping function parameterized by θ g xg g z θ g the input for the discriminator d is either a real or produced synthesized sample and the output from the generator is a single number that indicates the probability of the input is a real or fake sample hence the discriminator tries to learn a mapping parameterized by θ d yd d x θ d the approximation of the real data distribution pr x is denoted by pg x the discriminator tends to separate these two groups of samples while the generator is taught to deceive the discriminator as much as possible in such a process the gradient information is backpropagated from discriminator to generator so the generator updates its parameters in order to generate a sample that can deceive the discriminator the objective functions for the generator and discriminator are given by 15 l g min g e x g p g x log 1 d x g 16 l d max d e x r p r x log d x r e x g p g x log 1 d x g clearly the discriminator is a binary classifier with a maximum log likelihood objective function the desired result after training is that distribution formed by the generated xg that approximates the real data distribution pr x it is still very difficult however to train a gan due to its large number of training parameters and applied nonconvex cost function more specifically the generator and discriminator have trained alternately from scratch with the gradient based method but there is no guarantee for the balance between the training of generator and discriminator as a result the discriminator can sometimes become too strong compared to the generator and the produced samples become too easy to be separated from the real ones the gradient from discriminator will approach zero which provides no guidance for further training of the generator another problem commonly encountered is model collapse when the approximated distribution pg x learned by the generator focus on a few limited modes of the real data distribution pr x nguyen et al 2017 despite such training problems a number of variants of the gan are developed every year conditional generative adversarial nets cgan mirza and osindero 2014 is a conditional version of the gan it is conditioned on both the generator and the discriminator for example it can generate mnist digits conditioned on the class labels deep convolutional generative adversarial networks dcgans radford et al 2015 were developed for producing high dimensional results infogan chen et al 2016 is used for extracting useful features and representing them as encoding in an unsupervised manner when the dataset is simple class labels are unavailable and one only wants to see the most important features of a dataset and have control over them the infogan is preferred wasserstein gan wgan arjovsky et al 2017 incorporates the wasserstein distance into the loss function in order to make it much more stable that depends on the quality of the generated data 2 11 long short term memory long short term memory lstm is a recurrent neural network rnn used mainly for processing time series data originally discovered by hopfield 1982 and extended by williams et al 1986 see also nasrabadi and choo 1992 the rnns are especially suited for modeling time sequence data and the data with variable lengths they have the ability to selectively deliver the information through successive steps while dealing with sequential data one element at a time unlike feedforward neural networks the rnn has the so called recurrent edges to form circles which are self connected from a node to themselves across time specifically at time t the node with recurrent edges receives input data points x t and hidden node values h t 1 from the previous state the output y t at each time t is calculated based on the hidden node values at the same time thus the input x t 1 at time t 1 influences the output y t at time t through the recurrent connections mathematically the forward pass in a simple rnn is governed by following equations 17 h t σ h x t w hh h t 1 b h 18 y t σ y w yh h t b y where w hx is the weight between input and hidden node w hh is the weight for hidden node between successive time steps and w yh is the weight between hidden node and output b h and b y are bias parameters representing an offset σ h and σ y are activation functions such as tanh and relu to introduce nonlinearity as shown in fig 8 the network can be interpreted as noncyclic by unfolding it as a deep neural network with one layer per time step and shared weights across time steps using backpropagation the unfolded network is trained over many time steps werbos 1990 however backpropagating errors through many time steps may give rise to vanishing gradients pascanu et al 2012 described a comprehensive analysis of the vanishing gradient problems and in order to address it suggested adding a regularization term that imposes weights on the values so as to prevent the gradient from vanishing in particular the lstm hochreiter and schmidhuber 1997 was introduced to overcome this problem which replaces the hidden unit with a memory cell that contains a node with a self connected recurrent edge of fixed weight one to guarantee the flow of gradient across many time steps all elements of the lstm are illustrated in fig 9 the input node g t calculates a weighted sum of the current input data x t and previous hidden unit h t 1 and then runs through a tanh activation function 19 g t tanh w gx x t w gh h t 1 b g the input gate i t receives the same inputs as the input node but with a sigmoid activation function for controlling the information flow 20 i t sigmoid w ix x t w ih h t 1 b i specifically if its values are zero then flow from the input node is cut off if its value is one all the flow is passed through the forget gate f t introduced by gers 2001 also receives the same inputs as the input node and allows the network to flush the contents in the internal state 21 f t sigmoid w fx x t w fh h t 1 b f the internal state s t is the heart of each memory cell with a linear activation function it has the aforementioned recurrent edge with fixed unit weight to avoid gradient vanishing the internal state receives information from the input node g t and the previous internal state s t 1 under control of the input gate i t and forget gate f t 22 s t g t i t s t 1 f t where is pointwise multiplication the output gate o t and the internal state s t yield the value h t of the hidden unit at the current time step 23 o t sigmoid w ox x t w oh h t 1 b o 24 h t tanh s t o t usually the internal state runs through a tanh function to endow each cell s output with the same dynamic range as an ordinary tanh hidden unit currently the relu activation function is more common due to its greater dynamic range generally in the forward pass the lstm learns when to let activation operation into the internal state by the input and output gates if the two gates are closed their values are zero activation is dormant in the memory cell and does not affect the output at intermediate time steps in the backward pass the constant recurrent edge of the internal state governs the flow of the gradient over many time steps implying that the gates learn when to let error in and when to let it out therefore the lstm has a much better ability than the rnn to learn long range dependencies various lstm networks have been developed to process data in the real world generally speaking they can be divided into two categories the lstm dominated networks that are mainly built by the lstm cells for optimizing the connections of the inner lstm cells in order to enhance network properties such as stacked lstm network saleh et al 2018 bidirectional lstm network graves and schmidhuber 2005 multidimensional lstm network graves et al 2007 graph lstm network liang et al 2016 grid lstm network kalchbrenner et al 2015 and convolutional lstm network x shi et al 2015 integrated lstm networks which consist of the lstm layers and other components such as the cnn to integrate the advantages of different components such as dbn lstm network vohra et al 2015 cfcc lstm network y y yang et al 2018 c lstm network zhou et al 2016 and lstm in lstm network j j song et al 2016 have also been developed 3 applications given a large amount of data collected and stored over the past years the ml has been widely used in several massive and complex data intensive fields such as geoscience hydrology energy resources biology medicine and economy based on the application of the ml in other disciplines we can get an initial impression about the kinds of problems for which the ml is well suited and the state in which the ml is applied in various disciplines in what follows we briefly describe the application of the ml to two broad categories of disciplines namely fine and large scale systems involving porous media and describe how the ml methods are applied to solve specific problems in each field 3 1 small scale porous media unlike the large scale systems that have been investigated within the ai framework for a long time fine scale porous media have only recently experienced significant progress in the application of the ml methods one reason is the availability of high quality images thanks to the progress in imaging and non destructive scanning furthermore by the leap advances in characterization of complex materials sahimi 2003 torquato 2002 as well as the emerging stochastic simulation tahmasebi 2018 tahmasebi and sahimi 2015a 2013 using advanced ml and deep learning dl methods are becoming more practical such methods can produce a vast amount of data that is necessary for training the neural network in a reasonable time in this section we put the emphasis on geomaterials and in particular recent studies based on the dl important properties of porous media such as permeability and porosity as well as those describing the state of a porous medium in which two fluids are flowing such as saturation are among the properties that determine the efficiency of many other operations involving porous media estimation of such properties has been always a challenge to scientists both experimentally and computationally clearly the experimental methods can be costly and often need a considerable amount of time which makes them difficult to use on a large number of samples and hence the uncertainty and in turn an accurate estimation of the intrinsic properties of porous media is difficult to address similarly computational modeling with numerous samples is also time demanding through recent advances in the ai techniques as described earlier the prediction of such important properties without carrying out time consuming experiments is becoming possible a review of the literature indicates that as far as the application of the ai to various problems involving small scale porous media is concerned a few specific types of ai methods have been utilized such studies are focused on for example image enhancement kamrava et al 2019a estimation such properties as the permeability resistivity and diffusivity kamrava et al 2019b tembely and alsumaiti 2019 wu et al 2018 porous media reconstruction adams et al 2018 feng et al 2019a j 2019b liu et al 2019 mosser et al 2018a l 2017 shams et al 2019 tran and tran 2019 p and s wave velocities estimation karimpouli and tahmasebi 2019 segmentation karimpouli et al 2019 conditional simulation of three dimensional pore models mosser et al 2018b mapping between design variables and microstructures li et al 2018 z y yang et al 2018 classification of surface wettability yun 2017 microstructure synthesis fokina et al 2019 estimation of mechanical and thermal properties of porous materials avalos gauna and palafox novack 2019 pires de lima 2019 wei et al 2018 wu et al 2019 zhang et al 2019 and using the ml ability for estimation of other physical property bélisle et al 2015 kamrava et al 2019a developed a hybrid stochastic deep learning method hsdl by combining a stochastic algorithm for reconstruction of porous media and the dl method through a cnn for enhancing the resolution of their images kamrava et al 2019a they used a limited number of high resolution 2d images in a stochastic reconstruction method and generated numerous realizations capturing the multiscale morphology of the pore space then the generated images were preprocessed and used as a training dataset in the dl once the network was trained a single low resolution image was fed into the trained network for enhancing the image s resolutions using several statistical properties and morphological characteristics of the porous media kamrava et al 2019a made a quantitative comparison between the low resolution image and the high resolution ones generated by the dl some of the images generated by their stochastic reconstruction method the cross correlation based simulation or the ccsim tahmasebi et al 2012 are shown in fig 10 where a 2d image was used to generate multiple realizations that are different from each other and from the input image but with statistically similar morphologies this approach has been shown to be an efficient method for generating large training datasets for the dl method kamrava et al 2019a the overall schematic of their proposed method is shown in fig 11 as shown a limited number of high resolution 2d images are used in the stochastic reconstruction method in order to generate a large number of realizations to diversify the generated images further different filters were used with the generated realizations then the new realizations were used as a training dataset in the dl to enhance image quality based on a single image for testing the dl method an unused high resolution image the reference image was converted to a low resolution image and was fed to the network the enhanced image generated by the hsdl method was then compared with the original image and the enhanced images generated by various other image super resolution methods see fig 12 kamrava et al 2019a also made comparisons based on various statistical measures and porosity they reported that all the comparisons confirm that the enhanced image generated by the hsdl method has better resolution and very similar physical properties when compared to the original reference image kamrava et al 2019a aside from this study several other methods have been proposed to enhance the resolution of low quality images of porous media wang et al 2020 wang et al 2019 for example wang et al 2019 implemented a 3d cnn network to generate image super resolution and compared the results with bicubic interpolation as mentioned earlier another application of the dl to the fine scale porous media has been estimating important flow and transport properties of porous media such as permeability resistivity and diffusivity the idea is to use the dl s ability for unrevealing important and latent features in complex porous media and linking them to the properties and to reduce the computational time in other words although the training phase of the dl may require a large amount of time it can perform the estimation for new samples in a matter of seconds once the training phase is concluded wu et al 2018 used cnn to generate models of porous media and computed their permeability they noted that since the permeability is a sole function of the pore geometry it should be possible to estimate it based on the geometry and its images using the cnn and avoid direct simulations or pore network calculations their proposed algorithm was as follows 1 generating a large number of 2d images using the voronoi tessellation algorithm and calculating permeabilities of the generated images using the lattice boltzmann method which were then used as the training dataset 2 performing the training of the cnn and 3 testing the trained network with new images for predicting their permeability they also added to the cnn some physical properties that affect the permeability in order to enhance the accuracy of the predicted permeability but reported however that not all physic informed cnn perform better than the regular cnn they suggested that rather than training the network only with the target variable permeability if the porosity φ and specific surface s area are added to the network the results accuracy will increase some of the permeability predictions generated by the proposed method are presented in fig 13 the physic informed method has higher accuracy than the regular cnn while both predictions have about 10 error if we consider the lattice boltzmann calculated permeabilities as the true values they concluded that for some cases where the pores are dilated physics informed cnn does not provide more accurate predictions than the regular cnn as shown in fig 14 they also studied the effect of increasing the number of seeds n in the permeability prediction for material with dilated pores by physics informed cnn by seeds we mean the cells generated on a plane for partitioning it by the voronoi tessellation method fig 15 compares the permeabilities predicted by physics informed cnn the empirical kozeny carman equation and the numerical results obtained by the lattice boltzmann simulations the cnn predictions agree with lattice boltzmann simulations to within 10 percent they are also much more accurate than the predictions provided by the empirical kozeny carman equation wu et al 2019 used a cnn network to predict the effective diffusivity de of a porous material based on 2d images they generated a large dataset using reconstruction methods and calculated their effective diffusivity using the lattice boltzmann method considered as the true values the generated images and the corresponding effective diffusivities were used for training the network which was then used to predict the effective diffusivity for new images the results are shown in fig 16 when the true values of de is less than 0 2 and the porosity is between 0 28 and 0 98 then 95 of the predicted results by the proposed method are within 10 error from the lb methods and more accurate than the predictions by the effective medium approximation of bruggeman 1935 they also reported that when de 0 1 then the cnn predictions have high errors approximately more than 30 wu et al 2019 and since porosity is correlated with effective diffusivity it can be used to enhance the predictions by the cnn thus they developed physics informed cnn by adding the porosity of each porous material to the flattened feature map of the last pooling layer in order to form the first fully connected layer on the cnn wu et al 2019 also studied another type of cnn namely cnn with pre processed input where they removed the trapped and or dead end pores in the images of the porous structures in order to further enhancing the cnn predictions fig 17 compares the de predicted by the cnn and the lb method as well as the results by the cnn and bruggeman s equation de εβ 1 wu et al 2019 where β depends on the structure of porous media they reported that the cnn can better predict de for porous materials with complex structure than bruggeman equation they further expanded the cnn ability for predicting the effective diffusivities over various ranges and reported that the absolute error for the predicted de by cnn is smaller than 0 1 for all the ranges the relative error for the predicted de by the physic informed cnn is approximately 12 less than those of the regular cnn wu et al 2019 in another set of studies liu 2017 liu et al 2019 experimental tests using capillary tubes were carried out in order to measure the saturations phase conductance and two phase capillary pressure and relative permeabilities drainage and imbibition experiments were carried out for generating the relative permeability curves the data were then used as the training datasets in an ann to predict the relative permeability and capillary pressure the input parameters that were perceived as unnecessary were identified using sensitivity analysis two approaches were used in the study in the first a neural network was used to predict only the capillary pressure curve pcow using the input data that were related to the tube s cross section geometrical properties in the second approach they predicted the threshold capillary pressures and water oil relative permeabilities simultaneously using the neural net liu 2017 a comparison of the results for pcow by the trained neural network trained using both datasets versus the calculated results from experiments using the first approach is presented in figs 18 a and 18 b respectively the training datasets consisted of 3000 random polygons with shape factor from 0 to 0 04 and 3000 random polygons with shape factor from 0 04 to 0 07958 the two studies indicated that the results are better predicted when the second training dataset is used and that the predictions by the nn depend on the elongation factor of the system it should be noted that the input data and models were designed for pores with circular cross sections and for pores with more realistic geometry the same type of accuracy may not be obtained liu 2017 in a sperate study kamrava et al 2019b used a physics guided dl method to predict the permeability of porous media based on the morphological information gained from 3d images of the porous materials they noted that having a large amount of data for training a dl network is crucial the input data used in their study included 3d images of sandstone and a large number of realizations of porous media generated by a stochastic image based reconstruction technique as well as by a boolean method and their corresponding permeabilities in their method important features were extracted from the images using the dl algorithm which were then used for estimating the permeability the cnn as a nonlinear mapping between the given images and the given permeabilities was used which was connected to a standard shallow feed forward ann they evaluated the accuracy of their method by testing it using high resolution 3d images of real sandstone the schematic representation of their cnn is shown in fig 19 some of the extracted features for one of their samples are shown in fig 20 the cnn utilized in their study was based on a supervised learning algorithm this means that 3d segmented images of berea sandstone and their computed permeabilities were given as the input and output to the cnn in order to learn the mapping between them i e the training process they selected a set of diverse data based on their permeabilities from the lower median and upper quantiles of the permeability distribution the results are depicted in fig 21 to validate the trained network it was tested with a very distinct sample of sandstone fontainebleau with relatively low porosity 14 and very different morphology and permeability than the berea sandstone that had used in the development of the cnn a correlation coefficient of 0 9 was reported which is only slightly smaller than what had been achieved for the berea sandstone kamrava et al 2019b wei et al 2018 used the cnn for predicting effective thermal conductivities of porous materials wei et al 2018 they used a quartet structure generation set for generating model porous materials and calculated their effective thermal conductivity using the lb method they then compared various ml algorithms such as a cnn the svr and gaussian process regression gpr for predicting the effective thermal conductivity the results are shown in fig 22 all the ml algorithms had better accuracy for predicting the effective thermal conductivity than the maxwell eucken maxwell 1904 eukan 1932 and the bruggeman models a vast number of published papers in the field of fine scale porous media have been devoted to reconstruction which is often achieved using gan networks such studies are usually based on the methods in computer science and have not undergone considerable change therefore we only mention some of the key results here cang et al 2017 used a convolutional deep belief network to produce stochastic models of porous media although they used the method to model complex materials they can also be used for geomaterials as well several methods have also been used for reconstruction of the microstructure of porous materials for example gans was used for reconstruction of an image of a relatively homogenous sample of berea sandstone and a heterogeneous sample of estaillades carbonate liu et al 2019 subsamples of 3d images of porous rocks were used for training the network they also added nonlinear statistical information gained from 3d micro ct images of actual rock in the training the authors noted that gans generated reconstructed images have better quality for homogenous rocks than heterogeneous ones the generated images by the gans along with real samples are shown in fig 23 the gans were also for 2d and 3d image reconstruction using segmented volumetric images mosser et al 2018a l 2017 the accuracy of the model was verified using some of the statistical morphological and transport properties of the samples such as the euler characteristic two point statistics and directional single phase permeability some of the results are shown in fig 24 it was noted that the gans require graphics processing units gpu and large gpu memory due to the usage of 3d datasets in the training process in the same group of the application of the gan methods to porous media problems feng et al used conditional gan cgan which is claimed to be an improved version of the gans for generating realistic images feng et al 2019a j 2019b they noted that the gans input data are only a noise distribution such as gaussian or a uniform noise from specific samples whereas the cgans input data are gaussian noise and conditional data the added noise is meant to add diversity in the generated output feng et al mentioned that the cgans is a deep cnn in which every neuron in a layer is connected to all other neurons in another layer whereas neurons in a cnn are only connected locally between two adjacent layers with all the weights being similar in one layer the loss function is used to minimize the error in both the generator and discriminator sections feng et al 2019a 2019b the cgan is schematically shown in fig 25 the dl methods have also been used widely to address important problems in fluid mechanics raissi et al 2019a 2019b 2018 for example the dl is used to encode incompressible navier stokes equations integrated with the structure s dynamic motion equation in order to study vortex induced vibrations raissi et al 2019b the proposed method enables velocity and pressure quantification from flow snapshots in small subdomains raissi et al used the dl to predict fluid s lift and drag forces on the structure based on very limited information on the velocity field or snapshots of dye visualization they applied the navier stokes informed deep neural networks to predict equation for the structure s dynamic motion using their method for three scenarios first with given acting forces on the body they predicted the structure s motion second with the given velocity field and the structural motion at a limited location in space time they determined the lift drag forces the pressure field the entire velocity field and the structure s dynamic motion third with given concentration data in space time they obtained all flow fields components and structure s motion and the lift and drag forces some of their results for predicting the concentration of passive scalar velocity field and pressure lift and drag forces by their suggested algorithm are shown in figs 26 and 27 respectively the authors noted that their algorithm can accurately order of 10 3 reconstruct the velocity field the concentration of passive scalar and pressure without sufficient data on these quantities raissi et al 2019b 3 2 large scale systems 3 2 1 geoscience as the concepts of big data continue to impact society and science geosciences have also experienced a major transformation of the availability of data a number of geoscience data bank has been produced which provides tremendous potential for the application of the ml by geoscientists for accurate modeling of the state and evolution of the earth system indeed many ml methods are a natural fit for the problems encountered in geoscience applications one application is classification and pattern recognition methods that can be used to characterize objects and events in geosciences vital for understanding the earth s systems for example liu et al 2016 developed a deep cnn to tackle the problems linked with climate pattern recognition they classified tropical cyclones atmospheric rivers and weather fronts based on images provided by climate simulations and observations the model achieved fairly high classification accuracy ranging from 89 to 99 accuracy due to a lack of labeled data for a reasonable period of time in various regions racah et al 2016 implemented a 3d height width and time convolutional auto encoder cae to utilize temporal and unlabeled data to increase the accuracy of identifying extreme weather events lguensat et al 2017 developed a u net like a network that consists of a cae connected by a pixel wise classification layer to automatically detect and classify the eddies from sea surface height maps their global accuracy was around 90 on the other hand the ml can extract important geoscience variables that are hard to monitor and compute directly such as the concentration of pm2 5 in air using information about other variables collected through satellites or simulated with earth system models pm2 5 are very fine particles in the air that reduce visibility and cause the air to appear hazy when their levels are elevated klein et al 2018 demonstrated an accurate and scalable method of vegetation management along transmission lines by combining an ml algorithm with high and medium resolution multispectral satellite imagery the trained cnn models could detect the tree types estimate tree heights and determine the health of trees kosovic et al 2018 used the rf and ann to learn the nonlinear relationship between the fuel moisture content fmc and vegetation index derived from satellite observations the results for the test dataset indicated improvements in accuracy for both live and dead fmc estimation when compared with persistence and linear regressions predicting the variation tendency of the earth system for a long period of time is another significant task since it can help to appropriately manage the resources and adapt current policies with the given predicted information in the ml context this can be formulated as a problem in time series regression that forecasts the future trends of a geoscience variable with the provided history and current conditions y yang et al 2018 tried to improve the accuracy of predicting sea surface temperature using a two part model one 3d grid constraining local correlation and global coherence and the combination of one fc lstm layer and one convolutional layer they carried out experiments on two different datasets validated the results and concluded that the accuracy is better as compared with other methods ye et al 2019 developed an ensemble spatially explicit and continental scale projection model with 1 38 million grid cell sub models to predict forest cover dynamics each grid cell model was built as an lstm network and developed on a spatiotemporal dataset of different kinds of data the proposed model greatly outperformed a spatial econometric model with a 44 improvement in the root mean square error indicator in a similar way a significant amount of attention has been paid to predicting precipitation with different kinds of ml methods akbari asanjan et al 2018 kim et al 2017 shi et al 2017 2015 interpreting the interaction between different physical processes and estimating their internal relationships is also of great importance in problems related to earth science as one deals with complex and multivariate data identifying such relations from geoscience data can advance our understanding of the geological processes holtzman et al 2018 proposed an ml approach to identify the temporal patterns of earthquakes they transformed the waveform data to fingerprint reduced dimensionality and removed features common to all the signals with non negative matrix factorization nmf and hidden markov model hmm and finally clustered the fingerprints with the k means clustering described earlier they found that seismic events with similar spectral properties repeat on annual cycles within each cluster and that variations in acoustic properties and faulting processes accompany changes in thermomechanical state kratzert et al 2019 developed an entity aware ea lstm to investigate as to why the watersheds over the continental u s have the largest sensitivities to climate related forcing in extreme low and high flow periods the models predicted daily streamflow based on 32 input features from 531 basins the result showed that the proposed model is more accurate than the existing ones even for basins that were not used for training 3 2 2 hydrology three main approaches namely deterministic probabilistic and stochastic have been used in hydrological modeling to characterize the groundwater systems and other coupled models in hydrogeology such methods require identifying the right physics and implementing them in large models to reproduce the observations the process can be very time demanding if the number of variables and the size of the models increase significantly on the other hand data driven approaches such as those based on the ml allow a deeper understanding of complex problems without detailed knowledge on the internal structure of the observed system the application of the ml techniques in hydrology has been evolving as data and methods become more and more accessible for example in some cases related to hydrology carrying out large physics based simulations might not be required if the relationship between the input data and the target output can be extracted using data driven methods the following is a discussion of several examples of solving hydrological problems with the ml based on which similar results can be obtained faster as discussed earlier a prime motivation for utilizing the ml methods is taking advantage of the vast amount of data that is available and in this case those that are provided by scientific satellites the ml can extract useful hydrological information from such images tao et al 2016 developed a deep neural network to improve the accuracy of satellite precipitation data aim at avoiding bias and false alarms a stacked denoising autoencoder sdae was trained to process satellite cloud images in order to extract useful features over a window of 15 15 pixels they tested the model on two variables one for rain no rain detection and another one for precipitation quantification and obtained significant improvements in the predictions for both cases improving respectively by 33 and 43 on false alarm pixels and 98 and 78 bias reductions in precipitation rates over the validation periods of the summer and winter seasons zhan et al 2017 introduced a cnn to extract useful patterns for cloud and snow detection from the multispectral satellite images the results indicated that the proposed deep neural network model performs better than the state of the art methods both in quantitative and qualitative measures li et al 2017 included the geographical information in a deep learning architecture named geoi dbn to estimate the pm2 5 from satellite images it was demonstrated that geoi dbn produces more accurate predictions than traditional neural networks the out of sample cross validation r2 increased from 0 42 to 0 88 and rmse decreased from 29 96 to 13 03 µg m3 the ml algorithms can also be used for estimating the correlation between various hydrologic variables and modeling the data collected with sensor networks ghose et al 2018 used an lstm for rainfall runoff modeling based on meteorological observations three experiments were conducted and the results demonstrated that the proposed model was able to simulate the runoff with a high degree of accuracy when compared with a baseline hydrological model ding et al 2019 proposed a flood forecasting model by using the lstm and attention mechanisms to forecast flood the data were from the lech river basin in europe and the results indicated that the model worked well with high research value when compared with the svm fc network and the original lstm zhang et al 2018 developed an lstm to predict the variation trends of water table depth for a long period of time in agricultural areas the proposed model consisted of an lstm layer with another fc layer on top of it and a dropout technique that was added in the first lstm layer comparisons between the r2 scores of the proposed model and that of the double lstm model indicated that the proposed model s architecture is appropriate and exhibits a strong learning capability on time series data in another related field namely simulation of geological media simulation and inversion the ml algorithms can learn and generate complex data distributions which are used to produce realizations of subsurface structure that imitate the observed spatial patterns extracted from the field samples in porous media since the computational cost for probabilistic inversion with multiple point statistics techniques is often unaffordable and prohibitive for problems involving high dimensionality tahmasebi 2018 tahmasebi et al 2018 tahmasebi and sahimi 2015b laloy et al 2017 developed a vae to build a parametric base model with low dimensionality that parameterized a complex binary geological medium they compared their model with the widely used parametric representations for probabilistic inversion problem and showed that the proposed dimensionality reduction approach outperforms the pca optimization pca opca vo and durlofsky 2015 and discrete cosine transform dct techniques jafarpour and mclaughlin 2009 for unconditional geostatistical simulation of a channelized prior model the authors also trained a dnn of the gan type for the same purpose laloy et al 2018 the network was trained based on the training images and the final model could easily produce 2d and 3d unconditional realizations the most creative idea used in the work was that a very low dimensional parameterization was defined as an efficient probabilistic inversion using a state of the art markov chain monte carlo method 3 2 2 1 flood prediction flood is one of the most common natural disasters which could inflict enormous damage to life property and nature physics based modeling often requires a thorough understanding of and knowledge about the settings of hydrological parameters and the prediction of the flood is no exception statistical modeling usually requires a significant amount of data from measurement tools in order to make meaningful predictions therefore the ml approaches are a promising tool for flood prediction since they can automatically decorrelate the flood nonlinearity only according to the provided history information without explicitly describing the underlying physical mechanism furquim et al 2016 used a special theorem to preprocess the data collected from a river by means of wireless sensor tools and forecasted flood with the mlp and rnn the results showed that the mlp performs better than the rnn and that measurement of rainfall can improve the performance of both mlp and rnn since precipitation predictions are also part of the input variable for hydrological models the uncertainties of the precipitation predictions also affect the accuracy of flood predictions in order to combine both uncertainties ensemble prediction systems are usually employed doycheva et al 2017 proposed an approach for increasing the flood prediction accuracy by weighting ensemble members based on their skills the skill of each ensemble member was computed and tested by three types of ml algorithms namely the mlp svm and rotation forest they found that the uncertainty range in flood risk predictions is reduced with the proposed method and the ml approaches make the evaluation process more objective and automatic on the other hand liu et al 2017 developed a deep learning method by incorporating the sae and bpnn for the prediction of the flood which combines the strong feature representation capability of the sae and superior predicting capacity of the bpnn the sae is composed of several sparse autoencoders where the output of each layer is connected to the following layer as the input data the experimental data showed that the sae bpnn integrated algorithm outperforms the svm and rbf neural networks hu et al 2019 combined an lstm and reduced order model rom for flood prediction which is capable of forecasting the spatio temporal distribution of floods the proper orthogonal decomposition pod and singular value decomposition svd approaches were used to reduce the dimensional size of the input data in the lstm the lstm training and prediction processes are conducted over the reduced space this leads to an improvement in computational efficiency while maintaining accuracy the results showed that the lstm rom achieves comparable performance as the full model while its computational cost is reduced by a factor of three when compared with the full model simulations 3 2 3 energy resources with the recent improvements in data recording and collecting tools in exploration drilling and production operations various kinds of data have become available in the field of subsurface energy resources analyzing and interpreting seismic and micro seismic data improving current reservoir characterization and simulation techniques reducing the cost and time for drilling operations and exploring the factors related to drilling safety optimization of the performance of production operations are potential applications of the ml in the energy resources in reservoir engineering the massive number of grid blocks and the slow and iterative nature of solving the governing flow equations is the primary drawback of numerical reservoir simulators the ml models such as an ann putcha and ertekin 2018 sudakov et al 2019b coupled with other algorithms or independently can reproduce the time dependence of fluids and pressure distribution within the computational cells with high accuracy and higher computational efficiency numerous studies have been conducted to predict permeability erofeev et al 2019 sudakov et al 2019a tahmasebi and hezarkhani 2012a wood 2019 zhang et al 2018 and porosity ahmadi and chen 2018 alqahtani et al 2019 2018 from different kinds of source data such as well logs seismic data and digital images of rock the ml methods outperform traditional manual approaches in terms of error and prediction time in drilling engineering the ml can be used to predict and optimize the rate of penetration rop for increasing the efficiency of drilling operations soares and gray 2019 used three ml models the rf svm and ann described earlier to predict the rop based on four input features depth weight on bit drill string rotation speed and drilling fluid flow rate they found that the models reduce error significantly with constantly increased real time data and predict more accurately than analytical models with the same surface data the ml was also used to detect abnormal events during the drilling process gurina et al 2019 applied a gradient boosting classifier gbc to detect anomaly in drilling operations their proposed method compares real time measurements while drilling mwd data and similar information from the previously collected database and the implemented the network learned from the database according to their similarity to the real time predictions the results indicated that the model is robust since it performs well as some appropriate noise and shifts are added to the original date in a similar field where lost circulation during well drilling and completion increases the operation time and even devastates the well under worse conditions several pieces of research have been conducted to address this problem abbas et al 2019 geng et al 2019 shi et al 2019 in production engineering characterizing the flow regime along the pipeline is of great significance for determining the pressure gradient and then the global pressure drop eyo et al 2019 trained a real time flow regime monitor by learning the implicit relationship between the probability density function of collected voltage signals and observed flow regimes through kernel pca and multiclass svm the proposed model achieved better than 90 accuracy against visual observations by an expert on the test dataset the ml approaches can also be used in the more complex problems of well placement because of the heterogeneity and complexity of reservoirs the placement of wells has a large effect on the objective function response gradient boosting method nasir et al 2019 nwachukwu et al 2018 is usually combined with other optimization algorithms to forecast the reservoir response as the injector well locations are changed the results indicated that the hybrid models outperform traditional global optimization algorithms monitoring and optimizing the performance of production pumps is also beneficial for the development of the oil field several ml models such as the rf sneed 2017 svm oliveira santos et al 2018 and the pca sherif et al 2019 were used for effective electric submersible pump management the esp lifespan prediction and automatic fault diagnosis another energy source is landfills which are in essence large scale highly heterogeneous porous media in which the buried wastes undergo biodegradation through nonlinear reactions and generate a considerable amount of methane and other gasses convection and diffusion of the gasses toward the production and observation wells happen continuously giving rise to a highly dynamic system due to the nature of the wastes buried in the landfill their realistic modeling has been very difficult and has relied on empirical or semi empirical models in which the landfills are assumed to be homogeneous at the same time the problem of where to install new observation and production wells in the landfill is both very difficult and essential to operating the landfill safely sahimi and co workers sanchez et al 2007 li et al 2011 2012 2014 took the first steps toward the rational development of models of large scale landfills since one typically has considerable data for the rate of production of the gasses that are measured through the observation and production wells one can use them to not only reconstruct models of landfills but also use the data to train an ann and use it for predicting the future behavior of the landfill and in particular the temperature distribution and the rate of production of methane both are necessary for identifying hot zones in a landfill i e where the temperature has increased considerably and the amount of accumulated methane is also large so that an explosion may occur sahimi and co workers used an optimization method the genetic algorithm and ann to not only develop an optimal model of a landfill but also predict successfully the spatial distributions of the temperature and methane throughout the system 3 2 4 water resources fast advances in the development of the ml algorithms have contributed to many aspects of water resources and subsurface systems since various related applications have to automatically extract useful information from a great amount of data in real time moreover such research is typically multidisciplinary and thus dealing with the growing data volume with traditional workflow is not feasible although the current theoretical knowledge is not enough for us to interpret and explain every phenomenon that occurs in water resources and subsurface systems the ml approaches can provide us new insight to fill the knowledge gap the following describes some applications of the ml approaches to solving some typical problems in water resources which aims to provide a simple overview for water scientists and hydrologists about how the ml algorithm can be related and applied to their research field 3 2 4 1 modeling of groundwater level knowledge about the varying and complex trends of groundwater level gwl in an aquifer is necessary and helpful for water scientists and policymakers in order to better manage groundwater resources and maintain a balance between supply and demand it has been demonstrated that the ml is an efficient and effective approach for simulating and predicting the variational trend of the gwl in various aquifers wunsch et al 2018 applied nonlinear autoregressive with exogenous inputs neural network narx to forecast gwl in several wells precipitation and temperature were selected as the input features all the input time series were decorrelated based on the seasonal trends in order to identify time lags and specify the input and feedback delays needed for the narx the result indicated that the narx based gwl predictions are suitable for unaffected observation wells even with a few number of input variables ghose et al 2018 used the recurrent neural network to predict the gwl with a set of input variables such as rainfall temperature humidity runoff and evapotranspiration the results indicated that evapotranspiration loss and runoff are the most important parameters so that including them improved the model s efficiency guzman et al 2019 compared the narx and svr models for the gwl predictions for irrigation wells in the southeastern usa the best combination from three input variables i e daily gwl precipitation and evapotranspiration were evaluated the gwl plus precipitation scenario was identified as the optimal combination for model inputs with the svr outperforming the narx recently a few hybrid techniques have been used in which some specific data preprocessing were also deployed in order to increase the capabilities of machine learning the wavelet analysis technique is an example of data preprocessing which is very useful in gwl modeling mehrabi and sahimi 1997 sahimi 2000 wavelet is a multiresolution spectral analysis for both temporal and spatial data for example wavelets decompose time series in the time frequency domain in order to generate a time scale description of the processes and their relationships daubechies 1990 in many studies the discrete wavelet transforms dwt was used the decomposed sub time series were used as the inputs rather than the main time series several studies ebrahimi and rajaee 2017 rakhshandehroo et al 2018 yu et al 2018 zare and koch 2018 have shown that the hybrid models have better accuracy than regular models and a greater understanding and ability to simulate the gwl can be achieved on the other hand such ml models can be combined with conceptual numerical models such as modflow mcdonald and harbaugh 2003 to develop hybrid models such that each method can overcome the shortcomings of the other method for example the gwl data estimated by the modflow can be used to train the ml models if there is not enough real data note that the selection of input variables in the aforementioned studies depends on data availability therefore sometimes simple user defined relationships are used sahoo et al 2018 used three ml techniques the svr the rf and gradient boosting mechanism gbm to predict the gwl based on remotely sensed gridded rainfall and soil moisture data regression analysis between monthly lags up to 3 months lag of the three input variables and output variable at zero lag is however a tedious task due to a huge number of combination of variables as such a prior relative importance analysis of the input variables was undertaken in order to identify the relatively more important input variables see fig 28 thus analyzing the corresponding significance of each input variable can help one to identify the one that has the largest effect on the final prediction the previous studies showed that svr models yield more accurate predictions than other approaches 3 2 4 2 groundwater contaminant transport many studies have been undertaken to predict the transport of contaminants in groundwater as the water quality and pollution problem have attracted major concerns the prediction can help with a more reasonable design of groundwater protection programs appropriate treatment of contaminated aquifers and also decision making reliable predictions can however be made only with sufficient knowledge of geological and hydrological parameters of the subsurface system currently the ml approaches are considered to be an effective tool for discovering information and interpretation of raw data stanev et al 2018 and vesselinov et al 2019 developed similar hybrid methods to locate contaminant sources which decomposes the obtained contaminant mixtures according to non negative matrix tensor factorization nmf ntf method coupled with the k means clustering algorithm described in section 2 it has been found that the approach can identify the unknown number locations and properties of a set of contaminant sources from measured contaminant source mixtures with unknown mixing ratios without any additional information cipullo et al 2019 used an ann and the rf to forecast the changes with the time of the bioavailability of complex chemicals based on the data collected in a six month experiment they found that the predictions made by the ann are more continuous whereas the rf can describe the importance of each input variable in forecasting the toxicity mo et al 2019a combined a deep convolutional autoencoder network with an iterative locally updated ensemble smoother algorithm to detect groundwater contaminant sources their method is schematically shown in fig 29 the proposed network was developed to replace the forward model and estimate the functional mapping between high dimensional input and output fields since the cnns are designed to deal with images mo et al reformulated it as an image to image regression problem in the convolutional autoencoder network the functional mapping between input and output images is learned by a down sampling up sampling process that is the convolutional layers in the encoder are used to capture the abstract patterns in the input image and concatenate them as a high level feature map while the decoders are used to interpret these maps and generate the output a densely connected convolutional network structure named a dense block huang et al 2017 was introduced in order to add the connections between nonadjacent layers and improve the transferability of information along with the entire network the encoding and decoding layers are named as the transition layers dumoulin and visin 2016 which are arranged between two neighboring dense blocks to adapt the feature size through transposed convolution operation during the down sampling and up sampling process and to prevent the problem of feature maps explosion in another piece of work by mo et al 2019b the authors developed a convolutional adversarial autoencoder caae to represent the non gaussian conductivity fields with a low dimensional latent vector and incorporated the heterogeneity of formation see fig 30 furthermore a deep residual dense convolutional network drdcn was developed to substitute the forward model and learn the complex functional mappings between the inputs and the outputs this network is shown in fig 31 the drdcn was used as a surrogate model to estimate the relationship between the input conductivity field and the output hydraulic head and concentration fields moreover both input and output data were formed as images the dense block adds the connections between nonadjacent layers aiming to fully exploit the hierarchical features from the outputs of the preceding layers a multilevel residual learning structure was also used since it has been demonstrated that residual in residual dense block structure can effectively increase the network s capacity for estimating highly complex mappings wang et al 2018 the caae network was applied for parameterization of the conductivity field with a low dimensional latent representation the log conductivity field was approximated based on the following steps 1 beginning with an initial latent representation sampled from a normal distribution n 0 1 2 the corresponding log conductivity fields are produced using the caae s decoder 3 the surrogate model is evaluated to obtain the predicted initial output ensemble and 4 the latent representation and output ensembles are repeatedly updated for n iterations based on the current latent representation and output ensembles in the last step the current latent representation is fed into the caae s decoder to produce the log conductivity field which is then used as the input for the surrogate model to perform the prediction the posterior log conductivity fields are generated by the decoder using the last latent representation as input the results for estimating the 2d and 3d non gaussian conductivity fields indicate that the approach can produce an inversion comparable to those obtained by the original inverse method without surrogate modeling while the integrated method is more efficient since the training of the surrogate model requires only a small number of forward modeling others he et al 2019 tartakovsky et al 2018 tipireddy and tartakovsky 2018 a physics informed deep neural network for predicting the parameters of partial differential equations from a limited number of measurements was developed this approach enforces the physical laws described by the governing equations in the process of dnn training they showed that the predictions of the observed and unobserved variables can be used to effectively manage power grids and detect abnormal behaviors and faults 3 2 4 3 water resources management the characteristics and movement of a water body can be partially described by empirical correlations and physical governing equations which formulate the basis for physics based modeling applied in water resources management the ml algorithms provide an alternative method for estimating the relationship between the dependent and independent variables with negligible prior information being involved which can be an efficient and effective tool for solving the classification and prediction problems in the field of water resources management the ml algorithms can also be used for groundwater potential mapping the groundwater is still one of the primary sources for freshwater therefore accurate detection of the zones containing groundwater is an important problem for the establishment of related policies and appropriate management of groundwater resources various types of the ml models have been developed for groundwater potential modeling including the ann s a x lee et al 2018 the rf golkarian et al 2018 the brt kordestani et al 2019 and the svm naghibi et al 2017 all described earlier most recently hybrid modeling and ensemble learning have been applied to optimize the structure of the networks and improve its performance chen et al 2019a 2019b kordestani et al 2019 miraki et al 2019 it has been found that the sample size plays a very important role in the application of groundwater water potential mapping specifically moghaddam et al 2020 compared the performance of individual models and various combinations of them as the sample size was varied the results showed that the rf algorithm is more robust when the sample size is reduced on the other hand the ml algorithms have also been developed for runoff and suspended sediment simulation accurate prediction of runoff and sediment can help one to better manage and utilize the land and water resources many research areas such as water supply water quality and flood prediction are also related to this problem kisi and zounemat kermani 2016 proposed an adaptive neuro fuzzy embedded fuzzy c means clustering anfis fcm algorithm to predict the suspended sediment concentration the performance of the proposed model was compared with that of the standard anfis tahmasebi and hezarkhani 2012b the ann and the sediment rating curve they found that the proposed model has a higher prediction accuracy and requires less computational time than other methods buyukyildiz and kumcu 2017 used the ann svr and anfis to estimate the suspended sediment load in a local gaging station and also tested various combinations of the input data shiri et al 2016 developed an extreme learning machine elm to forecast daily water level change the results showed that the elm technique outperforms genetic programming gp and ann in terms of the speed of learning the relationship between different variables j h wang et al 2019 developed a dilated causal convolutional neural network dccnn that can predict the water level with 1 to 6 h earlier it was demonstrated that the proposed models both the svm and mlp provide acceptable results especially for predicting peak water levels 3 2 5 soil science soil moisture is a major variable in the field of environmental engineering and agriculture science continuously predicting the dynamic variations of soil moisture is beneficial to making early planning for agriculture and providing valuable information for decision makers in the related field this field has also witnessed the application of ml methods for addressing some important problems j song et al 2016 developed a hybrid model by incorporating the macroscopic cellular automata mca model into the deep belief network to forecast the soil moisture in a regularly irrigated paddy field the mca was also combined with the multilayer perceptron mlp to form a hybrid model called mlp mca which was set as the reference model the comparison indicated that the proposed hybrid model reduces the root mean square error by 18 and has a better performance zhang et al 2017 used a deep feedforward neural network to predict the distribution of soil moisture in a nation the raw data recorded for visible infrared imaging radiometer suite viirs continuously connected for four years were used as the input dataset they found that the proposed model can infer the nonlinear relationship between the remote sensing data and the local soil moisture distribution quantitatively the coefficient of determination was 0 9875 while the root mean square error was about 0 0084 which is superior when compared with the results obtained with the soil moisture active passive smap active radar soil moisture products aboutalebi et al 2019 used genetic programming gp to estimate soil moisture at different scales based on the multispectral image data with high resolution collected by an automated aircraft three variables the reflectance for the visible near infrared nir and thermal bands were used as the input variables to train a network they found that the predictions produced by the gp were comparable to those generated by the ann and svm whereas the master equation learned by the gp was applicable to other cases in different locations lee et al 2019 used a dl method to estimate soil moisture based on the remote sensing data the soil moisture estimated by the dl was more accurate with a high correlation coefficient of 0 89 a low root mean square error 3 825 and bias 0 039 fang et al 2019 used the lstm to forecast the variation tendency of the smap surface soil moisture product and obtained predictions with high accuracy they used one year smap level 3 passive data for the training and predicted the trend of the following year by the lstm they found that the lstm could expand the applicable range of the smap and can be as large as the entire continental u s and achieved high accuracy and less uncertainty at the same time although the number of training parameters for the lstm is usually very large the results demonstrated their superiority in time sequence prediction when compared with the traditional statistical methods the authors also mentioned that the lstm can only extract the soil moisture response patterns which are directly detected by the smap moreover the stochastic signal observed in the satellite images cannot be extended but is very useful for reducing errors fig 32 compares the time series for surface soil moisture at three core sites it is clear that the lstm accurately reproduces the patterns observed by the smap and that some systematic variance also exists as compared with the site data 3 2 6 carbon capture and storage carbon capture and storage ccs is a technique developed to reduce the effect of global climate change on human s life by gathering the carbon dioxide co2 in the air and storing them in the subsurface our daily life and companies in the industry still use fossil fuel as the major energy supply and discharge a large amount of co2 into the atmosphere which could be collected through a chemical reaction compressed and stored in an appropriate underground space xu et al 2019 some practical and significant problems still hinder the commercial development of large scale ccs for example co2 could leak out from the target storage formation through naturally existing fractures since the subsurface system is highly heterogeneous and numerical modeling requires a good understanding of the physics of the fluid flow accurately detecting that abnormal phenomena become very difficult recently efforts have been made to utilize the ml to address this issue lin et al 2017 applied the svr to forecasting the location and rate of leaked co2 based on limited pressure data points in particular the governing equations were incorporated into the algorithm to guide the training process similarly chen et al 2017 used a filter based approach to encompass measured data to monitor the cumulative co2 leakage rate and optimize the well locations specifically two ml algorithms the svr and multivariate adaptive regression splines were used to reduce the computational cost and derive models from the full physics based numerical models hubert and padovese 2019 proposed the use of acoustic signals for monitoring the presence of co2 leakage random forest and gradient boosted trees were utilized to analyze these signals furthermore a hidden markov model was used to smooth the predicted results by incorporating the known probability distribution of the leakage occurrences estimated from previous observations zhong et al 2019b developed a convolutional lstm convlstm to interpret the collected field pressure data and identify abnormal values the convolutional layers were used to incorporate both static and dynamic data specifically such data are represented as the three channels of a colored image see fig 33 a the injection rate for injection well f1 and bottom hole pressure for observation well f2 and f3 are dynamic data and formed as the first channel while two other channels contain the static data namely the effective reservoir porosity and permeability convlstm can overcome the main drawback of the lstm which is its inability to incorporating spatial information the proposed network received the concatenated image like data as the input and used the convlstm cells to extract useful spatial features from them see fig 33 b a dense layer was set as the output layer that provided the final predictions by linearizing the outcome from the previous layers based on linear regression the results indicated that the convlstm achieves comparable performance with vanilla lstm but it can also better interpret the change of magnitude and heterogeneity of the permeability field sparse data measurements and noisy data can however limit the model s performance and provide poor predictions in another paper zhong et al 2019a trained a surrogate model to forecast the movement of co2 plume with the reservoir heterogeneity considered see fig 34 they developed a conditional deep convolutional generative adversarial network cdc gan to estimate the relationship between two high dimensional fields namely the permeability and saturation of the co2 plume as conditioned to the model output time instead of using only a latent space vector z the conditional gan mirza and osindero 2014 is also conditioned to other known information so as to provide guidance for the training of the generator which improves the convergence of the gan significantly zhong et al 2019a conditioned the cdc gan to the time steps and used it as supplementary information to learn the corresponding relation between the inputs and outputs at each moment on the other hand a deep convolutional gan dc gan radford et al 2015 attempted to produce images based on latent vectors by training an autoencoder the encoder decoder structure combined with other deep learning techniques e g batch normalization layers enabled the network to appropriately down and up sample the images as well as making the training process more stably and smoothly compared with the predictions generated by a compositional reservoir simulator the proposed model accurately predicted the spatial distribution of co2 plume at each time step with high computational efficiency moreover the proposed method used the pattens extracted by the convolutional operation to make the prediction and therefore a limited understanding of the governing physics does not restrict its application wen et al 2019 developed a cdc gan to accurately predict the migration and saturation distributions of co2 plume in heterogeneous reservoirs the proposed model can not only interpolate between the training data ranges but also extrapolate to where no data are not available when extrapolating a transfer learning procedure was applied for extracting useful information from the new dataset instead of retraining the entire network from the beginning 4 conclusions and outlook the recent tremendous progress in the field of ai and ml is undeniable the advancement is indebted to the significant growth of computational power and the availability of advanced statistical methods which enable the ml methods to deal with more realistic problems the ml methods can be applied to large and complex datasets such as images signals multivariate data and so on due to the judicious application of ml our understanding of complex phenomena involving big data has shifted and improved significantly for example there are numerous examples in geosciences and porous media wherein the available data are very complex so much as that their interpretations require a lot of time without any guarantee that the hidden information can be identified we used to implement simple and human guided rules to interpret the earthquake signals and would make biased decisions with the aid of the ml and dl however valuable information can be decoded from such complex signals the field of porous media and more generally geoscience are now in the era of big data that may be way before other fields due to the necessity of collecting several datasets from various sources thus several spatial and spatio temporal methods have already been developed in these fields that are being used in other major research fields the recent ai techniques however have brought a commonality to all the fields through which they all can be connected and helped to make further progress the topic of this review paper is thus no exception we attempted to review most of the recently developed methods we believe that the following routes can be pursued to further develop the necessary ml methods related to porous media for both small and large scale ones recent dl methods should be applied for understanding complex data and taking advantage of other relevant data more physics informed ai methods should be developed because using the current methods from other fields for application to porous media and more generally geoscience may not consider the complex physics properly for example the governing equation for fluid flow and the physics of porous media cannot all be accounted for use with the advanced ml methods but they should be part of the computations thus the ml methods can be customized such that they not only discover the latent patterns in data but also reproduce the other aspects of the physics of the problem as well such a new ml algorithm will represent a combination of data driven and physics based modeling each of which completes the challenges in the other part for instance the ml methods can be developed to reduce the computational cost while the physics based method can add more reality to the ml workflows novel solutions for dealing with the issue of providing the necessary input data for the ml should be developed as some areas in porous media and geosciences do not provide rich enough data a scientific and physics based interpretation of the results obtained from the ml should be founded based on the experimental and computational methods the application of the ml can be enhanced by customizing the currently available ml methods for problems in porous media knowledge from big data and the ml should be combined in order to help with better integration of the two crucial fields benchmarking is an important missing ingredient in the current literature on the use of the ml algorithms in geosciences these fields require more meaningful comparisons between new ml methods in order to make systematic progress which allows the user to select the best method more confidently and with more ease boulesteix et al 2018 such data will be different from the existing ones as the inputs should all be translated in what ml often accepts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103619 appendix supplementary materials image application 1 
456,in recent years significant breakthroughs in exploring big data recognition of complex patterns and predicting intricate variables have been made one efficient way of analyzing big data recognizing complex patterns and extracting trends is through machine learning ml algorithms the field of porous media and more generally geoscience have also witnessed much progress and recent progress in developing various ml techniques have benefitted various problems in porous media and geoscience across disparate scales thus it is becoming increasingly clear that it is imperative to adopt advanced ml methods for the problems in porous media and geoscience because they enable researchers to solve many difficult problems at the same time one can use the already existing extensive knowledge of porous media to endow ml algorithms and develop novel physics guided methods the goal of this review paper is to provide the first comprehensive review of the recently developed methods in the ml algorithms and describe their application to porous media and geoscience thus we review the basic concept of the ml and describe more advanced methods known as deep learning algorithms then the application of such methods to various problems in porous media and geoscience such as hydrological modeling fluid flow in porous media and sub surface characterization are reviewed we also provide a discussion of future directions in this rapidly developing field keywords machine learning deep learning data analytics subsurface systems geomedia physics guided artificial intelligence 1 introduction the world is facing momentous challenges regarding climate change natural hazards water resources and energy consumption facing such challenges and addressing them requires development of solutions to some very difficult problem problems among such problems are those related to geosciences and in particular characterization and modeling of porous media and understanding fluid flow transport reaction adsorption and deformation there both at small and large scales although tremendous progress has been made as the so called big data which are needed for the task have been available historically and are being produced more than ever faghmous and kumar 2014 gupta and nearing 2014 mohaghegh 2018 2017 monteleoni et al 2013 sahimi 1995 tahmasebi et al 2018 vandal et al 2017 much remains to be done technological advances in data gathering computational power and demand for making high resolution and accurate models and cloud systems have made it possible to leverage big data and complex computations some of such information e g remote sensing images are for example collected daily which results in producing a large volume of data unlike other fields however most of the data in porous media and geosciences are often available publicly and thus by accessing them one can develop more efficient techniques traditionally computational approaches and experiments have been used to characterize and model small scale porous media ranging from membranes to adsorbents catalysts and core scale porous materials such methods can however be quite expensive and time demanding and their use at larger scales pose significant difficulties or when the number of samples to be studied is very large for example extensive experiments are conducted in very long workflows in order to characterize and describe the heterogeneity of porous materials and compute their important properties such as permeability electrical conductivity and sorption capacity furthermore carrying out such experiments requires extensive and often expensive equipment and instrumentation and expertise as well as an environment that yields reliable results on the other hand computational methods have also made substantial advances and they now can produce with acceptable error data that are comparable with the experimental results moreover compared to the experiments computational methods provide a more controlled environment and produce accurate results over a smaller time scale and often more economically but these methods can still be expensive and more importantly the acquired experience from the previous calculations cannot easily be used for future computations unless the problems in the past and future are closely linked therefore ideally a combination of experimental and computational modeling may be the most efficient way of characterization of porous materials at a small scale both experimental studies particularly for large scale problems and computational methods require a deep understanding of the physics of the system and the effect of various variables on the process of interest if one is to make accurate predictions such methods may not be able to address accurate predictions for multiphysics problems if the necessary workflows have not been developed for example it is currently very difficult to study by experiments the effect of morphology fluid temperature and mineralogy on the deformation of porous media and materials due to the necessity of calibrating various parameters in a similar manner coupling the physics flow reaction and deformation in a consistent and seamless computational framework remains very challenging one important approach to analyzing big data and using them in the modeling of porous media related problems is based on the use of artificial intelligence ai large datasets are almost always necessary for proper evaluation comparing ml algorithms and assessing their performance the need for big datasets in the ml contexts stems from the fact that the analytical examination and comparison of the most flexible ml algorithms can range from very difficult to nearly impossible thus big data is not only necessary for developing a successful application of ml but is also critical in verifying which ml performs better unlike the standard statistical methods that often do not perform well in dealing with big data the ml methods are more effective when they use big data significant progress has recently been made in the use of ai machine learning ml and more recently the so called deep learning dl for modeling of small scale porous media briefly dl is a subset of ml and ml itself is a subclass of ai ai is defined for most of those algorithms that try to solve a problem by integrating an algorithm with intelligence thus they can either perform work better than humans or they can intelligently solve a problem while ml refers to those methods that can use the computer power to learn a specific job and make decisions the dl algorithms on the other hand are developed to mimic the pattern recognition of the human brain moreover due to the aforementioned issues the ml techniques have emerged as promising alternatives for dealing with multi physics processes in particular discovering latent patterns extracting important features and identifying the connections between various variables when one deals with a large amount of data is not straightforward if one relies on the more traditional approaches whereas the use of the ml approaches exhibits much potential for helping one to make more informed and accurate decisions porous media problems from those at the smallest scale the nanoscale to the largest at field scales are often endowed with big data that are spatially distributed and can dynamically vary as well which add another level of complexity mohaghegh 2018 sahimi 1995 tahmasebi et al 2018 the growing availability of big data offers fertile grounds for the use of various ml and data mining techniques kim 2016 mohaghegh 2018 several key attributes distinguish the data in porous media and subsurface systems from the common data in other fields for example unlike the regular image recognition field in computer science the data in porous media are based on physical laws as such the ml techniques cannot sometimes be applied to such data directly implying that such techniques should be reformulated and tailored to the existing data unlocking the big data in geosciences and taking adaptable decisions require developing more advanced techniques for identification of complex dependencies and patterns which are otherwise either very difficult or nearly impossible all such issues along with the explosion of data sensing tools as well as the continuing development of advanced computational algorithms have provided the impetus for developing ml and automated methods that can address the aforementioned issues thus as we hope this review will demonstrate the ml techniques can be used as a very efficient alternative for identifying the patterns in large and multidimensional data such that linear and nonlinear correlations among various physical parameters can be discovered and extracted the results obtained so far by the application of the ml in engineering and science have indicated the discovery of new physics and patterns that could not be discerned before or required such a significant amount of time and resources as to render them practically impossible to use such abilities of the ml techniques have aided the researchers to uncover the potential of big data and enhance our understanding of complex phenomena related to porous media from small to the largest scale in geomedia based on such considerations the purpose of this paper is to provide a comprehensive review of the state of the art methods in the ml and advanced data analytics methods although a few reviews have already been published abrahart et al 2012 karpatne et al 2019 maier et al 2010 shen 2018 tyralis et al 2019 they are however specific to narrow issues whereas the present paper aims to describe and discuss the key studies in the field of porous media at various scales in table 1 we provide a brief overview of the main applications of the ml each of which will be discussed in this paper such methods include both supervised and unsupervised classes of the ml algorithms in fact the supervised methods are built based on true outputs in such methods one knows exactly what the response should be whereas the true outputs are not available in unsupervised algorithms in other words the supervised algorithms link two known datasets input and output by best estimating the parameters in the ml which allows one to quickly optimize the training this issue can be problematic in unsupervised methods as the network should discover the responses at the same time this freedom can allow the ml methods to identify the latent features more broadly as the output is not restricted to a specific response see fig 1 the rest of this paper is organized as follows first the basic concepts of artificial intelligence are reviewed then some of the popular data analytics and statistical methods are summarized the applications of ml and data analytics methods for porous media are discussed in the next section finally our conclusions and vision for future developments are summarized 2 artificial intelligence machine learning and data analytics artificial neural networks anns are computing systems that imitate the working mechanism of human brains they can complete their tasks without being explicitly programmed to follow some specific rules chen et al 2019 generally speaking the anns consist of parallelly operated elements named neurons that resemble the basic unit of the nervous system mathematically speaking a neuron in an ann serves as a nonlinear parameterized and bounded function which receives input data from the outside world and produces or outputs its predictions such as the value of a target variable in this context two different methods usually are applied to parameterize the function the first method parameterizes the input variable and forms a global input as a linear combination of the input variable xi weighted by a parameter wi named the weights that signifies the relative significance of the input the output y of the neuron is obtained as a nonlinear function of the global input such that 1 y f b i 1 n w i x i where b is the bias which is initially a random number eq 1 is shown graphically in fig 2 a for a simple model and in fig 2 b for a more complex and multilayer network a nonlinear transform function can also be parameterized given fas a gaussian radial basis function the output is calculated by 2 y exp i 1 n x i w i 2 2 b 0 2 where the wi are the coordinates of the multivariate normal distribution and a bias parameter b 0 is assumed along with all directions a neural network is represented mathematically as a nonlinear function of two or more neurons that can be connected through various ways and the structure of the network describes exactly the procedure for generating the output by combing and weighting the inputs the most common and widely used ann is the multilayer feedforward neural network the feedforward architecture allows data to be transferred only from the inputs to outputs with no loop appearing in the network as shown in fig 2 b a multilayer feedforward neural network contains three types of layers namely the input the hidden and the output layers the number of neurons in the input and output layer depends on the number of independent and dependent variables respectively for the hidden layer the number of neurons is a hyperparameter and can be optimized in order to achieve better performance this type of network is a so called supervised learning algorithm sla and is especially suitable for classification and regression the task of the sla is learning the relation that maps the input data that produces the output such input output pairs are usually referred to as the training data which consist of a set of training examples that are pairs of an input object and the desired output value based on which a function is inferred after analyzing the training data the sla produces a function that is used for mapping new examples generally a loss function l y y is first defined to measure the difference between the predicted value y and the true value y then an optimization algorithm is used to minimize the predicted error the difference between the two by iteratively updating the weight and bias parameters the most commonly used and effective algorithm to train an ann is backpropagation introduced by rumelhart et al 1985 based on the chain rule of differentiation the backpropagation algorithm computes the derivative of the loss function l with respect to all the variables and parameters in the network then the derivatives are utilized to update the weight and bias parameters according to a gradient descent algorithm ruder 2016 generally speaking the backpropagation process usually involves three stages the feedforward pass to calculate output based on the input data backpropagation of the associated error and finally updating of the weight and bias parameters mathematically a training example x is selected at random from the dataset and is fed into the network the value of each node in the hidden layers and the output layer is denoted by vj and y k respectively to this end the loss function l y k y k is calculated in order to measure the difference between the network output and real value for each output node k for which one calculates 3 δ k l y k y k y k l k a k where lk is the activation function for kth output node and ak is the weighted sum or the input for the activation function given the value for each node in the intermediate prior layer one calculates 4 δ j l j a j k δ k w k j where wkj is the weight from node j to k such a procedure is repeated to produce δ j for the jth node until the input layer each δ j computes the derivative of the loss function with respect to the jth node s activation function input given the values vj calculated in the forward pass and the values δ j calculated in the backward pass the derivative of the loss function l with respect to w jj is computed by 5 l w j j δ j v j the parameter is then updated based on gradient descent 6 w j j t w j j t 1 η l w j j t 1 where η is the learning rate that controls how much we adjust the weight with respect to the calculated loss gradient currently the most widely used optimization algorithm is the mini batch gradient descent which attempts to combine the strength of stochastic gradient descent sgd and batch gradient descent bgd the sgd updates the parameters after the computation on each training example for robustness whereas the bgd updates the parameters after the evaluation of all the training examples for efficiency there are also many variants of the sgd such as adagrad duchi et al 2011 adadelta zeiler 2012 rmsprop tieleman and hinton 2012 and adam kingma and ba 2015 the anns are reliable techniques when dealing with problems with an incomplete dataset or with highly complex and ill posed problems where decisions are usually made based on intuition they have been applied successfully to many problems in a number of fields bishop 1995 and can also be used for functional approximation haque and sudhakar 2002 prasad et al 2009 the anns are actually nonlinear parametric function approximators that establish a mapping between multiple inputs and a single output they can also be used effectively to solve problems in pattern recognition such as for instance in sound george et al 2013 olmez and dokur 2003 image sharma et al 2008 zhou et al 2002 or video recognition bhandarkar and chen 2005 karkanis et al 2001 such tasks can be completed without a priori definition of a pattern in such cases the anns learn to identify new patterns and have the ability to associate memories borders et al 2017 michel and farrell 1990 which means they can recall a pattern when given only a subset clue the network structure for such applications is usually complex with many interacting dynamic neurons 2 1 boosting algorithms boosting derives from the computational learning theory alpaydin n d freund 1995 hastie et al 2009 james et al 2000 schapire 1990 and usually performs a classification task it is one of the ensemble methods that aims to improve the prediction accuracy of a classifier by converting multiple weak learners into a strong one hence boosting it the main principle is that a single weak learner has limited capabilities and is difficult to improve but it can be combined with others to build a powerful and stable learner adaboost freund and schapire 1997 is the first successful boosting algorithm developed for binary classification the weak learner or the base learner used in adaboost is a decision tree with one level referred to as the decision stump generally speaking adaboost algorithm contains the following steps initially a weak learner is trained with an equally weighted training dataset then the distribution of the training data is adjusted based on the predicted results of the trained weak learner specifically the misclassified training data are associated with larger weights and then are used to train the next weak learner such steps are repeated until the number of weak classifiers recaches a predefined value finally the weak learners are weighed and combined to form a strong learner gradient boosting friedman et al 2000 extended the method of regression it works similar to a numerical optimization algorithm and iteratively searches for a new additive model that reduces the loss function at each step specifically an initial guess is made with a decision tree to maximally reduce the loss function then at each step a new decision tree is fitted to the residual between the data and the predicted results and added to the previous model to update the residual this step is repeated until a predefined maximum number of iterations is reached note that the decision trees added to the model at prior stages are not modified the added decision tree at each step is usually rescaled with a shrinkage parameter between 0 and 1 it is believed that a higher number of small steps provide higher accuracy than a lower number of large steps touzani et al 2018 in each iteration instead of using the complete training dataset a randomly selected subsample is used to fit the decision tree which can efficiently reduce the computational cost xgboost chen and guestrin 2016 which stands for extreme gradient boosting is a version of the gradient boosting machine but focuses on improving the computational speed and model performance xgboost uses a second order taylor s expansion for the objective function rather than a full objective function for optimization which allows faster computation moreover it introduces some regularization terms to prevent overfitting 2 2 principal component analysis principal component analysis pca is a nonparametric approach used for reducing the dimensionality of a dataset while preserving as much variability statistical information as possible it was invented by pearson pearson 1901 and later developed independently by hotelling hotelling 1933 the pca applies an orthogonal linear transformation to reframe the original correlated data in a new coordinate framework called principal components which are linearly uncorrelated jolliffe and cadima 2016 the projected data on each principal component conserves the maximum variance generally speaking the components are ordered according to the variance of the projected data which means the data projected onto the first component has the largest variance while the variance for the last one is the smallest huang et al 2017 there are two main approaches for the pca eigenvalue decomposition of the data covariance matrix and singular value decomposition svd of the centered data matrix a data matrix x of size m n with m and n being respectively the number of samples and the number of variables is centered by subtracting the column wise mean of x from the corresponding column the covariance matrix c of x a n n symmetrical matrix is defined by c x t x m 1 with t denoting the transpose operation the eigen decomposition diagonalizes c to produce vλv t where the columns of v and the diagonal elements λ i i 1 n of λ are the eigenvectors and eigenvalues of c respectively note that the eigenvalues of c are arranged in decreasing order along the main diagonal of λ and are paired with the columns of v the principal components are the columns of xv alternatively the svd of x yields usv t where the diagonal elements si i 1 n of s are related to the eigenvalues by λ i s i 2 n 1 the principal components are the columns of us generally one selects the first few significant principal components and then projects the data onto them jolliffe 2006 there are many applications of the pca that due to space limitation cannot all be described here it has for example been applied to exploratory analysis and data visualization hibbs et al 2005 pei et al 2007 in this case the pca projects high dimensional data onto a small number of principal components then one visualizes the transformed data in a projected 2d or 3d space the pca based clustering analysis is also very common metsalu and vilo 2015 yeung and ruzzo 2001 the variation trends of the data are mainly represented by the first few principal components while the residual noise is extracted by other components then it is possible to cluster the data by projecting the data onto the first few components regression analysis can also benefit from the dimension reduction provided by the pca abdul wahab et al 2005 camdevyren et al 2005 as sometimes direct regression analysis with high dimensional data could result in saturated models and unreasonable estimates furthermore it is possible to first perform the pca and then use the first few principal components as the inputs to facilitate regression analysis in the literature several variants of the pca have been developed with better numerical performance and desired properties functional pca aguilera et al 1999 allows predicting a time series beyond the discrete time observation times in the past sparse pca zou et al 2006 adds sparsity constrains to the inputs which searches for a linear combination of a few input variables instead of all of them multilinear pca lu et al 2006 can capture useful information from tensor representation and utilizes them to recognize faces kernel pca hoffmann 2007 as a nonlinear feature extractor is capable of preprocessing the dataset to simplify the classification problem robust pca candès et al 2009 is a modification of the standard pca that works well for grossly corrupted observations through decomposition in low rank and sparse matrices 2 3 multidimensional scaling multidimensional scaling mds is one of the widely used methods for visualizing the similarity between several datasets machado and lopes 2020 it calculates the pairwise distances between the datasets and represents them as n points in a cartesian space where n is the number of data points compared to the pca the mds can capture nonlinear dimensionality reduction and provide a more accurate representation of large datasets generally speaking given a distance matrix for representing the dissimilarity between each pair of point object and based on the desired number of dimensions n the mds inserts each data point in an n dimensional space in a way that the distances between the objects points are preserved after which the final distribution is visualized mathematically speaking let us assume that the distances between several points objects are stored in d dij where dij indicates the distance between points i and j d i j x i x j 2 y i y j 2 then the matrix b 1 2 j d 2 j formed where j is a centering matrix next the largest m eigenvalues and their corresponding eigenvectors e m are calculated based on which x e m λ m 1 2 is computed λ m is the diagonal matrix of b chen et al 2008 2 4 support vector machine support vector machine svm is a supervised learning algorithm that does not require any parameterization and hence no assumption about the true distribution of the dataset cortes and vapnik 1995 developed the current basic svm algorithm which due to its robustness and efficiency has been widely used for solving classification and regression problems the key idea of the svm is to project the dataset into another feature space where the dimensionality is higher compared to that of input space which makes the classification task much easier krell 2018 a simplistic but effective way of understanding how the svm works is by viewing it as a sort of a machine that separates the left and right side cars buildings and people on the sidewalks in order to make the widest lane the support vector classification aims to build a decision boundary within the feature space so that each data point is inserted into its corresponding class noble 2006 unlike other classification methods an optimal hyperplane is generated by the svm to maximize the margin between two classes and reduce the generalization error see fig 3 the distances between the hyperplane and the nearest point in each class are summed and maximized by the svm in practice if a dataset belongs to two different classes it is not always separable into two classes in this case the svm still produces a hyperplane that maximizes the margin and minimizes the classification error with a predefined loss function chapelle 2007 the svm is more appropriate for the classification problem where the classes can be separated with a linear straight boundary ben hur and ong 2008 however a curved decision boundary is also very common in the real world kernel functions as nonlinear terms can help transform the data and project onto a new space in which the differences are more visible many kernel functions have been utilized with the svms such as linear polynomial and radial basis function and sigmoid typical svm classifiers include hard margin svm pan et al 2005 soft margin svm wu and zhou 2005 k nearest neighbor svm zhang et al 2006 radial basis function svm prajapati and patle 2010 and translational invariant kernels svm decoste and scholkopf 2002 the support vector regression assumes that we have no knowledge about the joint distribution of the input and output variables and that a correlation between them can be found thus it tries to infer a function y f x according to the training dataset x xi yi i 1 n where xi is the ith input sample yi is the corresponding target value and n is the number of training samples this goal is equivalent to a regression function of the form 7 f x i 1 n a i a i k x i x b where k is a positive definite kernel function and a a and b are the parameters of the model a and a are calculated by minimizing the following objective function 8 1 2 i j 1 a i a i a j a j k x i x j ε i 1 n a i a i y i i 1 n a i a i which is subjected to the following constraint 9 i 1 n a i a i 0 and a i a i 0 c where parameter the c governs the smoothness of the approximation function and ε governs the precision of the approximation function there is an interaction between the parameters and hence increasing or decreasing one of them may change the values of the others both linear and nonlinear kernel functions have been used for support vector regression but the results provided by a nonlinear function have higher precision the optimization problem defined by eqs 8 and 9 is a general frame for the support vector regression problem 2 5 k means clustering k means clustering kmc forgy 1965 lloyd 1982 is used to separate n data points into k clusters specifically each dataset is classified into a cluster that has the smallest difference between its mean value and the value of the current data point the kmc can deal with a dataset with high dimensionality to identify the representative data which are known as the cluster centers for a training dataset x xi i 1 n the cluster centers μ μ j j 1 k are first randomly picked from the dataset then the following procedure is repeated until convergence is achieved for each data point xi one calculates the euclidean distance between xi and each cluster center and assigns the current data point to the nearest cluster s center c i argmin j x i μ j 2 for each cluster one recomputes its mean value to update the cluster s center i e one computes μ j i 1 n c i x i i 1 n c i the kmc stems from signal processing where it was originally used for vector quantization kovesi et al 2001 lee et al 1997 which means reducing the color palette of an image to a fixed number of colors currently it is used for cluster analysis kaur et al 2019 oyelade et al 2010 and feature learning coates et al 2011 lin and wu 2009 note that the number of clusters k is usually difficult to choose considering the fact that no external constraints are given moreover compared to other more sophisticated feature learning approaches such as autoencoder and the restricted boltzmann machine the kmc usually needs more data to achieve comparable performance since each data point only contributes to one feature coates and ng 2012 2 6 k nearest neighbor k nearest neighbor knn is a non parametric algorithm designed to solve regression and classification problems altman 1992 it evaluates the distance similarity between the test and training examples based on a predefined metric function thus the class of a test example depends on the majority vote from the selected neighbors specifically the distance between the test sample and all the stored training samples is calculated using a certain distance function after which k nearest neighbors of the test sample are selected then the test sample is assigned to the class that is most common among the selected neighbors the optimum k value is solved either by using all the examples and taking the inverted indexes jirina and jirina jr 2008 or by using ensemble learning hassanat et al 2014 since one must calculate the distance between each test sample to the selected training examples and store all training samples the computational cost and memory requirements are usually unaffordable various solutions have been proposed to overcome the two problems such as reducing the size of the training dataset gates 1972 hart 1968 and using approximate knn classification arya and mount 1993 zheng et al 2016 the knn preserves the consistency of the calculated results as the number of samples approaches becomes very large the error rate for a two class classification problem is not worse than the bayes error rate which is the lowest possible error rate for the given data distribution 2 7 random forest an algorithm for ensemble learning used for classification and regression is the random forest rf algorithm the rf constructs several decision trees in the training process it then produces the class that is the mode of the classes for the classification problem and the mean prediction value for the regression problem barandiaran 1998 breiman 2001 ho 1995 see fig 4 the rf in a broader view is constructed based on the idea of bagging or bootstrap aggregating proposed by breiman breiman 1996 to improve the classification methods in essence the bagging uses a noisy fitted function and based on its average it reduces the variance the decision trees are constructed by randomly picking a subset of training examples with replacement i e some examples are selected two or more times while others will not appear in that subset at all each decision tree is independently produced without any trimming and each node is split using a predefined number of randomly selected features by growing the forest up to a predefined number of trees it creates trees that have high variance and low bias belgiu and dragu 2016 given a training dataset x xi i 1 n with targets y yi i 1 n the original bagging algorithm eliminates a sample from the training data b times and fits a tree which results in new training and target data x b and y b this goal is achieved by training fitting a regression tree fb on x b and y b then the final estimate for a new data x is obtained by f 1 b b 1 b f b x based on this algorithm the rf method selects a random set of m features where m p or m log 2 p from each split location out of p original number of features during the learning process this helps with reducing the variance and improving accuracy thus the final classification decision is taken by averaging the class assignment probabilities calculated by all the produced trees the rf has been successfully used in several fields including classification of hyperspectral data ham et al 2005 species immitzer et al 2012 cancer statnikov et al 2008 traffic sign zaklouta et al 2011 and land cover gislason et al 2006 2 8 convolutional neural networks convolutional neural networks cnns developed by lecun lecun 1989 lecun and yoshua 1998 are feedforward neural networks since the data are passed from the input to the output only along the forward direction the input data for the cnn usually have a grid like topology including time series data 1d data grayscale images one channel with 2d inputs color images three channel data with 2d inputs and multidimensional time varying data 3d e g seismic data various architectures of cnn have been proposed in the literature with most of them containing convolutional and pooling layers see fig 5 generally speaking one or more fully connected layers are connected after the convolutional and pooling layers convolutional layers extract the features representing the input data specifically the convolutional operation moves a filter i e kernel across the input layer in a certain order and generates a feature map each movement produces one unit in the feature map by calculating a dot product between the filter and a small local region of the input typically there is more than one filter and each of them is assigned to detect a specific feature the dimension of the filter is usually much smaller than that of the input the mathematical formulation for the output of a convolutional layer expressed by 10 z i j σ k 1 k m l 1 l n w k l x i k 1 j l 1 b where w is the filter with a size of m n x is the input data or the output from previous layers b is the bias for the current convolutional filter and z is the output for the convolution operation in each convolutional layer a stride parameter s is defined to control the skip distance between consecutive filter moves the input data are sometimes padded with zeros around the border in order to control the spatial size of the output the size of zero padding p is a hyperparameter meaning that it should be optimized for a 2d input with dimensions of w h and a filter of size f f the size of the output from the convolutional operation or the size of the feature map is w f 2p s 1 h f 2p s 1 a nonlinear activation function σ is used to conduct an element wise transformation after the convolution operation traditionally the sigmoid and hyperbolic tangent functions were used to add nonlinearity but they bring about a vanishing gradient problem to retard the training process kolen and kremer 2010 glorot et al 2010 found that a piecewise linear activation function namely a rectified linear unit relu can relieve that problem relu defined as max 0 x where x is the input is faster to compute and outputs a sparse representation i e only about half of the hidden units are active and have non zero outputs the pooling layers aim at reducing the spatial resolution of the feature maps and canceling the influence caused by distorting and translating the input data ranzato et al 2007 average pooling is one of the popular functions that computes the mean value of a small local region in the feature maps and then passes it to the next operation recently max pooling has also been used for sending the maximum value of a small region to the next layer scherer et al 2010 found that max pooling technique is skilled in extracting the invariant features from image like data improving the generalization performance and converging much faster hence outperforming the traditional subsampling operation to extract the latent features with a high degree of abstraction multiple convolutional and pooling layers are connected where the simple features extracted by lower convolutional layers are further processed by the following layers to produce more complex feature representations the fully connected layers at the final layers aim to understand and utilize the extracted high level features and complete the reasoning tasks such as regression or classification zeiler and fergus 2014 the key idea of convolution operation namely sparse interactions parameter sharing and equivariant representations is to reform the world of machine learning and make them closer to human s abilities goodfellow et al 2016 for example the sparse interactions are achieved by reducing the size of the filter as they are smaller than that of input data and result in less memory volume and higher efficiency of the computations parameter sharing indicates that instead of studying a different set of parameters for each location only one set needs to be studied and therefore it further relieves the storage demanding for model parameters equivariant representations also imply that the way the output changes is the same as that of input data the capability of cnn to learn hierarchical representations of context invariant features enables its application to image classification and recognition there are many cnn based deep architectures in the literature lenet lecun et al 1998 lecun et al 1989 a pioneering work by lecun is used for handwritten digit recognition and later for reading zip codes digits alexnet krizhevsky et al 2012 is similar to lenet but much bigger and deeper with all the convolutional layers stacked together which won the ilsvrc 2012 competition award googlenet szegedy et al 2015 introduced a new architecture called inception in which the depth of the network is increased but with fewer parameters than the alexnet vggnet wang et al 2015 does a thorough analysis of the depth factor in a convnet and efficiently controls the number of parameters by using very small 3 3 convolution filters in all the layers resnet he et al 2016 is a learning framework in which the layers learn residual functions with respect to the inputs instead of learning unreferenced functions it was demonstrated that the residual networks are easier to optimize to gain much better accuracy the aforementioned cnns utilizes typically one or more fully connected layers between the last convolutional layer and the classifier in order to enable classification into a small number of classes currently end to end fully cnns such as the u net ronneberger et al 2015 and segnet badrinarayanan et al 2015 are preferred both of which have been used for image segmentation 2 9 autoencoder autoencoder is a widely used unsupervised learning algorithm which attempts to accurately reconstruct or reproduce the input data autoencoder was first introduced by hinton and the pdp group to address the problem of backpropagation without a teacher by using the input data as the teacher rumelhart and mcclelland 1987 a typical autoencoder consists of an encoder a decoder and a latent layer see fig 6 given the input data x r n where n is the data dimension the encoder function first projects it onto a latent space h rl with a weight matrix w n bias b n and an activation function σ r 0 1 11 h σ w n x b n the decoder function then transforms the latent representation into the output z r n through an inverse mode with a weight matrix w h bias b h and an activation function δ r 0 1 12 z δ w h h b h the tied weight strategy w n w h w has usually been applied to simplify the network structure therefore the parameters to be solved for are w b n b h given the training samples the objective function to train an autoencoder is to minimize the cost function 13 arg mi n w b n b h l x z which measures the difference between the input and reconstructed data by for example the mean square error the autoencoder is mostly used for reducing the dimensionality of input data and generating a latent representation of them while neglecting the effect of noise that may appear in the dataset specifically the implicit probability distribution of the data is estimated based on the training dataset with new samples drawn from the inferred data distribution the training data with high dimensionality is compressed to a low dimensional latent representation based on the assumption that the real data could be reconstructed from them with the help of the decoder this assumption is based on the relative characteristics of the patterns of the output that are stored in the latent layer and interpreted by the model there are different types of variants for autoencoder sparse autoencoder makhzani and frey 2013 adds regularization the sparsity constraint on the hidden neurons or the latent representation a neuron is defined as active when its output is close to one and inactive when it is close to zero a sparse autoencoder reduces the number of neurons being used by disabling some of them the constraint is realized by a penalty named kullback leibler kl divergence kullback and leibler 1951 14 d k l ρ ρ j ρ log ρ ρ j 1 ρ log 1 ρ 1 ρ j where ρ j is the average activation of the jth hidden unit with the average taken over the training set and ρ is the desired activation value close to zero a denoising autoencoder vincent et al 2008 was proposed for dealing with the small variations of the input data which can generate the same result even if the input data are partially destroyed in the training process we first corrupt the input data x with one of the two main kinds of corruption methods binary noise and gaussian noise in order to obtain a destroyed version x for binary noise several data points are randomly picked and their values are set to zero gaussian noise generates a set of random values following the gaussian distribution and adds them to the original data the destroyed input is sent to the vanilla autoencoder different from the archetypal autoencoder the outputs from encoder and decoder of variational autoencoder represent a sample drawn from a parameterized probability density function doersch 2016 given an example x the probabilistic encoder produces a distribution q φ h x e g gaussian over the possible values of latent representation h rather than a single point where the model parameters and latent states sample this statistical distribution h qϕ h x the probabilistic decoder p θ x h is a conditional generative model that computes the probability of generating x given the latent variable h a variational autoencoder then tries to approximate p θ x h by the given distribution q φ h x it is also of high significance to keep the balance between the reconstruction accuracy and matching the gaussian distribution therefore the loss function consists of these items the reconstruction accuracy is measured by the mean square error and the dissimilarity between the distribution of the latent representation while the gaussian distribution is represented by the kl divergence variation autoencoder could produce samples that do not appear in the real training dataset while the fully connected autoencoder commonly ignores the spatial structure within the image a convolutional autoencoder masci et al 2011 is proposed to handle this problem the convolutional autoencoder is a combination of the vanilla autoencoder and convolutional layers some redundancy of the model parameters is introduced to force the learned representation to be global i e covering the entire input data moreover since the weights and bias are shared as the filters are moved over the input data the spatial locality can be conserved on the other hand the basic autoencoders can be concatenated on top of each other in order to construct a deep stacked autoencoder qi et al 2014 which consists of an input layer several hidden layers and an output layer a stacked autoencoder inherits the benefit of deep neural networks and can still learn a deep representation of the input the first hidden layer tends to learn the low level features of the input data the hidden layer that follows it typically produces the high level features extracted from the pattern that appeared in the previous levels therefore a hierarchical representation of input data is extracted 2 10 generative adversarial networks generative adversarial networks gans are deep generative models that utilize various training methods in order to infer the probability density function of the real data based on the training dataset the standard gan consists of two adversarial networks a generator and a discriminator to compete with each other see fig 7 the goal of the generator is to produce realistic images that can deceive the discriminator while the discriminator tends to identify the fake images generated by the generator network and classifies the images sampled from the given image dataset as real goodfellow et al 2014 i j 2015 this minimax game is an alternative and creative means of inferring the probability density function of the original image dataset the gans have been used in many fields such as image to image translation ma et al 2018 improving image resolution wu et al 2017 medical image processing armanious et al 2018 and video frame prediction a x a x lee et al 2018 in all such applications the gans have shown to be superior to the other traditional methods in terms of producing images with high resolution and fidelity the vanilla gans draw samples directly from the assumed data distribution and do not explicitly model the underlying probability density function the input z for the generator g is simply random noise drawn from a prior distribution p z typically a gaussian or a uniform distribution the output xg from the generator is expected to be visually similar to the real sample xr which is sampled from the real data distribution pr x thus the generator tries to learn a nonlinear mapping function parameterized by θ g xg g z θ g the input for the discriminator d is either a real or produced synthesized sample and the output from the generator is a single number that indicates the probability of the input is a real or fake sample hence the discriminator tries to learn a mapping parameterized by θ d yd d x θ d the approximation of the real data distribution pr x is denoted by pg x the discriminator tends to separate these two groups of samples while the generator is taught to deceive the discriminator as much as possible in such a process the gradient information is backpropagated from discriminator to generator so the generator updates its parameters in order to generate a sample that can deceive the discriminator the objective functions for the generator and discriminator are given by 15 l g min g e x g p g x log 1 d x g 16 l d max d e x r p r x log d x r e x g p g x log 1 d x g clearly the discriminator is a binary classifier with a maximum log likelihood objective function the desired result after training is that distribution formed by the generated xg that approximates the real data distribution pr x it is still very difficult however to train a gan due to its large number of training parameters and applied nonconvex cost function more specifically the generator and discriminator have trained alternately from scratch with the gradient based method but there is no guarantee for the balance between the training of generator and discriminator as a result the discriminator can sometimes become too strong compared to the generator and the produced samples become too easy to be separated from the real ones the gradient from discriminator will approach zero which provides no guidance for further training of the generator another problem commonly encountered is model collapse when the approximated distribution pg x learned by the generator focus on a few limited modes of the real data distribution pr x nguyen et al 2017 despite such training problems a number of variants of the gan are developed every year conditional generative adversarial nets cgan mirza and osindero 2014 is a conditional version of the gan it is conditioned on both the generator and the discriminator for example it can generate mnist digits conditioned on the class labels deep convolutional generative adversarial networks dcgans radford et al 2015 were developed for producing high dimensional results infogan chen et al 2016 is used for extracting useful features and representing them as encoding in an unsupervised manner when the dataset is simple class labels are unavailable and one only wants to see the most important features of a dataset and have control over them the infogan is preferred wasserstein gan wgan arjovsky et al 2017 incorporates the wasserstein distance into the loss function in order to make it much more stable that depends on the quality of the generated data 2 11 long short term memory long short term memory lstm is a recurrent neural network rnn used mainly for processing time series data originally discovered by hopfield 1982 and extended by williams et al 1986 see also nasrabadi and choo 1992 the rnns are especially suited for modeling time sequence data and the data with variable lengths they have the ability to selectively deliver the information through successive steps while dealing with sequential data one element at a time unlike feedforward neural networks the rnn has the so called recurrent edges to form circles which are self connected from a node to themselves across time specifically at time t the node with recurrent edges receives input data points x t and hidden node values h t 1 from the previous state the output y t at each time t is calculated based on the hidden node values at the same time thus the input x t 1 at time t 1 influences the output y t at time t through the recurrent connections mathematically the forward pass in a simple rnn is governed by following equations 17 h t σ h x t w hh h t 1 b h 18 y t σ y w yh h t b y where w hx is the weight between input and hidden node w hh is the weight for hidden node between successive time steps and w yh is the weight between hidden node and output b h and b y are bias parameters representing an offset σ h and σ y are activation functions such as tanh and relu to introduce nonlinearity as shown in fig 8 the network can be interpreted as noncyclic by unfolding it as a deep neural network with one layer per time step and shared weights across time steps using backpropagation the unfolded network is trained over many time steps werbos 1990 however backpropagating errors through many time steps may give rise to vanishing gradients pascanu et al 2012 described a comprehensive analysis of the vanishing gradient problems and in order to address it suggested adding a regularization term that imposes weights on the values so as to prevent the gradient from vanishing in particular the lstm hochreiter and schmidhuber 1997 was introduced to overcome this problem which replaces the hidden unit with a memory cell that contains a node with a self connected recurrent edge of fixed weight one to guarantee the flow of gradient across many time steps all elements of the lstm are illustrated in fig 9 the input node g t calculates a weighted sum of the current input data x t and previous hidden unit h t 1 and then runs through a tanh activation function 19 g t tanh w gx x t w gh h t 1 b g the input gate i t receives the same inputs as the input node but with a sigmoid activation function for controlling the information flow 20 i t sigmoid w ix x t w ih h t 1 b i specifically if its values are zero then flow from the input node is cut off if its value is one all the flow is passed through the forget gate f t introduced by gers 2001 also receives the same inputs as the input node and allows the network to flush the contents in the internal state 21 f t sigmoid w fx x t w fh h t 1 b f the internal state s t is the heart of each memory cell with a linear activation function it has the aforementioned recurrent edge with fixed unit weight to avoid gradient vanishing the internal state receives information from the input node g t and the previous internal state s t 1 under control of the input gate i t and forget gate f t 22 s t g t i t s t 1 f t where is pointwise multiplication the output gate o t and the internal state s t yield the value h t of the hidden unit at the current time step 23 o t sigmoid w ox x t w oh h t 1 b o 24 h t tanh s t o t usually the internal state runs through a tanh function to endow each cell s output with the same dynamic range as an ordinary tanh hidden unit currently the relu activation function is more common due to its greater dynamic range generally in the forward pass the lstm learns when to let activation operation into the internal state by the input and output gates if the two gates are closed their values are zero activation is dormant in the memory cell and does not affect the output at intermediate time steps in the backward pass the constant recurrent edge of the internal state governs the flow of the gradient over many time steps implying that the gates learn when to let error in and when to let it out therefore the lstm has a much better ability than the rnn to learn long range dependencies various lstm networks have been developed to process data in the real world generally speaking they can be divided into two categories the lstm dominated networks that are mainly built by the lstm cells for optimizing the connections of the inner lstm cells in order to enhance network properties such as stacked lstm network saleh et al 2018 bidirectional lstm network graves and schmidhuber 2005 multidimensional lstm network graves et al 2007 graph lstm network liang et al 2016 grid lstm network kalchbrenner et al 2015 and convolutional lstm network x shi et al 2015 integrated lstm networks which consist of the lstm layers and other components such as the cnn to integrate the advantages of different components such as dbn lstm network vohra et al 2015 cfcc lstm network y y yang et al 2018 c lstm network zhou et al 2016 and lstm in lstm network j j song et al 2016 have also been developed 3 applications given a large amount of data collected and stored over the past years the ml has been widely used in several massive and complex data intensive fields such as geoscience hydrology energy resources biology medicine and economy based on the application of the ml in other disciplines we can get an initial impression about the kinds of problems for which the ml is well suited and the state in which the ml is applied in various disciplines in what follows we briefly describe the application of the ml to two broad categories of disciplines namely fine and large scale systems involving porous media and describe how the ml methods are applied to solve specific problems in each field 3 1 small scale porous media unlike the large scale systems that have been investigated within the ai framework for a long time fine scale porous media have only recently experienced significant progress in the application of the ml methods one reason is the availability of high quality images thanks to the progress in imaging and non destructive scanning furthermore by the leap advances in characterization of complex materials sahimi 2003 torquato 2002 as well as the emerging stochastic simulation tahmasebi 2018 tahmasebi and sahimi 2015a 2013 using advanced ml and deep learning dl methods are becoming more practical such methods can produce a vast amount of data that is necessary for training the neural network in a reasonable time in this section we put the emphasis on geomaterials and in particular recent studies based on the dl important properties of porous media such as permeability and porosity as well as those describing the state of a porous medium in which two fluids are flowing such as saturation are among the properties that determine the efficiency of many other operations involving porous media estimation of such properties has been always a challenge to scientists both experimentally and computationally clearly the experimental methods can be costly and often need a considerable amount of time which makes them difficult to use on a large number of samples and hence the uncertainty and in turn an accurate estimation of the intrinsic properties of porous media is difficult to address similarly computational modeling with numerous samples is also time demanding through recent advances in the ai techniques as described earlier the prediction of such important properties without carrying out time consuming experiments is becoming possible a review of the literature indicates that as far as the application of the ai to various problems involving small scale porous media is concerned a few specific types of ai methods have been utilized such studies are focused on for example image enhancement kamrava et al 2019a estimation such properties as the permeability resistivity and diffusivity kamrava et al 2019b tembely and alsumaiti 2019 wu et al 2018 porous media reconstruction adams et al 2018 feng et al 2019a j 2019b liu et al 2019 mosser et al 2018a l 2017 shams et al 2019 tran and tran 2019 p and s wave velocities estimation karimpouli and tahmasebi 2019 segmentation karimpouli et al 2019 conditional simulation of three dimensional pore models mosser et al 2018b mapping between design variables and microstructures li et al 2018 z y yang et al 2018 classification of surface wettability yun 2017 microstructure synthesis fokina et al 2019 estimation of mechanical and thermal properties of porous materials avalos gauna and palafox novack 2019 pires de lima 2019 wei et al 2018 wu et al 2019 zhang et al 2019 and using the ml ability for estimation of other physical property bélisle et al 2015 kamrava et al 2019a developed a hybrid stochastic deep learning method hsdl by combining a stochastic algorithm for reconstruction of porous media and the dl method through a cnn for enhancing the resolution of their images kamrava et al 2019a they used a limited number of high resolution 2d images in a stochastic reconstruction method and generated numerous realizations capturing the multiscale morphology of the pore space then the generated images were preprocessed and used as a training dataset in the dl once the network was trained a single low resolution image was fed into the trained network for enhancing the image s resolutions using several statistical properties and morphological characteristics of the porous media kamrava et al 2019a made a quantitative comparison between the low resolution image and the high resolution ones generated by the dl some of the images generated by their stochastic reconstruction method the cross correlation based simulation or the ccsim tahmasebi et al 2012 are shown in fig 10 where a 2d image was used to generate multiple realizations that are different from each other and from the input image but with statistically similar morphologies this approach has been shown to be an efficient method for generating large training datasets for the dl method kamrava et al 2019a the overall schematic of their proposed method is shown in fig 11 as shown a limited number of high resolution 2d images are used in the stochastic reconstruction method in order to generate a large number of realizations to diversify the generated images further different filters were used with the generated realizations then the new realizations were used as a training dataset in the dl to enhance image quality based on a single image for testing the dl method an unused high resolution image the reference image was converted to a low resolution image and was fed to the network the enhanced image generated by the hsdl method was then compared with the original image and the enhanced images generated by various other image super resolution methods see fig 12 kamrava et al 2019a also made comparisons based on various statistical measures and porosity they reported that all the comparisons confirm that the enhanced image generated by the hsdl method has better resolution and very similar physical properties when compared to the original reference image kamrava et al 2019a aside from this study several other methods have been proposed to enhance the resolution of low quality images of porous media wang et al 2020 wang et al 2019 for example wang et al 2019 implemented a 3d cnn network to generate image super resolution and compared the results with bicubic interpolation as mentioned earlier another application of the dl to the fine scale porous media has been estimating important flow and transport properties of porous media such as permeability resistivity and diffusivity the idea is to use the dl s ability for unrevealing important and latent features in complex porous media and linking them to the properties and to reduce the computational time in other words although the training phase of the dl may require a large amount of time it can perform the estimation for new samples in a matter of seconds once the training phase is concluded wu et al 2018 used cnn to generate models of porous media and computed their permeability they noted that since the permeability is a sole function of the pore geometry it should be possible to estimate it based on the geometry and its images using the cnn and avoid direct simulations or pore network calculations their proposed algorithm was as follows 1 generating a large number of 2d images using the voronoi tessellation algorithm and calculating permeabilities of the generated images using the lattice boltzmann method which were then used as the training dataset 2 performing the training of the cnn and 3 testing the trained network with new images for predicting their permeability they also added to the cnn some physical properties that affect the permeability in order to enhance the accuracy of the predicted permeability but reported however that not all physic informed cnn perform better than the regular cnn they suggested that rather than training the network only with the target variable permeability if the porosity φ and specific surface s area are added to the network the results accuracy will increase some of the permeability predictions generated by the proposed method are presented in fig 13 the physic informed method has higher accuracy than the regular cnn while both predictions have about 10 error if we consider the lattice boltzmann calculated permeabilities as the true values they concluded that for some cases where the pores are dilated physics informed cnn does not provide more accurate predictions than the regular cnn as shown in fig 14 they also studied the effect of increasing the number of seeds n in the permeability prediction for material with dilated pores by physics informed cnn by seeds we mean the cells generated on a plane for partitioning it by the voronoi tessellation method fig 15 compares the permeabilities predicted by physics informed cnn the empirical kozeny carman equation and the numerical results obtained by the lattice boltzmann simulations the cnn predictions agree with lattice boltzmann simulations to within 10 percent they are also much more accurate than the predictions provided by the empirical kozeny carman equation wu et al 2019 used a cnn network to predict the effective diffusivity de of a porous material based on 2d images they generated a large dataset using reconstruction methods and calculated their effective diffusivity using the lattice boltzmann method considered as the true values the generated images and the corresponding effective diffusivities were used for training the network which was then used to predict the effective diffusivity for new images the results are shown in fig 16 when the true values of de is less than 0 2 and the porosity is between 0 28 and 0 98 then 95 of the predicted results by the proposed method are within 10 error from the lb methods and more accurate than the predictions by the effective medium approximation of bruggeman 1935 they also reported that when de 0 1 then the cnn predictions have high errors approximately more than 30 wu et al 2019 and since porosity is correlated with effective diffusivity it can be used to enhance the predictions by the cnn thus they developed physics informed cnn by adding the porosity of each porous material to the flattened feature map of the last pooling layer in order to form the first fully connected layer on the cnn wu et al 2019 also studied another type of cnn namely cnn with pre processed input where they removed the trapped and or dead end pores in the images of the porous structures in order to further enhancing the cnn predictions fig 17 compares the de predicted by the cnn and the lb method as well as the results by the cnn and bruggeman s equation de εβ 1 wu et al 2019 where β depends on the structure of porous media they reported that the cnn can better predict de for porous materials with complex structure than bruggeman equation they further expanded the cnn ability for predicting the effective diffusivities over various ranges and reported that the absolute error for the predicted de by cnn is smaller than 0 1 for all the ranges the relative error for the predicted de by the physic informed cnn is approximately 12 less than those of the regular cnn wu et al 2019 in another set of studies liu 2017 liu et al 2019 experimental tests using capillary tubes were carried out in order to measure the saturations phase conductance and two phase capillary pressure and relative permeabilities drainage and imbibition experiments were carried out for generating the relative permeability curves the data were then used as the training datasets in an ann to predict the relative permeability and capillary pressure the input parameters that were perceived as unnecessary were identified using sensitivity analysis two approaches were used in the study in the first a neural network was used to predict only the capillary pressure curve pcow using the input data that were related to the tube s cross section geometrical properties in the second approach they predicted the threshold capillary pressures and water oil relative permeabilities simultaneously using the neural net liu 2017 a comparison of the results for pcow by the trained neural network trained using both datasets versus the calculated results from experiments using the first approach is presented in figs 18 a and 18 b respectively the training datasets consisted of 3000 random polygons with shape factor from 0 to 0 04 and 3000 random polygons with shape factor from 0 04 to 0 07958 the two studies indicated that the results are better predicted when the second training dataset is used and that the predictions by the nn depend on the elongation factor of the system it should be noted that the input data and models were designed for pores with circular cross sections and for pores with more realistic geometry the same type of accuracy may not be obtained liu 2017 in a sperate study kamrava et al 2019b used a physics guided dl method to predict the permeability of porous media based on the morphological information gained from 3d images of the porous materials they noted that having a large amount of data for training a dl network is crucial the input data used in their study included 3d images of sandstone and a large number of realizations of porous media generated by a stochastic image based reconstruction technique as well as by a boolean method and their corresponding permeabilities in their method important features were extracted from the images using the dl algorithm which were then used for estimating the permeability the cnn as a nonlinear mapping between the given images and the given permeabilities was used which was connected to a standard shallow feed forward ann they evaluated the accuracy of their method by testing it using high resolution 3d images of real sandstone the schematic representation of their cnn is shown in fig 19 some of the extracted features for one of their samples are shown in fig 20 the cnn utilized in their study was based on a supervised learning algorithm this means that 3d segmented images of berea sandstone and their computed permeabilities were given as the input and output to the cnn in order to learn the mapping between them i e the training process they selected a set of diverse data based on their permeabilities from the lower median and upper quantiles of the permeability distribution the results are depicted in fig 21 to validate the trained network it was tested with a very distinct sample of sandstone fontainebleau with relatively low porosity 14 and very different morphology and permeability than the berea sandstone that had used in the development of the cnn a correlation coefficient of 0 9 was reported which is only slightly smaller than what had been achieved for the berea sandstone kamrava et al 2019b wei et al 2018 used the cnn for predicting effective thermal conductivities of porous materials wei et al 2018 they used a quartet structure generation set for generating model porous materials and calculated their effective thermal conductivity using the lb method they then compared various ml algorithms such as a cnn the svr and gaussian process regression gpr for predicting the effective thermal conductivity the results are shown in fig 22 all the ml algorithms had better accuracy for predicting the effective thermal conductivity than the maxwell eucken maxwell 1904 eukan 1932 and the bruggeman models a vast number of published papers in the field of fine scale porous media have been devoted to reconstruction which is often achieved using gan networks such studies are usually based on the methods in computer science and have not undergone considerable change therefore we only mention some of the key results here cang et al 2017 used a convolutional deep belief network to produce stochastic models of porous media although they used the method to model complex materials they can also be used for geomaterials as well several methods have also been used for reconstruction of the microstructure of porous materials for example gans was used for reconstruction of an image of a relatively homogenous sample of berea sandstone and a heterogeneous sample of estaillades carbonate liu et al 2019 subsamples of 3d images of porous rocks were used for training the network they also added nonlinear statistical information gained from 3d micro ct images of actual rock in the training the authors noted that gans generated reconstructed images have better quality for homogenous rocks than heterogeneous ones the generated images by the gans along with real samples are shown in fig 23 the gans were also for 2d and 3d image reconstruction using segmented volumetric images mosser et al 2018a l 2017 the accuracy of the model was verified using some of the statistical morphological and transport properties of the samples such as the euler characteristic two point statistics and directional single phase permeability some of the results are shown in fig 24 it was noted that the gans require graphics processing units gpu and large gpu memory due to the usage of 3d datasets in the training process in the same group of the application of the gan methods to porous media problems feng et al used conditional gan cgan which is claimed to be an improved version of the gans for generating realistic images feng et al 2019a j 2019b they noted that the gans input data are only a noise distribution such as gaussian or a uniform noise from specific samples whereas the cgans input data are gaussian noise and conditional data the added noise is meant to add diversity in the generated output feng et al mentioned that the cgans is a deep cnn in which every neuron in a layer is connected to all other neurons in another layer whereas neurons in a cnn are only connected locally between two adjacent layers with all the weights being similar in one layer the loss function is used to minimize the error in both the generator and discriminator sections feng et al 2019a 2019b the cgan is schematically shown in fig 25 the dl methods have also been used widely to address important problems in fluid mechanics raissi et al 2019a 2019b 2018 for example the dl is used to encode incompressible navier stokes equations integrated with the structure s dynamic motion equation in order to study vortex induced vibrations raissi et al 2019b the proposed method enables velocity and pressure quantification from flow snapshots in small subdomains raissi et al used the dl to predict fluid s lift and drag forces on the structure based on very limited information on the velocity field or snapshots of dye visualization they applied the navier stokes informed deep neural networks to predict equation for the structure s dynamic motion using their method for three scenarios first with given acting forces on the body they predicted the structure s motion second with the given velocity field and the structural motion at a limited location in space time they determined the lift drag forces the pressure field the entire velocity field and the structure s dynamic motion third with given concentration data in space time they obtained all flow fields components and structure s motion and the lift and drag forces some of their results for predicting the concentration of passive scalar velocity field and pressure lift and drag forces by their suggested algorithm are shown in figs 26 and 27 respectively the authors noted that their algorithm can accurately order of 10 3 reconstruct the velocity field the concentration of passive scalar and pressure without sufficient data on these quantities raissi et al 2019b 3 2 large scale systems 3 2 1 geoscience as the concepts of big data continue to impact society and science geosciences have also experienced a major transformation of the availability of data a number of geoscience data bank has been produced which provides tremendous potential for the application of the ml by geoscientists for accurate modeling of the state and evolution of the earth system indeed many ml methods are a natural fit for the problems encountered in geoscience applications one application is classification and pattern recognition methods that can be used to characterize objects and events in geosciences vital for understanding the earth s systems for example liu et al 2016 developed a deep cnn to tackle the problems linked with climate pattern recognition they classified tropical cyclones atmospheric rivers and weather fronts based on images provided by climate simulations and observations the model achieved fairly high classification accuracy ranging from 89 to 99 accuracy due to a lack of labeled data for a reasonable period of time in various regions racah et al 2016 implemented a 3d height width and time convolutional auto encoder cae to utilize temporal and unlabeled data to increase the accuracy of identifying extreme weather events lguensat et al 2017 developed a u net like a network that consists of a cae connected by a pixel wise classification layer to automatically detect and classify the eddies from sea surface height maps their global accuracy was around 90 on the other hand the ml can extract important geoscience variables that are hard to monitor and compute directly such as the concentration of pm2 5 in air using information about other variables collected through satellites or simulated with earth system models pm2 5 are very fine particles in the air that reduce visibility and cause the air to appear hazy when their levels are elevated klein et al 2018 demonstrated an accurate and scalable method of vegetation management along transmission lines by combining an ml algorithm with high and medium resolution multispectral satellite imagery the trained cnn models could detect the tree types estimate tree heights and determine the health of trees kosovic et al 2018 used the rf and ann to learn the nonlinear relationship between the fuel moisture content fmc and vegetation index derived from satellite observations the results for the test dataset indicated improvements in accuracy for both live and dead fmc estimation when compared with persistence and linear regressions predicting the variation tendency of the earth system for a long period of time is another significant task since it can help to appropriately manage the resources and adapt current policies with the given predicted information in the ml context this can be formulated as a problem in time series regression that forecasts the future trends of a geoscience variable with the provided history and current conditions y yang et al 2018 tried to improve the accuracy of predicting sea surface temperature using a two part model one 3d grid constraining local correlation and global coherence and the combination of one fc lstm layer and one convolutional layer they carried out experiments on two different datasets validated the results and concluded that the accuracy is better as compared with other methods ye et al 2019 developed an ensemble spatially explicit and continental scale projection model with 1 38 million grid cell sub models to predict forest cover dynamics each grid cell model was built as an lstm network and developed on a spatiotemporal dataset of different kinds of data the proposed model greatly outperformed a spatial econometric model with a 44 improvement in the root mean square error indicator in a similar way a significant amount of attention has been paid to predicting precipitation with different kinds of ml methods akbari asanjan et al 2018 kim et al 2017 shi et al 2017 2015 interpreting the interaction between different physical processes and estimating their internal relationships is also of great importance in problems related to earth science as one deals with complex and multivariate data identifying such relations from geoscience data can advance our understanding of the geological processes holtzman et al 2018 proposed an ml approach to identify the temporal patterns of earthquakes they transformed the waveform data to fingerprint reduced dimensionality and removed features common to all the signals with non negative matrix factorization nmf and hidden markov model hmm and finally clustered the fingerprints with the k means clustering described earlier they found that seismic events with similar spectral properties repeat on annual cycles within each cluster and that variations in acoustic properties and faulting processes accompany changes in thermomechanical state kratzert et al 2019 developed an entity aware ea lstm to investigate as to why the watersheds over the continental u s have the largest sensitivities to climate related forcing in extreme low and high flow periods the models predicted daily streamflow based on 32 input features from 531 basins the result showed that the proposed model is more accurate than the existing ones even for basins that were not used for training 3 2 2 hydrology three main approaches namely deterministic probabilistic and stochastic have been used in hydrological modeling to characterize the groundwater systems and other coupled models in hydrogeology such methods require identifying the right physics and implementing them in large models to reproduce the observations the process can be very time demanding if the number of variables and the size of the models increase significantly on the other hand data driven approaches such as those based on the ml allow a deeper understanding of complex problems without detailed knowledge on the internal structure of the observed system the application of the ml techniques in hydrology has been evolving as data and methods become more and more accessible for example in some cases related to hydrology carrying out large physics based simulations might not be required if the relationship between the input data and the target output can be extracted using data driven methods the following is a discussion of several examples of solving hydrological problems with the ml based on which similar results can be obtained faster as discussed earlier a prime motivation for utilizing the ml methods is taking advantage of the vast amount of data that is available and in this case those that are provided by scientific satellites the ml can extract useful hydrological information from such images tao et al 2016 developed a deep neural network to improve the accuracy of satellite precipitation data aim at avoiding bias and false alarms a stacked denoising autoencoder sdae was trained to process satellite cloud images in order to extract useful features over a window of 15 15 pixels they tested the model on two variables one for rain no rain detection and another one for precipitation quantification and obtained significant improvements in the predictions for both cases improving respectively by 33 and 43 on false alarm pixels and 98 and 78 bias reductions in precipitation rates over the validation periods of the summer and winter seasons zhan et al 2017 introduced a cnn to extract useful patterns for cloud and snow detection from the multispectral satellite images the results indicated that the proposed deep neural network model performs better than the state of the art methods both in quantitative and qualitative measures li et al 2017 included the geographical information in a deep learning architecture named geoi dbn to estimate the pm2 5 from satellite images it was demonstrated that geoi dbn produces more accurate predictions than traditional neural networks the out of sample cross validation r2 increased from 0 42 to 0 88 and rmse decreased from 29 96 to 13 03 µg m3 the ml algorithms can also be used for estimating the correlation between various hydrologic variables and modeling the data collected with sensor networks ghose et al 2018 used an lstm for rainfall runoff modeling based on meteorological observations three experiments were conducted and the results demonstrated that the proposed model was able to simulate the runoff with a high degree of accuracy when compared with a baseline hydrological model ding et al 2019 proposed a flood forecasting model by using the lstm and attention mechanisms to forecast flood the data were from the lech river basin in europe and the results indicated that the model worked well with high research value when compared with the svm fc network and the original lstm zhang et al 2018 developed an lstm to predict the variation trends of water table depth for a long period of time in agricultural areas the proposed model consisted of an lstm layer with another fc layer on top of it and a dropout technique that was added in the first lstm layer comparisons between the r2 scores of the proposed model and that of the double lstm model indicated that the proposed model s architecture is appropriate and exhibits a strong learning capability on time series data in another related field namely simulation of geological media simulation and inversion the ml algorithms can learn and generate complex data distributions which are used to produce realizations of subsurface structure that imitate the observed spatial patterns extracted from the field samples in porous media since the computational cost for probabilistic inversion with multiple point statistics techniques is often unaffordable and prohibitive for problems involving high dimensionality tahmasebi 2018 tahmasebi et al 2018 tahmasebi and sahimi 2015b laloy et al 2017 developed a vae to build a parametric base model with low dimensionality that parameterized a complex binary geological medium they compared their model with the widely used parametric representations for probabilistic inversion problem and showed that the proposed dimensionality reduction approach outperforms the pca optimization pca opca vo and durlofsky 2015 and discrete cosine transform dct techniques jafarpour and mclaughlin 2009 for unconditional geostatistical simulation of a channelized prior model the authors also trained a dnn of the gan type for the same purpose laloy et al 2018 the network was trained based on the training images and the final model could easily produce 2d and 3d unconditional realizations the most creative idea used in the work was that a very low dimensional parameterization was defined as an efficient probabilistic inversion using a state of the art markov chain monte carlo method 3 2 2 1 flood prediction flood is one of the most common natural disasters which could inflict enormous damage to life property and nature physics based modeling often requires a thorough understanding of and knowledge about the settings of hydrological parameters and the prediction of the flood is no exception statistical modeling usually requires a significant amount of data from measurement tools in order to make meaningful predictions therefore the ml approaches are a promising tool for flood prediction since they can automatically decorrelate the flood nonlinearity only according to the provided history information without explicitly describing the underlying physical mechanism furquim et al 2016 used a special theorem to preprocess the data collected from a river by means of wireless sensor tools and forecasted flood with the mlp and rnn the results showed that the mlp performs better than the rnn and that measurement of rainfall can improve the performance of both mlp and rnn since precipitation predictions are also part of the input variable for hydrological models the uncertainties of the precipitation predictions also affect the accuracy of flood predictions in order to combine both uncertainties ensemble prediction systems are usually employed doycheva et al 2017 proposed an approach for increasing the flood prediction accuracy by weighting ensemble members based on their skills the skill of each ensemble member was computed and tested by three types of ml algorithms namely the mlp svm and rotation forest they found that the uncertainty range in flood risk predictions is reduced with the proposed method and the ml approaches make the evaluation process more objective and automatic on the other hand liu et al 2017 developed a deep learning method by incorporating the sae and bpnn for the prediction of the flood which combines the strong feature representation capability of the sae and superior predicting capacity of the bpnn the sae is composed of several sparse autoencoders where the output of each layer is connected to the following layer as the input data the experimental data showed that the sae bpnn integrated algorithm outperforms the svm and rbf neural networks hu et al 2019 combined an lstm and reduced order model rom for flood prediction which is capable of forecasting the spatio temporal distribution of floods the proper orthogonal decomposition pod and singular value decomposition svd approaches were used to reduce the dimensional size of the input data in the lstm the lstm training and prediction processes are conducted over the reduced space this leads to an improvement in computational efficiency while maintaining accuracy the results showed that the lstm rom achieves comparable performance as the full model while its computational cost is reduced by a factor of three when compared with the full model simulations 3 2 3 energy resources with the recent improvements in data recording and collecting tools in exploration drilling and production operations various kinds of data have become available in the field of subsurface energy resources analyzing and interpreting seismic and micro seismic data improving current reservoir characterization and simulation techniques reducing the cost and time for drilling operations and exploring the factors related to drilling safety optimization of the performance of production operations are potential applications of the ml in the energy resources in reservoir engineering the massive number of grid blocks and the slow and iterative nature of solving the governing flow equations is the primary drawback of numerical reservoir simulators the ml models such as an ann putcha and ertekin 2018 sudakov et al 2019b coupled with other algorithms or independently can reproduce the time dependence of fluids and pressure distribution within the computational cells with high accuracy and higher computational efficiency numerous studies have been conducted to predict permeability erofeev et al 2019 sudakov et al 2019a tahmasebi and hezarkhani 2012a wood 2019 zhang et al 2018 and porosity ahmadi and chen 2018 alqahtani et al 2019 2018 from different kinds of source data such as well logs seismic data and digital images of rock the ml methods outperform traditional manual approaches in terms of error and prediction time in drilling engineering the ml can be used to predict and optimize the rate of penetration rop for increasing the efficiency of drilling operations soares and gray 2019 used three ml models the rf svm and ann described earlier to predict the rop based on four input features depth weight on bit drill string rotation speed and drilling fluid flow rate they found that the models reduce error significantly with constantly increased real time data and predict more accurately than analytical models with the same surface data the ml was also used to detect abnormal events during the drilling process gurina et al 2019 applied a gradient boosting classifier gbc to detect anomaly in drilling operations their proposed method compares real time measurements while drilling mwd data and similar information from the previously collected database and the implemented the network learned from the database according to their similarity to the real time predictions the results indicated that the model is robust since it performs well as some appropriate noise and shifts are added to the original date in a similar field where lost circulation during well drilling and completion increases the operation time and even devastates the well under worse conditions several pieces of research have been conducted to address this problem abbas et al 2019 geng et al 2019 shi et al 2019 in production engineering characterizing the flow regime along the pipeline is of great significance for determining the pressure gradient and then the global pressure drop eyo et al 2019 trained a real time flow regime monitor by learning the implicit relationship between the probability density function of collected voltage signals and observed flow regimes through kernel pca and multiclass svm the proposed model achieved better than 90 accuracy against visual observations by an expert on the test dataset the ml approaches can also be used in the more complex problems of well placement because of the heterogeneity and complexity of reservoirs the placement of wells has a large effect on the objective function response gradient boosting method nasir et al 2019 nwachukwu et al 2018 is usually combined with other optimization algorithms to forecast the reservoir response as the injector well locations are changed the results indicated that the hybrid models outperform traditional global optimization algorithms monitoring and optimizing the performance of production pumps is also beneficial for the development of the oil field several ml models such as the rf sneed 2017 svm oliveira santos et al 2018 and the pca sherif et al 2019 were used for effective electric submersible pump management the esp lifespan prediction and automatic fault diagnosis another energy source is landfills which are in essence large scale highly heterogeneous porous media in which the buried wastes undergo biodegradation through nonlinear reactions and generate a considerable amount of methane and other gasses convection and diffusion of the gasses toward the production and observation wells happen continuously giving rise to a highly dynamic system due to the nature of the wastes buried in the landfill their realistic modeling has been very difficult and has relied on empirical or semi empirical models in which the landfills are assumed to be homogeneous at the same time the problem of where to install new observation and production wells in the landfill is both very difficult and essential to operating the landfill safely sahimi and co workers sanchez et al 2007 li et al 2011 2012 2014 took the first steps toward the rational development of models of large scale landfills since one typically has considerable data for the rate of production of the gasses that are measured through the observation and production wells one can use them to not only reconstruct models of landfills but also use the data to train an ann and use it for predicting the future behavior of the landfill and in particular the temperature distribution and the rate of production of methane both are necessary for identifying hot zones in a landfill i e where the temperature has increased considerably and the amount of accumulated methane is also large so that an explosion may occur sahimi and co workers used an optimization method the genetic algorithm and ann to not only develop an optimal model of a landfill but also predict successfully the spatial distributions of the temperature and methane throughout the system 3 2 4 water resources fast advances in the development of the ml algorithms have contributed to many aspects of water resources and subsurface systems since various related applications have to automatically extract useful information from a great amount of data in real time moreover such research is typically multidisciplinary and thus dealing with the growing data volume with traditional workflow is not feasible although the current theoretical knowledge is not enough for us to interpret and explain every phenomenon that occurs in water resources and subsurface systems the ml approaches can provide us new insight to fill the knowledge gap the following describes some applications of the ml approaches to solving some typical problems in water resources which aims to provide a simple overview for water scientists and hydrologists about how the ml algorithm can be related and applied to their research field 3 2 4 1 modeling of groundwater level knowledge about the varying and complex trends of groundwater level gwl in an aquifer is necessary and helpful for water scientists and policymakers in order to better manage groundwater resources and maintain a balance between supply and demand it has been demonstrated that the ml is an efficient and effective approach for simulating and predicting the variational trend of the gwl in various aquifers wunsch et al 2018 applied nonlinear autoregressive with exogenous inputs neural network narx to forecast gwl in several wells precipitation and temperature were selected as the input features all the input time series were decorrelated based on the seasonal trends in order to identify time lags and specify the input and feedback delays needed for the narx the result indicated that the narx based gwl predictions are suitable for unaffected observation wells even with a few number of input variables ghose et al 2018 used the recurrent neural network to predict the gwl with a set of input variables such as rainfall temperature humidity runoff and evapotranspiration the results indicated that evapotranspiration loss and runoff are the most important parameters so that including them improved the model s efficiency guzman et al 2019 compared the narx and svr models for the gwl predictions for irrigation wells in the southeastern usa the best combination from three input variables i e daily gwl precipitation and evapotranspiration were evaluated the gwl plus precipitation scenario was identified as the optimal combination for model inputs with the svr outperforming the narx recently a few hybrid techniques have been used in which some specific data preprocessing were also deployed in order to increase the capabilities of machine learning the wavelet analysis technique is an example of data preprocessing which is very useful in gwl modeling mehrabi and sahimi 1997 sahimi 2000 wavelet is a multiresolution spectral analysis for both temporal and spatial data for example wavelets decompose time series in the time frequency domain in order to generate a time scale description of the processes and their relationships daubechies 1990 in many studies the discrete wavelet transforms dwt was used the decomposed sub time series were used as the inputs rather than the main time series several studies ebrahimi and rajaee 2017 rakhshandehroo et al 2018 yu et al 2018 zare and koch 2018 have shown that the hybrid models have better accuracy than regular models and a greater understanding and ability to simulate the gwl can be achieved on the other hand such ml models can be combined with conceptual numerical models such as modflow mcdonald and harbaugh 2003 to develop hybrid models such that each method can overcome the shortcomings of the other method for example the gwl data estimated by the modflow can be used to train the ml models if there is not enough real data note that the selection of input variables in the aforementioned studies depends on data availability therefore sometimes simple user defined relationships are used sahoo et al 2018 used three ml techniques the svr the rf and gradient boosting mechanism gbm to predict the gwl based on remotely sensed gridded rainfall and soil moisture data regression analysis between monthly lags up to 3 months lag of the three input variables and output variable at zero lag is however a tedious task due to a huge number of combination of variables as such a prior relative importance analysis of the input variables was undertaken in order to identify the relatively more important input variables see fig 28 thus analyzing the corresponding significance of each input variable can help one to identify the one that has the largest effect on the final prediction the previous studies showed that svr models yield more accurate predictions than other approaches 3 2 4 2 groundwater contaminant transport many studies have been undertaken to predict the transport of contaminants in groundwater as the water quality and pollution problem have attracted major concerns the prediction can help with a more reasonable design of groundwater protection programs appropriate treatment of contaminated aquifers and also decision making reliable predictions can however be made only with sufficient knowledge of geological and hydrological parameters of the subsurface system currently the ml approaches are considered to be an effective tool for discovering information and interpretation of raw data stanev et al 2018 and vesselinov et al 2019 developed similar hybrid methods to locate contaminant sources which decomposes the obtained contaminant mixtures according to non negative matrix tensor factorization nmf ntf method coupled with the k means clustering algorithm described in section 2 it has been found that the approach can identify the unknown number locations and properties of a set of contaminant sources from measured contaminant source mixtures with unknown mixing ratios without any additional information cipullo et al 2019 used an ann and the rf to forecast the changes with the time of the bioavailability of complex chemicals based on the data collected in a six month experiment they found that the predictions made by the ann are more continuous whereas the rf can describe the importance of each input variable in forecasting the toxicity mo et al 2019a combined a deep convolutional autoencoder network with an iterative locally updated ensemble smoother algorithm to detect groundwater contaminant sources their method is schematically shown in fig 29 the proposed network was developed to replace the forward model and estimate the functional mapping between high dimensional input and output fields since the cnns are designed to deal with images mo et al reformulated it as an image to image regression problem in the convolutional autoencoder network the functional mapping between input and output images is learned by a down sampling up sampling process that is the convolutional layers in the encoder are used to capture the abstract patterns in the input image and concatenate them as a high level feature map while the decoders are used to interpret these maps and generate the output a densely connected convolutional network structure named a dense block huang et al 2017 was introduced in order to add the connections between nonadjacent layers and improve the transferability of information along with the entire network the encoding and decoding layers are named as the transition layers dumoulin and visin 2016 which are arranged between two neighboring dense blocks to adapt the feature size through transposed convolution operation during the down sampling and up sampling process and to prevent the problem of feature maps explosion in another piece of work by mo et al 2019b the authors developed a convolutional adversarial autoencoder caae to represent the non gaussian conductivity fields with a low dimensional latent vector and incorporated the heterogeneity of formation see fig 30 furthermore a deep residual dense convolutional network drdcn was developed to substitute the forward model and learn the complex functional mappings between the inputs and the outputs this network is shown in fig 31 the drdcn was used as a surrogate model to estimate the relationship between the input conductivity field and the output hydraulic head and concentration fields moreover both input and output data were formed as images the dense block adds the connections between nonadjacent layers aiming to fully exploit the hierarchical features from the outputs of the preceding layers a multilevel residual learning structure was also used since it has been demonstrated that residual in residual dense block structure can effectively increase the network s capacity for estimating highly complex mappings wang et al 2018 the caae network was applied for parameterization of the conductivity field with a low dimensional latent representation the log conductivity field was approximated based on the following steps 1 beginning with an initial latent representation sampled from a normal distribution n 0 1 2 the corresponding log conductivity fields are produced using the caae s decoder 3 the surrogate model is evaluated to obtain the predicted initial output ensemble and 4 the latent representation and output ensembles are repeatedly updated for n iterations based on the current latent representation and output ensembles in the last step the current latent representation is fed into the caae s decoder to produce the log conductivity field which is then used as the input for the surrogate model to perform the prediction the posterior log conductivity fields are generated by the decoder using the last latent representation as input the results for estimating the 2d and 3d non gaussian conductivity fields indicate that the approach can produce an inversion comparable to those obtained by the original inverse method without surrogate modeling while the integrated method is more efficient since the training of the surrogate model requires only a small number of forward modeling others he et al 2019 tartakovsky et al 2018 tipireddy and tartakovsky 2018 a physics informed deep neural network for predicting the parameters of partial differential equations from a limited number of measurements was developed this approach enforces the physical laws described by the governing equations in the process of dnn training they showed that the predictions of the observed and unobserved variables can be used to effectively manage power grids and detect abnormal behaviors and faults 3 2 4 3 water resources management the characteristics and movement of a water body can be partially described by empirical correlations and physical governing equations which formulate the basis for physics based modeling applied in water resources management the ml algorithms provide an alternative method for estimating the relationship between the dependent and independent variables with negligible prior information being involved which can be an efficient and effective tool for solving the classification and prediction problems in the field of water resources management the ml algorithms can also be used for groundwater potential mapping the groundwater is still one of the primary sources for freshwater therefore accurate detection of the zones containing groundwater is an important problem for the establishment of related policies and appropriate management of groundwater resources various types of the ml models have been developed for groundwater potential modeling including the ann s a x lee et al 2018 the rf golkarian et al 2018 the brt kordestani et al 2019 and the svm naghibi et al 2017 all described earlier most recently hybrid modeling and ensemble learning have been applied to optimize the structure of the networks and improve its performance chen et al 2019a 2019b kordestani et al 2019 miraki et al 2019 it has been found that the sample size plays a very important role in the application of groundwater water potential mapping specifically moghaddam et al 2020 compared the performance of individual models and various combinations of them as the sample size was varied the results showed that the rf algorithm is more robust when the sample size is reduced on the other hand the ml algorithms have also been developed for runoff and suspended sediment simulation accurate prediction of runoff and sediment can help one to better manage and utilize the land and water resources many research areas such as water supply water quality and flood prediction are also related to this problem kisi and zounemat kermani 2016 proposed an adaptive neuro fuzzy embedded fuzzy c means clustering anfis fcm algorithm to predict the suspended sediment concentration the performance of the proposed model was compared with that of the standard anfis tahmasebi and hezarkhani 2012b the ann and the sediment rating curve they found that the proposed model has a higher prediction accuracy and requires less computational time than other methods buyukyildiz and kumcu 2017 used the ann svr and anfis to estimate the suspended sediment load in a local gaging station and also tested various combinations of the input data shiri et al 2016 developed an extreme learning machine elm to forecast daily water level change the results showed that the elm technique outperforms genetic programming gp and ann in terms of the speed of learning the relationship between different variables j h wang et al 2019 developed a dilated causal convolutional neural network dccnn that can predict the water level with 1 to 6 h earlier it was demonstrated that the proposed models both the svm and mlp provide acceptable results especially for predicting peak water levels 3 2 5 soil science soil moisture is a major variable in the field of environmental engineering and agriculture science continuously predicting the dynamic variations of soil moisture is beneficial to making early planning for agriculture and providing valuable information for decision makers in the related field this field has also witnessed the application of ml methods for addressing some important problems j song et al 2016 developed a hybrid model by incorporating the macroscopic cellular automata mca model into the deep belief network to forecast the soil moisture in a regularly irrigated paddy field the mca was also combined with the multilayer perceptron mlp to form a hybrid model called mlp mca which was set as the reference model the comparison indicated that the proposed hybrid model reduces the root mean square error by 18 and has a better performance zhang et al 2017 used a deep feedforward neural network to predict the distribution of soil moisture in a nation the raw data recorded for visible infrared imaging radiometer suite viirs continuously connected for four years were used as the input dataset they found that the proposed model can infer the nonlinear relationship between the remote sensing data and the local soil moisture distribution quantitatively the coefficient of determination was 0 9875 while the root mean square error was about 0 0084 which is superior when compared with the results obtained with the soil moisture active passive smap active radar soil moisture products aboutalebi et al 2019 used genetic programming gp to estimate soil moisture at different scales based on the multispectral image data with high resolution collected by an automated aircraft three variables the reflectance for the visible near infrared nir and thermal bands were used as the input variables to train a network they found that the predictions produced by the gp were comparable to those generated by the ann and svm whereas the master equation learned by the gp was applicable to other cases in different locations lee et al 2019 used a dl method to estimate soil moisture based on the remote sensing data the soil moisture estimated by the dl was more accurate with a high correlation coefficient of 0 89 a low root mean square error 3 825 and bias 0 039 fang et al 2019 used the lstm to forecast the variation tendency of the smap surface soil moisture product and obtained predictions with high accuracy they used one year smap level 3 passive data for the training and predicted the trend of the following year by the lstm they found that the lstm could expand the applicable range of the smap and can be as large as the entire continental u s and achieved high accuracy and less uncertainty at the same time although the number of training parameters for the lstm is usually very large the results demonstrated their superiority in time sequence prediction when compared with the traditional statistical methods the authors also mentioned that the lstm can only extract the soil moisture response patterns which are directly detected by the smap moreover the stochastic signal observed in the satellite images cannot be extended but is very useful for reducing errors fig 32 compares the time series for surface soil moisture at three core sites it is clear that the lstm accurately reproduces the patterns observed by the smap and that some systematic variance also exists as compared with the site data 3 2 6 carbon capture and storage carbon capture and storage ccs is a technique developed to reduce the effect of global climate change on human s life by gathering the carbon dioxide co2 in the air and storing them in the subsurface our daily life and companies in the industry still use fossil fuel as the major energy supply and discharge a large amount of co2 into the atmosphere which could be collected through a chemical reaction compressed and stored in an appropriate underground space xu et al 2019 some practical and significant problems still hinder the commercial development of large scale ccs for example co2 could leak out from the target storage formation through naturally existing fractures since the subsurface system is highly heterogeneous and numerical modeling requires a good understanding of the physics of the fluid flow accurately detecting that abnormal phenomena become very difficult recently efforts have been made to utilize the ml to address this issue lin et al 2017 applied the svr to forecasting the location and rate of leaked co2 based on limited pressure data points in particular the governing equations were incorporated into the algorithm to guide the training process similarly chen et al 2017 used a filter based approach to encompass measured data to monitor the cumulative co2 leakage rate and optimize the well locations specifically two ml algorithms the svr and multivariate adaptive regression splines were used to reduce the computational cost and derive models from the full physics based numerical models hubert and padovese 2019 proposed the use of acoustic signals for monitoring the presence of co2 leakage random forest and gradient boosted trees were utilized to analyze these signals furthermore a hidden markov model was used to smooth the predicted results by incorporating the known probability distribution of the leakage occurrences estimated from previous observations zhong et al 2019b developed a convolutional lstm convlstm to interpret the collected field pressure data and identify abnormal values the convolutional layers were used to incorporate both static and dynamic data specifically such data are represented as the three channels of a colored image see fig 33 a the injection rate for injection well f1 and bottom hole pressure for observation well f2 and f3 are dynamic data and formed as the first channel while two other channels contain the static data namely the effective reservoir porosity and permeability convlstm can overcome the main drawback of the lstm which is its inability to incorporating spatial information the proposed network received the concatenated image like data as the input and used the convlstm cells to extract useful spatial features from them see fig 33 b a dense layer was set as the output layer that provided the final predictions by linearizing the outcome from the previous layers based on linear regression the results indicated that the convlstm achieves comparable performance with vanilla lstm but it can also better interpret the change of magnitude and heterogeneity of the permeability field sparse data measurements and noisy data can however limit the model s performance and provide poor predictions in another paper zhong et al 2019a trained a surrogate model to forecast the movement of co2 plume with the reservoir heterogeneity considered see fig 34 they developed a conditional deep convolutional generative adversarial network cdc gan to estimate the relationship between two high dimensional fields namely the permeability and saturation of the co2 plume as conditioned to the model output time instead of using only a latent space vector z the conditional gan mirza and osindero 2014 is also conditioned to other known information so as to provide guidance for the training of the generator which improves the convergence of the gan significantly zhong et al 2019a conditioned the cdc gan to the time steps and used it as supplementary information to learn the corresponding relation between the inputs and outputs at each moment on the other hand a deep convolutional gan dc gan radford et al 2015 attempted to produce images based on latent vectors by training an autoencoder the encoder decoder structure combined with other deep learning techniques e g batch normalization layers enabled the network to appropriately down and up sample the images as well as making the training process more stably and smoothly compared with the predictions generated by a compositional reservoir simulator the proposed model accurately predicted the spatial distribution of co2 plume at each time step with high computational efficiency moreover the proposed method used the pattens extracted by the convolutional operation to make the prediction and therefore a limited understanding of the governing physics does not restrict its application wen et al 2019 developed a cdc gan to accurately predict the migration and saturation distributions of co2 plume in heterogeneous reservoirs the proposed model can not only interpolate between the training data ranges but also extrapolate to where no data are not available when extrapolating a transfer learning procedure was applied for extracting useful information from the new dataset instead of retraining the entire network from the beginning 4 conclusions and outlook the recent tremendous progress in the field of ai and ml is undeniable the advancement is indebted to the significant growth of computational power and the availability of advanced statistical methods which enable the ml methods to deal with more realistic problems the ml methods can be applied to large and complex datasets such as images signals multivariate data and so on due to the judicious application of ml our understanding of complex phenomena involving big data has shifted and improved significantly for example there are numerous examples in geosciences and porous media wherein the available data are very complex so much as that their interpretations require a lot of time without any guarantee that the hidden information can be identified we used to implement simple and human guided rules to interpret the earthquake signals and would make biased decisions with the aid of the ml and dl however valuable information can be decoded from such complex signals the field of porous media and more generally geoscience are now in the era of big data that may be way before other fields due to the necessity of collecting several datasets from various sources thus several spatial and spatio temporal methods have already been developed in these fields that are being used in other major research fields the recent ai techniques however have brought a commonality to all the fields through which they all can be connected and helped to make further progress the topic of this review paper is thus no exception we attempted to review most of the recently developed methods we believe that the following routes can be pursued to further develop the necessary ml methods related to porous media for both small and large scale ones recent dl methods should be applied for understanding complex data and taking advantage of other relevant data more physics informed ai methods should be developed because using the current methods from other fields for application to porous media and more generally geoscience may not consider the complex physics properly for example the governing equation for fluid flow and the physics of porous media cannot all be accounted for use with the advanced ml methods but they should be part of the computations thus the ml methods can be customized such that they not only discover the latent patterns in data but also reproduce the other aspects of the physics of the problem as well such a new ml algorithm will represent a combination of data driven and physics based modeling each of which completes the challenges in the other part for instance the ml methods can be developed to reduce the computational cost while the physics based method can add more reality to the ml workflows novel solutions for dealing with the issue of providing the necessary input data for the ml should be developed as some areas in porous media and geosciences do not provide rich enough data a scientific and physics based interpretation of the results obtained from the ml should be founded based on the experimental and computational methods the application of the ml can be enhanced by customizing the currently available ml methods for problems in porous media knowledge from big data and the ml should be combined in order to help with better integration of the two crucial fields benchmarking is an important missing ingredient in the current literature on the use of the ml algorithms in geosciences these fields require more meaningful comparisons between new ml methods in order to make systematic progress which allows the user to select the best method more confidently and with more ease boulesteix et al 2018 such data will be different from the existing ones as the inputs should all be translated in what ml often accepts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103619 appendix supplementary materials image application 1 
457,wettability is a pore scale property that has an important impact on capillarity residual trapping and hysteresis in porous media systems in many applications the wettability of the rock surface is assumed to be constant in time and uniform in space however many fluids are capable of altering the wettability of rock surfaces permanently and dynamically in time experiments have shown wettability alteration wa can significantly decrease capillarity in co2 storage applications for these systems the standard capillary pressure model that assumes static wettability is insufficient to describe the physics in this paper we develop a new dynamic capillary pressure model that takes into account changes in wettability at the pore level by adding a dynamic term to the standard capillary pressure function we assume a pore scale wa mechanism that follows a sorption based model that is dependent on exposure time to a wa agent this model is coupled with a bundle of tubes bot model to simulate time dependent wa induced capillary pressure data the resulting capillary pressure curves are then used to quantify the dynamic component of the capillary pressure function this study shows the importance of time dependent wettability for determining capillary pressure over timescales of months to years the impact of wettability has implications for experimental methodology as well as macroscale simulation of wettability altering fluids keywords wettability alteration dynamic capillary pressure dynamic wettability bundle of tubes simulation upscaling 1 introduction wettability plays an important role in many industrial applications in particular subsurface porous media applications such as enhanced oil recovery eor and co2 storage blunt 2001 bonn et al 2009 iglauer et al 2014 2016 yu et al 2008 the wetting property of a given multiphase system in porous media is defined by the distribution of contact angles the contact angle ca is controlled by surface chemistry and associated forces acting at the molecular scale along the fluid fluid solid interface bonn et al 2009 in porous media applications microscale wettability determines the strength of pore scale capillary forces and the movement of fluid interfaces between individual pores at the core scale wettability impacts upscaled quantities and constitutive functions such as residual saturation relative permeability and capillary pressure which in turn affect field scale multiphase flow behavior the standard assumption is that wettability is a static property of the multiphase system however the composition of many fluids is capable of provoking the surfaces within pores to undergo wettability alteration wa via a change in ca ca change can alter capillary forces at the pore scale and thus affect residual saturations of the system ahmed and patzek 2003 blunt 1997 this effect has been exploited extensively in the petroleum industry where optimal wetting conditions in the reservoir are obtained through a variety of means that includes chemical treatment foams surfactants and low salinity water flooding see for example morrow et al 1986 buckley et al 1988 jadhunandan and morrow 1995 haagh et al 2017 singh and mohanty 2016 wettability is also recognized as a critical factor in geological co2 sequestration which exerts an important role on caprock performance kim et al 2012 tokunaga and wan 2013 the sealing potential of the caprock is highly dependent on co2 being a strongly non wetting fluid and wa may lead to conditions that allow for buoyant co2 to leak chiquet et al 2007a chalbaud et al 2009 besides wa can affect residual saturation and subsequently impact the trapping efficiency of injected co2 iglauer et al 2014 therefore reliable quantification of wettability is needed for safe and effective co2 storage despite the fact that wa is known to impact core scale capillarity and relative permeability behavior few detailed measurements are available to characterize the alteration of the constitutive function themselves plug and bruining 2007 have reported brine co2 gas liquid drainage imbibition experiment and showed capillary instability for a supercritical co2 brine system meaning that the capillary pressure measurements change steadily over time the imbibition curve also exhibited a significant deviation from the expected curve that was not explained by classical scaling arguments the authors proposed wa as an explanation but did not explicitly measure any changes in ca additionally recent experiments measured capillary pressure curves for a silicate sample using a fluid pairing of supercritical co2 and brine wang and tokunaga 2015 repeated drainage imbibition cycles were performed for 6 months and a clear reduction in capillary pressure was recorded for each subsequent drainage cycle the authors also attributed these deviations from the expected capillary curve to a change in wettability of the rock sample over time due to co2 exposure similar to aging this hypothesis was confirmed through observations of a wetting angle increase from 0 to 75 after 6 months of exposure it is also reported similar p c s instability and deviation in dolomite carbonate wang et al 2016 and quartz tokunaga et al 2013 wang et al 2016 sands for scco2 brine system more literature on wa and p c s measurements can be found in tokunaga and wan 2013 the above experiments reveal that capillary pressure curves are not static for rocks that undergo wa despite the fact they were performed following the standard multi step procedure i e where equilibrium is obtained after each incremental step in pressure therefore the standard capillary pressure models cannot be readily applied without additional dynamics to capture the long term impact of wa capillary pressure dynamics due to wa are distinct from non equilibrium flow dynamics e g hassanizadeh et al 2002 dahle et al 2005 barenblatt et al 2003 or from ca hysteresis generated by receding and advancing angles e g krumpfer and mccarthy 2010 eral et al 2013 wa alteration is a chemistry induced pore scale phenomenon that alters the capillary pressure function separately from the flow conditions or other instabilities that is contact angle has the potential to change even when the system is at rest on the other hand ne models are formulated to address dynamics in only for systems that are flowing and therefore a new approach is needed to account for permanent and continual alteration of capillary pressure functions for both flowing and non flowing systems we note that capillary driven flow may initiate if the ca change is large the standard approach to wa is to assume a change in surface chemistry that occurs instantaneously which results in an immediate shift in saturation functions delshad et al 2009 lashgari et al 2016 yu et al 2008 andersen et al 2015 adibhatia et al 2005 this implementation entails a heuristic approach that interpolates between the two end wetting states as a linear function of chemical agent concentration lashgari et al 2016 have derived an instantaneous wa model from gibbs energy and adsorption isotherms the proposed wa model is coupled with p c s relation through residual saturation these models neglect the impact of wa over longer timescales months to years they also do not capture pore scale heterogeneity in wetting properties in the available literature only one study al mutairi et al 2012 has included the effect of exposure time on wa and constitutive relations for core scale simulation but this numerical study does not sufficiently incorporate or upscale pore scale processes to core scale constitutive laws to our knowledge a rigorous mathematical upscaling of long term dynamics in p c s functions introduced by exposure to a wa agent has not been previously performed the focus of this paper is to propose a new dynamic capillary pressure model by upscaling the wa dynamics from the pore to the core scale section 2 describes our approach we start with direct simulations of p c s curves from a pore scale model represented by a cylindrical bundle of tubes wa is introduced at the pore scale using a mechanistic model for ca change as a function of exposure time to a reactive agent this model is developed based on the insights from laboratory experiments giving the flexibility to incorporate other data as appropriate we emphasize the ca model is only meant as a basis on which to demonstrate the upscaling approach in section 3 we present the resulting curves generated by the pore scale model using two different pore scale models for ca change these curves are then used to correlate the dynamic term in the upscaled p c s function finally we analyze the link between pore scale parameters and upscaled correlations 2 approach the extended p c s relationship introduces a dynamic component that captures the changing wettability as measured by the deviation of the dynamic capillary pressure from the equilibrium static capillary pressure this relationship can be described as follows 1 p c p c st in f dyn where p c st in represents the capillary pressure for the system given a static initial wetting state and f dyn represents the deviation from the static state the initial static curve can be described by the brooks corey model 2 p c st in c w s w s w c 1 s w c a w where cw is the entry pressure 1 aw is the pore size distribution index whereas swc is the residual water saturation the objective of this study is to characterize and quantify the dynamic term f dyn the key term of interest in the p c s model for a system that undergoes wa we propose an interpolation model to handle the wa dynamics in p c s relation to obtain an interpolation model the dynamic component in eq 1 can be scaled by the difference between two static curves each representing the initial and final wetting state capillary pressure curves to give a non dimensional quantity ω we call the dynamic coefficient which is defined as follows 3 ω p c st f p c st in f dyn where p c st f is the final wetting state capillary pressure in the previous studies e g see delshad et al 2009 lashgari et al 2016 yu et al 2008 andersen et al 2015 adibhatia et al 2005 the coefficient ω is assumed only chemistry dependent here ω in eq 3 is assumed to be a function of not only the chemistry but also the exposure time to the wa agent the expression in eq 3 can be substituted into eq 1 to obtain a dynamic interpolation model 4 p c 1 ω p c st in ω p c st f we note that in the model presented above we have defined the total capillary pressure pc as simply the measured difference in phase pressures at any point in time in a reservoir simulation this would be capillary pressure in a given grid cell whereas in a laboratory experiment designed to measure p c s data it corresponds to the pressure drop across the sample at equilibrium for quasi static displacement in a bundle of capillary tubes pc is the difference between boundary condition pressures i e p c p l r e s p r r e s see fig 1 the exact nature of ω and its functional dependencies can only be determined from a full characterization of p c s curves under different conditions these curves can be derived from laboratory experiments but this approach is costly and time consuming alternatively one may take a more theoretical approach by simulating p c s curves using a pore scale model that includes the impact of wa for a system that undergoes wa a significant change in ca could lead to the wetting phase becoming non wetting and vice versa for clarity we will continue to use w subscript for the phase that was originally wetting and nw for the phase that is originally non wetting regardless of the actual state of wettability in the system 2 1 pore scale model there are various choices of pore scale models available the easiest to implement and analyze is the bundle of tubes bot model which is a collection of capillary tubes with a distribution of radii herein we describe the model and the approach for the implementation of time dependent wa the bot model is a popular approach due to the simplicity of implementation and the ability to study the balance of energy and forces directly at the scale of interfaces bartley and ruth 1999 dahle et al 2005 helland and skjæveland 2006 the average behavior of a bot model can then be used to construct better constitutive functions at the macroscale dahle et al 2005 helland and skjæveland 2006 2007 there are known limitations to this pore scale approach i e lack of residual saturation and tortuosity we emphasize that this study is a first step at studying the impact of long term wa from pore to core scale and as such these secondary aspects are beyond the scope of this work in this paper we consider cylindrical bot having length l and is shown in fig 1 these tubes are designed to connect wetting right and non wetting left reservoirs with pressures labeled as p r res and p l res respectively let the reservoir pressures difference be defined as 5 δ p p l r e s p r r e s initially the tubes in the bundle are filled with a wetting phase to displace the wetting phase fluid in the m th tube the pressure drop has to exceed the local entry pressure p c m defined as dahle et al 2005 6 δ p p c m r m θ m where p c m rm θm is given by the young s equation 7 p c m r m θ m 2 σ cos θ m r m m 1 2 n where rm and θm are the tube radius and ca respectively for the m th tube n stands for number of tubes σ is fluid fluid interfacial tension as long as condition 6 is satisfied the fluid movement across the length of tube m can be approximated by the lucas washburn flow model washburn 1921 8 q m r m 2 δ p p c m r m θ m 8 μ nw x m i n t μ w l x m i n t where μ nw and μ w are non wetting and wetting fluid viscosities respectively the superscript int stands for fluid fluid interface and q m d x m i n t d t is the interface velocity the interface is assumed to be trapped when it reached at the outlet of the tube thus q m 0 a positive rate of change in x m i n t is associated with an increase in non wetting saturation for tube m from eq 8 one can then determine the required time to reach a specified interface position 2 2 pore scale time dependent wa model in this paper we consider a wa mechanism at the pore scale that evolves smoothly from an initial to final wetting state through exposure time the initial and final wetting states can be arbitrarily chosen i e from wetting to non wetting or vice versa the wa agent is defined as either the non wetting fluid itself or some reactive component therein we consider an alteration process within any given pore or tube that continues until the ultimate wetting state is reached locally in the pore the alteration is permanent but can also be halted at some intermediate wettability state if the wa agent is displaced from the pore if the agent is reintroduced to the pore at some later point alteration continues until the final state is reached to this end we introduce a general functional form of pore scale wa mechanism by ca change 9 θ m θ m in φ δ θ where δ θ θ m f θ m in θ m f and θ m in are the ultimate and initial contact angles respectively in eq 9 θm decreases and increases based on the choice of the initial and final wetting conditions the term φ 0 1 when φ 1 the ca attains its ultimate value and ca is fixed at the initial state when φ 0 in eq 9 is responsible for governing the wa dynamics wa involves complex physical and chemical processes whose description is beyond the scope of this work however we provide a brief summary of the role of adsorption desorption processes in ca change blut 2017 du et al 2019 such a hypothesis has been supported by experiment measurements for instance co2 water core flooding experiments show adsorption type relations between ca and pressure dickson et al 2006 jung and wan 2012 iglauer et al 2012 and also with exposure time jafari and jung 2016 saraji et al 2013 similar ca evolutions as a function of surfactant concentration and exposure time are reported in davis et al 2003 and morton et al 2005 for an oil droplet on a metal surface immersed in ionic surfactant solutions in morton et al 2004 a langmuir adsorption model is proposed to predict the experiment observations in davis et al 2003 given the insights above we consider a ca model that evolves according to the rate of adsorption of the wa agent on the surface of the pores following mckee 1991 van erp et al 2014 the dynamic parameter φ in eq 9 can be stated as 10 d φ d t j j where j and j represent rates of adsorption and desorption of a wa agent respectively at the solid surface in mckee 1991 j is taken to be proportional to the wa agent and the surface unoccupied by the adsorbed wa agent i e 11 j k 1 χ m 1 φ φ where k 1 is a rate constant φ represents the maximum surface saturated concentration and χm is a measure of the local exposure time of tube m the desorption rate can be related with the current surface concentration and is defined as 12 j k 2 φ where k 2 is a rate constant for desorption rate combining eqs 10 12 would give us 13 d φ d t k 1 χ m 1 φ φ k 2 φ assuming φ 1 and following mckee 1991 one can apply a perturbation analysis to eq 13 to obtain a first order approximation for φ in terms of χm 14 φ χ m c χ m where c k 2 k 1 is a parameter that controls the speed and extent of alteration χm is defined as the time integration of exposure to a wa agent here taken to be the local non wetting saturation of tube m 15 χ m 1 t 0 t x m i n t l d τ where t is a pre specified characteristic time in this paper the characteristic time is set as the time for one complete drainage displacement under static initial wetting conditions which can be pre computed according to eq 8 as an aside detailed laboratory work would be needed to further enrich the pore scale ca model by fitting c to experimental data this exercise is beyond the scope of this paper and we consider the underlying ca change in eq 14 a reasonable basis for which to perform the upscaling aspect of our study herein we consider two models for exposure time at the local scale for first we take the case where ca modification based on eq 14 is strictly dependent on local exposure time χm and leads to ca variation from tube to tube hereafter referred to as the non uniform wa mechanism we note that wettability gradients within individual tubes are not considered in this model and thus there is no variation in ca along the tube this is due to the flow model in eq 8 where the ca only affects the entry pressure of the tube the second is referred to as uniform wa which is based on the assumption that the wa agent dissolves into the wetting phase from the non wetting fluid and affects all tubes simultaneously in this case all tubes have the same properties that are governed by the bulk exposure time across the entire bundle i e rev we can define χ 16 χ 1 t 0 t s n w d τ as the bulk or average exposure time as a function of average saturation the uniform model is implemented into eq 14 by taking χ m χ in summary we introduce two types of wa mechanisms uniform and non uniform that serve as end members of possible wa mechanisms at the pore scale on the one end non uniform wa restricts alteration to only drained pores and excludes any interaction of the wa agent between pores this leads to significant heterogeneity in ca from one pore to another at the other end the uniform case assumes the wa agent can alter all pores simultaneously in reality wa will lie somewhere in between but we have chosen simpler end members to aid in further analysis of simulated data in the next section 2 3 simulation approach the uniform and non uniform approaches are coupled into the bot model following algorithm 1 for a single drainage imbibition cycle the objective is to perform simulated experiments that mimic laboratory derived capillary pressure curves i e the pressure is adjusted incrementally up or down after each step depending on if the bundle is under drainage or imbibition respectively contact angles are updated continuously throughout the flow processes in a step wise manner once the displacement is completed for each pressure increment that is contact angles that have been altered according to the uniform or non uniform mechanism are updated before the next pressure increment this is a reasonable approximation given that it is only entry pressures in individual tubes that are affected by ca change we control the pressure drop δp in the way that the wa process is completed within a few numbers of drainage imbibition cycles in the first drainage imbibition cycles the δp increment is such that tubes drain imbibe one at a time with a pressure drop close to the next tube entry pressure in the last drainage imbibition cycle every δp increment is reduced by two and three orders of magnitude for the non uniform and uniform case respectively consequently the flow slows down by the same magnitude irrespective of whether the tube drains or imbibes at the completion of the numerical experiment we obtain a set of p c s data points that can be plotted in the usual way once the capillary pressure curves are generated for both the uniform and non uniform approaches the resulting curves are used to quantify the dynamic coefficient in the interpolation function in eq 4 the goal is to develop a correlation model that involves only a single parameter and this parameter should have a clear relation with changes in the pore scale wa model parameter c 3 results in this section we present the simulated capillary pressure and associated results for each wa case we formulate a correlation model which is then fit to the simulated data finally we investigate the sensitivity of the correlated model to the pore scale wa parameter 3 1 bundle of tubes model set up the pore scale is described by a bot model see section 2 1 each tube in the bot is assigned a different radius r with the radii drawn from a truncated two parameter weibull distribution helland and skjæveland 2006 17 f r r r min r av η 1 η r av exp r r min r av η 1 exp r max r min r av η where r max r min and r av are the pore radii of the largest smallest and average pore sizes respectively and η is a dimensionless parameter the average is obtained by the mean of r max and r min the rock parameters and fluid properties are listed in table 1 3 2 static capillary pressure for end wetting states a starting point for the dynamic capillary pressure models presented in section 2 eqs 1 and 4 is characterizing the capillary pressure curves for the end wetting states given the same tube geometry and fluid pairing described above the capillary pressure saturation data are simulated under static conditions for both the initial and final wetting states only a single drainage experiment is needed in the static case to fully characterize the capillary pressure curve this is due to the lack of residual trapping in a bot model we emphasize that hysteresis is not possible for a bot if the contact angles in the tubes and other parameters are held constant the simulated static curves are then correlated with the brooks corey model 2 the resulting correlations can be found in fig 2 while fitted parameters for the brooks corey model can be found in table 2 note that the brooks corey model is undefined at zero irreducible wetting phase saturation thus we left a few pores undrained to allow for comparison between the brooks corey model and the simulated p c s data fig 2 compares the brooks corey formula 2 and the capillary pressure curves associated with static contact angles initial and final wetting states the brooks corey correlation gives an excellent match to the simulated p c s data under static conditions we observe that the pore size distribution index aw for the initial and final wetting states are the same which is expected since the same distribution of tube radii is used in both cases on the other hand the coefficient cw decreases by a factor of 0 85 from the initial to the final wetting state corresponding to a decrease in a core scale capillary entry pressure the leverett j scaling theory xu et al 2016 predicts that entry pressure scales by cos θ which agrees nicely with the reduction in cos θ by a factor of 0 83 for a ca change from 0 to 80 degrees we reiterate that for the static case where no wa occurs the brooks corey model describes both drainage and imbibition for the bot 3 3 simulated capillary pressure data we present the simulated capillary pressure data see fig 3 comparing the results of the uniform and non uniform approaches described previously the uniform data are generated with the pore scale wa parameter c 0 005 while for the non uniform data c 5 10 4 a total of two and four drainage imbibition cycles carried out for the uniform and non uniform cases respectively for both cases we observe a steady decrease in capillary pressure over time in the end a complete wettability change has evolved from the initial to final prescribed states whose static curves are plotted in fig 3 for reference having reached the final wetting state any additional drainage imbibition cycle would follow along the static curve for the final wetting state we remark that wettability induced dynamics also introduces an apparent hysteresis in the p c s data this effect is unique to the cylindrical bot model which we recall cannot exhibit hysteresis under static wettability conditions however a real porous medium may exhibit capillary pressure hysteresis with static wettability there are notable differences between the two sets of curves for the uniform case fig 3a there are distinct curves for each drainage and imbibition displacement the capillary pressure begins to decrease immediately and in a continuous manner over time this is because the ca fig 4a is changing for all tubes simultaneously based on the average exposure time over the entire bundle the uniformity results in the ca in smaller tubes being altered significantly at an early time at the same rate as the larger tubes and thus the capillary pressure is decreased even at low average wetting saturation in the first drainage curve the fast dynamics in ca change lead to a non monotone capillary curve at an early time in comparison the non uniform case fig 3b has a delay in exhibiting the effects of wa the initial drainage curve is identical to the initial wetting static curve and all subsequent drainage curves follow along the previous imbibition curve this is a result of the restriction on ca change to only tubes that are drained in other words at the tube level there is no change in entry pressure from the initial state or the state after a single drainage imbibition cycle until that tube is drained in contrast to the uniform case the capillary pressure at low sw is drawn towards the initial state this can be described by examining the ca per tube radius in time shown in fig 4b larger tubes that drain first and imbibe last resulting in longer local exposure time and thus more extensive ca change compared to the smaller tubes that drain last and imbibe first therefore the initial wetting state persists in the smaller tubes we recall that both the uniform and non uniform cases are selected as end members of possible wa mechanisms in real porous media in real systems wa in different sized pores may occur in a more complex manner we have observed above that capillary pressure curves in fig 3a and b are not a unique function of saturation that is they exhibit hysteresis for this simple bot geometry we note that the p c s data points are color coded according to time evolved at each data point this motivates a transformation of the data into the time domain by plotting against χ as shown in fig 5 for both cases in doing so we obtain a unique function with respect to exposure time for both the uniform and non uniform cases we note that the curves in fig 5 show that the capillary pressure increases and decreases with each drainage imbibition cycle in addition the transformation reveals the separate drainage curves for the non uniform case that were hidden in fig 3b 3 4 dynamic capillary pressure model development following the approach discussed in section 2 we applied eq 3 to calculate the dynamic coefficient ω for both the uniform and non uniform cases the resulting coefficient is plotted in fig 6 as a function of both sw top panels and χ bottom panels for both wa cases we recall that ω is a coefficient that interpolates between the capillary pressure at the initial and final wetting states at any given saturation where ω 0 gives the initial capillary pressure and ω 1 gives the final p c s curve for the uniform case ω is a non unique function of wetting phase saturation see fig 6a but with values that are continuously increasing as the dynamic capillary pressure moves towards the final wetting state for the non uniform case the dynamic coefficient in fig 6b also exhibits non uniqueness with respect to saturation reflecting the p c s data the capillary pressure persists at the initial state at low saturation this means that the value of ω decreases with decreasing sw along the drainage path and increases only along imbibition paths the complex relation of ω in saturation space makes it challenging to propose a functional form for ω s w relation in both cases figs 6c and 6d show that ω exhibits different behavior as a function of average exposure time for the uniform case fig 6c ω is smoothly increasing and uniquely related to χ mimicking the functional form of the pore scale model in eq 9 on the other hand the coefficient ω in the non uniform case fig 6d is not monotonically increasing in χ but continues to rise and fall with time despite the transformation to the temporal domain the curves in fig 6 give us important insight into the form of ω best suited to each wa case we take each case in turn 3 4 1 uniform case the smoothly varying functionality of ω and χ in fig 6c motivates an adsorption type model 18 ω χ β 1 χ where β 1 is a fitting parameter obtained from the best fit to the simulated data in fig 6c for this particular case the calibrated parameter is estimated to be β 1 0 01 the form of the dynamic p c s model for the uniform case is obtained by substituting eq 18 into eq 4 to give 19 p c χ β 1 χ p c st f p c st in p c st in 3 4 2 non uniform case the non trivial behavior of ω in fig 6d makes it challenging to propose a functional relation between the dynamic coefficient ω and χ directly as we did for the uniform wa case instead we observe that ω in fig 6b has a well behaved curvature for each drainage imbibition cycle along the saturation history further the curvature of each cycle is increasing with increasing exposure time given these insights we proposed a model for the dynamic coefficient that has the following form 20 ω s w χ s w α χ s w where α controls the curvature of the ω s w curve for each drainage imbibition cycle since ω is increasing function of exposure time α should decrease along the averaged variable χ the function form of ω in eq 20 is then matched with the ω s w data to analyze the dynamics of α along χ the obtained α χ relation is decreasing as hypothesized and in particular has the follwing form 21 α χ β 2 χ where β 2 is non dimensional fitting parameter for this particular simulation the parameter β 2 is estimated to be 0 004 for the four of drainage imbibition cycles the form of the dynamic p c s model for the non uniform case is then obtained by substituting eqs 20 and 21 into eq 4 22 p c χ s w β 2 χ s w p c st f p c st in p c st in the calibrated dynamic capillary pressure models in eqs 19 and 22 are compared with the simulated capillary pressure data in fig 3 with the results presented in fig 7 for each wa case we observe that the proposed dynamic models agree well with simulated dynamic capillary pressure curves the correlation coefficient for this comparison is r 2 0 9921 and r 2 0 98 for the uniform and non uniform case respectively thus we have obtained a single parameter model in both the uniform and non uniform wa cases that describe the evolution of dynamic capillarity over multiple drainage imbibition cycles rather than using a model consisting of multiple parameters that change with each cycle or hysteresis models 3 5 model sensitivity to pore scale model parameter we hypothesize that parameters β 1 in eq 19 and β 2 in eq 22 are dependent on the parameter c in eq 9 that controls the dynamics of wettability alteration at the pore scale we investigate this sensitivity by repeating the capillary pressure simulations for different values of the pore scale parameter c and determine the correlated value of β 1 and β 2 in each case for the uniform wa case fig 8 a shows that the interpolation model parameter is linearly proportional to the pore scale model parameter with a proportionality constant of 2 thus the relationship β 1 2 c can be used to predict the upscaled parameter directly from knowledge of the pore scale process in contrast the non uniform case in fig 8b shows a power law model where β 2 b 1 c b 2 is correlated with estimated parameters of b 1 3 3 10 6 and b 2 1 8 the general form of dynamic capillary pressure model can now be obtained for the uniform wa by incorporating the relationship for β 1 in eq 19 23 p c χ 2 c χ p c st f p c st in p c st in similarly we obtain a general non uniform model by substituting β 2 in eq 22 24 p c χ s w b 1 c b 2 χ s w p c st f p c st in p c st in in their final form the dynamic capillary pressure models in eqs 23 and 23 are dependent on two variables saturation and time and a single wettability parameter c the latter must be determined by fitting eq 9 with parameter c to laboratory experiments for a given sample exposed to a wa agent 3 6 applicability to arbitrary saturation history we note that the saturation history used to generate the p c s curves for the two wa cases in figs 3a and 3c can be thought of in each case as a single arbitrary path within an infinite number of possible paths if a different path had been chosen such as a flow reversal at intermediate saturation or a prolonged exposure time at a given saturation it would result in entirely different capillary pressure dynamics in order to test the dynamic models developed in eqs 19 and 22 for any arbitrary saturation history we generate many different p c s curves by taking numerous different paths in the saturation time domain the resulting simulated data forms a surface with respect to saturation and exposure time as shown in fig 9 a and b we then apply the calibrated dynamic models to the same saturation time paths used to generate the p c s χ surface the difference between the calibrated model and the simulated data is shown in fig 9c and d for the uniform and non uniform cases respectively a good comparison of the dynamic models to simulated data demonstrates that model calibration to a single saturation time path is robust enough to be applied to any possible path 3 7 discussion we investigated the potential of the interpolation based model to predict the wa induced dynamics in capillary pressure saturation relations in the interest of completeness we also explored other types of models to capture capillary pressure dynamics including the mixed wet model of skjæveland et al 2000 for brevity we do not report the results of that separate study herein we found that although other models could be calibrated with reasonable accuracy they all involved more than one calibration parameter up to four that need to be adjusted in each drainage imbibition cycle therefore the single parameter single valued interpolation model presented in this study is the preferred model due to its reliability for replicating the simulated bot data the proposed interpolation model is an upscaled model that allows for a change in capillary pressure as a function of upscaled variables saturation and exposure time to a wa agent we recall that exposure time is simply the integration of saturation history over time the model consists of three main components two capillary pressure functions at the initial and final wetting state and a dynamic interpolation coefficient that moves from one state to the other the initial and final capillary pressure functions can be determined a priori from static experiments using inert fluids in this study the initial and final states are represented by classical brooks corey functions the dynamic coefficient is thus the only variable correlated to dynamic capillary pressure simulations in this study we have shown that the coefficient can be easily correlated to saturation and exposure time via a single parameter we have observed that the form of the dynamic term is dependent on the underlying mechanisms for wa we employed two models uniform and non uniform that represent two end members of real systems one end member is identical ca throughout the rev while the other results in severely heterogeneous ca from small to large pores the differences in the two wa mechanisms changes the complexity of the resulting capillary dynamics in the uniform case the dynamic coefficient can be correlated to exposure time through a sorption type model which seems to be a natural result given the ca change at the pore scale is also based on a sorption model this is an interesting observation that requires more analysis in future work in the non uniform case the dynamic coefficient has no similar sorption form with increased exposure time but now with the product of saturation and exposure time as the dynamic variable the additional complexity is needed to draw the capillary pressure curve back to the initial wetting at low saturation a region of the p c s curve dominated by smaller pores where the ca takes longer to change an important result of this study is quantifying the link between the pore and core scale we showed that by varying the parameter that alters the speed and extent of ca change in each individual pore we could predict the resulting impact on dynamic capillary pressure in fact in both the uniform and non uniform cases there is a very simple scaling from the pore scale and macroscale parameters in the uniform case the two parameters are directly proportional while in the non uniform case the macroscale parameter scales with the pore scale parameter via a power law the implication of this result is that by knowing the mechanism that controls ca at the pore scale which can be obtained by a relatively simple batch experiment we can quantify a priori the macroscale dynamics without having to perform pore scale simulations this is an important generalization and valuable for making use of experimental data to inform macroscale constitutive functions we have quantified the ability of the interpolation model to capture underlying wa at the pore scale for a simple bot this result is a natural development from previous studies that incorporate the interpolation model directly into reservoir simulation of wettability alteration in those studies the model was matched directly to core scale data in a heuristic manner the contribution of this study is to quantify the pore scale underpinnings to the interpolation model through a direct and systematic manner thus providing additional evidence to the validity of this type of model for use in macroscale simulation despite the satisfactory and relatively straightforward correlation of the interpolation model to simulated p c s data the exact quantification of the dynamics is ultimately restricted by the simplicity of the bot model we chose this simple approach in order to isolate the wa process from other complexities associated with real pore networks and thus begin to assess the mechanisms linking the pore and core scales though beyond the scope of the current work further study is needed to determine whether the fundamental nature of dynamic behavior we observe will hold when additional complexity is added further advancements can be made by adding complexity to the porous media i e converging diverging throat diameters or tortuosity or to the pore scale wa model theoretical work using pore network models should also be combined with laboratory investigations to further calibrate the underlying wa mechanisms 4 conclusions in this paper we designed a framework to upscale the impact of time dependent wa mechanisms at the pore scale on the dynamics in capillary pressure saturation functions at the darcy scale we found that an interpolation based dynamic model can predict the change in capillary pressure due to underlying wa the form of the dynamic interpolation coefficient is dependent on exposure time to a chemical agent with a different mathematical form depending on the pore scale wa mechanism the correlated dynamic capillary pressure model shows an excellent match with simulated pc s data and reliably predicts capillary dynamics independent of the saturation time path more importantly the model relies on a single interpolation parameter that has a clear and simple relationship with the pore scale wa parameter credit authorship contribution statement abay molla kassa conceptualization data curation writing original draft writing review editing sarah eileen gasda conceptualization data curation writing original draft writing review editing kundan kumar writing review editing florin adrian radu writing review editing acknowledgement funding for this study was through the chi project n 255510 granted through the climit program of the research council of norway 
457,wettability is a pore scale property that has an important impact on capillarity residual trapping and hysteresis in porous media systems in many applications the wettability of the rock surface is assumed to be constant in time and uniform in space however many fluids are capable of altering the wettability of rock surfaces permanently and dynamically in time experiments have shown wettability alteration wa can significantly decrease capillarity in co2 storage applications for these systems the standard capillary pressure model that assumes static wettability is insufficient to describe the physics in this paper we develop a new dynamic capillary pressure model that takes into account changes in wettability at the pore level by adding a dynamic term to the standard capillary pressure function we assume a pore scale wa mechanism that follows a sorption based model that is dependent on exposure time to a wa agent this model is coupled with a bundle of tubes bot model to simulate time dependent wa induced capillary pressure data the resulting capillary pressure curves are then used to quantify the dynamic component of the capillary pressure function this study shows the importance of time dependent wettability for determining capillary pressure over timescales of months to years the impact of wettability has implications for experimental methodology as well as macroscale simulation of wettability altering fluids keywords wettability alteration dynamic capillary pressure dynamic wettability bundle of tubes simulation upscaling 1 introduction wettability plays an important role in many industrial applications in particular subsurface porous media applications such as enhanced oil recovery eor and co2 storage blunt 2001 bonn et al 2009 iglauer et al 2014 2016 yu et al 2008 the wetting property of a given multiphase system in porous media is defined by the distribution of contact angles the contact angle ca is controlled by surface chemistry and associated forces acting at the molecular scale along the fluid fluid solid interface bonn et al 2009 in porous media applications microscale wettability determines the strength of pore scale capillary forces and the movement of fluid interfaces between individual pores at the core scale wettability impacts upscaled quantities and constitutive functions such as residual saturation relative permeability and capillary pressure which in turn affect field scale multiphase flow behavior the standard assumption is that wettability is a static property of the multiphase system however the composition of many fluids is capable of provoking the surfaces within pores to undergo wettability alteration wa via a change in ca ca change can alter capillary forces at the pore scale and thus affect residual saturations of the system ahmed and patzek 2003 blunt 1997 this effect has been exploited extensively in the petroleum industry where optimal wetting conditions in the reservoir are obtained through a variety of means that includes chemical treatment foams surfactants and low salinity water flooding see for example morrow et al 1986 buckley et al 1988 jadhunandan and morrow 1995 haagh et al 2017 singh and mohanty 2016 wettability is also recognized as a critical factor in geological co2 sequestration which exerts an important role on caprock performance kim et al 2012 tokunaga and wan 2013 the sealing potential of the caprock is highly dependent on co2 being a strongly non wetting fluid and wa may lead to conditions that allow for buoyant co2 to leak chiquet et al 2007a chalbaud et al 2009 besides wa can affect residual saturation and subsequently impact the trapping efficiency of injected co2 iglauer et al 2014 therefore reliable quantification of wettability is needed for safe and effective co2 storage despite the fact that wa is known to impact core scale capillarity and relative permeability behavior few detailed measurements are available to characterize the alteration of the constitutive function themselves plug and bruining 2007 have reported brine co2 gas liquid drainage imbibition experiment and showed capillary instability for a supercritical co2 brine system meaning that the capillary pressure measurements change steadily over time the imbibition curve also exhibited a significant deviation from the expected curve that was not explained by classical scaling arguments the authors proposed wa as an explanation but did not explicitly measure any changes in ca additionally recent experiments measured capillary pressure curves for a silicate sample using a fluid pairing of supercritical co2 and brine wang and tokunaga 2015 repeated drainage imbibition cycles were performed for 6 months and a clear reduction in capillary pressure was recorded for each subsequent drainage cycle the authors also attributed these deviations from the expected capillary curve to a change in wettability of the rock sample over time due to co2 exposure similar to aging this hypothesis was confirmed through observations of a wetting angle increase from 0 to 75 after 6 months of exposure it is also reported similar p c s instability and deviation in dolomite carbonate wang et al 2016 and quartz tokunaga et al 2013 wang et al 2016 sands for scco2 brine system more literature on wa and p c s measurements can be found in tokunaga and wan 2013 the above experiments reveal that capillary pressure curves are not static for rocks that undergo wa despite the fact they were performed following the standard multi step procedure i e where equilibrium is obtained after each incremental step in pressure therefore the standard capillary pressure models cannot be readily applied without additional dynamics to capture the long term impact of wa capillary pressure dynamics due to wa are distinct from non equilibrium flow dynamics e g hassanizadeh et al 2002 dahle et al 2005 barenblatt et al 2003 or from ca hysteresis generated by receding and advancing angles e g krumpfer and mccarthy 2010 eral et al 2013 wa alteration is a chemistry induced pore scale phenomenon that alters the capillary pressure function separately from the flow conditions or other instabilities that is contact angle has the potential to change even when the system is at rest on the other hand ne models are formulated to address dynamics in only for systems that are flowing and therefore a new approach is needed to account for permanent and continual alteration of capillary pressure functions for both flowing and non flowing systems we note that capillary driven flow may initiate if the ca change is large the standard approach to wa is to assume a change in surface chemistry that occurs instantaneously which results in an immediate shift in saturation functions delshad et al 2009 lashgari et al 2016 yu et al 2008 andersen et al 2015 adibhatia et al 2005 this implementation entails a heuristic approach that interpolates between the two end wetting states as a linear function of chemical agent concentration lashgari et al 2016 have derived an instantaneous wa model from gibbs energy and adsorption isotherms the proposed wa model is coupled with p c s relation through residual saturation these models neglect the impact of wa over longer timescales months to years they also do not capture pore scale heterogeneity in wetting properties in the available literature only one study al mutairi et al 2012 has included the effect of exposure time on wa and constitutive relations for core scale simulation but this numerical study does not sufficiently incorporate or upscale pore scale processes to core scale constitutive laws to our knowledge a rigorous mathematical upscaling of long term dynamics in p c s functions introduced by exposure to a wa agent has not been previously performed the focus of this paper is to propose a new dynamic capillary pressure model by upscaling the wa dynamics from the pore to the core scale section 2 describes our approach we start with direct simulations of p c s curves from a pore scale model represented by a cylindrical bundle of tubes wa is introduced at the pore scale using a mechanistic model for ca change as a function of exposure time to a reactive agent this model is developed based on the insights from laboratory experiments giving the flexibility to incorporate other data as appropriate we emphasize the ca model is only meant as a basis on which to demonstrate the upscaling approach in section 3 we present the resulting curves generated by the pore scale model using two different pore scale models for ca change these curves are then used to correlate the dynamic term in the upscaled p c s function finally we analyze the link between pore scale parameters and upscaled correlations 2 approach the extended p c s relationship introduces a dynamic component that captures the changing wettability as measured by the deviation of the dynamic capillary pressure from the equilibrium static capillary pressure this relationship can be described as follows 1 p c p c st in f dyn where p c st in represents the capillary pressure for the system given a static initial wetting state and f dyn represents the deviation from the static state the initial static curve can be described by the brooks corey model 2 p c st in c w s w s w c 1 s w c a w where cw is the entry pressure 1 aw is the pore size distribution index whereas swc is the residual water saturation the objective of this study is to characterize and quantify the dynamic term f dyn the key term of interest in the p c s model for a system that undergoes wa we propose an interpolation model to handle the wa dynamics in p c s relation to obtain an interpolation model the dynamic component in eq 1 can be scaled by the difference between two static curves each representing the initial and final wetting state capillary pressure curves to give a non dimensional quantity ω we call the dynamic coefficient which is defined as follows 3 ω p c st f p c st in f dyn where p c st f is the final wetting state capillary pressure in the previous studies e g see delshad et al 2009 lashgari et al 2016 yu et al 2008 andersen et al 2015 adibhatia et al 2005 the coefficient ω is assumed only chemistry dependent here ω in eq 3 is assumed to be a function of not only the chemistry but also the exposure time to the wa agent the expression in eq 3 can be substituted into eq 1 to obtain a dynamic interpolation model 4 p c 1 ω p c st in ω p c st f we note that in the model presented above we have defined the total capillary pressure pc as simply the measured difference in phase pressures at any point in time in a reservoir simulation this would be capillary pressure in a given grid cell whereas in a laboratory experiment designed to measure p c s data it corresponds to the pressure drop across the sample at equilibrium for quasi static displacement in a bundle of capillary tubes pc is the difference between boundary condition pressures i e p c p l r e s p r r e s see fig 1 the exact nature of ω and its functional dependencies can only be determined from a full characterization of p c s curves under different conditions these curves can be derived from laboratory experiments but this approach is costly and time consuming alternatively one may take a more theoretical approach by simulating p c s curves using a pore scale model that includes the impact of wa for a system that undergoes wa a significant change in ca could lead to the wetting phase becoming non wetting and vice versa for clarity we will continue to use w subscript for the phase that was originally wetting and nw for the phase that is originally non wetting regardless of the actual state of wettability in the system 2 1 pore scale model there are various choices of pore scale models available the easiest to implement and analyze is the bundle of tubes bot model which is a collection of capillary tubes with a distribution of radii herein we describe the model and the approach for the implementation of time dependent wa the bot model is a popular approach due to the simplicity of implementation and the ability to study the balance of energy and forces directly at the scale of interfaces bartley and ruth 1999 dahle et al 2005 helland and skjæveland 2006 the average behavior of a bot model can then be used to construct better constitutive functions at the macroscale dahle et al 2005 helland and skjæveland 2006 2007 there are known limitations to this pore scale approach i e lack of residual saturation and tortuosity we emphasize that this study is a first step at studying the impact of long term wa from pore to core scale and as such these secondary aspects are beyond the scope of this work in this paper we consider cylindrical bot having length l and is shown in fig 1 these tubes are designed to connect wetting right and non wetting left reservoirs with pressures labeled as p r res and p l res respectively let the reservoir pressures difference be defined as 5 δ p p l r e s p r r e s initially the tubes in the bundle are filled with a wetting phase to displace the wetting phase fluid in the m th tube the pressure drop has to exceed the local entry pressure p c m defined as dahle et al 2005 6 δ p p c m r m θ m where p c m rm θm is given by the young s equation 7 p c m r m θ m 2 σ cos θ m r m m 1 2 n where rm and θm are the tube radius and ca respectively for the m th tube n stands for number of tubes σ is fluid fluid interfacial tension as long as condition 6 is satisfied the fluid movement across the length of tube m can be approximated by the lucas washburn flow model washburn 1921 8 q m r m 2 δ p p c m r m θ m 8 μ nw x m i n t μ w l x m i n t where μ nw and μ w are non wetting and wetting fluid viscosities respectively the superscript int stands for fluid fluid interface and q m d x m i n t d t is the interface velocity the interface is assumed to be trapped when it reached at the outlet of the tube thus q m 0 a positive rate of change in x m i n t is associated with an increase in non wetting saturation for tube m from eq 8 one can then determine the required time to reach a specified interface position 2 2 pore scale time dependent wa model in this paper we consider a wa mechanism at the pore scale that evolves smoothly from an initial to final wetting state through exposure time the initial and final wetting states can be arbitrarily chosen i e from wetting to non wetting or vice versa the wa agent is defined as either the non wetting fluid itself or some reactive component therein we consider an alteration process within any given pore or tube that continues until the ultimate wetting state is reached locally in the pore the alteration is permanent but can also be halted at some intermediate wettability state if the wa agent is displaced from the pore if the agent is reintroduced to the pore at some later point alteration continues until the final state is reached to this end we introduce a general functional form of pore scale wa mechanism by ca change 9 θ m θ m in φ δ θ where δ θ θ m f θ m in θ m f and θ m in are the ultimate and initial contact angles respectively in eq 9 θm decreases and increases based on the choice of the initial and final wetting conditions the term φ 0 1 when φ 1 the ca attains its ultimate value and ca is fixed at the initial state when φ 0 in eq 9 is responsible for governing the wa dynamics wa involves complex physical and chemical processes whose description is beyond the scope of this work however we provide a brief summary of the role of adsorption desorption processes in ca change blut 2017 du et al 2019 such a hypothesis has been supported by experiment measurements for instance co2 water core flooding experiments show adsorption type relations between ca and pressure dickson et al 2006 jung and wan 2012 iglauer et al 2012 and also with exposure time jafari and jung 2016 saraji et al 2013 similar ca evolutions as a function of surfactant concentration and exposure time are reported in davis et al 2003 and morton et al 2005 for an oil droplet on a metal surface immersed in ionic surfactant solutions in morton et al 2004 a langmuir adsorption model is proposed to predict the experiment observations in davis et al 2003 given the insights above we consider a ca model that evolves according to the rate of adsorption of the wa agent on the surface of the pores following mckee 1991 van erp et al 2014 the dynamic parameter φ in eq 9 can be stated as 10 d φ d t j j where j and j represent rates of adsorption and desorption of a wa agent respectively at the solid surface in mckee 1991 j is taken to be proportional to the wa agent and the surface unoccupied by the adsorbed wa agent i e 11 j k 1 χ m 1 φ φ where k 1 is a rate constant φ represents the maximum surface saturated concentration and χm is a measure of the local exposure time of tube m the desorption rate can be related with the current surface concentration and is defined as 12 j k 2 φ where k 2 is a rate constant for desorption rate combining eqs 10 12 would give us 13 d φ d t k 1 χ m 1 φ φ k 2 φ assuming φ 1 and following mckee 1991 one can apply a perturbation analysis to eq 13 to obtain a first order approximation for φ in terms of χm 14 φ χ m c χ m where c k 2 k 1 is a parameter that controls the speed and extent of alteration χm is defined as the time integration of exposure to a wa agent here taken to be the local non wetting saturation of tube m 15 χ m 1 t 0 t x m i n t l d τ where t is a pre specified characteristic time in this paper the characteristic time is set as the time for one complete drainage displacement under static initial wetting conditions which can be pre computed according to eq 8 as an aside detailed laboratory work would be needed to further enrich the pore scale ca model by fitting c to experimental data this exercise is beyond the scope of this paper and we consider the underlying ca change in eq 14 a reasonable basis for which to perform the upscaling aspect of our study herein we consider two models for exposure time at the local scale for first we take the case where ca modification based on eq 14 is strictly dependent on local exposure time χm and leads to ca variation from tube to tube hereafter referred to as the non uniform wa mechanism we note that wettability gradients within individual tubes are not considered in this model and thus there is no variation in ca along the tube this is due to the flow model in eq 8 where the ca only affects the entry pressure of the tube the second is referred to as uniform wa which is based on the assumption that the wa agent dissolves into the wetting phase from the non wetting fluid and affects all tubes simultaneously in this case all tubes have the same properties that are governed by the bulk exposure time across the entire bundle i e rev we can define χ 16 χ 1 t 0 t s n w d τ as the bulk or average exposure time as a function of average saturation the uniform model is implemented into eq 14 by taking χ m χ in summary we introduce two types of wa mechanisms uniform and non uniform that serve as end members of possible wa mechanisms at the pore scale on the one end non uniform wa restricts alteration to only drained pores and excludes any interaction of the wa agent between pores this leads to significant heterogeneity in ca from one pore to another at the other end the uniform case assumes the wa agent can alter all pores simultaneously in reality wa will lie somewhere in between but we have chosen simpler end members to aid in further analysis of simulated data in the next section 2 3 simulation approach the uniform and non uniform approaches are coupled into the bot model following algorithm 1 for a single drainage imbibition cycle the objective is to perform simulated experiments that mimic laboratory derived capillary pressure curves i e the pressure is adjusted incrementally up or down after each step depending on if the bundle is under drainage or imbibition respectively contact angles are updated continuously throughout the flow processes in a step wise manner once the displacement is completed for each pressure increment that is contact angles that have been altered according to the uniform or non uniform mechanism are updated before the next pressure increment this is a reasonable approximation given that it is only entry pressures in individual tubes that are affected by ca change we control the pressure drop δp in the way that the wa process is completed within a few numbers of drainage imbibition cycles in the first drainage imbibition cycles the δp increment is such that tubes drain imbibe one at a time with a pressure drop close to the next tube entry pressure in the last drainage imbibition cycle every δp increment is reduced by two and three orders of magnitude for the non uniform and uniform case respectively consequently the flow slows down by the same magnitude irrespective of whether the tube drains or imbibes at the completion of the numerical experiment we obtain a set of p c s data points that can be plotted in the usual way once the capillary pressure curves are generated for both the uniform and non uniform approaches the resulting curves are used to quantify the dynamic coefficient in the interpolation function in eq 4 the goal is to develop a correlation model that involves only a single parameter and this parameter should have a clear relation with changes in the pore scale wa model parameter c 3 results in this section we present the simulated capillary pressure and associated results for each wa case we formulate a correlation model which is then fit to the simulated data finally we investigate the sensitivity of the correlated model to the pore scale wa parameter 3 1 bundle of tubes model set up the pore scale is described by a bot model see section 2 1 each tube in the bot is assigned a different radius r with the radii drawn from a truncated two parameter weibull distribution helland and skjæveland 2006 17 f r r r min r av η 1 η r av exp r r min r av η 1 exp r max r min r av η where r max r min and r av are the pore radii of the largest smallest and average pore sizes respectively and η is a dimensionless parameter the average is obtained by the mean of r max and r min the rock parameters and fluid properties are listed in table 1 3 2 static capillary pressure for end wetting states a starting point for the dynamic capillary pressure models presented in section 2 eqs 1 and 4 is characterizing the capillary pressure curves for the end wetting states given the same tube geometry and fluid pairing described above the capillary pressure saturation data are simulated under static conditions for both the initial and final wetting states only a single drainage experiment is needed in the static case to fully characterize the capillary pressure curve this is due to the lack of residual trapping in a bot model we emphasize that hysteresis is not possible for a bot if the contact angles in the tubes and other parameters are held constant the simulated static curves are then correlated with the brooks corey model 2 the resulting correlations can be found in fig 2 while fitted parameters for the brooks corey model can be found in table 2 note that the brooks corey model is undefined at zero irreducible wetting phase saturation thus we left a few pores undrained to allow for comparison between the brooks corey model and the simulated p c s data fig 2 compares the brooks corey formula 2 and the capillary pressure curves associated with static contact angles initial and final wetting states the brooks corey correlation gives an excellent match to the simulated p c s data under static conditions we observe that the pore size distribution index aw for the initial and final wetting states are the same which is expected since the same distribution of tube radii is used in both cases on the other hand the coefficient cw decreases by a factor of 0 85 from the initial to the final wetting state corresponding to a decrease in a core scale capillary entry pressure the leverett j scaling theory xu et al 2016 predicts that entry pressure scales by cos θ which agrees nicely with the reduction in cos θ by a factor of 0 83 for a ca change from 0 to 80 degrees we reiterate that for the static case where no wa occurs the brooks corey model describes both drainage and imbibition for the bot 3 3 simulated capillary pressure data we present the simulated capillary pressure data see fig 3 comparing the results of the uniform and non uniform approaches described previously the uniform data are generated with the pore scale wa parameter c 0 005 while for the non uniform data c 5 10 4 a total of two and four drainage imbibition cycles carried out for the uniform and non uniform cases respectively for both cases we observe a steady decrease in capillary pressure over time in the end a complete wettability change has evolved from the initial to final prescribed states whose static curves are plotted in fig 3 for reference having reached the final wetting state any additional drainage imbibition cycle would follow along the static curve for the final wetting state we remark that wettability induced dynamics also introduces an apparent hysteresis in the p c s data this effect is unique to the cylindrical bot model which we recall cannot exhibit hysteresis under static wettability conditions however a real porous medium may exhibit capillary pressure hysteresis with static wettability there are notable differences between the two sets of curves for the uniform case fig 3a there are distinct curves for each drainage and imbibition displacement the capillary pressure begins to decrease immediately and in a continuous manner over time this is because the ca fig 4a is changing for all tubes simultaneously based on the average exposure time over the entire bundle the uniformity results in the ca in smaller tubes being altered significantly at an early time at the same rate as the larger tubes and thus the capillary pressure is decreased even at low average wetting saturation in the first drainage curve the fast dynamics in ca change lead to a non monotone capillary curve at an early time in comparison the non uniform case fig 3b has a delay in exhibiting the effects of wa the initial drainage curve is identical to the initial wetting static curve and all subsequent drainage curves follow along the previous imbibition curve this is a result of the restriction on ca change to only tubes that are drained in other words at the tube level there is no change in entry pressure from the initial state or the state after a single drainage imbibition cycle until that tube is drained in contrast to the uniform case the capillary pressure at low sw is drawn towards the initial state this can be described by examining the ca per tube radius in time shown in fig 4b larger tubes that drain first and imbibe last resulting in longer local exposure time and thus more extensive ca change compared to the smaller tubes that drain last and imbibe first therefore the initial wetting state persists in the smaller tubes we recall that both the uniform and non uniform cases are selected as end members of possible wa mechanisms in real porous media in real systems wa in different sized pores may occur in a more complex manner we have observed above that capillary pressure curves in fig 3a and b are not a unique function of saturation that is they exhibit hysteresis for this simple bot geometry we note that the p c s data points are color coded according to time evolved at each data point this motivates a transformation of the data into the time domain by plotting against χ as shown in fig 5 for both cases in doing so we obtain a unique function with respect to exposure time for both the uniform and non uniform cases we note that the curves in fig 5 show that the capillary pressure increases and decreases with each drainage imbibition cycle in addition the transformation reveals the separate drainage curves for the non uniform case that were hidden in fig 3b 3 4 dynamic capillary pressure model development following the approach discussed in section 2 we applied eq 3 to calculate the dynamic coefficient ω for both the uniform and non uniform cases the resulting coefficient is plotted in fig 6 as a function of both sw top panels and χ bottom panels for both wa cases we recall that ω is a coefficient that interpolates between the capillary pressure at the initial and final wetting states at any given saturation where ω 0 gives the initial capillary pressure and ω 1 gives the final p c s curve for the uniform case ω is a non unique function of wetting phase saturation see fig 6a but with values that are continuously increasing as the dynamic capillary pressure moves towards the final wetting state for the non uniform case the dynamic coefficient in fig 6b also exhibits non uniqueness with respect to saturation reflecting the p c s data the capillary pressure persists at the initial state at low saturation this means that the value of ω decreases with decreasing sw along the drainage path and increases only along imbibition paths the complex relation of ω in saturation space makes it challenging to propose a functional form for ω s w relation in both cases figs 6c and 6d show that ω exhibits different behavior as a function of average exposure time for the uniform case fig 6c ω is smoothly increasing and uniquely related to χ mimicking the functional form of the pore scale model in eq 9 on the other hand the coefficient ω in the non uniform case fig 6d is not monotonically increasing in χ but continues to rise and fall with time despite the transformation to the temporal domain the curves in fig 6 give us important insight into the form of ω best suited to each wa case we take each case in turn 3 4 1 uniform case the smoothly varying functionality of ω and χ in fig 6c motivates an adsorption type model 18 ω χ β 1 χ where β 1 is a fitting parameter obtained from the best fit to the simulated data in fig 6c for this particular case the calibrated parameter is estimated to be β 1 0 01 the form of the dynamic p c s model for the uniform case is obtained by substituting eq 18 into eq 4 to give 19 p c χ β 1 χ p c st f p c st in p c st in 3 4 2 non uniform case the non trivial behavior of ω in fig 6d makes it challenging to propose a functional relation between the dynamic coefficient ω and χ directly as we did for the uniform wa case instead we observe that ω in fig 6b has a well behaved curvature for each drainage imbibition cycle along the saturation history further the curvature of each cycle is increasing with increasing exposure time given these insights we proposed a model for the dynamic coefficient that has the following form 20 ω s w χ s w α χ s w where α controls the curvature of the ω s w curve for each drainage imbibition cycle since ω is increasing function of exposure time α should decrease along the averaged variable χ the function form of ω in eq 20 is then matched with the ω s w data to analyze the dynamics of α along χ the obtained α χ relation is decreasing as hypothesized and in particular has the follwing form 21 α χ β 2 χ where β 2 is non dimensional fitting parameter for this particular simulation the parameter β 2 is estimated to be 0 004 for the four of drainage imbibition cycles the form of the dynamic p c s model for the non uniform case is then obtained by substituting eqs 20 and 21 into eq 4 22 p c χ s w β 2 χ s w p c st f p c st in p c st in the calibrated dynamic capillary pressure models in eqs 19 and 22 are compared with the simulated capillary pressure data in fig 3 with the results presented in fig 7 for each wa case we observe that the proposed dynamic models agree well with simulated dynamic capillary pressure curves the correlation coefficient for this comparison is r 2 0 9921 and r 2 0 98 for the uniform and non uniform case respectively thus we have obtained a single parameter model in both the uniform and non uniform wa cases that describe the evolution of dynamic capillarity over multiple drainage imbibition cycles rather than using a model consisting of multiple parameters that change with each cycle or hysteresis models 3 5 model sensitivity to pore scale model parameter we hypothesize that parameters β 1 in eq 19 and β 2 in eq 22 are dependent on the parameter c in eq 9 that controls the dynamics of wettability alteration at the pore scale we investigate this sensitivity by repeating the capillary pressure simulations for different values of the pore scale parameter c and determine the correlated value of β 1 and β 2 in each case for the uniform wa case fig 8 a shows that the interpolation model parameter is linearly proportional to the pore scale model parameter with a proportionality constant of 2 thus the relationship β 1 2 c can be used to predict the upscaled parameter directly from knowledge of the pore scale process in contrast the non uniform case in fig 8b shows a power law model where β 2 b 1 c b 2 is correlated with estimated parameters of b 1 3 3 10 6 and b 2 1 8 the general form of dynamic capillary pressure model can now be obtained for the uniform wa by incorporating the relationship for β 1 in eq 19 23 p c χ 2 c χ p c st f p c st in p c st in similarly we obtain a general non uniform model by substituting β 2 in eq 22 24 p c χ s w b 1 c b 2 χ s w p c st f p c st in p c st in in their final form the dynamic capillary pressure models in eqs 23 and 23 are dependent on two variables saturation and time and a single wettability parameter c the latter must be determined by fitting eq 9 with parameter c to laboratory experiments for a given sample exposed to a wa agent 3 6 applicability to arbitrary saturation history we note that the saturation history used to generate the p c s curves for the two wa cases in figs 3a and 3c can be thought of in each case as a single arbitrary path within an infinite number of possible paths if a different path had been chosen such as a flow reversal at intermediate saturation or a prolonged exposure time at a given saturation it would result in entirely different capillary pressure dynamics in order to test the dynamic models developed in eqs 19 and 22 for any arbitrary saturation history we generate many different p c s curves by taking numerous different paths in the saturation time domain the resulting simulated data forms a surface with respect to saturation and exposure time as shown in fig 9 a and b we then apply the calibrated dynamic models to the same saturation time paths used to generate the p c s χ surface the difference between the calibrated model and the simulated data is shown in fig 9c and d for the uniform and non uniform cases respectively a good comparison of the dynamic models to simulated data demonstrates that model calibration to a single saturation time path is robust enough to be applied to any possible path 3 7 discussion we investigated the potential of the interpolation based model to predict the wa induced dynamics in capillary pressure saturation relations in the interest of completeness we also explored other types of models to capture capillary pressure dynamics including the mixed wet model of skjæveland et al 2000 for brevity we do not report the results of that separate study herein we found that although other models could be calibrated with reasonable accuracy they all involved more than one calibration parameter up to four that need to be adjusted in each drainage imbibition cycle therefore the single parameter single valued interpolation model presented in this study is the preferred model due to its reliability for replicating the simulated bot data the proposed interpolation model is an upscaled model that allows for a change in capillary pressure as a function of upscaled variables saturation and exposure time to a wa agent we recall that exposure time is simply the integration of saturation history over time the model consists of three main components two capillary pressure functions at the initial and final wetting state and a dynamic interpolation coefficient that moves from one state to the other the initial and final capillary pressure functions can be determined a priori from static experiments using inert fluids in this study the initial and final states are represented by classical brooks corey functions the dynamic coefficient is thus the only variable correlated to dynamic capillary pressure simulations in this study we have shown that the coefficient can be easily correlated to saturation and exposure time via a single parameter we have observed that the form of the dynamic term is dependent on the underlying mechanisms for wa we employed two models uniform and non uniform that represent two end members of real systems one end member is identical ca throughout the rev while the other results in severely heterogeneous ca from small to large pores the differences in the two wa mechanisms changes the complexity of the resulting capillary dynamics in the uniform case the dynamic coefficient can be correlated to exposure time through a sorption type model which seems to be a natural result given the ca change at the pore scale is also based on a sorption model this is an interesting observation that requires more analysis in future work in the non uniform case the dynamic coefficient has no similar sorption form with increased exposure time but now with the product of saturation and exposure time as the dynamic variable the additional complexity is needed to draw the capillary pressure curve back to the initial wetting at low saturation a region of the p c s curve dominated by smaller pores where the ca takes longer to change an important result of this study is quantifying the link between the pore and core scale we showed that by varying the parameter that alters the speed and extent of ca change in each individual pore we could predict the resulting impact on dynamic capillary pressure in fact in both the uniform and non uniform cases there is a very simple scaling from the pore scale and macroscale parameters in the uniform case the two parameters are directly proportional while in the non uniform case the macroscale parameter scales with the pore scale parameter via a power law the implication of this result is that by knowing the mechanism that controls ca at the pore scale which can be obtained by a relatively simple batch experiment we can quantify a priori the macroscale dynamics without having to perform pore scale simulations this is an important generalization and valuable for making use of experimental data to inform macroscale constitutive functions we have quantified the ability of the interpolation model to capture underlying wa at the pore scale for a simple bot this result is a natural development from previous studies that incorporate the interpolation model directly into reservoir simulation of wettability alteration in those studies the model was matched directly to core scale data in a heuristic manner the contribution of this study is to quantify the pore scale underpinnings to the interpolation model through a direct and systematic manner thus providing additional evidence to the validity of this type of model for use in macroscale simulation despite the satisfactory and relatively straightforward correlation of the interpolation model to simulated p c s data the exact quantification of the dynamics is ultimately restricted by the simplicity of the bot model we chose this simple approach in order to isolate the wa process from other complexities associated with real pore networks and thus begin to assess the mechanisms linking the pore and core scales though beyond the scope of the current work further study is needed to determine whether the fundamental nature of dynamic behavior we observe will hold when additional complexity is added further advancements can be made by adding complexity to the porous media i e converging diverging throat diameters or tortuosity or to the pore scale wa model theoretical work using pore network models should also be combined with laboratory investigations to further calibrate the underlying wa mechanisms 4 conclusions in this paper we designed a framework to upscale the impact of time dependent wa mechanisms at the pore scale on the dynamics in capillary pressure saturation functions at the darcy scale we found that an interpolation based dynamic model can predict the change in capillary pressure due to underlying wa the form of the dynamic interpolation coefficient is dependent on exposure time to a chemical agent with a different mathematical form depending on the pore scale wa mechanism the correlated dynamic capillary pressure model shows an excellent match with simulated pc s data and reliably predicts capillary dynamics independent of the saturation time path more importantly the model relies on a single interpolation parameter that has a clear and simple relationship with the pore scale wa parameter credit authorship contribution statement abay molla kassa conceptualization data curation writing original draft writing review editing sarah eileen gasda conceptualization data curation writing original draft writing review editing kundan kumar writing review editing florin adrian radu writing review editing acknowledgement funding for this study was through the chi project n 255510 granted through the climit program of the research council of norway 
458,this paper presents the enriched galerkin discretization for modeling fluid flow in fractured porous media using the mixed dimensional approach the proposed method has been tested against published benchmarks since fracture and porous media discontinuities can significantly influence single and multi phase fluid flow the heterogeneous and anisotropic matrix permeability setting is utilized to assess the enriched galerkin performance in handling the discontinuity within the matrix domain and between the matrix and fracture domains our results illustrate that the enriched galerkin method has the same advantages as the discontinuous galerkin method for example it conserves local and global fluid mass captures the pressure discontinuity and provides the optimal error convergence rate however the enriched galerkin method requires much fewer degrees of freedom than the discontinuous galerkin method in its classical form the pressure solutions produced by both methods are similar regardless of the conductive or non conductive fractures or heterogeneity in matrix permeability this analysis shows that the enriched galerkin scheme reduces the computational costs while offering the same accuracy as the discontinuous galerkin so that it can be applied for large scale flow problems furthermore the results of a time dependent problem for a three dimensional geometry reveal the value of correctly capturing the discontinuities as barriers or highly conductive fractures keywords fractured porous media mixed dimensional enriched galerkin finite element method heterogeneity local mass conservative 1 introduction modeling of fluid flow in fractured porous media is essential for a wide variety of applications including water resource management glaser et al 2017 peng et al 2017 geothermal energy willems and nick 2019 salimzadeh et al 2019a salimzadeh and nick 2019 oil and gas wheeler et al 2019 kadeethum et al 2019c andrianov and nick 2019 kadeethum et al 2020c induced seismicity rinaldi and rutqvist 2019 co2 sequestration salimzadeh et al 2018 and biomedical engineering vinje et al 2018 ruiz baier et al 2019 kadeethum et al 2020a a fractured porous medium can be decomposed into the bulk matrix and fracture domains which are generally anisotropic heterogeneous and have substantially discontinuous material properties that can span several orders of magnitude matthai and nick 2009 flemisch et al 2018 jia et al 2017 bisdom et al 2016 these discontinuities can critically enhance or hinder the flux within and between the bulk matrix and fracture domains accurately capturing the flow behavior controlled by these discontinuities in complex media is still challenging nick and matthai 2011a de dreuzy et al 2013 flemisch et al 2016 hoteit and firoozabadi 2008 zhang et al 2016 there are two main approaches to represent the fluid flow between the matrix and fracture domain nick and matthai 2011b flemisch et al 2018 juanes et al 2002 the first model an equi dimensional model discretizes the matrix and fracture domain with same dimensionality salinas et al 2018 this approach is straightforward to implement and no coupling condition is required this approach is utilized to model for example coupled hydromechanical of fractured rocks using the finite discrete element method latham et al 2013 2018 and can also capture fracture propagation using an immersed body method obeysekara et al 2018 or phase field approach santillan et al 2018 lee et al 2016b 2018 the second model which we call a mixed dimensional model hereafter reduces the fracture domain to a lower dimensionality by assuming the fracture thickness is much smaller compared to the size of matrix domain boon et al 2018 martin et al 2005 berrone et al 2018 the second approach has several benefits for example it reduces the degrees of freedom dof nick and matthai 2011a as the fracture domain is represented as the interface which is part of the matrix domain and subsequently this approach can improve mesh quality reduce the mesh skewness matthai et al 2010 since the fractures are interfaces one can use a larger mesh size which satisfies courant friedrichs lewy cfl condition more easily juanes et al 2002 nick and matthai 2011b in the past decades many approaches have been proposed to model the fractured porous media using the mixed dimensional approach 1 two point flux approximation in unstructured control volume finite difference technique karimi fard et al 2004 2 multi point flux approximation using mixed finite element method on general quadrilateral and hexahedral grids wheeler et al 2012 3 extended finite element combined with mixed finite element formulation d angelo and scotti 2012 prevost and sukumar 2016 sanborn and prevost 2011 4 embedded discrete fracture matrix dfm modeling with non conforming mesh hajibeygi et al 2011 odsaeter et al 2019 5 mixed approximation such as mimetic finite difference flemisch and helmig 2008 formaggia et al 2018 6 two field formulation using mixed finite element mfe martin et al 2005 fumagalli et al 2019 and 7 disconinuous galerkin dg method rivie et al 2000 hoteit and firoozabadi 2008 antonietti et al 2019 arnold et al 2002 we focus on the finite element based discretization such as the dg and mfe methods that ensure the local mass conservative property moreover they are flexible enough to discretize complex subsurface geometries such as intersections of fractures or irregular shaped matrix blocks additionally the aforementioned methods are capable of mimicking the fracture propagation in poroelastic media using either cohesive zone method or linear elastic fracture mechanics framework salimzadeh and khalili 2015 salimzadeh et al 2019b secchi and schrefler 2012 segura and carol 2008 however the mfe method requires an additional primary variable fluid velocity which may require more computational resources especially in a three dimensional domain kadeethum et al 2019a mesh adaptivity is also not straightforward to implement lee and wheeler 2017 and it requires the inversion of the permeability tensor which may lead to an ill posed problem choo and lee 2018 the dg method also can be considered as a computationally expensive method as it requires a large number of dof sun and liu 2009 lee et al 2016a to resolve some of the shortcomings mentioned above we propose an enriched galerkin eg discretization lee et al 2016a zi et al 2004 khoei et al 2018 to model fluid flow in fractured porous media using the mixed dimensional approach the eg method utilized in this study composes of the cg function space augmented by a piecewise constant function at the center of each element this method has the same interior penalty type bilinear form as the dg method sun and liu 2009 lee et al 2016a the eg method however only requires to have discontinuous constants as illustrated in fig 1 so it has fewer dof than the dg method fig 2 presents the comparison of the dof ratio among cg eg and dg methods and it shows that the eg method requires half of the dof needed by the dg method triangular elements with the first polynomial degree approximation note that this ratio decreases as the polynomial degree approximation increase the eg method has been developed to solve general elliptic and parabolic problems with dynamic mesh adaptivity lee and wheeler 2017 2018 lee et al 2018 and extended to address the multiphase fluid flow problems lee and wheeler 2018 recently the eg method has been also applied to solve the non linear poroelastic problem choo and lee 2018 kadeethum et al 2019b 2020b and compared its performance with other two and three field formulation methods kadeethum et al 2019a to the best of our knowledge this is the first attempt to apply the eg discretization in the mixed dimensional setting the rest of the paper is organized as follows the methodology section includes model description mathematical equations and their discretizations for the eg and dg methods subsequently the block structure used to compose the eg function space and the coupling terms between matrix and fracture domains is illustrated the numerical examples section presents five examples and the conclusion is finally provided 2 methodology 2 1 governing equations we first briefly introduce the equi dimensional model which is used to derive the mixed dimensional model we are interested in solving steady state and time dependent single phase fluid flow in fractured porous media on ω which composes of matrix and fracture domains ω m and ω f respectively let ω r d be the domain of interest in d dimensional space where d 1 2 or 3 bounded by boundary ω ω can be decomposed to pressure and flux boundaries ω p and ω q respectively the time domain is denoted by t 0 τ where τ 0 is the final time the illustration of the equi dimensional model is shown in fig 3 a this model composes of two matrix subdomains ω mi where i 1 2 and one fracture subdomain ω f note that for the sake of simplicity this setup is used to illustrate the governing equation with only two matrix subdomains but in a general case the domain may compose of nm subdomains i e i 1 2 n m moreover the domain may contain up to nf fractures where nm and nf are number of matrix subdomain and fracture respectively for simplicity in this section we will consider n f 1 the fractures may not cut through the matrix domain which we call immersed fracture setting this topic will be discussed in section 3 the governing system with initial and boundary conditions of the equi dimensional model assuming a slightly compressible fluid for the matrix domain is presented below 1 c ϕ m i t p m i k m i μ p m i ρ g g m i in ω m i t for i 1 2 2 p m p m d on ω p t 3 k m i μ p m i ρ g n q m n on ω q t 4 p m i p m i 0 in ω m i t at t 0 for i 1 2 and for the fracture domain is 5 c ϕ f t p f k f μ p f ρ g g f in ω f t 6 p f p f d on ω p t 7 k f μ p f ρ g n q f n on ω q t 8 p f p f 0 in ω f t at t 0 where i represents an index c is the fluid compressibility ϕm and ϕf are the matrix and fracture porosity ϕf is assumed to be one km and kf are the matrix and fracture permeability tensor respectively μ is fluid viscosity pm and pf are matrix and fracture pressure respectively ρ is fluid density g is the gravitational vector gm and gf are sink source for matrix and fracture domains respectively n is a normal unit vector to any surfaces pmd and pfd are prescribed pressure for matrix and fracture domains respectively qmn and qfn are prescribed flux for matrix and fracture domains respectively and p m i 0 and p f 0 are prescribed pressure for matrix and fracture domains at t 0 respectively to formulate the mixed dimensional setting as presented in fig 3b we integrate along the normal direction to the fracture plane martin et al 2005 as a result ω f is reduced to an interface γ note that the governing equation of the mixed dimensional setting used in this study is proposed by martin et al 2005 in the mixed finite element formulation which uses fluid pressure and fluid velocity as the primary variables the mixed dimensional setting has been used in the mixed formulation keilegavlen et al 2017 or adapted to finite volume discretization glaser et al 2017 stefansson et al 2018 glaser et al 2019 and dg discretization on polytopic grids antonietti et al 2019 the mixed dimension strong formulation and its boundary conditions for the matrix domain are similar to the equi dimensional model 1 to 3 but the strong formulation and its boundary conditions of the fracture domain are 9 c a f t p f t a f k f t μ t p f ρ g g f k m μ p m ρ g in γ t 10 p f p f d on γ p t 11 k f t μ p f ρ g n q f n on γ q t 12 p f p f 0 in γ t at t 0 where γ p and γ q represent pressure and flux boundaries of the fracture domain respectively k f t is the tangential fracture permeability tensor t and t are the tangential gradient and divergence operators which are defined as t n n and tr t respectively tr is trace operator af is a fracture aperture k m μ p m ρ g represents the fluid mass transfer between matrix and fracture domains martin et al 2005 is jump operator which will be discussed later in the discretization part and pfd and qfn are specified pressure and flux for the fracture domain respectively in this study if d 3 km as a full tensor is defined as 13 k m k m x x k m x y k m x z k m y x k m y y k m y z k m z x k m z y k m z z where all tensor components characterize the transformation of the components of the gradient of fluid pressure into the components of the velocity vector the k m x x k m y y and k m z z represent the matrix permeability in x y and z direction respectively kf on the other hand composes of two components 14 k f k f t 0 0 k f n where k f n is the normal fracture permeability note that we present here a general form of k f t to be specific k f t is a scalar if d 2 and tensor if d 3 to represent the fluid mass transfer between matrix and fracture domains following martin et al 2005 antonietti et al 2019 we define the coupling conditions between the matrix and fracture domain as 15 k m μ p m ρ g n α f 2 p m 1 p m 2 on γ 16 k m μ p m ρ g 2 α f 2 ξ 1 p m p f on γ where is an average operator which will be presented later in the discretization part αf represents a resistant factor of the mass transfer between the fracture and matrix domains defined as 17 α f 2 k f n a f and ξ 0 5 1 0 in this paper we set ξ 1 0 for the sake of simplicity in general case ξ is used to represent a family of the mixed dimensional model and more details can be found in martin et al 2005 antonietti et al 2019 2 2 numerical discretization in this section the discretization of the mixed dimensional model is illustrated the domain ω is partitioned into ne elements t h which is the family of elements t triangles in 2d tetrahedrons in 3d we will further denote by e a face of t as illustrated in fig 4 we denote ht as the diameter of t and define h max ht which we may refer as mesh size let e h denotes the set of all facets for the matrix domain e h 0 the internal facets e h d the dirichlet boundary faces and e h n denotes the neumann boundary faces following lee et al 2016a for any e e h 0 let t and t be two neighboring elements such that e t t let n and n be the outward normal unit vectors to t and t respectively see fig 4a e h e h 0 e h d e h n e h 0 e h d e h 0 e h n and e h d e h n we further define e h 1 e h 0 e h d the fracture domain is conforming with e h and it is named γ h intervals in 2d triangles in 3d domain hereafter as presented in fig 4b we will further denote by ef a face of γ h let λ h denotes the set of all facets for the fracture domain λ h 0 the internal facets λ h d the dirichlet boundary faces and λ h n denotes the neumann boundary faces and λ h d λ h n let n be the outward normal unit vector to γ h which is coincided with n of the t next we define the jump operator of the scalar and vector values as 18 x n x n x n and x n x n x n respectively moreover by assuming that the normal vector n is oriented from t to t we obtain 19 x x x following lee et al 2016a scovazzi et al 2017 the weighted average is defined as 20 x δ e δ e x 1 δ e x where 21 δ e k m e k m e k m e and 22 k m e n t k m n k m e n t k m n and a harmonic average of k m e and k m e is defined as 23 k m e 2 k m e k m e k m e k m e the arithmetic average δ e 0 5 is simply denoted by note that in general case δe is also defined for the kf however for the sake of simplicity we assume the material properties of the fracture domain are homogeneous hence we perform these operators in the matrix domain only remark 1 the numerical discretization discussed in this study only considers the case of a conforming mesh i e the fracture domain γ element is coincident with a set of faces of the matrix domain ω as illustrated in fig 4b this study focuses on two function spaces arising from eg and dg discretizations respectively we begin with defining the cg function space for the matrix pressure pm as 24 p h cg k t h ψ m c 0 ω ψ m t p k t t t h where p h cg k t h is the space for the cg approximation with kth degree polynomials for the pm unknown c 0 ω denotes the space of scalar valued piecewise continuous polynomials p k t is the space of polynomials of degree at most k over each element t and ψm denotes a generic function of p h cg k t h furthermore we define the following dg function space for the matrix pressure pm 25 p h dg k t h ψ m l 2 ω ψ m t p k t t t h where p h dg k t h is the space for the dg approximation with kth degree polynomials for the pm space and l 2 ω is the space of square integrable functions finally we define the eg function space for pm as 26 p h eg k t h p h cg k t h p h dg 0 t h where p h eg k t h is the space for the eg approximation with kth degree polynomials for the p space and p h dg 0 t h is the space for the dg approximation with 0th degree polynomials in other words a piecewise constant approximation note that the eg discretization is expected to be beneficial for an accurate approximation of the pm discontinuity across interfaces where high permeability contrast either between k m and k m or km and kf is observed on the other hand the material properties of the fracture domain are assumed to be homogeneous which leads to no discontinuity within the fracture domain therefore the cg discretization of the pf unknown will suffice in the following the pf function space is defined as 27 p h cg k γ h ψ f c 0 γ ψ f e p k e e γ h where p h cg k γ h is the space for the cg approximation with kth degree polynomials for the pf unknown c 0 γ denotes the space of scalar valued piecewise continuous polynomials p k e is the space of polynomials of degree at most k over each facet e and ψf denotes a generic function of p h cg k γ h remark 2 in this study we only focus on the mixed function space between the matrix and fracture domains arising from either p h eg k t h p h cg k γ h or p h dg k t h p h cg k γ h the fracture domain can be discretized by either p h eg k γ h or p h dg k γ h if there are any discontinuities inside the fracture medium the time domain t 0 τ is partitioned into nt open subintervals such that 0 t 0 t 1 t n t τ the length of the subinterval δtn is defined as δ t n t n t n 1 where n represents the current time step in this study implicit first order time discretization is utilized for a time domain of 1 and 5 as shown below for both p m h n and p f h n 28 p m h t p m h n p m h n 1 δ t n and p f h t p f h n p f h n 1 δ t n we denote that the temporal approximation of the function φ tn by φ n with given p m h n 1 and p f h n 1 we now seek the approximated solutions p m h n p h eg k t h and p f h n p h cg k γ h of pm tn and pf tn respectively satisfying 29 m p m h n p f h n p m h n 1 p f h n 1 ψ m ψ f a p m h n p f h n ψ m ψ f l ψ m ψ f 0 ψ m p h eg k t h and ψ f p h cg k γ h first the temporal discretization part is defined as 30 m p m h n p f h n p m h n 1 p f h n 1 ψ m ψ f m m p m h n p m h n 1 ψ m m f p f h n p f h n 1 ψ f where 31 m m p m h n p m h n 1 ψ m t t h t c ϕ m p m h n p m h n 1 δ t n ψ m d v and 32 m f p f h n p f h n 1 ψ f e γ h e c a f p f h n p f h n 1 δ t n ψ f d s here t dv and e ds refer to volume and surface integrals respectively next we define a p m h n p f h n ψ m ψ f as 33 a p m h n p f h n ψ m ψ f a p m h n ψ m b p f h n ψ m c p m h n ψ f d p f h n ψ f where 34 a p m h n ψ m t t h t k m μ p m h n ρ g ψ m d v e e h 1 γ h e k m μ p m h n ρ g δ e ψ m d s θ e e h 1 γ h e k m μ ψ m δ e p m h n d s e e h 1 γ h e β h e k m e μ p m h n ψ m d s e γ h e α f 2 p m h n ψ m d s e γ h e 2 α f 2 ξ 1 p m h n ψ m d s 35 b p f h n ψ m e γ h e 2 α f 2 ξ 1 p f h n ψ m d s 36 c p m h n ψ f e γ h e 2 α f 2 ξ 1 p m h n ψ f d s and 37 d p f h n ψ f e γ h e a f k f t μ p f h n ρ g ψ f d s e γ h e 2 α f 2 ξ 1 p f h n ψ f d s we note that the coupling conditions 15 and 16 are embedded in the above discretized equations in particular the conditions 15 and 16 are discretized as 38 i 1 p m h n ψ m e γ h e α f 2 p m h n ψ m d s ψ m p h eg k t h and 39 i 2 p m h n p f h n ψ m ψ f e γ h e 2 α f 2 ξ 1 p m h n ψ m d s e γ h e 2 α f 2 ξ 1 p m h n ψ f d s e γ h e 2 α f 2 ξ 1 p f h n ψ m d s e γ h e 2 α f 2 ξ 1 p f h n ψ f d s ψ m p h eg k t h and ψ f p h cg k γ h respectively finally we define l ψ m ψ f as 40 l ψ m ψ f ℓ m ψ m ℓ f ψ f where 41 ℓ m ψ m t t h t g m ψ m d v e e h n e q m n ψ m d s θ e e h d e k m μ ψ m p m d n d s e e h d e β h e k m e μ ψ p p m d n d s and 42 ℓ f ψ f e γ h e a f g f ψ f d s e f λ h n e q f n ψ f d s here the choices of the interior penalty method is provided by θ the discretization becomes the symmetric interior penalty galerkin method sipg when θ 1 the incomplete interior penalty galerkin method iipg when θ 0 and the non symmetric interior penalty galerkin method nipg when θ 1 riviere 2008 in this study we set θ 1 for the simplicity the interior penalty parameter β is a function of the polynomial degree k and the characteristic mesh size he which is defined as 43 h e meas t meas t 2 meas e where meas represents a measurement operator measuring length area or volume some studies for the optimal choice of β is provided in lee et al 2019 riviere 2008 remark 3 the neumann boundary condition is naturally applied on the boundary faces that belong to the neumann boundary domain for both the matrix and fracture domains e e h n and e f λ h n the dirichlet boundary condition on the other hand is weakly enforced on the dirichlet boundary faces e e h d for the matrix domain but strongly enforced on the dirichlet bondary faces of the fracture domain e f λ h d let ψ m π i 1 i 1 1 n p m π denote the set of basis functions of p h π k t h i e p h π k t h span ψ m i 1 i 1 1 n p m π having denoted by n p m π the number of dof for the π scalar valued space where π can mean either eg or dg in a similar way let ψ f cg i 3 i 3 1 n p f cg be the set of basis functions for the space p h cg k γ h n p f cg the number of dof for the cg scalar valued space hence two mixed function spaces 1 eg k cg k and 2 dg k cg k where k represents the degree of polynomial approximation are possible and will be compared in the numerical examples in the next section the matrix corresponding to the left hand side of 29 is assembled composing the following blocks 44 j m m π k π k i i i 2 m m ψ m i 2 ψ m i 1 a ψ m i 2 ψ m i 1 i 1 1 n p m π i 2 1 n p m π j m f π k cg k i i i 3 b ψ f cg i 3 ψ m π i 1 i 1 1 n p m π i 3 1 n p f cg j f m cg k π k i 4 i 2 c ψ m π i 2 ψ p f cg i 4 i 4 1 n p f cg i 2 1 n p m π j f f cg k cg k i 4 i 3 m f ψ f cg i 3 ψ f cg i 4 d ψ f cg i 3 ψ f cg i 4 i 4 1 n p f cg i 3 1 n p f cg in a similar way the right hand side of 29 gives rise to a block vector of components 45 ℓ m π k i 1 ℓ m ψ m π i 1 i 1 1 n p m π ℓ f cg k i 3 ℓ f ψ f cg i 3 i 3 1 n f cg the resulting block structure is thus 46 j m m π k π k j m f π k cg k j f m cg k π k j f f cg k cg k p m h π k p f h cg k ℓ m π k ℓ f cg k where p m h π k and p f h cg k collect the degrees of freedom for matrix and fracture pressure respectively finally we remark that owing to 26 the case π eg can be equivalently decomposed into a cg k dg0 cg k mixed function space resulting in 47 j m m cg k cg k j m m cg k dg 0 j m f cg k cg k j m m dg 0 cg k j m m dg 0 dg 0 j m f dg 0 cg k j f m cg k cg k j f m cg k dg 0 j f f cg k cg k p m h cg k p m h dg 0 p f h cg k ℓ m cg k ℓ m dg 0 ℓ f cg k this formulation makes the eg methodology easily implementable in any existing dg codes matrices and vectors are built by fenics form compiler alnaes et al 2015 the block structure is setup using multiphenics toolbox ballarin and rozza 2019 random field of permeability km is populated using scipy package jones et al 2001 β penalty parameter is set at 1 1 and 1 0 for dg and eg methods respectively remark 4 we note that cg dg and eg methods are based on galerkin method which could be extended to consider adaptive meshes that contain hanging nodes in addition there are various advanced development for each methods to enhance the efficiency including variable approximation order techniques especially for eg method an adaptive enrichment i e the piecewise constant functions only added to the elements where the sharp material discontinuities e g between matrix and fracture domains are observed can be developed however in our following numerical examples we focus on the classical form of each methods for the comparison by simulating the proposed mixed dimensional approach for modeling fractures 3 numerical examples we illustrate the capability of the eg method using seven numerical examples we begin with an analysis of the error convergence rate between the eg and dg methods to verify the developed block structure in the mixed dimensional setting we also investigate the eg performance in modeling the quarter five spot pattern and handling the fracture tip in the immersed fracture geometry next we test the eg method in a regular fracture network with and without a heterogeneity in matrix permeability input lastly we apply time dependent problems for two three dimensional geometries the first one represents the case where fractures are orthogonal to the axes and another represents geometry where fractures are given with arbitrary orientations with their interactions in a three dimensional domain 3 1 error convergence analysis to verify the implementation of the proposed block structure utilized to solve the mixed dimensional model using the eg method we illustrate the error convergence rate of the eg method and compare this value with the dg method the example used in this analysis is adapted from antonietti et al 2019 we take ω 0 1 2 and choose the exact solution in the matrix ω and fracture γ x y ω x y 1 as 48 p m e x y in ω 1 p m e x y 2 1 2 3 a f k f n 2 e in ω 2 p f e 1 2 a f k f n in γ by choosing k m i 48 satisfies the system of equations 1 9 15 and 16 presented in the methodology section with sink source terms g as follows 49 g m 2 e x y in ω 1 g m e x y in ω 2 g f e 2 in γ all other physical parameters are set to one and the homogeneous boundary conditions are applied to all boundaries the geometry used in this analysis and the illustration of the exact solution are presented in fig 5 a b respectively we calculate l 2 norm of the difference between the exact solution p and approximated solution ph and the results are presented in fig 5c d for matrix and fracture domains respectively for both matrix and fracture domains the eg and dg methods provide the expected convergence rate of two and three for polynomial degree approximation k of one and two respectively antonietti et al 2019 babuska 1973 3 2 quarter five spot example this numerical example tests the eg method performance in an injection production setting using five spot pattern and we adopt this example from chave et al 2018 antonietti et al 2019 the five spot pattern where one injection well is located in the middle and four producers are located at each corner of the square is commonly used in underground energy extraction chen et al 2006 due to the symmetry of this geometry only a quarter of the domain ω 0 1 2 is simulated the injection well is located at 0 0 and the production well is located at 1 1 we place the fracture with a f 0 0005 at γ x y ω x y 1 the geometry boundary conditions and mesh with h 1 2 10 1 applied in this analysis are shown in fig 6 the following source term is applied to the entire matrix domain including the injection and production wells 50 g m x y 10 1 tan 200 0 2 x 2 y 2 10 1 tanh 200 0 2 x 1 2 y 1 2 to investigate the effect of fracture conductivity we perform two simulations using different fracture conductivity inputs i we choose k m i k f n 1 and k f t 100 i for the permeable fracture case ii we assume the fracture is impermeable and set k m i k f n 1 10 2 and k f t i all of the remaining physical parameters are set to one results of two cases are presented in fig 7 a b for the pressure value and fig 7c d for the pressure profile along x y line the pressure profile of the permeable fracture case is smoothly decreasing from the injection well towards the production well the pressure profile of the impermeable fracture case on the other hand illustrates a jump across the fracture interface these findings comply with the results of the previous studies chave et al 2018 antonietti et al 2019 our results converge to the reference solution as the h is reduced from h 1 2 10 1 to h 7 2 10 2 note that antonietti et al 2019 perform these numerical experiments using the second order dg method with h 7 5 10 2 on polytopic grids antonietti et al 2019 there is no significant difference between the eg and dg results for both h values fig 7c d 3 3 immersed fracture example the numerical examples discussed so far contain a fracture that cut through the matrix domain to test the eg discretization capability in the immersed fracture setting we adopt this example from angot et al 2009 since we assume that the fracture tip is substantially small there is no fluid mass transfer between the matrix and fracture domains across the fracture tip see a in fig 8 hence the fracture boundary λ h n that intersects with the bulk matrix material internal boundary e h 0 is enforced with no flow boundary condition q f n 0 in this example we take the bulk matrix ω 0 1 2 and the fracture with a f 0 01 γ x y ω x 0 5 y 0 5 we perform three simulations using different fracture conductivity inputs i we assume k m i k f n 1 10 2 and k f t 1 10 6 i for the permeable fracture case and its geometry and boundary conditions are presented in fig 8a ii the partially permeable fracture case utilizes k m i k f n 1 10 2 and k f t 1 10 2 i this case geometry and boundary conditions are illustrated in fig 8b iii impermeable fracture geometry and boundary conditions are shown in fig 8c and it uses k m i k f n 1 10 7 and k f t 1 10 7 i all other physical parameters are equal to one example of mesh that contains n e 272 is shown in fig 8d in fig 9 the values of pm with n e 1 741 for all cases are presented the pressure plot along y 0 75 is presented in fig 10 the permeable and partially permeable fracture cases illustrate the continuity of the pressure while the impermeable fracture case shows the jump across the fracture interface our results converge n e 272 n e 1 741 and n e 6 698 to the solutions provided by angot et al 2009 moreover the eg and dg methods provide similar results note that the reference solutions are performed on the finite volume method with 65 536 control volumes 3 4 regular fracture network example we increase the complexity of the problem by increasing the number of fractures as shown in flemisch et al 2018 this example however is called regular fracture network since all fractures are orthogonal to the axes x or y the geometry used in this example was utilized by geiger et al 2013 for analyzing multi rate dual porosity model details for model geometry and boundary conditions are shown in fig 11 a we set k m i for all the matrix domain ω 0 1 2 and a f 1 10 4 for all fractures γ two fracture conductivity inputs are used i we choose k f n 1 10 4 and k f t 10 4 i for the permeable fracture case ii for the impermeable fracture case we assume k f n 1 10 4 and k f t 1 10 4 i all of the remaining physical parameters are set to one example of mesh that contains n e 184 is shown in fig 11d the pm results of the permeable and impermeable fractures are presented in fig 11b c respectively the permeable fracture case shows the smooth pm profile while the impermeable fracture case clearly illustrates the jump of pm across the fracture interface these results are the same as the reference solutions provided by flemisch et al 2018 figs 12 a b present the pressure plots along the lines y 0 7 and x 0 5 of the permeable fracture case the pressure plot along x 0 0 y 0 1 to x 0 9 y 1 0 line of impermeable fracture case is shown in fig 12c our results using n e 184 n e 382 n e 2 046 and n e 26 952 converge to the reference solutions provided by flemisch et al 2018 the reference solution is simulated based on finite volume method with equi dimensional setting and it contains 1 175 056 elements flemisch et al 2018 besides the evaluation of pressure results we also investigate the flux calculated at face a see fig 11a as follows 51 flux e a e k m μ p m ρ g n d s on a as presented in fig 13 the difference between each ne case is insignificant i e the different between n e 26 952 and n e 184 cases is less than 1 furthermore there is no difference between the eg and dg methods the dof comparison between eg and dg methods is shown in table 1 the eg method requires the dof in the matrix domain approximately half of that of the dg method note that the dof in the fracture domain is the same because we discretize the fracture domain using the cg method 3 5 the heterogeneous in bulk matrix permeability example the numerical examples presented so far only consider an homogeneous matrix permeability value in this section we examine the capability of the eg method in handling the discontinuity not only between the fracture and matrix domains but also within the matrix domain e g nick and matthai 2011b by employing the heterogeneous permeability in the bulk matrix we adapt the quarter five spot as in section 3 2 and regular fracture network as in section 3 4 we choose the finest mesh from both examples to test the eg method capability compared to that of the dg method in handling the sharp discontinuity between the maximum number of interfaces the geometries boundary conditions and input parameters are utilized as sections 3 2 and 3 4 except for the km value in this study k m k m i km value is randomly provided value for each cell we will distinguish in particular two different cases named low km case and high km case in the following the low km case is characterized by a log normal distribution with average k m 1 0 variance var k m 40 limited to minimum value k m min 1 0 10 2 and maximum value k m max 1 0 10 2 the high km case uses k m 30 0 var k m 90 k m min 1 0 10 1 and k m max 1 5 10 2 this heterogeneous fields for both examples are populated using scipy package jones et al 2001 as shown in figs 14 and 15 for low km and high km cases respectively 3 5 1 quarter five spot example the results for the low km case with permeable k f n 1 and k f t 100 i and impermeable k f n 1 10 2 and k f t i fractures are illustrated in fig 16 in general the pm profile from the two cases are more disperse than the homogeneous km setting the eg and dg methods provide similar results with n e 6 568 and h 7 2 10 2 the discussions regarding permeable and impermable fracture settings are provided as follows 1 low km with permeable fracture result illustrates the approximately smooth pm solution because the k m 1 0 is equal to k f n the plot along x y line as expected shows pm gradually decreases from the injection well to the production well this result complies with that of the homogeneous km setting 2 low km with impermeable fracture result and the plot along x y line exhibit a little jump of pm across the fracture interface this behavior is different from the homogeneous km setting since k m min k f n which lead to the less permeability contrast between the fracture and matrix domains the results for the high km case with permeable and impermeable fractures are presented in fig 17 using n e 6 568 the eg and dg results are approximately the same the observations concerning the fracture permeability are presented below 1 high km with permeable fracture displays a jump across the fracture domain because km around the fracture on the plotting line is higher than k f n as a result the fracture interface acts like a flow barrier the plot along x y line supports this observation as pm jumps across the fracture interface 2 high km with impermeable fracture illustrates a huge jump across the fracture interface as can be observed from the pm plot along x y line since km is much higher than k f n the pm variation is less pronounced compared to the low km one see figs 7b homogeneous and 16b heterogeneous because the fluid flow is blocked by the fracture interface sharp material discontinuity 3 5 2 regular fracture network example the pressure results of the low km case between the permeable k f n 1 10 4 and k f t 10 4 i and impermeable k f n 1 10 4 and k f t 1 10 4 i fractures are illustrated in fig 18 similar to the five spot example the eg and dg results are similar with n e 26 952 and pm results are more dispersive than the homogeneous km setting as shown in fig 19 a b the discussions regarding permeable and impermable fracture settings are provided as follows 1 low km with permeable fracture illustrates the fracture dominate flow regime even though k m 1 0 k mmin is set to 1 0 10 2 which is much less than k f n this setting reduces the impact of the matrix domain pm plot along y 0 7 and x 0 5 lines support this observation by showing the high pressure gradient in the matrix domain but pm becomes much less varied when entering the fracture domain 2 low km with impermeable fracture also presents the fracture dominate flow regime because k f n 1 10 4 which is less than k m min 1 0 10 2 therefore the flow in the matrix is blocked by the fracture domain the plot along x 0 0 y 0 1 to x 0 9 y 1 0 line illustrates jumps across the fracture domain which is supporting our observation the results for the high km case with permeable and impermeable fractures are presented in fig 19 with n e 26 952 the eg and dg results are approximately the same the observations concerning the fracture permeability are presented below 1 high km with permeable fracture shows that the matrix domain gains more momentum comparing to the low km case pm plot along y 0 7 and x 0 5 lines illustrates also approximately linear reduction along the matrix domain while pressure is almost constant in the fracture domain 2 high km with impermeable fracture clearly presents the fracture domain dominate the flow because k f n is much less than the km hence the flow is blocked by the fractures the plot along x 0 0 y 0 1 to x 0 9 y 1 0 line support this observation by illustrating multiple jumps across the fracture domain and no pm variation inside each matrix block 3 6 the heterogeneous and anisotropy in bulk matrix permeability example this section illustrates the comparison between the pm solutions of eg and dg methods using the heterogeneous and anisotropic km in contrast with the previous example we utilize the coarsest mesh n e 184 from the regular fracture network example the matrix permeability heterogeneity km is generated with the same specification as the low km case in the previous section however in this study the off diagonal terms of 13 are not zero and the km of each element is defined as follows 52 k m k m 0 1 k m 0 1 k m k m the generated heterogeneous field is shown in fig 20 a b for both diagonal and off diagonal terms the pressure results of the low km case between the permeable k f n 1 10 4 and k f t 10 4 i and impermeable k f n 1 10 4 and k f t 1 10 4 i fractures are presented in fig 21 the results of eg and dg methods are approximately similar for both fracture permeability settings these results illustrate that the eg method captures the discontinuities and allow using a coarse mesh to maintain accuracy 3 7 the time dependent problems from this section we consider time dependent problem where c and ϕmi are nonzero 1 8 besides we extend the spatial domain to three dimensional space to further illustrate the applicability of the proposed eg method here the first example considers the geometry containing only the orthogonal fractures to the axes x y or z and the second example assumes the geometry with arbitrary orientated natural fractures note that we here present only the results of the eg method the results of the dg method are comparable to those of the eg method 3 7 1 three dimensional regular fracture network example in this example we consider a three dimensional domain which is an analog of the two dimensional case presented in section 3 5 2 this domain contains a set of well interconnected fractures and meshed with tetrahedral and triangular elements for fractures with n e 9 544 the fracture geometry is based on the example in berre et al 2020 the details of geometry with initial and boundary conditions illustrated in fig 22 a here we consider two different scenarios case i permeable fracture case with k f n 1 10 6 and k f t 1 10 6 i and case ii impermeable fracture case with k f n 1 10 12 and k f t 1 10 12 i for both cases we employ a permeable porous medium by setting k m 1 0 1 0 0 1 i and all of the remaining physical parameters are set to one for the simplicity the temporal domain is given as t 0 100 where an uniform time step size δ t n 1 in fig 22b c the numerical results of the pm vm and vf for the permeable case i and impermeable fracture cases case ii at t 100 are presented a mere visual examination of these results already shows that the fracture permeability controls the flow field for the permeable fracture case the velocity at the corner of block b is higher than that on the opposite corner in block a as the fractures are closer to the open corner point in block b for the impermeable fracture case the velocity at the corner of block b is lower than that on the opposite corner in block a since block a is larger than block b and it can support flow for a longer time similar behaviors of the pressure values are observed in fig 23 a where the average pressure values of the full domain and block a and b are plotted for t 0 100 it is clear that the average pressure in block b drops faster than in block a for the impermeable case moreover fig 23b illustrates the value of pm along x 0 0 y 0 0 z 0 0 to x 1 0 y 1 0 z 1 0 line at t 25 and 75 3 7 2 algrøyna outcrop example the final example is a three dimensional fracture network built based on an outcrop map in algrøyna norway fumagalli et al 2019 the model has a size of 850 1400 600 m and contains 52 intersecting fractures the model is described in detail in berre et al 2020 the finite element mesh is discretized by tetrahedral rock matrix with n e 163 575 and triangular elements fractures with n e 329 080 see fig 24 a for more details as shown in this figure dirichlet boundary conditions are applied on two edges of the model to represent an injection and a production well all other boundaries are considered as no flow the rock matrix and the fractures are considered permeable k f n 1 10 2 m2 k f t 1 10 2 i m2 and k m 1 0 1 0 0 1 i m2 all of the remaining physical parameters are set to one and t 0 1 10 6 sec using an uniform δtn of 1 105 sec fig 24b and c show the simulation results including the pressure field the pressure iso surfaces and velocity vectors in the rock matrix at t 500 000 sec the pressure profiles along a line between two opposite corners of the model at different times are plotted in fig 24d this example illustrates the applicability of the presented eg method for a complex three dimensional fracture network with arbitrary orientations 4 conclusion this study presents the eg discretization for solving a single phase fluid flow in the fractured porous media using the mixed dimensional approach our proposed method has been tested against several published benchmarks and subsequently assessed its performance in the test cases with the heterogeneous and anisotropic matrix permeability our results illustrate that the pressure solutions resulted from the eg and dg method with the same mesh size are approximately similar furthermore the eg method enjoys the same benefits as the dg method for instance preserves local and global conservation for fluxes can handle discontinuity within and between the subdomains and has the optimal error convergence rate however it has much fewer degrees of freedom compared to that of the dg method in its classical form we note that this comparison can vary based on advanced developments of each method e g a hybridized discontinuous galerkin method or variable approximation orders besides the results of the time dependent problem for a three dimensional geometry highlight the importance of correctly capturing the discontinuities with conductivity values from barriers to highly conductive fractures present in geological media this work can be extended to multiphysics problems e g poroelastic and transport phenomena and general form of the mixed dimensional abstraction i e coupled between d and d n dimensionality where d and n are any integers and d n 0 credit authorship contribution statement t kadeethum conceptualization formal analysis software validation writing original draft h m nick conceptualization funding acquisition supervision writing review editing s lee conceptualization formal analysis supervision validation f ballarin conceptualization formal analysis software supervision validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research leading to these results has received funding from the danish hydrocarbon research and technology centre under the advanced water flooding program the computations in this work have been performed with the multiphenics library ballarin and rozza 2019 which is an extension of fenics alnaes et al 2015 for multiphysics problems we acknowledge developers and contributors to both libraries fb also thanks horizon 2020 program for grant h2020 erc cog 2015 aroma cfd project 681447 that supported the development of multiphenics supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103620 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
458,this paper presents the enriched galerkin discretization for modeling fluid flow in fractured porous media using the mixed dimensional approach the proposed method has been tested against published benchmarks since fracture and porous media discontinuities can significantly influence single and multi phase fluid flow the heterogeneous and anisotropic matrix permeability setting is utilized to assess the enriched galerkin performance in handling the discontinuity within the matrix domain and between the matrix and fracture domains our results illustrate that the enriched galerkin method has the same advantages as the discontinuous galerkin method for example it conserves local and global fluid mass captures the pressure discontinuity and provides the optimal error convergence rate however the enriched galerkin method requires much fewer degrees of freedom than the discontinuous galerkin method in its classical form the pressure solutions produced by both methods are similar regardless of the conductive or non conductive fractures or heterogeneity in matrix permeability this analysis shows that the enriched galerkin scheme reduces the computational costs while offering the same accuracy as the discontinuous galerkin so that it can be applied for large scale flow problems furthermore the results of a time dependent problem for a three dimensional geometry reveal the value of correctly capturing the discontinuities as barriers or highly conductive fractures keywords fractured porous media mixed dimensional enriched galerkin finite element method heterogeneity local mass conservative 1 introduction modeling of fluid flow in fractured porous media is essential for a wide variety of applications including water resource management glaser et al 2017 peng et al 2017 geothermal energy willems and nick 2019 salimzadeh et al 2019a salimzadeh and nick 2019 oil and gas wheeler et al 2019 kadeethum et al 2019c andrianov and nick 2019 kadeethum et al 2020c induced seismicity rinaldi and rutqvist 2019 co2 sequestration salimzadeh et al 2018 and biomedical engineering vinje et al 2018 ruiz baier et al 2019 kadeethum et al 2020a a fractured porous medium can be decomposed into the bulk matrix and fracture domains which are generally anisotropic heterogeneous and have substantially discontinuous material properties that can span several orders of magnitude matthai and nick 2009 flemisch et al 2018 jia et al 2017 bisdom et al 2016 these discontinuities can critically enhance or hinder the flux within and between the bulk matrix and fracture domains accurately capturing the flow behavior controlled by these discontinuities in complex media is still challenging nick and matthai 2011a de dreuzy et al 2013 flemisch et al 2016 hoteit and firoozabadi 2008 zhang et al 2016 there are two main approaches to represent the fluid flow between the matrix and fracture domain nick and matthai 2011b flemisch et al 2018 juanes et al 2002 the first model an equi dimensional model discretizes the matrix and fracture domain with same dimensionality salinas et al 2018 this approach is straightforward to implement and no coupling condition is required this approach is utilized to model for example coupled hydromechanical of fractured rocks using the finite discrete element method latham et al 2013 2018 and can also capture fracture propagation using an immersed body method obeysekara et al 2018 or phase field approach santillan et al 2018 lee et al 2016b 2018 the second model which we call a mixed dimensional model hereafter reduces the fracture domain to a lower dimensionality by assuming the fracture thickness is much smaller compared to the size of matrix domain boon et al 2018 martin et al 2005 berrone et al 2018 the second approach has several benefits for example it reduces the degrees of freedom dof nick and matthai 2011a as the fracture domain is represented as the interface which is part of the matrix domain and subsequently this approach can improve mesh quality reduce the mesh skewness matthai et al 2010 since the fractures are interfaces one can use a larger mesh size which satisfies courant friedrichs lewy cfl condition more easily juanes et al 2002 nick and matthai 2011b in the past decades many approaches have been proposed to model the fractured porous media using the mixed dimensional approach 1 two point flux approximation in unstructured control volume finite difference technique karimi fard et al 2004 2 multi point flux approximation using mixed finite element method on general quadrilateral and hexahedral grids wheeler et al 2012 3 extended finite element combined with mixed finite element formulation d angelo and scotti 2012 prevost and sukumar 2016 sanborn and prevost 2011 4 embedded discrete fracture matrix dfm modeling with non conforming mesh hajibeygi et al 2011 odsaeter et al 2019 5 mixed approximation such as mimetic finite difference flemisch and helmig 2008 formaggia et al 2018 6 two field formulation using mixed finite element mfe martin et al 2005 fumagalli et al 2019 and 7 disconinuous galerkin dg method rivie et al 2000 hoteit and firoozabadi 2008 antonietti et al 2019 arnold et al 2002 we focus on the finite element based discretization such as the dg and mfe methods that ensure the local mass conservative property moreover they are flexible enough to discretize complex subsurface geometries such as intersections of fractures or irregular shaped matrix blocks additionally the aforementioned methods are capable of mimicking the fracture propagation in poroelastic media using either cohesive zone method or linear elastic fracture mechanics framework salimzadeh and khalili 2015 salimzadeh et al 2019b secchi and schrefler 2012 segura and carol 2008 however the mfe method requires an additional primary variable fluid velocity which may require more computational resources especially in a three dimensional domain kadeethum et al 2019a mesh adaptivity is also not straightforward to implement lee and wheeler 2017 and it requires the inversion of the permeability tensor which may lead to an ill posed problem choo and lee 2018 the dg method also can be considered as a computationally expensive method as it requires a large number of dof sun and liu 2009 lee et al 2016a to resolve some of the shortcomings mentioned above we propose an enriched galerkin eg discretization lee et al 2016a zi et al 2004 khoei et al 2018 to model fluid flow in fractured porous media using the mixed dimensional approach the eg method utilized in this study composes of the cg function space augmented by a piecewise constant function at the center of each element this method has the same interior penalty type bilinear form as the dg method sun and liu 2009 lee et al 2016a the eg method however only requires to have discontinuous constants as illustrated in fig 1 so it has fewer dof than the dg method fig 2 presents the comparison of the dof ratio among cg eg and dg methods and it shows that the eg method requires half of the dof needed by the dg method triangular elements with the first polynomial degree approximation note that this ratio decreases as the polynomial degree approximation increase the eg method has been developed to solve general elliptic and parabolic problems with dynamic mesh adaptivity lee and wheeler 2017 2018 lee et al 2018 and extended to address the multiphase fluid flow problems lee and wheeler 2018 recently the eg method has been also applied to solve the non linear poroelastic problem choo and lee 2018 kadeethum et al 2019b 2020b and compared its performance with other two and three field formulation methods kadeethum et al 2019a to the best of our knowledge this is the first attempt to apply the eg discretization in the mixed dimensional setting the rest of the paper is organized as follows the methodology section includes model description mathematical equations and their discretizations for the eg and dg methods subsequently the block structure used to compose the eg function space and the coupling terms between matrix and fracture domains is illustrated the numerical examples section presents five examples and the conclusion is finally provided 2 methodology 2 1 governing equations we first briefly introduce the equi dimensional model which is used to derive the mixed dimensional model we are interested in solving steady state and time dependent single phase fluid flow in fractured porous media on ω which composes of matrix and fracture domains ω m and ω f respectively let ω r d be the domain of interest in d dimensional space where d 1 2 or 3 bounded by boundary ω ω can be decomposed to pressure and flux boundaries ω p and ω q respectively the time domain is denoted by t 0 τ where τ 0 is the final time the illustration of the equi dimensional model is shown in fig 3 a this model composes of two matrix subdomains ω mi where i 1 2 and one fracture subdomain ω f note that for the sake of simplicity this setup is used to illustrate the governing equation with only two matrix subdomains but in a general case the domain may compose of nm subdomains i e i 1 2 n m moreover the domain may contain up to nf fractures where nm and nf are number of matrix subdomain and fracture respectively for simplicity in this section we will consider n f 1 the fractures may not cut through the matrix domain which we call immersed fracture setting this topic will be discussed in section 3 the governing system with initial and boundary conditions of the equi dimensional model assuming a slightly compressible fluid for the matrix domain is presented below 1 c ϕ m i t p m i k m i μ p m i ρ g g m i in ω m i t for i 1 2 2 p m p m d on ω p t 3 k m i μ p m i ρ g n q m n on ω q t 4 p m i p m i 0 in ω m i t at t 0 for i 1 2 and for the fracture domain is 5 c ϕ f t p f k f μ p f ρ g g f in ω f t 6 p f p f d on ω p t 7 k f μ p f ρ g n q f n on ω q t 8 p f p f 0 in ω f t at t 0 where i represents an index c is the fluid compressibility ϕm and ϕf are the matrix and fracture porosity ϕf is assumed to be one km and kf are the matrix and fracture permeability tensor respectively μ is fluid viscosity pm and pf are matrix and fracture pressure respectively ρ is fluid density g is the gravitational vector gm and gf are sink source for matrix and fracture domains respectively n is a normal unit vector to any surfaces pmd and pfd are prescribed pressure for matrix and fracture domains respectively qmn and qfn are prescribed flux for matrix and fracture domains respectively and p m i 0 and p f 0 are prescribed pressure for matrix and fracture domains at t 0 respectively to formulate the mixed dimensional setting as presented in fig 3b we integrate along the normal direction to the fracture plane martin et al 2005 as a result ω f is reduced to an interface γ note that the governing equation of the mixed dimensional setting used in this study is proposed by martin et al 2005 in the mixed finite element formulation which uses fluid pressure and fluid velocity as the primary variables the mixed dimensional setting has been used in the mixed formulation keilegavlen et al 2017 or adapted to finite volume discretization glaser et al 2017 stefansson et al 2018 glaser et al 2019 and dg discretization on polytopic grids antonietti et al 2019 the mixed dimension strong formulation and its boundary conditions for the matrix domain are similar to the equi dimensional model 1 to 3 but the strong formulation and its boundary conditions of the fracture domain are 9 c a f t p f t a f k f t μ t p f ρ g g f k m μ p m ρ g in γ t 10 p f p f d on γ p t 11 k f t μ p f ρ g n q f n on γ q t 12 p f p f 0 in γ t at t 0 where γ p and γ q represent pressure and flux boundaries of the fracture domain respectively k f t is the tangential fracture permeability tensor t and t are the tangential gradient and divergence operators which are defined as t n n and tr t respectively tr is trace operator af is a fracture aperture k m μ p m ρ g represents the fluid mass transfer between matrix and fracture domains martin et al 2005 is jump operator which will be discussed later in the discretization part and pfd and qfn are specified pressure and flux for the fracture domain respectively in this study if d 3 km as a full tensor is defined as 13 k m k m x x k m x y k m x z k m y x k m y y k m y z k m z x k m z y k m z z where all tensor components characterize the transformation of the components of the gradient of fluid pressure into the components of the velocity vector the k m x x k m y y and k m z z represent the matrix permeability in x y and z direction respectively kf on the other hand composes of two components 14 k f k f t 0 0 k f n where k f n is the normal fracture permeability note that we present here a general form of k f t to be specific k f t is a scalar if d 2 and tensor if d 3 to represent the fluid mass transfer between matrix and fracture domains following martin et al 2005 antonietti et al 2019 we define the coupling conditions between the matrix and fracture domain as 15 k m μ p m ρ g n α f 2 p m 1 p m 2 on γ 16 k m μ p m ρ g 2 α f 2 ξ 1 p m p f on γ where is an average operator which will be presented later in the discretization part αf represents a resistant factor of the mass transfer between the fracture and matrix domains defined as 17 α f 2 k f n a f and ξ 0 5 1 0 in this paper we set ξ 1 0 for the sake of simplicity in general case ξ is used to represent a family of the mixed dimensional model and more details can be found in martin et al 2005 antonietti et al 2019 2 2 numerical discretization in this section the discretization of the mixed dimensional model is illustrated the domain ω is partitioned into ne elements t h which is the family of elements t triangles in 2d tetrahedrons in 3d we will further denote by e a face of t as illustrated in fig 4 we denote ht as the diameter of t and define h max ht which we may refer as mesh size let e h denotes the set of all facets for the matrix domain e h 0 the internal facets e h d the dirichlet boundary faces and e h n denotes the neumann boundary faces following lee et al 2016a for any e e h 0 let t and t be two neighboring elements such that e t t let n and n be the outward normal unit vectors to t and t respectively see fig 4a e h e h 0 e h d e h n e h 0 e h d e h 0 e h n and e h d e h n we further define e h 1 e h 0 e h d the fracture domain is conforming with e h and it is named γ h intervals in 2d triangles in 3d domain hereafter as presented in fig 4b we will further denote by ef a face of γ h let λ h denotes the set of all facets for the fracture domain λ h 0 the internal facets λ h d the dirichlet boundary faces and λ h n denotes the neumann boundary faces and λ h d λ h n let n be the outward normal unit vector to γ h which is coincided with n of the t next we define the jump operator of the scalar and vector values as 18 x n x n x n and x n x n x n respectively moreover by assuming that the normal vector n is oriented from t to t we obtain 19 x x x following lee et al 2016a scovazzi et al 2017 the weighted average is defined as 20 x δ e δ e x 1 δ e x where 21 δ e k m e k m e k m e and 22 k m e n t k m n k m e n t k m n and a harmonic average of k m e and k m e is defined as 23 k m e 2 k m e k m e k m e k m e the arithmetic average δ e 0 5 is simply denoted by note that in general case δe is also defined for the kf however for the sake of simplicity we assume the material properties of the fracture domain are homogeneous hence we perform these operators in the matrix domain only remark 1 the numerical discretization discussed in this study only considers the case of a conforming mesh i e the fracture domain γ element is coincident with a set of faces of the matrix domain ω as illustrated in fig 4b this study focuses on two function spaces arising from eg and dg discretizations respectively we begin with defining the cg function space for the matrix pressure pm as 24 p h cg k t h ψ m c 0 ω ψ m t p k t t t h where p h cg k t h is the space for the cg approximation with kth degree polynomials for the pm unknown c 0 ω denotes the space of scalar valued piecewise continuous polynomials p k t is the space of polynomials of degree at most k over each element t and ψm denotes a generic function of p h cg k t h furthermore we define the following dg function space for the matrix pressure pm 25 p h dg k t h ψ m l 2 ω ψ m t p k t t t h where p h dg k t h is the space for the dg approximation with kth degree polynomials for the pm space and l 2 ω is the space of square integrable functions finally we define the eg function space for pm as 26 p h eg k t h p h cg k t h p h dg 0 t h where p h eg k t h is the space for the eg approximation with kth degree polynomials for the p space and p h dg 0 t h is the space for the dg approximation with 0th degree polynomials in other words a piecewise constant approximation note that the eg discretization is expected to be beneficial for an accurate approximation of the pm discontinuity across interfaces where high permeability contrast either between k m and k m or km and kf is observed on the other hand the material properties of the fracture domain are assumed to be homogeneous which leads to no discontinuity within the fracture domain therefore the cg discretization of the pf unknown will suffice in the following the pf function space is defined as 27 p h cg k γ h ψ f c 0 γ ψ f e p k e e γ h where p h cg k γ h is the space for the cg approximation with kth degree polynomials for the pf unknown c 0 γ denotes the space of scalar valued piecewise continuous polynomials p k e is the space of polynomials of degree at most k over each facet e and ψf denotes a generic function of p h cg k γ h remark 2 in this study we only focus on the mixed function space between the matrix and fracture domains arising from either p h eg k t h p h cg k γ h or p h dg k t h p h cg k γ h the fracture domain can be discretized by either p h eg k γ h or p h dg k γ h if there are any discontinuities inside the fracture medium the time domain t 0 τ is partitioned into nt open subintervals such that 0 t 0 t 1 t n t τ the length of the subinterval δtn is defined as δ t n t n t n 1 where n represents the current time step in this study implicit first order time discretization is utilized for a time domain of 1 and 5 as shown below for both p m h n and p f h n 28 p m h t p m h n p m h n 1 δ t n and p f h t p f h n p f h n 1 δ t n we denote that the temporal approximation of the function φ tn by φ n with given p m h n 1 and p f h n 1 we now seek the approximated solutions p m h n p h eg k t h and p f h n p h cg k γ h of pm tn and pf tn respectively satisfying 29 m p m h n p f h n p m h n 1 p f h n 1 ψ m ψ f a p m h n p f h n ψ m ψ f l ψ m ψ f 0 ψ m p h eg k t h and ψ f p h cg k γ h first the temporal discretization part is defined as 30 m p m h n p f h n p m h n 1 p f h n 1 ψ m ψ f m m p m h n p m h n 1 ψ m m f p f h n p f h n 1 ψ f where 31 m m p m h n p m h n 1 ψ m t t h t c ϕ m p m h n p m h n 1 δ t n ψ m d v and 32 m f p f h n p f h n 1 ψ f e γ h e c a f p f h n p f h n 1 δ t n ψ f d s here t dv and e ds refer to volume and surface integrals respectively next we define a p m h n p f h n ψ m ψ f as 33 a p m h n p f h n ψ m ψ f a p m h n ψ m b p f h n ψ m c p m h n ψ f d p f h n ψ f where 34 a p m h n ψ m t t h t k m μ p m h n ρ g ψ m d v e e h 1 γ h e k m μ p m h n ρ g δ e ψ m d s θ e e h 1 γ h e k m μ ψ m δ e p m h n d s e e h 1 γ h e β h e k m e μ p m h n ψ m d s e γ h e α f 2 p m h n ψ m d s e γ h e 2 α f 2 ξ 1 p m h n ψ m d s 35 b p f h n ψ m e γ h e 2 α f 2 ξ 1 p f h n ψ m d s 36 c p m h n ψ f e γ h e 2 α f 2 ξ 1 p m h n ψ f d s and 37 d p f h n ψ f e γ h e a f k f t μ p f h n ρ g ψ f d s e γ h e 2 α f 2 ξ 1 p f h n ψ f d s we note that the coupling conditions 15 and 16 are embedded in the above discretized equations in particular the conditions 15 and 16 are discretized as 38 i 1 p m h n ψ m e γ h e α f 2 p m h n ψ m d s ψ m p h eg k t h and 39 i 2 p m h n p f h n ψ m ψ f e γ h e 2 α f 2 ξ 1 p m h n ψ m d s e γ h e 2 α f 2 ξ 1 p m h n ψ f d s e γ h e 2 α f 2 ξ 1 p f h n ψ m d s e γ h e 2 α f 2 ξ 1 p f h n ψ f d s ψ m p h eg k t h and ψ f p h cg k γ h respectively finally we define l ψ m ψ f as 40 l ψ m ψ f ℓ m ψ m ℓ f ψ f where 41 ℓ m ψ m t t h t g m ψ m d v e e h n e q m n ψ m d s θ e e h d e k m μ ψ m p m d n d s e e h d e β h e k m e μ ψ p p m d n d s and 42 ℓ f ψ f e γ h e a f g f ψ f d s e f λ h n e q f n ψ f d s here the choices of the interior penalty method is provided by θ the discretization becomes the symmetric interior penalty galerkin method sipg when θ 1 the incomplete interior penalty galerkin method iipg when θ 0 and the non symmetric interior penalty galerkin method nipg when θ 1 riviere 2008 in this study we set θ 1 for the simplicity the interior penalty parameter β is a function of the polynomial degree k and the characteristic mesh size he which is defined as 43 h e meas t meas t 2 meas e where meas represents a measurement operator measuring length area or volume some studies for the optimal choice of β is provided in lee et al 2019 riviere 2008 remark 3 the neumann boundary condition is naturally applied on the boundary faces that belong to the neumann boundary domain for both the matrix and fracture domains e e h n and e f λ h n the dirichlet boundary condition on the other hand is weakly enforced on the dirichlet boundary faces e e h d for the matrix domain but strongly enforced on the dirichlet bondary faces of the fracture domain e f λ h d let ψ m π i 1 i 1 1 n p m π denote the set of basis functions of p h π k t h i e p h π k t h span ψ m i 1 i 1 1 n p m π having denoted by n p m π the number of dof for the π scalar valued space where π can mean either eg or dg in a similar way let ψ f cg i 3 i 3 1 n p f cg be the set of basis functions for the space p h cg k γ h n p f cg the number of dof for the cg scalar valued space hence two mixed function spaces 1 eg k cg k and 2 dg k cg k where k represents the degree of polynomial approximation are possible and will be compared in the numerical examples in the next section the matrix corresponding to the left hand side of 29 is assembled composing the following blocks 44 j m m π k π k i i i 2 m m ψ m i 2 ψ m i 1 a ψ m i 2 ψ m i 1 i 1 1 n p m π i 2 1 n p m π j m f π k cg k i i i 3 b ψ f cg i 3 ψ m π i 1 i 1 1 n p m π i 3 1 n p f cg j f m cg k π k i 4 i 2 c ψ m π i 2 ψ p f cg i 4 i 4 1 n p f cg i 2 1 n p m π j f f cg k cg k i 4 i 3 m f ψ f cg i 3 ψ f cg i 4 d ψ f cg i 3 ψ f cg i 4 i 4 1 n p f cg i 3 1 n p f cg in a similar way the right hand side of 29 gives rise to a block vector of components 45 ℓ m π k i 1 ℓ m ψ m π i 1 i 1 1 n p m π ℓ f cg k i 3 ℓ f ψ f cg i 3 i 3 1 n f cg the resulting block structure is thus 46 j m m π k π k j m f π k cg k j f m cg k π k j f f cg k cg k p m h π k p f h cg k ℓ m π k ℓ f cg k where p m h π k and p f h cg k collect the degrees of freedom for matrix and fracture pressure respectively finally we remark that owing to 26 the case π eg can be equivalently decomposed into a cg k dg0 cg k mixed function space resulting in 47 j m m cg k cg k j m m cg k dg 0 j m f cg k cg k j m m dg 0 cg k j m m dg 0 dg 0 j m f dg 0 cg k j f m cg k cg k j f m cg k dg 0 j f f cg k cg k p m h cg k p m h dg 0 p f h cg k ℓ m cg k ℓ m dg 0 ℓ f cg k this formulation makes the eg methodology easily implementable in any existing dg codes matrices and vectors are built by fenics form compiler alnaes et al 2015 the block structure is setup using multiphenics toolbox ballarin and rozza 2019 random field of permeability km is populated using scipy package jones et al 2001 β penalty parameter is set at 1 1 and 1 0 for dg and eg methods respectively remark 4 we note that cg dg and eg methods are based on galerkin method which could be extended to consider adaptive meshes that contain hanging nodes in addition there are various advanced development for each methods to enhance the efficiency including variable approximation order techniques especially for eg method an adaptive enrichment i e the piecewise constant functions only added to the elements where the sharp material discontinuities e g between matrix and fracture domains are observed can be developed however in our following numerical examples we focus on the classical form of each methods for the comparison by simulating the proposed mixed dimensional approach for modeling fractures 3 numerical examples we illustrate the capability of the eg method using seven numerical examples we begin with an analysis of the error convergence rate between the eg and dg methods to verify the developed block structure in the mixed dimensional setting we also investigate the eg performance in modeling the quarter five spot pattern and handling the fracture tip in the immersed fracture geometry next we test the eg method in a regular fracture network with and without a heterogeneity in matrix permeability input lastly we apply time dependent problems for two three dimensional geometries the first one represents the case where fractures are orthogonal to the axes and another represents geometry where fractures are given with arbitrary orientations with their interactions in a three dimensional domain 3 1 error convergence analysis to verify the implementation of the proposed block structure utilized to solve the mixed dimensional model using the eg method we illustrate the error convergence rate of the eg method and compare this value with the dg method the example used in this analysis is adapted from antonietti et al 2019 we take ω 0 1 2 and choose the exact solution in the matrix ω and fracture γ x y ω x y 1 as 48 p m e x y in ω 1 p m e x y 2 1 2 3 a f k f n 2 e in ω 2 p f e 1 2 a f k f n in γ by choosing k m i 48 satisfies the system of equations 1 9 15 and 16 presented in the methodology section with sink source terms g as follows 49 g m 2 e x y in ω 1 g m e x y in ω 2 g f e 2 in γ all other physical parameters are set to one and the homogeneous boundary conditions are applied to all boundaries the geometry used in this analysis and the illustration of the exact solution are presented in fig 5 a b respectively we calculate l 2 norm of the difference between the exact solution p and approximated solution ph and the results are presented in fig 5c d for matrix and fracture domains respectively for both matrix and fracture domains the eg and dg methods provide the expected convergence rate of two and three for polynomial degree approximation k of one and two respectively antonietti et al 2019 babuska 1973 3 2 quarter five spot example this numerical example tests the eg method performance in an injection production setting using five spot pattern and we adopt this example from chave et al 2018 antonietti et al 2019 the five spot pattern where one injection well is located in the middle and four producers are located at each corner of the square is commonly used in underground energy extraction chen et al 2006 due to the symmetry of this geometry only a quarter of the domain ω 0 1 2 is simulated the injection well is located at 0 0 and the production well is located at 1 1 we place the fracture with a f 0 0005 at γ x y ω x y 1 the geometry boundary conditions and mesh with h 1 2 10 1 applied in this analysis are shown in fig 6 the following source term is applied to the entire matrix domain including the injection and production wells 50 g m x y 10 1 tan 200 0 2 x 2 y 2 10 1 tanh 200 0 2 x 1 2 y 1 2 to investigate the effect of fracture conductivity we perform two simulations using different fracture conductivity inputs i we choose k m i k f n 1 and k f t 100 i for the permeable fracture case ii we assume the fracture is impermeable and set k m i k f n 1 10 2 and k f t i all of the remaining physical parameters are set to one results of two cases are presented in fig 7 a b for the pressure value and fig 7c d for the pressure profile along x y line the pressure profile of the permeable fracture case is smoothly decreasing from the injection well towards the production well the pressure profile of the impermeable fracture case on the other hand illustrates a jump across the fracture interface these findings comply with the results of the previous studies chave et al 2018 antonietti et al 2019 our results converge to the reference solution as the h is reduced from h 1 2 10 1 to h 7 2 10 2 note that antonietti et al 2019 perform these numerical experiments using the second order dg method with h 7 5 10 2 on polytopic grids antonietti et al 2019 there is no significant difference between the eg and dg results for both h values fig 7c d 3 3 immersed fracture example the numerical examples discussed so far contain a fracture that cut through the matrix domain to test the eg discretization capability in the immersed fracture setting we adopt this example from angot et al 2009 since we assume that the fracture tip is substantially small there is no fluid mass transfer between the matrix and fracture domains across the fracture tip see a in fig 8 hence the fracture boundary λ h n that intersects with the bulk matrix material internal boundary e h 0 is enforced with no flow boundary condition q f n 0 in this example we take the bulk matrix ω 0 1 2 and the fracture with a f 0 01 γ x y ω x 0 5 y 0 5 we perform three simulations using different fracture conductivity inputs i we assume k m i k f n 1 10 2 and k f t 1 10 6 i for the permeable fracture case and its geometry and boundary conditions are presented in fig 8a ii the partially permeable fracture case utilizes k m i k f n 1 10 2 and k f t 1 10 2 i this case geometry and boundary conditions are illustrated in fig 8b iii impermeable fracture geometry and boundary conditions are shown in fig 8c and it uses k m i k f n 1 10 7 and k f t 1 10 7 i all other physical parameters are equal to one example of mesh that contains n e 272 is shown in fig 8d in fig 9 the values of pm with n e 1 741 for all cases are presented the pressure plot along y 0 75 is presented in fig 10 the permeable and partially permeable fracture cases illustrate the continuity of the pressure while the impermeable fracture case shows the jump across the fracture interface our results converge n e 272 n e 1 741 and n e 6 698 to the solutions provided by angot et al 2009 moreover the eg and dg methods provide similar results note that the reference solutions are performed on the finite volume method with 65 536 control volumes 3 4 regular fracture network example we increase the complexity of the problem by increasing the number of fractures as shown in flemisch et al 2018 this example however is called regular fracture network since all fractures are orthogonal to the axes x or y the geometry used in this example was utilized by geiger et al 2013 for analyzing multi rate dual porosity model details for model geometry and boundary conditions are shown in fig 11 a we set k m i for all the matrix domain ω 0 1 2 and a f 1 10 4 for all fractures γ two fracture conductivity inputs are used i we choose k f n 1 10 4 and k f t 10 4 i for the permeable fracture case ii for the impermeable fracture case we assume k f n 1 10 4 and k f t 1 10 4 i all of the remaining physical parameters are set to one example of mesh that contains n e 184 is shown in fig 11d the pm results of the permeable and impermeable fractures are presented in fig 11b c respectively the permeable fracture case shows the smooth pm profile while the impermeable fracture case clearly illustrates the jump of pm across the fracture interface these results are the same as the reference solutions provided by flemisch et al 2018 figs 12 a b present the pressure plots along the lines y 0 7 and x 0 5 of the permeable fracture case the pressure plot along x 0 0 y 0 1 to x 0 9 y 1 0 line of impermeable fracture case is shown in fig 12c our results using n e 184 n e 382 n e 2 046 and n e 26 952 converge to the reference solutions provided by flemisch et al 2018 the reference solution is simulated based on finite volume method with equi dimensional setting and it contains 1 175 056 elements flemisch et al 2018 besides the evaluation of pressure results we also investigate the flux calculated at face a see fig 11a as follows 51 flux e a e k m μ p m ρ g n d s on a as presented in fig 13 the difference between each ne case is insignificant i e the different between n e 26 952 and n e 184 cases is less than 1 furthermore there is no difference between the eg and dg methods the dof comparison between eg and dg methods is shown in table 1 the eg method requires the dof in the matrix domain approximately half of that of the dg method note that the dof in the fracture domain is the same because we discretize the fracture domain using the cg method 3 5 the heterogeneous in bulk matrix permeability example the numerical examples presented so far only consider an homogeneous matrix permeability value in this section we examine the capability of the eg method in handling the discontinuity not only between the fracture and matrix domains but also within the matrix domain e g nick and matthai 2011b by employing the heterogeneous permeability in the bulk matrix we adapt the quarter five spot as in section 3 2 and regular fracture network as in section 3 4 we choose the finest mesh from both examples to test the eg method capability compared to that of the dg method in handling the sharp discontinuity between the maximum number of interfaces the geometries boundary conditions and input parameters are utilized as sections 3 2 and 3 4 except for the km value in this study k m k m i km value is randomly provided value for each cell we will distinguish in particular two different cases named low km case and high km case in the following the low km case is characterized by a log normal distribution with average k m 1 0 variance var k m 40 limited to minimum value k m min 1 0 10 2 and maximum value k m max 1 0 10 2 the high km case uses k m 30 0 var k m 90 k m min 1 0 10 1 and k m max 1 5 10 2 this heterogeneous fields for both examples are populated using scipy package jones et al 2001 as shown in figs 14 and 15 for low km and high km cases respectively 3 5 1 quarter five spot example the results for the low km case with permeable k f n 1 and k f t 100 i and impermeable k f n 1 10 2 and k f t i fractures are illustrated in fig 16 in general the pm profile from the two cases are more disperse than the homogeneous km setting the eg and dg methods provide similar results with n e 6 568 and h 7 2 10 2 the discussions regarding permeable and impermable fracture settings are provided as follows 1 low km with permeable fracture result illustrates the approximately smooth pm solution because the k m 1 0 is equal to k f n the plot along x y line as expected shows pm gradually decreases from the injection well to the production well this result complies with that of the homogeneous km setting 2 low km with impermeable fracture result and the plot along x y line exhibit a little jump of pm across the fracture interface this behavior is different from the homogeneous km setting since k m min k f n which lead to the less permeability contrast between the fracture and matrix domains the results for the high km case with permeable and impermeable fractures are presented in fig 17 using n e 6 568 the eg and dg results are approximately the same the observations concerning the fracture permeability are presented below 1 high km with permeable fracture displays a jump across the fracture domain because km around the fracture on the plotting line is higher than k f n as a result the fracture interface acts like a flow barrier the plot along x y line supports this observation as pm jumps across the fracture interface 2 high km with impermeable fracture illustrates a huge jump across the fracture interface as can be observed from the pm plot along x y line since km is much higher than k f n the pm variation is less pronounced compared to the low km one see figs 7b homogeneous and 16b heterogeneous because the fluid flow is blocked by the fracture interface sharp material discontinuity 3 5 2 regular fracture network example the pressure results of the low km case between the permeable k f n 1 10 4 and k f t 10 4 i and impermeable k f n 1 10 4 and k f t 1 10 4 i fractures are illustrated in fig 18 similar to the five spot example the eg and dg results are similar with n e 26 952 and pm results are more dispersive than the homogeneous km setting as shown in fig 19 a b the discussions regarding permeable and impermable fracture settings are provided as follows 1 low km with permeable fracture illustrates the fracture dominate flow regime even though k m 1 0 k mmin is set to 1 0 10 2 which is much less than k f n this setting reduces the impact of the matrix domain pm plot along y 0 7 and x 0 5 lines support this observation by showing the high pressure gradient in the matrix domain but pm becomes much less varied when entering the fracture domain 2 low km with impermeable fracture also presents the fracture dominate flow regime because k f n 1 10 4 which is less than k m min 1 0 10 2 therefore the flow in the matrix is blocked by the fracture domain the plot along x 0 0 y 0 1 to x 0 9 y 1 0 line illustrates jumps across the fracture domain which is supporting our observation the results for the high km case with permeable and impermeable fractures are presented in fig 19 with n e 26 952 the eg and dg results are approximately the same the observations concerning the fracture permeability are presented below 1 high km with permeable fracture shows that the matrix domain gains more momentum comparing to the low km case pm plot along y 0 7 and x 0 5 lines illustrates also approximately linear reduction along the matrix domain while pressure is almost constant in the fracture domain 2 high km with impermeable fracture clearly presents the fracture domain dominate the flow because k f n is much less than the km hence the flow is blocked by the fractures the plot along x 0 0 y 0 1 to x 0 9 y 1 0 line support this observation by illustrating multiple jumps across the fracture domain and no pm variation inside each matrix block 3 6 the heterogeneous and anisotropy in bulk matrix permeability example this section illustrates the comparison between the pm solutions of eg and dg methods using the heterogeneous and anisotropic km in contrast with the previous example we utilize the coarsest mesh n e 184 from the regular fracture network example the matrix permeability heterogeneity km is generated with the same specification as the low km case in the previous section however in this study the off diagonal terms of 13 are not zero and the km of each element is defined as follows 52 k m k m 0 1 k m 0 1 k m k m the generated heterogeneous field is shown in fig 20 a b for both diagonal and off diagonal terms the pressure results of the low km case between the permeable k f n 1 10 4 and k f t 10 4 i and impermeable k f n 1 10 4 and k f t 1 10 4 i fractures are presented in fig 21 the results of eg and dg methods are approximately similar for both fracture permeability settings these results illustrate that the eg method captures the discontinuities and allow using a coarse mesh to maintain accuracy 3 7 the time dependent problems from this section we consider time dependent problem where c and ϕmi are nonzero 1 8 besides we extend the spatial domain to three dimensional space to further illustrate the applicability of the proposed eg method here the first example considers the geometry containing only the orthogonal fractures to the axes x y or z and the second example assumes the geometry with arbitrary orientated natural fractures note that we here present only the results of the eg method the results of the dg method are comparable to those of the eg method 3 7 1 three dimensional regular fracture network example in this example we consider a three dimensional domain which is an analog of the two dimensional case presented in section 3 5 2 this domain contains a set of well interconnected fractures and meshed with tetrahedral and triangular elements for fractures with n e 9 544 the fracture geometry is based on the example in berre et al 2020 the details of geometry with initial and boundary conditions illustrated in fig 22 a here we consider two different scenarios case i permeable fracture case with k f n 1 10 6 and k f t 1 10 6 i and case ii impermeable fracture case with k f n 1 10 12 and k f t 1 10 12 i for both cases we employ a permeable porous medium by setting k m 1 0 1 0 0 1 i and all of the remaining physical parameters are set to one for the simplicity the temporal domain is given as t 0 100 where an uniform time step size δ t n 1 in fig 22b c the numerical results of the pm vm and vf for the permeable case i and impermeable fracture cases case ii at t 100 are presented a mere visual examination of these results already shows that the fracture permeability controls the flow field for the permeable fracture case the velocity at the corner of block b is higher than that on the opposite corner in block a as the fractures are closer to the open corner point in block b for the impermeable fracture case the velocity at the corner of block b is lower than that on the opposite corner in block a since block a is larger than block b and it can support flow for a longer time similar behaviors of the pressure values are observed in fig 23 a where the average pressure values of the full domain and block a and b are plotted for t 0 100 it is clear that the average pressure in block b drops faster than in block a for the impermeable case moreover fig 23b illustrates the value of pm along x 0 0 y 0 0 z 0 0 to x 1 0 y 1 0 z 1 0 line at t 25 and 75 3 7 2 algrøyna outcrop example the final example is a three dimensional fracture network built based on an outcrop map in algrøyna norway fumagalli et al 2019 the model has a size of 850 1400 600 m and contains 52 intersecting fractures the model is described in detail in berre et al 2020 the finite element mesh is discretized by tetrahedral rock matrix with n e 163 575 and triangular elements fractures with n e 329 080 see fig 24 a for more details as shown in this figure dirichlet boundary conditions are applied on two edges of the model to represent an injection and a production well all other boundaries are considered as no flow the rock matrix and the fractures are considered permeable k f n 1 10 2 m2 k f t 1 10 2 i m2 and k m 1 0 1 0 0 1 i m2 all of the remaining physical parameters are set to one and t 0 1 10 6 sec using an uniform δtn of 1 105 sec fig 24b and c show the simulation results including the pressure field the pressure iso surfaces and velocity vectors in the rock matrix at t 500 000 sec the pressure profiles along a line between two opposite corners of the model at different times are plotted in fig 24d this example illustrates the applicability of the presented eg method for a complex three dimensional fracture network with arbitrary orientations 4 conclusion this study presents the eg discretization for solving a single phase fluid flow in the fractured porous media using the mixed dimensional approach our proposed method has been tested against several published benchmarks and subsequently assessed its performance in the test cases with the heterogeneous and anisotropic matrix permeability our results illustrate that the pressure solutions resulted from the eg and dg method with the same mesh size are approximately similar furthermore the eg method enjoys the same benefits as the dg method for instance preserves local and global conservation for fluxes can handle discontinuity within and between the subdomains and has the optimal error convergence rate however it has much fewer degrees of freedom compared to that of the dg method in its classical form we note that this comparison can vary based on advanced developments of each method e g a hybridized discontinuous galerkin method or variable approximation orders besides the results of the time dependent problem for a three dimensional geometry highlight the importance of correctly capturing the discontinuities with conductivity values from barriers to highly conductive fractures present in geological media this work can be extended to multiphysics problems e g poroelastic and transport phenomena and general form of the mixed dimensional abstraction i e coupled between d and d n dimensionality where d and n are any integers and d n 0 credit authorship contribution statement t kadeethum conceptualization formal analysis software validation writing original draft h m nick conceptualization funding acquisition supervision writing review editing s lee conceptualization formal analysis supervision validation f ballarin conceptualization formal analysis software supervision validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research leading to these results has received funding from the danish hydrocarbon research and technology centre under the advanced water flooding program the computations in this work have been performed with the multiphenics library ballarin and rozza 2019 which is an extension of fenics alnaes et al 2015 for multiphysics problems we acknowledge developers and contributors to both libraries fb also thanks horizon 2020 program for grant h2020 erc cog 2015 aroma cfd project 681447 that supported the development of multiphenics supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103620 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
459,in different in situ diffusion experiments carried out in fractured crystalline rocks sorbing radionuclides have shown a behaviour that strongly differs from what is predicted by homogeneous based models their breakthrough curves are in fact often characterised by a fast first arrival and these radionuclides can penetrate surprisingly long distances deep into the matrix the heterogeneous structure of mineral distribution and porosity geometry had been offered as an explanation for these discrepancies here we use reactive transport simulations to investigate the effect of the sparse distribution of sorption sites on the breakthrough curves of sorbing radionuclides at small scale the computed breakthrough curves significantly differ from those predicted using homogeneous models for instance the early part of these curves does not show any clear separation with the corresponding part of the curve of a non sorbing tracer and a long transition zone is observed with a very smooth slope of the tailing two different upscaling strategies aimed at propagating the signal of heterogeneous retention over larger scales are proposed and demonstrated against independent solutions computed at intermediate scales the upscaling strategies are also used to show that at large scales e g the scale of interest in a safety assessment study for a deep geological repository for nuclear waste the signature of mineralogical heterogeneity is smoothed out and the heterogeneous breakthrough curve is well approximated by a homogeneous solution where the radionuclide distribution coefficient for the pure mineral phase is scaled by the mineral volume fraction however the spatial persistence of the heterogeneous signature is significant when the sorbing mineral is present in a low amount keywords mineralogical heterogeneity heterogeneous retention fractured media upscaling 1 introduction since the pioneering work of fried et al 1977 considerable efforts have been devoted to understanding characterising and modelling the retention capacity of the rock matrix for radionuclides in fact the capacity of the rock matrix to retain potentially harmful radionuclides is a sine qua non condition for the consideration of a crystalline rock system as a potential host for the geological disposal of spent nuclear fuel different analytical solutions e g neretnieks 1980 tang et al 1981 sudicky and frind 1982 and mathematical formulations e g haggerty and gorelick 1995 carrera et al 1998 cvetkovic et al 1999 have been developed to account for the influence of matrix retention on radionuclide transport in fractured media and particular emphasis has been placed on analysing the effects of matrix diffusion and sorption processes on the radionuclide breakthrough curves observed at a fracture outlet haggerty et al 2000 in all the studies mentioned above radionuclide retention properties are assumed to be constant in the whole rock matrix in parallel to the increasing availability and reliability of micro characterisation methods such as x ray micro computed tomography x μ ct or the 14c polymethylmethacrylate 14c pmma method hellmuth et al 1993 siitari kauppi 2002 recently efforts have been placed in understanding the influence of the heterogeneity of the matrix of crystalline rocks on radionuclide diffusion and retention sardini et al 2007 interpreted data of an out diffusion experiment performed on a core sample of palmottu granite eastern finland using time domain random walk simulations the calculations were carried out in an inverse fashion on a two dimensional porosity map obtained using the 14c pmma method the results of this study point out that at the investigated spatial scale the considered granite behaves as a composite system with micro cracks acting as preferential diffusive pathways and with microporous zones controlling the late time diffusive discharge the composite behaviour of the rock matrix of crystalline rocks was also pointed out by other studies based on micro characterisation data e g robinet et al 2008 iraola et al 2017 voutilainen et al 2017 at a larger scale solutions that partly account for changes in diffusion and retention properties in the rock matrix were provided by painter et al 2008 who used a time domain lagrangian framework to propose a piecewise approximation of changes in diffusion and retention properties and cvetkovic 2010 who included a layer based parameterisation of the matrix more recently trinchero et al 2016 developed a numerical framework for the dynamic update of the retention properties of the rock matrix based on changes of the background geochemical conditions however the upscaling of the afore mentioned micro scale results to a larger scale remains a largely unresolved issue robinet et al 2008 in parallel to the modelling efforts different in situ experiments have been carried out to investigate the rock rention capacity e g the true block experiment winberg et al 2003 and the long term sorption diffusion experiment ltde sd nilsson et al 2010 at the äspö hard rock laboratory hrl in sweden the water phase diffusion experiment wpde timonen and koskinen 2018 at onkalo finland and the grimsel migration experiment hadermann and heer 1996 switzerland in some if not all of these experiments models based on the assumption of homogeneity of the rock matrix fail to describe field observations particularly for sorbing radionuclides e g vilks et al 2003 the heterogeneous behaviour observed in some of these experiments e g the very long penetration depth observed for strongly sorbing radionuclides has been attributed in part to the heterogeneous and sparse distribution of sorption sites e g iraola et al 2017 in this work we use numerical simulations and two novel upscaling methodologies to gain insight into the effect that the heterogeneous distribution of active mineral grains of the rock matrix i e mineral grains that have the potential to retain radionuclides has on radionuclide breakthrough curves in dual porosity systems the sparse distribution of mineral grains is simulated using bernoulli distributions that honour the considered bulk mineral volume fraction 2 problem formulation and research questions the classical models of retention of solutes in fractures build on the work of carslaw and jaeger 1959 who considered heat transfer in slabs the mathematical solution of carslaw and jaeger was then used by neretnieks 1980 to quantify the effect of retention in crystalline fractures by matrix diffusion and sorption this was the basis for all subsequent conceptual models of tracer transport in fractures with retention e g sudicky and frind 1982 cvetkovic et al 1999 in all these approaches the micro structure and processes related to sorption are homogenised in the sense that sorption reactions are expressed by a single equilibrium distribution coefficient k d while mineralogical heterogeneity is neglected these homogenised models are a practical necessity microscopic heterogeneity in the rock matrix of crystalline rocks is clearly present both as mineralogical and structural in form of micro fissures heterogeneity yet the issue of its effect on radionuclide transport and its possible upscaling to larger scales has not been systematically studied in this work we address the implications of physical and mineralogical heterogeneity of the rock matrix on radionuclide transport and retention by considering three scales as illustrated in fig 1 i the laboratory scale ii the field scale and iii the site scale our main focus is first to better understand the potential effect of micro scale reactions and heterogeneity on tracer transport through fractures in cristalline rocks and then to explore upscaling homogenisation strategies that can be used in applications the three specific research questions we address in this work are 1 what are the potential effects of micro scale heterogeneity on reactive tracer transport through fractures in crystalline rocks 2 how can we account for the physical and mineralogical heterogeneity of the rock matrix when modelling radionuclide transport in fractured media 3 what are the most efficient and reliable upscaling homogenisation strategies thus our work assesses radionuclide transport at different spatial scales and is organised as follows in the next section we summarise evidence of micro scale heterogeneity in the crystalline matrix next in section 4 we build a detailed process based model of reactive tracer transport in a single fracture on the laboratory scale using state of art simulation tools in section 5 we present and evaluate alternative strategies for upscaling from the laboratory scale to the field scale while in section 6 we consider upscaling to the site scale finally we draw main conclusions in section 8 3 evidence of mineralogical heterogeneity a number of characterisation studies carried out in the context of safety analyses of deep geological repositories for nuclear waste points out that the rock matrix of fractured crystalline media is highly heterogeneous e g posiva 2013 skb 2013 this heterogeneity is not only related to the pore space available for diffusion which is typically a complex distribution of inter and intra granular pore space sardini et al 2007 voutilainen et al 2012 svensson et al 2018 but is also associated with the sparse distribution of chemically active mineral grains that is mineral grains that interact with the dissolved solute species for instance a mineral phase that is particularly important in this context is biotite a phyllosilicate mineral within the mica group that contains iron in its reduced form biotite and its alteration product chlorite is in fact considered as an important buffer against the infiltration of oxidative redox fronts sidborn and neretnieks 2008 macquarrie et al 2010 trinchero et al 2017 moreover biotite has a high sorption capacity and a high affinity with potentially harmful radionuclides such as cesium kyllönen et al 2014 observations made with microscopy in a granitic rock sample taken at forsmark the site selected for geological disposal of high level radioactive waste in sweden suggest that a typical biotite crystal has an average size of 400 200 μm2 0 08 mm2 although aggregates of grains up to 2 mm2 can also be found drake et al 2006 a simple visual inspection of the pothomicrographs shows that biotite grains are indeed sparsely distributed fig 2 besides direct evidence the existence of a heterogeneous and sparse distribution of sorption sites and its relevance for the diffusion patterns of sorbing radionuclides has also been inferred from the results of different in situ experiments one of these experiments is the long term sorption diffusion experiment ltde sd carried out at the äspö hard rock laboratory hrl in sweden in the experiment 21 radionuclide trace elements and one stable trace element were injected circulated and sampled for around 200 days in a closed borehole section widestrand et al 2010 the trace elements included non sorbing and sorbing tracers two borehole sections were put in contact with the tracer labelled groundwater the first section was part of a natural fracture surface whereas the second section was in the unaltered matrix rock devoid of natural fractures one of the autoradiography analyses carried out on different core samples taken from the investigated rock volume is shown in fig 3 although no independent calibration was performed to separate the contributions from the different radionuclides it is thought that the black areas of the autoradiography are generated mostly by cesium namely the isotope cs 137 although other nuclides such as ni 63 and na 22 might also partly contribute to the blackening these black areas correlate well with the distribution of areas with dark mafic minerals e g biotite and chlorite titanite and amphibole see the photography of the core in fig 3 which points out the evident control of the sparse distribution of sorbing minerals on the diffusion of strongly sorbing radionuclides 4 cesium transport in a fracture with a physically and mineralogically heterogeneous matrix the domain considered is a single fracture matrix system of dimension 0 25 m 0 1 m 4 7 10 3 m the fracture is parallel to the x z plane and has an half aperture of 2 5 10 4 m the domain was discretised with 7 600 000 regular grid cells of dimension δ 2 5 10 4 m i e same size of the fracture half aperture the heterogeneity of the matrix was simulated using a micro dfn approach similar to that described in svensson et al 2018 i e three different fracture sets of length from one to a few millimeters were used to mimic grain boundary pores as well as micro fractures that transect different mineral grains the micro dfn was generated using the finite volume groundwater flow simulator darcytools svensson et al 2010 svensson and ferry 2014 in darcytools fracture orientation follows a fisher distribution but here a random orientation was used and spatial centers are statistically independent and follow a poisson process the generation of fractures is governed by the following equation 1 n i a l d l l r e f a l l r e f a where n is the number of fractures per unit volume i 1 m3 is the intensity a is the power law exponent l m is the fracture length and lref m is the reference length which was here set to 1 m the micro dfn was constructed using the same fracture recipe used by svensson et al 2018 to model a through diffusion experiment carried out in a rock sample taken at forsmark table 1 the micro dfn was then represented onto the underlying continuum grid using the formulation provided by svensson 2001a b thus obtaining dfn derived values of porosity and pore diffusivity fig 4 the fracture water volumetric flux was set to 10 8 m s and a first calculation was run where a non sorbing tracer was injected along the inlet boundary of the fracture for a short period of time this simulation and all the numerical calculations presented in this section were run on the supercomputer juqueen of the jülich supercomputing centre stephan and docter 2015 using the reactive transport code pflotran lichtner et al 2013 hammond et al 2014 snapshots of tracer concentration computed after 1 yr and 8 yr from the beginning of the injection are shown in fig 5 the finger shaped penetration profiles are related to preferential diffusive pathways which in turn correspond to zones of the matrix with higher intensity of micro fractures and grain boundary pores the breakthrough curve computed at the fracture outlet was fitted with sudicky s analytical solution sudicky and frind 1982 by setting the matrix porosity to the average value of the heterogeneous field ϕ 2 2 10 3 and using pore diffusivity as the only calibration parameter the resulting fit which was obtained with a value of pore diffusivity d p 3 5 10 13 m2 s is shown in fig 6 the late time deviation from the 3 2 slope of the breakthrough curve is due to the limited extension of the matrix the generally good agreement between the heterogeneous model and the homogeneous based analytical solution indicates that the behaviour of a non sorbing tracer can be described using a homogeneous model parameterised based on equivalent parameters in the reactive transport calculations a typical littorina type water e g trinchero et al 2017 and references therein was used as resident water the composition of the groundwater is listed in table 2 the same water containing cesium in trace concentration 1 10 8 mol l was injected at the inlet of the fracture for a short period of time cesium and the other cations sorb onto the available biotite grains via cation exchange the sorption model i e cation exchange reactions and related selectivity coefficients see table 3 is taken from kyllönen et al 2014 who assumed three different types of ion exchange sites similar to the model of bradbury and baeyens 2000 for illite sites on basal planes 95 abundance interlayer sites on crystal edges frayed edge sites fes 0 02 abundance and a third intermediate type site 5 abundance considering a mineral density of 3 000 kg m min 3 the cation exchange capacity of the exchanger is c e c min 48 9 eq m min 3 subscript min indicates here mineral two random uncorrelated realisations of biotite grains were generated in the first realisation denoted as model a ma around 8 of the grid cells of the matrix were assumed to contain biotite with a unitary volume fraction ω m min 3 m b 3 where b stands for bulk whereas in the second realisation model b mb a twofold biotite content was considered fig 7 the distribution of biotite was used to define the bulk cation exchange capacity of each grid cell c e c b c e c min ω no sorption sites are available in the fracture snapshots of the distribution of aqueous and sorbed cesium computed with mb after 30 yr from the beginning of the injection are shown in fig 8 compared to the non sorbing tracer fig 5 the penetration of cesium is much more limited the spots with low cesium aqueous concentration and high concentration of cesium in the solid phase correspond to biotite grains the cesium breakthrough curves computed for ma and mb are shown in fig 9 from a simple analysis of these curves the following conclusion can be drawn 1 unlike what was observed for the non sorbing tracer both breakthrough curves clearly deviate from the typically expected homogeneous behaviour 2 the tailing of the breakthrough curves is characterised by a long transition zone with the slope of the tailing gradually changing from a value of 1 2 to the 3 2 slope typically observed in dual porosity systems haggerty et al 2000 3 when less biotite is available in the matrix ma a shorter transition zone and a lower level of the late time tailing is observed 4 unlike what is expected considering a homogeneous matrix at early times no clear separation is observed between the cesium breakthrough curves and the breakthrough curve of the non sorbing tracer 5 compared to the non sorbing tracer the cesium breakthrough curves have a lower peak which is observed at slightly earlier time this evidence is also in contrast with what is expected when assuming a homogeneous distribution of sorption sites when the peak of the breakthrough curve is clearly displaced to later times 5 upscaling of heterogeneous retention the sketch of fig 1 shows a gross representation of the spatial scales that are typically assessed when studying radionuclide transport in fractured media drill cores with isolated fractures are sometimes used to carry out tracer test experiments in the lab e g hölttä et al 2008 tachi et al 2018 this spatial scale is denoted here as lab scale and can be roughly related with a characteristic length of 0 1 to 1 m in situ experiments such as those mentioned and discussed in sections 1 and 3 are used to investigate e g rock matrix retention properties at depth this scale denoted as field scale spans characteristic lengths from 1 to 10 m approximately in safety assessment studies for deep geological disposal of nuclear waste the scale of interest denoted here as site scale has a characteristic length of a few hundred meters to a few kilometers i e the distance travelled by a radionuclide released at repository depth and discharged somewhere in the surface system in this chapter we assess how laboratory tracer test data for sorbing nuclides can be upscaled to the field scale in the next chapter we also evaluate how the signature of mineralogical heterogeneity propagates and affects radionuclide transport and retention at the site scale 5 1 mineralogically heterogeneous models a single fracture matrix system of dimension 2 5 m 0 1 m is discretised using 4 000 000 regular grid cells of size δ 2 5 10 4 m the fracture is parallel to the x axis meaning that its length is 2 5 m all the parameters describing the system are specified in table 4 notice that the models discussed in this section are 2d a sensitivity analysis to adding the third dimension was carried out and the related results showed no significant change with the results presented hereafter sorption properties in the matrix are described by the following bernoulli distribution 2 f k p p k 1 p 1 k f o r k 0 1 where k is the local mineral volume fraction i e volume fraction of the sorbing mineral in the grid cell which is 1 if the mineral is present and 0 otherwise and p is the bulk mineral volume fraction in the matrix if the mineral is present a linear isotherm sorption model is applied with a distribution coefficient k d g 5 3 l kg which corresponds to a retardation factor r g 1 600 the calculations were carried out in the jureca booster module jülich supercomputing centre 2018 of the jülich supercomputing centre using pflotran a generic sorbing radionuclide was injected as a short pulse at the inlet of the fracture two observation points were included one at a distance of 0 25 m from the fracture inlet and the second at the fracture outlet 2 5 m the breakthrough curve computed in the first observation point is representative of lab scale while the second observation point simulates a breakthrough curve observed at field scale fig 10 shows the breakthrough curve computed at lab scale for a model with p 16 in the same plot homogeneous breakthrough curves computed using tang s solution tang et al 1981 and different retardation values in the matrix are also shown the breakthrough curve simulated with the 2d mineralogically heterogeneous model is characterised by similar features as those already observed and discussed for the 3d physically and mineralogically heterogeneous models section 4 for instance at early times there is no clear separation between the breakthrough curve of the heterogeneous model and the homogeneous solution for a non sorbing radionuclide while all the homogeneous curves computed with r 1 are shifted to later times also analogously to what was discussed in section 4 the tailing of the heterogeneous breakthrough curve is characterised by a transition zone with the slope log log scale being first very smooth and then approaching gradually the 3 2 slope the results obtained at field scale fig 11 point out that the signature of mineralogical heterogeneity propagates and has a significant effect over larger scales although the shape of the heterogeneous breakthrough curve is now smoothed out and the transition in the slope is no longer evident the curve still exhibits a significant early arrival and an anomalous rising limb interestingly the declining limb of the curve is reasonably well described by the homogeneous solution with k d k d g p however as for the lab scale none of the homogeneous solutions are able to capture the full shape of the curve the need to investigate how the heterogeneous signal propagates over larger spatial scales and practical limitations to further extend the heterogeneous mechanistic models motivates the development of upscaling strategies which are detailed in the next two sub sections 5 2 upscaling strategy i direct sampling ds of the retention time distribution rtd the two upscaling approaches proposed hereafter make use of standard concepts used in time domain random walk tdrw methods the calculations presented hereafter were carried out using the computer code marfa painter et al 2008 painter and mancillas 2013 the basic spatial unit used here is defined as a segment a segment is typically associated to a part of a particle pathline that lies in a fracture and is delimited by the intersection with two adjacent fractures nevertheless this definition is not stringent meaning that different and alternative conceptual representations of a segment can be envisaged here we define a segment as the fracture length sampled by a lab scale tracer test experiment the unit response of a lab scale tracer test can be expressed as painter et al 2008 3 f t r a n t 0 0 f r e t t τ β f β τ β τ f τ τ d τ d β where τ y is the groundwater travel time in the fracture segment with related probability density function fτ t τ t r e t 0 is the retention time in the matrix and fret is its corresponding probability density function denoted here as retention time distribution and β y m is a hydrodynamic control parameter denoted as transport resistance defined as e g cvetkovic et al 1999 4 β τ 0 τ d θ b θ where b m is the fracture half aperture and θ is the variable of integration the integration is along the segment notice that β and τ are strongly correlated which explains why the distribution of β is conditional on τ we consider here a simplified model denoted with superscript sm characterised by f τ δ τ τ 0 and b b 0 for this model the following relationship holds 5 f t r a n s m t f r e t s m t τ 0 by equating eqs 5 and 3 we obtain an equivalent retention time distribution which can be seen as a non parameteric function that lumps together all the random processes that might affect a particle i e in plane dispersion matrix diffusion and retention into either fracture surface or in the rock matrix f r e t s m can be directly inferred from the lab scale breakthrough curve this is why the proposed approach is denoted as direct sampling ds approach f r e t s m is then used in tdrw simulations to route particles along pathlines made up of n segments under the basic premise that each segment is characterised by the same hydrodynamic processes i e same distribution of τ and β the practical implementation of the upscaling exercise is carried out using the following stepwise approach 1 the breakthrough curve of the sorbing radionuclide obtained at 0 25 m distance from the inlet fig 10 is used to extract f r e t s m 2 for the ith particle the retention time t r e t t t r a n τ 0 is sampled from f r e t s m and the particle clock is set to t c l k i j t c l k i j 1 t r e t τ 0 notice that index j indicates the fracture segment and that t c l k i 0 0 i e the particle clock is set to 0 at the inlet segment 3 for the ith particle step 2 is repeated ten times until obtaining the particle arrival time t ar i at the end of a pathline of length 2 5 m the cumulative breakthrough curve is computed from all the particle arrival times and the instantaneous breakthrough curve is reconstructed using a kernel based method painter et al 2008 the result of the tdrw simulation are compared with the breakthrough curve obtained using pflotran at a distance of 2 5 m from the inlet fig 11 the results of the exercise are shown in fig 12 the small fluctuations observed in the marfa solution at late times are related to statistical noise intrinsic in particle based monte carlo methods yet the agreement between the breakthrough curve computed with marfa and the synthetic solution obtained with pflotran is very good which indicates that the heterogeneous signal observed at lab scale and affected by heterogeneous retention can be extracted and successfully convoluted at larger scale this evidence motivates the development of a parameteric upscaling strategy next sub section that has a broader application and is not only limited to pathlines with fixed fracture aperture 5 3 upscaling strategy ii three parallel pathway model tppm different analytical solutions were derived for fret e g tang et al 1981 sudicky and frind 1982 cvetkovic 2010 here we will use the simplest solution which is based on the assumption of a homogeneous infinite matrix tang et al 1981 and which expresses the retention time distribution as 6 f r e t h t r e t κ β 2 π t r e t 3 2 exp κ 2 β 2 4 t r e t where h is the heaviside function in eq 6 κ m2 y 0 5 is a material parameter group defined as κ ϕ d e r d p ϕ 3 r we assume here that mass discharge at the fracture outlet is the linear combination of three parallel pathways 7 f t r a n t α f t r a n n s t β f t r a n m s t γ f t r a n h s t where ftran is defined as in eq 3 and α β and γ are three coefficients that provide the relative amount of mass that is routed through each of the three pathways mass conservation is ensured by setting 8 α β γ 1 the three pathways are characterised by the same transport resistance while κ varies as each pathway is characterised by different retention properties in the non sorbing pathway superscript ns there is no sorption r 1 in the mildy sorbing pathway superscript ms retardation is equal to half of the retardation in the mineral grains r r g 2 whereas in the highly sorbing pathway superscript hs r r g the corresponding retention time distributions are defined as f r e t n s f r e t m s and f r e t h s no mass exchange is considered between pathways before discussing details of the upscaling approach it is worthwhile noting that evidence of a similar composite behaviour of the heterogeneous rock matrix has been found in different laboratory and field experiments tsukamoto and ohe 1993 johansson et al 1998 nilsson et al 2010 moreover iraola et al 2017 recently used a two parallel pathways model to qualitatively interpret results of the ltde sd experiment whereas trinchero et al 2018 pointed out that the transport of oxygen in a granitic rock is characterised by a composite behaviour with the average oxygen penetration profile being the result of multiple parallel diffusive pathway characterised by different reactivity the three parallel pathway model tppm was implemented in marfa using a conditional sampling scheme explained in detail in appendix a the models used in this upscaling exercise are analogous to that used in section 5 2 and illustrated in section 5 1 however here different mineral distributions are considered which span from p 04 to p 5 i e respectively 4 and 50 of sorbing mineral volume fraction the exercise is carried out using a stepwise approach step 1 calibration the breakthrough curves of the lab scale experiments are best fitted using marfa and the tppm a single segment is used and the best fit is performed by mean squared error minimisation step 2 validation for each considered mineral distribution the coefficients estimated in step 1 are used to run a marfa calculation with the tppm and 10 equally spaced segments the results of step 2 are compared with the breakthrough curves obtained using pflotran and the related mineral distribution at a distance of 2 5 m from the inlet the results of step 1 are shown in fig 13 and the coefficients estimated for each considered mineral distribution are plotted in fig 14 the tppm model describes well the lab scale breakthrough curves which indicates that consistently with what was observed in previous studies e g robinet et al 2008 iraola et al 2017 trinchero et al 2018 the medium indeed behaves as a composite system until a volume fraction of the sorbing mineral of around 15 the system is fairly well approximated by a dual parallel pathway model built as a linear combination of the non sorbing and the mildly sorbing pathway for a higher amount of sorbing mineral an additional degree of freedom i e the highly sorbing pathway is needed to properly capture the composite behaviour of the medium the results of step 2 are shown in fig 15 slight differences between the pflotran and marfa solutions are observed in the rising limb of the breakthrough curve computed for a very low amount of sorbing mineral p 04 for all the other breakthrough curves the agreement between the synthetic experiment and the upscaled solution is very good this suggests that as already observed in section 5 2 when the signal of a lab scale experiment affected by heterogeneous retention is properly captured it can then be successfully propagated along the pathway to describe the signal over a much larger scale 6 large scale modelling 6 1 models with constant transport properties in sections 5 2 and 5 3 we have demonstrated that the signal observed in a tracer test analysed at lab scale and affected by heterogenous retention can be extracted and propagated over larger scales in this section we take advantage of this methodological approach to investigate the effect that heterogeneous retention in the matrix due to the sparse availability of sorption sites has on radionuclide transport at site scale i e the scale of interest in a safety assessment study for a deep geological repository for nuclear waste we consider a pathline of 250 m length constituted by 1000 fracture segments of 0 25 m length each fracture segment and the surrounding matrix are described by the parameters in table 4 it is also assumed that the amount of sorbing mineral is p 16 the propagation of the heterogeneous signal is done using tppm notice that analogous results have been obtained using the ds approach results are not shown here for the sake of brevity the heterogeneous breakthrough curve is compared with three homogeneous calculations one where no sorption is included a second where homogeneous retention is set with r 1 600 and a third with r 250 i e k d k d g p in the whole matrix the comparison of the different breakthrough curves is shown in fig 16 at large scale the heterogeneous signature is no longer evident as already noticed in the previous sections the breakthrough curve computed with the homogeneous model and with r 1 600 i e retardation equal to retardation in the pure mineral phase strongly overestimates the retardation capacity of the matrix interestingly the heterogenous breakthrough curve is very well described by a model where the distribution coefficient in the pure mineral phase is scaled by the mineral volume fraction 6 2 spatial persistence of the heterogeneous signature we have discussed earlier that at small scale the sparse distribution of mineral grains leads to a heterogeneous signature which affects the early part rising limb of the breakthrough curves of sorbing radionuclides at large scale the deviation from the homogeneous based solution is smoothed out we aim at assessing here the spatial persistence of this heterogeneous signature to this end the five different mineral distributions of section 5 3 are considered i e p 04 to p 5 for each mineral distribution the tppm model is run with the related coefficients see fig 14 along a pathline made up of n equally time spaced segments each segment being equal to the lab scale model of section 5 1 these models are here denoted as heterogeneous models a corresponding homogeneous calculation homogeneous model from now on was also run with k d k d g p and the relative difference in the first arrival time was computed as t h o m t h e t t h e t where thom and thet t is the time when 1 of the total injected mass reaches the considered segment outlet for the homogeneous and heterogeneous model respectively fig 17 shows the relative error in the estimate of the first arrival time as a function of cumulative β for the five considered mineral distributions it is interesting to notice that for a high amount of sorbing mineral e g p 0 5 the homogeneous model largely over predicts the first arrival time at small scales but the solution approaches the homogeneous breakthrough curves at relatively short distances for a small amount of sorbing mineral the error at small scales is lower but the spatial persistence of the heterogeneous signature is much longer e g errors above 20 for β 50 000 y m at large scales the results of all the tested heterogeneous models approach those of the equivalent homogeneous models although a slightly negative error is observed as the homogeneous model slightly under predicts the early arrival time see e g the rising limb of the two curves in fig 16 the significance of biotite for the retention of potentially harmful radionuclides particularly cesium e g cs 137 and cs 135 and sodium has been discussed early in this manuscript at forsmark biotite is found to be present in a relatively low amount e g from 0 8 to 8 vol in granitic rock drake et al 2006 in the last swedish safety analysis for a deep geological repository for spent nuclear fuel sr site typical values of cumulative β associated with failing canisters varied from about 1 5 104 y m to 105 y m skb 2010 it turns out that the significantly long persistence of the heterogeneous signature pointed out in this work might have implications for calculations of radionuclide transport carried out at the scale of a typical safety assessment study 6 3 models based on different transport classes radionuclide transport calculations carried out at the site scale entail the need to account for different rock types drake et al 2006 that are characterised by specific retention properties which are in turn related to the different mineralogical properties of the considered lithological units crawford 2008 this heterogeneity is typically treated by defining a finite number of transport classes with related retention properties that are sampled statistically conditioned on the lithological unit to which the given fracture segment belongs e g hartley et al 2018 here the effect of longitudinal heterogeneity is assessed in an idealised way by defining five simplified transport classes characterised by different amount of sorbing mineral tc1 with p 04 tc2 with p 08 tc3 with p 16 tc4 with p 32 and tc5 with p 5 the transport calculation is carried along a pathline made up of 1000 equally time spaced segments each segment being equal to the lab scale model of section 5 1 furthermore each segment is related to one of the five transport classes and the tppm is used with the coefficients shown in fig 14 the five different transport classes were distributed along the considered transport pathway using a sequential indicator simulation that was carried out with the computer code sgems remy et al 2009 equal fractions of each transport class were considered i e 20 each which gives an average amount of sorbing mineral equal to p 0 22 a spherical isotropic semivariogram with a range equal to 10 m was used an additional case was also included in the analysis in which the same proportion of transport classes was distributed in an uncorrelated way along the pathway the two resulting distributions of transport classes are shown in fig 18 the two models are here defined as correlated and uncorrelated models in fig 19 the breakthrough curve computed with the correlated model is compared with the related breakthrough curves obtained using three different homogeneous models one where sorption in the matrix is assumed to be equal to sorption in the pure mineral phase r 1 600 a second where no sorption in the matrix is considered and a third where the distribution coefficient is scaled by the average mineral volume fraction k d k d g p i e r 352 results are very similar to those already discussed in section 6 1 the model with no sorption and the model with sorption equal to sorption in the pure mineral phase respectively significantly underestimate and overestimate the retention capacity of crystalline rocks on the contrary the equivalent homogeneous model based on the distribution coefficient scaled by the average mineral volume fraction arithmetic mean provides a good approximation to the heterogeneous solution although the rising limb of the curve is displaced to earlier times a similar slight underestimation of the early arrival was already discussed in the previous section see also fig 17 the results of the correlated and the uncorrelated heterogeneous models were almost identical results not shown here which indicates that longitudinal correlation in transport classes has a modest effect on radionuclide transport this finding provides support for the methodology employed in previous safety cases to populate fracture segments with transport classes sampled statistically without considering any spatial correlation see e g hartley et al 2018 7 discussion micro characterisation studies based on e g micro computed tomography or digital autoradiography have pointed out that in granitic rocks sorbing minerals are sparsely distributed and this heterogeneous distribution leads to diffusion patterns that differ from what is typically predicted by homogeneous based models this heterogeneous behaviour has been observed in different in situ diffusion experiments the available experimental evidence has led us to formulate three research questions see section 2 based on the results of this work the following answers can be given i at small scale the sparse distribution of sorption sites which is intrinsically related to the underlying micro scale heterogeneity has a significant impact on diffusive patterns leading to heterogeneous breakthrough curves for sorbing radionuclides ii these heterogeneous breakthrough curves can be used to infer non parameteric retention time distribution functions or alternatively to calibrate the parameters of a model based on parallel diffusive pathways that account for the underlying micro scale heterogeneity and iii at large scale the heterogeneous breakthrough curves are smoothed out and the breakthrough curves of sorbing radionuclides are well described by homogeneous solutions based on equivalent distribution coefficients the proposed upscaling procedures are based on idealised segmented pathways characterised by constant hydrodynamic properties i e τ and β real fracture systems are obviously much more complex and might show strong contrasts in velocity and aperture between nearby fractures e g skb 2013 moreover fractures might have experienced different hydrothermal alteration events that could have altered the properties of the rock matrix close the flowing fracture therefore additional work is needed to make the proposed framework applicable to study contaminant transport in large scale fractured media a possible approach would be to use a piecewise approximation in which fracture segments are grouped into transport classes and the related rtd functions are derived from reactive transport models as shown in section 5 1 a similar approach based on transport classes was used in the in last finnish safety assessment study of olkiluoto the designated location for a repository of spent nuclear fuel to account for different radionuclide retention properties of different types of rock matrices posiva 2012 however it is worthwhile pointing out here that one of the main outcomes of this work is that at large scale the heterogeneous and sparse availability of sorption sites can be well described by an equivalent distribution coefficient this simple yet practical approach is of direct relevance for any existing radionuclide transport model in fractured crystalline rock another pending task is to elucidate the mutual interplay between fracture internal aperture heterogeneity and heterogeneous retention which could be done by combing theoretical models e g cvetkovic and cheng 2008 with direct simulations the simulations would focus on resolving both the complex topology of a real fracture and the surrounding rock matrix which in turn should include an explicit description of the sparsely distributed sorption sites 8 summary and conclusions in the first part of this work we have presented a detailed process based simulation of a radionuclide tracer test in a synthetic fracture matrix system to analyse the influence of micro scale heterogeneity on breakthrough curves of sorbing radionuclides the results of the reactive transport simulations point out that at small scale e g the scale representative of a laboratory tracer test experiment the breakthrough curve is characterised by a shape that significantly differs from homogeneous based solutions with a very early arrival and a smooth slope of the tailing at intermediate times this heteorgeneous signature is caused by the sparse distribution of sorption sites starting from this evidence and using a formulation based on retention time distributions we have provided two different upscaling strategies the main general conclusions can be summarised as follows 1 the heterogeneous radionuclide breakthrough curve observed at small scale e g lab scale can be used to extract the radionuclide retention time distribution in the matrix 2 using the retention time distribution extracted from point 1 the heterogeneous signal can be successfully propagated along larger scale 3 at large scales e g site scale the heterogeneous signature of the smal scale signal is smoothed out and the breakthrough curve is well approximated by a homogeneous model in which the distribution coefficient of the sorbing mineral grains is scaled by the mineral volume fraction 4 the heterogeneous signature is shown to persist particularly when the sorbing mineral is present in low amount changes in retention properties along a transport pathway have also been assessed by defining simplified transport classes characterised by different amount of sorbing mineral the results of these longitudinally heterogeneous models have shown that at large scale the resulting radionuclide breakthrough curve can fairly well be described by an equivalent homogeneous model based on the distribution coefficient of the pure mineral phase scaled by the average amount of sorbing mineral along the pathway the spatial correlation of the different transport classes along the transport pathway was shown to have a negligible effect on the results author contribution p t developed the numerical models p t v c and j o s interpreted the results all authors contributed to and reviewed the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests acknowledgements pt and vc thank the swedish nuclear fuel and waste management company skb for the financial support db and gd acknowledge financial support provided by the german federal ministry of education and research bmbf grant agreement 02nuk053a and the initiative and networking fund of the helmholtz association hgf grant so 093 within the collaborative project icross the authors gratefully acknowledge the computing time granted through jara on the supercomputers juqueen and jureca at forschungszentrum jlich the paper has benefited from insightful comments by johan byegård henrik drake kersti nilsson marja siitari kauppi and three anonimous reviewers appendix a implementation of the tppm in marfa for each particle that is routed along a segment a random number is drawn r u 0 1 and the conditional sampling is performed following the pseudocode listed below the correct implementation of the conditional sampling scheme has been verified by comparing the results of a marfa transport calculation carried out along a single segment with the results obtained using an analytical solution based on eqs 6 8 results are not shown here for the sake of brevity 
459,in different in situ diffusion experiments carried out in fractured crystalline rocks sorbing radionuclides have shown a behaviour that strongly differs from what is predicted by homogeneous based models their breakthrough curves are in fact often characterised by a fast first arrival and these radionuclides can penetrate surprisingly long distances deep into the matrix the heterogeneous structure of mineral distribution and porosity geometry had been offered as an explanation for these discrepancies here we use reactive transport simulations to investigate the effect of the sparse distribution of sorption sites on the breakthrough curves of sorbing radionuclides at small scale the computed breakthrough curves significantly differ from those predicted using homogeneous models for instance the early part of these curves does not show any clear separation with the corresponding part of the curve of a non sorbing tracer and a long transition zone is observed with a very smooth slope of the tailing two different upscaling strategies aimed at propagating the signal of heterogeneous retention over larger scales are proposed and demonstrated against independent solutions computed at intermediate scales the upscaling strategies are also used to show that at large scales e g the scale of interest in a safety assessment study for a deep geological repository for nuclear waste the signature of mineralogical heterogeneity is smoothed out and the heterogeneous breakthrough curve is well approximated by a homogeneous solution where the radionuclide distribution coefficient for the pure mineral phase is scaled by the mineral volume fraction however the spatial persistence of the heterogeneous signature is significant when the sorbing mineral is present in a low amount keywords mineralogical heterogeneity heterogeneous retention fractured media upscaling 1 introduction since the pioneering work of fried et al 1977 considerable efforts have been devoted to understanding characterising and modelling the retention capacity of the rock matrix for radionuclides in fact the capacity of the rock matrix to retain potentially harmful radionuclides is a sine qua non condition for the consideration of a crystalline rock system as a potential host for the geological disposal of spent nuclear fuel different analytical solutions e g neretnieks 1980 tang et al 1981 sudicky and frind 1982 and mathematical formulations e g haggerty and gorelick 1995 carrera et al 1998 cvetkovic et al 1999 have been developed to account for the influence of matrix retention on radionuclide transport in fractured media and particular emphasis has been placed on analysing the effects of matrix diffusion and sorption processes on the radionuclide breakthrough curves observed at a fracture outlet haggerty et al 2000 in all the studies mentioned above radionuclide retention properties are assumed to be constant in the whole rock matrix in parallel to the increasing availability and reliability of micro characterisation methods such as x ray micro computed tomography x μ ct or the 14c polymethylmethacrylate 14c pmma method hellmuth et al 1993 siitari kauppi 2002 recently efforts have been placed in understanding the influence of the heterogeneity of the matrix of crystalline rocks on radionuclide diffusion and retention sardini et al 2007 interpreted data of an out diffusion experiment performed on a core sample of palmottu granite eastern finland using time domain random walk simulations the calculations were carried out in an inverse fashion on a two dimensional porosity map obtained using the 14c pmma method the results of this study point out that at the investigated spatial scale the considered granite behaves as a composite system with micro cracks acting as preferential diffusive pathways and with microporous zones controlling the late time diffusive discharge the composite behaviour of the rock matrix of crystalline rocks was also pointed out by other studies based on micro characterisation data e g robinet et al 2008 iraola et al 2017 voutilainen et al 2017 at a larger scale solutions that partly account for changes in diffusion and retention properties in the rock matrix were provided by painter et al 2008 who used a time domain lagrangian framework to propose a piecewise approximation of changes in diffusion and retention properties and cvetkovic 2010 who included a layer based parameterisation of the matrix more recently trinchero et al 2016 developed a numerical framework for the dynamic update of the retention properties of the rock matrix based on changes of the background geochemical conditions however the upscaling of the afore mentioned micro scale results to a larger scale remains a largely unresolved issue robinet et al 2008 in parallel to the modelling efforts different in situ experiments have been carried out to investigate the rock rention capacity e g the true block experiment winberg et al 2003 and the long term sorption diffusion experiment ltde sd nilsson et al 2010 at the äspö hard rock laboratory hrl in sweden the water phase diffusion experiment wpde timonen and koskinen 2018 at onkalo finland and the grimsel migration experiment hadermann and heer 1996 switzerland in some if not all of these experiments models based on the assumption of homogeneity of the rock matrix fail to describe field observations particularly for sorbing radionuclides e g vilks et al 2003 the heterogeneous behaviour observed in some of these experiments e g the very long penetration depth observed for strongly sorbing radionuclides has been attributed in part to the heterogeneous and sparse distribution of sorption sites e g iraola et al 2017 in this work we use numerical simulations and two novel upscaling methodologies to gain insight into the effect that the heterogeneous distribution of active mineral grains of the rock matrix i e mineral grains that have the potential to retain radionuclides has on radionuclide breakthrough curves in dual porosity systems the sparse distribution of mineral grains is simulated using bernoulli distributions that honour the considered bulk mineral volume fraction 2 problem formulation and research questions the classical models of retention of solutes in fractures build on the work of carslaw and jaeger 1959 who considered heat transfer in slabs the mathematical solution of carslaw and jaeger was then used by neretnieks 1980 to quantify the effect of retention in crystalline fractures by matrix diffusion and sorption this was the basis for all subsequent conceptual models of tracer transport in fractures with retention e g sudicky and frind 1982 cvetkovic et al 1999 in all these approaches the micro structure and processes related to sorption are homogenised in the sense that sorption reactions are expressed by a single equilibrium distribution coefficient k d while mineralogical heterogeneity is neglected these homogenised models are a practical necessity microscopic heterogeneity in the rock matrix of crystalline rocks is clearly present both as mineralogical and structural in form of micro fissures heterogeneity yet the issue of its effect on radionuclide transport and its possible upscaling to larger scales has not been systematically studied in this work we address the implications of physical and mineralogical heterogeneity of the rock matrix on radionuclide transport and retention by considering three scales as illustrated in fig 1 i the laboratory scale ii the field scale and iii the site scale our main focus is first to better understand the potential effect of micro scale reactions and heterogeneity on tracer transport through fractures in cristalline rocks and then to explore upscaling homogenisation strategies that can be used in applications the three specific research questions we address in this work are 1 what are the potential effects of micro scale heterogeneity on reactive tracer transport through fractures in crystalline rocks 2 how can we account for the physical and mineralogical heterogeneity of the rock matrix when modelling radionuclide transport in fractured media 3 what are the most efficient and reliable upscaling homogenisation strategies thus our work assesses radionuclide transport at different spatial scales and is organised as follows in the next section we summarise evidence of micro scale heterogeneity in the crystalline matrix next in section 4 we build a detailed process based model of reactive tracer transport in a single fracture on the laboratory scale using state of art simulation tools in section 5 we present and evaluate alternative strategies for upscaling from the laboratory scale to the field scale while in section 6 we consider upscaling to the site scale finally we draw main conclusions in section 8 3 evidence of mineralogical heterogeneity a number of characterisation studies carried out in the context of safety analyses of deep geological repositories for nuclear waste points out that the rock matrix of fractured crystalline media is highly heterogeneous e g posiva 2013 skb 2013 this heterogeneity is not only related to the pore space available for diffusion which is typically a complex distribution of inter and intra granular pore space sardini et al 2007 voutilainen et al 2012 svensson et al 2018 but is also associated with the sparse distribution of chemically active mineral grains that is mineral grains that interact with the dissolved solute species for instance a mineral phase that is particularly important in this context is biotite a phyllosilicate mineral within the mica group that contains iron in its reduced form biotite and its alteration product chlorite is in fact considered as an important buffer against the infiltration of oxidative redox fronts sidborn and neretnieks 2008 macquarrie et al 2010 trinchero et al 2017 moreover biotite has a high sorption capacity and a high affinity with potentially harmful radionuclides such as cesium kyllönen et al 2014 observations made with microscopy in a granitic rock sample taken at forsmark the site selected for geological disposal of high level radioactive waste in sweden suggest that a typical biotite crystal has an average size of 400 200 μm2 0 08 mm2 although aggregates of grains up to 2 mm2 can also be found drake et al 2006 a simple visual inspection of the pothomicrographs shows that biotite grains are indeed sparsely distributed fig 2 besides direct evidence the existence of a heterogeneous and sparse distribution of sorption sites and its relevance for the diffusion patterns of sorbing radionuclides has also been inferred from the results of different in situ experiments one of these experiments is the long term sorption diffusion experiment ltde sd carried out at the äspö hard rock laboratory hrl in sweden in the experiment 21 radionuclide trace elements and one stable trace element were injected circulated and sampled for around 200 days in a closed borehole section widestrand et al 2010 the trace elements included non sorbing and sorbing tracers two borehole sections were put in contact with the tracer labelled groundwater the first section was part of a natural fracture surface whereas the second section was in the unaltered matrix rock devoid of natural fractures one of the autoradiography analyses carried out on different core samples taken from the investigated rock volume is shown in fig 3 although no independent calibration was performed to separate the contributions from the different radionuclides it is thought that the black areas of the autoradiography are generated mostly by cesium namely the isotope cs 137 although other nuclides such as ni 63 and na 22 might also partly contribute to the blackening these black areas correlate well with the distribution of areas with dark mafic minerals e g biotite and chlorite titanite and amphibole see the photography of the core in fig 3 which points out the evident control of the sparse distribution of sorbing minerals on the diffusion of strongly sorbing radionuclides 4 cesium transport in a fracture with a physically and mineralogically heterogeneous matrix the domain considered is a single fracture matrix system of dimension 0 25 m 0 1 m 4 7 10 3 m the fracture is parallel to the x z plane and has an half aperture of 2 5 10 4 m the domain was discretised with 7 600 000 regular grid cells of dimension δ 2 5 10 4 m i e same size of the fracture half aperture the heterogeneity of the matrix was simulated using a micro dfn approach similar to that described in svensson et al 2018 i e three different fracture sets of length from one to a few millimeters were used to mimic grain boundary pores as well as micro fractures that transect different mineral grains the micro dfn was generated using the finite volume groundwater flow simulator darcytools svensson et al 2010 svensson and ferry 2014 in darcytools fracture orientation follows a fisher distribution but here a random orientation was used and spatial centers are statistically independent and follow a poisson process the generation of fractures is governed by the following equation 1 n i a l d l l r e f a l l r e f a where n is the number of fractures per unit volume i 1 m3 is the intensity a is the power law exponent l m is the fracture length and lref m is the reference length which was here set to 1 m the micro dfn was constructed using the same fracture recipe used by svensson et al 2018 to model a through diffusion experiment carried out in a rock sample taken at forsmark table 1 the micro dfn was then represented onto the underlying continuum grid using the formulation provided by svensson 2001a b thus obtaining dfn derived values of porosity and pore diffusivity fig 4 the fracture water volumetric flux was set to 10 8 m s and a first calculation was run where a non sorbing tracer was injected along the inlet boundary of the fracture for a short period of time this simulation and all the numerical calculations presented in this section were run on the supercomputer juqueen of the jülich supercomputing centre stephan and docter 2015 using the reactive transport code pflotran lichtner et al 2013 hammond et al 2014 snapshots of tracer concentration computed after 1 yr and 8 yr from the beginning of the injection are shown in fig 5 the finger shaped penetration profiles are related to preferential diffusive pathways which in turn correspond to zones of the matrix with higher intensity of micro fractures and grain boundary pores the breakthrough curve computed at the fracture outlet was fitted with sudicky s analytical solution sudicky and frind 1982 by setting the matrix porosity to the average value of the heterogeneous field ϕ 2 2 10 3 and using pore diffusivity as the only calibration parameter the resulting fit which was obtained with a value of pore diffusivity d p 3 5 10 13 m2 s is shown in fig 6 the late time deviation from the 3 2 slope of the breakthrough curve is due to the limited extension of the matrix the generally good agreement between the heterogeneous model and the homogeneous based analytical solution indicates that the behaviour of a non sorbing tracer can be described using a homogeneous model parameterised based on equivalent parameters in the reactive transport calculations a typical littorina type water e g trinchero et al 2017 and references therein was used as resident water the composition of the groundwater is listed in table 2 the same water containing cesium in trace concentration 1 10 8 mol l was injected at the inlet of the fracture for a short period of time cesium and the other cations sorb onto the available biotite grains via cation exchange the sorption model i e cation exchange reactions and related selectivity coefficients see table 3 is taken from kyllönen et al 2014 who assumed three different types of ion exchange sites similar to the model of bradbury and baeyens 2000 for illite sites on basal planes 95 abundance interlayer sites on crystal edges frayed edge sites fes 0 02 abundance and a third intermediate type site 5 abundance considering a mineral density of 3 000 kg m min 3 the cation exchange capacity of the exchanger is c e c min 48 9 eq m min 3 subscript min indicates here mineral two random uncorrelated realisations of biotite grains were generated in the first realisation denoted as model a ma around 8 of the grid cells of the matrix were assumed to contain biotite with a unitary volume fraction ω m min 3 m b 3 where b stands for bulk whereas in the second realisation model b mb a twofold biotite content was considered fig 7 the distribution of biotite was used to define the bulk cation exchange capacity of each grid cell c e c b c e c min ω no sorption sites are available in the fracture snapshots of the distribution of aqueous and sorbed cesium computed with mb after 30 yr from the beginning of the injection are shown in fig 8 compared to the non sorbing tracer fig 5 the penetration of cesium is much more limited the spots with low cesium aqueous concentration and high concentration of cesium in the solid phase correspond to biotite grains the cesium breakthrough curves computed for ma and mb are shown in fig 9 from a simple analysis of these curves the following conclusion can be drawn 1 unlike what was observed for the non sorbing tracer both breakthrough curves clearly deviate from the typically expected homogeneous behaviour 2 the tailing of the breakthrough curves is characterised by a long transition zone with the slope of the tailing gradually changing from a value of 1 2 to the 3 2 slope typically observed in dual porosity systems haggerty et al 2000 3 when less biotite is available in the matrix ma a shorter transition zone and a lower level of the late time tailing is observed 4 unlike what is expected considering a homogeneous matrix at early times no clear separation is observed between the cesium breakthrough curves and the breakthrough curve of the non sorbing tracer 5 compared to the non sorbing tracer the cesium breakthrough curves have a lower peak which is observed at slightly earlier time this evidence is also in contrast with what is expected when assuming a homogeneous distribution of sorption sites when the peak of the breakthrough curve is clearly displaced to later times 5 upscaling of heterogeneous retention the sketch of fig 1 shows a gross representation of the spatial scales that are typically assessed when studying radionuclide transport in fractured media drill cores with isolated fractures are sometimes used to carry out tracer test experiments in the lab e g hölttä et al 2008 tachi et al 2018 this spatial scale is denoted here as lab scale and can be roughly related with a characteristic length of 0 1 to 1 m in situ experiments such as those mentioned and discussed in sections 1 and 3 are used to investigate e g rock matrix retention properties at depth this scale denoted as field scale spans characteristic lengths from 1 to 10 m approximately in safety assessment studies for deep geological disposal of nuclear waste the scale of interest denoted here as site scale has a characteristic length of a few hundred meters to a few kilometers i e the distance travelled by a radionuclide released at repository depth and discharged somewhere in the surface system in this chapter we assess how laboratory tracer test data for sorbing nuclides can be upscaled to the field scale in the next chapter we also evaluate how the signature of mineralogical heterogeneity propagates and affects radionuclide transport and retention at the site scale 5 1 mineralogically heterogeneous models a single fracture matrix system of dimension 2 5 m 0 1 m is discretised using 4 000 000 regular grid cells of size δ 2 5 10 4 m the fracture is parallel to the x axis meaning that its length is 2 5 m all the parameters describing the system are specified in table 4 notice that the models discussed in this section are 2d a sensitivity analysis to adding the third dimension was carried out and the related results showed no significant change with the results presented hereafter sorption properties in the matrix are described by the following bernoulli distribution 2 f k p p k 1 p 1 k f o r k 0 1 where k is the local mineral volume fraction i e volume fraction of the sorbing mineral in the grid cell which is 1 if the mineral is present and 0 otherwise and p is the bulk mineral volume fraction in the matrix if the mineral is present a linear isotherm sorption model is applied with a distribution coefficient k d g 5 3 l kg which corresponds to a retardation factor r g 1 600 the calculations were carried out in the jureca booster module jülich supercomputing centre 2018 of the jülich supercomputing centre using pflotran a generic sorbing radionuclide was injected as a short pulse at the inlet of the fracture two observation points were included one at a distance of 0 25 m from the fracture inlet and the second at the fracture outlet 2 5 m the breakthrough curve computed in the first observation point is representative of lab scale while the second observation point simulates a breakthrough curve observed at field scale fig 10 shows the breakthrough curve computed at lab scale for a model with p 16 in the same plot homogeneous breakthrough curves computed using tang s solution tang et al 1981 and different retardation values in the matrix are also shown the breakthrough curve simulated with the 2d mineralogically heterogeneous model is characterised by similar features as those already observed and discussed for the 3d physically and mineralogically heterogeneous models section 4 for instance at early times there is no clear separation between the breakthrough curve of the heterogeneous model and the homogeneous solution for a non sorbing radionuclide while all the homogeneous curves computed with r 1 are shifted to later times also analogously to what was discussed in section 4 the tailing of the heterogeneous breakthrough curve is characterised by a transition zone with the slope log log scale being first very smooth and then approaching gradually the 3 2 slope the results obtained at field scale fig 11 point out that the signature of mineralogical heterogeneity propagates and has a significant effect over larger scales although the shape of the heterogeneous breakthrough curve is now smoothed out and the transition in the slope is no longer evident the curve still exhibits a significant early arrival and an anomalous rising limb interestingly the declining limb of the curve is reasonably well described by the homogeneous solution with k d k d g p however as for the lab scale none of the homogeneous solutions are able to capture the full shape of the curve the need to investigate how the heterogeneous signal propagates over larger spatial scales and practical limitations to further extend the heterogeneous mechanistic models motivates the development of upscaling strategies which are detailed in the next two sub sections 5 2 upscaling strategy i direct sampling ds of the retention time distribution rtd the two upscaling approaches proposed hereafter make use of standard concepts used in time domain random walk tdrw methods the calculations presented hereafter were carried out using the computer code marfa painter et al 2008 painter and mancillas 2013 the basic spatial unit used here is defined as a segment a segment is typically associated to a part of a particle pathline that lies in a fracture and is delimited by the intersection with two adjacent fractures nevertheless this definition is not stringent meaning that different and alternative conceptual representations of a segment can be envisaged here we define a segment as the fracture length sampled by a lab scale tracer test experiment the unit response of a lab scale tracer test can be expressed as painter et al 2008 3 f t r a n t 0 0 f r e t t τ β f β τ β τ f τ τ d τ d β where τ y is the groundwater travel time in the fracture segment with related probability density function fτ t τ t r e t 0 is the retention time in the matrix and fret is its corresponding probability density function denoted here as retention time distribution and β y m is a hydrodynamic control parameter denoted as transport resistance defined as e g cvetkovic et al 1999 4 β τ 0 τ d θ b θ where b m is the fracture half aperture and θ is the variable of integration the integration is along the segment notice that β and τ are strongly correlated which explains why the distribution of β is conditional on τ we consider here a simplified model denoted with superscript sm characterised by f τ δ τ τ 0 and b b 0 for this model the following relationship holds 5 f t r a n s m t f r e t s m t τ 0 by equating eqs 5 and 3 we obtain an equivalent retention time distribution which can be seen as a non parameteric function that lumps together all the random processes that might affect a particle i e in plane dispersion matrix diffusion and retention into either fracture surface or in the rock matrix f r e t s m can be directly inferred from the lab scale breakthrough curve this is why the proposed approach is denoted as direct sampling ds approach f r e t s m is then used in tdrw simulations to route particles along pathlines made up of n segments under the basic premise that each segment is characterised by the same hydrodynamic processes i e same distribution of τ and β the practical implementation of the upscaling exercise is carried out using the following stepwise approach 1 the breakthrough curve of the sorbing radionuclide obtained at 0 25 m distance from the inlet fig 10 is used to extract f r e t s m 2 for the ith particle the retention time t r e t t t r a n τ 0 is sampled from f r e t s m and the particle clock is set to t c l k i j t c l k i j 1 t r e t τ 0 notice that index j indicates the fracture segment and that t c l k i 0 0 i e the particle clock is set to 0 at the inlet segment 3 for the ith particle step 2 is repeated ten times until obtaining the particle arrival time t ar i at the end of a pathline of length 2 5 m the cumulative breakthrough curve is computed from all the particle arrival times and the instantaneous breakthrough curve is reconstructed using a kernel based method painter et al 2008 the result of the tdrw simulation are compared with the breakthrough curve obtained using pflotran at a distance of 2 5 m from the inlet fig 11 the results of the exercise are shown in fig 12 the small fluctuations observed in the marfa solution at late times are related to statistical noise intrinsic in particle based monte carlo methods yet the agreement between the breakthrough curve computed with marfa and the synthetic solution obtained with pflotran is very good which indicates that the heterogeneous signal observed at lab scale and affected by heterogeneous retention can be extracted and successfully convoluted at larger scale this evidence motivates the development of a parameteric upscaling strategy next sub section that has a broader application and is not only limited to pathlines with fixed fracture aperture 5 3 upscaling strategy ii three parallel pathway model tppm different analytical solutions were derived for fret e g tang et al 1981 sudicky and frind 1982 cvetkovic 2010 here we will use the simplest solution which is based on the assumption of a homogeneous infinite matrix tang et al 1981 and which expresses the retention time distribution as 6 f r e t h t r e t κ β 2 π t r e t 3 2 exp κ 2 β 2 4 t r e t where h is the heaviside function in eq 6 κ m2 y 0 5 is a material parameter group defined as κ ϕ d e r d p ϕ 3 r we assume here that mass discharge at the fracture outlet is the linear combination of three parallel pathways 7 f t r a n t α f t r a n n s t β f t r a n m s t γ f t r a n h s t where ftran is defined as in eq 3 and α β and γ are three coefficients that provide the relative amount of mass that is routed through each of the three pathways mass conservation is ensured by setting 8 α β γ 1 the three pathways are characterised by the same transport resistance while κ varies as each pathway is characterised by different retention properties in the non sorbing pathway superscript ns there is no sorption r 1 in the mildy sorbing pathway superscript ms retardation is equal to half of the retardation in the mineral grains r r g 2 whereas in the highly sorbing pathway superscript hs r r g the corresponding retention time distributions are defined as f r e t n s f r e t m s and f r e t h s no mass exchange is considered between pathways before discussing details of the upscaling approach it is worthwhile noting that evidence of a similar composite behaviour of the heterogeneous rock matrix has been found in different laboratory and field experiments tsukamoto and ohe 1993 johansson et al 1998 nilsson et al 2010 moreover iraola et al 2017 recently used a two parallel pathways model to qualitatively interpret results of the ltde sd experiment whereas trinchero et al 2018 pointed out that the transport of oxygen in a granitic rock is characterised by a composite behaviour with the average oxygen penetration profile being the result of multiple parallel diffusive pathway characterised by different reactivity the three parallel pathway model tppm was implemented in marfa using a conditional sampling scheme explained in detail in appendix a the models used in this upscaling exercise are analogous to that used in section 5 2 and illustrated in section 5 1 however here different mineral distributions are considered which span from p 04 to p 5 i e respectively 4 and 50 of sorbing mineral volume fraction the exercise is carried out using a stepwise approach step 1 calibration the breakthrough curves of the lab scale experiments are best fitted using marfa and the tppm a single segment is used and the best fit is performed by mean squared error minimisation step 2 validation for each considered mineral distribution the coefficients estimated in step 1 are used to run a marfa calculation with the tppm and 10 equally spaced segments the results of step 2 are compared with the breakthrough curves obtained using pflotran and the related mineral distribution at a distance of 2 5 m from the inlet the results of step 1 are shown in fig 13 and the coefficients estimated for each considered mineral distribution are plotted in fig 14 the tppm model describes well the lab scale breakthrough curves which indicates that consistently with what was observed in previous studies e g robinet et al 2008 iraola et al 2017 trinchero et al 2018 the medium indeed behaves as a composite system until a volume fraction of the sorbing mineral of around 15 the system is fairly well approximated by a dual parallel pathway model built as a linear combination of the non sorbing and the mildly sorbing pathway for a higher amount of sorbing mineral an additional degree of freedom i e the highly sorbing pathway is needed to properly capture the composite behaviour of the medium the results of step 2 are shown in fig 15 slight differences between the pflotran and marfa solutions are observed in the rising limb of the breakthrough curve computed for a very low amount of sorbing mineral p 04 for all the other breakthrough curves the agreement between the synthetic experiment and the upscaled solution is very good this suggests that as already observed in section 5 2 when the signal of a lab scale experiment affected by heterogeneous retention is properly captured it can then be successfully propagated along the pathway to describe the signal over a much larger scale 6 large scale modelling 6 1 models with constant transport properties in sections 5 2 and 5 3 we have demonstrated that the signal observed in a tracer test analysed at lab scale and affected by heterogenous retention can be extracted and propagated over larger scales in this section we take advantage of this methodological approach to investigate the effect that heterogeneous retention in the matrix due to the sparse availability of sorption sites has on radionuclide transport at site scale i e the scale of interest in a safety assessment study for a deep geological repository for nuclear waste we consider a pathline of 250 m length constituted by 1000 fracture segments of 0 25 m length each fracture segment and the surrounding matrix are described by the parameters in table 4 it is also assumed that the amount of sorbing mineral is p 16 the propagation of the heterogeneous signal is done using tppm notice that analogous results have been obtained using the ds approach results are not shown here for the sake of brevity the heterogeneous breakthrough curve is compared with three homogeneous calculations one where no sorption is included a second where homogeneous retention is set with r 1 600 and a third with r 250 i e k d k d g p in the whole matrix the comparison of the different breakthrough curves is shown in fig 16 at large scale the heterogeneous signature is no longer evident as already noticed in the previous sections the breakthrough curve computed with the homogeneous model and with r 1 600 i e retardation equal to retardation in the pure mineral phase strongly overestimates the retardation capacity of the matrix interestingly the heterogenous breakthrough curve is very well described by a model where the distribution coefficient in the pure mineral phase is scaled by the mineral volume fraction 6 2 spatial persistence of the heterogeneous signature we have discussed earlier that at small scale the sparse distribution of mineral grains leads to a heterogeneous signature which affects the early part rising limb of the breakthrough curves of sorbing radionuclides at large scale the deviation from the homogeneous based solution is smoothed out we aim at assessing here the spatial persistence of this heterogeneous signature to this end the five different mineral distributions of section 5 3 are considered i e p 04 to p 5 for each mineral distribution the tppm model is run with the related coefficients see fig 14 along a pathline made up of n equally time spaced segments each segment being equal to the lab scale model of section 5 1 these models are here denoted as heterogeneous models a corresponding homogeneous calculation homogeneous model from now on was also run with k d k d g p and the relative difference in the first arrival time was computed as t h o m t h e t t h e t where thom and thet t is the time when 1 of the total injected mass reaches the considered segment outlet for the homogeneous and heterogeneous model respectively fig 17 shows the relative error in the estimate of the first arrival time as a function of cumulative β for the five considered mineral distributions it is interesting to notice that for a high amount of sorbing mineral e g p 0 5 the homogeneous model largely over predicts the first arrival time at small scales but the solution approaches the homogeneous breakthrough curves at relatively short distances for a small amount of sorbing mineral the error at small scales is lower but the spatial persistence of the heterogeneous signature is much longer e g errors above 20 for β 50 000 y m at large scales the results of all the tested heterogeneous models approach those of the equivalent homogeneous models although a slightly negative error is observed as the homogeneous model slightly under predicts the early arrival time see e g the rising limb of the two curves in fig 16 the significance of biotite for the retention of potentially harmful radionuclides particularly cesium e g cs 137 and cs 135 and sodium has been discussed early in this manuscript at forsmark biotite is found to be present in a relatively low amount e g from 0 8 to 8 vol in granitic rock drake et al 2006 in the last swedish safety analysis for a deep geological repository for spent nuclear fuel sr site typical values of cumulative β associated with failing canisters varied from about 1 5 104 y m to 105 y m skb 2010 it turns out that the significantly long persistence of the heterogeneous signature pointed out in this work might have implications for calculations of radionuclide transport carried out at the scale of a typical safety assessment study 6 3 models based on different transport classes radionuclide transport calculations carried out at the site scale entail the need to account for different rock types drake et al 2006 that are characterised by specific retention properties which are in turn related to the different mineralogical properties of the considered lithological units crawford 2008 this heterogeneity is typically treated by defining a finite number of transport classes with related retention properties that are sampled statistically conditioned on the lithological unit to which the given fracture segment belongs e g hartley et al 2018 here the effect of longitudinal heterogeneity is assessed in an idealised way by defining five simplified transport classes characterised by different amount of sorbing mineral tc1 with p 04 tc2 with p 08 tc3 with p 16 tc4 with p 32 and tc5 with p 5 the transport calculation is carried along a pathline made up of 1000 equally time spaced segments each segment being equal to the lab scale model of section 5 1 furthermore each segment is related to one of the five transport classes and the tppm is used with the coefficients shown in fig 14 the five different transport classes were distributed along the considered transport pathway using a sequential indicator simulation that was carried out with the computer code sgems remy et al 2009 equal fractions of each transport class were considered i e 20 each which gives an average amount of sorbing mineral equal to p 0 22 a spherical isotropic semivariogram with a range equal to 10 m was used an additional case was also included in the analysis in which the same proportion of transport classes was distributed in an uncorrelated way along the pathway the two resulting distributions of transport classes are shown in fig 18 the two models are here defined as correlated and uncorrelated models in fig 19 the breakthrough curve computed with the correlated model is compared with the related breakthrough curves obtained using three different homogeneous models one where sorption in the matrix is assumed to be equal to sorption in the pure mineral phase r 1 600 a second where no sorption in the matrix is considered and a third where the distribution coefficient is scaled by the average mineral volume fraction k d k d g p i e r 352 results are very similar to those already discussed in section 6 1 the model with no sorption and the model with sorption equal to sorption in the pure mineral phase respectively significantly underestimate and overestimate the retention capacity of crystalline rocks on the contrary the equivalent homogeneous model based on the distribution coefficient scaled by the average mineral volume fraction arithmetic mean provides a good approximation to the heterogeneous solution although the rising limb of the curve is displaced to earlier times a similar slight underestimation of the early arrival was already discussed in the previous section see also fig 17 the results of the correlated and the uncorrelated heterogeneous models were almost identical results not shown here which indicates that longitudinal correlation in transport classes has a modest effect on radionuclide transport this finding provides support for the methodology employed in previous safety cases to populate fracture segments with transport classes sampled statistically without considering any spatial correlation see e g hartley et al 2018 7 discussion micro characterisation studies based on e g micro computed tomography or digital autoradiography have pointed out that in granitic rocks sorbing minerals are sparsely distributed and this heterogeneous distribution leads to diffusion patterns that differ from what is typically predicted by homogeneous based models this heterogeneous behaviour has been observed in different in situ diffusion experiments the available experimental evidence has led us to formulate three research questions see section 2 based on the results of this work the following answers can be given i at small scale the sparse distribution of sorption sites which is intrinsically related to the underlying micro scale heterogeneity has a significant impact on diffusive patterns leading to heterogeneous breakthrough curves for sorbing radionuclides ii these heterogeneous breakthrough curves can be used to infer non parameteric retention time distribution functions or alternatively to calibrate the parameters of a model based on parallel diffusive pathways that account for the underlying micro scale heterogeneity and iii at large scale the heterogeneous breakthrough curves are smoothed out and the breakthrough curves of sorbing radionuclides are well described by homogeneous solutions based on equivalent distribution coefficients the proposed upscaling procedures are based on idealised segmented pathways characterised by constant hydrodynamic properties i e τ and β real fracture systems are obviously much more complex and might show strong contrasts in velocity and aperture between nearby fractures e g skb 2013 moreover fractures might have experienced different hydrothermal alteration events that could have altered the properties of the rock matrix close the flowing fracture therefore additional work is needed to make the proposed framework applicable to study contaminant transport in large scale fractured media a possible approach would be to use a piecewise approximation in which fracture segments are grouped into transport classes and the related rtd functions are derived from reactive transport models as shown in section 5 1 a similar approach based on transport classes was used in the in last finnish safety assessment study of olkiluoto the designated location for a repository of spent nuclear fuel to account for different radionuclide retention properties of different types of rock matrices posiva 2012 however it is worthwhile pointing out here that one of the main outcomes of this work is that at large scale the heterogeneous and sparse availability of sorption sites can be well described by an equivalent distribution coefficient this simple yet practical approach is of direct relevance for any existing radionuclide transport model in fractured crystalline rock another pending task is to elucidate the mutual interplay between fracture internal aperture heterogeneity and heterogeneous retention which could be done by combing theoretical models e g cvetkovic and cheng 2008 with direct simulations the simulations would focus on resolving both the complex topology of a real fracture and the surrounding rock matrix which in turn should include an explicit description of the sparsely distributed sorption sites 8 summary and conclusions in the first part of this work we have presented a detailed process based simulation of a radionuclide tracer test in a synthetic fracture matrix system to analyse the influence of micro scale heterogeneity on breakthrough curves of sorbing radionuclides the results of the reactive transport simulations point out that at small scale e g the scale representative of a laboratory tracer test experiment the breakthrough curve is characterised by a shape that significantly differs from homogeneous based solutions with a very early arrival and a smooth slope of the tailing at intermediate times this heteorgeneous signature is caused by the sparse distribution of sorption sites starting from this evidence and using a formulation based on retention time distributions we have provided two different upscaling strategies the main general conclusions can be summarised as follows 1 the heterogeneous radionuclide breakthrough curve observed at small scale e g lab scale can be used to extract the radionuclide retention time distribution in the matrix 2 using the retention time distribution extracted from point 1 the heterogeneous signal can be successfully propagated along larger scale 3 at large scales e g site scale the heterogeneous signature of the smal scale signal is smoothed out and the breakthrough curve is well approximated by a homogeneous model in which the distribution coefficient of the sorbing mineral grains is scaled by the mineral volume fraction 4 the heterogeneous signature is shown to persist particularly when the sorbing mineral is present in low amount changes in retention properties along a transport pathway have also been assessed by defining simplified transport classes characterised by different amount of sorbing mineral the results of these longitudinally heterogeneous models have shown that at large scale the resulting radionuclide breakthrough curve can fairly well be described by an equivalent homogeneous model based on the distribution coefficient of the pure mineral phase scaled by the average amount of sorbing mineral along the pathway the spatial correlation of the different transport classes along the transport pathway was shown to have a negligible effect on the results author contribution p t developed the numerical models p t v c and j o s interpreted the results all authors contributed to and reviewed the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests acknowledgements pt and vc thank the swedish nuclear fuel and waste management company skb for the financial support db and gd acknowledge financial support provided by the german federal ministry of education and research bmbf grant agreement 02nuk053a and the initiative and networking fund of the helmholtz association hgf grant so 093 within the collaborative project icross the authors gratefully acknowledge the computing time granted through jara on the supercomputers juqueen and jureca at forschungszentrum jlich the paper has benefited from insightful comments by johan byegård henrik drake kersti nilsson marja siitari kauppi and three anonimous reviewers appendix a implementation of the tppm in marfa for each particle that is routed along a segment a random number is drawn r u 0 1 and the conditional sampling is performed following the pseudocode listed below the correct implementation of the conditional sampling scheme has been verified by comparing the results of a marfa transport calculation carried out along a single segment with the results obtained using an analytical solution based on eqs 6 8 results are not shown here for the sake of brevity 
