index,text
380,the intent of this paper is to provide both an introduction to the concepts of information theory and to review how such concepts might be effectively applied in computing information reduction via upscaling methods the concept of using scaling postulates to reduce the dimensionality or amount of information in a problem is a central idea in upscaling in this paper we present a number of introductory examples that help illustrate the fundamental concepts and definitions of information theory while the subject matter relates primarily to applications from hydrologic systems the concepts are general and may be applied to upscaling in any physical system two more involved examples are investigated the first examines how information content is changed by upscaling for the observable volume fraction in a heterogeneous porous material with a binary distribution of textures in this example we also focus on how the entropy of the data changes with increasing sample size and how the sample size relates to the distribution of the observed volume fraction the notion of typical sets from information theory is introduced and a connection between this concept and the concept of a representative elementary volume rev or rv is made in a second example we examine similar issues related to information content sample size upscaling and the distribution of the upscaled variables to add additional concreteness to the example we also introduce the idea of a simple utility function as a constraint for discerning among different upscaling options this latter concept is important when the relative costs of upscaling i e the loss of resolution must be weighted against the benefits reduction in the number of degrees of freedom such analyses may be important when considering upscaling options model selection in applications keywords upscaling information shannon entropy heterogeneous porous media utility function nomenclature gci grid convergence index pdf discrete probability density function pmf discrete probability mass function rv representative volume ϵ1 an error metric for the cumulative normal distribution used to establish the sequence average bounds ϵ2 an error metric for the information used to establish the sequence information bounds ϵ3 an error metric for the combination of the sequence averages and the sequence information ε f porosity for a porous medium κeff the scalar effective permeability μ a sequence μmicro a sequence containing data defined at the microscale μmacro a sequence containing macroscale data derived by upscaling a microscale sequence μmicro μ the expected value of a sequence or the average of the expected value of an ensemble of sequences μf fluid dynamic viscosity ψ a generic variable ψ the empirical frequency based average of ψ ψ the probabilistic average of ψ ϕ the volume fraction of high conductivity in a binary high and low conductivity medium ϕ the frequency based average volume fraction of high conductivity in a binary high and low conductivity medium φ the cumulative normal distribution ρ a probability of a positive event in a binomial distribution ρf the fluid density σ standard deviation of a discrete pmf σμ standard deviation of a discrete pmf also the expected value of the standard deviation of an ensemble of sequences σ ϕ 2 the variance of the predicted and observed distribution for ϕ ξ microscale utility function ξ macroscale utility function a a set a number of distinct elements in a set dkl the kullback leibler divergence defined by eq 22 g the magnitude of the gravity vector h the entropy average information defined by eqs 7 or 8 h b the entropy for a binomial distribution h 1 the entropy computed for a microscale sequence h 1 the entropy computed for a sequence or statistical macroscale variables computed by upscaling over an ensemble of microscale sequences h the maximum entropy assuming a uniform distribution the maximum entropy in bits is equal to h log 2 n where n is the number of elements in the sequence h hydraulic head b i information in the base b i information in binary units defined by eq 3 i a sequence ii containing the information associated with a set of probabilities pi i the second rank identity tensor k conversion constant for information k 0 low value for hydraulic conductivity k 1 high value for hydraulic conductivity keff the effective hydraulic conductivity i e volume averaged for a particular configuration sequence keff the scalar effective hydraulic conductivity averaged over a number of randomly generated configurations members of an ensemble k e f f the scalar effective hydraulic conductivity probabilistically averaged for an infinite domain ℓ characteristic length associated with a microscale sequence l characteristic length associated with a macroscale sequence m an ensemble of sequences m the number of elements in a set of probabilities also the number of categories used to characterize the empirical pmf for an ensemble of data sequences n the number of elements in a set of probabilities also the number of symbols in an alphabet n number of elements in a sequence or number of columns in a sequence if two dimensional p k the empirical probability density function computed by binning hydraulic conductivity data p a discrete set containing probabilities p m i c r o a pmf corresponding to a microscale sequence μ micro p m a c r o a pmf corresponding to a macroscale sequence μ macro pi a discrete probability for the i th event outcome of a sample space r 0 the characteristic size of the averaging volume r the number of rows where each row is a sequence in a two dimensional collection of sequences r the redundancy defined by eq 21 u the macroscale darcy superficial velocity ux the macroscale darcy superficial velocity component in the coordinate x direction u a set comprising a uniform probability density function w a time space weighting function 1 introduction spatially heterogeneous materials are relevant to a huge array of applications in subsurface hydrology heterogeneity can influence both the amount e g through the hydraulic conductivity and storage properties and quality through the effective transport behavior induced by velocity fluctuations of groundwater subsurface heterogeneities in particular are notoriously difficult to measure and it is frequently assumed that some kind of statistical or combination of deterministic and statistical models are needed in order to treat the problem of incomplete characterization upscaling is the process by which any of a suite of statistical approaches and scaling postulates about these statistics are used to change the scale of description of a problem frequently there is also an expectation that the amount of information required to model a physical system is also reduced in the remainder of this paper we consider the reduction of information to be the objective in the types of upscaling investigated thus for models to be useful there must be redundant information that can be eliminated to reduce the information content and hence the dimensionality of the system by averaging in this context the word redundant indicates information that is somehow beyond what is strictly necessary to address the specific objectives being sought the underlying assumption in the idea of redundancy is that there are physical properties of a system that need only be known in a statistical e g averaged sense and that the statistics of the system are not strongly coupled to the particular microscale configuration of the property i e the statistics are approximately stationary from realization to realization cf wood 2009 information theory is a framework in which the elements defining data can be given a meaningful measure while the methods of information theory focus on assigning information measures to data they do not focus at all on the semantics i e the meaning of the data as a discipline information theory covers more than can be easily reviewed here good reviews of the history and techniques of information theory can be found in a number of excellent texts including jumarie 1990 chap 1 3 arndt 2001 chp 1 4 the texts by cover and thomas 2012 and mackay 2003 each provide an extensive discourse of information theory with applications primarily in the context of communication theory the texts by tribus 1961 and rosenkrantz 2012 provide unique connections between information theory and statistical thermodynamics brillouin 1962 tribus 1961 tribus and mcirvine 1971 this topic appears to be continually debated and rediscovered de beauregard and tribus 1990 jauch and baron 1972 denbigh and denbigh 1986 ben naim 2008 though it is usually to the benefit of the topic as a whole a concise and clear introduction to the information theory specifically oriented toward the non expert is given by stone 2015 in the context of upscaling the use of information theory for measuring information changes during changes of scale is not a new one for example maximum entropy constraints have been a very common application of information theory in upscaling the following is a non exhaustive survey of upscaling works that have used information theory in non trivial ways in hydrology the work by vieux 1993 and by niedda 2004 used entropy metrics to determine the optimal coarse graining of information from digital elevation models for modeling catchment runoff in petroleum resource engineering homogenization theory and maximum entropy methods have been used to predict the effective permeability of reservoirs tuncay and ortoleva 2002 the work by koutsourelakis 2007 illustrated a similar use of entropy to determine the optimal upscaling of microscale mechanics in discrete heterogeneous materials a number of researchers have used maximum entropy conditions for model selection upscaling the mechanics heterogeneous materials upscaling approaches have included random matrix methods das and ghanem 2009 guilleminot et al 2011 guilleminot and soize 2013 and more classical homogenization theory sena michael et al 2013 berdichevsky 2008 boso and tartakovsky 2018 used the mutual information defined for continuous variables to optimize estimates of pdfs representing upscaled hydraulic conductivity while information theory has been used for various purposes in upscaling with a few exceptions it has not been used to assess the amount of information compression created through the upscaling process this is one of the focuses of this paper the primary purposes of this paper are as follows 1 to provide a basic explanation about how information theory can be applied to the problem of upscaling of heterogeneous materials this effort will be specifically oriented to the non expert and where necessary will provide concrete definitions to aid in clarity the discussion and examples are intended primarily for those who are new to the concepts of information theory 2 to provide an outline of how information theory can be used to estimate the amount of compression that is achieved by upscaling here upscaling is meant in a broad sense and includes any of a host of methods that can be used to filter microscale information to generate macroscale representations 3 to give some specific non trivial examples of how information theory can be used to concretely estimate the amount of information reduction realized using upscaling methods for statistically random data sets one of the downsides to information theory is that it is not easily applied to continuous fields it is possible to develop a coherent theory of relative information and the related concept of differential entropy jumarie 1990 for such fields but there is still significant research to be done in this area while some applications e g analyses where continuous distributions must be used because of differentiability requirements the concept must be approached with great care because of a number of open mathematical issues regarding the differential entropy white 1965 dinur and levine 1975 hnizdo and gilson 2010 maynar and trizac 2011 regardless particularly in light of the introductory nature of the work the focus in this paper is on the representation of information for discrete sequences the remainder of this work is organized as follows in section 2 we provide an overview of the basic features of information theory with an effort to provide concrete examples that help illustrate the concepts involved in section 3 we investigate the application of information theory to upscaling again a number of motivating examples are given to help illustrate the idea of information compression engendered by upscaling in section 4 we provide a more extensive example of a random binary field composed of high and low conductivity materials here we examine the statistical properties of the volume fractions of the material and illustrate how both the statistics of the field and information theoretical measures can be used to describe such fields in section 5 we continue the discussion of the volume fraction of the binary conductivity field by addressing the question of how a typical representative of such a field can be defined this leads to the definition of a representative volume as a typical sequence in the sense of information theory finally in section 6 we present an analysis of the effective hydraulic conductivity for the same binary ensembles in particular we illustrate how one might optimally select a particular discrete probability density function to summarize the statistics of the effective hydraulic conductivity and present some estimates of the amount of information reduction that is accomplished by use of the upscaled representation 2 information theory although information theory has long been a component of communications engineering and related disciplines it has been recognized for some time as also being an important component in the description of systems with a large number of degrees of freedom for example it was recognized by physicists early on that the thermodynamic concept of entropy and the information theory concept of average information per symbol had essentially the same underpinning jaynes 1957a peters 1975 tribus 1961 tribus and mcirvine 1971 ben naim 2008 although more direct correspondences took some time to develop with the advent of modern computing especially where large data sets are used such as in image processing data mining and machine learning the language of information theory has found new applications in a wide array of fields not previously imagined the use of information theory is now begin adopted more widely in the physical sciences and engineering disciplines one of the difficulties with information theory that perhaps slowed its adoption is that because of its origins it has been largely described using highly discipline specific language while the concepts of information theory are widely useful much of the language adopted is somewhat specialized no single word in information theory is more misunderstood than the word information itself the word information in the context of information theory does not imply anything specifically about meaning or syntax this diverges sharply from the colloquial use of the term before describing information however we first establish some basic notation and definitions for the kinds of systems that will be examined in this work 2 1 process description and notation for this work we will focus on finite length data sequences that represent data although these sequences may be arbitrarily large and may be structured as 2 d 3 d or higher dimensional fields although such sequences may be correlated in general we will assume that they are composed of a finite number of independent and identically distributed i i d random variables unless otherwise specified in particular this means that every such sequence is described by discrete rather than continuous measures and knowledge of nearby elements of the sequence has no bearing on the probabilities associated with a particular element although correlated processes are also possible to analyze cover and thomas 2012 chp 4 algoet and cover 1988 barron et al 1985 this adds complication that is not critical to the development of introductory ideas and will not be a primary focus in this work discussion of extensions is considered at the end of section 5 note however that because gaussian correlated fields can be approximated by higher order markov fields rue and held 2005 lindgren et al 2011 the necessary framework exists to examine correlated processes using information theory e g yang and liu 2004 munoz et al 2009 melnik and usatenko 2018 shi and yang 2016 for easy reference we outline here the notation used in the remainder of the paper sequences of realized or supposed data of length n are denoted by μ n or μ 1 μ 2 μ 3 μ n or both an ensemble containing r sequences each of length n is denoted by the sequence of sequences m r n μ 1 n μ 2 n μ r n here each element can be indexed by μji where i indexes the position in the sequence and j indexes the realization number for the sequence in the ensemble for sequences that are short the subscript n is unnecessary and will often be dropped sets will be denoted by upper case script letters or braces or both e g a a 1 a 2 a n a discrete probability mass function pmf also a mass histogram will be denoted by an upper case script letter whereas the probability of individual events will be given by an upper case subscripted roman letter i e p p 1 p 2 p m for the case of the bernoulli distribution ρ will be used to align with convention note that n is usually adopted to denote the length of sequences μ r denotes the number of realizations or sometimes equivalently rows in a 2 dimensional sequence m or r 0 in some applications to denote the number of categories bins defined for an ensemble of sequences m r n the special symbol u u 1 u 2 u m is used to denote a discrete uniform probability density function for n possible observations each with probability equal to p i 1 m discrete probability density functions pdf also a density histogram are also considered in this work for density functions the area of each category is equal to its relative probability cf tribus 1961 chp 2 a discrete probability density function will be denoted by a lower case script letter with individual probability density values given by lower case roman letters e g p p 1 p 2 p m cumulative distribution functions are the sums of the corresponding probability density function the symbols pr x is used to indicate the probability of the outcome x regardless of distribution type discrete or continuous we note the following distinction an empirical probability function is one generated directly from the relative frequencies of events in an ensemble a theoretical probability distribution is one that is assumed to be true either axiomatically or by derivation no effort is made to engage in the inapposite contest regarding subjective versus objective probabilities both seem necessary and helpful in generating coherent physical theories especially when large data sets are involved cf the empirical bayes terminology given by efron 2005 the following averages are defined for a random variable ψ angled brackets ψ indicates a frequency based average whereas an overbar ψ indicate probabilistic averages based on an a priori or bayesian assumption of an appropriate distribution a number of definitions and examples are indicated in the text both begin with bold font indicating the definition or example definitions are set apart by vertical spacing the end of an example is denoted with a black triangle 2 2 sequences and sets to begin we define the related notions of sequences and sets as they apply to information theory with discrete data definition finite sequences and sets in information theory for the purposes of this work a sequence is any finite collection of data where the data can be put on a one one correspondence with the natural numbers higher dimensional sequences gómez pérez et al 2018 can be expressed as sequences of multi dimensional arrays uniquely numbered for example using conventional index notation in short every object in a sequence has a unique label in contrast sets are collections of unique objects called elements where order typically does not matter repetition of an element in a set is allowed but it technically adds no additional value to do so thus μ h h t h t h is a data sequence that might represent six outcomes of a coin toss μi represents the i th element of the sequence and s h t is the set sample space of possible values the elements in the sequence may take on if the probabilities associated with each outcome were uniform as the would be for a fair coin then the probability space could be represented by p h 1 2 t 1 2 where p is a set where each element is of the form μi pi the first entry in each element of p represents the outcome and the second entry represents the probability associated with that outcome sometimes p will be specified as containing only the probabilities but it is understood that this is an ordered set where each element of the set corresponds to the element of the sequence at the same position thus when presented with the sample space s h t from above one can write the more compact form with a slight abuse of notation p 1 2 1 2 without confusion in the material that follows an attempt is made to keep the notation clear but without being overly focused on formalism the data comprising a sequence might be strings scalars or d dimensional arrays as a concrete example suppose we have a numerical model e g finite difference output for a concentration field on a grid suppose further that the grid is cartesian and 3 dimensional with 10 points in each axis direction and that there are 100 time points the concentration field is an 4 dimensional array containing a sequence of the form μ i j k ℓ c x i y j z k t ℓ i 1 10 j 1 10 k 1 10 ℓ 1 100 containing a total of n 100 000 points note that although we conventionally think of concentration as being a continuous field variable in this instance it is a discrete variable represented by 100 000 ordered numbers each with a unique label equivalently and depending on our needs for the data we might think of the sequence as a collection of 100 arrays of concentration each of size 10 10 10 where each array represents the state of the system for time t ℓ from this last example it should be clear that a sequence can be synonymous with the discrete representation of a physical spatial field such as a concentration field or a spatial field describing the distribution of hydraulic conductivity in a subsurface domain 2 3 information with the notion the nature of the data we are discussing fixed a definition of information can be proposed to discuss information we first need to establish a representation for information the representation is formed by two elements 1 a collection of m distinct symbols forming the alphabet for representation for our purpose the alphabet is usually formed by integers or real numbers in general this alphabet has to have an established meaning but otherwise could be comprised of letters numbers morse code digits or any other similar system the probability mass distribution of the symbols is given by p s y m b o l p 1 p 2 p m 2 a means of encoding an intended sequence via an encoding algorithm into a set of symbols that can be interpreted again the encoding process can be any of a number of possible algorithms but the encoding decoding process must have an established meaning as an example of this process we can return to the case described above for the outcome of a sequence of six coin tosses we represented this by μ h h t h t h this representation uses an alphabet h or t to encode the outcome of six trials of tossing a coin in this particular case even without stating which symbol represented which outcome the meaning of h and t is fairly clear to english readers equivalently if we make the correspondence h 0 and t 1 then we could represent the same sequence of outcomes by the binary sequence μ 0 0 1 0 1 0 there are a few of problems that arise in this definition first of all the concept of having an established meaning has to be taken as an axiomatic statement in practice this means that by encoding in an alphabet one really means taking information already encoded in one scheme with established meaning e g in english using the english alphabet and re encoding it in another alphabet e g converting english to morse code or binary digits it is only the second of these two encoding schemes that we are focused on while all of this detail might seem to be drifting toward pedantic it is necessary background to provide a reasonable definition for the term information with these preliminary ideas in place the definition of information can be given in short information is just equal to the number of digits required to encode a message using a specified set of symbols definition information for a given sequence encoded in a given alphabet the information represented by the set is equal to the number of digits required using any convenient base number system for representation to encode the sequence in this context encoding means only to propose a pattern of symbols in the encoding alphabet that unambiguously labels the data as an example suppose that a system can be in one of ten different but equally likely states these states can represent any level of complexity for the system the critical component here is that only ten different states are of interest using a base ten numeric alphabet i e the conventional decimal number system then the state of the system can be encoded uniquely by associating the state with one of a sequence of ten numbers if μ represents the state of the system then upon encoding the state is given by any one of the members of the set a 0 1 2 3 4 5 6 7 8 9 for the sequence with only one value μi there are 101 possible states of the system if we assume that the probability of each symbol is uniform then the information i is given by one base ten symbol with probability 1 10 note that to count the number of symbols required to represent any of the outcomes we could compute i log 10 10 1 base 10 symbol note that more generally for a system with 10 n possible states the information is 1 10 i log 10 10 n log 10 1 10 n n here note that we have assumed a uniform probability associated with a positive outcome for any one element in the sequence in general the probability of the elements in a sequence need not be uniform the definitions for information still apply thus general definition for the information ii representing by a particular probabilistic or empirical outcome μi of a sequence of data is given by 2 10 i i k log 10 p i μ i where pi μi is the probability of observing the event μi out of a total of n symbols in the sequence and k is a positive constant often assumed to be unity it is frequently assumed that k 1 it is also convenient to use binary units base 2 units in information theory so for this work the information is given by 3 i i log 2 p i μ i for the remainder of the paper we will use the base 2 system exclusively information in base 2 units is given by the symbol i a few additional notes are helpful here 1 it is not conventional to include the information represented by the algorithm code that is used for the encoding purposes this is consistent with the ideas outlined above where it is assumed that the encoding must have an established meaning however this is a significant issue for systems in which the encoding and decoding part of the process are relevant such as in the metrics regarding the complexity of a encoding algorithms 2 it is not necessary that the information content be an integer value for example in the system referenced above suppose that conditions changed so that there were now only eight possible states rather than 10 we can still represent the state of the system in base 10 units however because two of the states that are possible to represent never occur the representation is not as efficient in the sense that it takes more base 10 digits to represent the answer if we compute the information we find that the result is generally an irrational number if we decide to truncate the representations at say the sixth digit then we have 10 i log 10 8 0 90308 base 10 units this is not efficient because it uses six digits to represent the result if we revise the base of the logarithm to the same base as the number of possible outcomes octal numbering then this new representation would yield an information content of 1 unit and hence only 1 digit to represent the outcome this single digit it would still be capable of representing all possible outcomes and hence it is more efficient 3 for data to contain information the data must provide results about a process that does not have a certain outcome for this reason the information content of an outcome is sometimes called the surprisal tribus 1961 if a process is one that is certain then by definition one already has all of the information about the process and any data sampled from the process information is should be interpreted in the sense of providing new data regarding an outcome thus a deterministically known value has an information content equal to zero 2 4 entropy following from the example above suppose now we observe an ensemble of r sequences m r n from the ensemble as a whole it is possible to compute the empirical frequency based distribution of symbols p s y m b o l p 1 p 2 p m where p 1 is computed from the number of occurrences of symbol 1 divided by r n etc note that this is a probability function defining the microscale statistics with an empirical distribution of symbols established we can compute the probability associated with each observed sequence p p 1 p 2 p r and the corresponding information associated with each sequence through eq 3 note that this is probability distribution defining the microscale statistics this generates a sequence of observed information values i i 1 i 2 i r of length i r the empirical average information from sequence i is defined as follows 4 i 1 r k 1 k r i j 1 r k 1 k r log 2 p j note that this is not the only way to represent this sum suppose that some of the sequences in the ensemble have the same information content for example if a value for the information sequence were repeated three times e g i 1 2 2 2 3 5 where the value 2 is repeated three times then the degeneracy e g mcquarrie 1976 is m 3 for that outcome the degeneracies can be specified by a list a sequence m 1 m 2 m r 0 thus instead of having r independent outcomes for the information the actual number of possible outcomes is r 0 r because some outcomes are repeated note that the degeneracies are bound by n i 1 i r 0 m i a mass histogram of the information content hist i is just the ordered set of information values and their associated degeneracies 5 h i s t i i 1 m 1 i 1 m 2 i r 0 m r 0 using the concept of degeneracies the average information can be written in the alternative form h i 1 r k 1 k r 0 m k log 2 p k the developments above can be rearranged into the suggestive form 6 h i k 1 k r 0 m k r log 2 p k as n increases we expect the empirical probabilities pk to become increasingly certain in accordance with the law of large numbers similarly as r increases we would also expect mk r pk in probability by the weak law of large numbers cf cover and thomas 2012 8 2 also by the law of large numbers this leads us to the definition of the entropy definition entropy the entropy h of an ensemble of possible observed or supposed sequences is equal to the average information we distinguish between two forms of the entropy cf stone 2015 chp 2 1 the empirical entropy and 2 the probabilistic entropy the empirical entropy is computed from the information associated with an ensemble m r n r 0 mk where the mk represent the degeneracy of each category k and r 0 is the number of categories by 7 h i k 1 k r 0 m k r log 2 p k the probabilistic entropy is computed when the probabilities pk are known this happens either when 1 the probabilities for the microscale process are given a priori by a specified pmf or pdf or 2 the probabilities are computed from an ensemble as both r and n tend toward infinity assuming only that the conditions for the weak law of large numbers hold under those conditions the probabilistic entropy is defined by 8 h i k 1 k r 0 p k log 2 p k the following alternative form is helpful for interpreting the entropy as the weighted average information 9 h k 1 k r 0 η k where the weighted information is defined by η k p k i k or η k m k r i k depending on application as a final note the maximum entropy for any set or sequence is equal to the logarithm of number of unique values that appear as an example suppose n a 2300 unique elements this is the cardinality of the sequence or set then the maximum entropy possible for a would occur for a uniform distribution leading to 10 h log 2 a log 2 n note that this is a theoretical maximum it is not an achievable maximum for sequences that do not have a uniform distribution but is useful for comparative purposes for reference a comparison of the functions defining information and entropy as a function of p is given in fig 1 example 1 simple entropy computation consider a set of data representing a simple categorization scheme for citizen scientists reporting weekly stream levels in the month of september the scheme is to report one of 4 possible semi quantitative stream levels as follows dry d low l medium m and high h suppose we had a report μ containing 4 weeks of data as follows μ d l h m now the question is how much information is available in each reported week this question out of the context of information theory is a subjective one however knowing no more about the data set we can determine a method to encode the stream data as follows suppose we can assume that the stream heights have equal probability i e the distribution is the uniform one adopting a binary scheme for encoding the data there are four possible states of the system which can be represented by the following encoding h 11 m 10 l 01 d 00 clearly our data record can be encoded entirely using a system of two binary digits thus the information contained in each such observation is equal to two bits the stream heights recorded this way would be represented by the report sequence μ 00 01 11 10 knowing the encoding scheme anyone would be able to decode the representation to know the river height in fact the commas are superfluous in this case because there were 4 observations each requiring 2 bits to report the total information content of the report is i 8 bits note that this is consistent with the definition of information given above for the uniform distribution u the probabilities are just p j u j 1 n i 1 n 11 i log 2 1 4 log 2 1 4 log 2 1 4 log 2 1 4 8 bits there are a total of 4 4 4 4 256 possible reports sequences that can be observed assuming a uniform distribution the average information from any one report is 12 h j 1 j 256 p j log 2 p j log 2 256 8 bits example 2 entropy computation nonuniform probabilities as a more interesting example let s now assume that from previous observations we have the following historical probabilities p h 0 05 p m 0 2 p l 0 6 p d 0 15 for this case the probabilities of any particular record being observed are not equal and must be computed in all 256 probabilities would need to be computed p 1 p h p h p h p h p 2 p h p h p h p m p 256 p d p d p d p d while slightly more complicated the result is straightforward to compute the result is 13 h i 1 i 256 p i log 2 p i 6 13 bits which is actually slightly less than the previous result where it was assumed that each record had equal probability of being observed this is a general feature of the entropy the maximum entropy observable for a discrete system is created by the distribution that is closest to uniform jaynes 1982 appendix a the result underscores the idea that what the entropy measures is primarily the relative shape of a distribution relatively flat as opposed to peaked as a side note the concept of using maximum entropy to guide the possible choices of distributions has been adopted as a sort of objective application of occam s razor especially as promoted by jaynes 1982 2003 as an example jaynes 1957a b has shown that the use of maximum entropy distributions allows one to determine for example the laws of statistical mechanics jaynes 1982 without imposing the complexities associated with the concept of ergodicity example 3 entropy computation ensembles although information theoretic ideas work most effectively with arbitrarily large sequences and ensembles it is still instructive to look at some examples of small ones suppose we have the ensemble 14 m 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 p s y m b o l 1 1 4 0 3 4 the information content associated with each sequence is given below to the right of the original array 15 m 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 3 25 4 83 1 66 3 25 3 25 the empirical entropy is then computed from 16 h 1 5 k 1 k 5 i k 3 25 noting that the set of sequences has a degeneracy of 3 with h i s t i 1 66 1 3 25 3 4 83 1 with the mk given by 1 3 1 on the information content for this example r 0 is also 3 the entropy is given by the alternative equation eq 7 17 h 1 5 k 1 k 3 m k i k 3 25 the results above can be interpreted in any number of ways the most straightforward one is as follows the current representation of our ensemble uses 4 bits per sequence to encode the data the entropy however indicates that the actual information represented by the ensemble is only 3 25 bits per sequence this suggests that there should be a more efficient encoding scheme for the data set the development of efficient encoding schemes is one of the primary objectives in communication theory cover and thomas 2012 mackay 2003 however it is not a central component of the material presented in this paper and it will not be discussed extensively in the material that follows 3 multiscale systems upscaling and data compression in this section we make the transition to applying the concepts of information theory to the problem of the change in information content of a discrete field upon averaging it is necessary to first establish some additional vocabulary routinely used in this context definition multiscale microscale macroscale a system is called multiscale if for some property ψ it has or can be idealized as having at least two identifiable time or length scales that can be characterized by a metric cf cushman 1984 baveye and sposito 1984 in general there may be any number of identifiable time or length scales or even a distribution of such scales for the remainder of this work we will consider examples that have exactly two time or length scales the two scales are generally termed the microscale for the smaller scale and the macroscale for the larger of the pair the two scales are related through a filtering or local empirical averaging operation usually of the form assuming two spatial dimensions define the problem 18 ψ x t τ 0 τ t y r 2 ψ x y t τ w y τ d v y d τ or in strictly discrete notation 19 ψ x t k i 1 k n t i 1 i r j 1 i n ψ i j k w i j k here ψ represents and arbitrary random variable of space and or time and w is a weighting function usually a compact operator this latter function must integrate or sum to unity when the space integration or sum is omitted then the conventional time average is formed similarly when the time integration or sum is omitted the conventional volume average is formed when the processes under consideration become essentially decorrelated between the microscale and the macroscale then the length scales are said to be separable and the system is called hierarchical this leads to the classical concept of a hierarchy of length scales defined by 20 ℓ r 0 l where ℓ represents a characteristic microscale length scale l a characteristic macroscale length scale and r 0 is a length scale associated with the weighting function w to be concrete for spatial fields with finite variance one might think of the characteristic length scale for ℓ as being a measure of the integral scale associated with ψ l then is the characteristic length scale associated with ψ the terms microstate and macrostate are used frequently in physics these terms are convenient labels for the state of a sequence before and after upscaling thus a sequence containing microscale variables defines the microstate of the system after upscaling the sequence of macroscale variables defines the macrostate of the system like other quantities of interest information can also be multiscale arndt 2001 chp 2 a graphical representation of how information can be considered multiscale appears as fig 2 the primary point in this representation is to illustrate the following 1 information has the same status in the hierarchy as the dependent variables defined at that scale and 2 entropy is inherently an averaged coarse grained quantity a statistic summarizing the state of the system at the associated lower fine grained scale example 4 entropy change via upscaling suppose we have sequences μ 1 μ 2 and μ 3 of binary data representing three different weeks of rain data the data simply indicates if there was rain r or no rain n suppose the observed sequences were μ 1 n n n n r n n μ 2 r n r n r n n and μ 3 r r r n n n n we consider these daily data values to represent microstate of the rainfall data each daily result can be interpreted as having information equal to 1 bit here we are not considering possible changes in this value from non uniform probabilities now define the macrostate as being the simple average i e ψ i μ i the average number of days of rain in week i then the possible macrostates for the sequences above are given by new sequences that contain only one variable ψ 1 1 7 ψ 2 3 7 and ψ 3 3 7 this example illustrates how two different microstates can correspond to a single macrostate in mathematical terms the mapping is surjective not bijective 3 1 data compression and upscaling there are two kinds of data compression lossy and lossless in lossy data compression one or more appropriate filters are applied to the data set often heterogeneously in a way that is not reversible in lossless data compression the amount of potential compression is much more limited however the compression is completely reversible so that the original data set can be retrieved with exact fidelity using the definition offered a the start of this paper upscaling is essentially a process of lossy data compression one of the most important aspects of upscaling has been stated succinctly by mackay 2003 by you can t do inference or lossy data compression without making assumptions in other words one must make one or more generally very strong assumptions regarding both 1 the statistical structure and 2 the relative utility of a data set i e some measure of the value of the data to assess the acceptability of the results often these components are not explicitly stated or computed nonetheless they are implicitly imposed by the methods adopted during the upscaling process there can be substantial benefit to attempting to make such assumptions as explicit as possible in applications to multiphase porous materials assumptions of the first type have been called scaling postulates or scaling laws wood 2009 wood and valdés parada 2013 scaling postulates in this sense are axiomatic statements cf the exchange given in baveye 2009 and wood 2010 based on the statistical properties and inference of set of measurements the idea of imposing axiomatic statements regarding the statistical structure in order to constrain a solution probably started with jaynes 1957a who imposed a scaling postulate of maximum entropy for determining the density distribution in the context of statistical mechanics the use of the second component identified above the utility function is susbstantially less common in upscaling efforts but the idea has been recognized as a potentially helpful one wood 2009 in section 6 an upscaling example with and explicit utility function will be presented for the purposes of this paper a scaling postulate is a statement about the statistical structure of ensembles m r n i i d sequences μ n scaling postulates and utility functions are defined for the purposes of this work as follows definition scaling postulates suppose we have an observed finite ensemble of sequences m r n a scaling postulate is an axiomatic statement regarding the statistical properties of the ensemble each scaling postulate must be 1 consistent with the observed data e g the ensemble must meet the requirements of appropriate statistical tests such as a test that compares the observed distribution with the postulated one and 2 should subscribe to maximum entropy principles as outlined by jaynes 2003 1957a 1957b i e the postulates should impose as few constraints on the system as is possible while still maintaining the ability to meet the objectives of the analysis definition utility function in the context of upscaling a microscale utility function ξ is a function that assigns a positive 0 value to all possible microscale ξi sequences μ in the ensemble m r n there is a corresponding macroscale utility function ξ i that assigns a positive nonzero value to each compressed upscaled sequence in m r nu utility functions may be based on quantitative metrics with concrete physical meaning or on empirical relationships developed to express preference the essential idea of such functions is to provide a weighting accounting for the usefulness of upscaled representations that are neither too complex e g representations that are overfit nor too simple e g representations that fail to produce results that are useful given the objectives sometimes a utility function is expressed by its inverse a cost function the cost function provides a constraint that is minimized the two are not always compatible but if the ultimate goal is strictly to provide an objective function one can create a utility function from a cost or loss function c by the mapping ξ α c 1 or ξ α c 1 for constant α there are many examples of simple utility functions that are used in optimization problems the conventional weight decay regularizer used in machine learning algorithms mackay 2003 can be considered to be a microscale utility function ξ other examples include preference modifiers in reinforcement learning ziebart et al 2008 the concept of the utility function has also been applied previously to upscaling problems gorguluarslan and choi 2014 it is difficult to summarize all the necessary features of utility functions succinctly additional discussion of the general properties of utility functions are available elsewhere e g morgenstern and von neumann 1953 generally utility functions are employed as either 1 an independent function which must be maximized e g gorguluarslan and choi 2014 or 2 as a component of a objective function expressed in terms of lagrange multipliers gottwald and braun 2019 a concrete but simple example of the use of a utility function will be presented in 6 with these definitions in place the concept of data compression can be defined definition data compression upscaling data compression or upscaling is the process by which one transforms a microscale sequence of data of an ensemble of sequences m r n by applying a set of scaling postulates the postulates allow imposing a linear or nonlinear rule e g local weighted averaging thresholding etc that decreases the number of degrees of freedom of the sequence the upscaled sequences which may be scalars are denoted by m ru nu where 1 ru r 1 nu n in the next two examples both lossless and lossy compression are described although lossless compression does not have a significant role in upscaling as defined here it is nonetheless a useful concept for understanding compression of data generally example 5 lossless compression as an example of lossless compression consider the following string which might represent for example days where an observed river flow was below some minimum value μ 0 1 0 0 0 1 2040 1 s total 1 where the long run of 1 s contains a total of 2040 of this integer so that the total string length is 2045 digits now we could store this as a sequence of 2045 binary digits however the long list of 1 s in the middle suggest that another approach might be possible if instead we represent this as follows μ a 0 1 0 0 0 the non repeated part of the data μ b 1 1 1 1 1 1 1 1 a dictionary entry containing a pattern eight repetitions of the digit 1 μ c 1 1 1 1 1 1 1 1 number of replications of the dictionary pattern the binary representation of 255 with this encoding called run length encoding the new representation can capture the details of the string exactly using only 21 binary digits rather than 2045 a significant savings in this case this example however does bring up an important point now along with the file one must also include the dictionary for encoding and equivalently decoding the compressed file since this dictionary will be different in general for every such file compressed clearly if that additional information can be constructed using less than the 2024 digits saved by the encoding then there is still a net savings there is no need for the concept of utility in lossless compression because the utility remains unchanged before and after the compression process lossless compression schemes are complicated in ways that are not necessarily obvious while one may propose schemes that are capable of re encoding some sequences in a manner that requires less information it is not possible to generate lossless compression that makes all sequences smaller some sequences must necessarily become larger mackay 2003 chp 4 it is also important to note that while the encoding for a lossless encoder is efficient in that it reduces the size of the representation of the data the data themselves have the same information content before and after encoding thus there is no notion of change in utility for lossless compression since the utility before and after the compression is identical by design while lossless compression schemes are sometimes used in upscaling e g for example assuming that there is periodic structure to physical fields in this case represented as sequences can be considered a proposition leading to lossless compression however the analysis of lossless compression is complex and well beyond the scope of this paper example 6 lossy compression via upscaling as an example of a lossy compression method consider the data given in fig 3 representing a unit hydrograph for a small watershed suppose we want to store this function as its finite fourier transform this is possible within the nyquist limitations imposed by the measurement resolution we can transform our data in the conventional way suppose we choose a transform in terms of sine functions h t i 1 i 20 μ i sin i π t with a resolution represented by the first 20 amplitudes the result can be given by the vector such as the example following μ m i c r o 20 66 0 0 76 0 0 23 0 20 0 18 0 23 0 0 20 0 0 0 0 24 0 0 0 42 0 14 0 0 where the appropriate frequency is implied by position in the sequence this sequence represents the data illustrated in fig 3 a accurately in an averaged sense however depending on how much loss we are willing to consider reasonable we can store these data much more efficiently by the applying a high frequency cutoff filter which can be considered a nonlinear upscaling method if we cut off all frequencies higher than those associated with n 3 the following representation is found μ m a c r o 20 66 0 0 76 this represents a sine series in only 3 amplitudes and frequencies the maximum absolute error is less than 15 of maximum scale if this is an acceptable amount of loss of information then the amount of compression offers by this particular filter model is substantial compared to the original note that defining an acceptable amount of loss requires defining the utility function ξ as mentioned previously a simple application of a utility function will be provided in section 6 definition relative redundancy in the applications to upscaling the relative redundancy measures how much average information has been compressed or eliminated by upscaling the redundancy is computed by 21 r 1 h 2 h 1 here h 1 is the entropy of the set before applying the upscaling rules and h 2 is the entropy of the set after applying them note that in some applications the comparison entropy is the one associated with the uniform distribution so that the redundancy is a comparison to the information content had the distribution been uniform shannon 1948 often this comparison is called simply the redundancy similarly the quantity 1 r is sometimes called the relative entropy shannon 1948 however the term relative entropy is usually reserved for the kullback leibler divergence defined below definition kullback leibler divergence a related idea known as the kullback leibler divergence or relative entropy or information gain loss is similar to the redundancy this quantity is used to compare any two distributions defined over the same sample space and is defined by in base 2 units 22 d k l p 1 p 2 x ξ p 1 x log 2 p 1 x p 2 x the kullback leibler divergence hereafter kl divergence measures the change in information content of using the distribution p 1 instead of the distribution p 2 where p 2 is often treated as being a reference distribution e g uniform note that for this metric the two distributions must cover the same sample space when the comparison is done with the uniform distribution the kl divergence has particular intuitive meaning 23 d k l p u i 1 i n p i x log 2 p i x 1 n 24 i 1 i n p i x log 2 n p i x 25 i 1 i n p i x log 2 p i x log 2 n 26 h u h p so the kl divergence explains the difference in bits for base 2 computations in average information of an observation with its actual probability distribution p as compared with the information contained if the distribution had been the maximal one u example 7 entropy reduction via nonlinear upscaling in this example we consider an upscaling rule that is not defined by the conventional linear average but rather by a nonlinear averaging operation the example also illustrates a two scale evaluation of the entropy as illustrated graphically in fig 2 the averaging rule proposed is as follows given a binary sequence of data with an even number of elements upscaling is conducted by conducting an exclusive or operation xor on each consecutive subsequence of 2 elements as an explicit example consider the following sequence where we suppose that the following probabilities exist for each element p 1 0 7 p 0 0 3 note that the xor operation is defined by xor 1 0 xor 1 1 1 and xor 0 0 xor 1 1 0 27 μ m i c r o 1 0 0 0 0 1 1 1 28 p m i c r o 0 7 0 3 0 3 0 3 0 7 0 7 0 7 here p m i c r o is the set of probabilities associated with each of the elements in μ micro the corresponding entropy h 1 for this observation is 29 h 1 i 1 i 8 p i log 2 p i 4 0 7 log 2 0 7 4 0 3 log 2 0 3 3 525 bits the upscaled sequence and probability array after the application of the xor operation is 30 μ m a c r o 1 0 1 0 31 p m a c r o 0 7 0 3 0 21 0 3 0 3 0 09 0 3 0 7 0 21 0 7 0 7 0 49 the entropy for the corresponding macroscale result is 32 h 2 i 1 i 4 p i log 2 p i 33 0 21 log 2 0 21 0 09 log 2 0 09 0 21 log 0 21 0 49 log 2 0 49 34 1 763 bits note that if the distribution had instead been uniform we would have computed h 2 u 2 bits the redundancy in the data between the microscale and macroscale representations is given by 35 r 1 1 763 3 525 0 5 or a 50 reduction in information when adopting the macroscale representation this should match the intuition about this problem since our initial data set was represented by an upscaled data set that was exactly half as large as the original finally it is interesting to compute the amount of information that is embedded in the entropy due to the non uniform compared to uniform distribution to compute that we can compute the kl divergence by 36 d k l p 1 p 2 x ξ p 1 x log 2 p 1 x u 2 x 0 21 log 2 0 21 0 25 0 09 log 2 0 09 0 25 0 21 log 0 21 0 25 0 49 log 2 0 49 0 25 0 2374 bits note that this is exactly the difference in bits between the actual entropy h 2 1 763 bits and the entropy that would have been computed had the distribution been uniform h 2 u 2 bits 4 application 1 upscaling the volume fraction most of the discussion to this point has involved relatively simple examples with the specific intention of explaining the fundamental ideas of how upscaling and information theory are related with these ideas in place we can now examine some slightly more complex problems involving spatial fields this has particular relevance to spatial upscaling methods which is our primary interest in this section we examine the upscaling of the volume fraction of a material with a combination of high and low conductivity media this is investigated at a number of different observation scales to examine how scale affects the statistics and information content of the material in section 6 we will examine the more complicated case of determining the effective hydraulic conductivity of this heterogeneous material to start we consider the case of a two material porous medium distributed by a uncorrelated random process in 2 dimensions 4 1 a random binary bernoulli medium the discussion above can be extended to higher dimensional sequences we consider a domain composed of two materials and we assume that the entire domain for this material can be mapped to a square grid of cells of the two materials present one material is of coarse texture with corresponding high hydraulic conductivity and the other of fine texture with low hydraulic conductivity we start by supposing that we can sample non overlapping regions that contain exactly 4 grid cells i e we are sampling regions that have 2 grid cells horizontally by 2 grid cells vertically for a total of 4 grid cells see fig 4 the distribution is binary and we assume equal probabilities so that ρ 1 2 and 1 ρ 1 2 there are only 16 unique configurations so these creates an ensemble m 16 4 in fig 4 all possible configurations of combining high and low conductivity on a 2 by 2 grid are presented where a white cell indicates high conductivity and a shaded cell low each of the possible configurations shown in fig 4 have the same probability of occurring specifically p 1 2 4 1 16 if we use conventional matrix notation and assume that we number in conventional row column format starting from the upper left corner then every sequence in the sample space with n 4 elements can be specified by a matrix of numbers of the form μ μ 11 μ 12 μ 21 μ 22 listing the sequence by rows uniquely labels the sequence so that for example the configuration given by the fourth column of the third row of fig 4 the representation could be given by μ k 1 k 0 k 1 k 1 where here k 1 represents high conductivity and k 0 low conductivity in upscaling methods the particular averaging chosen and this may include selection of method domain size as well as other considerations is usually tied to the particular phenomenon being studied baveye and sposito 1984 miller and gray 2005 auriault et al 2010 chp 2 5 1 and upon what the demands on the data are information theory provides a quantitative method for linking upscaling the required properties of the upscaled data and measures of data compression when combined with the concepts of utility described above the result is a framework that has the capacity to address the relative merits of upscaling strategies as a simple concrete example illustrating how different data demands naturally arise for the same data consider a randomly sampled configuration of the 16 realizations of 2 by 2 squares described above now consider the difference between the following two questions 1 what is the exact configuration of high and low conductivities in the sampled 2 by 2 square 2 what is the volume fraction of high and low conductivities in the sampled region these questions ask for very different kinds of information in the first the exact microscale structure needs to be known in detail essentially no upscaling is possible given the data demands however in the second question only a statistical or summary metric needs to be known the volume fraction intuitively one expects that the first question to require more information than does the second the second question implies the development of a macroscale representation creating a macrostate for the system of the volume fraction which we expect to contain less information using the tools developed in the previous sections it is not difficult to compute the information content of the microstate fields versus that of the macrostate fields example 8 upscaling the volume fraction of a binary medium here we consider the properties of the binary random media with ρ 1 2 described above the information content for any one configuration is found by first noting that the probability of each configuration is equal to p 1 2 4 thus the information content of each configuration is 37 i log 2 1 2 4 4 bits the average information content the entropy in binary units for the full set of 16 possible configurations at the microscale is where u i 1 2 4 1 16 38 h 1 i 1 i 16 u i log 2 u i i 1 i 16 1 16 log 2 1 16 4 bits upon averaging the macrostate variable becomes only the volume fraction all microscale information is suppressed the distribution of the possible volume fractions after averaging is given by re grouping the realizations by their macroscale property in fig 5 we have provided a representation of the histogram of the realizations as grouped by volume fraction the microstate configurations are shown only for visualization purposes upon averaging we know only that the volume fraction belongs to the sample space s ϕ ϕ 0 1 4 1 2 3 4 or 1 i e m 5 and the associated probabilities of those outcomes pr ϕ 0 1 16 pr ϕ 1 4 4 16 pr ϕ 1 2 6 16 pr ϕ 3 4 4 16 and pr ϕ 1 1 16 the average information content for the macrostate result is easily computed from 39 h 2 i 1 i 5 p i log 2 p i 40 1 16 log 2 1 16 4 16 log 2 4 16 6 16 log 2 6 16 4 16 log 2 4 16 1 16 log 2 1 16 41 2 031 bits this leads to a redundancy compression between the microscale and macroscale of 42 r 1 h 2 h 1 1 2 031 4 0 508 in other words about a 51 compression in the information content after averaging we emphasize that this illustrates lossy compression of the information rather than a lossless compression in the information the microstate information is no longer recoverable once the data has been transformed by averaging however under the assumption that the detailed microscale information has low value compared to the macroscale result this loss may been deemed acceptable correspondingly if the loss of microscale detail is an acceptable outcome then the averaging results in a roughly 51 compression in the information required to communicate the volume fraction of the set 4 2 entropy and sequence size binomial distribution in the example given above one would expect that as the sampled volume becomes larger the volume fraction should begin to approach the value ϕ 1 2 with increasing certainty for further analysis to be tractable it is necessary to impose some scaling postulates specifically we impose the following 1 that the distribution is given by the independent binary probabilities defined previously and 2 the statistics can be assumed to be spatially stationary that is the statistics stay the same regardless of how large the domain becomes with these scaling rules imposes the binary system defined above is simple enough that it can be analyzed exactly so the appropriate probabilities can be computed for the volume fraction as the macroscale property it is reasonably straightforward to compute the mean and variance of the volume fraction as n n 2 increases the general case where 0 ϕ 1 and the probability of a cell being high conductivity is ρ is examined in the following the probability distribution function for the obtaining m occurrences of high conductivity in n total grid cells is given by the binomial distribution b n ρ with probability distribution function 43 p m n m ρ n m ρ m 1 ρ n m 44 n m n m ρ m 1 ρ n m noting that the distribution of volume fraction is just this distribution re scaled to the interval ϕ 0 1 we find that the probability mass function for volume fraction ϕ is given directly by 45 p ϕ m n m n m ρ m 1 ρ n m 0 ϕ m 1 the probability density function which is more useful for comparing with continuous distributions is given by 46 p ϕ m n n m n m ρ m 1 ρ n m 0 ϕ m 1 where here it is assumed that the distribution domain is normalized to ϕ 0 1 with increments of size δ ϕ 1 n so that p ϕ m p ϕ m δ ϕ for any value n the volume averaged mean and variance for are given by 47 ϕ m 0 m n m n p m 1 n m 0 m n m p m ρ as n 48 σ ϕ 2 m 0 n n m n ϕ 2 p m 1 n ρ 1 ρ as n for the information content for any realization sequence with m being the occurrences of high conductivity is 49 i log 2 ρ m 1 ρ n m m log 2 ρ n m log 2 1 ρ the empirical entropy is the average of the information taken over all r realizations 50 h 2 1 r m 0 m r m log 2 ρ n m log 2 1 ρ as n then the most likely value for m tends to the mean value m n ρ with probability 1 correspondingly as r then m r nρ thus as either r or n or both become large enough 51 h 2 1 r m 0 m r n ρ log 2 ρ n 1 ρ log 2 1 ρ n ρ log 2 ρ n 1 ρ log 2 1 ρ this last expression is 52 h 2 n h b as n r in particular for the case ρ 1 2 we have the result h b 1 i n h b n in other words when the microscale distribution is uniform ρ 1 ρ 1 2 then the information in each realization tends toward a constant i n and the averaged information macroscale entropy h 2 is also tends toward a constant h 2 n as n tends toward infinity while eq 52 is an interesting result in practice n may need to be quite large indeed for the approximation to hold this can be seen in the following example for this problem with ensembles of increasing size example 9 entropy for ensembles of increasing size in this example we consider the statistics of the volume fraction ϕ for the case ρ 1 ρ 1 2 as the sample size increases in table 1 a summary of the properties of the volume fraction ϕ ensembles of various physical sizes n n 1000 realizations of each ensemble were created to generate these statistics in fig 6 the probability density and mass functions are plotted for each of the cases n 25 100 900 and 9000 one complexity in interpreting these results arises from the normalization of the pdfs to the interval 0 ϕ 1 it is clear that with increasing n each pdf becomes increasingly peaked with decreasing variance in other words the relative uncertainty in the volume fraction ϕ decreases significantly with increasing n this can be seen in fig 6 a and b this observation is discussed in the context of the central limit theorem in the next section note that in table 1 the entropy of the distribution is actually increasing while this may seem counter intuitive at first examining the raw histogram for the number of occurrences of outcomes with increasing sample size makes the result clearer in fig 6 c it is clear that in terms of probability mass distributions the distributions become flatter and wider with increasing n this is consistent with the increase in the entropy examination of the kl divergence in table 1 is helpful in interpretation the divergence computed in table 1 compares the observed pmf with a uniform distribution on the same interval the smaller the value of the kl divergence then the closer the observed distribution is to uniform thus the kl divergence tells us that in some sense the pmf becomes more uniform as n increases note that unlike the variance re scaling the horizontal axis has no effect on the entropy or the kl divergence computed for each case i e the entropy and kl divergence depends only on the probabilities involved 5 typical sets and the representative volume considering that nearly every realized version of the n 9000 case has nearly the same volume fraction near one half fig 6 a and b there is an enormous potential for information reduction on the basis of the scaling rules that were imposed the amount of information needed to represent the volume fraction in this case is roughly close to the amount of information needed to store any one of the realizations assuming that the realization is typical in some sense 5 1 central limit theorem applied to an ensemble of sequences this concept can be made more concrete suppose we consider an ensemble of sequences m r n for each of the r realization of sequences in the ensemble we compute the average value ϕ now we define a related ensemble of sequences m ϕ r n in which each element is defined as the average of the sequence in m r n up to the current element in the sequence call this value ϕi then ϕi is defined by 53 ϕ i μ i 1 i j 0 j i μ j thus ϕi is the running average of the elements of μi up through the current value of the index i we can represent a new sequence μ n ϕ in the ensemble m ϕ r n by 54 μ n ϕ ϕ 1 ϕ 2 ϕ 3 ϕ n although μ n ϕ is a sequence of averaged values it is also a sequence of random outcomes of a random variable regardless of the size of n it is easy to show that the most likely fraction of high conductivity cells in the sequence is ρ papoulis 1965 chp 3 so that ϕn ρ as n becomes large the corresponding number of high conductivity cells in the sequence μ n is approximately n h n ρ in fact we can say more about the approximate distribution of ϕn due to the central limit theorem the theorem states the following central limit theorem let μ n be a sequence of mutually independent random variables with a common density distribution p assume that there is an ensemble m r n of such sequences as defined previously suppose that μ is the expected value of the average of the ensemble of sequences and that σ μ 2 is the expected value of the variance of the ensemble of sequences with both statistics being finite let ϕ n 1 n μ 1 μ 2 μ n also note that each ϕi is itself a sum i e this is finding the expectation of the average value then for every fixed ϵ1 as both n and r become arbitrarily large lindeberg 1922 feller 1968 55 pr ϕ n μ σ μ n ϵ 1 φ ϵ 1 where φ is the standard normal cumulative distribution function with mean zero and variance of 1 56 φ ϵ 1 1 2 π y y ϵ 1 exp y 2 2 d y note that although this theorem implies weak convergence in the mean as r and n tend toward infinity in applications the values of r and n may need to be only on the order of 10 s or 100 s to obtain good results estimates of the error for such processes can be computed via the barry eseen theorem korolev and shevtsova 2010 5 2 the typical set criterion 1 it is possible to use these ideas to estimate the properties of sequences that are typical and in alignment with the theorems of information theory in this paper two necessary components of typical sequences are identified 1 the sequences must have a distribution of symbols that is in some sense close to its probabilistic distribution in other words both n and r need to be large enough such that it is reasonable to assume the central limit theorem is applicable 2 the sequences identified in part 1 above must have sufficiently similar information contents conventionally it is only the second of these that is discussed explicitly in information theory however the first criterion is imposed implicitly and its importance is sometimes lost in the subsequent discussion to determine the sequences that are close to the probabilistic averages we need only note that the central limit theorem applies to the random variable ϕn assuming that n is sufficiently large 57 pr ϕ n μ σ μ n ϵ 1 pr ϕ n μ σ μ n ϵ 1 φ ϵ 1 φ ϵ 1 δ φ to use this expression to determine the value of n required follows typical applications of statistics using the standard normal density distribution example 10 typical set probabalistic limits as an example consider a sequence of n bernoulli trials with ρ 1 2 hence the expected variance σ μ 2 ρ 1 ρ 1 4 suppose further that we want all typical sequences to be within 5 of the mean and we would like to have a 99 confidence in that computation assuming that n is large enough that the central limit theorem applies then we have the condition 58 pr ϕ n μ μ ϵ 1 0 99 where ϵ 1 0 05 putting this into standard form gives 59 pr ϕ n μ σ μ n 0 05 n μ σ μ 0 99 60 ϵ 1 ϵ 1 n μ σ μ 61 δ φ ϵ 1 φ ϵ 1 φ ϵ 1 0 99 or by symmetry φ ϵ 1 0 995 the root of eq 61 i e the inverse of the standard normal function φ 1 ϵ 1 which is tabulated or available as a function in many software packages gives ϵ 1 φ 1 0 995 2 57 combining these results gives 62 pr ϕ n μ μ ϵ 1 0 99 solving eq 60 for n and recalling ϵ 1 0 05 ϵ 1 2 57 we find 63 n ϵ 1 σ μ ϵ 1 μ 2 or n 2642 even a rough visual interpolation of the density functions plotted in fig 6 will suggest that these results are consistent with the plotted functions we note that we used the central limit theorem to obtain these results one can also generate a similar result with less accuracy using chebychev s inequality bachmat and bear 1987 this result provides a first metric for typicality let aμ be the set of all possible sequences μ n μ 1 μ 2 μ 3 μ n as defined previously then the following sequences are typical in the sense of being close enough to the probabilistic expectation for ϕ 64 t n ϵ 1 μ n a μ pr ϕ n μ μ ϵ 1 δ φ ϵ 1 here ϵ 1 ϵ 1 σ μ n μ 5 3 the typical set criterion 2 in information theory the typical set is usually explicitly defined in terms of only the information content this creates some problems primarily the problem that arises is due to the fact that one assumes a priori that the ensemble of sequences involved m r n are distributed close to their probabilistic ratios as discussed in section 5 2 but no explicit requirement is formulated to reinforce this necessity for example see mackay 2003 p 80 where the concept of the asymptotic distribution is used to define the probability of a typical sequence but this argument is not later enforced in the definition of the typical set the analysis presented in the previous section fills this role and ensures that the underlying assumptions are in fact met before any consideration regarding the information content is considered the definition of the set t n ϵ 1 exists only to formally impose this condition fig 7 for the second part of the analysis we need to establish what it means for a sequence to have an information content that is typical note that first any such sequence must be in the set t n ϵ 1 by definition every such sequence has roughly the appropriate distribution of symbols such that it is close to the probabilistic distribution p i e criterion 1 in general for a discrete distribution p with m categories sequences that were near their probabilistic distribution in symbol content would have the approximate probability 65 pr μ n ϕ i 1 i m p i n i where ni is the number of occurrences of symbol i in the sequence i e ni is the microscale degeneracy by definition a typical sequence is on where ni npi making this substitution and taking logarithms it is straightforward to show 66 i μ n ϕ log 2 pr μ n ϕ n i 1 i m p i log 2 p i n h p where h p is the entropy for the discrete distribution p in the case of the bernoulli variables discussed here the discussion above means that every typical sequence must have a distribution of approximately nρ high conductivity cells and n 1 ρ low conductivity cells to be in t n ϵ 1 the probability of generating any one such a sequence is 67 pr μ n ϕ ρ n ρ 1 ρ n 1 ρ the information content of any such sequence is approximately 68 i μ n ϕ log 2 pr μ n ϕ n ρ log 2 ρ n 1 ρ log 2 1 ρ n h b where h b is the entropy of the bernoulli density distribution b in general for any discrete density distribution p a typical sequence as measured by information content is then a sequence whose information content is near i μ n h p in other words for a specified allowable acceptance region of ϵ2 a typical sequence for a distribution p would be expected to have an information content in the range 69 n h p ϵ 2 i μ n h p ϵ 2 or equivalently within the range of probabilities 70 2 n h p ϵ 2 pr μ 2 n h p ϵ 2 adopting the notation of mackay 2003 the set of typical sequences t n ϵ 2 is given by 71 t n ϵ 2 μ n a μ 1 n log 2 1 pr μ n h p ϵ 2 from a more practical perspective one might impose the following condition defining typicality in terms of information content 72 i μ n n h p 1 ϵ 2 where ϵ 2 ϵ 2 n h p represents the fractional deviation allowed in information content from the expected value n h p as with the set t n ϵ 1 as typical set as measured by information content is controlled by how much error ϵ2 one is willing to accept typical sets can potentially be defined for any sequence length n note that because like the mean the information is a summary statistic i e i μ n log 2 p 1 log 2 p n we might hope that the information content also exhibits central limit type behavior for sufficiently large r and n in the appendix it is illustrated that this is true at least for bernoulli trials thus one can use the central limit theorem similarly to the example provided in 5 2 to determine a required sequence length n to assure that the ensemble of realized sequences is distributed roughly according to the correct expected probabilistic ratios with these definitions in place the definition of a typical set can now be stated more concretely definition typical set suppose that there exists an ensemble m r n of sequences μ μ 1 μ 2 μ 3 μ n distributed with a probability density p a typical sequence is one where 1 the sequence is typical in its distribution of symbols that is it is in the set t n ϵ 1 and 2 the sequence is typical in its information content that is it is in the set t n ϵ 2 using the notation from above the typical set is the collection of typical sequences stated by 73 t n μ a μ μ t n ϵ 1 μ t n ϵ 2 example 11 typical set information limits as an example of how both volume fraction or information content can be used to define the typical set fig 8 illustrates the results of generating bernoulli sequences with p 1 10 and with n 100 or n 500 the two parameters for establishing the typical set interval are set to ϵ 1 ϵ 2 0 1 in fig 8 a c the results of generating sequences of length n 100 are shown on the left the sequences are illustrated where the light colors represent the high k regions and dark colors represent the low k regions as indicated on the color bar the sequences in a and b are arranged from the highest value of volume fraction or information content at the top of the region to the lowest value on the bottom of the region in this particular case both volume fraction and information content exactly correspond so the two figures list the sequences from top to bottom identically although this will not always be the case the shades regions at the top and bottom of figures a and b show the sequences that would be qualified as non typical by each metric in fig 8 c a histogram of the volume fraction or relative information content is shown with the grey shaded region indicating the typical sets in fig 8 d f similar results are plotted but for the case n 500 these results show several important features first the criteria based on the volume fraction ϕ or more generally on some statistic ψ μ n and on the information content i μ n ϕ give in general different results when ϵ 1 ϵ 2 for the case shown here the volume fraction criterion is more limiting i e more sequences are rejected but this is not necessarily always the case the limiting feature will be determined by the structure of the fields involved and the particular values of ϵ1 and ϵ2 or equivalently the values of ϵ 1 and ϵ 2 that are selected second the results show that as the sequence length increases the variance of the distribution decreases this occurs in alignment with the law of large numbers however when applied this way the statistical rule is usually called the asymptotic equipartition theorem this will be discussed in detail in the section following the necessity of using both the probabilistic eq 64 and the informatic eq 71 definitions of the typical set can be seen more clearly by examining the uniform case for binary random variables ρ 1 ρ 1 2 the results for that case are provided in fig 9 there are several notable features for the case ρ 1 2 the first is that the distribution is uniform and the corresponding microscale entropy h b is maximum however because the distribution is uniform this means that all possible microscale distributions are equally likely each with a probability of pr μ 1 2 n while information content generally makes a sensible metric for defining the typical set in general it must also be remembered that the definition of typical sets require that the realized sequences first be distributed near their probabilistic proportions as an example of the importance of this suppose we examine 100 sequences of bernoulli trials with p 1 2 each with n 500 elements of these suppose that one were to obtain one sequence containing all 1 s there is only one such sequence and the probability of obtaining ϕ 1 in this way is 1 2 500 a very small number in contrast the probability of obtaining a sequence with ϕ of exactly 1 2 is about 1 2 500 1 2 495 that is there are approximately 2495 such sequences the chance of observing a sequence of 500 1 s in a row is so small that when realizing 100 such sequences none of them would be expected to contain only 1 s thus in a set of sequences that did contain such a realization that realization would not belong to typical set because it would lead to an observed distribution for ϕ that was not consistent with its probabilistic one in this case even though each realization contains the same amount of microscale information iμ this does not mean that all sets are indeed typical as is often stated essentially what the first part of the criteria for the typical set eq 71 imposes is an enforcement of what is required when defining the typical sequence in terms of information to begin with i e that the distribution of realized sequences must first be consistent with the probabilistic distribution before they can be categorized with regard to information content 5 4 the representative volume as a typical set the definition of the representative elementary volume rev or equivalently representative volume element rve has been presented many times usually in a semi quantitative way e g whitaker 1999 bear 1972 but also using quantitative metrics e g bachmat and bear 1987 pecullan et al 1999 to simplify notation we will adopt the terminology representative volume rv indicating that the volume is not necessary elementary i e it is not the smallest such volume nor is it necessarily a volume element in any formal sense e g as would be seen for periodic media here we make a link with information theory by illustrating how the concept of an rv can be defined using the notion of belonging to a typical set which is a common means of categorizing sequences in information theory to start the discussion we explicitly note the fact that for finite length sequences the best that one can do is to offer a probabilistic interpretation of the notion of typical because finite ensembles m r n will always have uncertainty in the computed statistics under such conditions the concept of typical itself is somewhat subjective in the sense that it is dependent upon how much error we are willing to allow and potentially to the utility function assigned to that error this is the focus of much of the discussion in this section the problem of defining and rv as a typical sequences becomes more concrete as the length of each sequence n increases suppose we are interested in a particular macroscopic statistic of the sequences such as the volume fraction the average of μ i ϕi as n becomes large enough almost all sequences measured by the average ϕi are in the typical set this is an important feature of typical sets in many applications e g communication theory compression of files and it has applications in upscaling when one wants to define a representative volume jaynes 1957a made this point clear by stating the theory makes definite predictions as to experimental behavior only when and to the extent that it leads to sharp distributions in the original work by shannon 1948 there is a theorem that has become to be known as the asymptotic equipartition theorem or due to later extensions others the shannon mcmillan breiman theorem algoet and cover 1988 gray 2011 this is roughly speaking an application of the law of large numbers to the idea of the average or other statistics of sequences in short the idea is that for long enough sequences the probability density distribution becomes highly peaked thus the probability that a statistic ψ μ n of the sequence μ n varies from its expected value becomes exceedingly small again following mackay 2003 who also provides a careful and well explained derivation a useful statement of the theorem is given as follows theorem the asymptotic equipartition principle aep for an ensemble of sequences m r n composed of n i i d random variables with discrete density distribution p every randomly realized outcome in the ensemble μ n μ 1 μ 2 μ 3 μ n can be measured by the average and the information content to generate the typical set tn for n sufficiently large μ n is almost certain to belong to the set tn from a more practical sense this theorem states that once the notion of typical sets has been fixed then as one examines sequences with increasing numbers of elements the probability of observing a result with an average value different from that of the typical set becomes increasingly unlikely in other words for any value ϵ3 indicating the probability that one of the possible sequences is not in the typical set there is always a value of n large enough such that 74 pr ψ μ n t n ϵ 3 or equivalently pr ψ μ n t n 1 ϵ 3 proofs of this theorem for the information component t n ϵ 2 can be found elsewhere shannon 1948 mackay 2003 cover and thomas 2012 proofs for the volumetric or other variable content t n ϵ 1 rely strictly on the validity central limit theorem the developments above provide one method of defining the concept of the representative volume for uncorrelated fields although a number of definitions for the rv have been proposed bachmat and bear 1987 wood and valdés parada 2013 rozenbaum and du roscoat 2014 wu et al 2017 jiang et al 2018 definition of the rv on the basis of information theoretic concepts appears to be a novel approach with that goal the following definition for the rv in the context of information theory is proposed definition representative volume rv uncorrelated case let μ μ 1 μ 2 μ 3 μ n be a discrete sequence of length n used to encode a field property measured by the macroscopic variable ψ in 1 2 or 3 dimensions defined by an i i d random process with a discrete probability distribution function with m categories the sequence is said to be an representative volume rv if for some small value of error ϵ3 with either ϵ 1 ϵ 3 or ϵ 2 ϵ 3 depending upon which of the two control the analysis the sequence is a member of tn with probability greater than 1 ϵ 3 in other words the sequence must be typical both in terms of its probabilistic distribution and in terms of its information content and the number of elements in the sequence must be large enough such that nearly every realization is a member of the typical set with probability 1 ϵ 3 as an example of how an rv might be defined for the volume fraction ϕ is given as follows in fig 6 both the theoretical distribution and a distribution generated by an observed ensemble of realizations via monte carlo simulations for the volume fraction ϕ are given as a function of increasing n it is clear that as n increases the width of the distribution decreases roughly as 1 n while all realizations for ϕ are still possible as n increases the probability of encountering a randomly generated realization where ϕ is significantly far from the value ϕ 1 2 becomes increasingly remote for a fixed values of ϵ1 ϵ2 and ϵ3 this means that as n increases nearly all of the realized sequences are in the typical set tn example 12 the representative volume as a typical set consider the example of the binary conductivity example given in 4 for the case n 9025 n 95 does the 95 95 region represent an rv for the volume fraction of course there is no one answer to this question suppose we set ϵ 3 0 01 now we ask the question is the n 9025 case representative we know already that for the case ρ 1 2 all sequences are typical in terms of information content this leaves only the membership in the set t n ϵ 1 as the controlling feature of the rv this is equivalent to imposing from eq 64 75 pr ϕ n μ μ 0 01 δ φ 0 99 this is very much like the problem addressed in example 10 except now in addition to typicality we demand that n is large enough such that nearly every realized sequence generated at random is typical with very high fidelity recall that μ 1 2 and σ μ 1 2 so that μ σ μ 1 from the central limit theorem we have 76 pr ϕ n μ σ μ n 0 01 n 0 99 77 ϵ 3 0 01 n 78 φ ϵ 3 φ ϵ 3 0 99 or φ ϵ 3 0 995 the root of eq 78 gives ϵ 3 φ 1 0 995 2 57 combining these results we find 79 pr ϕ n μ 0 01 0 99 n 2 54 0 01 2 or n 64516 this is a very large number of realizations even for the very narrow peak illustrated in fig 6 for n 9025 the sequences generated cannot be said to be representative at a level of ϵ 3 0 01 this is a very stringent requirement it states that we are allowing less than one out of 100 realizations to have a volume fraction that is be more than 0 01 μ 0 0005 away from the mean value of ϕ 1 2 obviously we can relax our standards on ϵ3 if we like to generate a higher error rate as is typical there is no concrete way to set ϵ3 without some external constraint such as a utility function for representing benefit of minimizing errors the use of a utility function to sharpen the analysis is explored in section 6 5 5 a short note on information in correlated fields the information associated with spatially correlated fields uses similar concepts as for uncorrelated fields but with recognition that joint probabilities are no longer independent while the material in this paper is focused primarily on uncorrelated fields the extension to correlated fields is not substantially different and the necessary framework has already been established the transition to correlated fields should involved two components first the correlated sequences which could be time series or spatial fields would be approximated by a higher order markov process lindgren et al 2011 rue and held 2005 second depending upon the specific statistical nature of the fields there are a number of extensions to the aep to high order markov processes continuous distributions and even nonstationary processes e g barron et al 1985 algoet and cover 1988 yang and liu 2004 melnik and usatenko 2018 in combination the developments above should be extendable to cases where time or space correlated sequences can be considered 6 application 2 upscaling hydraulic conductivity improved constraints via imposition of a utility function as a second application we examine the problem of upscaling the hydraulic conductivity generated for the ensembles of high and low conductivity materials described above with ρ 1 2 as will be described in additional detail in the material following this process involves 1 generating an ensemble of 2 dimensional sequences of hydraulic conductivity 2 solving the momentum balance equation for each realization of the ensemble and 3 computing the effective hydraulic conductivity keff by 80 k e f f u x δ h l where ux is the spatially averaged empirical velocity and δh is the hydraulic head drop and l is the domain length in the average direction of flow which is aligned with the x axis there are several significant differences between the problem of the volume fraction generated by a bernoulli process and the associated problem for the effective hydraulic conductivity and associated velocity field these are as follows 1 the dimensionality of the problem is a crucial component of the sequences because of the local velocity is determined by the nonlocal distribution of high and low conductivities for the volume fraction the dimensionality of the sequences made no change in the effective property derived the volume fraction for the hydraulic conductivity however the solution locally depends strongly on the solution in a neighborhood of any square 2 while the underlying process that defines the distribution of the local microscale conductivity is still a bernoulli process the dependent variable of interest is no longer defined exclusively by the hydraulic conductivity the dependent variable is now technically the velocity field and this is determined through a balance equation and appropriate boundary conditions and these can influence the resulting values of the effective parameters pecullan et al 1999 for this latter problem the issue is ameliorated somewhat by the use of periodic boundary conditions where possible 3 unlike the case for the distribution of volume fraction ϕ we do not know the probability distribution of the effective hydraulic conductivity keff or equivalently the velocity field in each cell at the microscale in other words even given the microscale velocity field which is computed as part of the upscaling process we do not know how to assign a probability to each element in the sequence p s y m b o l such that the probability of observing keff can be computed from it to complicate the problem the resulting probabilities would have to be assigned to events the value of the effective hydraulic conductivity that were no longer discrete ones thus the probability would need to be specified by either a continuous pdf or by a discretized approximation to the distribution none of this is to say that such a description is not possible however it would take a careful derivation via upscaling of the micro macro connections with a particular emphasis on information theoretic concepts to accomplish this while we use upscaling techniques in the material that follows the are not done in manner described above connecting the upscaling needed to define keff with the underlying microcale information theoretic properties needed to characterize it is an interesting question but it is well beyond the scope of this more introductory survey however we will describe some macroscale information theoretic conclusions that can be made from the upscaling effort 4 as a final complication we note that the information measures of data regarding the finite ensemble of conductivity fields will depend upon how the data is segmented at one extreme one can develop a maximum entropy estimate for the data set by simply enumerating the number of unique values of keff within the sequence of outcomes at the other extreme one could summarize the data by lumping all data points into a single bin equal to the mean which would contain zero information in between these two extremes are segmentations of the data with binning to create degeneracy this points to a important property regarding the entropy of data much like for example the variance of finite data set the value predicted for the data will depend on how it is segmented one solution to this issue will be presented below 6 1 upscaling as mentioned above to the authors knowledge a derivation of the effective hydraulic conductivity with attendant descriptions of the microscale information features does not exist however more conventional upscaling without any specific efforts to connect to information theory have been studied using many different upscaling methods for decades e g kim and torquato 1990 hui and ke da 1992 obnosov 1996 and reviews of most of the primary methods are available in the literature olariu et al 2013 renard and de marsily 1997 for this paper we follow essentially the derivation for the effective hydraulic conductivity permeability outlined by whitaker 1999 chp 5 it is easily shown that the closure problem given by whitaker 1999 chp 5 is equivalent to solving the microscale equation in a representative volume with a pressure gradient oriented with one of the coordinate axes we have chosen the x direction although the y direction should give identical results the effective hydraulic conductivity is then determined by inverting the expression 81 u 0 u i κ eff μ f p β i where here represents the spatial superficial empirical average whitaker 1999 chp 1 2 82 u x r v x u r w r x d v y and w is a generally compact weighting function defining the volume average for this work we use the uniform weighting function w 1 v where v is the volume of the region of interest for the remaining terms κ is the permeability tensor μf is the fluid viscosity p β is the gradient of the intrinsic average pressure and i is the unit tensor in the x coordinate direction noting that when p β δ p l 0 0 and κ κi this reduces to 83 u 0 u x κ eff ρ f g μ f δ p l ρ f g where we have scaled by ρfg to convert to the conventional effective hydraulic conductivity keff instead of the effective permeability κeff and the pressure into pressure head h via 84 k eff κ eff ρ f g μ f 85 δ h l δ p ρ f g thus from our numerical results we compute 86 k e f f u x δ h l we note the following for comparative purposes for a random checkerboard with ρ 1 2 in the limit as n the hydraulic conductivity approaches cf dykhne 1971 torquato and hyun 2001 obnosov 1996 87 k e f f k 0 k 1 for this expression to be valid the ratio of k 1 k 0 must not be too large to our knowledge no analytical results exist for the variance of the upscaled hydraulic conductivity as a function of n n n however it has been suggested that the hydraulic conductivity or equivalently the permeability might follow the central limit theorem for random checkerboards distributed by i i d processes olariu et al 2013 6 2 simulations as mentioned previously one of the problems encountered with this simple geometry is that there are discrete boundaries that can have poor numerical convergence with increasing conductivity ratio however for relatively small conductivity ratios this does not create substantial problems thus in these examples the ratio of hydraulic conductivities will be kept to k 1 k 0 4 even with these conditions the convergence rate with decreasing grid size shown only convergence of order 1 29 as opposed to second order convergence which would be expected for a homogeneous domain we set the inlet and outlet hydraulic head boundary condition such that the average gradient of the hydraulic conductivity would always equals to d h d x 1 periodic hydraulic conductivity was applied at the top and the bottom boundaries by utilizing a linear extrusion in comsol multiphysics 5 4 note that the finite element implementation in comsol multiphysics 5 4 imposes flux continuity at the discontinuous surfaces and allows for concentration jumps at the discontinuous interface between high and low conductivities thus the scheme is well suited to the problem of resolving fluxes at the interfaces and corners as discussed by olariu et al 2013 we have summarized the computational parameters in table 2 in fig 10 we have illustrated one realization of the 4 4 case and the associated darcy velocity field to assure that we had achieved grid independence the grid convergence index gci roache 1994 was computed for the simulations this index provides a bound on the estimated error of the numerically converged solution and is reported in table 2 for all simulations we imposed the condition that the gci be on the order of 1 10 5 or less indicating an essentially grid independent solution 6 3 entropy computation for binned data sequences as described in the introductory paragraphs of this section for the problem of the effective hydraulic conductivity we are not attempting to connect the microscale structure directly to the macroscale effective conductivity however there is still substantial potential for upscaling the method that will be pursued here addresses the question what data about the ensemble of macroscale effective hydraulic conductivities needs to be kept this is an example of a problem where defining a utility function is essential for developing a concrete result in table 3 an example of binned data for keff for the 4 4 case will be described in detail the remaining domain sizes are treated similarly for now we need know only the following 1 the data represent the computation of keff from 65 536 realizations of the binary hydraulic conductivity fields described previously for this particular example each realization is 4 cells by 4 cells in size the resulting effective hydraulic conductivities represents a macroscale ensemble m 65536 1 of effective hydraulic conductivity values 2 the 65 536 values for keff for this particular set of realizations turns out to have only 4729 unique values thus there is substantial degeneracy in the data 3 recall r 0 is the number of bins used to represent a discrete probability density or mass distribution the values of h r 0 are computed by binning the data a number of different binning structures were examined these started with a large number of bins to achieve as close to a uniform maximum entropy distribution as possible and subsequent computations decreased the number of bins by factors of 2 4 the maximum possible entropy indicating uniform distribution of the data is given by the log of the number of distinct data in this case h log 2 4729 12 21 there is a technical point to be made about the entropy of a distribution versus the entropy of an ensemble of sequences the entropy of a distribution represents the expected amount of information required to encoded events drawn from that distribution goodfellow et al 2016 it does not however represent the information required to encode the distribution itself this is an important distinction the difference between the two is only multiplication by a constant which represents how the probabilities themselves are expressed as an finite alphabet therefore the entropy of a distribution is useful for relative comparisons about events and ratios of entropies give the correct value of the information encoded by two discrete distributions the reason for this distinction is important when considering overall information compression between say the data encoded by an ensemble and representation of the same ensemble by a density histogram this distinction will be explained further in the next example for the case under consideration we have an sequence of data for keff extracted from an ensemble of realizations while these data meet the technical requirements for the central limit theorem to apply we do encounter a frequent problem with analyzing finite data sets the entropy depends upon the discretization the number of bins used to construct the empirical pdf unlike conventional statistics like the mean or variance the entropy is strongly dependent upon the discretization used to describe the discrete pdf as an example consider a discrete uniform distribution on x 0 1 the statistics for the discrete uniform distribution do not have a strong dependence on the discretization of the distribution the mean is always correct and the variance rapidly approaches the asymptotic value of 1 12 as the number r 0 of bins grows with only 4 bins the variance is already at 93 of its asymptotic value the entropy on the other hand changes substantially with bin size to see this we need only recall that the entropy for the uniform distribution for x 0 1 with r 0 bins is 88 h i 1 i r 0 1 r 0 log 2 1 r 0 log 2 n this feature of the entropy makes is very unlike conventional statistical measures of discrete data in context this makes sense the entropy is a measure of the amount of information encoded by a sequence the longer the sequence in this case the larger the number of bins the more information is retained as an illustration of this we have computed the entropy for the ensemble of 65 536 values of keff for the 4 4 configurations of high and low conductivities table 3 this represents a computation for keff for each possible configuration these entropies were computed for various estimated discrete probability density functions to begin the analysis of the data our first objective was to compute the largest entropy possible for the data by increasing the number of bins used to represent it while the data were not uniformly distributed it was possible to obtain a representation that was nearly uniform with an computed variance of h 12 08 which is close to the theoretical maximum h 12 21 then the number of bins for the discrete pdf were decreased by a factor of 2 until only a single bin represented all of the data with a corresponding entropy of 0 to help visualize these data they are plotted in fig 11 a number of researchers have shown that the sum of i i d random variables converges to a normal probability distribution in the sense of entropy linnik 1959 barron 1986 it is also shown that when the data are described by a normal distribution the entropy decreases with each power of 2 in the bin number to be more clear suppose the discrete pdf is defined with r 0 bins of size δb with p i p i δ b a bin set that was half as big is represented with a number of bins equal to r 0 1 2 1 r 0 in general we set the number of bins to r 0 k 2 k r 0 with r 0 0 r 0 and the entropy scales as 89 h r 0 k i 1 i r 0 p i log 2 p i δ b 2 k where k 0 1 2 indicates the number of doublings for δb a small amount of algebra gives 90 h r 0 k i 1 i r 0 p i log 2 p i k for all cases the k l divergence was computed from 91 d k l p u i 1 i r 0 p i log 2 r 0 p i i 1 i r 0 p i log 2 p i log 2 r 0 with r 0 4729 this allows us to use the distribution with maximum entropy as a constant comparator one might argue that using the uniform distribution with the same number of bins i e using r 0 k k 0 instead of r 0 would be more consistent with the definition of the kl divergence however our particular choice which is only possible for uniform distributions is useful in that the sum of the empirical entropy plus the kl divergence is equal to h for all histogram bin sets thus the kl divergence in this case allows us to think of the maximum entropy being divided up into 1 the measured entropy embedded in the discrete pmf and 2 the remaining entropy which has been eliminated by the representation chosen to denote that we are adopting a particular variation the kl divergence we have denoted it by d k l we expect the data from the ensemble of keff to be well represented by a normal distribution assuming that the data can be binned appropriately under those conditions the entropy should scale as shown in eq 90 the question now is to determine what bin size selection yields the best choice among the possibilities fig 11 suggests that there is an interval of bin numbers where the entropy scales as for a normal distribution to find the best bin distribution it is necessary to add constraining information in terms of a utility function ξ for this problem we adopt the conventional coefficient of determination for the linear plot of discretized normal density distribution compared with the empirical distribution we discretize the domain of the keff as having r 0 bins with r 0 1 bin boundaries defined by b b 1 b 2 b r 0 1 the discretized normal density distribution is defined for the bins defined by b by cf cover and thomas 2012 8 3 92 n i 1 δ b pr b i k e f f b i 1 1 δ b y b i y b i 1 1 2 π σ 2 exp y k e f f 2 2 σ 2 d y for this case we took ξ r 2 where r 2 was determined by plotting the empirical probability defined at resolution k but evaluated at the r 0 original bins versus the discretized normal n i also evaluated at the original number of r 0 bins the problem then resolves to finding the particular set of bins b such that r 2 is maximized the details are presented in the next example example 13 computation of hydraulic conductivities for this example we compute the effective hydraulic conductivity for r realizations of 2d random sequences each with n n n cells of high and low conductivity cells the probability of a high conductivity cell was ρ 1 2 the effective hydraulic conductivity was computed numerically by constructing domains subdivided into n n 2 square cells representing either a high conductivity k 1 0 004 m s 1 or a low conductivity k 0 0 001 m s 1 material as described above the remaining properties for the simulations are listed in table 2 sequences were constructed to represent square 2 dimensional domains in other words the sequences were of the form μ n μ 11 μ 12 μ 1 n μ n 1 μ n 2 μ n n for the first two domain sized studied 3 3 and 4 4 the effective hydraulic conductivity for all possible configurations was computed for the remaining cases it was not practical to compute all possible realizations instead monte carlo methods were used to sample the space of possible configurations generating a sample of keff if sampled densely enough the sampled data sets can provide reasonable estimates for the distribution of keff the statistical properties of the ensemble of the keff are given in table 4 the behavior of the standard deviation for keff for each ensemble size indicates that it is proportional to 1 n which is consistent with the limiting behavior predicted by central limit theorem to determine the histogram binning b with the highest utility for the keff ensembles computed for each domain size we maximized the observed value of r 2 as described above the data from these computations are summarized in table 5 a few notes are necessary regarding these data for the ensembles corresponding to n 9 and n 16 exhaustive sampling of all possible configurations was possible for these cases the entropy estimates are quite robust in the sense that the empirical entropy and the probabilistic entropy should coincide for the remaining cases it was only possible to compute a sample of the possible configurations as an example the 10 10 case has 2100 possible configurations clearly it is not practical to sample this data set exhaustively as an example of the results of the best fit for the normal approximation to the histogram appear in fig 12 in fig 12 a we show the maximum entropy histogram for the data for the 4 4 case this histogram is nearly uniform in the sense that the dkl is a small fraction of the maximum entropy about 1 as indicated in table 5 a series of bin distributions were tried for this case and the optimal one from the perspective of the utility function ξ r 2 was the one with k 12 r 0 k 37 this discrete pdf is illustrated in fig 12 b for this distribution the two competing objectives of minimizing the information content and maximizing the fidelity of representation by a normal distribution are met the resulting pdf represents a compression of the information comparing the normal distribution to the uniform distribution by over 64 thus the final result is one that is both statistically robust it is the best representation of the histogram as informed by the central limit theorem and one that minimized data to the extent possible given the constraints of the utility function note that entropy minimization problems with more complex utility or cost functions are possible using conventional lagrange multiplier techniques boso and tartakovsky 2018 the remaining ensembles were treated similarly all ensembles were able to be fit with a normal distribution reasonably well the smallest domain size the 3 3 case showed both the lowest compression and the least r 2 value for normal distribution fit this is somewhat expected while the number of realizations was large r 512 each sequence was somewhat small n 9 the central limit theorem conditions required both r and n to be sufficiently large in order for it to apply when the number of cells n is small no number of realizations will overcome the fact that the distribution of hydraulic conductivities is different from normal regardless information compression was significant ranging between 56 to 68 these rates of compression are reasonably large however for context recall that some of the original sequences were quite large for example the 30 30 case contained 998 sequences of n 900 length the raw bit length each sequence is a log 2 1 2 900 900 bits the entropy for the ensemble is easily computed to be the average information for the sequences which is also h m 900 bits for the information encoded by the distribution recall that the entropy of a distribution encodes the average information required to represent the outcomes the keff of the distribution the distribution itself requires more information to encode as an example for the n 4 case given above there were 37 bins used to represent the discrete probability distribution for each of these 37 bins an associated probability is required suppose we choose to represent these probabilities using 4 decimal digits this defines an alphabet for the probabilities a 0 0000 0 0001 0 0002 1 000 the total number of discrete data points in base 10 units involved are now 37 bins resolved with an alphabet of 10 001 probability values the maximum amount of information required to represent the data is i log 2 37 log 2 10001 18 4 bits it is possible to improve on this slightly noting that not all of the members of the alphabet the probabilities are equally likely but this makes only a small difference in this example since the maximum entropy estimate is already small compared to 900 now we are in a position to examine the relative compression of data comparing the information encoded in the full ensemble of microscale conductivities to the maximum information encoded in the mass histogram the total data compression is 93 r e n s e m b l e 1 h n k h m 1 18 4 900 0 980 the information compression measuring the full ensemble of r 998 realizations of length n 900 as compared to the entropy encoded in the probability mass or density histogram with probabilities resolved to 1 part in 10 000 is about 98 a very significant savings in the space needed to represent the conductivity field finally in fig 13 we have plotted the best fit normal distributions with the corresponding discrete pdfs for the cases that we were able to obtain data for in all cases except the 3 3 case the normal fit to the data provided an excellent approximation the 3 3 has such a small number of possible configurations 512 that it does not have enough degrees of freedom for the results to be distributed by the normal two interesting features arise in these fits to the discrete pdfs first the averaged value of keff was approximately k e f f 0 0021 table 4 whereas the estimates for infinite system was k e f f 0 002 these values are reasonably close with a difference of only 5 it is unclear why there would be any discrepancy the most likely explanation is that the numerical results are affected by boundary conditions these effects become negligible as the domain size becomes arbitrarily large second we noted above that the standard deviation scales as σ o b s σ 0 n the value of σ 0 for these data was σ 0 0 00158 it is interesting to note that the variance for the underlying conductivity field is σ k 0 00155 the closeness of these two values suggests that there is possibly a valid perturbation model for the variance that has not yet been reported in the literature 7 conclusions although the primary purpose of this paper was to provide an introduction to the concepts of information theory as it applies to upscaling it also provided an opportunity to present some novel contributions in summary the primary contributions of the paper are as follows 1 the distinction between microscale and macroscale variables was defined this was done in a manner consistent with previous work 2 we defined both the empirical entropy arising from measurement of an ensemble and the probabilistic entropy predicted by knowing the exact probability distribution associated with the process the entropy is inherently an averaged and hence macroscale variable we defined the entropy h 1 a macroscale variable arising from the average information of a microscale process and the entropy h 2 a variable at a scale above what we defined as macroscale arising via conducting a second average of the information of a macroscale effective parameter in our case the effective hydraulic conductivity keff both were useful in assessing the amount of information embedded in multiple scales of resolution for a ensemble of sequences representing a physical process 3 the notion of the typical sequence was extended by requiring not only that the macroscale information content be typical but also that the sequences under consideration are typical in the probabilistic sense the examples given were for the case where a binary sequence with equal probabilities was concerned while it is true that all such sequences have the same information content not all sequences should necessarily be considered typical because the information content already requires integrating the data i e determining the probability of the sequence it is inherently macroscale therefore adding the constraint that a complimentary variable in our examples the volume fraction ϕ should also be distributed in a probabilistically representative way is consistent with the notion of typical 4 the representative volume rv was defined using an information theoretic context while this may not be the optimal way to define and rv it provides another metric by which one might consider the notion of what is representative and what is not the advantage that this has is that it joins the conventional theories to define the rv with the well defined notions of typicality in information theory 5 the process of information compression for a sequence representing macroscopic data was addressed in this case the effort was not to explicitly link the microscale information structure to the macroscale effective hydraulic conductivity although this is an interesting problem but to assess what kinds of compression might be acceptable under a constraint imposed by a utility function while the utility function chosen was particularly simple it was defined as the r 2 value for the normal fit to the keff pdf data it nonetheless provided an example where the simultaneous constraints of entropy minimization and utility function maximization were employed declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the anonymous reviewers for providing helpful comments on earlier drafts of this paper the utility of the berry esseen theorem was pointed out by one reviewer appendix a is the information content binomially distributed for binomial distributions in this appendix we consider the distribution for the information content of a sequence of bernoulli trials with length n for every such trial the probability of observing the particular sequence of m high conductivity or outcomes of 1 rather than 0 out of n outcomes is a 1 p μ n ρ m 1 ρ n m the information content for each such outcome is by definition i μ m n m log 2 ρ n m log 2 1 ρ or a 2 i μ m n n m n log 2 ρ n m n log 2 1 ρ as a side note the expected values of m n and n m n are good estimators for ρ and 1 ρ respectively the entropy for the bernoulli distribution is given by a 3 h b ρ log 2 ρ 1 ρ log 2 1 ρ at this juncture it is interesting to look at the average information content or relative information in the terminology of shannon 1948 compared with the entropy to do so we plot 1 niμ and h b in fig a 14 note that in the region where the information content crosses the line of constant entropy the information content is near the entropy of h b by definition this region represents typical sets as qualified by the set t n ϵ 2 the probability distribution for iμ can be obtained by noting the linearity in m of eq a 2 solving this expression for m yields an inverse function predicting m given iμ this expression is a 4 m i μ n n i μ n log 2 1 ρ log 2 1 ρ log 2 ρ this expression links particular values of the information content to values of m from which we can compute the associated probability from a 5 p m i μ n n m ρ m 1 ρ n m where on the right hand side m is determined by the inverse function m m i μ n this effectively allows us to predict the probability associated with a particular value of iμ and n note that the right hand side of eq a 5 is a bernoulli probability density and that m iμ n is a linear function of iμ this means that the resulting probability density function is also bernoulli which illustrates the objective of this appendix for large enough ensembles m r n where recall r is the number of realized sequences and n the length of each sequence the predictions from eq a 5 are very good when compared to the histogram of the information content extracted from m r n a specific example can be seen for the case n 100 ρ 1 5 in fig 8 c where eq a 5 is plotted by the solid blue line n b this is plotted as a continuous line only for ease of comparison 
380,the intent of this paper is to provide both an introduction to the concepts of information theory and to review how such concepts might be effectively applied in computing information reduction via upscaling methods the concept of using scaling postulates to reduce the dimensionality or amount of information in a problem is a central idea in upscaling in this paper we present a number of introductory examples that help illustrate the fundamental concepts and definitions of information theory while the subject matter relates primarily to applications from hydrologic systems the concepts are general and may be applied to upscaling in any physical system two more involved examples are investigated the first examines how information content is changed by upscaling for the observable volume fraction in a heterogeneous porous material with a binary distribution of textures in this example we also focus on how the entropy of the data changes with increasing sample size and how the sample size relates to the distribution of the observed volume fraction the notion of typical sets from information theory is introduced and a connection between this concept and the concept of a representative elementary volume rev or rv is made in a second example we examine similar issues related to information content sample size upscaling and the distribution of the upscaled variables to add additional concreteness to the example we also introduce the idea of a simple utility function as a constraint for discerning among different upscaling options this latter concept is important when the relative costs of upscaling i e the loss of resolution must be weighted against the benefits reduction in the number of degrees of freedom such analyses may be important when considering upscaling options model selection in applications keywords upscaling information shannon entropy heterogeneous porous media utility function nomenclature gci grid convergence index pdf discrete probability density function pmf discrete probability mass function rv representative volume ϵ1 an error metric for the cumulative normal distribution used to establish the sequence average bounds ϵ2 an error metric for the information used to establish the sequence information bounds ϵ3 an error metric for the combination of the sequence averages and the sequence information ε f porosity for a porous medium κeff the scalar effective permeability μ a sequence μmicro a sequence containing data defined at the microscale μmacro a sequence containing macroscale data derived by upscaling a microscale sequence μmicro μ the expected value of a sequence or the average of the expected value of an ensemble of sequences μf fluid dynamic viscosity ψ a generic variable ψ the empirical frequency based average of ψ ψ the probabilistic average of ψ ϕ the volume fraction of high conductivity in a binary high and low conductivity medium ϕ the frequency based average volume fraction of high conductivity in a binary high and low conductivity medium φ the cumulative normal distribution ρ a probability of a positive event in a binomial distribution ρf the fluid density σ standard deviation of a discrete pmf σμ standard deviation of a discrete pmf also the expected value of the standard deviation of an ensemble of sequences σ ϕ 2 the variance of the predicted and observed distribution for ϕ ξ microscale utility function ξ macroscale utility function a a set a number of distinct elements in a set dkl the kullback leibler divergence defined by eq 22 g the magnitude of the gravity vector h the entropy average information defined by eqs 7 or 8 h b the entropy for a binomial distribution h 1 the entropy computed for a microscale sequence h 1 the entropy computed for a sequence or statistical macroscale variables computed by upscaling over an ensemble of microscale sequences h the maximum entropy assuming a uniform distribution the maximum entropy in bits is equal to h log 2 n where n is the number of elements in the sequence h hydraulic head b i information in the base b i information in binary units defined by eq 3 i a sequence ii containing the information associated with a set of probabilities pi i the second rank identity tensor k conversion constant for information k 0 low value for hydraulic conductivity k 1 high value for hydraulic conductivity keff the effective hydraulic conductivity i e volume averaged for a particular configuration sequence keff the scalar effective hydraulic conductivity averaged over a number of randomly generated configurations members of an ensemble k e f f the scalar effective hydraulic conductivity probabilistically averaged for an infinite domain ℓ characteristic length associated with a microscale sequence l characteristic length associated with a macroscale sequence m an ensemble of sequences m the number of elements in a set of probabilities also the number of categories used to characterize the empirical pmf for an ensemble of data sequences n the number of elements in a set of probabilities also the number of symbols in an alphabet n number of elements in a sequence or number of columns in a sequence if two dimensional p k the empirical probability density function computed by binning hydraulic conductivity data p a discrete set containing probabilities p m i c r o a pmf corresponding to a microscale sequence μ micro p m a c r o a pmf corresponding to a macroscale sequence μ macro pi a discrete probability for the i th event outcome of a sample space r 0 the characteristic size of the averaging volume r the number of rows where each row is a sequence in a two dimensional collection of sequences r the redundancy defined by eq 21 u the macroscale darcy superficial velocity ux the macroscale darcy superficial velocity component in the coordinate x direction u a set comprising a uniform probability density function w a time space weighting function 1 introduction spatially heterogeneous materials are relevant to a huge array of applications in subsurface hydrology heterogeneity can influence both the amount e g through the hydraulic conductivity and storage properties and quality through the effective transport behavior induced by velocity fluctuations of groundwater subsurface heterogeneities in particular are notoriously difficult to measure and it is frequently assumed that some kind of statistical or combination of deterministic and statistical models are needed in order to treat the problem of incomplete characterization upscaling is the process by which any of a suite of statistical approaches and scaling postulates about these statistics are used to change the scale of description of a problem frequently there is also an expectation that the amount of information required to model a physical system is also reduced in the remainder of this paper we consider the reduction of information to be the objective in the types of upscaling investigated thus for models to be useful there must be redundant information that can be eliminated to reduce the information content and hence the dimensionality of the system by averaging in this context the word redundant indicates information that is somehow beyond what is strictly necessary to address the specific objectives being sought the underlying assumption in the idea of redundancy is that there are physical properties of a system that need only be known in a statistical e g averaged sense and that the statistics of the system are not strongly coupled to the particular microscale configuration of the property i e the statistics are approximately stationary from realization to realization cf wood 2009 information theory is a framework in which the elements defining data can be given a meaningful measure while the methods of information theory focus on assigning information measures to data they do not focus at all on the semantics i e the meaning of the data as a discipline information theory covers more than can be easily reviewed here good reviews of the history and techniques of information theory can be found in a number of excellent texts including jumarie 1990 chap 1 3 arndt 2001 chp 1 4 the texts by cover and thomas 2012 and mackay 2003 each provide an extensive discourse of information theory with applications primarily in the context of communication theory the texts by tribus 1961 and rosenkrantz 2012 provide unique connections between information theory and statistical thermodynamics brillouin 1962 tribus 1961 tribus and mcirvine 1971 this topic appears to be continually debated and rediscovered de beauregard and tribus 1990 jauch and baron 1972 denbigh and denbigh 1986 ben naim 2008 though it is usually to the benefit of the topic as a whole a concise and clear introduction to the information theory specifically oriented toward the non expert is given by stone 2015 in the context of upscaling the use of information theory for measuring information changes during changes of scale is not a new one for example maximum entropy constraints have been a very common application of information theory in upscaling the following is a non exhaustive survey of upscaling works that have used information theory in non trivial ways in hydrology the work by vieux 1993 and by niedda 2004 used entropy metrics to determine the optimal coarse graining of information from digital elevation models for modeling catchment runoff in petroleum resource engineering homogenization theory and maximum entropy methods have been used to predict the effective permeability of reservoirs tuncay and ortoleva 2002 the work by koutsourelakis 2007 illustrated a similar use of entropy to determine the optimal upscaling of microscale mechanics in discrete heterogeneous materials a number of researchers have used maximum entropy conditions for model selection upscaling the mechanics heterogeneous materials upscaling approaches have included random matrix methods das and ghanem 2009 guilleminot et al 2011 guilleminot and soize 2013 and more classical homogenization theory sena michael et al 2013 berdichevsky 2008 boso and tartakovsky 2018 used the mutual information defined for continuous variables to optimize estimates of pdfs representing upscaled hydraulic conductivity while information theory has been used for various purposes in upscaling with a few exceptions it has not been used to assess the amount of information compression created through the upscaling process this is one of the focuses of this paper the primary purposes of this paper are as follows 1 to provide a basic explanation about how information theory can be applied to the problem of upscaling of heterogeneous materials this effort will be specifically oriented to the non expert and where necessary will provide concrete definitions to aid in clarity the discussion and examples are intended primarily for those who are new to the concepts of information theory 2 to provide an outline of how information theory can be used to estimate the amount of compression that is achieved by upscaling here upscaling is meant in a broad sense and includes any of a host of methods that can be used to filter microscale information to generate macroscale representations 3 to give some specific non trivial examples of how information theory can be used to concretely estimate the amount of information reduction realized using upscaling methods for statistically random data sets one of the downsides to information theory is that it is not easily applied to continuous fields it is possible to develop a coherent theory of relative information and the related concept of differential entropy jumarie 1990 for such fields but there is still significant research to be done in this area while some applications e g analyses where continuous distributions must be used because of differentiability requirements the concept must be approached with great care because of a number of open mathematical issues regarding the differential entropy white 1965 dinur and levine 1975 hnizdo and gilson 2010 maynar and trizac 2011 regardless particularly in light of the introductory nature of the work the focus in this paper is on the representation of information for discrete sequences the remainder of this work is organized as follows in section 2 we provide an overview of the basic features of information theory with an effort to provide concrete examples that help illustrate the concepts involved in section 3 we investigate the application of information theory to upscaling again a number of motivating examples are given to help illustrate the idea of information compression engendered by upscaling in section 4 we provide a more extensive example of a random binary field composed of high and low conductivity materials here we examine the statistical properties of the volume fractions of the material and illustrate how both the statistics of the field and information theoretical measures can be used to describe such fields in section 5 we continue the discussion of the volume fraction of the binary conductivity field by addressing the question of how a typical representative of such a field can be defined this leads to the definition of a representative volume as a typical sequence in the sense of information theory finally in section 6 we present an analysis of the effective hydraulic conductivity for the same binary ensembles in particular we illustrate how one might optimally select a particular discrete probability density function to summarize the statistics of the effective hydraulic conductivity and present some estimates of the amount of information reduction that is accomplished by use of the upscaled representation 2 information theory although information theory has long been a component of communications engineering and related disciplines it has been recognized for some time as also being an important component in the description of systems with a large number of degrees of freedom for example it was recognized by physicists early on that the thermodynamic concept of entropy and the information theory concept of average information per symbol had essentially the same underpinning jaynes 1957a peters 1975 tribus 1961 tribus and mcirvine 1971 ben naim 2008 although more direct correspondences took some time to develop with the advent of modern computing especially where large data sets are used such as in image processing data mining and machine learning the language of information theory has found new applications in a wide array of fields not previously imagined the use of information theory is now begin adopted more widely in the physical sciences and engineering disciplines one of the difficulties with information theory that perhaps slowed its adoption is that because of its origins it has been largely described using highly discipline specific language while the concepts of information theory are widely useful much of the language adopted is somewhat specialized no single word in information theory is more misunderstood than the word information itself the word information in the context of information theory does not imply anything specifically about meaning or syntax this diverges sharply from the colloquial use of the term before describing information however we first establish some basic notation and definitions for the kinds of systems that will be examined in this work 2 1 process description and notation for this work we will focus on finite length data sequences that represent data although these sequences may be arbitrarily large and may be structured as 2 d 3 d or higher dimensional fields although such sequences may be correlated in general we will assume that they are composed of a finite number of independent and identically distributed i i d random variables unless otherwise specified in particular this means that every such sequence is described by discrete rather than continuous measures and knowledge of nearby elements of the sequence has no bearing on the probabilities associated with a particular element although correlated processes are also possible to analyze cover and thomas 2012 chp 4 algoet and cover 1988 barron et al 1985 this adds complication that is not critical to the development of introductory ideas and will not be a primary focus in this work discussion of extensions is considered at the end of section 5 note however that because gaussian correlated fields can be approximated by higher order markov fields rue and held 2005 lindgren et al 2011 the necessary framework exists to examine correlated processes using information theory e g yang and liu 2004 munoz et al 2009 melnik and usatenko 2018 shi and yang 2016 for easy reference we outline here the notation used in the remainder of the paper sequences of realized or supposed data of length n are denoted by μ n or μ 1 μ 2 μ 3 μ n or both an ensemble containing r sequences each of length n is denoted by the sequence of sequences m r n μ 1 n μ 2 n μ r n here each element can be indexed by μji where i indexes the position in the sequence and j indexes the realization number for the sequence in the ensemble for sequences that are short the subscript n is unnecessary and will often be dropped sets will be denoted by upper case script letters or braces or both e g a a 1 a 2 a n a discrete probability mass function pmf also a mass histogram will be denoted by an upper case script letter whereas the probability of individual events will be given by an upper case subscripted roman letter i e p p 1 p 2 p m for the case of the bernoulli distribution ρ will be used to align with convention note that n is usually adopted to denote the length of sequences μ r denotes the number of realizations or sometimes equivalently rows in a 2 dimensional sequence m or r 0 in some applications to denote the number of categories bins defined for an ensemble of sequences m r n the special symbol u u 1 u 2 u m is used to denote a discrete uniform probability density function for n possible observations each with probability equal to p i 1 m discrete probability density functions pdf also a density histogram are also considered in this work for density functions the area of each category is equal to its relative probability cf tribus 1961 chp 2 a discrete probability density function will be denoted by a lower case script letter with individual probability density values given by lower case roman letters e g p p 1 p 2 p m cumulative distribution functions are the sums of the corresponding probability density function the symbols pr x is used to indicate the probability of the outcome x regardless of distribution type discrete or continuous we note the following distinction an empirical probability function is one generated directly from the relative frequencies of events in an ensemble a theoretical probability distribution is one that is assumed to be true either axiomatically or by derivation no effort is made to engage in the inapposite contest regarding subjective versus objective probabilities both seem necessary and helpful in generating coherent physical theories especially when large data sets are involved cf the empirical bayes terminology given by efron 2005 the following averages are defined for a random variable ψ angled brackets ψ indicates a frequency based average whereas an overbar ψ indicate probabilistic averages based on an a priori or bayesian assumption of an appropriate distribution a number of definitions and examples are indicated in the text both begin with bold font indicating the definition or example definitions are set apart by vertical spacing the end of an example is denoted with a black triangle 2 2 sequences and sets to begin we define the related notions of sequences and sets as they apply to information theory with discrete data definition finite sequences and sets in information theory for the purposes of this work a sequence is any finite collection of data where the data can be put on a one one correspondence with the natural numbers higher dimensional sequences gómez pérez et al 2018 can be expressed as sequences of multi dimensional arrays uniquely numbered for example using conventional index notation in short every object in a sequence has a unique label in contrast sets are collections of unique objects called elements where order typically does not matter repetition of an element in a set is allowed but it technically adds no additional value to do so thus μ h h t h t h is a data sequence that might represent six outcomes of a coin toss μi represents the i th element of the sequence and s h t is the set sample space of possible values the elements in the sequence may take on if the probabilities associated with each outcome were uniform as the would be for a fair coin then the probability space could be represented by p h 1 2 t 1 2 where p is a set where each element is of the form μi pi the first entry in each element of p represents the outcome and the second entry represents the probability associated with that outcome sometimes p will be specified as containing only the probabilities but it is understood that this is an ordered set where each element of the set corresponds to the element of the sequence at the same position thus when presented with the sample space s h t from above one can write the more compact form with a slight abuse of notation p 1 2 1 2 without confusion in the material that follows an attempt is made to keep the notation clear but without being overly focused on formalism the data comprising a sequence might be strings scalars or d dimensional arrays as a concrete example suppose we have a numerical model e g finite difference output for a concentration field on a grid suppose further that the grid is cartesian and 3 dimensional with 10 points in each axis direction and that there are 100 time points the concentration field is an 4 dimensional array containing a sequence of the form μ i j k ℓ c x i y j z k t ℓ i 1 10 j 1 10 k 1 10 ℓ 1 100 containing a total of n 100 000 points note that although we conventionally think of concentration as being a continuous field variable in this instance it is a discrete variable represented by 100 000 ordered numbers each with a unique label equivalently and depending on our needs for the data we might think of the sequence as a collection of 100 arrays of concentration each of size 10 10 10 where each array represents the state of the system for time t ℓ from this last example it should be clear that a sequence can be synonymous with the discrete representation of a physical spatial field such as a concentration field or a spatial field describing the distribution of hydraulic conductivity in a subsurface domain 2 3 information with the notion the nature of the data we are discussing fixed a definition of information can be proposed to discuss information we first need to establish a representation for information the representation is formed by two elements 1 a collection of m distinct symbols forming the alphabet for representation for our purpose the alphabet is usually formed by integers or real numbers in general this alphabet has to have an established meaning but otherwise could be comprised of letters numbers morse code digits or any other similar system the probability mass distribution of the symbols is given by p s y m b o l p 1 p 2 p m 2 a means of encoding an intended sequence via an encoding algorithm into a set of symbols that can be interpreted again the encoding process can be any of a number of possible algorithms but the encoding decoding process must have an established meaning as an example of this process we can return to the case described above for the outcome of a sequence of six coin tosses we represented this by μ h h t h t h this representation uses an alphabet h or t to encode the outcome of six trials of tossing a coin in this particular case even without stating which symbol represented which outcome the meaning of h and t is fairly clear to english readers equivalently if we make the correspondence h 0 and t 1 then we could represent the same sequence of outcomes by the binary sequence μ 0 0 1 0 1 0 there are a few of problems that arise in this definition first of all the concept of having an established meaning has to be taken as an axiomatic statement in practice this means that by encoding in an alphabet one really means taking information already encoded in one scheme with established meaning e g in english using the english alphabet and re encoding it in another alphabet e g converting english to morse code or binary digits it is only the second of these two encoding schemes that we are focused on while all of this detail might seem to be drifting toward pedantic it is necessary background to provide a reasonable definition for the term information with these preliminary ideas in place the definition of information can be given in short information is just equal to the number of digits required to encode a message using a specified set of symbols definition information for a given sequence encoded in a given alphabet the information represented by the set is equal to the number of digits required using any convenient base number system for representation to encode the sequence in this context encoding means only to propose a pattern of symbols in the encoding alphabet that unambiguously labels the data as an example suppose that a system can be in one of ten different but equally likely states these states can represent any level of complexity for the system the critical component here is that only ten different states are of interest using a base ten numeric alphabet i e the conventional decimal number system then the state of the system can be encoded uniquely by associating the state with one of a sequence of ten numbers if μ represents the state of the system then upon encoding the state is given by any one of the members of the set a 0 1 2 3 4 5 6 7 8 9 for the sequence with only one value μi there are 101 possible states of the system if we assume that the probability of each symbol is uniform then the information i is given by one base ten symbol with probability 1 10 note that to count the number of symbols required to represent any of the outcomes we could compute i log 10 10 1 base 10 symbol note that more generally for a system with 10 n possible states the information is 1 10 i log 10 10 n log 10 1 10 n n here note that we have assumed a uniform probability associated with a positive outcome for any one element in the sequence in general the probability of the elements in a sequence need not be uniform the definitions for information still apply thus general definition for the information ii representing by a particular probabilistic or empirical outcome μi of a sequence of data is given by 2 10 i i k log 10 p i μ i where pi μi is the probability of observing the event μi out of a total of n symbols in the sequence and k is a positive constant often assumed to be unity it is frequently assumed that k 1 it is also convenient to use binary units base 2 units in information theory so for this work the information is given by 3 i i log 2 p i μ i for the remainder of the paper we will use the base 2 system exclusively information in base 2 units is given by the symbol i a few additional notes are helpful here 1 it is not conventional to include the information represented by the algorithm code that is used for the encoding purposes this is consistent with the ideas outlined above where it is assumed that the encoding must have an established meaning however this is a significant issue for systems in which the encoding and decoding part of the process are relevant such as in the metrics regarding the complexity of a encoding algorithms 2 it is not necessary that the information content be an integer value for example in the system referenced above suppose that conditions changed so that there were now only eight possible states rather than 10 we can still represent the state of the system in base 10 units however because two of the states that are possible to represent never occur the representation is not as efficient in the sense that it takes more base 10 digits to represent the answer if we compute the information we find that the result is generally an irrational number if we decide to truncate the representations at say the sixth digit then we have 10 i log 10 8 0 90308 base 10 units this is not efficient because it uses six digits to represent the result if we revise the base of the logarithm to the same base as the number of possible outcomes octal numbering then this new representation would yield an information content of 1 unit and hence only 1 digit to represent the outcome this single digit it would still be capable of representing all possible outcomes and hence it is more efficient 3 for data to contain information the data must provide results about a process that does not have a certain outcome for this reason the information content of an outcome is sometimes called the surprisal tribus 1961 if a process is one that is certain then by definition one already has all of the information about the process and any data sampled from the process information is should be interpreted in the sense of providing new data regarding an outcome thus a deterministically known value has an information content equal to zero 2 4 entropy following from the example above suppose now we observe an ensemble of r sequences m r n from the ensemble as a whole it is possible to compute the empirical frequency based distribution of symbols p s y m b o l p 1 p 2 p m where p 1 is computed from the number of occurrences of symbol 1 divided by r n etc note that this is a probability function defining the microscale statistics with an empirical distribution of symbols established we can compute the probability associated with each observed sequence p p 1 p 2 p r and the corresponding information associated with each sequence through eq 3 note that this is probability distribution defining the microscale statistics this generates a sequence of observed information values i i 1 i 2 i r of length i r the empirical average information from sequence i is defined as follows 4 i 1 r k 1 k r i j 1 r k 1 k r log 2 p j note that this is not the only way to represent this sum suppose that some of the sequences in the ensemble have the same information content for example if a value for the information sequence were repeated three times e g i 1 2 2 2 3 5 where the value 2 is repeated three times then the degeneracy e g mcquarrie 1976 is m 3 for that outcome the degeneracies can be specified by a list a sequence m 1 m 2 m r 0 thus instead of having r independent outcomes for the information the actual number of possible outcomes is r 0 r because some outcomes are repeated note that the degeneracies are bound by n i 1 i r 0 m i a mass histogram of the information content hist i is just the ordered set of information values and their associated degeneracies 5 h i s t i i 1 m 1 i 1 m 2 i r 0 m r 0 using the concept of degeneracies the average information can be written in the alternative form h i 1 r k 1 k r 0 m k log 2 p k the developments above can be rearranged into the suggestive form 6 h i k 1 k r 0 m k r log 2 p k as n increases we expect the empirical probabilities pk to become increasingly certain in accordance with the law of large numbers similarly as r increases we would also expect mk r pk in probability by the weak law of large numbers cf cover and thomas 2012 8 2 also by the law of large numbers this leads us to the definition of the entropy definition entropy the entropy h of an ensemble of possible observed or supposed sequences is equal to the average information we distinguish between two forms of the entropy cf stone 2015 chp 2 1 the empirical entropy and 2 the probabilistic entropy the empirical entropy is computed from the information associated with an ensemble m r n r 0 mk where the mk represent the degeneracy of each category k and r 0 is the number of categories by 7 h i k 1 k r 0 m k r log 2 p k the probabilistic entropy is computed when the probabilities pk are known this happens either when 1 the probabilities for the microscale process are given a priori by a specified pmf or pdf or 2 the probabilities are computed from an ensemble as both r and n tend toward infinity assuming only that the conditions for the weak law of large numbers hold under those conditions the probabilistic entropy is defined by 8 h i k 1 k r 0 p k log 2 p k the following alternative form is helpful for interpreting the entropy as the weighted average information 9 h k 1 k r 0 η k where the weighted information is defined by η k p k i k or η k m k r i k depending on application as a final note the maximum entropy for any set or sequence is equal to the logarithm of number of unique values that appear as an example suppose n a 2300 unique elements this is the cardinality of the sequence or set then the maximum entropy possible for a would occur for a uniform distribution leading to 10 h log 2 a log 2 n note that this is a theoretical maximum it is not an achievable maximum for sequences that do not have a uniform distribution but is useful for comparative purposes for reference a comparison of the functions defining information and entropy as a function of p is given in fig 1 example 1 simple entropy computation consider a set of data representing a simple categorization scheme for citizen scientists reporting weekly stream levels in the month of september the scheme is to report one of 4 possible semi quantitative stream levels as follows dry d low l medium m and high h suppose we had a report μ containing 4 weeks of data as follows μ d l h m now the question is how much information is available in each reported week this question out of the context of information theory is a subjective one however knowing no more about the data set we can determine a method to encode the stream data as follows suppose we can assume that the stream heights have equal probability i e the distribution is the uniform one adopting a binary scheme for encoding the data there are four possible states of the system which can be represented by the following encoding h 11 m 10 l 01 d 00 clearly our data record can be encoded entirely using a system of two binary digits thus the information contained in each such observation is equal to two bits the stream heights recorded this way would be represented by the report sequence μ 00 01 11 10 knowing the encoding scheme anyone would be able to decode the representation to know the river height in fact the commas are superfluous in this case because there were 4 observations each requiring 2 bits to report the total information content of the report is i 8 bits note that this is consistent with the definition of information given above for the uniform distribution u the probabilities are just p j u j 1 n i 1 n 11 i log 2 1 4 log 2 1 4 log 2 1 4 log 2 1 4 8 bits there are a total of 4 4 4 4 256 possible reports sequences that can be observed assuming a uniform distribution the average information from any one report is 12 h j 1 j 256 p j log 2 p j log 2 256 8 bits example 2 entropy computation nonuniform probabilities as a more interesting example let s now assume that from previous observations we have the following historical probabilities p h 0 05 p m 0 2 p l 0 6 p d 0 15 for this case the probabilities of any particular record being observed are not equal and must be computed in all 256 probabilities would need to be computed p 1 p h p h p h p h p 2 p h p h p h p m p 256 p d p d p d p d while slightly more complicated the result is straightforward to compute the result is 13 h i 1 i 256 p i log 2 p i 6 13 bits which is actually slightly less than the previous result where it was assumed that each record had equal probability of being observed this is a general feature of the entropy the maximum entropy observable for a discrete system is created by the distribution that is closest to uniform jaynes 1982 appendix a the result underscores the idea that what the entropy measures is primarily the relative shape of a distribution relatively flat as opposed to peaked as a side note the concept of using maximum entropy to guide the possible choices of distributions has been adopted as a sort of objective application of occam s razor especially as promoted by jaynes 1982 2003 as an example jaynes 1957a b has shown that the use of maximum entropy distributions allows one to determine for example the laws of statistical mechanics jaynes 1982 without imposing the complexities associated with the concept of ergodicity example 3 entropy computation ensembles although information theoretic ideas work most effectively with arbitrarily large sequences and ensembles it is still instructive to look at some examples of small ones suppose we have the ensemble 14 m 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 p s y m b o l 1 1 4 0 3 4 the information content associated with each sequence is given below to the right of the original array 15 m 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 3 25 4 83 1 66 3 25 3 25 the empirical entropy is then computed from 16 h 1 5 k 1 k 5 i k 3 25 noting that the set of sequences has a degeneracy of 3 with h i s t i 1 66 1 3 25 3 4 83 1 with the mk given by 1 3 1 on the information content for this example r 0 is also 3 the entropy is given by the alternative equation eq 7 17 h 1 5 k 1 k 3 m k i k 3 25 the results above can be interpreted in any number of ways the most straightforward one is as follows the current representation of our ensemble uses 4 bits per sequence to encode the data the entropy however indicates that the actual information represented by the ensemble is only 3 25 bits per sequence this suggests that there should be a more efficient encoding scheme for the data set the development of efficient encoding schemes is one of the primary objectives in communication theory cover and thomas 2012 mackay 2003 however it is not a central component of the material presented in this paper and it will not be discussed extensively in the material that follows 3 multiscale systems upscaling and data compression in this section we make the transition to applying the concepts of information theory to the problem of the change in information content of a discrete field upon averaging it is necessary to first establish some additional vocabulary routinely used in this context definition multiscale microscale macroscale a system is called multiscale if for some property ψ it has or can be idealized as having at least two identifiable time or length scales that can be characterized by a metric cf cushman 1984 baveye and sposito 1984 in general there may be any number of identifiable time or length scales or even a distribution of such scales for the remainder of this work we will consider examples that have exactly two time or length scales the two scales are generally termed the microscale for the smaller scale and the macroscale for the larger of the pair the two scales are related through a filtering or local empirical averaging operation usually of the form assuming two spatial dimensions define the problem 18 ψ x t τ 0 τ t y r 2 ψ x y t τ w y τ d v y d τ or in strictly discrete notation 19 ψ x t k i 1 k n t i 1 i r j 1 i n ψ i j k w i j k here ψ represents and arbitrary random variable of space and or time and w is a weighting function usually a compact operator this latter function must integrate or sum to unity when the space integration or sum is omitted then the conventional time average is formed similarly when the time integration or sum is omitted the conventional volume average is formed when the processes under consideration become essentially decorrelated between the microscale and the macroscale then the length scales are said to be separable and the system is called hierarchical this leads to the classical concept of a hierarchy of length scales defined by 20 ℓ r 0 l where ℓ represents a characteristic microscale length scale l a characteristic macroscale length scale and r 0 is a length scale associated with the weighting function w to be concrete for spatial fields with finite variance one might think of the characteristic length scale for ℓ as being a measure of the integral scale associated with ψ l then is the characteristic length scale associated with ψ the terms microstate and macrostate are used frequently in physics these terms are convenient labels for the state of a sequence before and after upscaling thus a sequence containing microscale variables defines the microstate of the system after upscaling the sequence of macroscale variables defines the macrostate of the system like other quantities of interest information can also be multiscale arndt 2001 chp 2 a graphical representation of how information can be considered multiscale appears as fig 2 the primary point in this representation is to illustrate the following 1 information has the same status in the hierarchy as the dependent variables defined at that scale and 2 entropy is inherently an averaged coarse grained quantity a statistic summarizing the state of the system at the associated lower fine grained scale example 4 entropy change via upscaling suppose we have sequences μ 1 μ 2 and μ 3 of binary data representing three different weeks of rain data the data simply indicates if there was rain r or no rain n suppose the observed sequences were μ 1 n n n n r n n μ 2 r n r n r n n and μ 3 r r r n n n n we consider these daily data values to represent microstate of the rainfall data each daily result can be interpreted as having information equal to 1 bit here we are not considering possible changes in this value from non uniform probabilities now define the macrostate as being the simple average i e ψ i μ i the average number of days of rain in week i then the possible macrostates for the sequences above are given by new sequences that contain only one variable ψ 1 1 7 ψ 2 3 7 and ψ 3 3 7 this example illustrates how two different microstates can correspond to a single macrostate in mathematical terms the mapping is surjective not bijective 3 1 data compression and upscaling there are two kinds of data compression lossy and lossless in lossy data compression one or more appropriate filters are applied to the data set often heterogeneously in a way that is not reversible in lossless data compression the amount of potential compression is much more limited however the compression is completely reversible so that the original data set can be retrieved with exact fidelity using the definition offered a the start of this paper upscaling is essentially a process of lossy data compression one of the most important aspects of upscaling has been stated succinctly by mackay 2003 by you can t do inference or lossy data compression without making assumptions in other words one must make one or more generally very strong assumptions regarding both 1 the statistical structure and 2 the relative utility of a data set i e some measure of the value of the data to assess the acceptability of the results often these components are not explicitly stated or computed nonetheless they are implicitly imposed by the methods adopted during the upscaling process there can be substantial benefit to attempting to make such assumptions as explicit as possible in applications to multiphase porous materials assumptions of the first type have been called scaling postulates or scaling laws wood 2009 wood and valdés parada 2013 scaling postulates in this sense are axiomatic statements cf the exchange given in baveye 2009 and wood 2010 based on the statistical properties and inference of set of measurements the idea of imposing axiomatic statements regarding the statistical structure in order to constrain a solution probably started with jaynes 1957a who imposed a scaling postulate of maximum entropy for determining the density distribution in the context of statistical mechanics the use of the second component identified above the utility function is susbstantially less common in upscaling efforts but the idea has been recognized as a potentially helpful one wood 2009 in section 6 an upscaling example with and explicit utility function will be presented for the purposes of this paper a scaling postulate is a statement about the statistical structure of ensembles m r n i i d sequences μ n scaling postulates and utility functions are defined for the purposes of this work as follows definition scaling postulates suppose we have an observed finite ensemble of sequences m r n a scaling postulate is an axiomatic statement regarding the statistical properties of the ensemble each scaling postulate must be 1 consistent with the observed data e g the ensemble must meet the requirements of appropriate statistical tests such as a test that compares the observed distribution with the postulated one and 2 should subscribe to maximum entropy principles as outlined by jaynes 2003 1957a 1957b i e the postulates should impose as few constraints on the system as is possible while still maintaining the ability to meet the objectives of the analysis definition utility function in the context of upscaling a microscale utility function ξ is a function that assigns a positive 0 value to all possible microscale ξi sequences μ in the ensemble m r n there is a corresponding macroscale utility function ξ i that assigns a positive nonzero value to each compressed upscaled sequence in m r nu utility functions may be based on quantitative metrics with concrete physical meaning or on empirical relationships developed to express preference the essential idea of such functions is to provide a weighting accounting for the usefulness of upscaled representations that are neither too complex e g representations that are overfit nor too simple e g representations that fail to produce results that are useful given the objectives sometimes a utility function is expressed by its inverse a cost function the cost function provides a constraint that is minimized the two are not always compatible but if the ultimate goal is strictly to provide an objective function one can create a utility function from a cost or loss function c by the mapping ξ α c 1 or ξ α c 1 for constant α there are many examples of simple utility functions that are used in optimization problems the conventional weight decay regularizer used in machine learning algorithms mackay 2003 can be considered to be a microscale utility function ξ other examples include preference modifiers in reinforcement learning ziebart et al 2008 the concept of the utility function has also been applied previously to upscaling problems gorguluarslan and choi 2014 it is difficult to summarize all the necessary features of utility functions succinctly additional discussion of the general properties of utility functions are available elsewhere e g morgenstern and von neumann 1953 generally utility functions are employed as either 1 an independent function which must be maximized e g gorguluarslan and choi 2014 or 2 as a component of a objective function expressed in terms of lagrange multipliers gottwald and braun 2019 a concrete but simple example of the use of a utility function will be presented in 6 with these definitions in place the concept of data compression can be defined definition data compression upscaling data compression or upscaling is the process by which one transforms a microscale sequence of data of an ensemble of sequences m r n by applying a set of scaling postulates the postulates allow imposing a linear or nonlinear rule e g local weighted averaging thresholding etc that decreases the number of degrees of freedom of the sequence the upscaled sequences which may be scalars are denoted by m ru nu where 1 ru r 1 nu n in the next two examples both lossless and lossy compression are described although lossless compression does not have a significant role in upscaling as defined here it is nonetheless a useful concept for understanding compression of data generally example 5 lossless compression as an example of lossless compression consider the following string which might represent for example days where an observed river flow was below some minimum value μ 0 1 0 0 0 1 2040 1 s total 1 where the long run of 1 s contains a total of 2040 of this integer so that the total string length is 2045 digits now we could store this as a sequence of 2045 binary digits however the long list of 1 s in the middle suggest that another approach might be possible if instead we represent this as follows μ a 0 1 0 0 0 the non repeated part of the data μ b 1 1 1 1 1 1 1 1 a dictionary entry containing a pattern eight repetitions of the digit 1 μ c 1 1 1 1 1 1 1 1 number of replications of the dictionary pattern the binary representation of 255 with this encoding called run length encoding the new representation can capture the details of the string exactly using only 21 binary digits rather than 2045 a significant savings in this case this example however does bring up an important point now along with the file one must also include the dictionary for encoding and equivalently decoding the compressed file since this dictionary will be different in general for every such file compressed clearly if that additional information can be constructed using less than the 2024 digits saved by the encoding then there is still a net savings there is no need for the concept of utility in lossless compression because the utility remains unchanged before and after the compression process lossless compression schemes are complicated in ways that are not necessarily obvious while one may propose schemes that are capable of re encoding some sequences in a manner that requires less information it is not possible to generate lossless compression that makes all sequences smaller some sequences must necessarily become larger mackay 2003 chp 4 it is also important to note that while the encoding for a lossless encoder is efficient in that it reduces the size of the representation of the data the data themselves have the same information content before and after encoding thus there is no notion of change in utility for lossless compression since the utility before and after the compression is identical by design while lossless compression schemes are sometimes used in upscaling e g for example assuming that there is periodic structure to physical fields in this case represented as sequences can be considered a proposition leading to lossless compression however the analysis of lossless compression is complex and well beyond the scope of this paper example 6 lossy compression via upscaling as an example of a lossy compression method consider the data given in fig 3 representing a unit hydrograph for a small watershed suppose we want to store this function as its finite fourier transform this is possible within the nyquist limitations imposed by the measurement resolution we can transform our data in the conventional way suppose we choose a transform in terms of sine functions h t i 1 i 20 μ i sin i π t with a resolution represented by the first 20 amplitudes the result can be given by the vector such as the example following μ m i c r o 20 66 0 0 76 0 0 23 0 20 0 18 0 23 0 0 20 0 0 0 0 24 0 0 0 42 0 14 0 0 where the appropriate frequency is implied by position in the sequence this sequence represents the data illustrated in fig 3 a accurately in an averaged sense however depending on how much loss we are willing to consider reasonable we can store these data much more efficiently by the applying a high frequency cutoff filter which can be considered a nonlinear upscaling method if we cut off all frequencies higher than those associated with n 3 the following representation is found μ m a c r o 20 66 0 0 76 this represents a sine series in only 3 amplitudes and frequencies the maximum absolute error is less than 15 of maximum scale if this is an acceptable amount of loss of information then the amount of compression offers by this particular filter model is substantial compared to the original note that defining an acceptable amount of loss requires defining the utility function ξ as mentioned previously a simple application of a utility function will be provided in section 6 definition relative redundancy in the applications to upscaling the relative redundancy measures how much average information has been compressed or eliminated by upscaling the redundancy is computed by 21 r 1 h 2 h 1 here h 1 is the entropy of the set before applying the upscaling rules and h 2 is the entropy of the set after applying them note that in some applications the comparison entropy is the one associated with the uniform distribution so that the redundancy is a comparison to the information content had the distribution been uniform shannon 1948 often this comparison is called simply the redundancy similarly the quantity 1 r is sometimes called the relative entropy shannon 1948 however the term relative entropy is usually reserved for the kullback leibler divergence defined below definition kullback leibler divergence a related idea known as the kullback leibler divergence or relative entropy or information gain loss is similar to the redundancy this quantity is used to compare any two distributions defined over the same sample space and is defined by in base 2 units 22 d k l p 1 p 2 x ξ p 1 x log 2 p 1 x p 2 x the kullback leibler divergence hereafter kl divergence measures the change in information content of using the distribution p 1 instead of the distribution p 2 where p 2 is often treated as being a reference distribution e g uniform note that for this metric the two distributions must cover the same sample space when the comparison is done with the uniform distribution the kl divergence has particular intuitive meaning 23 d k l p u i 1 i n p i x log 2 p i x 1 n 24 i 1 i n p i x log 2 n p i x 25 i 1 i n p i x log 2 p i x log 2 n 26 h u h p so the kl divergence explains the difference in bits for base 2 computations in average information of an observation with its actual probability distribution p as compared with the information contained if the distribution had been the maximal one u example 7 entropy reduction via nonlinear upscaling in this example we consider an upscaling rule that is not defined by the conventional linear average but rather by a nonlinear averaging operation the example also illustrates a two scale evaluation of the entropy as illustrated graphically in fig 2 the averaging rule proposed is as follows given a binary sequence of data with an even number of elements upscaling is conducted by conducting an exclusive or operation xor on each consecutive subsequence of 2 elements as an explicit example consider the following sequence where we suppose that the following probabilities exist for each element p 1 0 7 p 0 0 3 note that the xor operation is defined by xor 1 0 xor 1 1 1 and xor 0 0 xor 1 1 0 27 μ m i c r o 1 0 0 0 0 1 1 1 28 p m i c r o 0 7 0 3 0 3 0 3 0 7 0 7 0 7 here p m i c r o is the set of probabilities associated with each of the elements in μ micro the corresponding entropy h 1 for this observation is 29 h 1 i 1 i 8 p i log 2 p i 4 0 7 log 2 0 7 4 0 3 log 2 0 3 3 525 bits the upscaled sequence and probability array after the application of the xor operation is 30 μ m a c r o 1 0 1 0 31 p m a c r o 0 7 0 3 0 21 0 3 0 3 0 09 0 3 0 7 0 21 0 7 0 7 0 49 the entropy for the corresponding macroscale result is 32 h 2 i 1 i 4 p i log 2 p i 33 0 21 log 2 0 21 0 09 log 2 0 09 0 21 log 0 21 0 49 log 2 0 49 34 1 763 bits note that if the distribution had instead been uniform we would have computed h 2 u 2 bits the redundancy in the data between the microscale and macroscale representations is given by 35 r 1 1 763 3 525 0 5 or a 50 reduction in information when adopting the macroscale representation this should match the intuition about this problem since our initial data set was represented by an upscaled data set that was exactly half as large as the original finally it is interesting to compute the amount of information that is embedded in the entropy due to the non uniform compared to uniform distribution to compute that we can compute the kl divergence by 36 d k l p 1 p 2 x ξ p 1 x log 2 p 1 x u 2 x 0 21 log 2 0 21 0 25 0 09 log 2 0 09 0 25 0 21 log 0 21 0 25 0 49 log 2 0 49 0 25 0 2374 bits note that this is exactly the difference in bits between the actual entropy h 2 1 763 bits and the entropy that would have been computed had the distribution been uniform h 2 u 2 bits 4 application 1 upscaling the volume fraction most of the discussion to this point has involved relatively simple examples with the specific intention of explaining the fundamental ideas of how upscaling and information theory are related with these ideas in place we can now examine some slightly more complex problems involving spatial fields this has particular relevance to spatial upscaling methods which is our primary interest in this section we examine the upscaling of the volume fraction of a material with a combination of high and low conductivity media this is investigated at a number of different observation scales to examine how scale affects the statistics and information content of the material in section 6 we will examine the more complicated case of determining the effective hydraulic conductivity of this heterogeneous material to start we consider the case of a two material porous medium distributed by a uncorrelated random process in 2 dimensions 4 1 a random binary bernoulli medium the discussion above can be extended to higher dimensional sequences we consider a domain composed of two materials and we assume that the entire domain for this material can be mapped to a square grid of cells of the two materials present one material is of coarse texture with corresponding high hydraulic conductivity and the other of fine texture with low hydraulic conductivity we start by supposing that we can sample non overlapping regions that contain exactly 4 grid cells i e we are sampling regions that have 2 grid cells horizontally by 2 grid cells vertically for a total of 4 grid cells see fig 4 the distribution is binary and we assume equal probabilities so that ρ 1 2 and 1 ρ 1 2 there are only 16 unique configurations so these creates an ensemble m 16 4 in fig 4 all possible configurations of combining high and low conductivity on a 2 by 2 grid are presented where a white cell indicates high conductivity and a shaded cell low each of the possible configurations shown in fig 4 have the same probability of occurring specifically p 1 2 4 1 16 if we use conventional matrix notation and assume that we number in conventional row column format starting from the upper left corner then every sequence in the sample space with n 4 elements can be specified by a matrix of numbers of the form μ μ 11 μ 12 μ 21 μ 22 listing the sequence by rows uniquely labels the sequence so that for example the configuration given by the fourth column of the third row of fig 4 the representation could be given by μ k 1 k 0 k 1 k 1 where here k 1 represents high conductivity and k 0 low conductivity in upscaling methods the particular averaging chosen and this may include selection of method domain size as well as other considerations is usually tied to the particular phenomenon being studied baveye and sposito 1984 miller and gray 2005 auriault et al 2010 chp 2 5 1 and upon what the demands on the data are information theory provides a quantitative method for linking upscaling the required properties of the upscaled data and measures of data compression when combined with the concepts of utility described above the result is a framework that has the capacity to address the relative merits of upscaling strategies as a simple concrete example illustrating how different data demands naturally arise for the same data consider a randomly sampled configuration of the 16 realizations of 2 by 2 squares described above now consider the difference between the following two questions 1 what is the exact configuration of high and low conductivities in the sampled 2 by 2 square 2 what is the volume fraction of high and low conductivities in the sampled region these questions ask for very different kinds of information in the first the exact microscale structure needs to be known in detail essentially no upscaling is possible given the data demands however in the second question only a statistical or summary metric needs to be known the volume fraction intuitively one expects that the first question to require more information than does the second the second question implies the development of a macroscale representation creating a macrostate for the system of the volume fraction which we expect to contain less information using the tools developed in the previous sections it is not difficult to compute the information content of the microstate fields versus that of the macrostate fields example 8 upscaling the volume fraction of a binary medium here we consider the properties of the binary random media with ρ 1 2 described above the information content for any one configuration is found by first noting that the probability of each configuration is equal to p 1 2 4 thus the information content of each configuration is 37 i log 2 1 2 4 4 bits the average information content the entropy in binary units for the full set of 16 possible configurations at the microscale is where u i 1 2 4 1 16 38 h 1 i 1 i 16 u i log 2 u i i 1 i 16 1 16 log 2 1 16 4 bits upon averaging the macrostate variable becomes only the volume fraction all microscale information is suppressed the distribution of the possible volume fractions after averaging is given by re grouping the realizations by their macroscale property in fig 5 we have provided a representation of the histogram of the realizations as grouped by volume fraction the microstate configurations are shown only for visualization purposes upon averaging we know only that the volume fraction belongs to the sample space s ϕ ϕ 0 1 4 1 2 3 4 or 1 i e m 5 and the associated probabilities of those outcomes pr ϕ 0 1 16 pr ϕ 1 4 4 16 pr ϕ 1 2 6 16 pr ϕ 3 4 4 16 and pr ϕ 1 1 16 the average information content for the macrostate result is easily computed from 39 h 2 i 1 i 5 p i log 2 p i 40 1 16 log 2 1 16 4 16 log 2 4 16 6 16 log 2 6 16 4 16 log 2 4 16 1 16 log 2 1 16 41 2 031 bits this leads to a redundancy compression between the microscale and macroscale of 42 r 1 h 2 h 1 1 2 031 4 0 508 in other words about a 51 compression in the information content after averaging we emphasize that this illustrates lossy compression of the information rather than a lossless compression in the information the microstate information is no longer recoverable once the data has been transformed by averaging however under the assumption that the detailed microscale information has low value compared to the macroscale result this loss may been deemed acceptable correspondingly if the loss of microscale detail is an acceptable outcome then the averaging results in a roughly 51 compression in the information required to communicate the volume fraction of the set 4 2 entropy and sequence size binomial distribution in the example given above one would expect that as the sampled volume becomes larger the volume fraction should begin to approach the value ϕ 1 2 with increasing certainty for further analysis to be tractable it is necessary to impose some scaling postulates specifically we impose the following 1 that the distribution is given by the independent binary probabilities defined previously and 2 the statistics can be assumed to be spatially stationary that is the statistics stay the same regardless of how large the domain becomes with these scaling rules imposes the binary system defined above is simple enough that it can be analyzed exactly so the appropriate probabilities can be computed for the volume fraction as the macroscale property it is reasonably straightforward to compute the mean and variance of the volume fraction as n n 2 increases the general case where 0 ϕ 1 and the probability of a cell being high conductivity is ρ is examined in the following the probability distribution function for the obtaining m occurrences of high conductivity in n total grid cells is given by the binomial distribution b n ρ with probability distribution function 43 p m n m ρ n m ρ m 1 ρ n m 44 n m n m ρ m 1 ρ n m noting that the distribution of volume fraction is just this distribution re scaled to the interval ϕ 0 1 we find that the probability mass function for volume fraction ϕ is given directly by 45 p ϕ m n m n m ρ m 1 ρ n m 0 ϕ m 1 the probability density function which is more useful for comparing with continuous distributions is given by 46 p ϕ m n n m n m ρ m 1 ρ n m 0 ϕ m 1 where here it is assumed that the distribution domain is normalized to ϕ 0 1 with increments of size δ ϕ 1 n so that p ϕ m p ϕ m δ ϕ for any value n the volume averaged mean and variance for are given by 47 ϕ m 0 m n m n p m 1 n m 0 m n m p m ρ as n 48 σ ϕ 2 m 0 n n m n ϕ 2 p m 1 n ρ 1 ρ as n for the information content for any realization sequence with m being the occurrences of high conductivity is 49 i log 2 ρ m 1 ρ n m m log 2 ρ n m log 2 1 ρ the empirical entropy is the average of the information taken over all r realizations 50 h 2 1 r m 0 m r m log 2 ρ n m log 2 1 ρ as n then the most likely value for m tends to the mean value m n ρ with probability 1 correspondingly as r then m r nρ thus as either r or n or both become large enough 51 h 2 1 r m 0 m r n ρ log 2 ρ n 1 ρ log 2 1 ρ n ρ log 2 ρ n 1 ρ log 2 1 ρ this last expression is 52 h 2 n h b as n r in particular for the case ρ 1 2 we have the result h b 1 i n h b n in other words when the microscale distribution is uniform ρ 1 ρ 1 2 then the information in each realization tends toward a constant i n and the averaged information macroscale entropy h 2 is also tends toward a constant h 2 n as n tends toward infinity while eq 52 is an interesting result in practice n may need to be quite large indeed for the approximation to hold this can be seen in the following example for this problem with ensembles of increasing size example 9 entropy for ensembles of increasing size in this example we consider the statistics of the volume fraction ϕ for the case ρ 1 ρ 1 2 as the sample size increases in table 1 a summary of the properties of the volume fraction ϕ ensembles of various physical sizes n n 1000 realizations of each ensemble were created to generate these statistics in fig 6 the probability density and mass functions are plotted for each of the cases n 25 100 900 and 9000 one complexity in interpreting these results arises from the normalization of the pdfs to the interval 0 ϕ 1 it is clear that with increasing n each pdf becomes increasingly peaked with decreasing variance in other words the relative uncertainty in the volume fraction ϕ decreases significantly with increasing n this can be seen in fig 6 a and b this observation is discussed in the context of the central limit theorem in the next section note that in table 1 the entropy of the distribution is actually increasing while this may seem counter intuitive at first examining the raw histogram for the number of occurrences of outcomes with increasing sample size makes the result clearer in fig 6 c it is clear that in terms of probability mass distributions the distributions become flatter and wider with increasing n this is consistent with the increase in the entropy examination of the kl divergence in table 1 is helpful in interpretation the divergence computed in table 1 compares the observed pmf with a uniform distribution on the same interval the smaller the value of the kl divergence then the closer the observed distribution is to uniform thus the kl divergence tells us that in some sense the pmf becomes more uniform as n increases note that unlike the variance re scaling the horizontal axis has no effect on the entropy or the kl divergence computed for each case i e the entropy and kl divergence depends only on the probabilities involved 5 typical sets and the representative volume considering that nearly every realized version of the n 9000 case has nearly the same volume fraction near one half fig 6 a and b there is an enormous potential for information reduction on the basis of the scaling rules that were imposed the amount of information needed to represent the volume fraction in this case is roughly close to the amount of information needed to store any one of the realizations assuming that the realization is typical in some sense 5 1 central limit theorem applied to an ensemble of sequences this concept can be made more concrete suppose we consider an ensemble of sequences m r n for each of the r realization of sequences in the ensemble we compute the average value ϕ now we define a related ensemble of sequences m ϕ r n in which each element is defined as the average of the sequence in m r n up to the current element in the sequence call this value ϕi then ϕi is defined by 53 ϕ i μ i 1 i j 0 j i μ j thus ϕi is the running average of the elements of μi up through the current value of the index i we can represent a new sequence μ n ϕ in the ensemble m ϕ r n by 54 μ n ϕ ϕ 1 ϕ 2 ϕ 3 ϕ n although μ n ϕ is a sequence of averaged values it is also a sequence of random outcomes of a random variable regardless of the size of n it is easy to show that the most likely fraction of high conductivity cells in the sequence is ρ papoulis 1965 chp 3 so that ϕn ρ as n becomes large the corresponding number of high conductivity cells in the sequence μ n is approximately n h n ρ in fact we can say more about the approximate distribution of ϕn due to the central limit theorem the theorem states the following central limit theorem let μ n be a sequence of mutually independent random variables with a common density distribution p assume that there is an ensemble m r n of such sequences as defined previously suppose that μ is the expected value of the average of the ensemble of sequences and that σ μ 2 is the expected value of the variance of the ensemble of sequences with both statistics being finite let ϕ n 1 n μ 1 μ 2 μ n also note that each ϕi is itself a sum i e this is finding the expectation of the average value then for every fixed ϵ1 as both n and r become arbitrarily large lindeberg 1922 feller 1968 55 pr ϕ n μ σ μ n ϵ 1 φ ϵ 1 where φ is the standard normal cumulative distribution function with mean zero and variance of 1 56 φ ϵ 1 1 2 π y y ϵ 1 exp y 2 2 d y note that although this theorem implies weak convergence in the mean as r and n tend toward infinity in applications the values of r and n may need to be only on the order of 10 s or 100 s to obtain good results estimates of the error for such processes can be computed via the barry eseen theorem korolev and shevtsova 2010 5 2 the typical set criterion 1 it is possible to use these ideas to estimate the properties of sequences that are typical and in alignment with the theorems of information theory in this paper two necessary components of typical sequences are identified 1 the sequences must have a distribution of symbols that is in some sense close to its probabilistic distribution in other words both n and r need to be large enough such that it is reasonable to assume the central limit theorem is applicable 2 the sequences identified in part 1 above must have sufficiently similar information contents conventionally it is only the second of these that is discussed explicitly in information theory however the first criterion is imposed implicitly and its importance is sometimes lost in the subsequent discussion to determine the sequences that are close to the probabilistic averages we need only note that the central limit theorem applies to the random variable ϕn assuming that n is sufficiently large 57 pr ϕ n μ σ μ n ϵ 1 pr ϕ n μ σ μ n ϵ 1 φ ϵ 1 φ ϵ 1 δ φ to use this expression to determine the value of n required follows typical applications of statistics using the standard normal density distribution example 10 typical set probabalistic limits as an example consider a sequence of n bernoulli trials with ρ 1 2 hence the expected variance σ μ 2 ρ 1 ρ 1 4 suppose further that we want all typical sequences to be within 5 of the mean and we would like to have a 99 confidence in that computation assuming that n is large enough that the central limit theorem applies then we have the condition 58 pr ϕ n μ μ ϵ 1 0 99 where ϵ 1 0 05 putting this into standard form gives 59 pr ϕ n μ σ μ n 0 05 n μ σ μ 0 99 60 ϵ 1 ϵ 1 n μ σ μ 61 δ φ ϵ 1 φ ϵ 1 φ ϵ 1 0 99 or by symmetry φ ϵ 1 0 995 the root of eq 61 i e the inverse of the standard normal function φ 1 ϵ 1 which is tabulated or available as a function in many software packages gives ϵ 1 φ 1 0 995 2 57 combining these results gives 62 pr ϕ n μ μ ϵ 1 0 99 solving eq 60 for n and recalling ϵ 1 0 05 ϵ 1 2 57 we find 63 n ϵ 1 σ μ ϵ 1 μ 2 or n 2642 even a rough visual interpolation of the density functions plotted in fig 6 will suggest that these results are consistent with the plotted functions we note that we used the central limit theorem to obtain these results one can also generate a similar result with less accuracy using chebychev s inequality bachmat and bear 1987 this result provides a first metric for typicality let aμ be the set of all possible sequences μ n μ 1 μ 2 μ 3 μ n as defined previously then the following sequences are typical in the sense of being close enough to the probabilistic expectation for ϕ 64 t n ϵ 1 μ n a μ pr ϕ n μ μ ϵ 1 δ φ ϵ 1 here ϵ 1 ϵ 1 σ μ n μ 5 3 the typical set criterion 2 in information theory the typical set is usually explicitly defined in terms of only the information content this creates some problems primarily the problem that arises is due to the fact that one assumes a priori that the ensemble of sequences involved m r n are distributed close to their probabilistic ratios as discussed in section 5 2 but no explicit requirement is formulated to reinforce this necessity for example see mackay 2003 p 80 where the concept of the asymptotic distribution is used to define the probability of a typical sequence but this argument is not later enforced in the definition of the typical set the analysis presented in the previous section fills this role and ensures that the underlying assumptions are in fact met before any consideration regarding the information content is considered the definition of the set t n ϵ 1 exists only to formally impose this condition fig 7 for the second part of the analysis we need to establish what it means for a sequence to have an information content that is typical note that first any such sequence must be in the set t n ϵ 1 by definition every such sequence has roughly the appropriate distribution of symbols such that it is close to the probabilistic distribution p i e criterion 1 in general for a discrete distribution p with m categories sequences that were near their probabilistic distribution in symbol content would have the approximate probability 65 pr μ n ϕ i 1 i m p i n i where ni is the number of occurrences of symbol i in the sequence i e ni is the microscale degeneracy by definition a typical sequence is on where ni npi making this substitution and taking logarithms it is straightforward to show 66 i μ n ϕ log 2 pr μ n ϕ n i 1 i m p i log 2 p i n h p where h p is the entropy for the discrete distribution p in the case of the bernoulli variables discussed here the discussion above means that every typical sequence must have a distribution of approximately nρ high conductivity cells and n 1 ρ low conductivity cells to be in t n ϵ 1 the probability of generating any one such a sequence is 67 pr μ n ϕ ρ n ρ 1 ρ n 1 ρ the information content of any such sequence is approximately 68 i μ n ϕ log 2 pr μ n ϕ n ρ log 2 ρ n 1 ρ log 2 1 ρ n h b where h b is the entropy of the bernoulli density distribution b in general for any discrete density distribution p a typical sequence as measured by information content is then a sequence whose information content is near i μ n h p in other words for a specified allowable acceptance region of ϵ2 a typical sequence for a distribution p would be expected to have an information content in the range 69 n h p ϵ 2 i μ n h p ϵ 2 or equivalently within the range of probabilities 70 2 n h p ϵ 2 pr μ 2 n h p ϵ 2 adopting the notation of mackay 2003 the set of typical sequences t n ϵ 2 is given by 71 t n ϵ 2 μ n a μ 1 n log 2 1 pr μ n h p ϵ 2 from a more practical perspective one might impose the following condition defining typicality in terms of information content 72 i μ n n h p 1 ϵ 2 where ϵ 2 ϵ 2 n h p represents the fractional deviation allowed in information content from the expected value n h p as with the set t n ϵ 1 as typical set as measured by information content is controlled by how much error ϵ2 one is willing to accept typical sets can potentially be defined for any sequence length n note that because like the mean the information is a summary statistic i e i μ n log 2 p 1 log 2 p n we might hope that the information content also exhibits central limit type behavior for sufficiently large r and n in the appendix it is illustrated that this is true at least for bernoulli trials thus one can use the central limit theorem similarly to the example provided in 5 2 to determine a required sequence length n to assure that the ensemble of realized sequences is distributed roughly according to the correct expected probabilistic ratios with these definitions in place the definition of a typical set can now be stated more concretely definition typical set suppose that there exists an ensemble m r n of sequences μ μ 1 μ 2 μ 3 μ n distributed with a probability density p a typical sequence is one where 1 the sequence is typical in its distribution of symbols that is it is in the set t n ϵ 1 and 2 the sequence is typical in its information content that is it is in the set t n ϵ 2 using the notation from above the typical set is the collection of typical sequences stated by 73 t n μ a μ μ t n ϵ 1 μ t n ϵ 2 example 11 typical set information limits as an example of how both volume fraction or information content can be used to define the typical set fig 8 illustrates the results of generating bernoulli sequences with p 1 10 and with n 100 or n 500 the two parameters for establishing the typical set interval are set to ϵ 1 ϵ 2 0 1 in fig 8 a c the results of generating sequences of length n 100 are shown on the left the sequences are illustrated where the light colors represent the high k regions and dark colors represent the low k regions as indicated on the color bar the sequences in a and b are arranged from the highest value of volume fraction or information content at the top of the region to the lowest value on the bottom of the region in this particular case both volume fraction and information content exactly correspond so the two figures list the sequences from top to bottom identically although this will not always be the case the shades regions at the top and bottom of figures a and b show the sequences that would be qualified as non typical by each metric in fig 8 c a histogram of the volume fraction or relative information content is shown with the grey shaded region indicating the typical sets in fig 8 d f similar results are plotted but for the case n 500 these results show several important features first the criteria based on the volume fraction ϕ or more generally on some statistic ψ μ n and on the information content i μ n ϕ give in general different results when ϵ 1 ϵ 2 for the case shown here the volume fraction criterion is more limiting i e more sequences are rejected but this is not necessarily always the case the limiting feature will be determined by the structure of the fields involved and the particular values of ϵ1 and ϵ2 or equivalently the values of ϵ 1 and ϵ 2 that are selected second the results show that as the sequence length increases the variance of the distribution decreases this occurs in alignment with the law of large numbers however when applied this way the statistical rule is usually called the asymptotic equipartition theorem this will be discussed in detail in the section following the necessity of using both the probabilistic eq 64 and the informatic eq 71 definitions of the typical set can be seen more clearly by examining the uniform case for binary random variables ρ 1 ρ 1 2 the results for that case are provided in fig 9 there are several notable features for the case ρ 1 2 the first is that the distribution is uniform and the corresponding microscale entropy h b is maximum however because the distribution is uniform this means that all possible microscale distributions are equally likely each with a probability of pr μ 1 2 n while information content generally makes a sensible metric for defining the typical set in general it must also be remembered that the definition of typical sets require that the realized sequences first be distributed near their probabilistic proportions as an example of the importance of this suppose we examine 100 sequences of bernoulli trials with p 1 2 each with n 500 elements of these suppose that one were to obtain one sequence containing all 1 s there is only one such sequence and the probability of obtaining ϕ 1 in this way is 1 2 500 a very small number in contrast the probability of obtaining a sequence with ϕ of exactly 1 2 is about 1 2 500 1 2 495 that is there are approximately 2495 such sequences the chance of observing a sequence of 500 1 s in a row is so small that when realizing 100 such sequences none of them would be expected to contain only 1 s thus in a set of sequences that did contain such a realization that realization would not belong to typical set because it would lead to an observed distribution for ϕ that was not consistent with its probabilistic one in this case even though each realization contains the same amount of microscale information iμ this does not mean that all sets are indeed typical as is often stated essentially what the first part of the criteria for the typical set eq 71 imposes is an enforcement of what is required when defining the typical sequence in terms of information to begin with i e that the distribution of realized sequences must first be consistent with the probabilistic distribution before they can be categorized with regard to information content 5 4 the representative volume as a typical set the definition of the representative elementary volume rev or equivalently representative volume element rve has been presented many times usually in a semi quantitative way e g whitaker 1999 bear 1972 but also using quantitative metrics e g bachmat and bear 1987 pecullan et al 1999 to simplify notation we will adopt the terminology representative volume rv indicating that the volume is not necessary elementary i e it is not the smallest such volume nor is it necessarily a volume element in any formal sense e g as would be seen for periodic media here we make a link with information theory by illustrating how the concept of an rv can be defined using the notion of belonging to a typical set which is a common means of categorizing sequences in information theory to start the discussion we explicitly note the fact that for finite length sequences the best that one can do is to offer a probabilistic interpretation of the notion of typical because finite ensembles m r n will always have uncertainty in the computed statistics under such conditions the concept of typical itself is somewhat subjective in the sense that it is dependent upon how much error we are willing to allow and potentially to the utility function assigned to that error this is the focus of much of the discussion in this section the problem of defining and rv as a typical sequences becomes more concrete as the length of each sequence n increases suppose we are interested in a particular macroscopic statistic of the sequences such as the volume fraction the average of μ i ϕi as n becomes large enough almost all sequences measured by the average ϕi are in the typical set this is an important feature of typical sets in many applications e g communication theory compression of files and it has applications in upscaling when one wants to define a representative volume jaynes 1957a made this point clear by stating the theory makes definite predictions as to experimental behavior only when and to the extent that it leads to sharp distributions in the original work by shannon 1948 there is a theorem that has become to be known as the asymptotic equipartition theorem or due to later extensions others the shannon mcmillan breiman theorem algoet and cover 1988 gray 2011 this is roughly speaking an application of the law of large numbers to the idea of the average or other statistics of sequences in short the idea is that for long enough sequences the probability density distribution becomes highly peaked thus the probability that a statistic ψ μ n of the sequence μ n varies from its expected value becomes exceedingly small again following mackay 2003 who also provides a careful and well explained derivation a useful statement of the theorem is given as follows theorem the asymptotic equipartition principle aep for an ensemble of sequences m r n composed of n i i d random variables with discrete density distribution p every randomly realized outcome in the ensemble μ n μ 1 μ 2 μ 3 μ n can be measured by the average and the information content to generate the typical set tn for n sufficiently large μ n is almost certain to belong to the set tn from a more practical sense this theorem states that once the notion of typical sets has been fixed then as one examines sequences with increasing numbers of elements the probability of observing a result with an average value different from that of the typical set becomes increasingly unlikely in other words for any value ϵ3 indicating the probability that one of the possible sequences is not in the typical set there is always a value of n large enough such that 74 pr ψ μ n t n ϵ 3 or equivalently pr ψ μ n t n 1 ϵ 3 proofs of this theorem for the information component t n ϵ 2 can be found elsewhere shannon 1948 mackay 2003 cover and thomas 2012 proofs for the volumetric or other variable content t n ϵ 1 rely strictly on the validity central limit theorem the developments above provide one method of defining the concept of the representative volume for uncorrelated fields although a number of definitions for the rv have been proposed bachmat and bear 1987 wood and valdés parada 2013 rozenbaum and du roscoat 2014 wu et al 2017 jiang et al 2018 definition of the rv on the basis of information theoretic concepts appears to be a novel approach with that goal the following definition for the rv in the context of information theory is proposed definition representative volume rv uncorrelated case let μ μ 1 μ 2 μ 3 μ n be a discrete sequence of length n used to encode a field property measured by the macroscopic variable ψ in 1 2 or 3 dimensions defined by an i i d random process with a discrete probability distribution function with m categories the sequence is said to be an representative volume rv if for some small value of error ϵ3 with either ϵ 1 ϵ 3 or ϵ 2 ϵ 3 depending upon which of the two control the analysis the sequence is a member of tn with probability greater than 1 ϵ 3 in other words the sequence must be typical both in terms of its probabilistic distribution and in terms of its information content and the number of elements in the sequence must be large enough such that nearly every realization is a member of the typical set with probability 1 ϵ 3 as an example of how an rv might be defined for the volume fraction ϕ is given as follows in fig 6 both the theoretical distribution and a distribution generated by an observed ensemble of realizations via monte carlo simulations for the volume fraction ϕ are given as a function of increasing n it is clear that as n increases the width of the distribution decreases roughly as 1 n while all realizations for ϕ are still possible as n increases the probability of encountering a randomly generated realization where ϕ is significantly far from the value ϕ 1 2 becomes increasingly remote for a fixed values of ϵ1 ϵ2 and ϵ3 this means that as n increases nearly all of the realized sequences are in the typical set tn example 12 the representative volume as a typical set consider the example of the binary conductivity example given in 4 for the case n 9025 n 95 does the 95 95 region represent an rv for the volume fraction of course there is no one answer to this question suppose we set ϵ 3 0 01 now we ask the question is the n 9025 case representative we know already that for the case ρ 1 2 all sequences are typical in terms of information content this leaves only the membership in the set t n ϵ 1 as the controlling feature of the rv this is equivalent to imposing from eq 64 75 pr ϕ n μ μ 0 01 δ φ 0 99 this is very much like the problem addressed in example 10 except now in addition to typicality we demand that n is large enough such that nearly every realized sequence generated at random is typical with very high fidelity recall that μ 1 2 and σ μ 1 2 so that μ σ μ 1 from the central limit theorem we have 76 pr ϕ n μ σ μ n 0 01 n 0 99 77 ϵ 3 0 01 n 78 φ ϵ 3 φ ϵ 3 0 99 or φ ϵ 3 0 995 the root of eq 78 gives ϵ 3 φ 1 0 995 2 57 combining these results we find 79 pr ϕ n μ 0 01 0 99 n 2 54 0 01 2 or n 64516 this is a very large number of realizations even for the very narrow peak illustrated in fig 6 for n 9025 the sequences generated cannot be said to be representative at a level of ϵ 3 0 01 this is a very stringent requirement it states that we are allowing less than one out of 100 realizations to have a volume fraction that is be more than 0 01 μ 0 0005 away from the mean value of ϕ 1 2 obviously we can relax our standards on ϵ3 if we like to generate a higher error rate as is typical there is no concrete way to set ϵ3 without some external constraint such as a utility function for representing benefit of minimizing errors the use of a utility function to sharpen the analysis is explored in section 6 5 5 a short note on information in correlated fields the information associated with spatially correlated fields uses similar concepts as for uncorrelated fields but with recognition that joint probabilities are no longer independent while the material in this paper is focused primarily on uncorrelated fields the extension to correlated fields is not substantially different and the necessary framework has already been established the transition to correlated fields should involved two components first the correlated sequences which could be time series or spatial fields would be approximated by a higher order markov process lindgren et al 2011 rue and held 2005 second depending upon the specific statistical nature of the fields there are a number of extensions to the aep to high order markov processes continuous distributions and even nonstationary processes e g barron et al 1985 algoet and cover 1988 yang and liu 2004 melnik and usatenko 2018 in combination the developments above should be extendable to cases where time or space correlated sequences can be considered 6 application 2 upscaling hydraulic conductivity improved constraints via imposition of a utility function as a second application we examine the problem of upscaling the hydraulic conductivity generated for the ensembles of high and low conductivity materials described above with ρ 1 2 as will be described in additional detail in the material following this process involves 1 generating an ensemble of 2 dimensional sequences of hydraulic conductivity 2 solving the momentum balance equation for each realization of the ensemble and 3 computing the effective hydraulic conductivity keff by 80 k e f f u x δ h l where ux is the spatially averaged empirical velocity and δh is the hydraulic head drop and l is the domain length in the average direction of flow which is aligned with the x axis there are several significant differences between the problem of the volume fraction generated by a bernoulli process and the associated problem for the effective hydraulic conductivity and associated velocity field these are as follows 1 the dimensionality of the problem is a crucial component of the sequences because of the local velocity is determined by the nonlocal distribution of high and low conductivities for the volume fraction the dimensionality of the sequences made no change in the effective property derived the volume fraction for the hydraulic conductivity however the solution locally depends strongly on the solution in a neighborhood of any square 2 while the underlying process that defines the distribution of the local microscale conductivity is still a bernoulli process the dependent variable of interest is no longer defined exclusively by the hydraulic conductivity the dependent variable is now technically the velocity field and this is determined through a balance equation and appropriate boundary conditions and these can influence the resulting values of the effective parameters pecullan et al 1999 for this latter problem the issue is ameliorated somewhat by the use of periodic boundary conditions where possible 3 unlike the case for the distribution of volume fraction ϕ we do not know the probability distribution of the effective hydraulic conductivity keff or equivalently the velocity field in each cell at the microscale in other words even given the microscale velocity field which is computed as part of the upscaling process we do not know how to assign a probability to each element in the sequence p s y m b o l such that the probability of observing keff can be computed from it to complicate the problem the resulting probabilities would have to be assigned to events the value of the effective hydraulic conductivity that were no longer discrete ones thus the probability would need to be specified by either a continuous pdf or by a discretized approximation to the distribution none of this is to say that such a description is not possible however it would take a careful derivation via upscaling of the micro macro connections with a particular emphasis on information theoretic concepts to accomplish this while we use upscaling techniques in the material that follows the are not done in manner described above connecting the upscaling needed to define keff with the underlying microcale information theoretic properties needed to characterize it is an interesting question but it is well beyond the scope of this more introductory survey however we will describe some macroscale information theoretic conclusions that can be made from the upscaling effort 4 as a final complication we note that the information measures of data regarding the finite ensemble of conductivity fields will depend upon how the data is segmented at one extreme one can develop a maximum entropy estimate for the data set by simply enumerating the number of unique values of keff within the sequence of outcomes at the other extreme one could summarize the data by lumping all data points into a single bin equal to the mean which would contain zero information in between these two extremes are segmentations of the data with binning to create degeneracy this points to a important property regarding the entropy of data much like for example the variance of finite data set the value predicted for the data will depend on how it is segmented one solution to this issue will be presented below 6 1 upscaling as mentioned above to the authors knowledge a derivation of the effective hydraulic conductivity with attendant descriptions of the microscale information features does not exist however more conventional upscaling without any specific efforts to connect to information theory have been studied using many different upscaling methods for decades e g kim and torquato 1990 hui and ke da 1992 obnosov 1996 and reviews of most of the primary methods are available in the literature olariu et al 2013 renard and de marsily 1997 for this paper we follow essentially the derivation for the effective hydraulic conductivity permeability outlined by whitaker 1999 chp 5 it is easily shown that the closure problem given by whitaker 1999 chp 5 is equivalent to solving the microscale equation in a representative volume with a pressure gradient oriented with one of the coordinate axes we have chosen the x direction although the y direction should give identical results the effective hydraulic conductivity is then determined by inverting the expression 81 u 0 u i κ eff μ f p β i where here represents the spatial superficial empirical average whitaker 1999 chp 1 2 82 u x r v x u r w r x d v y and w is a generally compact weighting function defining the volume average for this work we use the uniform weighting function w 1 v where v is the volume of the region of interest for the remaining terms κ is the permeability tensor μf is the fluid viscosity p β is the gradient of the intrinsic average pressure and i is the unit tensor in the x coordinate direction noting that when p β δ p l 0 0 and κ κi this reduces to 83 u 0 u x κ eff ρ f g μ f δ p l ρ f g where we have scaled by ρfg to convert to the conventional effective hydraulic conductivity keff instead of the effective permeability κeff and the pressure into pressure head h via 84 k eff κ eff ρ f g μ f 85 δ h l δ p ρ f g thus from our numerical results we compute 86 k e f f u x δ h l we note the following for comparative purposes for a random checkerboard with ρ 1 2 in the limit as n the hydraulic conductivity approaches cf dykhne 1971 torquato and hyun 2001 obnosov 1996 87 k e f f k 0 k 1 for this expression to be valid the ratio of k 1 k 0 must not be too large to our knowledge no analytical results exist for the variance of the upscaled hydraulic conductivity as a function of n n n however it has been suggested that the hydraulic conductivity or equivalently the permeability might follow the central limit theorem for random checkerboards distributed by i i d processes olariu et al 2013 6 2 simulations as mentioned previously one of the problems encountered with this simple geometry is that there are discrete boundaries that can have poor numerical convergence with increasing conductivity ratio however for relatively small conductivity ratios this does not create substantial problems thus in these examples the ratio of hydraulic conductivities will be kept to k 1 k 0 4 even with these conditions the convergence rate with decreasing grid size shown only convergence of order 1 29 as opposed to second order convergence which would be expected for a homogeneous domain we set the inlet and outlet hydraulic head boundary condition such that the average gradient of the hydraulic conductivity would always equals to d h d x 1 periodic hydraulic conductivity was applied at the top and the bottom boundaries by utilizing a linear extrusion in comsol multiphysics 5 4 note that the finite element implementation in comsol multiphysics 5 4 imposes flux continuity at the discontinuous surfaces and allows for concentration jumps at the discontinuous interface between high and low conductivities thus the scheme is well suited to the problem of resolving fluxes at the interfaces and corners as discussed by olariu et al 2013 we have summarized the computational parameters in table 2 in fig 10 we have illustrated one realization of the 4 4 case and the associated darcy velocity field to assure that we had achieved grid independence the grid convergence index gci roache 1994 was computed for the simulations this index provides a bound on the estimated error of the numerically converged solution and is reported in table 2 for all simulations we imposed the condition that the gci be on the order of 1 10 5 or less indicating an essentially grid independent solution 6 3 entropy computation for binned data sequences as described in the introductory paragraphs of this section for the problem of the effective hydraulic conductivity we are not attempting to connect the microscale structure directly to the macroscale effective conductivity however there is still substantial potential for upscaling the method that will be pursued here addresses the question what data about the ensemble of macroscale effective hydraulic conductivities needs to be kept this is an example of a problem where defining a utility function is essential for developing a concrete result in table 3 an example of binned data for keff for the 4 4 case will be described in detail the remaining domain sizes are treated similarly for now we need know only the following 1 the data represent the computation of keff from 65 536 realizations of the binary hydraulic conductivity fields described previously for this particular example each realization is 4 cells by 4 cells in size the resulting effective hydraulic conductivities represents a macroscale ensemble m 65536 1 of effective hydraulic conductivity values 2 the 65 536 values for keff for this particular set of realizations turns out to have only 4729 unique values thus there is substantial degeneracy in the data 3 recall r 0 is the number of bins used to represent a discrete probability density or mass distribution the values of h r 0 are computed by binning the data a number of different binning structures were examined these started with a large number of bins to achieve as close to a uniform maximum entropy distribution as possible and subsequent computations decreased the number of bins by factors of 2 4 the maximum possible entropy indicating uniform distribution of the data is given by the log of the number of distinct data in this case h log 2 4729 12 21 there is a technical point to be made about the entropy of a distribution versus the entropy of an ensemble of sequences the entropy of a distribution represents the expected amount of information required to encoded events drawn from that distribution goodfellow et al 2016 it does not however represent the information required to encode the distribution itself this is an important distinction the difference between the two is only multiplication by a constant which represents how the probabilities themselves are expressed as an finite alphabet therefore the entropy of a distribution is useful for relative comparisons about events and ratios of entropies give the correct value of the information encoded by two discrete distributions the reason for this distinction is important when considering overall information compression between say the data encoded by an ensemble and representation of the same ensemble by a density histogram this distinction will be explained further in the next example for the case under consideration we have an sequence of data for keff extracted from an ensemble of realizations while these data meet the technical requirements for the central limit theorem to apply we do encounter a frequent problem with analyzing finite data sets the entropy depends upon the discretization the number of bins used to construct the empirical pdf unlike conventional statistics like the mean or variance the entropy is strongly dependent upon the discretization used to describe the discrete pdf as an example consider a discrete uniform distribution on x 0 1 the statistics for the discrete uniform distribution do not have a strong dependence on the discretization of the distribution the mean is always correct and the variance rapidly approaches the asymptotic value of 1 12 as the number r 0 of bins grows with only 4 bins the variance is already at 93 of its asymptotic value the entropy on the other hand changes substantially with bin size to see this we need only recall that the entropy for the uniform distribution for x 0 1 with r 0 bins is 88 h i 1 i r 0 1 r 0 log 2 1 r 0 log 2 n this feature of the entropy makes is very unlike conventional statistical measures of discrete data in context this makes sense the entropy is a measure of the amount of information encoded by a sequence the longer the sequence in this case the larger the number of bins the more information is retained as an illustration of this we have computed the entropy for the ensemble of 65 536 values of keff for the 4 4 configurations of high and low conductivities table 3 this represents a computation for keff for each possible configuration these entropies were computed for various estimated discrete probability density functions to begin the analysis of the data our first objective was to compute the largest entropy possible for the data by increasing the number of bins used to represent it while the data were not uniformly distributed it was possible to obtain a representation that was nearly uniform with an computed variance of h 12 08 which is close to the theoretical maximum h 12 21 then the number of bins for the discrete pdf were decreased by a factor of 2 until only a single bin represented all of the data with a corresponding entropy of 0 to help visualize these data they are plotted in fig 11 a number of researchers have shown that the sum of i i d random variables converges to a normal probability distribution in the sense of entropy linnik 1959 barron 1986 it is also shown that when the data are described by a normal distribution the entropy decreases with each power of 2 in the bin number to be more clear suppose the discrete pdf is defined with r 0 bins of size δb with p i p i δ b a bin set that was half as big is represented with a number of bins equal to r 0 1 2 1 r 0 in general we set the number of bins to r 0 k 2 k r 0 with r 0 0 r 0 and the entropy scales as 89 h r 0 k i 1 i r 0 p i log 2 p i δ b 2 k where k 0 1 2 indicates the number of doublings for δb a small amount of algebra gives 90 h r 0 k i 1 i r 0 p i log 2 p i k for all cases the k l divergence was computed from 91 d k l p u i 1 i r 0 p i log 2 r 0 p i i 1 i r 0 p i log 2 p i log 2 r 0 with r 0 4729 this allows us to use the distribution with maximum entropy as a constant comparator one might argue that using the uniform distribution with the same number of bins i e using r 0 k k 0 instead of r 0 would be more consistent with the definition of the kl divergence however our particular choice which is only possible for uniform distributions is useful in that the sum of the empirical entropy plus the kl divergence is equal to h for all histogram bin sets thus the kl divergence in this case allows us to think of the maximum entropy being divided up into 1 the measured entropy embedded in the discrete pmf and 2 the remaining entropy which has been eliminated by the representation chosen to denote that we are adopting a particular variation the kl divergence we have denoted it by d k l we expect the data from the ensemble of keff to be well represented by a normal distribution assuming that the data can be binned appropriately under those conditions the entropy should scale as shown in eq 90 the question now is to determine what bin size selection yields the best choice among the possibilities fig 11 suggests that there is an interval of bin numbers where the entropy scales as for a normal distribution to find the best bin distribution it is necessary to add constraining information in terms of a utility function ξ for this problem we adopt the conventional coefficient of determination for the linear plot of discretized normal density distribution compared with the empirical distribution we discretize the domain of the keff as having r 0 bins with r 0 1 bin boundaries defined by b b 1 b 2 b r 0 1 the discretized normal density distribution is defined for the bins defined by b by cf cover and thomas 2012 8 3 92 n i 1 δ b pr b i k e f f b i 1 1 δ b y b i y b i 1 1 2 π σ 2 exp y k e f f 2 2 σ 2 d y for this case we took ξ r 2 where r 2 was determined by plotting the empirical probability defined at resolution k but evaluated at the r 0 original bins versus the discretized normal n i also evaluated at the original number of r 0 bins the problem then resolves to finding the particular set of bins b such that r 2 is maximized the details are presented in the next example example 13 computation of hydraulic conductivities for this example we compute the effective hydraulic conductivity for r realizations of 2d random sequences each with n n n cells of high and low conductivity cells the probability of a high conductivity cell was ρ 1 2 the effective hydraulic conductivity was computed numerically by constructing domains subdivided into n n 2 square cells representing either a high conductivity k 1 0 004 m s 1 or a low conductivity k 0 0 001 m s 1 material as described above the remaining properties for the simulations are listed in table 2 sequences were constructed to represent square 2 dimensional domains in other words the sequences were of the form μ n μ 11 μ 12 μ 1 n μ n 1 μ n 2 μ n n for the first two domain sized studied 3 3 and 4 4 the effective hydraulic conductivity for all possible configurations was computed for the remaining cases it was not practical to compute all possible realizations instead monte carlo methods were used to sample the space of possible configurations generating a sample of keff if sampled densely enough the sampled data sets can provide reasonable estimates for the distribution of keff the statistical properties of the ensemble of the keff are given in table 4 the behavior of the standard deviation for keff for each ensemble size indicates that it is proportional to 1 n which is consistent with the limiting behavior predicted by central limit theorem to determine the histogram binning b with the highest utility for the keff ensembles computed for each domain size we maximized the observed value of r 2 as described above the data from these computations are summarized in table 5 a few notes are necessary regarding these data for the ensembles corresponding to n 9 and n 16 exhaustive sampling of all possible configurations was possible for these cases the entropy estimates are quite robust in the sense that the empirical entropy and the probabilistic entropy should coincide for the remaining cases it was only possible to compute a sample of the possible configurations as an example the 10 10 case has 2100 possible configurations clearly it is not practical to sample this data set exhaustively as an example of the results of the best fit for the normal approximation to the histogram appear in fig 12 in fig 12 a we show the maximum entropy histogram for the data for the 4 4 case this histogram is nearly uniform in the sense that the dkl is a small fraction of the maximum entropy about 1 as indicated in table 5 a series of bin distributions were tried for this case and the optimal one from the perspective of the utility function ξ r 2 was the one with k 12 r 0 k 37 this discrete pdf is illustrated in fig 12 b for this distribution the two competing objectives of minimizing the information content and maximizing the fidelity of representation by a normal distribution are met the resulting pdf represents a compression of the information comparing the normal distribution to the uniform distribution by over 64 thus the final result is one that is both statistically robust it is the best representation of the histogram as informed by the central limit theorem and one that minimized data to the extent possible given the constraints of the utility function note that entropy minimization problems with more complex utility or cost functions are possible using conventional lagrange multiplier techniques boso and tartakovsky 2018 the remaining ensembles were treated similarly all ensembles were able to be fit with a normal distribution reasonably well the smallest domain size the 3 3 case showed both the lowest compression and the least r 2 value for normal distribution fit this is somewhat expected while the number of realizations was large r 512 each sequence was somewhat small n 9 the central limit theorem conditions required both r and n to be sufficiently large in order for it to apply when the number of cells n is small no number of realizations will overcome the fact that the distribution of hydraulic conductivities is different from normal regardless information compression was significant ranging between 56 to 68 these rates of compression are reasonably large however for context recall that some of the original sequences were quite large for example the 30 30 case contained 998 sequences of n 900 length the raw bit length each sequence is a log 2 1 2 900 900 bits the entropy for the ensemble is easily computed to be the average information for the sequences which is also h m 900 bits for the information encoded by the distribution recall that the entropy of a distribution encodes the average information required to represent the outcomes the keff of the distribution the distribution itself requires more information to encode as an example for the n 4 case given above there were 37 bins used to represent the discrete probability distribution for each of these 37 bins an associated probability is required suppose we choose to represent these probabilities using 4 decimal digits this defines an alphabet for the probabilities a 0 0000 0 0001 0 0002 1 000 the total number of discrete data points in base 10 units involved are now 37 bins resolved with an alphabet of 10 001 probability values the maximum amount of information required to represent the data is i log 2 37 log 2 10001 18 4 bits it is possible to improve on this slightly noting that not all of the members of the alphabet the probabilities are equally likely but this makes only a small difference in this example since the maximum entropy estimate is already small compared to 900 now we are in a position to examine the relative compression of data comparing the information encoded in the full ensemble of microscale conductivities to the maximum information encoded in the mass histogram the total data compression is 93 r e n s e m b l e 1 h n k h m 1 18 4 900 0 980 the information compression measuring the full ensemble of r 998 realizations of length n 900 as compared to the entropy encoded in the probability mass or density histogram with probabilities resolved to 1 part in 10 000 is about 98 a very significant savings in the space needed to represent the conductivity field finally in fig 13 we have plotted the best fit normal distributions with the corresponding discrete pdfs for the cases that we were able to obtain data for in all cases except the 3 3 case the normal fit to the data provided an excellent approximation the 3 3 has such a small number of possible configurations 512 that it does not have enough degrees of freedom for the results to be distributed by the normal two interesting features arise in these fits to the discrete pdfs first the averaged value of keff was approximately k e f f 0 0021 table 4 whereas the estimates for infinite system was k e f f 0 002 these values are reasonably close with a difference of only 5 it is unclear why there would be any discrepancy the most likely explanation is that the numerical results are affected by boundary conditions these effects become negligible as the domain size becomes arbitrarily large second we noted above that the standard deviation scales as σ o b s σ 0 n the value of σ 0 for these data was σ 0 0 00158 it is interesting to note that the variance for the underlying conductivity field is σ k 0 00155 the closeness of these two values suggests that there is possibly a valid perturbation model for the variance that has not yet been reported in the literature 7 conclusions although the primary purpose of this paper was to provide an introduction to the concepts of information theory as it applies to upscaling it also provided an opportunity to present some novel contributions in summary the primary contributions of the paper are as follows 1 the distinction between microscale and macroscale variables was defined this was done in a manner consistent with previous work 2 we defined both the empirical entropy arising from measurement of an ensemble and the probabilistic entropy predicted by knowing the exact probability distribution associated with the process the entropy is inherently an averaged and hence macroscale variable we defined the entropy h 1 a macroscale variable arising from the average information of a microscale process and the entropy h 2 a variable at a scale above what we defined as macroscale arising via conducting a second average of the information of a macroscale effective parameter in our case the effective hydraulic conductivity keff both were useful in assessing the amount of information embedded in multiple scales of resolution for a ensemble of sequences representing a physical process 3 the notion of the typical sequence was extended by requiring not only that the macroscale information content be typical but also that the sequences under consideration are typical in the probabilistic sense the examples given were for the case where a binary sequence with equal probabilities was concerned while it is true that all such sequences have the same information content not all sequences should necessarily be considered typical because the information content already requires integrating the data i e determining the probability of the sequence it is inherently macroscale therefore adding the constraint that a complimentary variable in our examples the volume fraction ϕ should also be distributed in a probabilistically representative way is consistent with the notion of typical 4 the representative volume rv was defined using an information theoretic context while this may not be the optimal way to define and rv it provides another metric by which one might consider the notion of what is representative and what is not the advantage that this has is that it joins the conventional theories to define the rv with the well defined notions of typicality in information theory 5 the process of information compression for a sequence representing macroscopic data was addressed in this case the effort was not to explicitly link the microscale information structure to the macroscale effective hydraulic conductivity although this is an interesting problem but to assess what kinds of compression might be acceptable under a constraint imposed by a utility function while the utility function chosen was particularly simple it was defined as the r 2 value for the normal fit to the keff pdf data it nonetheless provided an example where the simultaneous constraints of entropy minimization and utility function maximization were employed declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the anonymous reviewers for providing helpful comments on earlier drafts of this paper the utility of the berry esseen theorem was pointed out by one reviewer appendix a is the information content binomially distributed for binomial distributions in this appendix we consider the distribution for the information content of a sequence of bernoulli trials with length n for every such trial the probability of observing the particular sequence of m high conductivity or outcomes of 1 rather than 0 out of n outcomes is a 1 p μ n ρ m 1 ρ n m the information content for each such outcome is by definition i μ m n m log 2 ρ n m log 2 1 ρ or a 2 i μ m n n m n log 2 ρ n m n log 2 1 ρ as a side note the expected values of m n and n m n are good estimators for ρ and 1 ρ respectively the entropy for the bernoulli distribution is given by a 3 h b ρ log 2 ρ 1 ρ log 2 1 ρ at this juncture it is interesting to look at the average information content or relative information in the terminology of shannon 1948 compared with the entropy to do so we plot 1 niμ and h b in fig a 14 note that in the region where the information content crosses the line of constant entropy the information content is near the entropy of h b by definition this region represents typical sets as qualified by the set t n ϵ 2 the probability distribution for iμ can be obtained by noting the linearity in m of eq a 2 solving this expression for m yields an inverse function predicting m given iμ this expression is a 4 m i μ n n i μ n log 2 1 ρ log 2 1 ρ log 2 ρ this expression links particular values of the information content to values of m from which we can compute the associated probability from a 5 p m i μ n n m ρ m 1 ρ n m where on the right hand side m is determined by the inverse function m m i μ n this effectively allows us to predict the probability associated with a particular value of iμ and n note that the right hand side of eq a 5 is a bernoulli probability density and that m iμ n is a linear function of iμ this means that the resulting probability density function is also bernoulli which illustrates the objective of this appendix for large enough ensembles m r n where recall r is the number of realized sequences and n the length of each sequence the predictions from eq a 5 are very good when compared to the histogram of the information content extracted from m r n a specific example can be seen for the case n 100 ρ 1 5 in fig 8 c where eq a 5 is plotted by the solid blue line n b this is plotted as a continuous line only for ease of comparison 
381,the smoothed particle hydrodynamics sph method is used in this paper to simulate the transport process of coastal pollutant by solving the two dimensional 2d depth averaged advection diffusion equation in lagrangian framework to avoid directly discretizing the second order derivatives the diffusion terms are decomposed into two first order derivatives by introducing the diffusive flux to verify the proposed lagrangian particle model numerical experiments on four typical tests are performed including the discontinuity pure advection grid anisotropy and large deforming flow the high resolution total variation diminishing tvd scheme the modified tvd lax friedrichs method with superbee limiter mtvdlf superbee is also implemented for detailed comparison the simulation results indicated that the present sph based model has better performance than mtvdlf superbee in addition the stability convergence and efficiency of the proposed model are numerically analyzed the lagrangian particle model with diffusive flux gives satisfactory results with big time step and hence allows large courant number guaranteeing the computational efficiency keywords pollutant transport lagrangian particle model advection diffusion equation mtvdlf superbee sph 1 introduction in recent years coastal environmental problems have attracted wide attention due to their impact on human activities and safety usually sewage outlets are laid along the coastal line and the delivered pollutants are transported by the hydrodynamic environment in the offshore which is often complex and thus complicates the transport process numerical simulation is an effective tool to study the transport principle of pollutants it is known that the transport process of pollutant is modeled by the advection diffusion type equations which can be solved by the traditional grid based numerical method such as the finite difference method fdm alhumaizi 2004 leonard 1979 sankaranarayanan 1998 wang and hutter 2001 finite element method fem idelsohn and oñate 1994 ramadhyani and patankar 1985 finite volume method fvm idelsohn and oñate 1994 and others finlayson 1992 the numerical methods used to solve the advection diffusion equation can be categorized into eulerian and lagrangian ones without special treatment most numerical methods under the eulerian framework cannot obtain good results when solving advection dominant problems with large peclet number pe either producing excessive numerical diffusion or causing non physical oscillations alhumaizi 2004 wang and hutter 2001 methods with first order accuracy such as upwind and lax friedrichs schemes introduce large numerical diffusion or peak clipping wang and hutter 2001 second order accurate methods such as lax wendroff beam warming and fromm cause non physical oscillations leonard 1979 wang and hutter 2001 third order accurate quadratic upstream interpolation for convective kinematics quick scheme still induces noticeable non physical oscillations in the regions of discontinuity or sharp gradient devote and imberger 2009 neumann et al 2011 the total variation diminishing tvd methods utilize flux limiter or slop limiter alhumaizi 2004 wang and hutter 2001 yee 1987 to avoid spurious oscillations in strong gradient regions but small courant number cr has to be used with the expense of high computation cost lin and falconer 1997 the lagrangian method with moving grids devote and imberger 2009 can obtain satisfactory results for one dimensional advection diffusion problems even with high pe and cr because under the lagrangian framework the advection terms vanish by introduction of the moving coordinate however this method will encounter mesh distortion problem when dealing with high dimensional problems eulerian lagrangian or semi lagrangian methods widely adopted in atmospheric simulation pudykiewicz and staniforth 1984 zhang et al 1993 combine the advantages of both eulerian and lagrangian methods however error accumulation in interpolation for advection terms often causes degradation of numerical solution and high order interpolation is hardly constructed on unstructured grids instability problem may also be triggered when discontinuity is involved in the solution on account of the difficulties encountered in grid based methods various meshless methods have been proposed the work of this paper is based on the meshless particle method i e smoothed particle hydrodynamic sph which has good lagrangian property sph is one of the early proposed meshless methods it was invented by lucy 1977 and gingold and monaghan 1977 to solve the astrophysical problems since there is no need to set grids it is considered to be suitable for dealing with large deformation problems in particular in sph the fluid is separated into a certain number of particles each of which carries the information of velocity pressure location density and others depending on the specific problems the motion of fluid or material is simulated through the movement of particles it is not necessary to define particle connectivity in advance when the field variables and their first and second order derivatives are approximated by sph the compactly supported kernel function is utilized to build up the interaction between particles now in the field of computational fluid dynamics cfd more concentrations are focused on the sph method as it has better ability to treat complicated physical problems such as big deformation free surface and multi phase flow than the traditional grid based methods larbe and price 2012 monaghan and kocharyan 1995 monaghan 2012 ye et al 2019 there are some studies on using sph to solve the advection diffusion equation and heat conduction equation with similar form cleary and monaghan 1999 put forward an alternative to the standard sph with the purpose of dealing with the heat flux across discontinuities in material properties zhu and fox 2001 used sph to simulate the diffusion process in porous media in which the diffusive coefficients are highly anisotropic monaghan 2005 discussed how to approximate the second order derivative in sph and demonstrated how to effectively solve the heat conduction and salt diffusion equation aristodemo et al 2010 proposed a two phase sph model with respect to the advection diffusion process and gave techniques on how to approximate the second order derivative when the fluids have different density and the diffusion coefficients are discontinuous recently mayoral villa et al 2016 made use of the open source code dualsphysics to study the migration process of radionuclides in aqueous systems by coupling the fluid equation and advection diffusion equation chang and chang 2017 coupled the shallow water equation and the advection diffusion equation to simulate the pollutant transport in open channel flow concentrating their attention on the steep gradient problems in these simulations the second order diffusion terms are mostly discretized directly by the sph formulation first introduced by brookshaw 1985 the accuracy of this strategy is generally first order and highly affected by particle distribution note that when sph is used to solve the heat equation it can be regarded as an eulerian method on account of the absence of the advection term this paper aims to contribute some basic ideas and pure lagrangian models for simulating offshore pollutant transport to better discretize the diffusion terms in the lagrangian model diffusive fluxes are introduced in this study we focus on the transport process advection and diffusion of pollutants under a given flow field although pollutant transport often couples with the hydrodynamics field since the concern is on the numerical scheme to verify the effectiveness typical numerical examples with known hydrodynamic environment are used to validate the methods the rest of the paper is organized as follows in section 2 the governing equations for modeling pollutant transport are introduced in section 3 the basic theory of sph and how to use it to discretize the governing equations are briefly introduced in section 4 four 2d numerical examples are employed to test the developed algorithm by comparing with analytical solutions and results of grid based method its accuracy stability and computational efficiency are studied in the last section some conclusions are drawn and possible extension of the current research is discussed 2 governing equations most of pollutants in the coastal water are discharged from sewage outlet which can be considered as a point source as aforementioned we use the advection diffusion equation to express the transport of pollutant and this process in shallow water can be described by the following 2d advection diffusion equation because the vertical mixing completes in a very short time 1 c t d i v u c d i v k c q where c x y t is the concentration of pollutant u is the velocity in space k is the diffusive tensor q q x y t is a source or sink term eq 1 denotes the passive mass transport of pollutant when the advection process is dominant it shows the hyperbolic property and is difficult to solve for eulerian methods whereas if the diffusion process is dominant it shows the parabolic property finlayson 1992 keramat et al 2020 integrated along the vertical direction and expressed in scalar form we get the depth averaged equation which denotes the dispersion process of conservative material as 2 h c t u h c x v h c y x k 11 h c x x k 12 h c y y k 21 h c x y k 22 h c y q where c x y t is the mean vertical concentration h h x y t is the water depth u u x y t and v v x y t are the vertical averaged flow velocity along x and y axis respectively which can be given by measured or simulated results of hydrodynamics both the advection and diffusion terms in eq 2 are linear while the source term is often nonlinear in this paper we take the water depth h as a known constant and neglect the source or sink term consequently eq 2 is simplified to 3 c t u c x v c y x k 11 c x x k 12 c y y k 21 c x y k 22 c y the physical meaning of k taylor 1954 holly and usseglio 1984 rodriguez et al 1995 park and seo 2018 deserves some illumination the dispersion process of pollutant in seawater is supposed to be anisotropic due to the irregularity of the flow the dispersion coefficient along the flow direction longitudinal dispersion coefficient caused by velocity gradient over the cross section is larger than lateral mixing coefficient turbulent diffusion is dominant in eq 2 k kij i j 1 2 is a symmetrical array of dispersion coefficients components k 11 x y t and k 22 x y t are depth averaged diffusion coefficients along the x and y axis respectively while k 12 x y t and k 21 x y t are the coefficients for cross terms with the same value they vanish in the natural coordinate system park and seo 2018 and are included in the model on account of the adoption of cartesian coordinate assume that the coefficients of longitudinal dispersion along the streamline and transverse turbulent diffusion perpendicular to the streamline are denoted by α and β respectively then the relation between the coefficients mentioned above can be expressed as holly and usseglio 1984 4 k 11 α cos 2 θ β sin 2 θ k 12 k 21 α β sin θ cos θ k 22 α sin 2 θ β cos 2 θ in which θ positive counter clockwise is the angle from x axis to the streamline with the definition of material derivative d d t t u x v y eq 3 can be transformed into lagrangian form as 5 d c d t x k 11 c x x k 12 c y y k 21 c x y k 22 c y on the moving coordinate system 6 d x d t u and d y d t v note that in lagrangian system it has no need to discretize the advection terms which makes the solution process more convenient the variable c no longer represents the concentration at a fixed point as in eulerian methods but rather the one carried by particles with changing position 3 numerical formulations in this section we discuss how sph method is applied to solve the advection and anisotropic dispersion equation in lagrangian framework the sph method approximates arbitrary field function and differential operator by a kernel function which includes two main procedures one is the kernel approximation and the other is the particle approximation in this paper we study how to use sph to approximate the first order derivatives rather than directly approximate the second order diffusion terms cleary and monaghan 1999 zhu and fox 2001 mayoral villa et al 2016 by introducing an extra variable q known as diffusive flux jeong et al 2003 eq 5 can be split into 7 q k c d c d t q the second order diffusion term is decomposed into two first order equations which not only circumvents the deficiency of approximating the second order term directly monaghan 2005 but also favors the discretization of cross terms induced by the anisotropic dispersion 3 1 sph approximation for any physical quantity f at position x a the kernel approximation of its gradient is expressed as violeau 2012 8 f a ω f x w d x where fa f x a represents approximation w w x a x h is the kernel function which owes the properties of normalization symmetry and compactness violeau 2012 and h is the smoothing length integration by parts generates 9 f a ω f x w d x ω f x w d x in term of the gauss theorem and the compact property of the kernel eq 9 can be further expressed as 10 f a ω f x w d x the cubic spline function widely used in sph is employed herein as the kernel 11 w q h λ h d 2 3 q 2 1 2 q 3 0 q 1 1 6 2 q 3 1 q 2 0 2 q where q x a x h d is the number of spatial dimensions and the normalizing constant λ is 15 7 π in two dimensions to satisfy the normalization condition in sph method continuous media is discretized into a finite number of n particles each particle a 1 n carries the qualities including mass ma density ρ a velocity ua and va concentration ca diffusion flux q a and position x a for particle mass m a ρ a 0 v a 0 where ρ a 0 is the initial density and v a 0 is the volume obtained by partitioning the domain ω into n parts in 2d cases the volume is an area segment initially particles are distributed at cartesian grid points with an equal distance δx and δy the volume of particle a can then be represented as v a 0 δ x δ y during motion a particle is allowed to change its density and volume but its mass generally keeps as a constant the kernel gradient has the following entity 12 b w a b a w a b w x b x a h a x a where b wab and a wab are the kernel gradient with respect to x b and x a respectively and x b is the discretization product of the continuum variable x in eq 8 by applying eq 12 to eq 10 particle approximation of the integral term is obtained as 13 f a b f b a w a b m b ρ b according to the antisymmetric property of the kernel gradient violeau 2012 14 ω w d x 0 eq 13 can be rewritten as 15 f a b f b f a a w a b m b ρ b or 16 f a b f b f a a w a b m b ρ b for the purpose of alleviating the inconsistency caused by irregular distribution of particles 3 2 semi discrete form by applying the sph gradient approximation eq 15 to the lagrangian pollutant transport model eq 7 the semi discrete form is obtained as 17 d c a d t b m b ρ b q b q a a w a b q a k b m b ρ b c b c a a w a b when the diffusion coefficient is constant and isotropic eq 17 is further simplified as 18 d c a d t b m b ρ b q b q a a w a b q a k b m b ρ b c b c a a w a b where k is a scalar independent of time and position in this paper we focus on numerically solving the isotropic case eq 18 for which analytical solutions often exist and hence the efficiency and accuracy of the present particle transport model can be conveniently investigated note that the anisotropic case eq 17 can be solved in a similar procedure 3 3 time integration for time integration implicit integration methods can be used such as the first order backward euler the second order tischer schemes monaghan and kocharyan 1995 the leap frog scheme larbe and price 2012 and the crank nicolson method rook et al 2007 however explicit methods are relatively simple easy to code and can be efficiently parallelized as a result the one step explicit methods are mostly adopted in sph although multi step explicit methods can also be used in this work we applied the one step explicit euler forward method with which the temporal derivative term in eq 18 is discretized as 19 c a n 1 c a n d t b m b ρ b q b n q a n a w a b where q a n k b m b ρ b c b n c a n a w a b and n and n 1 denote the current and next time step respectively for tracking the trajectory of particles eq 6 is discretized as 20 x a n 1 x a n d t u a t n x a n where x x y and u u v are the position and velocity vector respectively the explicit euler method can satisfy the need of accuracy by specifying a time step sufficiently small 3 4 boundary condition although sph has been successful in a broad range of applications several stumbling problems need to be overcome among which boundary condition enforcement is a subtle but difficult issue the logical difficulty is that sph was invented to deal with astrophysical problems for which an important task is to find the system boundary however in practical applications the influence of boundaries has to be taken into account note that there is no reason to assume correct boundary conditions will be automatically implemented in sph because the physical boundary of a system domain does not coincide with the sph interaction boundary the dummy particle method proposed by takeda et al 1994 is used herein in which dummy particles are located outside the system boundary see fig 1 but are included in the sph interaction range 4 numerical results and discussion in this section the developed pollutant transport model is tested via four typical examples in which the flow characters correspond to some particular phenomena in coastal water the initial distribution of pollutant is assumed to be instantaneous plane source the discontinuities or initial sharp gradient problems are supposed to be tough for traditional eulerian methods for the purpose of comparison the high resolution mtvdlf superbee method is also implemented which is considered to have the best performance among the tvd methods wang and hutter 2001 with the help of slop or flux limiter the tvd methods can effectively reduce numerical diffusion and suppress spurious oscillations the accuracy of sph is analyzed by examining the l 1 and l 2 norm errors the stability and computational efficiency of sph are also studied and its robustness is evaluated by changing the peclet number and courant number defined by pex uδx k 11 and crx uδt δx respectively the larger the peclet number is the more evident the hyperbolic property of advection diffusion equation is otherwise the equation shows more parabolic property when pe the equation becomes pure advection that is completely hyperbolic type 4 1 oblique velocity field diagonal transport as is well known when we simulate the hydrodynamics or mass transport with grid based methods in ocean environment the streamlines can hardly align with the grid lines all the time i e the grid anisotropic problem then results with severe false diffusion are obtained ramadhyani and patankar 1985 in sph we do not need to set grids in the computational domain as a result we will not suffer from the grid anisotropic problem in this example the computational domain is square the velocity field is uniform and the streamlines are parallel with the diagonal line comparing with the other three examples this test is more idealized and has exact solution which facilitates further analysis of the proposed model this example is aimed to test the stability accuracy and efficiency in both advection and diffusion dominant cases the computational efficiency is examined when comparable results are given by sph and mtvdlf superbee with different temporal and spatial resolutions the gaussian is employed as the initial distribution of the instantaneous source 21 c x y 0 exp x x 0 2 2 σ x 2 y y 0 2 2 σ y 2 x y ω where x 0 y 0 is the center of the pollutant σ x and σ y are the variance in x and y direction respectively which determine the range of pollutant distribution the computational domain is 0 x 6400 m and 0 y 6400 m σ x σ y 200 the center of source is initially located at x 0 y 0 1300 m 1300 m the flow velocity is u v 1 m s and the initial value of the concentration is from 0 mg l to 1 mg l the dirichlet type boundary condition this is also applied to the rest three examples is prescribed as c x 0 0 c x 6400 0 x 0 6400 c 0 y 0 c 6400 y 0 y 0 6400 the exact solution is 22 c x y t σ x σ x 2 2 kt σ y σ y 2 2 kt exp x x 0 ut 2 2 σ x 2 2 kt y y 0 vt 2 2 σ y 2 2 kt to measure the difference between the numerical results and analytical solutions the error functions of l 1 and l 2 norm are defined as 23 l 1 a 1 n c a e c a n a 1 n c a e 24 l 2 a 1 n c a e c a n 2 1 2 a 1 n c a e 2 1 2 where c a e is the exact solution and c a n is the numerical solution for comparing the stability and efficiency of sph with grid based method adopted both in the advection dominant and diffusion dominant cases seven tests are carried out which can be divided into two groups by the value of diffusion coefficient see table 1 when the diffusion coefficient is 0 1 cases 1 3 with the peclet number up to the magnitude of hundreds or even thousands they are advection dominant which are difficult to handle for grid based methods we performed the test by specifying large time step dt to verify if sph is more stable than the grid based method the initial particle spacing is dx dy 100 m and the number of particles in the domain is 64 64 the smoothing length for particle searching is h 1 33dx the simulation time is 3600 s two layers of dummy particles are placed outside of the boundary see fig 2 for the inlet boundary when the dummy particles cross the boundary line they will become fluid particles and new dummy particles are reproduced outside the boundary for the outlet boundary when dummy particles run out of the computational domain they will be deleted to save computing resources hou et al 2014 the concentration of dummy particles is assigned zero according to the prescribed dirichlet boundary condition the l 1 and l 2 norm errors and the computational time are shown in table 1 apparently the sph method gives results with higher accuracy and efficiency in advection dominant cases in contrast in case 3 when advection dominates the mtvdlf superbee method cannot produce a good result see figs 3 b and 4 even with much more cpu time while when diffusion dominates the two methods have comparable performance in terms of accuracy and cpu time it is noted that the courant number should be much less than one in the mltvd superbee scheme which means that the stability condition is extremely rigorous considering both the cfl condition for advection term and the stability condition for diffusion term the stability condition for eulerian methods is δ t min δ x u δ x 2 4 k provided that both the space interval and velocity in x and y direction are identical in terms of simulation results and error analysis we can state that the stability condition of sph is less rigorous than the one for mltvd superbee the convergence rate of the present method is examined by performing simulations with initial particle spacing of 40 m 50 m 80 m and 100 m to reduce the effect of error in time integration on the convergence rate a sufficiently small time step δt 0 1 s is taken the error data are shown in fig 5 the fitted convergence rate using the least square method is 2 1 which indicates that the developed model has second order accuracy in space 4 2 solid body rotation of gaussian distribution circulation is a common phenomenon in coastal water and sea bays such as the circulation flow composed by the long shore currents rip currents and on shore currents the characteristic of circulation imposes big influence on the pollutant transport in near shore zone in this example and the rest two ones the numerical model for circulation flow field is tested in this example the circulation is solid body rotation expressed as 25 u ω y v ω x where the angular velocity of rotation is ω 0 001 π rad s the gaussian distribution same as the one in example 1 is used to denote the initial profile of pollutant except that the center of the pollutant is located at x 0 y 0 900 m 1700 m as the mixing coefficients in both longitudinal and transverse directions are assumed to be identical i e the diffusion is isotropic the location of the center is decided by the advection behavior after one rotation the center should be exactly at the initial location where the analytical solution is 26 c x y t σ x σ x 2 2 kt σ y σ y 2 2 kt exp x x 0 2 2 σ x 2 2 kt y y 0 2 2 σ y 2 2 kt the diffusion coefficient is k 10 m2 s the time step is δt 1 s and the initial particle spacing is δx 50 m dummy particles are used to enforce the boundary condition of zero concentration different from inlet and outlet boundaries in example 1 dummy particles are fixed and their velocities are set to zero which holds for the rest 2 examples the simulation results at different instants are shown in fig 6 and it is seen that the shape of distribution is still circular even after 1600 s as it should be the transport process is also simulated by the mtvdlf superbee scheme with all the parameters same as used in sph as shown in figs 7 and 8 the distribution is distorted and there is an evident shifting phenomenon along y axis after one rotation in fig 8 the numerical results of both the meshless and grid based method are compared with the analytical solutions in x and y direction showing that sph has better agreement with the analytical solution as aforementioned the grid anisotropic problem also encountered in this test due to the circulation character of the flow field the grid distribution does not align with the streamline all the time which makes the results of grid based methods distorted this does not occur in the lagrangian particle model 4 3 modified molenkamp problem martinez 2006 in this example we focus on the ability of the developed particle transport model to handle problems with discontinuity in pure advection it corresponds to some actual issue such as the contaminant poured into the water body in the moments resulting in a steep gradient of concentration this case is more difficult to simulate than the gaussian one and it is used to further confirm the superiority of sph in this case we compare not only the performance of the two methods under the same temporal and spatial resolution but also the efficiency when they achieve comparable results the flow field is still the solid body rotation as specified in example 2 and the algebraic formula of the distribution is 27 c r 1 1 2 1 tanh 2 b r 2 r 1 r r 1 r 2 2 0 r r 1 r 1 r r 2 r r 2 where r x 1 2cos ωt 2 y 1 2sin ωt 2 1 2 b 3 r 1 0 1 and r 2 0 4 this is a pure advection problem in solid body rotation and thus the initial distribution can be taken as the exact solution after one rotation the results at different time with parameters of dt 1 dx 0 04 and particle number 50 50 are displayed in fig 9 the location and concentration of each particle are clearly shown note that the pollutant distribution supposed to be circular is actually irregular due to the coarse discretization figure 10 compares the sph results with the exact profiles after one rotation in different directions the coincidence of these two profiles implies that the sph method can well handle the discontinuity problem in the pure advection case the slight shifting in x direction in fig 10a is solely induced by the time integration algorithm for comparison this case is also simulated by the mtvdlf superbee scheme with the same parameters as those in sph and the results are shown in fig 11 a there is an obvious peak clipping phenomenon in the polluted zone with the wiggles appeared all over the domain after refining the grids dx 0 005 and shortening the time step dt 0 025 the results are improved see figs 11b and 12 at the expense of more cpu time up to 2406 s while sph needs roughly 1 s because the treatment of advection terms is avoided in addition with smaller space and time steps more intense wiggles are observed as shown in fig 11b and profile shifting in the y direction still exists see fig 12b these phenomena further demonstrate the superiority of sph over grid based high resolution methods therefore the sph method is more suitable for practical applications especially in the ocean area where physical processes are in a larger space time scale 4 4 anisotropic rotation with strong deformation pudykiewicz and staniforth 1984 the flow field of circulation in ocean has a character of strong deformation the pollutant transport in this type of flow becomes more complicated with both the advection motion and the dispersion process in this example we only test the pure advection behavior to inspect the performance of the particle transport model which is compared with that of mtvdlf superbee under different temporal resolution as there is no analytical solution available for this case error analysis is not performed the gaussian is used again to denote the initial pollutant distribution the flow field is given by a streamline function 28 ψ l v m a x 4 1 4 x 2 l 2 cos π y l and hence the velocities in x and y directions are 29 u d x d t ψ y π 4 v m a x 1 4 x 2 l 2 sin π y l 30 v d y d t ψ x 2 v m a x x l cos π y l the streamlines and the flow velocity are shown in fig 13 with vmax 3 m s the computational domain is the same as example 2 the initial particle spacing is dx 50 m the time step is dt 2 5 s and the simulation time is 3400 s the particle distributions at different time are shown in fig 14 and it is clear that with the increasing of time the distribution becomes more irregular due to the non uniformity of the flow velocity comparisons between the results of sph and mtvdlf superbee are shown in figs 15 and 16 the isolines of the sph solution are much smoother than those of mtvdlf superbee see fig 15 implying better resolution at 3400 s a slight fluctuation at the most outside isoline appears in sph see fig 15a due to the disordered particle distribution see fig 14 as shown in fig 16 the sph solution has a higher central intensity than that of the mtvdlf superbee the numerical diffusion in mtvdlf superbee resulting from grid anisotropy is the main reason for the large discrepancy of the amplitude 5 conclusions in this paper the meshless sph method is applied to solve the 2d depth averaged advection diffusion equation in lagrangian framework it provides a new idea for accurate simulation of pollutant transport process in offshore water for the convenience of treating second order derivatives including the cross terms induced by the anisotropy of diffusion coefficient the diffusive mass flux is introduced to separate the original one second order equation into two first order ones the sph based pollutant transport model with diffusive flux is tested by four typical examples each of which has the tough point for solving with numerical tools and is in accordance with the phenomenon of pollutant transport in coastal water the high resolution mtvdlf superbee scheme is also implemented for the purpose of providing reference solutions numerical results indicate that the sph based model has the superiority in solving the advection dominated problems for which numerical diffusion or peak clipping phenomena often appears when they are solved by grid based methods while when diffusion dominates the sph based model has comparable performance with the mtvdlf superbee method in terms of efficiency and accuracy even with a discontinuity the present model can still give good results while the mtvdlf superbee scheme leads to numerical diffusion wiggles and profile shifting the stability accuracy and computational efficiency are also analyzed numerically it is shown that the present method has a looser stability condition and higher accuracy than the grid based methods especially in advection dominated cases with respect to efficiency to achieve similar results in the discontinuity problem sph costs far less time than that for mtvdlf superbee the sph method can also circumvent the grid anisotropic problem since it does not need to set the grids it performs well even though the streamlines are oblique and curving the analysis of convergence indicates that the present sph particle model has second order convergence rate for pollutant transport in flows with large deformation compared with the mtvdlf superbee scheme the sph solution has a higher central intensity even though there are slight fluctuations in the concentration isolines due to irregularity of particles this reflects an inherent defect of sph for flows with possible cavity which deserves a further study and the particle redistribution scheme xu et al 2009 might be useful the study of improved sph based methods that can well adapt to irregular distribution of particles is also in the plan furthermore the present model will be tested for transport problems with anisotropic dispersion credit authorship contribution statement wanying liu software validation writing original draft qingzhi hou conceptualization methodology writing review editing jijian lian data curation funding acquisition anmin zhang resources visualization investigation jianwu dang supervision project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was funded by the national key research and development program of china grant no 2018yfc1407403 and the national natural science foundation of china grant no u1765202 52079090 51478305 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103764 appendix supplementary materials image application 1 
381,the smoothed particle hydrodynamics sph method is used in this paper to simulate the transport process of coastal pollutant by solving the two dimensional 2d depth averaged advection diffusion equation in lagrangian framework to avoid directly discretizing the second order derivatives the diffusion terms are decomposed into two first order derivatives by introducing the diffusive flux to verify the proposed lagrangian particle model numerical experiments on four typical tests are performed including the discontinuity pure advection grid anisotropy and large deforming flow the high resolution total variation diminishing tvd scheme the modified tvd lax friedrichs method with superbee limiter mtvdlf superbee is also implemented for detailed comparison the simulation results indicated that the present sph based model has better performance than mtvdlf superbee in addition the stability convergence and efficiency of the proposed model are numerically analyzed the lagrangian particle model with diffusive flux gives satisfactory results with big time step and hence allows large courant number guaranteeing the computational efficiency keywords pollutant transport lagrangian particle model advection diffusion equation mtvdlf superbee sph 1 introduction in recent years coastal environmental problems have attracted wide attention due to their impact on human activities and safety usually sewage outlets are laid along the coastal line and the delivered pollutants are transported by the hydrodynamic environment in the offshore which is often complex and thus complicates the transport process numerical simulation is an effective tool to study the transport principle of pollutants it is known that the transport process of pollutant is modeled by the advection diffusion type equations which can be solved by the traditional grid based numerical method such as the finite difference method fdm alhumaizi 2004 leonard 1979 sankaranarayanan 1998 wang and hutter 2001 finite element method fem idelsohn and oñate 1994 ramadhyani and patankar 1985 finite volume method fvm idelsohn and oñate 1994 and others finlayson 1992 the numerical methods used to solve the advection diffusion equation can be categorized into eulerian and lagrangian ones without special treatment most numerical methods under the eulerian framework cannot obtain good results when solving advection dominant problems with large peclet number pe either producing excessive numerical diffusion or causing non physical oscillations alhumaizi 2004 wang and hutter 2001 methods with first order accuracy such as upwind and lax friedrichs schemes introduce large numerical diffusion or peak clipping wang and hutter 2001 second order accurate methods such as lax wendroff beam warming and fromm cause non physical oscillations leonard 1979 wang and hutter 2001 third order accurate quadratic upstream interpolation for convective kinematics quick scheme still induces noticeable non physical oscillations in the regions of discontinuity or sharp gradient devote and imberger 2009 neumann et al 2011 the total variation diminishing tvd methods utilize flux limiter or slop limiter alhumaizi 2004 wang and hutter 2001 yee 1987 to avoid spurious oscillations in strong gradient regions but small courant number cr has to be used with the expense of high computation cost lin and falconer 1997 the lagrangian method with moving grids devote and imberger 2009 can obtain satisfactory results for one dimensional advection diffusion problems even with high pe and cr because under the lagrangian framework the advection terms vanish by introduction of the moving coordinate however this method will encounter mesh distortion problem when dealing with high dimensional problems eulerian lagrangian or semi lagrangian methods widely adopted in atmospheric simulation pudykiewicz and staniforth 1984 zhang et al 1993 combine the advantages of both eulerian and lagrangian methods however error accumulation in interpolation for advection terms often causes degradation of numerical solution and high order interpolation is hardly constructed on unstructured grids instability problem may also be triggered when discontinuity is involved in the solution on account of the difficulties encountered in grid based methods various meshless methods have been proposed the work of this paper is based on the meshless particle method i e smoothed particle hydrodynamic sph which has good lagrangian property sph is one of the early proposed meshless methods it was invented by lucy 1977 and gingold and monaghan 1977 to solve the astrophysical problems since there is no need to set grids it is considered to be suitable for dealing with large deformation problems in particular in sph the fluid is separated into a certain number of particles each of which carries the information of velocity pressure location density and others depending on the specific problems the motion of fluid or material is simulated through the movement of particles it is not necessary to define particle connectivity in advance when the field variables and their first and second order derivatives are approximated by sph the compactly supported kernel function is utilized to build up the interaction between particles now in the field of computational fluid dynamics cfd more concentrations are focused on the sph method as it has better ability to treat complicated physical problems such as big deformation free surface and multi phase flow than the traditional grid based methods larbe and price 2012 monaghan and kocharyan 1995 monaghan 2012 ye et al 2019 there are some studies on using sph to solve the advection diffusion equation and heat conduction equation with similar form cleary and monaghan 1999 put forward an alternative to the standard sph with the purpose of dealing with the heat flux across discontinuities in material properties zhu and fox 2001 used sph to simulate the diffusion process in porous media in which the diffusive coefficients are highly anisotropic monaghan 2005 discussed how to approximate the second order derivative in sph and demonstrated how to effectively solve the heat conduction and salt diffusion equation aristodemo et al 2010 proposed a two phase sph model with respect to the advection diffusion process and gave techniques on how to approximate the second order derivative when the fluids have different density and the diffusion coefficients are discontinuous recently mayoral villa et al 2016 made use of the open source code dualsphysics to study the migration process of radionuclides in aqueous systems by coupling the fluid equation and advection diffusion equation chang and chang 2017 coupled the shallow water equation and the advection diffusion equation to simulate the pollutant transport in open channel flow concentrating their attention on the steep gradient problems in these simulations the second order diffusion terms are mostly discretized directly by the sph formulation first introduced by brookshaw 1985 the accuracy of this strategy is generally first order and highly affected by particle distribution note that when sph is used to solve the heat equation it can be regarded as an eulerian method on account of the absence of the advection term this paper aims to contribute some basic ideas and pure lagrangian models for simulating offshore pollutant transport to better discretize the diffusion terms in the lagrangian model diffusive fluxes are introduced in this study we focus on the transport process advection and diffusion of pollutants under a given flow field although pollutant transport often couples with the hydrodynamics field since the concern is on the numerical scheme to verify the effectiveness typical numerical examples with known hydrodynamic environment are used to validate the methods the rest of the paper is organized as follows in section 2 the governing equations for modeling pollutant transport are introduced in section 3 the basic theory of sph and how to use it to discretize the governing equations are briefly introduced in section 4 four 2d numerical examples are employed to test the developed algorithm by comparing with analytical solutions and results of grid based method its accuracy stability and computational efficiency are studied in the last section some conclusions are drawn and possible extension of the current research is discussed 2 governing equations most of pollutants in the coastal water are discharged from sewage outlet which can be considered as a point source as aforementioned we use the advection diffusion equation to express the transport of pollutant and this process in shallow water can be described by the following 2d advection diffusion equation because the vertical mixing completes in a very short time 1 c t d i v u c d i v k c q where c x y t is the concentration of pollutant u is the velocity in space k is the diffusive tensor q q x y t is a source or sink term eq 1 denotes the passive mass transport of pollutant when the advection process is dominant it shows the hyperbolic property and is difficult to solve for eulerian methods whereas if the diffusion process is dominant it shows the parabolic property finlayson 1992 keramat et al 2020 integrated along the vertical direction and expressed in scalar form we get the depth averaged equation which denotes the dispersion process of conservative material as 2 h c t u h c x v h c y x k 11 h c x x k 12 h c y y k 21 h c x y k 22 h c y q where c x y t is the mean vertical concentration h h x y t is the water depth u u x y t and v v x y t are the vertical averaged flow velocity along x and y axis respectively which can be given by measured or simulated results of hydrodynamics both the advection and diffusion terms in eq 2 are linear while the source term is often nonlinear in this paper we take the water depth h as a known constant and neglect the source or sink term consequently eq 2 is simplified to 3 c t u c x v c y x k 11 c x x k 12 c y y k 21 c x y k 22 c y the physical meaning of k taylor 1954 holly and usseglio 1984 rodriguez et al 1995 park and seo 2018 deserves some illumination the dispersion process of pollutant in seawater is supposed to be anisotropic due to the irregularity of the flow the dispersion coefficient along the flow direction longitudinal dispersion coefficient caused by velocity gradient over the cross section is larger than lateral mixing coefficient turbulent diffusion is dominant in eq 2 k kij i j 1 2 is a symmetrical array of dispersion coefficients components k 11 x y t and k 22 x y t are depth averaged diffusion coefficients along the x and y axis respectively while k 12 x y t and k 21 x y t are the coefficients for cross terms with the same value they vanish in the natural coordinate system park and seo 2018 and are included in the model on account of the adoption of cartesian coordinate assume that the coefficients of longitudinal dispersion along the streamline and transverse turbulent diffusion perpendicular to the streamline are denoted by α and β respectively then the relation between the coefficients mentioned above can be expressed as holly and usseglio 1984 4 k 11 α cos 2 θ β sin 2 θ k 12 k 21 α β sin θ cos θ k 22 α sin 2 θ β cos 2 θ in which θ positive counter clockwise is the angle from x axis to the streamline with the definition of material derivative d d t t u x v y eq 3 can be transformed into lagrangian form as 5 d c d t x k 11 c x x k 12 c y y k 21 c x y k 22 c y on the moving coordinate system 6 d x d t u and d y d t v note that in lagrangian system it has no need to discretize the advection terms which makes the solution process more convenient the variable c no longer represents the concentration at a fixed point as in eulerian methods but rather the one carried by particles with changing position 3 numerical formulations in this section we discuss how sph method is applied to solve the advection and anisotropic dispersion equation in lagrangian framework the sph method approximates arbitrary field function and differential operator by a kernel function which includes two main procedures one is the kernel approximation and the other is the particle approximation in this paper we study how to use sph to approximate the first order derivatives rather than directly approximate the second order diffusion terms cleary and monaghan 1999 zhu and fox 2001 mayoral villa et al 2016 by introducing an extra variable q known as diffusive flux jeong et al 2003 eq 5 can be split into 7 q k c d c d t q the second order diffusion term is decomposed into two first order equations which not only circumvents the deficiency of approximating the second order term directly monaghan 2005 but also favors the discretization of cross terms induced by the anisotropic dispersion 3 1 sph approximation for any physical quantity f at position x a the kernel approximation of its gradient is expressed as violeau 2012 8 f a ω f x w d x where fa f x a represents approximation w w x a x h is the kernel function which owes the properties of normalization symmetry and compactness violeau 2012 and h is the smoothing length integration by parts generates 9 f a ω f x w d x ω f x w d x in term of the gauss theorem and the compact property of the kernel eq 9 can be further expressed as 10 f a ω f x w d x the cubic spline function widely used in sph is employed herein as the kernel 11 w q h λ h d 2 3 q 2 1 2 q 3 0 q 1 1 6 2 q 3 1 q 2 0 2 q where q x a x h d is the number of spatial dimensions and the normalizing constant λ is 15 7 π in two dimensions to satisfy the normalization condition in sph method continuous media is discretized into a finite number of n particles each particle a 1 n carries the qualities including mass ma density ρ a velocity ua and va concentration ca diffusion flux q a and position x a for particle mass m a ρ a 0 v a 0 where ρ a 0 is the initial density and v a 0 is the volume obtained by partitioning the domain ω into n parts in 2d cases the volume is an area segment initially particles are distributed at cartesian grid points with an equal distance δx and δy the volume of particle a can then be represented as v a 0 δ x δ y during motion a particle is allowed to change its density and volume but its mass generally keeps as a constant the kernel gradient has the following entity 12 b w a b a w a b w x b x a h a x a where b wab and a wab are the kernel gradient with respect to x b and x a respectively and x b is the discretization product of the continuum variable x in eq 8 by applying eq 12 to eq 10 particle approximation of the integral term is obtained as 13 f a b f b a w a b m b ρ b according to the antisymmetric property of the kernel gradient violeau 2012 14 ω w d x 0 eq 13 can be rewritten as 15 f a b f b f a a w a b m b ρ b or 16 f a b f b f a a w a b m b ρ b for the purpose of alleviating the inconsistency caused by irregular distribution of particles 3 2 semi discrete form by applying the sph gradient approximation eq 15 to the lagrangian pollutant transport model eq 7 the semi discrete form is obtained as 17 d c a d t b m b ρ b q b q a a w a b q a k b m b ρ b c b c a a w a b when the diffusion coefficient is constant and isotropic eq 17 is further simplified as 18 d c a d t b m b ρ b q b q a a w a b q a k b m b ρ b c b c a a w a b where k is a scalar independent of time and position in this paper we focus on numerically solving the isotropic case eq 18 for which analytical solutions often exist and hence the efficiency and accuracy of the present particle transport model can be conveniently investigated note that the anisotropic case eq 17 can be solved in a similar procedure 3 3 time integration for time integration implicit integration methods can be used such as the first order backward euler the second order tischer schemes monaghan and kocharyan 1995 the leap frog scheme larbe and price 2012 and the crank nicolson method rook et al 2007 however explicit methods are relatively simple easy to code and can be efficiently parallelized as a result the one step explicit methods are mostly adopted in sph although multi step explicit methods can also be used in this work we applied the one step explicit euler forward method with which the temporal derivative term in eq 18 is discretized as 19 c a n 1 c a n d t b m b ρ b q b n q a n a w a b where q a n k b m b ρ b c b n c a n a w a b and n and n 1 denote the current and next time step respectively for tracking the trajectory of particles eq 6 is discretized as 20 x a n 1 x a n d t u a t n x a n where x x y and u u v are the position and velocity vector respectively the explicit euler method can satisfy the need of accuracy by specifying a time step sufficiently small 3 4 boundary condition although sph has been successful in a broad range of applications several stumbling problems need to be overcome among which boundary condition enforcement is a subtle but difficult issue the logical difficulty is that sph was invented to deal with astrophysical problems for which an important task is to find the system boundary however in practical applications the influence of boundaries has to be taken into account note that there is no reason to assume correct boundary conditions will be automatically implemented in sph because the physical boundary of a system domain does not coincide with the sph interaction boundary the dummy particle method proposed by takeda et al 1994 is used herein in which dummy particles are located outside the system boundary see fig 1 but are included in the sph interaction range 4 numerical results and discussion in this section the developed pollutant transport model is tested via four typical examples in which the flow characters correspond to some particular phenomena in coastal water the initial distribution of pollutant is assumed to be instantaneous plane source the discontinuities or initial sharp gradient problems are supposed to be tough for traditional eulerian methods for the purpose of comparison the high resolution mtvdlf superbee method is also implemented which is considered to have the best performance among the tvd methods wang and hutter 2001 with the help of slop or flux limiter the tvd methods can effectively reduce numerical diffusion and suppress spurious oscillations the accuracy of sph is analyzed by examining the l 1 and l 2 norm errors the stability and computational efficiency of sph are also studied and its robustness is evaluated by changing the peclet number and courant number defined by pex uδx k 11 and crx uδt δx respectively the larger the peclet number is the more evident the hyperbolic property of advection diffusion equation is otherwise the equation shows more parabolic property when pe the equation becomes pure advection that is completely hyperbolic type 4 1 oblique velocity field diagonal transport as is well known when we simulate the hydrodynamics or mass transport with grid based methods in ocean environment the streamlines can hardly align with the grid lines all the time i e the grid anisotropic problem then results with severe false diffusion are obtained ramadhyani and patankar 1985 in sph we do not need to set grids in the computational domain as a result we will not suffer from the grid anisotropic problem in this example the computational domain is square the velocity field is uniform and the streamlines are parallel with the diagonal line comparing with the other three examples this test is more idealized and has exact solution which facilitates further analysis of the proposed model this example is aimed to test the stability accuracy and efficiency in both advection and diffusion dominant cases the computational efficiency is examined when comparable results are given by sph and mtvdlf superbee with different temporal and spatial resolutions the gaussian is employed as the initial distribution of the instantaneous source 21 c x y 0 exp x x 0 2 2 σ x 2 y y 0 2 2 σ y 2 x y ω where x 0 y 0 is the center of the pollutant σ x and σ y are the variance in x and y direction respectively which determine the range of pollutant distribution the computational domain is 0 x 6400 m and 0 y 6400 m σ x σ y 200 the center of source is initially located at x 0 y 0 1300 m 1300 m the flow velocity is u v 1 m s and the initial value of the concentration is from 0 mg l to 1 mg l the dirichlet type boundary condition this is also applied to the rest three examples is prescribed as c x 0 0 c x 6400 0 x 0 6400 c 0 y 0 c 6400 y 0 y 0 6400 the exact solution is 22 c x y t σ x σ x 2 2 kt σ y σ y 2 2 kt exp x x 0 ut 2 2 σ x 2 2 kt y y 0 vt 2 2 σ y 2 2 kt to measure the difference between the numerical results and analytical solutions the error functions of l 1 and l 2 norm are defined as 23 l 1 a 1 n c a e c a n a 1 n c a e 24 l 2 a 1 n c a e c a n 2 1 2 a 1 n c a e 2 1 2 where c a e is the exact solution and c a n is the numerical solution for comparing the stability and efficiency of sph with grid based method adopted both in the advection dominant and diffusion dominant cases seven tests are carried out which can be divided into two groups by the value of diffusion coefficient see table 1 when the diffusion coefficient is 0 1 cases 1 3 with the peclet number up to the magnitude of hundreds or even thousands they are advection dominant which are difficult to handle for grid based methods we performed the test by specifying large time step dt to verify if sph is more stable than the grid based method the initial particle spacing is dx dy 100 m and the number of particles in the domain is 64 64 the smoothing length for particle searching is h 1 33dx the simulation time is 3600 s two layers of dummy particles are placed outside of the boundary see fig 2 for the inlet boundary when the dummy particles cross the boundary line they will become fluid particles and new dummy particles are reproduced outside the boundary for the outlet boundary when dummy particles run out of the computational domain they will be deleted to save computing resources hou et al 2014 the concentration of dummy particles is assigned zero according to the prescribed dirichlet boundary condition the l 1 and l 2 norm errors and the computational time are shown in table 1 apparently the sph method gives results with higher accuracy and efficiency in advection dominant cases in contrast in case 3 when advection dominates the mtvdlf superbee method cannot produce a good result see figs 3 b and 4 even with much more cpu time while when diffusion dominates the two methods have comparable performance in terms of accuracy and cpu time it is noted that the courant number should be much less than one in the mltvd superbee scheme which means that the stability condition is extremely rigorous considering both the cfl condition for advection term and the stability condition for diffusion term the stability condition for eulerian methods is δ t min δ x u δ x 2 4 k provided that both the space interval and velocity in x and y direction are identical in terms of simulation results and error analysis we can state that the stability condition of sph is less rigorous than the one for mltvd superbee the convergence rate of the present method is examined by performing simulations with initial particle spacing of 40 m 50 m 80 m and 100 m to reduce the effect of error in time integration on the convergence rate a sufficiently small time step δt 0 1 s is taken the error data are shown in fig 5 the fitted convergence rate using the least square method is 2 1 which indicates that the developed model has second order accuracy in space 4 2 solid body rotation of gaussian distribution circulation is a common phenomenon in coastal water and sea bays such as the circulation flow composed by the long shore currents rip currents and on shore currents the characteristic of circulation imposes big influence on the pollutant transport in near shore zone in this example and the rest two ones the numerical model for circulation flow field is tested in this example the circulation is solid body rotation expressed as 25 u ω y v ω x where the angular velocity of rotation is ω 0 001 π rad s the gaussian distribution same as the one in example 1 is used to denote the initial profile of pollutant except that the center of the pollutant is located at x 0 y 0 900 m 1700 m as the mixing coefficients in both longitudinal and transverse directions are assumed to be identical i e the diffusion is isotropic the location of the center is decided by the advection behavior after one rotation the center should be exactly at the initial location where the analytical solution is 26 c x y t σ x σ x 2 2 kt σ y σ y 2 2 kt exp x x 0 2 2 σ x 2 2 kt y y 0 2 2 σ y 2 2 kt the diffusion coefficient is k 10 m2 s the time step is δt 1 s and the initial particle spacing is δx 50 m dummy particles are used to enforce the boundary condition of zero concentration different from inlet and outlet boundaries in example 1 dummy particles are fixed and their velocities are set to zero which holds for the rest 2 examples the simulation results at different instants are shown in fig 6 and it is seen that the shape of distribution is still circular even after 1600 s as it should be the transport process is also simulated by the mtvdlf superbee scheme with all the parameters same as used in sph as shown in figs 7 and 8 the distribution is distorted and there is an evident shifting phenomenon along y axis after one rotation in fig 8 the numerical results of both the meshless and grid based method are compared with the analytical solutions in x and y direction showing that sph has better agreement with the analytical solution as aforementioned the grid anisotropic problem also encountered in this test due to the circulation character of the flow field the grid distribution does not align with the streamline all the time which makes the results of grid based methods distorted this does not occur in the lagrangian particle model 4 3 modified molenkamp problem martinez 2006 in this example we focus on the ability of the developed particle transport model to handle problems with discontinuity in pure advection it corresponds to some actual issue such as the contaminant poured into the water body in the moments resulting in a steep gradient of concentration this case is more difficult to simulate than the gaussian one and it is used to further confirm the superiority of sph in this case we compare not only the performance of the two methods under the same temporal and spatial resolution but also the efficiency when they achieve comparable results the flow field is still the solid body rotation as specified in example 2 and the algebraic formula of the distribution is 27 c r 1 1 2 1 tanh 2 b r 2 r 1 r r 1 r 2 2 0 r r 1 r 1 r r 2 r r 2 where r x 1 2cos ωt 2 y 1 2sin ωt 2 1 2 b 3 r 1 0 1 and r 2 0 4 this is a pure advection problem in solid body rotation and thus the initial distribution can be taken as the exact solution after one rotation the results at different time with parameters of dt 1 dx 0 04 and particle number 50 50 are displayed in fig 9 the location and concentration of each particle are clearly shown note that the pollutant distribution supposed to be circular is actually irregular due to the coarse discretization figure 10 compares the sph results with the exact profiles after one rotation in different directions the coincidence of these two profiles implies that the sph method can well handle the discontinuity problem in the pure advection case the slight shifting in x direction in fig 10a is solely induced by the time integration algorithm for comparison this case is also simulated by the mtvdlf superbee scheme with the same parameters as those in sph and the results are shown in fig 11 a there is an obvious peak clipping phenomenon in the polluted zone with the wiggles appeared all over the domain after refining the grids dx 0 005 and shortening the time step dt 0 025 the results are improved see figs 11b and 12 at the expense of more cpu time up to 2406 s while sph needs roughly 1 s because the treatment of advection terms is avoided in addition with smaller space and time steps more intense wiggles are observed as shown in fig 11b and profile shifting in the y direction still exists see fig 12b these phenomena further demonstrate the superiority of sph over grid based high resolution methods therefore the sph method is more suitable for practical applications especially in the ocean area where physical processes are in a larger space time scale 4 4 anisotropic rotation with strong deformation pudykiewicz and staniforth 1984 the flow field of circulation in ocean has a character of strong deformation the pollutant transport in this type of flow becomes more complicated with both the advection motion and the dispersion process in this example we only test the pure advection behavior to inspect the performance of the particle transport model which is compared with that of mtvdlf superbee under different temporal resolution as there is no analytical solution available for this case error analysis is not performed the gaussian is used again to denote the initial pollutant distribution the flow field is given by a streamline function 28 ψ l v m a x 4 1 4 x 2 l 2 cos π y l and hence the velocities in x and y directions are 29 u d x d t ψ y π 4 v m a x 1 4 x 2 l 2 sin π y l 30 v d y d t ψ x 2 v m a x x l cos π y l the streamlines and the flow velocity are shown in fig 13 with vmax 3 m s the computational domain is the same as example 2 the initial particle spacing is dx 50 m the time step is dt 2 5 s and the simulation time is 3400 s the particle distributions at different time are shown in fig 14 and it is clear that with the increasing of time the distribution becomes more irregular due to the non uniformity of the flow velocity comparisons between the results of sph and mtvdlf superbee are shown in figs 15 and 16 the isolines of the sph solution are much smoother than those of mtvdlf superbee see fig 15 implying better resolution at 3400 s a slight fluctuation at the most outside isoline appears in sph see fig 15a due to the disordered particle distribution see fig 14 as shown in fig 16 the sph solution has a higher central intensity than that of the mtvdlf superbee the numerical diffusion in mtvdlf superbee resulting from grid anisotropy is the main reason for the large discrepancy of the amplitude 5 conclusions in this paper the meshless sph method is applied to solve the 2d depth averaged advection diffusion equation in lagrangian framework it provides a new idea for accurate simulation of pollutant transport process in offshore water for the convenience of treating second order derivatives including the cross terms induced by the anisotropy of diffusion coefficient the diffusive mass flux is introduced to separate the original one second order equation into two first order ones the sph based pollutant transport model with diffusive flux is tested by four typical examples each of which has the tough point for solving with numerical tools and is in accordance with the phenomenon of pollutant transport in coastal water the high resolution mtvdlf superbee scheme is also implemented for the purpose of providing reference solutions numerical results indicate that the sph based model has the superiority in solving the advection dominated problems for which numerical diffusion or peak clipping phenomena often appears when they are solved by grid based methods while when diffusion dominates the sph based model has comparable performance with the mtvdlf superbee method in terms of efficiency and accuracy even with a discontinuity the present model can still give good results while the mtvdlf superbee scheme leads to numerical diffusion wiggles and profile shifting the stability accuracy and computational efficiency are also analyzed numerically it is shown that the present method has a looser stability condition and higher accuracy than the grid based methods especially in advection dominated cases with respect to efficiency to achieve similar results in the discontinuity problem sph costs far less time than that for mtvdlf superbee the sph method can also circumvent the grid anisotropic problem since it does not need to set the grids it performs well even though the streamlines are oblique and curving the analysis of convergence indicates that the present sph particle model has second order convergence rate for pollutant transport in flows with large deformation compared with the mtvdlf superbee scheme the sph solution has a higher central intensity even though there are slight fluctuations in the concentration isolines due to irregularity of particles this reflects an inherent defect of sph for flows with possible cavity which deserves a further study and the particle redistribution scheme xu et al 2009 might be useful the study of improved sph based methods that can well adapt to irregular distribution of particles is also in the plan furthermore the present model will be tested for transport problems with anisotropic dispersion credit authorship contribution statement wanying liu software validation writing original draft qingzhi hou conceptualization methodology writing review editing jijian lian data curation funding acquisition anmin zhang resources visualization investigation jianwu dang supervision project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was funded by the national key research and development program of china grant no 2018yfc1407403 and the national natural science foundation of china grant no u1765202 52079090 51478305 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103764 appendix supplementary materials image application 1 
382,we upscale reactive mixing using effective dispersion coefficients to capture the combined effect of pore scale heterogeneity and molecular diffusion on the evolution of the mixing interface between two initially segregated dissolved species effective dispersion coefficients are defined in terms of the average spatial variance of the solute distribution evolving from a pointlike injection that is the transport green function we numerically investigate the temporal behavior of the longitudinal effective dispersion coefficients for two porous media of different pore scale heterogeneity as measured by the statistics of the flow speed and different péclet numbers we find that the effective dispersion coefficients evolve with time or equivalently travel distance as the solute samples the pore scale flow heterogeneity due to advection and transverse diffusion the effective dispersion coefficients evolve from the value of molecular diffusion to the corresponding hydrodynamic dispersion coefficients thus at times smaller than the diffusion time over a characteristic pore length the effective dispersion coefficients can be significantly smaller than the hydrodynamic dispersion coefficients this difference can explain frequently observed mismatches between pore scale reactive mixing data and predictions using darcy scale transport descriptions based on hydrodynamic dispersion coefficients that are constant in time this suggests that the notion of incomplete mixing on the support scale can be quantified in terms of effective pore scale dispersion coefficients we use effective dispersion in order to approximate the transport green function in terms of a gaussian shaped distribution that is characterized by the effective variance this is approximation is termed dispersive lamella based on this representation we study reactive mixing between two initially segregated solutes the dispersive lamella approach accurately predicts the evolution of the product mass of an instantaneous bimolecular reaction obtained from direct numerical simulations this demonstrates that effective dispersion is an accurate measure for width of the mixing interface between the two reacting species these results shed some new light on pore scale mixing the notion of incomplete mixing and its prediction and upscaling in terms of an effective mixing model keywords effective dispersion mixing upscaling incomplete mixing lamellar mixing reactive mixing 1 introduction one of the main challenges in understanding and quantifying the dynamics of pore scale mixing and mixing limited chemical reactions stems from the intrinsic spatial variability in the medium and flow properties which together with microscale mass transfer impact on the mixing and transport behaviors of dissolved species mixing is the key process that brings reactants into contact and thus enables chemical reactions classical darcy scale transport models assume complete mixing of reactants at the support scale and quantifiy the impact of pore scale velocity fluctuations on solute mixing in terms of constant hydrodynamic dispersion coefficients steefel et al 2005 dentz et al 2011 berkowitz et al 2016 a series of numerical kapoor et al 1997 meile and tuncay 2006 li et al 2006 alhashmi et al 2015 and experimental works raje and kapoor 2000 gramling et al 2002 willingham et al 2008 de anna et al 2014b jiménez martínez et al 2015 have found that this approach is not able to quantify and in general overestimates pore scale mixing and reaction which can been traced back to pore scale fluctuations and the notion of incomplete mixing at the support scale oates 2007 tartakovsky et al 2009 the applicability and breakdown of classical advection dispersion reaction models for the upscaling of pore scale reactive transport was investigated in detail by battiato et al 2009 battiato and tartakovsky 2011 and porta et al 2012 in terms of the relevant péclet and damköhler numbers using volume averaging techniques the impact of incomplete mixing at the pore scale on darcy scale reactive transport has been modeled with a series of different approaches valocchi et al 2019 benson and meerschaert 2008 edery et al 2009 and ding et al 2013 use lagrangian reactive transport formulations that implement upscaled rules for the motion and interaction of reacting particles in order to emulate the impact of incomplete mixing at the pore scale on reactive transport at the darcy scale oates 2007 bellin et al 2011 and chiogna and bellin 2013 account for incomplete mixing on the support through the distribution of concentration point values which is modeled as a beta distribution the parameters of the beta distribution can be related to local scale transport parameters and heterogeneity characteristics sanchez vila et al 2010 and ginn 2018 employ time dependent effective reaction coefficients in order to account for the reduction of reaction efficiency due to pore scale incomplete mixing lamellar reactive mixing approaches ranz 1979 solve diffusion and reaction on advected and deformed material elements so called lamellae the impact of flow heterogeneity on mixing and reaction is quantified through its stretching and compression action on the lamellae that form the mixing interface separating two reactants these concepts have been used for the quantification and upscaling of darcy scale mixing villermaux 2012 borgne et al 2013 2015 and mixing limited reactive transport borgne et al 2014 bandopadhyay et al 2017 as well as pore scale mixing lester et al 2016 kree and villermaux 2017 and reaction de anna et al 2014a b and reactive mixing in poiseuille flow perez et al 2019b the stretched lamella approach considers the ensemble of lamellae that provide the deformed support of the mixing interface as independent and therefore mass transfer and superposition of lamellae is not accounted for villermaux 2012 the merging of lamellae due to diffusion can be modeled based on the assumption that lamellae aggregate in a random fashion which has been quantified by an extension of the smoluchowski coagulation model villermaux 2012 borgne et al 2015 de anna et al 2014a quantify the impact of coalescence on late time reaction rates using a phenomenological argument that relates the number of merged lamellae to the apparent interface width perez et al 2019b and perez et al 2020 proposed to quantify the effective interface width and thus the mixing area in poiseuille and pore scale flows based on temporally evolving effective dispersion coefficients dentz and carrera 2007 this approach termed the dispersive lamella predicts the full evolution of the global reaction rate for a fast irreversible bimolecular reaction including the early time stretching dominated and late time coalescence dominated behaviors similar approaches based on the evolution of effective dispersion coefficients dentz et al 2000 have been used by cirpka and kitanidis 2000 and cirpka 2002 for the upscaling of darcy scale reactive transport and the modeling and prediction of column scale reactive transport jose and cirpka 2004 in this paper we integrate effective dispersion into the framework of lamellar mixing in order to systematically quantify the impact of heterogeneity induced mixing on pore scale reaction and provide a physics based quantitative explanation of reaction phenomena due to incomplete pore scale mixing to this end we define a dispersive lamella as an effective representation of the partial plume that evolves from a point like solute injection or in other words the transport green function the dispersive lamella is characterized by the effective dispersion which quantifies the average width of the transport green function due to advective heterogeneitiy and molecular diffusion we analyze effective dispersion in terms of the evolution of the effective variance by using random walk particle tracking simulations for conservative transport of solute plumes evolving from a line source perpendicular to the mean flow direction and from point injections we then evaluate the predictions for the reaction produced from this methodology against direct reactive numerical simulations the paper is organized as follows section 2 presents the research methodology it defines the transport problem under consideration and details the direct numerical simulation including medium generation the numerical flow transport and reaction simulations section 3 1 develops the dispersive lamella approach and defines the concept of effective dispersion for conservative pore scale transport section 3 2 analyzes the evolution of effective and apparent dispersion in the light of pore scale flow variability and diffusion section 4 uses the dispersive lamella to evaluate pore scale reactive mixing in comparison to the behavior described by hydrodynamic dispersion and direct numerical results 2 methodology in the following we describe the pore scale flow and transport equations the pore scale reactive mixing scenario and the numerical simulations including medium generation numerical flow transport and reaction simulations 2 1 pore scale flow and transport solute transport in the void space of a porous medium is described by the advection diffusion equation for the passive scalar c x t 1 c x t t v x c x t d 2 c x t 0 where d is the molecular diffusion coefficient the flow velocity v x satisfies the stokes equation leal 2007 2 2 v x 1 μ p x where p x is the fluid pressure and μ dynamic viscosity the stokes equation is solved together with the continuity equation 3 v x 0 that guarantees the incompressibility of the fluid we consider a two dimensional flow and transport domain of length ℓ and width w at the grain surfaces and horizontal domain boundaries no flux conditions are imposed both for flow and transport transport scenarios can be characterized by the péclet number which measures the relative importance of diffusive and advective transport mechanisms over a length scale which here is chosen to be equal to the average pore length ℓ p the péclet number here is defined as p e v ℓ p 2 d where v is the mean flow velocity the péclet number denotes the ratio of the characteristic diffusion time τ d ℓ p 2 2 d and the advection time τ v ℓ p v over the length scale ℓ p 2 2 pore scale reactive mixing we investigate pore scale reactive mixing in terms of the instantaneous irreversible bimolecular reaction a b c the concentrations ca x t cb x t and cc x t of the reactant and product species evolve according to 4a c i x t t v x c i x t d 2 c i x t r x t 4b c c x t t v x c c x t d 2 c c x t r x t where i a b and r x t is the local reaction rate we assume that the diffusion coefficients are the same for the reactant and product species under this condition the reactive transport problem 4 can be formulated in terms of the concentrations cac x t and cbc x t of the conservative species ac and bc the concentrations of the conservative components satisfy the transport eq 4 for r 0 for the instantaneous reaction under consideration here species a and b cannot coexist and therefore c c x t min c a c x t c b c x t the reactivity of the system is measured here by the total product mass 5 m c t ω f d x c c x t where ω f denotes the pore space we consider as initial condition two slabs of uniform distributions of species a and species b of length ℓ s that extend over the domain width w that is 6 c a x t 0 c 0 i ℓ s x 0 c b x t 0 c 0 i 0 x ℓ s where c 0 is the initial concentration and i is the indicator function that is equal to one if its argument is true and 0 otherwise the initial product concentration is c c x t 0 0 clasically the impact of pore scale velocity fluctuations is quantified in terms of constant hydrodynamic dispersion bear 1972 within a fickian transport framework for the scenario under consideration here darcy scale transport is classically represented by the one dimensional advection dispersion equations 7a c i x t t v c i x t x d 2 c i x t x 2 r x t 7b c c x t t v c c x t x d 2 c c x t x 2 r x t where d is the constant longitudinal hydrodynamic dispersion coefficient and c i x t denotes the bulk averaged species concentrations quintard and whitaker 1994b c in the case ℓ s the evolution of the total product mass is predicted by hydrodynamic dispersion as gramling et al 2002 8 m c t c 0 w ϕ 4 d t π the solution for the initial conditions 6 is given in appendix b as outlined in the introduction it has been shown in a series of laboratory experiments that the classical darcy scale transport framework is not able to capture observed reaction behaviors and generally overestimates the actual product mass raje and kapoor 2000 gramling et al 2002 de anna et al 2014b in section 4 we elaborate on the predictions for the product mass using the dispersive lamella methodology and compare them to direct reactive numerical simulations 2 3 numerical simulations in the following we report on the numerical medium generation and medium properties numerical flow solution and transport simulations using random walk particle tracking 2 3 1 medium generation we simulate transport in two different 2 dimensional synthetic heterogeneous porous media the synthetic porous media consist of random packings of equally sized circular grains the representation of granular obstacles in porous media using circular grains has the advantage that the impact of fundamental geometrical features on reaction and transport can be studied in a systematic way we generate the porous media by sampling the positions of circular grains from a uniform distribution the grain diameters are constant and given by d 9 3 10 5 m the placement of a new grain is performed such that the distance between the new grain and any previously placed grain is larger than d ϵ m with ϵ m 0 a fixed tolerance the pore size distribution and thus the degree of heterogeneity of the medium is controlled by the tolerance ϵ m the algorithm stops when the target porosity of ϕ 0 5 is achieved we consider two media of dimensions ℓ w 7 5 10 3 m 2 10 3 m and ℓ w 7 2 10 3 m 2 10 3 m the two media differ in the distributions of pore sizes which leads to different flow heterogeneity as measured by the variance σ ν 2 of the logarithm ν ln v x of the flow speed the relation between the pore size distribution and flow speed distribution has been studied by de anna et al 2017 and dentz et al 2018 the distributions of pore sizes are estimated using the algorithm of rabbani et al 2014 the first medium med1 is generated by fixing a tolerance of ϵ m 2 5 10 6 m which gives a relatively narrow pore size distribution as show in the inset of fig 2 the average pore length is ℓ p 3 10 5 m which means the medium has the dimensions ℓ w 251 ℓ p 67 ℓ p the resulting geometry is displayed together with the flow field in the top panel of fig 1 the distributions of flow speeds and throat widths is shown in fig 2 the variance of the log speed is σ ν 2 1 19 for the second medium med2 we allowed for a tolerance of ϵ m 3 10 7 m which induces a broader pore size distribution illustrated in the inset of fig 2 the average pore length is ℓ p 2 6 10 5 m which means the medium has the dimensions ℓ w 279 ℓ p 77 ℓ p it differs from med1 by a broader distribution of pore throats this results in a more heterogeneous flow field illustrated in the bottom panel of fig 1 the variance of the log speed is σ ν 2 2 1 that is almost double the variance of med1 2 3 2 flow in the following we summarize the methodology to solve the flow field the binary images of the geometry are composed of regular pixels that represent either void or solid we employ a regular hexaedron mesh compatible with openfoam the mesh cells have a size of 1 25 10 6 m in all directions δ x δ y for both media this discretization level is selected such that the radius of a grain is divided in 35 cells which corresponds to an accurate representation of the solid beads see also gjetvaj et al 2015 the resulting discretization for the regular grid consists of 6022 1600 cells corresponding to x and y dimensions respectively for med1 and of 5800 1600 cells for med2 we prescribe pressure boundary conditions at the inlet and outlet and no slip conditions at the void solid interfaces and at the remaining domain boundaries at the left boundary a pressure of 10 pa at the right boundary zero pressure are applied we then solve the flow with the simple algorithm weller et al 1998 implemented in openfoam note that in order to minimize boundary effects twenty layers are added at the inlet and outlet of both the geometries after convergence that is once the residual of the pressure and flow fields between two consecutive steps are smaller than a criterion ϵ f 10 8 we extract the complete velocity field velocity values are given at every interface of the mesh in the normal direction to the face figs 1 and 2 respectively display the flow fields and the distributions of the flow speeds in the two porous media the mean flow speeds are for med1 v 5 33 10 4 m s and for med2 v 6 15 10 4 m s 2 3 3 random walk particle tracking the transport problem can be formulated in a lagrangian modeling framework based on the equivalence of 1 with the langevin equations risken 1996 perez et al 2019a 9 d x t d t v x t 2 d ξ t the langevin equations are discretized according to 10 x t δ t x t v x t δ t 2 d δ t ζ t the time increment δt is variable and varies between 10 6 s at early and 10 3 s at late times the advective displacement during δt given by the second term on the right side of 10 is determined based on an extension of the pollock algorithm mostaghimi et al 2012 pollock 1988 puyguiraud et al 2019 that accounts for the no slip boundary condition at solid grains the noise ζ t is modeled here as a uniform random variable with values in 3 3 so that ζ t 0 and ζ i t ζ j t δ i j this choice avoids the costly numerical generation of gaussian random numbers the central limit theorem guarantees that the sum of random displacements is gaussian in order to study the dispersion of a conservative solute we consider a line source perpendicular to the mean flow direction for the investigation of reactive mixing we consider two adjacent slabs representing the initial distributions of the two conservative components the simulation parameters are summarized in table 1 conservative transport the transport scenario is characterized by a line source perpendicular to the mean flow direction at x 1 0 25 10 3 m located after the first grains and spanning the full domain cross section the line is composed of 800 points at each point of the line 2 104 particles are released to simulate the impact of molecular diffusion the considered scenario is characterized by a p e 20 and p e 100 the specific parameters are detailed in table 1 reactive transport the reactive transport scenario is characterized by a uniform initial placement of ac and bc particles in two adjacent slabs of width ℓ s ℓ 6 which is equivalent to a species a adjacent to species b we inject the same number of ac and bc particles which are propagated conservatively according to 10 we ran the simulations for the two media and for the two péclet numbers considered in the line injection case we found that incomplete mixing due to finite number of particles is negligible for n 0 108 particles we use 108 ac and 108 bc particles which means in total n 0 2 10 8 particles in order to determine the number of c particles generated at any time t n n δ t we proceed as follows we discretize the domain into square bins of side length 1 25 10 6 m thus the number δnc of c particles generated at time tn within a pixel is given by the minimum of the number δnac of ac and δnbc of bc particles δ n c min δ n a c δ n b c the total number nc of c particles in the domain at time tn is computed by summing over all the cells of the domain the mass of c is computed from the number nc of c particles as 11 m c n c n 0 where n 0 is the total number of particles initially placed in the domain with this definition the initial concentration of ac and bc in each of the slabs is given by c 0 1 2 ℓ s w see also perez et al 2019a 3 effective dispersion and dispersive lamellae in this section we define the concept of effective dispersion and the dispersive lamella approach to upscale pore scale reactive mixing to the darcy scale solute mixing and dispersion is analyzed by considering the distribution c x t for the normalized line source ϕ c x t 0 w 1 δ x ρ y where ρ y 1 if y is in the pore space and 0 else such that 12 1 w 0 w d y ρ y ϕ where w is the width of the medium the evolution of the concentration distribution from the line source is illustrated in fig 3 the line is composed of the partial plumes g x t y which satisfy 1 for the point like initial condition g x t 0 y δ x δ y y ρ y the concentration can then be expressed as 13 c x t 1 w ϕ 0 w d y ρ y g x t y note that g x t y is a green function of the transport problem fig 4 illustrates snapshots of the spatial distribution of two green functions originating from two different locations at different times 3 1 dispersive lamella the upscaling excercise is related to finding an upscaled effective expression ge x t y for the transport green function that captures the salient features of pore scale mixing and that is able to quantify the evolution of the mixing volume associated to the evolution of the complex concentration distribution illustrated in fig 3 to this end we define the center of mass velocity ν t y of the partial plumes and the effective dispersion coefficient in the mean flow direction 14 ν t y d m 1 t y d t d e t 1 2 d σ e 2 t d t where the longitudinal moments of g x t y 15 m i t y ω f d x x i g x t y σ 2 t y m 2 t y m 1 t y 2 for i 1 2 where ω f denotes the pore space the m 1 t y denote the center of mass positions of the partial plumes originating in y the global center of mass position and the effective variance are defined by 16 m 1 t 1 w ϕ 0 w d y ρ y m 1 t y σ e 2 t 1 w ϕ 0 w d y ρ y m 2 t y m 1 t y 2 the square root of the effective variance σ e 2 t is a measure for the average width of the partial plumes that form the line source and thus for the width of the mixing area effective dispersion is a key concept for the quantification of the mixing interface the dispersive lamella approach models the green function as g e x t y g ℓ x m 1 t y t g y m 2 t y t y where g ℓ x t satisfies the one dimensional dispersion equation 17 g ℓ x t t d e t 2 g ℓ x t x 2 0 for the initial condition g ℓ x t δ x see appendix a the subscript ℓ stands for lamella the centered transverse green function g y t y describes solute dispersion in transverse direction see appendix a the effective green function ge x t y is an upscaled object in that its values are defined at all positions in space it is defined with respect to the bulk volume thus the total mass in the domain is given by ϕ times the space integral of ge x t y the centered green function g ℓ x t denotes the dispersive lamella note that the advective dispersion of the mixing front is quantified by the fluctuations of the center of mass positions m 1 t y between the partial plumes forming the mixing interface this spreading effect is separated from mixing through centering of the green function the upscaled concentration distribution c x t evolving from an initial line source is thus constructed from ge x t y as 18 c x t 1 w 0 w d y g ℓ x m 1 t y t g y m 2 t y t y the solution of eq 17 gives for g ℓ x t the gaussian distribution 19 g e x t exp x 2 2 σ e 2 t 2 π σ e 2 t the dispersive lamella is a measure for the concentration in the coordinate system attached to the center of mass position m t y which delineates the mixing interface the concentration distribution homogenizes with time as the variability of the center of mass positions decreases and dispersion increases as illustrated in fig 4 for two partial plumes that evolve from different initial positions by comparison with fig 3 we can identify the contributions of these partial plumes to the overall plume with increasing time the two partial plumes converge to similar behavior due to transverse heterogeneity sampling both the center of mass positions as well as the plume shape converge as discussed in appendix a the vertically integrated profiles can be well approximated by a gaussian shaped distribution similar approaches were used for the quantification of mixing and reactive mixing in darcy scale heterogeneous porous media cirpka and kitanidis 2000 jose and cirpka 2004 3 2 effective dispersion coefficients we discuss here the evolution of effective pore scale dispersion which is the centerpiece of the upscaled reactive mixing approach presented in the previous section effective pore scale dispersion is measured by the effective variance σ e 2 t defined in 16 in addition we consider the apparent variance which is defined by 20 σ a 2 t 1 w ϕ 0 w d y ρ y m 2 t y 1 w ϕ 0 w d y ρ y m 1 t y 2 it can be readily shown by using expression 13 that σ a 2 t is the longitudinal second central moment of c x t thus it measures the overall extension of the disperse concentration distribution evolving from the initial line source the asymptotic hydrodynamic dispersion coefficient is defined by fitting the linear function 2 d t a to σ a 2 t for t τd the difference between the apparent and effective variances is given by 21 σ a 2 t σ e 2 t 1 w ϕ 0 w d y ρ y m 1 t y 2 1 w ϕ 0 w d y ρ y m 1 t y 2 this means that it quantifies the fluctuations of the center of mass positions of the green functions of which the concentration distribution is composed fig 5 illustrates the variability in the evolution of the center of mass positions in med1 and med2 the center of mass positions for the partial plumes are conditioned by the velocities at the starting point and converge asymptotically toward the mean flow velocity the fluctuations are larger in the more heterogeneous med2 fig 6 shows the evolution of the effective and apparent variances for med1 and med2 at short times t τv both the apparent and effective variances behave as 2dt because the solute has not yet noted the advective variability along the line as illustrated in fig 3 for t τv the initially straight line starts being distorted due to the advective heterogeneity experienced along the line this leads to a strong increase of the apparent variance which as mentioned above is a measure for the spread of the line due to differences in the center of mass positions of the partial plumes that form the line the effective variance evolves slower and the two quantities start diverging for t τv the difference between the apparent and effective variances is larger for the more heterogeneous med2 which reflects the stronger fluctuations of the center of mass positions of the partial plumes as shown in fig 5 when transverse diffusion activates advective heterogeneity as a mixing mechanism the effective variance increases this increase in the actual width of the interface for t τv is also illustrated in the evolution of the line source in fig 3 in fact as shown in fig 4 transverse mass transfer makes each partial plume sample the vertical velocity contrast so that i the center of mass velocities evolve toward the average flow velocity and ii its longitudinal variance increases due to the impact of velocity fluctuations on scales smaller than the plume scale dentz and de barros 2015 advective differences between the partial plumes are smoothed out due to transverse diffusion and transfered to effective dispersion as shown in figs 5 and 6 at asymptotic times for t τd the apparent and effective variances converge to the same asymptotic behavior that is characterized by the hydrodynamic dispersion coefficient d which shows that the center of mass fluctuations between partial plumes eventually vanish 4 pore scale mixing and reaction in this section we elaborate on the implications of the dispersive lamella methodology onto the reaction rate predictions the fickian framework generally overestimates the amount of reaction occuring at the pore scale raje and kapoor 2000 gramling et al 2002 the dispersive lamella approach suggests that such behaviors can be understood by preasymptotic mixing as described by effective dispersion and quantified by the effective variance σ e 2 t which measures the effective interface width in order to quantify the reaction product for the instantaneous bimolecular reaction discussed in sections 2 2 and 2 3 3 we consider the concentration profiles across the interface separating the ac and bc species in the coordinate system attached to the center of mass positions m t y which delineates the mixing interface the average concentration c a c x t of the conservative component ac across the mixing interface is given by 22 c a c x t 0 d x 0 w d y g ℓ x x t g y t y 1 2 erfc x 2 σ e 2 t where we used expression 19 for g ℓ x t and the fact that the integral of g y t y over y is equal to one the concentration of bc is given by c b c x t 1 c a c x t note that this solution assumes that the initial distributions of the ac and bc species extend from minus infinity to zero and from 0 to infinity respectively the solution for the slab initial distribution is given in appendix b for the scenario under consideration there is virtually no difference between the two solutions the product concentration c c x t is obtained as outlined in section 2 from c c x t min c a c x t c b c x t as 23 c c x t 1 2 erfc x 2 σ e 2 t the product mass is obtained from 23 by integration of space and multiplication by ϕ as 24 m c t c 0 w ϕ 2 σ e 2 t π fig 7 shows the evolution of the product mass described by expression 24 using the effective variance together with the direct numerical simulation results for med1 and med2 for p e 20 and p e 100 the mass predictions from effective dispersion match accurately the reaction rates at all times for the two media and the two péclet number regimes deviations between the mass obtained from the direction numerical simulations and the dispersive lamella in term of effective dispersion can be explained by the finite size of the model medium for comparison we also display the behavior obtained by using the diffusion and hydrodynamic dispersion coefficients at early times for t τv diffusion is the main mixing mechanism driving the reaction and the product mass is expected to increases as m c t w ϕ 4 d t π this behavior is represented by the effective variance which at short times evolves as 2dt as discussed in the previous section with increasing time t τv both the length and spread of the mixing interface increase due to advective variability along the interface with increasing time advective heterogeneity is activated as a mixing mechanism by transverse diffusion which leads to a strong increase in the simulated product mass as correctly described by effective dispersion at times t τd the prediction in terms of the effective variances converges to fickian like behavior hydrodynamic dispersion predicts consistently higher values than effective dispersion thus the fact that the variance of the mixing interface evolves non linearly according to effective dispersion in contrast to the behavior predicted by constant hydrodynamic dispersion can explain features of incomplete mixing in fact the behaviors shown in fig 7 are qualitatively very similar to the ones observed in the experiments by gramling et al 2002 thus the evolution of the product mass from pore scale reactive mixing can be explained and quantified in terms of effective dispersion coefficients which capture the width of the mixing interface between two initially segregated reacting species 5 conclusions we study the upscaling and quantification of mixing in two dimensional synthetic porous media in terms of effective dispersion coefficients direct numerical simulations give insight into conservative mixing and spreading behavior of a solute distribution originating from a uniform line source at short times smaller than the characteristic advection time over a pore length mixing is dominated by diffusion for increasing time advective heterogeneity along the interface is activated as a mixing mechanism by transverse diffusion which increases the interface width eventually at asymptotically long times the mixing eventually becomes fickian and may be described by hydrodynamic dispersion this evolution is at the root of the notion of incomplete mixing these mechanisms are captured by a dispersive lamella approach that approximates the transport green function by a gaussian distribution characterized by the effective longitudinal variance and a center of mass position that reflects the advective variability the effective longitudinal variance characterizes the average width of the transport green function diffusive sampling of the transverse velocity contrast smoothes the variability of the center of mass position which converges toward the mean flow velocity and leads to an increase of longitudinal mixing thus the variability of the center of mass position between the dispersive lamellae accounts for the advective spreading of the interface while effective dispersion quantifies the actual mixing in this work we have studied and quantified effective dispersion from detailed 2d numerical simulations and used its evolution in order to describe the mass production in a fast bimolecular irreversible reaction for which the diffusion constant is assumed to be the same for both species we have seen the reaction mass predicted from the dispersive lamella methodology accurately match results from direct numerical reactive simulations for different levels of heterogeneity and different péclet regimes note that while we introduced the methology in two dimensions we anticipate that it is also valid in three dimensions one would however expect different temporal evolution of the apparent and effective dispersion coefficients due to more efficient transverse sampling of the pore scale heterogeneity while the quantification of constant hydrodynamic dispersion coefficients has been studied extensively in the literature we are not aware of similar works on the concept of effective dispersion unlike for dispersion in poiseuille flow and darcy scale heterogeneous porous media similar approaches such as volume averaging brenner and edwards 1993 quintard and whitaker 1994a and stochastic averaging dagan 1989 can be used to relate the evolution of effective dispersion to the medium structure and pore scale hydrodynamics furthermore continuous time random walks for pore scale transport puyguiraud et al 2019 2020 provide an efficient framework to quantify advective spreading represented by the center of mass fluctuations between the dispersive lamellae that form the interface the proposed dispersive lamella approach provides a systematic framework to integrate these modeling approaches and upscaling techniques for the physics based prediction of pore scale mixing and reaction credit authorship contribution statement alexandre puyguiraud data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation visualization writing original draft writing review editing lazaro j perez data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation visualization writing original draft writing review editing juan j hidalgo data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation visualization writing original draft writing review editing marco dentz data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation visualization writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been supported by the european research council under the european union s seventh framework programme fp7 2007 2013 erc grant agreement no 617511 mhetscale and the spanish ministry of science and innovation through the project hydropore pid2019 106887gb c31 juan j hidalgo acknowledges the support of the spanish ministry of science and innovation through the ramon y cajal program appendix a dispersive lamella in order to derive the governing equation for the green function in the dispersive lamella approach we consider the langevin equation equivalent to the advection diffusion eq 1 for pore scale transport a 1 d x t y d t u t y 2 d ξ t where we defined the lagrangian velocity a 2 u t y v x t y the initial particle position is x t 0 y 0 y ω f the symbol ξ t denotes a gaussian white noise characterized by unit mean and correlation ξ i t ξ j t δ i j δ t t the angular brackets denote the average over all noise realizations we separate the lagrangian velocity into its noise mean and fluctuation u t y around it a 3 u t y u t y u t y the fluctuation mean is zero by definition and its covariance function is given by a 4 c i j t t y v i t y v j t y it is in general not stationary but may become stationary in the long time limit we model the fluctuation u t y as a gaussian distributed random vector ζ t y characterized by zero mean and the covariance matrix a 4 thus the langevin eq a 1 can be approximated as a 5 d x t y d t u t y ζ t y 2 d ξ t note that this approximation models particle motion in the bulk and is not restricted to the pore space anymore in this sense it represents the upscaling and homogenization step we define the effective green function ge x t y in terms of the effective particle trajectories x t y as a 6 g e x t δ x x t y where the angular brackets here stand for the average over all noise realizations note that the dirac delta refers to the bulk volume thus ge x t is the bulk concentration and its integral is equal to ϕ the langevin eq a 5 is equivalent to the advection dispersion equation a 7 g e x t y t u t y g e x t y d e t y g e x t y 0 where the dispersion tensor d e t y is defined by its components as a 8 d i j e t y d δ i j 0 t d t c i j t t y this approximation implies that the green function ge x t y for an infinite medium is given by a gaussian distribution characterized by the centered mean and variance a 9 m t y 0 t d t u t y κ t y 2 d t 1 2 0 t d t d e t y where 1 denotes the identity matrix the quality of the gaussian approximation is illustrated in fig a 8 which shows the vertically integrated green function obtained from the direct numerical simulations note that we do not directly evaluate the accuracy of the gaussian approximation for the green function the gaussian approximation is the basis for the predictions of reactive mixing in section 4 which serves as a pragmatic a posteriori evaluation of the validity and accuracy of the gaussian approximation we replace the local effective dispersion tensor dij t y by its vertical average a 10 d i j e t 1 w ϕ 0 w d y ρ y d i j e t y which quantifies the average width of the mixing interface based on the random nature of the media under consideration the off diagonal coefficients of the effective dispersion tensor are considered small and set to zero d i j e t 0 for i j under this condition the green function ge x t y can be decomposed into a 11 g e x t y g ℓ x m 1 t y t g y m 2 t y t y the g ℓ x t y satisfies the diffusion equation a 12 g ℓ x t t d e t g e x t y 0 where we set d e t d 11 e t the transverse green function g y t y captures the solute extension in vertical direction it satisfies a 13 g y t y t d 22 e t 2 g y t y y 2 0 furthermore g y t y satisfies a 14 0 w d y g y t y 0 w d y g y t y 1 the explicit analytical expression for g ℓ x t y for a medium with infinite longitudinal extension is a 15 g ℓ x t y exp x 2 2 σ e 2 t 2 π σ e 2 t this is a good approximation if the plume is far from the vertical boundaries fig a 8 shows the vertically integrated concentration distribution obtained from the direct numerical simulation and the gaussian model a 15 while the concentration distribution is subject to fluctuations that reflect the alternation between void and solid it is compact and it can be well characterized in terms of the center of mass position and longitudinal variance appendix b solution for slab injection perez et al 2019b provide the analytical solutions for an initial distribution of two adjacent slabs of a and b which we summarize here for convenience for a constant hydrodynamic dispersion coefficient d the product mass is given by b 1 m c t w ϕ c 0 4 d t π 1 exp τ d 4 t π τ d 4 t erfc τ d 4 t where we define τ d ℓ s 2 d the diffusion time over the initial slab extension in the case of a temporally variable effective longitudinal dispersion coefficient the total mass evolves as b 2 m c t w ϕ c 0 2 π σ e t 1 exp a e t 2 π a e t erfc a e t where we define a e t ℓ s 2 σ e t 2 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103782 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
382,we upscale reactive mixing using effective dispersion coefficients to capture the combined effect of pore scale heterogeneity and molecular diffusion on the evolution of the mixing interface between two initially segregated dissolved species effective dispersion coefficients are defined in terms of the average spatial variance of the solute distribution evolving from a pointlike injection that is the transport green function we numerically investigate the temporal behavior of the longitudinal effective dispersion coefficients for two porous media of different pore scale heterogeneity as measured by the statistics of the flow speed and different péclet numbers we find that the effective dispersion coefficients evolve with time or equivalently travel distance as the solute samples the pore scale flow heterogeneity due to advection and transverse diffusion the effective dispersion coefficients evolve from the value of molecular diffusion to the corresponding hydrodynamic dispersion coefficients thus at times smaller than the diffusion time over a characteristic pore length the effective dispersion coefficients can be significantly smaller than the hydrodynamic dispersion coefficients this difference can explain frequently observed mismatches between pore scale reactive mixing data and predictions using darcy scale transport descriptions based on hydrodynamic dispersion coefficients that are constant in time this suggests that the notion of incomplete mixing on the support scale can be quantified in terms of effective pore scale dispersion coefficients we use effective dispersion in order to approximate the transport green function in terms of a gaussian shaped distribution that is characterized by the effective variance this is approximation is termed dispersive lamella based on this representation we study reactive mixing between two initially segregated solutes the dispersive lamella approach accurately predicts the evolution of the product mass of an instantaneous bimolecular reaction obtained from direct numerical simulations this demonstrates that effective dispersion is an accurate measure for width of the mixing interface between the two reacting species these results shed some new light on pore scale mixing the notion of incomplete mixing and its prediction and upscaling in terms of an effective mixing model keywords effective dispersion mixing upscaling incomplete mixing lamellar mixing reactive mixing 1 introduction one of the main challenges in understanding and quantifying the dynamics of pore scale mixing and mixing limited chemical reactions stems from the intrinsic spatial variability in the medium and flow properties which together with microscale mass transfer impact on the mixing and transport behaviors of dissolved species mixing is the key process that brings reactants into contact and thus enables chemical reactions classical darcy scale transport models assume complete mixing of reactants at the support scale and quantifiy the impact of pore scale velocity fluctuations on solute mixing in terms of constant hydrodynamic dispersion coefficients steefel et al 2005 dentz et al 2011 berkowitz et al 2016 a series of numerical kapoor et al 1997 meile and tuncay 2006 li et al 2006 alhashmi et al 2015 and experimental works raje and kapoor 2000 gramling et al 2002 willingham et al 2008 de anna et al 2014b jiménez martínez et al 2015 have found that this approach is not able to quantify and in general overestimates pore scale mixing and reaction which can been traced back to pore scale fluctuations and the notion of incomplete mixing at the support scale oates 2007 tartakovsky et al 2009 the applicability and breakdown of classical advection dispersion reaction models for the upscaling of pore scale reactive transport was investigated in detail by battiato et al 2009 battiato and tartakovsky 2011 and porta et al 2012 in terms of the relevant péclet and damköhler numbers using volume averaging techniques the impact of incomplete mixing at the pore scale on darcy scale reactive transport has been modeled with a series of different approaches valocchi et al 2019 benson and meerschaert 2008 edery et al 2009 and ding et al 2013 use lagrangian reactive transport formulations that implement upscaled rules for the motion and interaction of reacting particles in order to emulate the impact of incomplete mixing at the pore scale on reactive transport at the darcy scale oates 2007 bellin et al 2011 and chiogna and bellin 2013 account for incomplete mixing on the support through the distribution of concentration point values which is modeled as a beta distribution the parameters of the beta distribution can be related to local scale transport parameters and heterogeneity characteristics sanchez vila et al 2010 and ginn 2018 employ time dependent effective reaction coefficients in order to account for the reduction of reaction efficiency due to pore scale incomplete mixing lamellar reactive mixing approaches ranz 1979 solve diffusion and reaction on advected and deformed material elements so called lamellae the impact of flow heterogeneity on mixing and reaction is quantified through its stretching and compression action on the lamellae that form the mixing interface separating two reactants these concepts have been used for the quantification and upscaling of darcy scale mixing villermaux 2012 borgne et al 2013 2015 and mixing limited reactive transport borgne et al 2014 bandopadhyay et al 2017 as well as pore scale mixing lester et al 2016 kree and villermaux 2017 and reaction de anna et al 2014a b and reactive mixing in poiseuille flow perez et al 2019b the stretched lamella approach considers the ensemble of lamellae that provide the deformed support of the mixing interface as independent and therefore mass transfer and superposition of lamellae is not accounted for villermaux 2012 the merging of lamellae due to diffusion can be modeled based on the assumption that lamellae aggregate in a random fashion which has been quantified by an extension of the smoluchowski coagulation model villermaux 2012 borgne et al 2015 de anna et al 2014a quantify the impact of coalescence on late time reaction rates using a phenomenological argument that relates the number of merged lamellae to the apparent interface width perez et al 2019b and perez et al 2020 proposed to quantify the effective interface width and thus the mixing area in poiseuille and pore scale flows based on temporally evolving effective dispersion coefficients dentz and carrera 2007 this approach termed the dispersive lamella predicts the full evolution of the global reaction rate for a fast irreversible bimolecular reaction including the early time stretching dominated and late time coalescence dominated behaviors similar approaches based on the evolution of effective dispersion coefficients dentz et al 2000 have been used by cirpka and kitanidis 2000 and cirpka 2002 for the upscaling of darcy scale reactive transport and the modeling and prediction of column scale reactive transport jose and cirpka 2004 in this paper we integrate effective dispersion into the framework of lamellar mixing in order to systematically quantify the impact of heterogeneity induced mixing on pore scale reaction and provide a physics based quantitative explanation of reaction phenomena due to incomplete pore scale mixing to this end we define a dispersive lamella as an effective representation of the partial plume that evolves from a point like solute injection or in other words the transport green function the dispersive lamella is characterized by the effective dispersion which quantifies the average width of the transport green function due to advective heterogeneitiy and molecular diffusion we analyze effective dispersion in terms of the evolution of the effective variance by using random walk particle tracking simulations for conservative transport of solute plumes evolving from a line source perpendicular to the mean flow direction and from point injections we then evaluate the predictions for the reaction produced from this methodology against direct reactive numerical simulations the paper is organized as follows section 2 presents the research methodology it defines the transport problem under consideration and details the direct numerical simulation including medium generation the numerical flow transport and reaction simulations section 3 1 develops the dispersive lamella approach and defines the concept of effective dispersion for conservative pore scale transport section 3 2 analyzes the evolution of effective and apparent dispersion in the light of pore scale flow variability and diffusion section 4 uses the dispersive lamella to evaluate pore scale reactive mixing in comparison to the behavior described by hydrodynamic dispersion and direct numerical results 2 methodology in the following we describe the pore scale flow and transport equations the pore scale reactive mixing scenario and the numerical simulations including medium generation numerical flow transport and reaction simulations 2 1 pore scale flow and transport solute transport in the void space of a porous medium is described by the advection diffusion equation for the passive scalar c x t 1 c x t t v x c x t d 2 c x t 0 where d is the molecular diffusion coefficient the flow velocity v x satisfies the stokes equation leal 2007 2 2 v x 1 μ p x where p x is the fluid pressure and μ dynamic viscosity the stokes equation is solved together with the continuity equation 3 v x 0 that guarantees the incompressibility of the fluid we consider a two dimensional flow and transport domain of length ℓ and width w at the grain surfaces and horizontal domain boundaries no flux conditions are imposed both for flow and transport transport scenarios can be characterized by the péclet number which measures the relative importance of diffusive and advective transport mechanisms over a length scale which here is chosen to be equal to the average pore length ℓ p the péclet number here is defined as p e v ℓ p 2 d where v is the mean flow velocity the péclet number denotes the ratio of the characteristic diffusion time τ d ℓ p 2 2 d and the advection time τ v ℓ p v over the length scale ℓ p 2 2 pore scale reactive mixing we investigate pore scale reactive mixing in terms of the instantaneous irreversible bimolecular reaction a b c the concentrations ca x t cb x t and cc x t of the reactant and product species evolve according to 4a c i x t t v x c i x t d 2 c i x t r x t 4b c c x t t v x c c x t d 2 c c x t r x t where i a b and r x t is the local reaction rate we assume that the diffusion coefficients are the same for the reactant and product species under this condition the reactive transport problem 4 can be formulated in terms of the concentrations cac x t and cbc x t of the conservative species ac and bc the concentrations of the conservative components satisfy the transport eq 4 for r 0 for the instantaneous reaction under consideration here species a and b cannot coexist and therefore c c x t min c a c x t c b c x t the reactivity of the system is measured here by the total product mass 5 m c t ω f d x c c x t where ω f denotes the pore space we consider as initial condition two slabs of uniform distributions of species a and species b of length ℓ s that extend over the domain width w that is 6 c a x t 0 c 0 i ℓ s x 0 c b x t 0 c 0 i 0 x ℓ s where c 0 is the initial concentration and i is the indicator function that is equal to one if its argument is true and 0 otherwise the initial product concentration is c c x t 0 0 clasically the impact of pore scale velocity fluctuations is quantified in terms of constant hydrodynamic dispersion bear 1972 within a fickian transport framework for the scenario under consideration here darcy scale transport is classically represented by the one dimensional advection dispersion equations 7a c i x t t v c i x t x d 2 c i x t x 2 r x t 7b c c x t t v c c x t x d 2 c c x t x 2 r x t where d is the constant longitudinal hydrodynamic dispersion coefficient and c i x t denotes the bulk averaged species concentrations quintard and whitaker 1994b c in the case ℓ s the evolution of the total product mass is predicted by hydrodynamic dispersion as gramling et al 2002 8 m c t c 0 w ϕ 4 d t π the solution for the initial conditions 6 is given in appendix b as outlined in the introduction it has been shown in a series of laboratory experiments that the classical darcy scale transport framework is not able to capture observed reaction behaviors and generally overestimates the actual product mass raje and kapoor 2000 gramling et al 2002 de anna et al 2014b in section 4 we elaborate on the predictions for the product mass using the dispersive lamella methodology and compare them to direct reactive numerical simulations 2 3 numerical simulations in the following we report on the numerical medium generation and medium properties numerical flow solution and transport simulations using random walk particle tracking 2 3 1 medium generation we simulate transport in two different 2 dimensional synthetic heterogeneous porous media the synthetic porous media consist of random packings of equally sized circular grains the representation of granular obstacles in porous media using circular grains has the advantage that the impact of fundamental geometrical features on reaction and transport can be studied in a systematic way we generate the porous media by sampling the positions of circular grains from a uniform distribution the grain diameters are constant and given by d 9 3 10 5 m the placement of a new grain is performed such that the distance between the new grain and any previously placed grain is larger than d ϵ m with ϵ m 0 a fixed tolerance the pore size distribution and thus the degree of heterogeneity of the medium is controlled by the tolerance ϵ m the algorithm stops when the target porosity of ϕ 0 5 is achieved we consider two media of dimensions ℓ w 7 5 10 3 m 2 10 3 m and ℓ w 7 2 10 3 m 2 10 3 m the two media differ in the distributions of pore sizes which leads to different flow heterogeneity as measured by the variance σ ν 2 of the logarithm ν ln v x of the flow speed the relation between the pore size distribution and flow speed distribution has been studied by de anna et al 2017 and dentz et al 2018 the distributions of pore sizes are estimated using the algorithm of rabbani et al 2014 the first medium med1 is generated by fixing a tolerance of ϵ m 2 5 10 6 m which gives a relatively narrow pore size distribution as show in the inset of fig 2 the average pore length is ℓ p 3 10 5 m which means the medium has the dimensions ℓ w 251 ℓ p 67 ℓ p the resulting geometry is displayed together with the flow field in the top panel of fig 1 the distributions of flow speeds and throat widths is shown in fig 2 the variance of the log speed is σ ν 2 1 19 for the second medium med2 we allowed for a tolerance of ϵ m 3 10 7 m which induces a broader pore size distribution illustrated in the inset of fig 2 the average pore length is ℓ p 2 6 10 5 m which means the medium has the dimensions ℓ w 279 ℓ p 77 ℓ p it differs from med1 by a broader distribution of pore throats this results in a more heterogeneous flow field illustrated in the bottom panel of fig 1 the variance of the log speed is σ ν 2 2 1 that is almost double the variance of med1 2 3 2 flow in the following we summarize the methodology to solve the flow field the binary images of the geometry are composed of regular pixels that represent either void or solid we employ a regular hexaedron mesh compatible with openfoam the mesh cells have a size of 1 25 10 6 m in all directions δ x δ y for both media this discretization level is selected such that the radius of a grain is divided in 35 cells which corresponds to an accurate representation of the solid beads see also gjetvaj et al 2015 the resulting discretization for the regular grid consists of 6022 1600 cells corresponding to x and y dimensions respectively for med1 and of 5800 1600 cells for med2 we prescribe pressure boundary conditions at the inlet and outlet and no slip conditions at the void solid interfaces and at the remaining domain boundaries at the left boundary a pressure of 10 pa at the right boundary zero pressure are applied we then solve the flow with the simple algorithm weller et al 1998 implemented in openfoam note that in order to minimize boundary effects twenty layers are added at the inlet and outlet of both the geometries after convergence that is once the residual of the pressure and flow fields between two consecutive steps are smaller than a criterion ϵ f 10 8 we extract the complete velocity field velocity values are given at every interface of the mesh in the normal direction to the face figs 1 and 2 respectively display the flow fields and the distributions of the flow speeds in the two porous media the mean flow speeds are for med1 v 5 33 10 4 m s and for med2 v 6 15 10 4 m s 2 3 3 random walk particle tracking the transport problem can be formulated in a lagrangian modeling framework based on the equivalence of 1 with the langevin equations risken 1996 perez et al 2019a 9 d x t d t v x t 2 d ξ t the langevin equations are discretized according to 10 x t δ t x t v x t δ t 2 d δ t ζ t the time increment δt is variable and varies between 10 6 s at early and 10 3 s at late times the advective displacement during δt given by the second term on the right side of 10 is determined based on an extension of the pollock algorithm mostaghimi et al 2012 pollock 1988 puyguiraud et al 2019 that accounts for the no slip boundary condition at solid grains the noise ζ t is modeled here as a uniform random variable with values in 3 3 so that ζ t 0 and ζ i t ζ j t δ i j this choice avoids the costly numerical generation of gaussian random numbers the central limit theorem guarantees that the sum of random displacements is gaussian in order to study the dispersion of a conservative solute we consider a line source perpendicular to the mean flow direction for the investigation of reactive mixing we consider two adjacent slabs representing the initial distributions of the two conservative components the simulation parameters are summarized in table 1 conservative transport the transport scenario is characterized by a line source perpendicular to the mean flow direction at x 1 0 25 10 3 m located after the first grains and spanning the full domain cross section the line is composed of 800 points at each point of the line 2 104 particles are released to simulate the impact of molecular diffusion the considered scenario is characterized by a p e 20 and p e 100 the specific parameters are detailed in table 1 reactive transport the reactive transport scenario is characterized by a uniform initial placement of ac and bc particles in two adjacent slabs of width ℓ s ℓ 6 which is equivalent to a species a adjacent to species b we inject the same number of ac and bc particles which are propagated conservatively according to 10 we ran the simulations for the two media and for the two péclet numbers considered in the line injection case we found that incomplete mixing due to finite number of particles is negligible for n 0 108 particles we use 108 ac and 108 bc particles which means in total n 0 2 10 8 particles in order to determine the number of c particles generated at any time t n n δ t we proceed as follows we discretize the domain into square bins of side length 1 25 10 6 m thus the number δnc of c particles generated at time tn within a pixel is given by the minimum of the number δnac of ac and δnbc of bc particles δ n c min δ n a c δ n b c the total number nc of c particles in the domain at time tn is computed by summing over all the cells of the domain the mass of c is computed from the number nc of c particles as 11 m c n c n 0 where n 0 is the total number of particles initially placed in the domain with this definition the initial concentration of ac and bc in each of the slabs is given by c 0 1 2 ℓ s w see also perez et al 2019a 3 effective dispersion and dispersive lamellae in this section we define the concept of effective dispersion and the dispersive lamella approach to upscale pore scale reactive mixing to the darcy scale solute mixing and dispersion is analyzed by considering the distribution c x t for the normalized line source ϕ c x t 0 w 1 δ x ρ y where ρ y 1 if y is in the pore space and 0 else such that 12 1 w 0 w d y ρ y ϕ where w is the width of the medium the evolution of the concentration distribution from the line source is illustrated in fig 3 the line is composed of the partial plumes g x t y which satisfy 1 for the point like initial condition g x t 0 y δ x δ y y ρ y the concentration can then be expressed as 13 c x t 1 w ϕ 0 w d y ρ y g x t y note that g x t y is a green function of the transport problem fig 4 illustrates snapshots of the spatial distribution of two green functions originating from two different locations at different times 3 1 dispersive lamella the upscaling excercise is related to finding an upscaled effective expression ge x t y for the transport green function that captures the salient features of pore scale mixing and that is able to quantify the evolution of the mixing volume associated to the evolution of the complex concentration distribution illustrated in fig 3 to this end we define the center of mass velocity ν t y of the partial plumes and the effective dispersion coefficient in the mean flow direction 14 ν t y d m 1 t y d t d e t 1 2 d σ e 2 t d t where the longitudinal moments of g x t y 15 m i t y ω f d x x i g x t y σ 2 t y m 2 t y m 1 t y 2 for i 1 2 where ω f denotes the pore space the m 1 t y denote the center of mass positions of the partial plumes originating in y the global center of mass position and the effective variance are defined by 16 m 1 t 1 w ϕ 0 w d y ρ y m 1 t y σ e 2 t 1 w ϕ 0 w d y ρ y m 2 t y m 1 t y 2 the square root of the effective variance σ e 2 t is a measure for the average width of the partial plumes that form the line source and thus for the width of the mixing area effective dispersion is a key concept for the quantification of the mixing interface the dispersive lamella approach models the green function as g e x t y g ℓ x m 1 t y t g y m 2 t y t y where g ℓ x t satisfies the one dimensional dispersion equation 17 g ℓ x t t d e t 2 g ℓ x t x 2 0 for the initial condition g ℓ x t δ x see appendix a the subscript ℓ stands for lamella the centered transverse green function g y t y describes solute dispersion in transverse direction see appendix a the effective green function ge x t y is an upscaled object in that its values are defined at all positions in space it is defined with respect to the bulk volume thus the total mass in the domain is given by ϕ times the space integral of ge x t y the centered green function g ℓ x t denotes the dispersive lamella note that the advective dispersion of the mixing front is quantified by the fluctuations of the center of mass positions m 1 t y between the partial plumes forming the mixing interface this spreading effect is separated from mixing through centering of the green function the upscaled concentration distribution c x t evolving from an initial line source is thus constructed from ge x t y as 18 c x t 1 w 0 w d y g ℓ x m 1 t y t g y m 2 t y t y the solution of eq 17 gives for g ℓ x t the gaussian distribution 19 g e x t exp x 2 2 σ e 2 t 2 π σ e 2 t the dispersive lamella is a measure for the concentration in the coordinate system attached to the center of mass position m t y which delineates the mixing interface the concentration distribution homogenizes with time as the variability of the center of mass positions decreases and dispersion increases as illustrated in fig 4 for two partial plumes that evolve from different initial positions by comparison with fig 3 we can identify the contributions of these partial plumes to the overall plume with increasing time the two partial plumes converge to similar behavior due to transverse heterogeneity sampling both the center of mass positions as well as the plume shape converge as discussed in appendix a the vertically integrated profiles can be well approximated by a gaussian shaped distribution similar approaches were used for the quantification of mixing and reactive mixing in darcy scale heterogeneous porous media cirpka and kitanidis 2000 jose and cirpka 2004 3 2 effective dispersion coefficients we discuss here the evolution of effective pore scale dispersion which is the centerpiece of the upscaled reactive mixing approach presented in the previous section effective pore scale dispersion is measured by the effective variance σ e 2 t defined in 16 in addition we consider the apparent variance which is defined by 20 σ a 2 t 1 w ϕ 0 w d y ρ y m 2 t y 1 w ϕ 0 w d y ρ y m 1 t y 2 it can be readily shown by using expression 13 that σ a 2 t is the longitudinal second central moment of c x t thus it measures the overall extension of the disperse concentration distribution evolving from the initial line source the asymptotic hydrodynamic dispersion coefficient is defined by fitting the linear function 2 d t a to σ a 2 t for t τd the difference between the apparent and effective variances is given by 21 σ a 2 t σ e 2 t 1 w ϕ 0 w d y ρ y m 1 t y 2 1 w ϕ 0 w d y ρ y m 1 t y 2 this means that it quantifies the fluctuations of the center of mass positions of the green functions of which the concentration distribution is composed fig 5 illustrates the variability in the evolution of the center of mass positions in med1 and med2 the center of mass positions for the partial plumes are conditioned by the velocities at the starting point and converge asymptotically toward the mean flow velocity the fluctuations are larger in the more heterogeneous med2 fig 6 shows the evolution of the effective and apparent variances for med1 and med2 at short times t τv both the apparent and effective variances behave as 2dt because the solute has not yet noted the advective variability along the line as illustrated in fig 3 for t τv the initially straight line starts being distorted due to the advective heterogeneity experienced along the line this leads to a strong increase of the apparent variance which as mentioned above is a measure for the spread of the line due to differences in the center of mass positions of the partial plumes that form the line the effective variance evolves slower and the two quantities start diverging for t τv the difference between the apparent and effective variances is larger for the more heterogeneous med2 which reflects the stronger fluctuations of the center of mass positions of the partial plumes as shown in fig 5 when transverse diffusion activates advective heterogeneity as a mixing mechanism the effective variance increases this increase in the actual width of the interface for t τv is also illustrated in the evolution of the line source in fig 3 in fact as shown in fig 4 transverse mass transfer makes each partial plume sample the vertical velocity contrast so that i the center of mass velocities evolve toward the average flow velocity and ii its longitudinal variance increases due to the impact of velocity fluctuations on scales smaller than the plume scale dentz and de barros 2015 advective differences between the partial plumes are smoothed out due to transverse diffusion and transfered to effective dispersion as shown in figs 5 and 6 at asymptotic times for t τd the apparent and effective variances converge to the same asymptotic behavior that is characterized by the hydrodynamic dispersion coefficient d which shows that the center of mass fluctuations between partial plumes eventually vanish 4 pore scale mixing and reaction in this section we elaborate on the implications of the dispersive lamella methodology onto the reaction rate predictions the fickian framework generally overestimates the amount of reaction occuring at the pore scale raje and kapoor 2000 gramling et al 2002 the dispersive lamella approach suggests that such behaviors can be understood by preasymptotic mixing as described by effective dispersion and quantified by the effective variance σ e 2 t which measures the effective interface width in order to quantify the reaction product for the instantaneous bimolecular reaction discussed in sections 2 2 and 2 3 3 we consider the concentration profiles across the interface separating the ac and bc species in the coordinate system attached to the center of mass positions m t y which delineates the mixing interface the average concentration c a c x t of the conservative component ac across the mixing interface is given by 22 c a c x t 0 d x 0 w d y g ℓ x x t g y t y 1 2 erfc x 2 σ e 2 t where we used expression 19 for g ℓ x t and the fact that the integral of g y t y over y is equal to one the concentration of bc is given by c b c x t 1 c a c x t note that this solution assumes that the initial distributions of the ac and bc species extend from minus infinity to zero and from 0 to infinity respectively the solution for the slab initial distribution is given in appendix b for the scenario under consideration there is virtually no difference between the two solutions the product concentration c c x t is obtained as outlined in section 2 from c c x t min c a c x t c b c x t as 23 c c x t 1 2 erfc x 2 σ e 2 t the product mass is obtained from 23 by integration of space and multiplication by ϕ as 24 m c t c 0 w ϕ 2 σ e 2 t π fig 7 shows the evolution of the product mass described by expression 24 using the effective variance together with the direct numerical simulation results for med1 and med2 for p e 20 and p e 100 the mass predictions from effective dispersion match accurately the reaction rates at all times for the two media and the two péclet number regimes deviations between the mass obtained from the direction numerical simulations and the dispersive lamella in term of effective dispersion can be explained by the finite size of the model medium for comparison we also display the behavior obtained by using the diffusion and hydrodynamic dispersion coefficients at early times for t τv diffusion is the main mixing mechanism driving the reaction and the product mass is expected to increases as m c t w ϕ 4 d t π this behavior is represented by the effective variance which at short times evolves as 2dt as discussed in the previous section with increasing time t τv both the length and spread of the mixing interface increase due to advective variability along the interface with increasing time advective heterogeneity is activated as a mixing mechanism by transverse diffusion which leads to a strong increase in the simulated product mass as correctly described by effective dispersion at times t τd the prediction in terms of the effective variances converges to fickian like behavior hydrodynamic dispersion predicts consistently higher values than effective dispersion thus the fact that the variance of the mixing interface evolves non linearly according to effective dispersion in contrast to the behavior predicted by constant hydrodynamic dispersion can explain features of incomplete mixing in fact the behaviors shown in fig 7 are qualitatively very similar to the ones observed in the experiments by gramling et al 2002 thus the evolution of the product mass from pore scale reactive mixing can be explained and quantified in terms of effective dispersion coefficients which capture the width of the mixing interface between two initially segregated reacting species 5 conclusions we study the upscaling and quantification of mixing in two dimensional synthetic porous media in terms of effective dispersion coefficients direct numerical simulations give insight into conservative mixing and spreading behavior of a solute distribution originating from a uniform line source at short times smaller than the characteristic advection time over a pore length mixing is dominated by diffusion for increasing time advective heterogeneity along the interface is activated as a mixing mechanism by transverse diffusion which increases the interface width eventually at asymptotically long times the mixing eventually becomes fickian and may be described by hydrodynamic dispersion this evolution is at the root of the notion of incomplete mixing these mechanisms are captured by a dispersive lamella approach that approximates the transport green function by a gaussian distribution characterized by the effective longitudinal variance and a center of mass position that reflects the advective variability the effective longitudinal variance characterizes the average width of the transport green function diffusive sampling of the transverse velocity contrast smoothes the variability of the center of mass position which converges toward the mean flow velocity and leads to an increase of longitudinal mixing thus the variability of the center of mass position between the dispersive lamellae accounts for the advective spreading of the interface while effective dispersion quantifies the actual mixing in this work we have studied and quantified effective dispersion from detailed 2d numerical simulations and used its evolution in order to describe the mass production in a fast bimolecular irreversible reaction for which the diffusion constant is assumed to be the same for both species we have seen the reaction mass predicted from the dispersive lamella methodology accurately match results from direct numerical reactive simulations for different levels of heterogeneity and different péclet regimes note that while we introduced the methology in two dimensions we anticipate that it is also valid in three dimensions one would however expect different temporal evolution of the apparent and effective dispersion coefficients due to more efficient transverse sampling of the pore scale heterogeneity while the quantification of constant hydrodynamic dispersion coefficients has been studied extensively in the literature we are not aware of similar works on the concept of effective dispersion unlike for dispersion in poiseuille flow and darcy scale heterogeneous porous media similar approaches such as volume averaging brenner and edwards 1993 quintard and whitaker 1994a and stochastic averaging dagan 1989 can be used to relate the evolution of effective dispersion to the medium structure and pore scale hydrodynamics furthermore continuous time random walks for pore scale transport puyguiraud et al 2019 2020 provide an efficient framework to quantify advective spreading represented by the center of mass fluctuations between the dispersive lamellae that form the interface the proposed dispersive lamella approach provides a systematic framework to integrate these modeling approaches and upscaling techniques for the physics based prediction of pore scale mixing and reaction credit authorship contribution statement alexandre puyguiraud data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation visualization writing original draft writing review editing lazaro j perez data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation visualization writing original draft writing review editing juan j hidalgo data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation visualization writing original draft writing review editing marco dentz data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation visualization writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been supported by the european research council under the european union s seventh framework programme fp7 2007 2013 erc grant agreement no 617511 mhetscale and the spanish ministry of science and innovation through the project hydropore pid2019 106887gb c31 juan j hidalgo acknowledges the support of the spanish ministry of science and innovation through the ramon y cajal program appendix a dispersive lamella in order to derive the governing equation for the green function in the dispersive lamella approach we consider the langevin equation equivalent to the advection diffusion eq 1 for pore scale transport a 1 d x t y d t u t y 2 d ξ t where we defined the lagrangian velocity a 2 u t y v x t y the initial particle position is x t 0 y 0 y ω f the symbol ξ t denotes a gaussian white noise characterized by unit mean and correlation ξ i t ξ j t δ i j δ t t the angular brackets denote the average over all noise realizations we separate the lagrangian velocity into its noise mean and fluctuation u t y around it a 3 u t y u t y u t y the fluctuation mean is zero by definition and its covariance function is given by a 4 c i j t t y v i t y v j t y it is in general not stationary but may become stationary in the long time limit we model the fluctuation u t y as a gaussian distributed random vector ζ t y characterized by zero mean and the covariance matrix a 4 thus the langevin eq a 1 can be approximated as a 5 d x t y d t u t y ζ t y 2 d ξ t note that this approximation models particle motion in the bulk and is not restricted to the pore space anymore in this sense it represents the upscaling and homogenization step we define the effective green function ge x t y in terms of the effective particle trajectories x t y as a 6 g e x t δ x x t y where the angular brackets here stand for the average over all noise realizations note that the dirac delta refers to the bulk volume thus ge x t is the bulk concentration and its integral is equal to ϕ the langevin eq a 5 is equivalent to the advection dispersion equation a 7 g e x t y t u t y g e x t y d e t y g e x t y 0 where the dispersion tensor d e t y is defined by its components as a 8 d i j e t y d δ i j 0 t d t c i j t t y this approximation implies that the green function ge x t y for an infinite medium is given by a gaussian distribution characterized by the centered mean and variance a 9 m t y 0 t d t u t y κ t y 2 d t 1 2 0 t d t d e t y where 1 denotes the identity matrix the quality of the gaussian approximation is illustrated in fig a 8 which shows the vertically integrated green function obtained from the direct numerical simulations note that we do not directly evaluate the accuracy of the gaussian approximation for the green function the gaussian approximation is the basis for the predictions of reactive mixing in section 4 which serves as a pragmatic a posteriori evaluation of the validity and accuracy of the gaussian approximation we replace the local effective dispersion tensor dij t y by its vertical average a 10 d i j e t 1 w ϕ 0 w d y ρ y d i j e t y which quantifies the average width of the mixing interface based on the random nature of the media under consideration the off diagonal coefficients of the effective dispersion tensor are considered small and set to zero d i j e t 0 for i j under this condition the green function ge x t y can be decomposed into a 11 g e x t y g ℓ x m 1 t y t g y m 2 t y t y the g ℓ x t y satisfies the diffusion equation a 12 g ℓ x t t d e t g e x t y 0 where we set d e t d 11 e t the transverse green function g y t y captures the solute extension in vertical direction it satisfies a 13 g y t y t d 22 e t 2 g y t y y 2 0 furthermore g y t y satisfies a 14 0 w d y g y t y 0 w d y g y t y 1 the explicit analytical expression for g ℓ x t y for a medium with infinite longitudinal extension is a 15 g ℓ x t y exp x 2 2 σ e 2 t 2 π σ e 2 t this is a good approximation if the plume is far from the vertical boundaries fig a 8 shows the vertically integrated concentration distribution obtained from the direct numerical simulation and the gaussian model a 15 while the concentration distribution is subject to fluctuations that reflect the alternation between void and solid it is compact and it can be well characterized in terms of the center of mass position and longitudinal variance appendix b solution for slab injection perez et al 2019b provide the analytical solutions for an initial distribution of two adjacent slabs of a and b which we summarize here for convenience for a constant hydrodynamic dispersion coefficient d the product mass is given by b 1 m c t w ϕ c 0 4 d t π 1 exp τ d 4 t π τ d 4 t erfc τ d 4 t where we define τ d ℓ s 2 d the diffusion time over the initial slab extension in the case of a temporally variable effective longitudinal dispersion coefficient the total mass evolves as b 2 m c t w ϕ c 0 2 π σ e t 1 exp a e t 2 π a e t erfc a e t where we define a e t ℓ s 2 σ e t 2 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103782 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
383,we study the upscaling of pore scale transport of passive solute in a carbonate rock sample it is characterized by microporous regions displaying heterogeneous porosity distribution that are accessible due to diffusion only and a strongly heterogeneous mobile pore space characterized by a broad distribution of flow velocities we observe breakthrough curves that are characterized by strong tailing which can be attributed to velocity variability in the flowing medium portion and solute retention in the microporous space using accurate numerical flow and transport simulations we separate these two mechanisms by analyzing the statistics of residence times in the mobile phase and the trapping and residence time statistics in the mmobile phase we employ a continuous time random walk framework in order to upscale transport using a particle based implementation of mobile immobile mass transfer and heterogeneous advection this approach is based on the statistics of the characteristic mobile and immobile residence times and mass transfer rates between the two continua while classical mobile immobile approaches model mass transfer as a constant rate process we find that the trapping rate increases with increasing mobile residence times until it reaches a constant asymptotic value based on these findings and the statistical characteristics of travel and retention times we derive an upscaled lagrangian transport model that separates the processes of heterogeneous advection and diffusion in the immobile microporous space and provides accurate descriptions of the observed non fickian breakthrough curves these results shed light on transport upscaling in highly complex dual porosity rocks for which mobile immobile mass transfer are controlled by a dual multirate process controlled by the heterogeneity of both the flow field in the connected porosity and the diffusion in the no flow regions keywords non fickian dispersion heterogeneous porous media upscaling time domain random walk continuous time random walk dual multirate mass transfer model 1 introduction solute transport in the laminar flow through the void space of a porous medium is due to molecular diffusion and advection despite the simplicity of these fundamental processes observed transport is characterized by complex features such as strong breakthrough curve tailing non gaussian concentration distributions anomalous dispersion incomplete mixing and intermittent lagrangian flow properties cortis and berkowitz 2004 seymour et al 2004 bijeljic et al 2011 de anna et al 2013 bijeljic et al 2013 kang et al 2014 holzner et al 2015 morales et al 2017 these behaviors are due to the intricate structure of the pore space and the multi scale heterogeneity distribution which cause broad distributions of advective and diffusive mass transfer time scales and transport pathways bijeljic et al 2011 porta et al 2015 puyguiraud et al 2019a the understanding of these heterogeneity mechanisms and their quantification in upscaled transport models are key issues in many academic and engineering applications concerned with the large scale macroscopic prediction of the fate of conservative and reactive solutes in geological and engineered media such as the assessment of groundwater contamination and remediation geological storage of nuclear waste geothermal energy production and underground storage of carbon dioxide domenico and schwartz 1997 poinssot and geckeis 2012 niemi et al 2017 classical upscaling approaches quantify darcy scale transport in terms of hydrodynamic dispersion coefficients bear 1972 which incorporate the large scale dispersive effect of pore scale velocity fluctuations the key issue is evidently the determination of the macroscopic dispersion coefficient for instance de josselin de jong 1958 and saffman 1959 used lagrangian stochastic models for pore scale particle motion in order to derive expressions for the hydrodynamic dispersion coefficients their approaches are based on the fact that velocities vary on typical length scales the pore lengths thus particles spend more time in low than in high flow velocity regions this behavior which lies at the origin of pore scale lagrangian intermittency dentz et al 2016 is modeled by a distribution of travel times the spatial transitions and the transition times depend on both the pore velocities and molecular diffusion since these pioneering works hydrodynamic dispersion and its dependence on the local péclet number the ratio of the characteristic diffusion time to the advection time was the subject of numerous experimental numerical and theoretical investigations scheven 2013 swanson et al 2015 pfannkuch 1963 rashidi et al 1996 jourak et al 2013 bijeljic and blunt 2006 systematic upscaling approaches have been based on generalized taylor dispersion theory brenner 1980 salles et al 1993 volume averaging quintard and whitaker 1994 davit et al 2012 2013 and continuous time random walk ctrw bijeljic and blunt 2006 ctrw methods similar to the approaches involved in the works of de josselin de jong 1958 and saffman 1959 were used to model non fickian pore scale transport features such as anomalous dispersion breakthrough curves tailing and intermittent lagrangian particle velocities bijeljic et al 2011 de anna et al 2013 gjetvaj et al 2015 kang et al 2014 puyguiraud et al 2019b the implementation of ctrw is often handled by modeling particle transport through transitions over fixed spatial scales characterized by random transition times berkowitz et al 2006 the time domain random walk tdrw method that will be used in this study is also based on particles motion over fixed distance transition times are determined kinematically from the eulerian flow field and the spatially distributed properties for instance the porosity and diffusivity that can be mapped either from tomographic imaging or from statistical models thus these methods provide a means to relate pore scale flow properties to darcy scale transport behavior bijeljic et al 2011 puyguiraud et al 2019 2020 the presence of immobile medium regions that consist of dead end pores regions of low flow in the wake of solid grains and microporosity where diffusion dominates are common features of porous media such as reservoir rocks the large difference in terms of péclet number between these zones of the porous media and the connected network of pores that forms the usual macroporosity the flowing porosity supports the use of dual continuum mobile immobile mass transfer approaches first proposed by van genuchten and wierenga 1976 this approach has been widely used to take into account the often encountered control of spatially distributed diffusive zones on the overall hydrodynamic transport and specifically on the occurrence of often highly non fickian breakthrough curves observed experimentally from laboratory to field scales the heterogeneous medium is modeled by overlapping mobile and immobile continua at each point in space the system state is defined by a mobile and series of immobile concentrations the mobile and immobile continua communicate through linear mass transfer in two equation models ahmadi et al 1998 cherblanc et al 2007 which can be formulated in a way that allows the immobile concentrations to be written as linear functionals of the mobile concentration which are characterized by a memory kernel haggerty and gorelick 1995 carrera et al 1998 that accounts for the microscale mass transfer processes as such it is a method to upscale pore scale transport many implementations of this approach consider a constant average velocity in the mobile medium portion liu and kitanidis 2012 porta et al 2013 2015 however advective heterogeneity this means velocity variability in the flowing medium portion by itself gives rise to anomalous transport bijeljic et al 2011 de anna et al 2013 kang et al 2014 puyguiraud et al 2019 this is why the importance of pore scale velocity statistics and their relation to the complex medium and heterogeneity structure have been studied in a series of recent experimental and numerical works siena et al 2014 matyka et al 2016 holzner et al 2015 de anna et al 2017 alim et al 2017 dentz et al 2018 aramideh et al 2018 some authors have coupled ctrw models of advective heterogeneity with trapping in immobile regions gjetvaj et al 2015 dentz et al 2018 key items for these modeling approaches are the identification of the dominant pore scale transport and mass transfer processes their relation to the pore scale medium and the flow properties these aims have been pursued by experiments swanson et al 2015 numerical simulations de vries et al 2017 ceriotti et al 2019 and formal upscaling using volume averaging davit et al 2012 orgogozo et al 2013 porta et al 2015 and lagrangian ctrw based methods gjetvaj et al 2015 dentz et al 2018 volume averaging delineates a mobile region the flowing porosity and immobile regions such as biofilms orgogozo et al 2013 based on a velocity cutoff determined from a péclet criterion porta et al 2015 the large scale transport model is then obtained by averaging the microscale transport equations over a unit cell that is statistically representative of the properties of the medium and the flow and contains two distinct medium portions which as outlined above are connected through mass transfer across domain boundaries lagrangian stochastic models margolin et al 2003 benson and meerschaert 2009 dentz et al 2012 comolli et al 2016 formulate mass transfer between mobile and immobile medium regions through compound stochastic poisson processes feller 1968 this means that mass transfer events occur at constant rate quantified by the poisson process which renders the residence time in immobile regions as the sum over individual trapping times a compound poisson process as shown by margolin et al 2003 benson and meerschaert 2009 and discussed further in this paper this formulation is equivalent to eulerian mobile immobile mass transfer formulations this study aims at testing our capability of characterizing and upscaling hydrodynamic transport in heterogeneous natural reservoirs where both velocity distribution and immobile domain heterogeneity cause anomalous transport starting from the model assuming that mobile immobile mass transfers are controlled by a poisson process for that we use as an example of highly heterogeneous media a carbonate sample imaged using x ray microtomography that displays marked bimodal structural heterogeneity caused by the presence of connected macro porosity and microporous material that results from grain sedimentation and diagenesis events the image is processed in order to map the mobile and the immobile domain and direct numerical simulations of flow and transport are performed we investigate in detail the statistics of mobile and immobile particle motion in terms of the respective residence times the trapping rates and the mobile and immobile times between trapping events using the detailed statistical analysis we discuss the salient features of transport at the pore scale and quantify them in an upscaled transport model based on a lagrangian formulation that implements in a simple form the specific process that characterizes the spatial distribution of the mobile immobile mass transfers in heterogeneous media the paper is organized as follows section 2 details the methodology it describes the numerical solution of the direct flow and transport problem the simulation setup the boundary conditions and the model outputs which allow investigating in detail the statistics of mobile and immobile particle motion in terms of the respective residence times the trapping rates and the mobile and immobile times between trapping events in section 3 we consider transport in a simple fracture matrix setup in order to present the concept of the basic lagrangian methodology for a simple mobile immobile system for which the single trapping rate upscaling ctrw formulation is detailed and then validated using the direct simulation results then following the same approaches we investigate in section 4 the transport behavior in the carbonate rock sample using direct numerical simulations as well as the upscaling of transport in the immobile domain using a statistical multi trapping approach then in section 5 we derive a new upscaled lagrangian model and validate it by comparison with the results of the direct numerical simulations conclusions are presented in section 6 2 material and methods 2 1 sample mc10 properties the mc10 carbonate sample is a porous and permeable rock made of quasi pure calcite the structure that results from complex sedimentation and diagenetic events is made of impervious grains of variable characteristic length ranging from few tens to about 150 µm and microporous material that ensure the cohesion of the rock when performing x ray microtomography of such single solid phase material the x ray energy attenuation integrated over each voxel of the final 3 dimensional image denotes the porosity for this study we use a cropped sub volume made of 900 900 900 cubic voxels of side dimension d x 1 6867 10 6 m mc10 is characterized by connected macropores of mean size 70 10 6 m the microporous material diagenetic cement is considered immobile regarding fluid flow and only accessible to solute by diffusion it displays variable porosity made by pores that are smaller than the imaging resolution accordingly connected macro porosity delimits the mobile domain while the microporous material delimits the immobile domain details on the sample characteristics and the methodologies applied to process the x ray tomographic image are given in appendix a the effective diffusion de in each location of the immobile domain i e in each voxel is the product of the molecular diffusion d 0 times the effective porosity ϕe 1 d e x d 0 ϕ e x d 0 ϕ x κ x where κ denotes the immobile domain tortuosity that can be considered as a constant or a function of the immobile domain porosity ϕ such as the formulation derived from the electric tortuosity by archie 1942 κ x ϕ x 1 m accordingly 1 can be rewritten 2 d e x d 0 ϕ x m with m ranging from 1 assuming that there is no tortuosity effect to about 4 5 in microporous limestones gouze et al 2008 the diffusion coefficient d 0 is constant for all simulations and set to 10 9 m2s 1 note that the porosity of the immobile domain is defined as the porosity accessible by a solute diffusing from the mobile domain and thus can be different from the total porosity of the immobile domain for instance if porous zones are embedded in zones considered as non diffusive as explained below for each voxel of the immobile domain transport by diffusion is impossible below a given porosity value different approaches using for instance percolation theory critical path analysis or effective medium approximation theory can be used to evaluate the porosity threshold ζ below which the system is non percolating for diffusion see hunt and sahimi 2017 hommel et al 2018 and references herein for the sample considered here applying a porosity threshold consists in transforming the fraction of the immobile domain where ϕ ζ into solid 3 d e x d 0 ϕ e for ϕ ζ 0 for ϕ ζ the porosity value of the immobile domain resulting from the image processing ranges from 0 045 to 0 193 with mean porosity 0 108 hebert et al 2015 fig 1 displays a cross section normal to the main flow in the segmented image of the 9003 voxel sample where the fraction of immobile domain corresponding to porosity below threshold values of 10 are enlighten for instance applying a porosity threshold of ζ 0 1 acts as removing 39 of the immobile domain the mean porosity of the remaining fraction of the immobile domain is then 0 175 however applying this threshold does not change noticeably the area of the mobile immobile interface which is 1 22 105 m2 per m3 of mobile domain when ζ 0 0 and 1 20 105 m 1 when ζ 0 1 i e a decrease of 1 64 in this paper different tortuosity models are investigated assuming a porosity threshold of ζ 0 1 and tortuosity defined by κ ϕ ϕ 1 m with m 2 5 is the most realistic model garing et al 2014 but models with m 1 5 and 4 5 as well as with a constant tortuosity model κ 1 8 and ζ 0 are investigated in order to explore the feedback control of the immobile domain diffusivity on the overall solute transport and on the upscaling feasibility a comprehensive characterization of the diffusion properties according to the assumption made on tortuosity are given in appendix b 2 2 mobile domain flow we consider the flow in the sample at low reynolds so that the pore scale flow velocity v x is solution of the stokes equation 4 2 v x 1 μ p x where p x is the fluid pressure the 9003 cubic voxels mesh is directly used as the meshed domain for openfoam calculations using a permeameter like configuration i constant pressure is applied at the inlet z 0 and the outlet z l z boundaries where 20 pixels layers of unitary porosity are added in order to obtain an accurate determination of the velocity components at the inlet and outlet of the domain ii the domain is bounded by solid at x 0 x l x y 0 and y l y iii no slip conditions are applied at the mobile solid and the immobile solid domain boundaries lx ly and lz denote the domain lengths in the x y and z directions respectively the flow equations are solved via a finite volume scheme implemented in the simple algorithm of openfoam https cfd direct openfoam user guide v7 fvsolution this algorithm solves the steady state stokes equation 4 and continuity equation v x 0 following an iterative procedure convergence is reached when the difference in terms of pressure and velocity components between the current and the previous steps is smaller than a threshold once convergence has been reached we extract the velocity field components that are computed at each of the voxel interface the fluid velocity in the direction along the z axis the main flow direction displays an asymmetric shape with some negative values that emphasize the high complexity of the flow field triggered by the high heterogeneity of the mobile domain the fluid velocity perpendicular to the main flow direction is quasi symmetric fig 2 the flux weighted velocity norm pdf p e v v v p e v where pe v denotes the pdf of the eulerian velocity norm is displayed in fig 2 puyguiraud et al 2019 showed that for stationary system p e is equal to the lagrangian velocity pdf which is the core information required for upscaling advective transport in the mobile domain puyguiraud et al 2019 upscaling of the advective transport using p e for this highly heterogeneous sample is beyond the scope of the present work that focuses on upscaling the immobile domain transport and will be presented in a future dedicated paper nevertheless we note that the pdf p e presented in fig 2 is quite similar to that of the sandstone sample presented in fig 2 in puyguiraud et al 2019 for which upscaling methods were proposed by the authors 2 3 transport simulations transport in the mobile immobile domain is described by the generic advection diffusion equation which is considered to apply at the scale of each voxel 5 c x t t v x c x t d e 2 c x t 0 where de is the effective diffusion coefficient and v x is the flow velocity in the mobile domain de reduces to the molecular diffusion coefficient d 0 whereas the flow velocity is zero in the immobile domain equation 5 is solved numerically using a time domain random walk tdrw method russian et al 2016 which is based on the formulation of eq 5 as a master equation using a finite volume discretization of the spatial operators a complete description of the tdrw method a demonstration of its equivalence with eq 5 and its implementation using voxelized images of porous media can be found in dentz et al 2012 and russian et al 2016 the main features of the method are given below the domain discretization used for transport is the same as the one used for computing the flow the tdrw approach models the displacement of particles in space and time their ensemble average giving the solution of the transport equation for the considered media for each particle each motion event is denoted by a single jump from one voxel to one of the 6 face neighboring voxels as such the jump distance ξ is constant and equal to the voxel size dx the direction and the jump duration are controlled by the local properties of the voxels i e the fluid velocity and the effective diffusion coefficient the recursive relations that describe the random walk from position x j to position x i of a given particle at jump n is 6 x i n 1 x j n ξ t n 1 t n τ j with ξ ξ denoting the transition length the probability wij for a transition of length ξ from pixel j to pixel i and the transition time τj associated to pixel j are given by 7 w i j b i j j k b k j τ j 1 j k b k j where the notation jk indicates the summation over the nearest neighbors of pixel j the bij are given by 8 b i j d e i j ξ 2 v i j 2 ξ v i j v i j 1 where d e i j denotes the harmonic mean of the diffusion coefficients of pixels i and j and vij denotes the velocity component of v j in the direction of pixel i v i j v j ξ i j as a convention voxel i is downstream from pixel j if vij 0 note that the tdrw can be seen as a continuous time random walk ctrw because it treats time as an exponentially distributed continuous random variable whose mean may vary between voxels in this paper we use the term tdrw for the numerical random walk method used to solve the direct problem and the term ctrw for the upscaled random walk framework 2 4 tdrw simulations setup the applied boundary condition at the sample inlet z 0 is a pulse of constant concentration in the mobile domain only this is performed by applying a flux weighted injection of the particles at t 0 by construction the pulse is formally an exponential concentration function of characteristic time τ j z d x 2 russian et al 2016 the main result is given by the first passage time at the outlet of the mobile domain which denotes the inert tracer breakthrough curve btc no flux boundary condition is set at x 0 x l x y 0 y l y as well as z 0 and z l z in the immobile domain simulations are performed for different values of péclet number which is defined by p e v l d 0 where l is a characteristic length which is taken here as the average pore length each simulation involves at least 107 particles the statistics concerning the characteristics of the trapping events such as the trapping time in the immobile domain and the survival time in the mobile domain between two trapping events are obtained by sampling more than 109 events the definition of a trapping event is provided in section 2 5 2 5 model output the mass transfers occurring in the sample are probed by a set of statistical distributions which are given as probability density functions pdfs denoted ψ ˇ where the overlying reversed hat symbol indicates that they are derived from the results of the direct tdrw simulations these pdfs describe the advection diffusion transport in the mobile domain the exchange between the mobile and the immobile domains and the diffusive transport in the immobile domain in terms of random walk process we will name each intrusion of a particle into the immobile domain a trapping event trapping time pdf denoted ψ ˇ τ i m is the pdf of the time τim p n p spent by the particles p p 1 p in the immobile domain during the trapping events n p n p 1 n p where p is the total number of particles exiting at the sample s outlet and n p is the total number of trapping events encountered by particle p immobile time pdf denoted ψ ˇ t i m is the pdf of the time tim p spent by the particles in the immobile domain to cross the entire domain i e from z 0 to z l z for a given particle p t i m n p τ i m d n survival time pdf denoted ψ ˇ τ s is the pdf of the times τs p n p spent by the particles p in the mobile domain between trapping events n 1 and n mobile time pdf denoted ψ ˇ t m is the pdf of the time tm p spent by the particles in the mobile domain to cross the entire domain for a given particle p t m n τ s d n ϵ where ϵ is the sum of the time spent to move from the inlet to the location of the first trapping event and of the time spent to move from the exit location of trapping event n p to the outlet trapping rate pdf denoted ψ ˇ γ is the pdf of γ p n p t m p first passage time pdf denoted ψ ˇ t t is the pdf of the first passage time tt p spent by the particles to cross the domain and is equivalent to the breakthrough curve btc by definition for each particle p 9 t t p t m p t i m p t m p i 1 n p τ i m p i 3 modeling transport in a single fracture with mobile immobile mass transfer in this section we investigate the case of the transport of a passive tracer in a simple mobile immobile domain system that can be adequately represented as a single linear fracture the mobile domain crossing a continuous porous matrix the immobile domain the different pdfs characterizing the transport process described in section 2 5 will be computed using direct tdrw simulations and will later be compared to those resulting from tdrw simulations performed for the mc10 carbonate sample furthermore we present a 1d ctrw model that upscales transport in the fracture matrix system and introduces the main features and concepts used for the upscaling of transport in the mc10 carbonate sample the detailed fracture matrix transport model can be formulated in the most general form as 10 ϕ y c x t t u y c x t x d y c x t 0 where ϕ y is porosity which is equal to ϕm within the fracture and ϕim in the matrix u y is the darcy velocity which is equal to u in the fracture and 0 in the matrix similarly d y is the diffusion coefficient which is equal to dmϕm in the fracture and equal to dimϕim in the matrix where dm and dim denote the diffusion coefficient in the mobile and the immobile domain respectively in the following we consider two equivalent upscaled transport approaches 3 1 upscaling by vertical averaging upscaled transport in this fracture matrix system can be described by a multirate mass transfer model haggerty and gorelick 1995 carrera et al 1998 in the following we briefly outline the steps that lead to such a description in order to highlight the underlying assumptions the upscaled multirate mass transfer description for the fracture matrix system is obtained by vertical averaging to this end we define the averages concentration over the fracture and matrix cross sections as 11 c m x t 1 d m 0 d m d y c x t c i m x t 1 d i m 0 d i m d y c x t where dm is the width of the fracture and dim of the matrix averaging 10 over the fracture cross section gives 12 ϕ m c m x t t u c m x t x d m ϕ m 2 c m x t x 2 1 d m ϕ m d m c m x t y y 0 while the equation for purely diffusive transport in the matrix domain is 13 ϕ i m c i m x t t d i m ϕ i m 2 c i m x t y 2 0 the boundary condition are c i m x y 0 t c m x y 0 t as an expression of concentration continuity we approximate c m x t c m x t which assumes fast equilibration over the fracture cross section using flux continuity across the fracture matrix interface we obtain in appendix c 14 c m x t t v c m x t x d m 2 c m x t x 2 β c i m x t t where we defined the capacity coefficient β d i m ϕ i m d m ϕ m and the pore velocity v u ϕ m the average matrix concentration can be expressed as a linear functional of the average fracture concentration appendix c 15 c i m x t 0 t d t φ t t c m x t the memory function is well known carrera et al 1998 and can be expressed in laplace space as 16 φ λ tanh λ τ d λ τ d where we define the characteristic diffusion time τ d d i m 2 d i m in the matrix combining 14 and 15 we obtain the integro partial differential equation 17 c m x t t v c m x t x d m 2 c m x t x 2 β t 0 t d t φ t t c m x t which is equivalent to the multirate mass transfer model of haggerty and gorelick 1995 carrera et al 1998 in the following we describe the formulation of this upscaled model in a lagrangian framework 3 2 upscaled lagrangian model the upscaled lagrangian approach models one dimensional advective diffusive transport along the fracture which is interrupted by trapping events that are poisson distributed this means that transitions from the fracture to the matrix occur at constant rate γ which can be quantified by the diffusion rate over the fracture cross section at each trapping event a particle is trapped for a random time distributed according to ψim t these are the principal ingredients of the upscaled transport model in the following we formulate this model in the tdrw framework one dimensional advective diffusion particle motion at constant velocity v and diffusion coefficient dm is described by particle transitions over the fixed distance ℓ by a random time tm the probability wu for upstream particle motion is 18 w u d m τ v ℓ 2 the probability for downstream motion is accordingly w d 1 w u the time τv is defined by 19 τ v ℓ v 1 2 p e p e v ℓ d m the transition time tm is exponentially distributed 20 ψ i m t τ v exp t τ v these rules represent mobile transport as a tdrw model for advection diffusion with constant velocity v and diffusion coefficient dm russian et al 2016 this motion is combined with the trapping rules outlined in the following during a transition of duration tm nt trapping events occur such that the total transition time is given by 21 t t t m i 1 n t τ i m the number nt of trapping events is distributed according to the poisson distribution 22 p n t γ t n t exp γ t n t with mean n t γ t thus the total transition time tt describes a compound poisson process its pdf ψ t can be expressed in laplace space as margolin et al 2003 dentz et al 2012 23 ψ λ 1 1 λ τ v γ τ v 1 ψ i m λ laplace transformed quantities are marked by an asterisk the laplace variable is denoted by λ in order to show the equivalence of this lagrangian formulation with the mrmt model 17 we derive in appendix d for the concentration cm x t in the fracture 24 c m x t t v c m x t x d m 2 c m x t x 2 c i m x t t the concentration cim x t in the matrix is given by 25 c i m x t γ 0 t d t ϑ t t c m x t where the memory kernel ϑ t is defined by 26 ϑ t t d t ψ i m t it denotes the probability that the trapping time is larger than t using 25 in 24 we obtain for cm x t the governing equation 27 c m x t t v c m x t x d m 2 c m x t x 2 γ t 0 t d t ϑ t t c m x t this equation and equation 17 are equivalent if 28 γ ϑ t β φ t we first recall that φ t is normalized to 1 which can be seen by taking the limit λ 0 in 16 while the integral over ϑ t is equal to τim the mean trapping time thus we obtain 29 γ τ i m β this equivalence identifies the trapping rate γ and trapping time distribution ψim t as the key quantities in the upscaled model both quantities can be accessed by random walk particle tracking simulations as outlined in section 2 4 in the following we use this general framework for the upscaling of transport in the mc10 carbonate sample 3 3 ctrw upscaled model versus tdrw model results we tested the ctrw model by comparing the results with direct tdrw simulations for the simplest idealized 2 dimensional representation of a single fracture system the computational domain is a porous medium the immobile domain of dimension lz 20000 ly 1001 pixels embedding a fracture the mobile domain of aperture 1 pixel located at y 500 so that the immobile domain depth on each side of the fracture is ℓ im 500 pixels the pixel size is denoted ξ as in section 2 3 the flow velocity v in the fracture is constant the inlet is located at z 0 where a pulse injection is applied see section 2 4 and the outlet is located at z 20000 where the pdf of the first passage time or breakthrough curve ψ ˇ t t is monitored with a fracture aperture ξ the problem is simply characterized by the péclet number p e v ξ d 0 we performed simulations for constant diffusivity in the immobile domain d e x y d and for random lognormal distribution with de x y taken as the spatial geometric mean of the pixel diffusion simulations are performed with ξ 10 5 m d 0 10 9 m2 s 1 d e 1 774 10 11 m2 s 1 and 10 12 d e x t 10 9 m2 s 1 for the lognormal distributed diffusion model in the matrix a main attribute of the compound poisson process described in section 3 2 is that the distribution of the survival time in the mobile domain τs is exponentially distributed 30 ψ τ s t γ exp γ t where γ 1 τ s fig 3 displays the survival time distribution ψ ˇ τ s computed from the tdrw which is well fitted by an exponential distribution of mean γ n tm 1 τs fig 4 shows the perfect agreement between the breakthrough curves or first passage time pdfs ψ ˇ t t resulting from the upscaled ctrw simulations and those obtained from the tdrw simulations for pe values ranging from 1 to 100 the results are similar for the homogeneous immobile domain and for the random lognormal distribution with the same geometric mean diffusion as expected nœtinger and estebenet 2000 russian et al 2016 4 tdrw modeling of transport in the carbonate sample this section concerns direct simulations performed with the tdrw model i e simulations of the 3 dimensional domain simulations are performed according to the algorithm and the boundary conditions described in sections 2 2 and 2 3 respectively the results presented in this section focus on 4 distinct models that characterize the immobile domain diffusivity distribution in terms of tortuosity κ and porosity threshold ζ see table b 1 the simplest model assumes constant tortuosity κ 1 8 and no porosity threshold while the three other assume porosity dependent tortuosity κ ϕ m with m 1 5 2 5 or 4 5 and a porosity threshold ζ 0 1 4 1 trapping properties of the mc10 sample here the trapping characteristics of the mc10 sample are analyzed and compared to the ctrw model discussed in section 3 we recall that this model is characterized by the following feature the conditional pdf pn n tm that measures the number of trapping events conditioned to the time spent in the mobile domain is a poisson distribution with a constant trapping rate γ this means that the time spent in the mobile domain between two trapping events or survival time τs is characterized by an exponential distribution the pdf ψ ˇ τ s computed for the mc10 sample and the one corresponding to equation 30 with the same average values τs are displayed in fig 5 while the pdf pn n tm computed for the mc10 sample and the one corresponding to equation 22 where the constant trapping rate γ γ are displayed in fig 6 the latter is obtained by computing the pdf of n t m from equation 22 for each range of tm the survival time pdf ψ ˇ τ s does not depend on the average fluid velocity in the sample i e does not depend on the pe value and is controlled by the transport properties at the mobile immobile domains interface and thus is controlled by the effective diffusion of the immobile domain in the vicinity of the mobile immobile interface that is to say by the local porosity fig 5 shows that the ψ ˇ τ s pdfs are visually identical when applying a porosity threshold ζ 0 1 or not emphasizing that the value of the porosity threshold does not change noticeably the properties of the immobile domain at the mobile immobile interface nor its topology as it is shown also in appendix b as expected ψ ˇ τ s is strongly shifted toward larger time values when the immobile diffusivity at the mobile immobile interface decreases the important point is that ψ ˇ τ s curves are as a general rule not exponential distributions and display an over representation of the short survival times we see also larger maximum values compared to what is predicted by the exponential distribution but this feature decreases when m increases the conditional pdf pn n tm resulting from the mc10 simulations is compared to the one computed assuming a poisson distribution following eq 22 with a constant trapping rate γ in fig 6 the conditional pdf pn n tm resulting from the mc10 simulations is noticeably different from the one computed assuming a poisson distribution with a constant trapping rate for a given mobile time the theoretical poisson model predicts less trapping events than what is measured for the mc10 sample this discrepancy increases with the value of tm the trapping rate distribution encompasses the information about the trapping process which is controlled by the complex interactions between the mobile and immobile transport process as such one can expect that the trapping rate distribution is a macroscopic observable that characterizes the mobile immobile mass transfer and in a similar manner that the memory function is the macroscopic observable that enciphers the entire properties of immobile domain diffusive transport properties fig 7 displays the pdf of γ ψ ˇ γ and reports the percentage of the particles that do not encounter trapping when traveling from the inlet to the outlet for different properties of the immobile domain this percentage depends evidently on the value of the pe number but also on the properties of the immobile domain for instance it ranges from 1 8 to 95 6 for pe 100 depending on the value of m it follows that the average trapping rate γ cannot be inferred from 1 τs because the statistics of τs concern only particles that encounter trapping whereas a certain number of particles never visit the immobile domain yet interestingly the results presented in fig 7 show that all the pdfs ψ ˇ γ have almost the same average value γ 1 51 0 18 s 1 independently of the immobile domain properties which means that this value is an intrinsic property of the mc10 sample probably related to the geometry of the mobile domain and its interface with the immobile domain despite the distributions ψ ˇ γ being strongly dissimilar altogether these results suggest that the assumptions supporting the ctrw model of section 3 2 are not strictly met for the carbonate sample considered here taking into account these results the issue that will be investigated next is to evaluate to which extent the ctrw model is robust enough to model transport in heterogeneous media such as the mc10 sample or alternatively what additional relationship between the trapping rate properties and the mobile domain properties are required to derive a reliable upscaled model for complex systems such as the mc10 sample 4 2 upscaling the impact of diffusion in the immobile domain for each trapping event the particles that enter the immobile domain at a given location can exit at another location in the case of the single fracture model with homogeneous equivalent immobile domain the relocation distance along the linear continuous mobile immobile interface is a sharp distribution well described by its mean value 0 conversely the relocation of the particles in the mc10 sample is much less predictable due to the strong heterogeneity of the system in which the immobile domain is formed of heterogeneous clusters spatially distributed this is triggered principally by non continuous mobile immobile interfaces lacunar interface and the possibility of particles to utilize the immobile domain to take a shortcut from a given flow path to another conversely the 1 dimensional ctrw model imposes by construction that particles enter and exit the immobile domain at the same location for each trapping event simulating such a situation while keeping the complete 3 dimensional computation of the transport in the mobile domain is viewed as potentially instructive for understanding conjointly the effect of the particles relocation at the mobile immobile interface owing to the strong heterogeneity of the interface and the statistical representativeness of the trapping time pdf ψ ˇ τ i m for modeling the immobile domain transport properties at the scale of the sample to this end the tdrw solver is modified such that transport in the mobile domain and the trapping process are kept unchanged but the time spent in the immobile domain is drawn from the trapping time pdf ψ ˇ τ i m previously computed during the corresponding tdrw simulation involving the full direct simulation of the transport in the mobile and the immobile domain doing this imposes by construction that particles enter and exit the immobile domain at the same location for each trapping event similarly to the ctrw upscaled model from now on the model in which the trapping time pdf is used to model the time spent in the immobile domain at each trapping event is called the upscal tdrw model in contrast to the full tdrw model 4 2 1 control of the immobile domain diffusion properties over mobile immobile mass transfer fig 8 compiles the main information concerning the results in terms of first passage time tt mobile time tm immobile time tim and survival time τs for the full tdrw model and the upscal tdrw model these data are very valuable for understanding the control of the immobile domain properties on the way particles sample the system the capacity of the immobile domain to trigger shortcuts between zones of the mobile domain with different flow properties decreases when moving from the κ 1 8 ζ 0 model to the m 2 5 ζ 0 1 model and m 4 5 ζ 0 1 model because 1 applying a porosity threshold decreases the probability of having immobile domain clusters connected to many pores and 2 increasing the value of m acts as increasing the tortuosity i e the effective diffusion time in the immobile domain see table b 1 consequently comparing the results for the full model with the upscal model for which particles are forced to exit the immobile domain where they entered for each of the trapping events allows not only to understand the feedback effect of the particle relocation process at the mobile immobile interface on the overall transport that is quantified by the first passage time pdf but also to decompose the overall transport process in terms of the time spent in the mobile domain and the immobile domain for the κ 1 8 ζ 0 model the first passage time pdf ψ ˇ t t obtained for the full and the upscal model are noticeably dissimilar their respective shape being fully controlled by the immobile time distribution for intermediate and long times conversely the mobile time pdf are the same but different from the mobile time pdf computed assuming no immobile domain i e depending only on the mobile domain properties this indicates that the transport is strongly controlled by the broad spatial redistribution of the particles among mobile zones of distinctly different flow rates as a general rule one can conclude that the discrepancy between the upscal and the full model in terms of first passage time pdf ψ ˇ t t fig 8 originates from the fact that both the trapping rate γ and the trapping time in the immobile domain τim are different as a result of the distinct particle relocation processes when encountering trapping events from these observations one can speculate that for the κ 1 8 ζ 0 model the upscaling of such a system with a one dimensional model where particles sample the immobile domain according to the ensemble average statistics of the mobile displacement will fail even if one considers a non unique trapping rate that would be related to the mobile time the two other models of immobile domain m 2 5 and 4 5 share the same spatial geometry i e the same boundaries because they share the same porosity threshold ζ 0 1 but differ from the effective diffusion spatial distribution and mean increasing the value of m acts as decreasing 1 the mean distance of penetration of the particle into the immobile domain and 2 the relocation distance between the entrance and the exit of the particle in the immobile domain during each trapping event as such the model characterized by m 4 5 is the most similar to the simple fracture model presented in section 3 3 in terms of geometry indeed the results presented in fig 8 for m 4 5 ζ 0 1 show that the first passage time pdfs ψ ˇ t t are almost similar for the full and the upscal models while the mobile time pdfs of tm ψ ˇ t m overlap the pdfs of tm for the case where there is no immobile domain this means that the immobile domain heterogeneity does not control the advective transport in the mobile domain similarly to what occurs in the simple fracture model for the model where one sets m to the value of 2 5 which is the most realistic parameterization fig 8 tells us following the same argumentation as for the m 4 5 case that the immobile domain heterogeneity weakly controls the mobile domain transport 4 2 2 on the control of the mobile domain transport on the trapping rate fig 9 shows that the trapping rate γ is not constant but depends on the mobile time tm the function γ t depends on the properties of the immobile domain controlled by κ and ζ but also on the pe value which means that this function γ tm is not an intrinsic property of the system but depends on the flow rate yet we observed for instance for the immobile domain characterized by m 2 5 and ξ 0 1 that the trapping rate is actually constant for value of tm larger than 200 s materialized by the vertical dashed line in fig 9 the system behaves as a constant trapping rate for range of tm which increases as m increases fig 10 compares the theoretical conditional pdf pn n tm assuming a poisson distribution following eq 22 where the trapping rate is a function of tm using the values given in fig 9 with the conditional pdf pn n tm resulting from the mc10 simulations it can be seen that using the γ tm function reestablishes the consistency with the compound poisson process model compared to fig 6 this gives us the sound basis for implementing the ctrw approach presented in section 3 2 but implemented with the γ tm function for upscaling the transport in the mc10 sample 5 ctrw upscaling of mc10 as shown above the dependence of the trapping rate γ on the time spent by a particle in the mobile domain tm is a critical feature triggered by the heterogeneity of the mobile immobile domain interface accordingly the proposed model is based on the implementation of the mobile time dependence of the trapping rate in the ctrw upscaling model that was used to model the transport in the single fracture in section 3 2 one speculates that the number of trapping events n t m during a mobile transition of duration tm is poisson distributed in which the trapping rate is given by the γ tm function to test this assumption we build a simple upscaled model in which the transport processes is modeled by the unconditional downstream motion of particles with constant distance lz and a random transition time tm distributed according to ψ ˇ t m plotted in fig 8 the first passage time for each particle injected at t 0 is similar to equation 9 31 t t t m i 1 n t τ i m here the number of trapping events nt is a random variable distributed according to the poisson distribution 32 p n t m n t n exp n t n with n t t m γ t m the trapping time τim is also a random variable distributed according to ψ ˇ τ i m the results of the upscaled ctrw model are first compared with those obtained with the upscal tdrw simulations in the left plot of fig 11 the upscal tdrw simulations integrate the same relocation process as in the upscaled ctrw model as such and because both the models share the same mobile time distribution ψ ˇ t m computed assuming no immobile domain the comparison of the ctrw with the tdrw upscal model is a sound validation of the approach used to model the trapping rate and the trapping time in the immobile domain independently of retro action of the immobile domain on the mobile domain time distribution the results given in fig 11 show that the ctrw is perfectly reproducing the btc computed with the tdrw upscal model for different properties of the immobile domain the comparison of the btcs computed with the ctrw model using the mobile time distribution ψ ˇ t m resulting from the full tdrw model with those computed with the tdrw are given in fig 11 right plot for different immobile domain parameters as well as for different values of pe in fig 12 as expected the ctrw model does not reproduce well the tdrw data for the case where m 1 5 and ζ 0 for which the immobile domain provides the largest opportunity for particle to shortcut the main flow streams clearly this specific process cannot be taken into account by the lagrangian ctrw model conversely we observe a good fit of the upscaled ctrw model results with those computed by the direct tdrw simulations for the reference case where κ x ϕ x 1 m with m 2 5 specifically the upscaled model reproduces perfectly the data for long times t 102 s for both ζ 0 and 0 1 while the fit at intermediate times is better for the case where the porosity threshold is applied i e ζ 0 1 the ability of the ctrw model to reproduce the direct simulations for times ranging over six orders of magnitude and for different values of the pe number is shown in fig 12 this figure also displays the pdfs ψ t t computed by the ctrw model assuming a single trapping rate value γ γ p thus allowing us to figure out the noticeable improvement of using the temporally evolving γ tm for upscaling the btcs at long times 6 conclusions with the objective of upscaling transport in heterogeneous media a lagrangian transport model that separates the processes of transport in mobile and immobile domains is presented along with its particle based ctrw implementation assuming an homogeneous immobile domain and a continuous mobile immobile interface mass transfer occurs as a compound poisson process with constant mobile immobile exchanges rate we show that this model is equivalent to the multirate mass transfer model of haggerty and gorelick 1995 with the mean trapping event number of the lagrangian model being equal to the capacity ratio of the multirate mass transfer model in other words the transport process in the lagrangian ctrw model is characterized by the mean trapping event number which also denotes the ratio of the solute mass in the immobile domain to that in the mobile domain at equilibrium we show that this 1 dimensional ctrw model perfectly reproduces the direct 2 dimensional tdrw simulations of the transport of solute in a linear fracture embedded into a porous matrix however this simple upscaled model is not able to reproduce accurately the trans port in the digitalized carbonate sample mc10 that is used to illustrate highly heterogeneous porous media for which we performed direct 3 dimensional tdrw simulations yet the direct simulations allow the thorough statistical analysis of the mass transfer dynamics within the two domains and of the mass exchanged at their interface which is spatially discontinuous and heterogeneous in terms of trapping rate we found that this heterogeneity of the mobile immobile interface together with the complexity of the flow in the mobile domain causes a deviation from the ctrw model presented in section 3 2 or equivalently a deviation from the mrmt model for instance the discrepancy between the computed survival time distribution for mc10 and the exponential distribution characterizing the single trapping rate model arises from the non uniqueness of the trapping rate γ is a function of tm fig 9 the survival time distribution displays a power law trend for short survival times which denotes the superposition of exponential distributions of τs tm with distinct average τs tm 1 γ tm conversely the increase of the occurrence of larger survival times compared to the exponential distribution denotes the lacunarity of the mobile immobile interface the distance and thus the time between two trapping events can be augmented due to the absence of available immobile domain in some parts of the mobile domain introducing this functional dependence of the trapping rate to the mobile time allows complying with a mobile time dependent compound poisson process in other words the mass transfers at the scale of the sample can be modeled as the ensemble average of residence time dependent mass transfers that can individually be modeled as single rate processes the comparison of the upscaled model against the direct 3 dimensional tdrw for different assumed properties of the immobile domain and different values of the péclet number permits to prove the efficiency of the model to reproduce the complex mass transfers in the two domains and at their interface as long as the spreading of solute due to the immobile domain does not reach a level where it produces a strong decorrelation of the velocity experienced by the particles in the mobile domain this situation occurs when immobile domain clusters allow short cut connections between zones of the mobile domain displaying distinctly different flow rates fortunately this situation is quite unlikely in reservoir rocks since the diffusivity in the immobile domain is generally decreasing from the mobile immobile interface ensuring together with the presence of non diffusing zones a certain insulation between adjacent flowing pore networks the final conclusion of this study is that the proposed upscaled lagrangian transport model provides an accurate description of the observed non fickian breakthrough curves in heterogeneous dual porosity media even when they are displayingbroad distributions of flow velocity values and highly heterogeneous immobile zones such as the carbonate example studied here this model is a dual multirate mass transfer model dmrmt in which the multiple rates of trapping arise from both the heterogeneity of the diffusion in immobile domain and the heterogeneity of the flow in the mobile domain acknowledgments pg and dr acknowledge funding from the cnrsiea through the project crosscale ex pics n 260280090 md and ap acknowledge funding from the spanish ministry of science and innovation through the project hydropore pid2019 106887gb c31 credit authorship contribution statement philippe gouze conceptualization software formal analysis investigation methodology writing original draft writing review editing alexandre puyguiraud formal analysis methodology writing review editing delphine roubinet formal analysis methodology writing review editing marco dentz conceptualization formal analysis methodology writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a mc10 sample description mc10 sample is a reefal carbonate sample from llucmajor platform majorca spain and is chosen as a typical example of complex porous media carbonate reservoirs are known to be heterogeneous over a wide range of scales as the result of the deposition and multiple diagenetic processes the sample is a cylinder of 9 mm diameter that was imaged using x ray computed microtomography which is a non destructive imaging technique that produces 3 dimensional images from a set usually more than 1500 2d x ray attenuation images taken at different angles the sample was imaged at the id19 beamline of the european synchrotron radiation facility esrf grenoble france the 3 dimensional reconstruction process that was performed using the single distance phase retrieval algorithm described by paganin et al 2002 applying an unsharp filter before reconstruction the final 3 dimensional image is formed of cubic voxels associated with a value quanitifying the x ray attenuation integrated over the voxel volume this value ranges from that corresponding to the x ray attenuation of air for voxels belonging to macropores to that corresponding to the x ray attenuation of the solid rock i e where porosity is zero the voxel size denotes the optical resolution of the image the total x ray energy attenuation depends on the attenuation of the fraction of the rock forming minerals and of the porosity for a single component solid phase such as the sample considered here that is made of calcite the x ray energy attenuation integrated over each voxel of the final 3 dimensional image denotes the porosity the mc10 sample was previously investigated by smal et al 2018 for illustrating the application of a new segmentation algorithm allowing mapping the unresolved porosity i e the fraction of the pore space containing pores that are smaller than the imaging resolution while connected macropores allow solute transport by advection and diffusion solute transport in the microporous material is assumed to be controlled by diffusion accordingly connected macro porosity delimits the mobile domain while the microporous material delimits the immobile domain applying the methodology proposed by smal et al 2018 one obtains a segmented image formed of a connected mobile domain an immobile domain which in our case is almost completely surrounding the mobile domain and patches of solid material where no solute transport occurs see fig 1 note that the immobile domain contains isolated macropores voxels of porosity equal to 1 corresponding to the non connected macro porosity the 3 dimensional image used in this paper to illustrate mobile immobile solute transport and its upscaling is a cubic sub volume of side size 900 900 900 cubic voxels the voxel size is 1 6867 10 6 m the average pore size is evaluated from the distribution of the chord length distribution function torquato and lu 1993 as 70 10 6 m accordingly the mobile domain is formed by around 104 pores this 900 900 900 image is obtained from a volume of 300 300 300 voxels cropped from the segmented image of the cylindrical rock sample then having each voxel divided by 3 in each of the directions 1 voxel is meshed in 27 voxels this procedure allows obtaining a sufficient resolution for an accurate calculation of the stokes flow appendix b immobile domain properties different tortuosity models are investigated assuming a porosity threshold of ζ 0 1 and tortuosity defined by κ ϕ ϕ 1 m with m 1 5 2 5 and 4 5 as well as with a constant tortuosity model κ 1 8 and ζ 0 table b 1 displays the average effective diffusion de and a geometric evaluation of the diffusion characteristic time t d ℓ i m 2 2 d e for these different models with ℓ im approximated by the ratio of the immobile volume to the mobile immobile interface area the immobile domain transport properties by diffusion can also be characterized by the memory function φ t that denotes the probability that a particle entering the immobile zone at t 0 remains there until time t for a given digitized rock sample segmented into mobile and immobile domains φ t characterizes the geometry and the volume fraction of the immobile domain and the topology of the mobile immobile interface gouze et al 2008 φ t is derived from the trapping time pdf ψ ˇ τ i m with the relation φ t 1 0 t ψ ˇ τ i m t d t as shown in fig b 13 the assumptions made on the formulation of the tortuosity and its parameterization equations 1 3 are noticeably modifying φ t specifically one can see that increasing the exponent m of the tortuosity model equation 2 and implementing a porosity threshold ζ 0 changes the shape and the average slope of the memory function and therefore should have a critical effect on the overall hydrodynamic transport in the sample the characteristic diffusion time in the immobile domain tc is given by the mean trapping time ψ ˇ τ i m table b 1 displays the values of tc for these different models the model with constant tortuosity κ x 1 8 and the model with κ x ϕ x 1 m with m 1 5 display similar values of tc conversely increasing the exponent m increases noticeably the characteristic diffusion time i e lengthen the effective diffusion path length whereas applying a threshold ζ 0 1 reduces the immobile domain extension and consequently the value of tc the ratio td tc is given in table b 1 this ratio is a qualitative indication of the effective intricacy and heterogeneity of the diffusion paths characterized by the mean diffusion time tc compared to the mean diffusion time expected from a simple geometry of the immobile domain of similar volume and mobile immobile interface area when all the porosity of the immobile domain is considered ζ 0 0 the ratio td tc is ranging from 22 to 37 emphasizing the strong complexity of the diffusion path that can be guessed from the cross section presented in fig 1 when the zones of the smallest porosity values are not participating to diffusion ζ 0 1 the ratio td tc is much smaller t d t c 5 8 0 5 and apparently independent of the tortuosity model suggesting that applying the porosity threshold acts as decreasing the effective intricacy of the diffusion paths appendix c vertical averaging of the mobile immobile model of a single fracture embedded in an homogeneous matrix flux continuity at the fracture matrix interface implies c 1 ϕ m d m c m x t y y 0 ϕ i m d i m c i m y t y y 0 such that c 2 ϕ m c m x t t q c m x t x ϕ m d m 2 c m x t x 2 1 d m ϕ i m d i m c i m y t y y 0 the flux over the interface is obtained by integration of 13 over y which gives c 3 ϕ m d m c m y t y y 0 ϕ i m d i m c i m x t t the matrix concentration is obtained from 13 in laplace space we obtain c 4 c i m y λ cosh y d i m λ d i m cosh λ τ d c m x λ where we defined τ d d i m 2 d i m the average matrix concentration is then obtained by integration over y which gives c 5 c i m y λ φ λ c m x λ where the memory function is defined by 16 the memory function at λ 0 is φ 0 1 which means c 6 0 d t φ t 1 appendix d upscaled lagrangian model the particle density in the lagrangian model is obtained from ctrw theory berkowitz et al 2006 russian et al 2016 as d 1 c i t 0 t d t r i t 0 t t d t ψ t d 2 r i t δ i 0 0 t d t ψ t t w u r i 1 t w d r i 1 t these equations are combined into a single generalized master equation in laplace space d 3 λ c i λ δ i 0 λ ψ λ 1 ψ λ w u c i 1 λ w d c i 1 λ c i λ inserting 23 into d 3 and using the definition 18 of the transition probabilities we obtain d 4 λ c i λ δ i 0 d m c i 1 λ c i 1 λ 2 c i 1 λ ℓ 2 v c i 1 λ c i λ ℓ 1 λ 1 γ 1 ψ i m λ in the limit ℓ 0 we obtain d 5 λ c x λ δ x d m 2 c x λ x 2 v c x λ x 1 λ 1 γ 1 ψ i m λ the concentration c x λ denotes the sum of the fracture and matrix concentrations c x λ c m x λ c i m x λ we identify d 6 c i m x λ γ ϑ λ c m x λ where the memory kernel ϑ t is defined through its laplace transform as d 7 ϑ λ 1 λ 1 ψ i m λ using d 6 and d 7 in 10 we obtain for c m x λ d 8 λ c m x λ v c m x λ x d m 2 c m x λ x 2 δ x λ c i m x λ finally inverse laplace transform of d 8 gives 24 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103781 appendix e supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
383,we study the upscaling of pore scale transport of passive solute in a carbonate rock sample it is characterized by microporous regions displaying heterogeneous porosity distribution that are accessible due to diffusion only and a strongly heterogeneous mobile pore space characterized by a broad distribution of flow velocities we observe breakthrough curves that are characterized by strong tailing which can be attributed to velocity variability in the flowing medium portion and solute retention in the microporous space using accurate numerical flow and transport simulations we separate these two mechanisms by analyzing the statistics of residence times in the mobile phase and the trapping and residence time statistics in the mmobile phase we employ a continuous time random walk framework in order to upscale transport using a particle based implementation of mobile immobile mass transfer and heterogeneous advection this approach is based on the statistics of the characteristic mobile and immobile residence times and mass transfer rates between the two continua while classical mobile immobile approaches model mass transfer as a constant rate process we find that the trapping rate increases with increasing mobile residence times until it reaches a constant asymptotic value based on these findings and the statistical characteristics of travel and retention times we derive an upscaled lagrangian transport model that separates the processes of heterogeneous advection and diffusion in the immobile microporous space and provides accurate descriptions of the observed non fickian breakthrough curves these results shed light on transport upscaling in highly complex dual porosity rocks for which mobile immobile mass transfer are controlled by a dual multirate process controlled by the heterogeneity of both the flow field in the connected porosity and the diffusion in the no flow regions keywords non fickian dispersion heterogeneous porous media upscaling time domain random walk continuous time random walk dual multirate mass transfer model 1 introduction solute transport in the laminar flow through the void space of a porous medium is due to molecular diffusion and advection despite the simplicity of these fundamental processes observed transport is characterized by complex features such as strong breakthrough curve tailing non gaussian concentration distributions anomalous dispersion incomplete mixing and intermittent lagrangian flow properties cortis and berkowitz 2004 seymour et al 2004 bijeljic et al 2011 de anna et al 2013 bijeljic et al 2013 kang et al 2014 holzner et al 2015 morales et al 2017 these behaviors are due to the intricate structure of the pore space and the multi scale heterogeneity distribution which cause broad distributions of advective and diffusive mass transfer time scales and transport pathways bijeljic et al 2011 porta et al 2015 puyguiraud et al 2019a the understanding of these heterogeneity mechanisms and their quantification in upscaled transport models are key issues in many academic and engineering applications concerned with the large scale macroscopic prediction of the fate of conservative and reactive solutes in geological and engineered media such as the assessment of groundwater contamination and remediation geological storage of nuclear waste geothermal energy production and underground storage of carbon dioxide domenico and schwartz 1997 poinssot and geckeis 2012 niemi et al 2017 classical upscaling approaches quantify darcy scale transport in terms of hydrodynamic dispersion coefficients bear 1972 which incorporate the large scale dispersive effect of pore scale velocity fluctuations the key issue is evidently the determination of the macroscopic dispersion coefficient for instance de josselin de jong 1958 and saffman 1959 used lagrangian stochastic models for pore scale particle motion in order to derive expressions for the hydrodynamic dispersion coefficients their approaches are based on the fact that velocities vary on typical length scales the pore lengths thus particles spend more time in low than in high flow velocity regions this behavior which lies at the origin of pore scale lagrangian intermittency dentz et al 2016 is modeled by a distribution of travel times the spatial transitions and the transition times depend on both the pore velocities and molecular diffusion since these pioneering works hydrodynamic dispersion and its dependence on the local péclet number the ratio of the characteristic diffusion time to the advection time was the subject of numerous experimental numerical and theoretical investigations scheven 2013 swanson et al 2015 pfannkuch 1963 rashidi et al 1996 jourak et al 2013 bijeljic and blunt 2006 systematic upscaling approaches have been based on generalized taylor dispersion theory brenner 1980 salles et al 1993 volume averaging quintard and whitaker 1994 davit et al 2012 2013 and continuous time random walk ctrw bijeljic and blunt 2006 ctrw methods similar to the approaches involved in the works of de josselin de jong 1958 and saffman 1959 were used to model non fickian pore scale transport features such as anomalous dispersion breakthrough curves tailing and intermittent lagrangian particle velocities bijeljic et al 2011 de anna et al 2013 gjetvaj et al 2015 kang et al 2014 puyguiraud et al 2019b the implementation of ctrw is often handled by modeling particle transport through transitions over fixed spatial scales characterized by random transition times berkowitz et al 2006 the time domain random walk tdrw method that will be used in this study is also based on particles motion over fixed distance transition times are determined kinematically from the eulerian flow field and the spatially distributed properties for instance the porosity and diffusivity that can be mapped either from tomographic imaging or from statistical models thus these methods provide a means to relate pore scale flow properties to darcy scale transport behavior bijeljic et al 2011 puyguiraud et al 2019 2020 the presence of immobile medium regions that consist of dead end pores regions of low flow in the wake of solid grains and microporosity where diffusion dominates are common features of porous media such as reservoir rocks the large difference in terms of péclet number between these zones of the porous media and the connected network of pores that forms the usual macroporosity the flowing porosity supports the use of dual continuum mobile immobile mass transfer approaches first proposed by van genuchten and wierenga 1976 this approach has been widely used to take into account the often encountered control of spatially distributed diffusive zones on the overall hydrodynamic transport and specifically on the occurrence of often highly non fickian breakthrough curves observed experimentally from laboratory to field scales the heterogeneous medium is modeled by overlapping mobile and immobile continua at each point in space the system state is defined by a mobile and series of immobile concentrations the mobile and immobile continua communicate through linear mass transfer in two equation models ahmadi et al 1998 cherblanc et al 2007 which can be formulated in a way that allows the immobile concentrations to be written as linear functionals of the mobile concentration which are characterized by a memory kernel haggerty and gorelick 1995 carrera et al 1998 that accounts for the microscale mass transfer processes as such it is a method to upscale pore scale transport many implementations of this approach consider a constant average velocity in the mobile medium portion liu and kitanidis 2012 porta et al 2013 2015 however advective heterogeneity this means velocity variability in the flowing medium portion by itself gives rise to anomalous transport bijeljic et al 2011 de anna et al 2013 kang et al 2014 puyguiraud et al 2019 this is why the importance of pore scale velocity statistics and their relation to the complex medium and heterogeneity structure have been studied in a series of recent experimental and numerical works siena et al 2014 matyka et al 2016 holzner et al 2015 de anna et al 2017 alim et al 2017 dentz et al 2018 aramideh et al 2018 some authors have coupled ctrw models of advective heterogeneity with trapping in immobile regions gjetvaj et al 2015 dentz et al 2018 key items for these modeling approaches are the identification of the dominant pore scale transport and mass transfer processes their relation to the pore scale medium and the flow properties these aims have been pursued by experiments swanson et al 2015 numerical simulations de vries et al 2017 ceriotti et al 2019 and formal upscaling using volume averaging davit et al 2012 orgogozo et al 2013 porta et al 2015 and lagrangian ctrw based methods gjetvaj et al 2015 dentz et al 2018 volume averaging delineates a mobile region the flowing porosity and immobile regions such as biofilms orgogozo et al 2013 based on a velocity cutoff determined from a péclet criterion porta et al 2015 the large scale transport model is then obtained by averaging the microscale transport equations over a unit cell that is statistically representative of the properties of the medium and the flow and contains two distinct medium portions which as outlined above are connected through mass transfer across domain boundaries lagrangian stochastic models margolin et al 2003 benson and meerschaert 2009 dentz et al 2012 comolli et al 2016 formulate mass transfer between mobile and immobile medium regions through compound stochastic poisson processes feller 1968 this means that mass transfer events occur at constant rate quantified by the poisson process which renders the residence time in immobile regions as the sum over individual trapping times a compound poisson process as shown by margolin et al 2003 benson and meerschaert 2009 and discussed further in this paper this formulation is equivalent to eulerian mobile immobile mass transfer formulations this study aims at testing our capability of characterizing and upscaling hydrodynamic transport in heterogeneous natural reservoirs where both velocity distribution and immobile domain heterogeneity cause anomalous transport starting from the model assuming that mobile immobile mass transfers are controlled by a poisson process for that we use as an example of highly heterogeneous media a carbonate sample imaged using x ray microtomography that displays marked bimodal structural heterogeneity caused by the presence of connected macro porosity and microporous material that results from grain sedimentation and diagenesis events the image is processed in order to map the mobile and the immobile domain and direct numerical simulations of flow and transport are performed we investigate in detail the statistics of mobile and immobile particle motion in terms of the respective residence times the trapping rates and the mobile and immobile times between trapping events using the detailed statistical analysis we discuss the salient features of transport at the pore scale and quantify them in an upscaled transport model based on a lagrangian formulation that implements in a simple form the specific process that characterizes the spatial distribution of the mobile immobile mass transfers in heterogeneous media the paper is organized as follows section 2 details the methodology it describes the numerical solution of the direct flow and transport problem the simulation setup the boundary conditions and the model outputs which allow investigating in detail the statistics of mobile and immobile particle motion in terms of the respective residence times the trapping rates and the mobile and immobile times between trapping events in section 3 we consider transport in a simple fracture matrix setup in order to present the concept of the basic lagrangian methodology for a simple mobile immobile system for which the single trapping rate upscaling ctrw formulation is detailed and then validated using the direct simulation results then following the same approaches we investigate in section 4 the transport behavior in the carbonate rock sample using direct numerical simulations as well as the upscaling of transport in the immobile domain using a statistical multi trapping approach then in section 5 we derive a new upscaled lagrangian model and validate it by comparison with the results of the direct numerical simulations conclusions are presented in section 6 2 material and methods 2 1 sample mc10 properties the mc10 carbonate sample is a porous and permeable rock made of quasi pure calcite the structure that results from complex sedimentation and diagenetic events is made of impervious grains of variable characteristic length ranging from few tens to about 150 µm and microporous material that ensure the cohesion of the rock when performing x ray microtomography of such single solid phase material the x ray energy attenuation integrated over each voxel of the final 3 dimensional image denotes the porosity for this study we use a cropped sub volume made of 900 900 900 cubic voxels of side dimension d x 1 6867 10 6 m mc10 is characterized by connected macropores of mean size 70 10 6 m the microporous material diagenetic cement is considered immobile regarding fluid flow and only accessible to solute by diffusion it displays variable porosity made by pores that are smaller than the imaging resolution accordingly connected macro porosity delimits the mobile domain while the microporous material delimits the immobile domain details on the sample characteristics and the methodologies applied to process the x ray tomographic image are given in appendix a the effective diffusion de in each location of the immobile domain i e in each voxel is the product of the molecular diffusion d 0 times the effective porosity ϕe 1 d e x d 0 ϕ e x d 0 ϕ x κ x where κ denotes the immobile domain tortuosity that can be considered as a constant or a function of the immobile domain porosity ϕ such as the formulation derived from the electric tortuosity by archie 1942 κ x ϕ x 1 m accordingly 1 can be rewritten 2 d e x d 0 ϕ x m with m ranging from 1 assuming that there is no tortuosity effect to about 4 5 in microporous limestones gouze et al 2008 the diffusion coefficient d 0 is constant for all simulations and set to 10 9 m2s 1 note that the porosity of the immobile domain is defined as the porosity accessible by a solute diffusing from the mobile domain and thus can be different from the total porosity of the immobile domain for instance if porous zones are embedded in zones considered as non diffusive as explained below for each voxel of the immobile domain transport by diffusion is impossible below a given porosity value different approaches using for instance percolation theory critical path analysis or effective medium approximation theory can be used to evaluate the porosity threshold ζ below which the system is non percolating for diffusion see hunt and sahimi 2017 hommel et al 2018 and references herein for the sample considered here applying a porosity threshold consists in transforming the fraction of the immobile domain where ϕ ζ into solid 3 d e x d 0 ϕ e for ϕ ζ 0 for ϕ ζ the porosity value of the immobile domain resulting from the image processing ranges from 0 045 to 0 193 with mean porosity 0 108 hebert et al 2015 fig 1 displays a cross section normal to the main flow in the segmented image of the 9003 voxel sample where the fraction of immobile domain corresponding to porosity below threshold values of 10 are enlighten for instance applying a porosity threshold of ζ 0 1 acts as removing 39 of the immobile domain the mean porosity of the remaining fraction of the immobile domain is then 0 175 however applying this threshold does not change noticeably the area of the mobile immobile interface which is 1 22 105 m2 per m3 of mobile domain when ζ 0 0 and 1 20 105 m 1 when ζ 0 1 i e a decrease of 1 64 in this paper different tortuosity models are investigated assuming a porosity threshold of ζ 0 1 and tortuosity defined by κ ϕ ϕ 1 m with m 2 5 is the most realistic model garing et al 2014 but models with m 1 5 and 4 5 as well as with a constant tortuosity model κ 1 8 and ζ 0 are investigated in order to explore the feedback control of the immobile domain diffusivity on the overall solute transport and on the upscaling feasibility a comprehensive characterization of the diffusion properties according to the assumption made on tortuosity are given in appendix b 2 2 mobile domain flow we consider the flow in the sample at low reynolds so that the pore scale flow velocity v x is solution of the stokes equation 4 2 v x 1 μ p x where p x is the fluid pressure the 9003 cubic voxels mesh is directly used as the meshed domain for openfoam calculations using a permeameter like configuration i constant pressure is applied at the inlet z 0 and the outlet z l z boundaries where 20 pixels layers of unitary porosity are added in order to obtain an accurate determination of the velocity components at the inlet and outlet of the domain ii the domain is bounded by solid at x 0 x l x y 0 and y l y iii no slip conditions are applied at the mobile solid and the immobile solid domain boundaries lx ly and lz denote the domain lengths in the x y and z directions respectively the flow equations are solved via a finite volume scheme implemented in the simple algorithm of openfoam https cfd direct openfoam user guide v7 fvsolution this algorithm solves the steady state stokes equation 4 and continuity equation v x 0 following an iterative procedure convergence is reached when the difference in terms of pressure and velocity components between the current and the previous steps is smaller than a threshold once convergence has been reached we extract the velocity field components that are computed at each of the voxel interface the fluid velocity in the direction along the z axis the main flow direction displays an asymmetric shape with some negative values that emphasize the high complexity of the flow field triggered by the high heterogeneity of the mobile domain the fluid velocity perpendicular to the main flow direction is quasi symmetric fig 2 the flux weighted velocity norm pdf p e v v v p e v where pe v denotes the pdf of the eulerian velocity norm is displayed in fig 2 puyguiraud et al 2019 showed that for stationary system p e is equal to the lagrangian velocity pdf which is the core information required for upscaling advective transport in the mobile domain puyguiraud et al 2019 upscaling of the advective transport using p e for this highly heterogeneous sample is beyond the scope of the present work that focuses on upscaling the immobile domain transport and will be presented in a future dedicated paper nevertheless we note that the pdf p e presented in fig 2 is quite similar to that of the sandstone sample presented in fig 2 in puyguiraud et al 2019 for which upscaling methods were proposed by the authors 2 3 transport simulations transport in the mobile immobile domain is described by the generic advection diffusion equation which is considered to apply at the scale of each voxel 5 c x t t v x c x t d e 2 c x t 0 where de is the effective diffusion coefficient and v x is the flow velocity in the mobile domain de reduces to the molecular diffusion coefficient d 0 whereas the flow velocity is zero in the immobile domain equation 5 is solved numerically using a time domain random walk tdrw method russian et al 2016 which is based on the formulation of eq 5 as a master equation using a finite volume discretization of the spatial operators a complete description of the tdrw method a demonstration of its equivalence with eq 5 and its implementation using voxelized images of porous media can be found in dentz et al 2012 and russian et al 2016 the main features of the method are given below the domain discretization used for transport is the same as the one used for computing the flow the tdrw approach models the displacement of particles in space and time their ensemble average giving the solution of the transport equation for the considered media for each particle each motion event is denoted by a single jump from one voxel to one of the 6 face neighboring voxels as such the jump distance ξ is constant and equal to the voxel size dx the direction and the jump duration are controlled by the local properties of the voxels i e the fluid velocity and the effective diffusion coefficient the recursive relations that describe the random walk from position x j to position x i of a given particle at jump n is 6 x i n 1 x j n ξ t n 1 t n τ j with ξ ξ denoting the transition length the probability wij for a transition of length ξ from pixel j to pixel i and the transition time τj associated to pixel j are given by 7 w i j b i j j k b k j τ j 1 j k b k j where the notation jk indicates the summation over the nearest neighbors of pixel j the bij are given by 8 b i j d e i j ξ 2 v i j 2 ξ v i j v i j 1 where d e i j denotes the harmonic mean of the diffusion coefficients of pixels i and j and vij denotes the velocity component of v j in the direction of pixel i v i j v j ξ i j as a convention voxel i is downstream from pixel j if vij 0 note that the tdrw can be seen as a continuous time random walk ctrw because it treats time as an exponentially distributed continuous random variable whose mean may vary between voxels in this paper we use the term tdrw for the numerical random walk method used to solve the direct problem and the term ctrw for the upscaled random walk framework 2 4 tdrw simulations setup the applied boundary condition at the sample inlet z 0 is a pulse of constant concentration in the mobile domain only this is performed by applying a flux weighted injection of the particles at t 0 by construction the pulse is formally an exponential concentration function of characteristic time τ j z d x 2 russian et al 2016 the main result is given by the first passage time at the outlet of the mobile domain which denotes the inert tracer breakthrough curve btc no flux boundary condition is set at x 0 x l x y 0 y l y as well as z 0 and z l z in the immobile domain simulations are performed for different values of péclet number which is defined by p e v l d 0 where l is a characteristic length which is taken here as the average pore length each simulation involves at least 107 particles the statistics concerning the characteristics of the trapping events such as the trapping time in the immobile domain and the survival time in the mobile domain between two trapping events are obtained by sampling more than 109 events the definition of a trapping event is provided in section 2 5 2 5 model output the mass transfers occurring in the sample are probed by a set of statistical distributions which are given as probability density functions pdfs denoted ψ ˇ where the overlying reversed hat symbol indicates that they are derived from the results of the direct tdrw simulations these pdfs describe the advection diffusion transport in the mobile domain the exchange between the mobile and the immobile domains and the diffusive transport in the immobile domain in terms of random walk process we will name each intrusion of a particle into the immobile domain a trapping event trapping time pdf denoted ψ ˇ τ i m is the pdf of the time τim p n p spent by the particles p p 1 p in the immobile domain during the trapping events n p n p 1 n p where p is the total number of particles exiting at the sample s outlet and n p is the total number of trapping events encountered by particle p immobile time pdf denoted ψ ˇ t i m is the pdf of the time tim p spent by the particles in the immobile domain to cross the entire domain i e from z 0 to z l z for a given particle p t i m n p τ i m d n survival time pdf denoted ψ ˇ τ s is the pdf of the times τs p n p spent by the particles p in the mobile domain between trapping events n 1 and n mobile time pdf denoted ψ ˇ t m is the pdf of the time tm p spent by the particles in the mobile domain to cross the entire domain for a given particle p t m n τ s d n ϵ where ϵ is the sum of the time spent to move from the inlet to the location of the first trapping event and of the time spent to move from the exit location of trapping event n p to the outlet trapping rate pdf denoted ψ ˇ γ is the pdf of γ p n p t m p first passage time pdf denoted ψ ˇ t t is the pdf of the first passage time tt p spent by the particles to cross the domain and is equivalent to the breakthrough curve btc by definition for each particle p 9 t t p t m p t i m p t m p i 1 n p τ i m p i 3 modeling transport in a single fracture with mobile immobile mass transfer in this section we investigate the case of the transport of a passive tracer in a simple mobile immobile domain system that can be adequately represented as a single linear fracture the mobile domain crossing a continuous porous matrix the immobile domain the different pdfs characterizing the transport process described in section 2 5 will be computed using direct tdrw simulations and will later be compared to those resulting from tdrw simulations performed for the mc10 carbonate sample furthermore we present a 1d ctrw model that upscales transport in the fracture matrix system and introduces the main features and concepts used for the upscaling of transport in the mc10 carbonate sample the detailed fracture matrix transport model can be formulated in the most general form as 10 ϕ y c x t t u y c x t x d y c x t 0 where ϕ y is porosity which is equal to ϕm within the fracture and ϕim in the matrix u y is the darcy velocity which is equal to u in the fracture and 0 in the matrix similarly d y is the diffusion coefficient which is equal to dmϕm in the fracture and equal to dimϕim in the matrix where dm and dim denote the diffusion coefficient in the mobile and the immobile domain respectively in the following we consider two equivalent upscaled transport approaches 3 1 upscaling by vertical averaging upscaled transport in this fracture matrix system can be described by a multirate mass transfer model haggerty and gorelick 1995 carrera et al 1998 in the following we briefly outline the steps that lead to such a description in order to highlight the underlying assumptions the upscaled multirate mass transfer description for the fracture matrix system is obtained by vertical averaging to this end we define the averages concentration over the fracture and matrix cross sections as 11 c m x t 1 d m 0 d m d y c x t c i m x t 1 d i m 0 d i m d y c x t where dm is the width of the fracture and dim of the matrix averaging 10 over the fracture cross section gives 12 ϕ m c m x t t u c m x t x d m ϕ m 2 c m x t x 2 1 d m ϕ m d m c m x t y y 0 while the equation for purely diffusive transport in the matrix domain is 13 ϕ i m c i m x t t d i m ϕ i m 2 c i m x t y 2 0 the boundary condition are c i m x y 0 t c m x y 0 t as an expression of concentration continuity we approximate c m x t c m x t which assumes fast equilibration over the fracture cross section using flux continuity across the fracture matrix interface we obtain in appendix c 14 c m x t t v c m x t x d m 2 c m x t x 2 β c i m x t t where we defined the capacity coefficient β d i m ϕ i m d m ϕ m and the pore velocity v u ϕ m the average matrix concentration can be expressed as a linear functional of the average fracture concentration appendix c 15 c i m x t 0 t d t φ t t c m x t the memory function is well known carrera et al 1998 and can be expressed in laplace space as 16 φ λ tanh λ τ d λ τ d where we define the characteristic diffusion time τ d d i m 2 d i m in the matrix combining 14 and 15 we obtain the integro partial differential equation 17 c m x t t v c m x t x d m 2 c m x t x 2 β t 0 t d t φ t t c m x t which is equivalent to the multirate mass transfer model of haggerty and gorelick 1995 carrera et al 1998 in the following we describe the formulation of this upscaled model in a lagrangian framework 3 2 upscaled lagrangian model the upscaled lagrangian approach models one dimensional advective diffusive transport along the fracture which is interrupted by trapping events that are poisson distributed this means that transitions from the fracture to the matrix occur at constant rate γ which can be quantified by the diffusion rate over the fracture cross section at each trapping event a particle is trapped for a random time distributed according to ψim t these are the principal ingredients of the upscaled transport model in the following we formulate this model in the tdrw framework one dimensional advective diffusion particle motion at constant velocity v and diffusion coefficient dm is described by particle transitions over the fixed distance ℓ by a random time tm the probability wu for upstream particle motion is 18 w u d m τ v ℓ 2 the probability for downstream motion is accordingly w d 1 w u the time τv is defined by 19 τ v ℓ v 1 2 p e p e v ℓ d m the transition time tm is exponentially distributed 20 ψ i m t τ v exp t τ v these rules represent mobile transport as a tdrw model for advection diffusion with constant velocity v and diffusion coefficient dm russian et al 2016 this motion is combined with the trapping rules outlined in the following during a transition of duration tm nt trapping events occur such that the total transition time is given by 21 t t t m i 1 n t τ i m the number nt of trapping events is distributed according to the poisson distribution 22 p n t γ t n t exp γ t n t with mean n t γ t thus the total transition time tt describes a compound poisson process its pdf ψ t can be expressed in laplace space as margolin et al 2003 dentz et al 2012 23 ψ λ 1 1 λ τ v γ τ v 1 ψ i m λ laplace transformed quantities are marked by an asterisk the laplace variable is denoted by λ in order to show the equivalence of this lagrangian formulation with the mrmt model 17 we derive in appendix d for the concentration cm x t in the fracture 24 c m x t t v c m x t x d m 2 c m x t x 2 c i m x t t the concentration cim x t in the matrix is given by 25 c i m x t γ 0 t d t ϑ t t c m x t where the memory kernel ϑ t is defined by 26 ϑ t t d t ψ i m t it denotes the probability that the trapping time is larger than t using 25 in 24 we obtain for cm x t the governing equation 27 c m x t t v c m x t x d m 2 c m x t x 2 γ t 0 t d t ϑ t t c m x t this equation and equation 17 are equivalent if 28 γ ϑ t β φ t we first recall that φ t is normalized to 1 which can be seen by taking the limit λ 0 in 16 while the integral over ϑ t is equal to τim the mean trapping time thus we obtain 29 γ τ i m β this equivalence identifies the trapping rate γ and trapping time distribution ψim t as the key quantities in the upscaled model both quantities can be accessed by random walk particle tracking simulations as outlined in section 2 4 in the following we use this general framework for the upscaling of transport in the mc10 carbonate sample 3 3 ctrw upscaled model versus tdrw model results we tested the ctrw model by comparing the results with direct tdrw simulations for the simplest idealized 2 dimensional representation of a single fracture system the computational domain is a porous medium the immobile domain of dimension lz 20000 ly 1001 pixels embedding a fracture the mobile domain of aperture 1 pixel located at y 500 so that the immobile domain depth on each side of the fracture is ℓ im 500 pixels the pixel size is denoted ξ as in section 2 3 the flow velocity v in the fracture is constant the inlet is located at z 0 where a pulse injection is applied see section 2 4 and the outlet is located at z 20000 where the pdf of the first passage time or breakthrough curve ψ ˇ t t is monitored with a fracture aperture ξ the problem is simply characterized by the péclet number p e v ξ d 0 we performed simulations for constant diffusivity in the immobile domain d e x y d and for random lognormal distribution with de x y taken as the spatial geometric mean of the pixel diffusion simulations are performed with ξ 10 5 m d 0 10 9 m2 s 1 d e 1 774 10 11 m2 s 1 and 10 12 d e x t 10 9 m2 s 1 for the lognormal distributed diffusion model in the matrix a main attribute of the compound poisson process described in section 3 2 is that the distribution of the survival time in the mobile domain τs is exponentially distributed 30 ψ τ s t γ exp γ t where γ 1 τ s fig 3 displays the survival time distribution ψ ˇ τ s computed from the tdrw which is well fitted by an exponential distribution of mean γ n tm 1 τs fig 4 shows the perfect agreement between the breakthrough curves or first passage time pdfs ψ ˇ t t resulting from the upscaled ctrw simulations and those obtained from the tdrw simulations for pe values ranging from 1 to 100 the results are similar for the homogeneous immobile domain and for the random lognormal distribution with the same geometric mean diffusion as expected nœtinger and estebenet 2000 russian et al 2016 4 tdrw modeling of transport in the carbonate sample this section concerns direct simulations performed with the tdrw model i e simulations of the 3 dimensional domain simulations are performed according to the algorithm and the boundary conditions described in sections 2 2 and 2 3 respectively the results presented in this section focus on 4 distinct models that characterize the immobile domain diffusivity distribution in terms of tortuosity κ and porosity threshold ζ see table b 1 the simplest model assumes constant tortuosity κ 1 8 and no porosity threshold while the three other assume porosity dependent tortuosity κ ϕ m with m 1 5 2 5 or 4 5 and a porosity threshold ζ 0 1 4 1 trapping properties of the mc10 sample here the trapping characteristics of the mc10 sample are analyzed and compared to the ctrw model discussed in section 3 we recall that this model is characterized by the following feature the conditional pdf pn n tm that measures the number of trapping events conditioned to the time spent in the mobile domain is a poisson distribution with a constant trapping rate γ this means that the time spent in the mobile domain between two trapping events or survival time τs is characterized by an exponential distribution the pdf ψ ˇ τ s computed for the mc10 sample and the one corresponding to equation 30 with the same average values τs are displayed in fig 5 while the pdf pn n tm computed for the mc10 sample and the one corresponding to equation 22 where the constant trapping rate γ γ are displayed in fig 6 the latter is obtained by computing the pdf of n t m from equation 22 for each range of tm the survival time pdf ψ ˇ τ s does not depend on the average fluid velocity in the sample i e does not depend on the pe value and is controlled by the transport properties at the mobile immobile domains interface and thus is controlled by the effective diffusion of the immobile domain in the vicinity of the mobile immobile interface that is to say by the local porosity fig 5 shows that the ψ ˇ τ s pdfs are visually identical when applying a porosity threshold ζ 0 1 or not emphasizing that the value of the porosity threshold does not change noticeably the properties of the immobile domain at the mobile immobile interface nor its topology as it is shown also in appendix b as expected ψ ˇ τ s is strongly shifted toward larger time values when the immobile diffusivity at the mobile immobile interface decreases the important point is that ψ ˇ τ s curves are as a general rule not exponential distributions and display an over representation of the short survival times we see also larger maximum values compared to what is predicted by the exponential distribution but this feature decreases when m increases the conditional pdf pn n tm resulting from the mc10 simulations is compared to the one computed assuming a poisson distribution following eq 22 with a constant trapping rate γ in fig 6 the conditional pdf pn n tm resulting from the mc10 simulations is noticeably different from the one computed assuming a poisson distribution with a constant trapping rate for a given mobile time the theoretical poisson model predicts less trapping events than what is measured for the mc10 sample this discrepancy increases with the value of tm the trapping rate distribution encompasses the information about the trapping process which is controlled by the complex interactions between the mobile and immobile transport process as such one can expect that the trapping rate distribution is a macroscopic observable that characterizes the mobile immobile mass transfer and in a similar manner that the memory function is the macroscopic observable that enciphers the entire properties of immobile domain diffusive transport properties fig 7 displays the pdf of γ ψ ˇ γ and reports the percentage of the particles that do not encounter trapping when traveling from the inlet to the outlet for different properties of the immobile domain this percentage depends evidently on the value of the pe number but also on the properties of the immobile domain for instance it ranges from 1 8 to 95 6 for pe 100 depending on the value of m it follows that the average trapping rate γ cannot be inferred from 1 τs because the statistics of τs concern only particles that encounter trapping whereas a certain number of particles never visit the immobile domain yet interestingly the results presented in fig 7 show that all the pdfs ψ ˇ γ have almost the same average value γ 1 51 0 18 s 1 independently of the immobile domain properties which means that this value is an intrinsic property of the mc10 sample probably related to the geometry of the mobile domain and its interface with the immobile domain despite the distributions ψ ˇ γ being strongly dissimilar altogether these results suggest that the assumptions supporting the ctrw model of section 3 2 are not strictly met for the carbonate sample considered here taking into account these results the issue that will be investigated next is to evaluate to which extent the ctrw model is robust enough to model transport in heterogeneous media such as the mc10 sample or alternatively what additional relationship between the trapping rate properties and the mobile domain properties are required to derive a reliable upscaled model for complex systems such as the mc10 sample 4 2 upscaling the impact of diffusion in the immobile domain for each trapping event the particles that enter the immobile domain at a given location can exit at another location in the case of the single fracture model with homogeneous equivalent immobile domain the relocation distance along the linear continuous mobile immobile interface is a sharp distribution well described by its mean value 0 conversely the relocation of the particles in the mc10 sample is much less predictable due to the strong heterogeneity of the system in which the immobile domain is formed of heterogeneous clusters spatially distributed this is triggered principally by non continuous mobile immobile interfaces lacunar interface and the possibility of particles to utilize the immobile domain to take a shortcut from a given flow path to another conversely the 1 dimensional ctrw model imposes by construction that particles enter and exit the immobile domain at the same location for each trapping event simulating such a situation while keeping the complete 3 dimensional computation of the transport in the mobile domain is viewed as potentially instructive for understanding conjointly the effect of the particles relocation at the mobile immobile interface owing to the strong heterogeneity of the interface and the statistical representativeness of the trapping time pdf ψ ˇ τ i m for modeling the immobile domain transport properties at the scale of the sample to this end the tdrw solver is modified such that transport in the mobile domain and the trapping process are kept unchanged but the time spent in the immobile domain is drawn from the trapping time pdf ψ ˇ τ i m previously computed during the corresponding tdrw simulation involving the full direct simulation of the transport in the mobile and the immobile domain doing this imposes by construction that particles enter and exit the immobile domain at the same location for each trapping event similarly to the ctrw upscaled model from now on the model in which the trapping time pdf is used to model the time spent in the immobile domain at each trapping event is called the upscal tdrw model in contrast to the full tdrw model 4 2 1 control of the immobile domain diffusion properties over mobile immobile mass transfer fig 8 compiles the main information concerning the results in terms of first passage time tt mobile time tm immobile time tim and survival time τs for the full tdrw model and the upscal tdrw model these data are very valuable for understanding the control of the immobile domain properties on the way particles sample the system the capacity of the immobile domain to trigger shortcuts between zones of the mobile domain with different flow properties decreases when moving from the κ 1 8 ζ 0 model to the m 2 5 ζ 0 1 model and m 4 5 ζ 0 1 model because 1 applying a porosity threshold decreases the probability of having immobile domain clusters connected to many pores and 2 increasing the value of m acts as increasing the tortuosity i e the effective diffusion time in the immobile domain see table b 1 consequently comparing the results for the full model with the upscal model for which particles are forced to exit the immobile domain where they entered for each of the trapping events allows not only to understand the feedback effect of the particle relocation process at the mobile immobile interface on the overall transport that is quantified by the first passage time pdf but also to decompose the overall transport process in terms of the time spent in the mobile domain and the immobile domain for the κ 1 8 ζ 0 model the first passage time pdf ψ ˇ t t obtained for the full and the upscal model are noticeably dissimilar their respective shape being fully controlled by the immobile time distribution for intermediate and long times conversely the mobile time pdf are the same but different from the mobile time pdf computed assuming no immobile domain i e depending only on the mobile domain properties this indicates that the transport is strongly controlled by the broad spatial redistribution of the particles among mobile zones of distinctly different flow rates as a general rule one can conclude that the discrepancy between the upscal and the full model in terms of first passage time pdf ψ ˇ t t fig 8 originates from the fact that both the trapping rate γ and the trapping time in the immobile domain τim are different as a result of the distinct particle relocation processes when encountering trapping events from these observations one can speculate that for the κ 1 8 ζ 0 model the upscaling of such a system with a one dimensional model where particles sample the immobile domain according to the ensemble average statistics of the mobile displacement will fail even if one considers a non unique trapping rate that would be related to the mobile time the two other models of immobile domain m 2 5 and 4 5 share the same spatial geometry i e the same boundaries because they share the same porosity threshold ζ 0 1 but differ from the effective diffusion spatial distribution and mean increasing the value of m acts as decreasing 1 the mean distance of penetration of the particle into the immobile domain and 2 the relocation distance between the entrance and the exit of the particle in the immobile domain during each trapping event as such the model characterized by m 4 5 is the most similar to the simple fracture model presented in section 3 3 in terms of geometry indeed the results presented in fig 8 for m 4 5 ζ 0 1 show that the first passage time pdfs ψ ˇ t t are almost similar for the full and the upscal models while the mobile time pdfs of tm ψ ˇ t m overlap the pdfs of tm for the case where there is no immobile domain this means that the immobile domain heterogeneity does not control the advective transport in the mobile domain similarly to what occurs in the simple fracture model for the model where one sets m to the value of 2 5 which is the most realistic parameterization fig 8 tells us following the same argumentation as for the m 4 5 case that the immobile domain heterogeneity weakly controls the mobile domain transport 4 2 2 on the control of the mobile domain transport on the trapping rate fig 9 shows that the trapping rate γ is not constant but depends on the mobile time tm the function γ t depends on the properties of the immobile domain controlled by κ and ζ but also on the pe value which means that this function γ tm is not an intrinsic property of the system but depends on the flow rate yet we observed for instance for the immobile domain characterized by m 2 5 and ξ 0 1 that the trapping rate is actually constant for value of tm larger than 200 s materialized by the vertical dashed line in fig 9 the system behaves as a constant trapping rate for range of tm which increases as m increases fig 10 compares the theoretical conditional pdf pn n tm assuming a poisson distribution following eq 22 where the trapping rate is a function of tm using the values given in fig 9 with the conditional pdf pn n tm resulting from the mc10 simulations it can be seen that using the γ tm function reestablishes the consistency with the compound poisson process model compared to fig 6 this gives us the sound basis for implementing the ctrw approach presented in section 3 2 but implemented with the γ tm function for upscaling the transport in the mc10 sample 5 ctrw upscaling of mc10 as shown above the dependence of the trapping rate γ on the time spent by a particle in the mobile domain tm is a critical feature triggered by the heterogeneity of the mobile immobile domain interface accordingly the proposed model is based on the implementation of the mobile time dependence of the trapping rate in the ctrw upscaling model that was used to model the transport in the single fracture in section 3 2 one speculates that the number of trapping events n t m during a mobile transition of duration tm is poisson distributed in which the trapping rate is given by the γ tm function to test this assumption we build a simple upscaled model in which the transport processes is modeled by the unconditional downstream motion of particles with constant distance lz and a random transition time tm distributed according to ψ ˇ t m plotted in fig 8 the first passage time for each particle injected at t 0 is similar to equation 9 31 t t t m i 1 n t τ i m here the number of trapping events nt is a random variable distributed according to the poisson distribution 32 p n t m n t n exp n t n with n t t m γ t m the trapping time τim is also a random variable distributed according to ψ ˇ τ i m the results of the upscaled ctrw model are first compared with those obtained with the upscal tdrw simulations in the left plot of fig 11 the upscal tdrw simulations integrate the same relocation process as in the upscaled ctrw model as such and because both the models share the same mobile time distribution ψ ˇ t m computed assuming no immobile domain the comparison of the ctrw with the tdrw upscal model is a sound validation of the approach used to model the trapping rate and the trapping time in the immobile domain independently of retro action of the immobile domain on the mobile domain time distribution the results given in fig 11 show that the ctrw is perfectly reproducing the btc computed with the tdrw upscal model for different properties of the immobile domain the comparison of the btcs computed with the ctrw model using the mobile time distribution ψ ˇ t m resulting from the full tdrw model with those computed with the tdrw are given in fig 11 right plot for different immobile domain parameters as well as for different values of pe in fig 12 as expected the ctrw model does not reproduce well the tdrw data for the case where m 1 5 and ζ 0 for which the immobile domain provides the largest opportunity for particle to shortcut the main flow streams clearly this specific process cannot be taken into account by the lagrangian ctrw model conversely we observe a good fit of the upscaled ctrw model results with those computed by the direct tdrw simulations for the reference case where κ x ϕ x 1 m with m 2 5 specifically the upscaled model reproduces perfectly the data for long times t 102 s for both ζ 0 and 0 1 while the fit at intermediate times is better for the case where the porosity threshold is applied i e ζ 0 1 the ability of the ctrw model to reproduce the direct simulations for times ranging over six orders of magnitude and for different values of the pe number is shown in fig 12 this figure also displays the pdfs ψ t t computed by the ctrw model assuming a single trapping rate value γ γ p thus allowing us to figure out the noticeable improvement of using the temporally evolving γ tm for upscaling the btcs at long times 6 conclusions with the objective of upscaling transport in heterogeneous media a lagrangian transport model that separates the processes of transport in mobile and immobile domains is presented along with its particle based ctrw implementation assuming an homogeneous immobile domain and a continuous mobile immobile interface mass transfer occurs as a compound poisson process with constant mobile immobile exchanges rate we show that this model is equivalent to the multirate mass transfer model of haggerty and gorelick 1995 with the mean trapping event number of the lagrangian model being equal to the capacity ratio of the multirate mass transfer model in other words the transport process in the lagrangian ctrw model is characterized by the mean trapping event number which also denotes the ratio of the solute mass in the immobile domain to that in the mobile domain at equilibrium we show that this 1 dimensional ctrw model perfectly reproduces the direct 2 dimensional tdrw simulations of the transport of solute in a linear fracture embedded into a porous matrix however this simple upscaled model is not able to reproduce accurately the trans port in the digitalized carbonate sample mc10 that is used to illustrate highly heterogeneous porous media for which we performed direct 3 dimensional tdrw simulations yet the direct simulations allow the thorough statistical analysis of the mass transfer dynamics within the two domains and of the mass exchanged at their interface which is spatially discontinuous and heterogeneous in terms of trapping rate we found that this heterogeneity of the mobile immobile interface together with the complexity of the flow in the mobile domain causes a deviation from the ctrw model presented in section 3 2 or equivalently a deviation from the mrmt model for instance the discrepancy between the computed survival time distribution for mc10 and the exponential distribution characterizing the single trapping rate model arises from the non uniqueness of the trapping rate γ is a function of tm fig 9 the survival time distribution displays a power law trend for short survival times which denotes the superposition of exponential distributions of τs tm with distinct average τs tm 1 γ tm conversely the increase of the occurrence of larger survival times compared to the exponential distribution denotes the lacunarity of the mobile immobile interface the distance and thus the time between two trapping events can be augmented due to the absence of available immobile domain in some parts of the mobile domain introducing this functional dependence of the trapping rate to the mobile time allows complying with a mobile time dependent compound poisson process in other words the mass transfers at the scale of the sample can be modeled as the ensemble average of residence time dependent mass transfers that can individually be modeled as single rate processes the comparison of the upscaled model against the direct 3 dimensional tdrw for different assumed properties of the immobile domain and different values of the péclet number permits to prove the efficiency of the model to reproduce the complex mass transfers in the two domains and at their interface as long as the spreading of solute due to the immobile domain does not reach a level where it produces a strong decorrelation of the velocity experienced by the particles in the mobile domain this situation occurs when immobile domain clusters allow short cut connections between zones of the mobile domain displaying distinctly different flow rates fortunately this situation is quite unlikely in reservoir rocks since the diffusivity in the immobile domain is generally decreasing from the mobile immobile interface ensuring together with the presence of non diffusing zones a certain insulation between adjacent flowing pore networks the final conclusion of this study is that the proposed upscaled lagrangian transport model provides an accurate description of the observed non fickian breakthrough curves in heterogeneous dual porosity media even when they are displayingbroad distributions of flow velocity values and highly heterogeneous immobile zones such as the carbonate example studied here this model is a dual multirate mass transfer model dmrmt in which the multiple rates of trapping arise from both the heterogeneity of the diffusion in immobile domain and the heterogeneity of the flow in the mobile domain acknowledgments pg and dr acknowledge funding from the cnrsiea through the project crosscale ex pics n 260280090 md and ap acknowledge funding from the spanish ministry of science and innovation through the project hydropore pid2019 106887gb c31 credit authorship contribution statement philippe gouze conceptualization software formal analysis investigation methodology writing original draft writing review editing alexandre puyguiraud formal analysis methodology writing review editing delphine roubinet formal analysis methodology writing review editing marco dentz conceptualization formal analysis methodology writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a mc10 sample description mc10 sample is a reefal carbonate sample from llucmajor platform majorca spain and is chosen as a typical example of complex porous media carbonate reservoirs are known to be heterogeneous over a wide range of scales as the result of the deposition and multiple diagenetic processes the sample is a cylinder of 9 mm diameter that was imaged using x ray computed microtomography which is a non destructive imaging technique that produces 3 dimensional images from a set usually more than 1500 2d x ray attenuation images taken at different angles the sample was imaged at the id19 beamline of the european synchrotron radiation facility esrf grenoble france the 3 dimensional reconstruction process that was performed using the single distance phase retrieval algorithm described by paganin et al 2002 applying an unsharp filter before reconstruction the final 3 dimensional image is formed of cubic voxels associated with a value quanitifying the x ray attenuation integrated over the voxel volume this value ranges from that corresponding to the x ray attenuation of air for voxels belonging to macropores to that corresponding to the x ray attenuation of the solid rock i e where porosity is zero the voxel size denotes the optical resolution of the image the total x ray energy attenuation depends on the attenuation of the fraction of the rock forming minerals and of the porosity for a single component solid phase such as the sample considered here that is made of calcite the x ray energy attenuation integrated over each voxel of the final 3 dimensional image denotes the porosity the mc10 sample was previously investigated by smal et al 2018 for illustrating the application of a new segmentation algorithm allowing mapping the unresolved porosity i e the fraction of the pore space containing pores that are smaller than the imaging resolution while connected macropores allow solute transport by advection and diffusion solute transport in the microporous material is assumed to be controlled by diffusion accordingly connected macro porosity delimits the mobile domain while the microporous material delimits the immobile domain applying the methodology proposed by smal et al 2018 one obtains a segmented image formed of a connected mobile domain an immobile domain which in our case is almost completely surrounding the mobile domain and patches of solid material where no solute transport occurs see fig 1 note that the immobile domain contains isolated macropores voxels of porosity equal to 1 corresponding to the non connected macro porosity the 3 dimensional image used in this paper to illustrate mobile immobile solute transport and its upscaling is a cubic sub volume of side size 900 900 900 cubic voxels the voxel size is 1 6867 10 6 m the average pore size is evaluated from the distribution of the chord length distribution function torquato and lu 1993 as 70 10 6 m accordingly the mobile domain is formed by around 104 pores this 900 900 900 image is obtained from a volume of 300 300 300 voxels cropped from the segmented image of the cylindrical rock sample then having each voxel divided by 3 in each of the directions 1 voxel is meshed in 27 voxels this procedure allows obtaining a sufficient resolution for an accurate calculation of the stokes flow appendix b immobile domain properties different tortuosity models are investigated assuming a porosity threshold of ζ 0 1 and tortuosity defined by κ ϕ ϕ 1 m with m 1 5 2 5 and 4 5 as well as with a constant tortuosity model κ 1 8 and ζ 0 table b 1 displays the average effective diffusion de and a geometric evaluation of the diffusion characteristic time t d ℓ i m 2 2 d e for these different models with ℓ im approximated by the ratio of the immobile volume to the mobile immobile interface area the immobile domain transport properties by diffusion can also be characterized by the memory function φ t that denotes the probability that a particle entering the immobile zone at t 0 remains there until time t for a given digitized rock sample segmented into mobile and immobile domains φ t characterizes the geometry and the volume fraction of the immobile domain and the topology of the mobile immobile interface gouze et al 2008 φ t is derived from the trapping time pdf ψ ˇ τ i m with the relation φ t 1 0 t ψ ˇ τ i m t d t as shown in fig b 13 the assumptions made on the formulation of the tortuosity and its parameterization equations 1 3 are noticeably modifying φ t specifically one can see that increasing the exponent m of the tortuosity model equation 2 and implementing a porosity threshold ζ 0 changes the shape and the average slope of the memory function and therefore should have a critical effect on the overall hydrodynamic transport in the sample the characteristic diffusion time in the immobile domain tc is given by the mean trapping time ψ ˇ τ i m table b 1 displays the values of tc for these different models the model with constant tortuosity κ x 1 8 and the model with κ x ϕ x 1 m with m 1 5 display similar values of tc conversely increasing the exponent m increases noticeably the characteristic diffusion time i e lengthen the effective diffusion path length whereas applying a threshold ζ 0 1 reduces the immobile domain extension and consequently the value of tc the ratio td tc is given in table b 1 this ratio is a qualitative indication of the effective intricacy and heterogeneity of the diffusion paths characterized by the mean diffusion time tc compared to the mean diffusion time expected from a simple geometry of the immobile domain of similar volume and mobile immobile interface area when all the porosity of the immobile domain is considered ζ 0 0 the ratio td tc is ranging from 22 to 37 emphasizing the strong complexity of the diffusion path that can be guessed from the cross section presented in fig 1 when the zones of the smallest porosity values are not participating to diffusion ζ 0 1 the ratio td tc is much smaller t d t c 5 8 0 5 and apparently independent of the tortuosity model suggesting that applying the porosity threshold acts as decreasing the effective intricacy of the diffusion paths appendix c vertical averaging of the mobile immobile model of a single fracture embedded in an homogeneous matrix flux continuity at the fracture matrix interface implies c 1 ϕ m d m c m x t y y 0 ϕ i m d i m c i m y t y y 0 such that c 2 ϕ m c m x t t q c m x t x ϕ m d m 2 c m x t x 2 1 d m ϕ i m d i m c i m y t y y 0 the flux over the interface is obtained by integration of 13 over y which gives c 3 ϕ m d m c m y t y y 0 ϕ i m d i m c i m x t t the matrix concentration is obtained from 13 in laplace space we obtain c 4 c i m y λ cosh y d i m λ d i m cosh λ τ d c m x λ where we defined τ d d i m 2 d i m the average matrix concentration is then obtained by integration over y which gives c 5 c i m y λ φ λ c m x λ where the memory function is defined by 16 the memory function at λ 0 is φ 0 1 which means c 6 0 d t φ t 1 appendix d upscaled lagrangian model the particle density in the lagrangian model is obtained from ctrw theory berkowitz et al 2006 russian et al 2016 as d 1 c i t 0 t d t r i t 0 t t d t ψ t d 2 r i t δ i 0 0 t d t ψ t t w u r i 1 t w d r i 1 t these equations are combined into a single generalized master equation in laplace space d 3 λ c i λ δ i 0 λ ψ λ 1 ψ λ w u c i 1 λ w d c i 1 λ c i λ inserting 23 into d 3 and using the definition 18 of the transition probabilities we obtain d 4 λ c i λ δ i 0 d m c i 1 λ c i 1 λ 2 c i 1 λ ℓ 2 v c i 1 λ c i λ ℓ 1 λ 1 γ 1 ψ i m λ in the limit ℓ 0 we obtain d 5 λ c x λ δ x d m 2 c x λ x 2 v c x λ x 1 λ 1 γ 1 ψ i m λ the concentration c x λ denotes the sum of the fracture and matrix concentrations c x λ c m x λ c i m x λ we identify d 6 c i m x λ γ ϑ λ c m x λ where the memory kernel ϑ t is defined through its laplace transform as d 7 ϑ λ 1 λ 1 ψ i m λ using d 6 and d 7 in 10 we obtain for c m x λ d 8 λ c m x λ v c m x λ x d m 2 c m x λ x 2 δ x λ c i m x λ finally inverse laplace transform of d 8 gives 24 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103781 appendix e supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
384,non structural mitigation measures to the globally increasing flood events include forecast based alert generation however the extreme rainfall forecasts are associated with low hit rate high false alarm and spatiotemporal bias which makes it difficult to rely on them further the losses due to flood in a region not only depend on rainfall severity but also on topography socioeconomic conditions and exposure of the region to floods here we introduce a new concept of spatial flood risk mapping and forecasting at weather to medium range based on forecasted hazard embedded with vulnerability topographic and socioeconomic and exposure information we define hazard as the probability of extreme rainfall event during upcoming days given an available weather forecast for the same days as hindcast is used for computation of probabilities hazard contains prior information about the false alarm hit rate and spatiotemporal bias of the forecast vulnerability is calculated by averaging the topographic and socioeconomic indicators and exposure is calculated using a land use land cover map topographic vulnerability is computed with digital elevation model using height above the nearest drainage method and data envelopment analysis is performed to derive the socioeconomic vulnerability based on the demographic census data for a specific region and a specific event the relative flood risk maps are generated at an administrative level e g district subdistrict or village level for india and the high risk areas can be identified from those maps for mitigation the methodology is demonstrated for a very recent extremely severe flood event that happened in kerala india in august 2018 it is evident from the results that the high risk areas forecasted well in advance as high lead time as 15 days match fairly well with the areas which suffered maximum losses because of direct flood graphical abstract image graphical abstract keywords extreme rainfall flood risk weather forecast hazard vulnerability exposure 1 introduction climate change has caused a considerable impact to the global water cycle which lead to changes in seasonal patterns as well as an increase in the frequency of extreme rainfall events oki and kanae 2006 intergovernmental panel on climate change ipcc 2012 reports that this increase in extreme rainfall is statistically significant in many parts of the globe it also states with medium confidence that these changes in extremes are attributed to anthropogenic influences in the indian sub continent as well both empirical methods and model projections have shown an increase in the frequency and magnitude of extreme rainfall events goswami et al 2006 preethi et al 2017 roxy et al 2017 a decrease in the total rainfall and intensification of extremes in the tropics for 21st century has been projected in the ipcc reports seneviratne et al 2012 irrespective of climate change the importance of forecast and understanding its uncertainty have a very high societal relevance increasing extremes in a changing climate further increases its importance extreme rainfall events not only affect our day to day lives but also result in floods and landslides which cause huge loss of lives and property dottori et al 2018 fowler et al 2010 every year a billion people are affected and thousands die because of these extreme events ifrc rcs 2011 according to the national disaster management authority ndma 2008 floods have become a cause of concern because of an increasing trend in flood related losses in india some examples of such extreme events include the heavy rainfall event in mumbai india on 26th july 2005 which recorded 944 mm rainfall in 24 h jenamani et al 2006 the rainfall and resulting flood caused the death of almost 1000 people and an economic loss of about us 100 million kumar et al 2008 uttarakhand in india received almost 340 mm rainfall in a day 375 above daily mean on 17th june 2013 which resulted in severe flash floods dube et al 2014 chennai city experienced a terrible flood during november december 2015 which caused at least 400 deaths economic loss of us 1120 million narasimhan et al 2016 seenirajan et al 2017 a significant amount of research has been conducted in the past decades to understand these events a large proportion of these events could not have been predicted accurately and hence resulted in devastation coumou and rahmstorf 2012 the commonly used alert generation and warning system for flood are mostly based on the rainfall forecast hence an accurate prediction of extreme rainfall at an administrative level is very important for the stakeholders and decision makers however the complex multiscale atmospheric processes responsible for the occurrence of any extreme rainfall event and their inherent variability makes them difficult to predict fritsch et al 1998 coarse resolution dynamical models often fail to predict these extreme rainfall events with accuracy as these have high false alarm low hit rate and spatio temporal biases březková et al 2010 khaladkar et al 2007 selvam 2011 shastri et al 2017 prediction skills can be improved using regional models at a high spatiotemporal resolution but these simulations are computationally very intensive thus are difficult to perform on real time dodla and ratna 2010 very heavy rainfall in a short time span often results in floods when it exceeds the ground absorption capacity and the runoff exceeds the capacity of river system neuendorf et al 2005 this makes mitigation planning difficult flood risk associated with these events are difficult to predict as it not only depends on the complexity of processes related to extreme rainfall but also on the interaction between these events and the geography population infrastructure the preparedness of the region balk et al 2012 flood risk can be defined as a product of hazard vulnerability and exposure ipcc 2012 kron 2005 karmakar et al 2010 which is used for climatological projections in the risk framework for climate applications hazard is defined in ipcc 2012 as the potential occurrence of a natural or human induced physical event that may cause loss of life injury or other health impacts as well as damage and loss to property infrastructure livelihoods service provision and environmental resources calculating flood risk also requires vulnerability which works as a proxy of human environment relationship turner et al 2003 ipcc 2012 defines vulnerability as the propensity or predisposition to be adversely affected in order to get the overall vulnerability of a region various classes of vulnerability are combined together karmakar et al 2010 the exposure ipcc 2012 component of risk is defined as the presence location of people livelihoods environmental services and resources infrastructure or economic social or cultural assets in places that could be adversely affected by physical events and which thereby are subject to potential future harm loss or damage in the changing climate along with an increase in the amount and frequency of extreme rainfall events the exposure of humans to flood is also increasing hirabayashi et al 2013 thus it is considered for flood risk quantification however this approach of defining climatological flood risk cannot be used for event specific flood mitigation here we propose a new methodology to use this concept of flood risk at weather to medium range scale to generate event specific risk maps traditionally the concept of hazard is used at a climate scale where it is defined as the probability of extreme rainfall above a specific threshold and computed from the long term data here we have introduced a concept of hazard which is computed at weather to medium range in the forecast system hazard is computed as the probability of occurrence of an extreme rainfall in any grid given a forecast value flood risk maps are generated by combining hazard with socioeconomic and topographic vulnerability and exposure of a region to demonstrate and evaluate this methodology it is applied to the flood event in kerala india which occurred during august 2018 the following section discusses the case study and the data used the limitations of the state of art weather forecast system is discussed in the section 3 sections 4 and 5 contains the methodology and results respectively and the paper is summarised in the last section 2 case study and data 2 1 case study description kerala flood of august 2018 kerala is a southern coastal state of india spread over an area of 38 863 km2 and is divided into 14 districts and 63 subdistricts fig 1 a apel et al 2009 it is one of the most densely populated indian states with a population of over 33 million 860 people per square kilometre and with a gross domestic product gdp of us 120 billion department of economics and statistics government of kerala http www ecostat kerala gov in index php economy many other human development indicators for kerala are at par with those of developed countries like literacy rate 93 11 life expectancy 77 years and a sex ratio of 1084 women per 1000 men census 2011 it should also be noted that kerala is a coastal state extending like a thin strip along the west coast of india and the grids of state of the art weather forecasting model such as the global ensemble forecast system gefs reforecast version 2 are too coarse spatial resolution of 1 1 for the present one which is used in the study to obtain district and state specific information fig 1b between 1st june and 18th august 2018 the cumulative rainfall in kerala was 42 in excess of the normal average which caused the worst flood in august in the state since 1924 during this period maximum rainfall occurred on 15th august and the daily accumulated rainfall is shown in fig 2 a coastal parts of kerala received extremely heavy rainfall during the event from fig 2a it is evident that thrissur malappuram alappuzha pattanamtitta and kollam districts received the rainfall amount more than 120 mm day whereas in the other parts of kerala the intensity was more than 60 mm day 1259 out of 1664 villages spread across all 14 districts supplementary table s1 were affected kpdna 2018 nearly 341 landslides were reported from 10 districts with idukki being the worst hit district with 143 landslides the seven most affected districts were alappuzha ernakulam idukki kottayam pattanamtitta thrissur and wayanad malappuram and palakkad suffered moderate losses in this flood this affected 5 4 million people displaced 1 4 million people and took 433 lives 22nd may 29th august 2018 kpdna 2018 the district wise total crop transport aquaculture health and heritage losses in million usd as obtained from the kpdna 2018 reports are presented in fig 2b it is interesting to note that there is lack of consistency between the districts with high rainfall and the districts with high loss it may also be possible that the heavy rainfall that occurred at different places during the 1st half of august 2018 is responsible for such an inconsistency it is quite evident that very high loss fig 2b cannot be attributed to the rainfall intensity fig 2a only and there exists lot of other factors related to vulnerability and exposure here in this study we focus on the same for the forecasts of weather to medium range flood risk map according to the kpdna 2018 report though heavy rainfall was forecasted well in advance a lack of planning caused the overflow of reservoirs the late pre emptying of reservoirs is often making flood damage worse in india and elsewhere these problems could have been avoided by the help of a reliable forecast system and proper mitigation plan the method proposed here is applied to the kerala flood of august 2018 for demonstration 2 2 data the data obtained for different modules of the weather to medium range flood risk forecast models are mentioned in the following subsections 2 2 1 hazard the 6 hourly rainfall data at a spatial resolution of 1 1 for a period of 1985 to 2015 from global ensemble forecast system gefs reforecast version 2 is used as the reforecasted rainfall dataset hamill et al 2013 reforecast also called hindcast data are retrospective forecasts for the past using the same model configuration employed for operational forecasts the datasets are generated from an 11 member ensemble forecast every day from 00 utc for a period of december 1984 to present day and is available at a 3 hourly time step for first 8 days horizontal resolution is t254 50 km and 6 hourly time step for 8 16 days t190 70 km at 42 vertical levels the daily gridded rainfall data for the period 1985 to 2015 from the india meteorological department imd at a spatial resolution of 0 25 0 25 is used as the observational dataset pai et al 2015 this dataset is developed based on ground observations from 6995 stations across india using an inverse distance weighing scheme and is available for the period of 1901 2015 pai et al 2015 the data is re gridded to a resolution of 1 1 to match the resolution of forecast data fig 1b shows the grids considered in this study to generate the forecasted hazard for kerala during august 2018 flood event to compare the model performance in forecasting the extreme rainfall event tropical rainfall measurement mission trmm 3 hourly 0 25 0 25 rainfall product is used to generate daily accumulated rainfall for 15th august 2018 the day of maximum rainfall during the flood event we have used the trmm data as the gridded data from imd was not available for 2018 2 2 2 socioeconomic and topographic vulnerability to calculate the socioeconomic vulnerability the subdistrict level demographic and economic information is obtained from the census of india 2011 as per the availability of data from census of india a set of relevant indicators are chosen to appropriately represent the status of socio economy of the region vittal et al 2020 table 1 shows the list of indicators chosen along with corresponding justifications in support of their selection for the topographic information we have procured digital elevation model dem product farr et al 2007 from nasa version 3 0 shuttle radar topography mission srtm at global 1 arc s 30 m resolution which is further used in the height above the nearest drainage hand model to generate the topographic vulnerability 2 2 3 exposure here the land use land cover lulc data is used as a proxy to quantify the exposure component of risk as it directly affects the severity of flood for example but not limited to the land use pattern specifically over urbanized region leads to increasing the severity of flood due to increase in an impervious fraction which eventually increases the exposure of land use to flood over urban hotspots karmakar et al 2010 the decadal 100 m resolution lulc dataset for 2005 from ornl daac roy et al 2016 is used for this purpose which classifies the lulc into 19 classes as provided in supplementary table s2 3 limitations of state of the art weather forecasts the state of the art flood alert generation system uses the forecasted rainfall amount during extreme rainfall events however these forecasts are often associated with very low hit rate and high false alarm along with spatiotemporal biases here we consider the weather to medium range forecasts of extreme rainfall event in kerala on 15th august 2018 at multiple lead times since the meteorological forecasts have significant bias we apply bias correction by scaling maraun 2016 using the hindcast and observed data the bias corrected outputs are presented in fig 3 following the conventional approach bias correction is done for the entire available hindcast dataset but the result is presented only for a single storm even at lead day 1 the model completely fails to produce the magnitude of the extreme rainfall fig 2a over the state of kerala further to this at a lead time of 10 and 15 days the forecasts are not at all showing indications of extremes over kerala and at a lead time of 2 3 days it starts showing a bit of indication of moderate rainfall clearly these are not sufficient for an evacuation action or taking any other precautionary measures we also find that these forecasts are associated with very low hit rate and high false alarms here we define hit rate as the fraction of extremes predicted by the model successfully and false alarm as the fraction of time model predicts a non extreme event as an extreme hit rate and false alarm are calculated as 1 h i t r a t e a a c 2 f a l s e a l a r m b a b where a number of intersects between a and b b number of intersects between a and b c number of intersects between a and b a number of times model forecasts an extreme b number of times observation shows an extreme a number of time model forecasts a non extreme rainfall b number of time observation shows a non extreme rainfall the hit rate and false alarm associated with the forecasts are given in supplementary figure s1 the hit rate always remains around 10 at any lead time with a very slight improvement at a lead time of 1 day false alarm remains as high as 90 it is evident that they are not enough for decision making towards disaster mitigation 4 method here to address the above mentioned problems associated with the uncertain and biased forecasts we propose a novel approach of forecasting flood risk at weather to extended range scale this is in contrast to the conventional approach of flood forecasting which uses bias corrected weather forecasts conventionally the risk to extremes is defined as the product of hazard vulnerability and exposure as chen et al 2015 gusain et al 2020 ipcc 2012 karmakar et al 2010 kron 2005 sahani et al 2019 3 r i s k h a z a r d v u l n e r a b i l i t y e x p o s u r e the concept of risk has been widely used at a climate scale however we propose the same at a weather to medium range the difference in the concept of risks between weather to medium range and climate scales lies in the definition of hazards we define hazard as the probability of an extreme event above a defined threshold given the forecasts for the same day the flood risk at a weather to medium range incorporates weather to medium range forecasts which essentially makes the hazard component dynamic whereas the remaining components of flood risk vulnerability and exposure are considered to be static during extreme events based on the forecasted risk maps generated at a weather to medium range the high risk areas can be identified and targeted first hence a location specific evacuation and flood mitigation can be done well in advance to reduce the losses fig 4 shows a complete flowchart of the methodology used to generate weather to medium range event specific flood risk maps further these maps need to be generated at an administrative eg district subdistrict or village for india level depending upon the data availability for better efficiency in decision making in the present study flood risk is generated at subdistrict level based on the authoritarian decision process and availability of demographic data 4 1 hazard hazard is typically defined as the probability of an extreme event this concept is traditionally used at a climate scale gusain et al 2020 sajjad et al 2020 as for example a hazard associated with 95thpercentile of rainfall is 0 05 here we propose to define hazard at a weather to medium range and define it as the probability of getting an extreme rainfall in any grid given a forecast value this is a dependant of the forecasts and hence with the change in lead time and subsequent forecasts the hazard value at a location gets modified at a location for example for a grid if we define extreme rainfall event as the rainfall above a threshold of 99th percentile the hazard may be defined as 4 haza rd p o o 99 h f where o observed daily rainfall o 99 99th percentile of rainfall observations h forecasted hindcast rainfall with its value denoted as f in order to generate the conditional probability given in eq 4 we first obtain the joint probability of observed and hindcast from the same model which is being used for forecast to generate the joint probability copula is used which creates the multivariate distribution based on the individual marginal distributions dupuis 2007 ghosh 2010 copula does not need the marginal distributions to follow a specific and same distribution this makes copula advantageous over conventional multivariate parametric multivariate distributions like normal log normal the first step in applying the copula based approach is to obtain the marginal distribution functions of associated variables which are the observed rainfall o and hindcast rainfall h as both observational and hindcast rainfall data contain zero rainfall values mixed marginal distributions are used gamma distributions are fitted to the non zero values of rainfall to model the probability mass function of zero and non zero rainfall we apply bernouli trials the cdf of the variables x which stands for both o and h is given by 5 f x x p 1 p g x x x 0 p x 0 where p probability of getting zero rainfall n 1 n n total no of days n 1 no of zero rainfall days gx x cdf of nonzero rainfall obtained by fitting a gamma distribution after generating the marginal distributions archimedean copula is used to generate the bivariate distribution in this case o and h are the two variables for which copula is to be fitted by definition of copula a two dimensional distribution function is given by 6 f o h c f o o f h f where c copula fo and f h marginal distribution functions of o and h in the approach based on copula we consider two variables u fo o and v fh f to be the cdf of o and h respectively where u and v are uniformly distributed random variables with values u and v here a single parametric copula is used with parameter θ ghosh 2010 nelsen 1999 zhang and singh 2006 7 c θ u v 1 u v where is a convex decreasing function copula generator the parameter θ is generated using the relationship between kendall s coefficient of correlation τ and t ghosh 2010 karmakar and simonovic 2009 8 τ 1 4 0 1 t t d t where t u or v kendall s τ calculated using the following equation 9 τ n n 2 1 i j s i g n x i x j y i y j where s i g n 1 f o r x i x j y i y j 0 s i g n 0 f o r x i x j y i y j 0 i j 1 2 n three types of archimedean copulas are used namely gumbel frank and clayton the relationship between τ and θ and the mathematical equations for each of these copulas are given in supplementary table s3 the choice of copula to best fit the distributions is crucial to the above calculations favre et al 2004 the copula having minimum akaike information criterion aic and bayesian information criterion bic score is considered to be the best fitted copula for correct representation of extreme events tail dependence test is important for the selection of copula ghosh 2010 poulin et al 2007 as we are dealing with extreme rainfall events copula having the highest upper tail dependence coefficient is desirable here the upper tail dependence coefficient is computed using a nonparametric estimator cfg caperaa fougeres genest capéraà et al 2000 based on aic bic and upper tail dependence coefficient the best copula is selected the other important criteria is the selection of grids for analysing the forecasts ideally the forecasted and obserevd rainfall should belong to the same grid however there is a high possibility of spatial bias in the forecasts that generates rainfall to a neighbouring place of the area of interest to overcome this we perform three anlyses a considering only the grid of interest for both observations and the forecasts b considering maximum of the observed and forecasted rainfall over a 3 3 box of 9 grids with the grid of interest at the centre c considering maximum of the observed and forecasted rainfall over a 5 5 box of 25 grids with the grid of interst at the centre the hazard values thus generated using eq 4 are in gridded form which are converted into subdistrict level by area weighted average method 4 2 vulnerability the vulnerability component of flood risk is calculated as the average of socioeconomic and topographic vulnerabilities 4 2 1 socioeconomic vulnerability here a measure of a region s susceptibility to flood damage is referred as flood vulnerability which includes a portion of population susceptible to either emotional mental or physical damage in addition the seriousness of current situation and previous experiences with disastrous event may further influence the vulnerability karmakar et al 2010 during a disastrous event the socio economic vulnerability mainly focuses on response reaction and resistance for a population of a region along with the damage caused to the economic sector here the framework proposed by sherly et al 2015 and vittal et al 2020 is implemented to estimate the socio economic vulnerability over the study region the major steps followed which include judicious selection standardization and aggregation of indicators and subsequent ranking of sub districts have been shown in fig 5 a step by step exposition has been provided in the following paragraphs choice of indicator plays an important role in socioeconomic vulnerability quantification the indicators chosen should be relevant justifiable and a good representative of the social and economic condition of the concerned region vittal et al 2020 the set of indicators selected for our study along with their justification are given in table 1 identifying an indicator as positive sensitive or negative adaptive has a significant influence on the overall vulnerability and thus appropriate recognition of such indicators is crucial vittal et al 2020 sharma et al 2020 a positive negative indicator increases decreases the vulnerability and consequently affect the flood risk here the vulnerability indicators are standardized mainly to make the indicators dimensionless which will allow us to compare the different indicators over the study region the method of standardization for an indicator wu et al 2002 karmakar et al 2010 is provided in the equation below 10a v i s t d v i v m i n v m a x v m i n for negative sensitive indicators 10b v i s t d v m a x v i v m a x v m i n for positive adaptive indicators where v i s t d standardized vulnerability indicator of ith subdistrict vi vulnerability indicator of ith subdistrict vmin minimum vulnerability indicator in all the subdistricts vmax maximum vulnerability indicator in all the subdistricts these equations consider both maximum and minimum values in the expression and ensure that the vulnerability values are within a 0 1 interval wu et al 2002 and always non negative karmakar et al 2010 during standardization the indicators were adjusted for their sign which indicates whether each indicator contributes positively or negatively to overall vulnerability the inherent subjectivity involved in the selection of threshold values for different classes or ranges makes it difficult to categorize the vulnerability holand et al 2011 mitchem 2004 uitto 1998 to address this issue there have been different approaches adopted by the researchers ranging from simple averaging karmakar et al 2010 to more complex cluster analysis kok et al 2016 sietz et al 2011 although these approaches are equally efficient for an elegant representation these lack in reducing the subjectivity in selection of weights rygel et al 2006 contrarily data envelopment analysis dea huang et al 2011 sherly et al 2015 vittal et al 2020 wei et al 2004 does not require weight assignment thereby reducing the subjectivity and also introduces a new classification approach with minimal possibility of rank reversal huang et al 2011 wei et al 2004 in addition dea does not make any assumption on the form of the functions as it is a non parametric technique therefore the present study implements dea to aggregate the standardized indicators and subsequently rank each spatial unit known as decision making units dmus i e subdistricts in the present study to use the dea model efficiently the indicators should have very low correlation in order to decorrelate the highly correlated indicators and decrease the dimensionality of these indicators principal component analysis pca is performed principal components pcs explaining 75 of the variability are considered as inputs for the dea following the approach of sherly et al 2015 and vittal et al 2020 a dummy value 1 unity is assigned as output in the dea since it represents the state of the system prior to the occurrence of a hazard hence the socioeconomic vulnerability of each dmu subdistrict is obtained by subtracting the relative efficiency of that subdistrict from unity in our case 2 pcs are considered as input and the banker charnes cooper bcc model banker et al 1984 of dea is used to rank the dmus by calculating their relative efficiencies lower the efficiency lower is the rank and higher is the vulnerability 4 2 2 topographic vulnerability to quantify the flood risk during extreme rainfall events elevation of a region plays a very important role topographic vulnerability accounts for the elevation and hand method rahmati et al 2018 rennó et al 2008 nobre et al 2011 2016 is used to quantify this parameter the model normalises digital elevation model dem values by changing the elevation with respect to sea level into elevation with respect to nearest drainage the advantage of using hand values to calculate topographic vulnerability is that grids with different elevation values but same hand value are considered equally vulnerable a flow chart of the methodology followed to calculate topographic vulnerability at grid level is shown in fig 6 and the steps followed are described considering an example dem in supplementary figure s2 firstly the dem with sinks is used as input for the hand model a sink or depression is an area or a point which has an elevation lower than all its neighbouring area or point rieger 1998 marked in red circles in supplementary figure s2a as it does not have any drainage outlet flow network generation is not possible with dem having sinks depression breaching method martz and garbrecht 1999 is used to connect these sinks and a hydrologically coherent dem is generated supplementary figure s2c the breaching method connects two neighbouring sinks by lowering elevation of some points in the shortest path connecting them flow direction is computed from this hydrologically coherent dem using d8 method tarboron 1997 in this method flow from each grid is assigned to one of its eight neighbours having the steepest slope to generate a local drainage direction ldd the ldd generated using hydrologically coherent dem is called coherent ldd supplementary figure s2d next a flow accumulation for each grid is computed by the adding the number of grids draining into that grid to get an accumulated area map supplementary figure s2e in order to generate a drainage network map channel initiation is done by setting up a threshold of accumulated area set as 10 in the example in the map grids having an area above the threshold value are considered in the drainage network this threshold can be applied manually or automatically by using an accurate drainage network map called mapped stream network msn as input using the ldd with drainage network nearest drainage map is generated and each grid is associated with the grid that it drains into known as the drainage grid an example is shown in supplementary figure s2g where each drainage grid is given a colour and the associated grids are represented as a lighter shade of the same colour the relative height of each grid is calculated as the difference of height of that grid and its drainage grid to generate a hand map supplementary figure s2h to calculate topographic vulnerability for kerala an automated geographic information system gis tool of the hand model developed by rahmati et al al 2018 is used due to the unavailability of an accurate msn an appropriate threshold is to be selected manually by trial and error method as suggested by rahmati et al 2018 here 25 000 is found to an appropriate threshold as higher values fail to give any positive value to the grids in the low lying areas coastal areas and lower values result in a very high density drainage network using this drainage network nearest area map is generated and hand values calculated at a grid level 30 m these gridded hand values are converted to subdistrict level by weighted average method lower the hand value higher is the topographic vulnerability subdistrict level hand values are standardised between 0 and 1 and are subtracted from 1 to obtain the topographic vulnerability 4 3 exposure lulc of an area is used as indicator to quantify exposure of any area to flood lulc is a primary characteristic of any region and decides the soil permeability run off etc following the methodology used by karmakar et al 2010 each lulc category is assigned with a degree of importance di for example the built in area has more pavement and concrete surfaces which increases the run off and is more prone to losses during flood and thus is assigned a higher di value on the other hand grassland like open areas have very low di values since they allow infiltration and decrease run off so suffer lesser losses di values associated with each of the lulc type as described by karmakar et al 2010 are given in table 2 using the di values and the fraction of area they covered by any lulc type in a subdistrict exposure is calculated as 11 e i l 1 n d i l a i l a i where ei exposure of subdistrict i l lulc type dil degree of importance for lulc type l a i l area occupied by lulc type l in subdistrict i ai area of subdistrict i the subdistrict wise exposure values obtained are then standardised between 0 and 1 and exposure maps are generated 5 results 5 1 hazard in the proposed methodology hazard is calculated for different lead times using the rainfall forecast as the conditional probability of extreme rainfall event given the forecasts eq 4 to get the conditional probability joint probability needs to be calculated from the bivariate distribution of observed and forecasted hindcasted rainfall a bivariate copula is used and three archimedean copulas are fitted gumbel frank and clayton in order to find the best fit among these three gumbel is found to be the best fitted for all the grids and at all the lead times as it has the minimum aic and bic values and the maximum upper tail dependence coefficient calculated using the cfg estimator hence gumbel copula is identified as the most suitable and is used to generate the conditional probability of extreme rainfall given the forecasts and to calculate the hazard the daily accumulated rainfall forecasted at lead day 15 forecasted on 1st august 2018 to lead day 1 forecast done on 15th august 2018 is used in eq 4 to forecast hazard at respective lead days for the event that took place on 15th august 2018 here to consider the spatial bias we propose to apply three approaches as mentioned in section 4 1 by considering the rainfall data a only at the grid of interest b which is maximum over 3 3 grid boxes centred on the grid of interest and c which is maximum over 5 5 grid boxes centred on the grid of interest first we calculate the hazard for each grid using the observed and forecasted rainfall of the grid of interest at different lead times supplementary figure s3 for case a the higher hazard areas are not matching with the most affected areas hatched areas until lead day 2 even on lead day 1 only some of the hatched parts are shown to be having a high forecasted hazard value this may be attributed to the spatial bias present in the model which results in forecasting the rainfall in the neighbouring grid the other two analyses case b and c are done by considering the maximum rainfall in a 3 3 box of 9 grids and 5 5 box of 25 grids with the grid of interest at the centre the hazard maps generated using the 3 3 fig 7 and 5 5 boxes supplementary figure s4 shows high values at the areas that suffered maximum losses from lead day 15 this further proves that the rainfall forecast is likely to have a spatial bias in the model which can be overcome by also including the neighbouring grids in the analysis since the 3 3 and 5 5 cases cases b and c do not show much disagreement the maximum rainfall forecasted in a 3 3 box is used to generate the gridded hazard maps for each grid fig 7b 1 6 further which is used to calculate risk hazard for the same event is generated using rainfall forecast from all the 11 ensembles present in the gefs it found that no major difference exists in the hazard values between different ensembles this is because of the consideration of conditional probabilities from the hindcast in the gridded hazard maps from lead day 5 most of the grids start showing very high value which implies our method is able to predict the extreme event the hazard maps generated by this method are able to take into account the model s inability to forecast extreme rainfall magnitude correctly we further use area weighted average method to convert these gridded values to subdistrict scale values supplementary figure s5 the subdistrict wise hazard values are standardised between 0 and 1 and divided in to five categories very low 0 0 2 low 0 2 0 4 medium 0 4 0 6 high 0 6 0 8 and very high 0 8 1 starting from day 15 till lead day 10 the standardized hazard values are showing a low to very low values in most of the affected areas these values start to increase by lead day 5 and show high to very high hazard in most of the affected areas as these subdistrict wise hazard values are obtained based on the method that considers 3 3 grids these show a very little variability amongst adjacent subdistricts hence it is difficult to identify the high risk zones in the area under consideration using these subdistrict wise hazard maps to understand the applicability of the model for other extremes we apply the same to six extreme events that took place during 1985 2015 in the study region supplementary table s4 we present the composites of hazard values with their band from six events at different lead days supplementary figure s6 b i it is quite interesting to note that within lead days of 10 for all the cases of extremes the hazard value comes almost same and this is a good indication of identifying a correct threshold of hazard for a specific grid such an identification of threshold must be done probably with a higher number of extreme events corresponding to different high percentiles we further compute the false alarm ratio by considering the days to have false alarms when they are not extreme days but the model shows a higher hazard with respect to the threshold corresponding to the specific grid and lead day supplementary figure s7 we find still a huge false alarm ratio exists with our proposed post processing approach the false alarm ratio drops a bit at a lead day of 1 such a huge false alarm for the west coast of india during monsoon was also reported in shastri et al 2017 reducing false alarm needs improvements in the weather models and such improvements are not possible using post processing techniques alone for this specific region and the season for a coastal region it also needs finer resolution models to take care of the sea land inerface however given the forecasts these are the best estimates and among the high hazard zones the hotspots are identified with the help of vulnerability and exposure values hence we introduce the concept of risk that considers all three aspects hazard vulnerability and exposure the next subsections present the results obtained from the vulnerability analysis 5 2 vulnerability 5 2 1 socioeconomic vulnerability subdistrict wise socioeconomic vulnerability is computed using dea approach which calculates the relative efficiency of each subdistrict using various vulnerability indicators fig 8 a shows the subdistrict wise socioeconomic vulnerability map of kerala to understand the importance of socio economic vulnerability we overlaid the economic loss map on the vulnerability map showing the highly affected districts with hatching we find that most of the highly affected regions have high socio economic vulnerability the results also show the correctness of the selection of the socioeconomic indicators for this present study 5 2 2 topographic vulnerability the topographic vulnerability values calculated using the hand method and the vulnerability map is given in fig 8b among the highly affected districts alappuzha kottayam and some parts of pattanamtitta and wayanad have very high topographic vulnerability but idukki and eastern parts of pattanamtitta show very low to low vulnerabilities though losses in these areas are high this is because idukki and parts of pattanamtitta lie in western ghats mountain ranges resulting in higher hand values and thus lower topographic vulnerability 5 3 exposure exposure is calculated at subdistrict level as the sum of product of each the di value of a lulc type with the fraction of area covered by each type using eq 11 and exposure maps are generated fig 8c subdistricts with more crop areas and built in areas such as ernakulam kottayam and wayanad are more exposed to flood risks because of a high di value of these lulc types idukki and parts of alappuzha show low exposure due to the presence of more forest barren lands and water bodies which are associated with low di values 5 4 risk subdistrict wise relative risk maps are generated for the extreme rainfall event that took place on 15th august at different lead days by combining hazard vulnerability and exposure using eq 3 fig 9 risk is standardised and is categorised into 5 predefined categories very low 20 low 20 40 medium 40 60 high 60 80 and very high 80 it should be noted that very low here refers to very low relative risk compared to other sub district but at an absolute level they may be still high this risk map helps to prioritize sub district level the action plans by selecting the region with highest relative risk subdistricts with relatively higher relative risk are identified in this process which could have been prioritised for implementation of evacuation strategies planning response and recovery practices and other mitigation comparing the generated relative risk maps fig 9 with the actual loss data fig 2b it is evident that high relative risks in the districts like wayanad thrissur ernakulam and kottayam are forecasted by the model well in advance with a lead time of up to 15 days some subdistricts in malapuram which had reported moderate losses fig 2b are also showing high to very high relative risk throughout the 15 days forecast the predicted relative risk in kannur region is higher in the extended range 15 8 days forecasts because of the higher rainfall prediction in the region during that period this relative risk eventually reduces with lead time as the rainfall forecast improves the model failed to forecasts the high relative risk in some parts of alappuzha district because of the low exposure value despite having a very high vulnerability value fig 8 the low exposure value in that area is attributed to the small water bodies which cover a significant fraction of the total area and these water bodies are assigned with a very low di value 0 1 losses in idukki palakkad and eastern parts of pattanamtitta are mostly attributed to landslides in the western ghats during the event our model does not consider landslides and hence those areas are not identified by the relative flood risk maps as high risk zones the future scope is to consider a landslide model in this framework to further improve the forecasting skill the loss data for the flood is available at district level whereas we are predicting risk at subdistrict level it is evident from fig 9 that in some of the affected districts not all subdistricts are showing a high relative risk this may be due to the fact that the reported loss in a district is dominated by some of the subdistricts in it overall this method is successful in identifying most areas effected by direct flood as high relative risk areas at a lead time of almost 15 days 6 summary extreme rainfall events show an increasing trend in the indian subcontinent and so do the resultant flood events the state of the art rainfall forecast system is usually associated with a very high false alarm low hit rate and spatiotemporal biases in recent years there have been a number of cases where the prediction of extreme rainfall is either spatially or temporally inaccurate or the amount of rainfall predicted is wrong altogether these forecasts were not good enough to be implemented in mitigation planning thus resulting in huge loss of life and property here a new methodology has been proposed which generates relative flood risk maps at weather to medium range as a product of hazard vulnerability and exposure this methodology is applied to the august 2018 kerala floods in india for demonstration the concepts of vulnerability and exposure do not really add to the predictive information but helps to identify the regions with high risk well in advance the flood event during 2018 over kerala showed that all the areas with high flood losses did not necessarily experienced spatially highest amount of rainfall in the state the losses were also governed by the vulnerability and exposure therefore this concept is brought into the forecast system with the proposed approach hazard is defined as the probability of getting an extreme rainfall above 99th percentile in any grid given a forecast value gefs forecast data is used to generate hazard forecast up to 15 lead days in order to find the conditional probability a bivariate copula is fitted to the gefs hindcast and imd observed rainfall data for the period 1985 to 2015 cdf of observed and hindcasted rainfall data is found by fitting a mixed distribution where gamma distribution is used for the non zero values for selection of the best copula three archimedean copulas are considered namely gumbel frank and clayton gumbel is selected as it showed minimum aic bic values and maximum tail dependence among the three archimedean copulas in order to take care of the spatial bias present in the rainfall forecast model the maximum rainfall observed and forecasted in the neighbouring 3 3 box of 9 grids with the grid of interest in centre is considered as the rainfall observed and forecasted respectively for a grid the hazard generated is able to predict extreme rainfall by showing high values but fails to identify areas under high risk due to coarse spatial resolution hence along with the hazard values other local parameters such as the social and economic conditions topography and lulc are also incorporated in terms of vulnerability and exposure in order to find the relative risk of the units in the region of interest vulnerability is defined as average of socioeconomic and topographic vulnerabilities socioeconomic vulnerability describes the socioeconomic conditions of an area from the census of india 2011 data various positive and negative indicators are considered in order to quantify the socioeconomic vulnerability pca is performed to reduce the dimensionality of these indicators and 2 pcs explaining 75 of the variability are used as input for the dea framework bcc model of dea is used to calculate the relative efficiency of the subdistricts in order to rank them socioeconomic vulnerability is computed by subtracting the relative efficiency from 1 topographic vulnerability is calculated using the hand method which uses dem output and generates hand maps where the relative height of each grid with respect to the flow path is considered the smaller hand values show larger vulnerabilities so the value subtracted from 1 is used as the topographic vulnerability exposure for each subdistrict is calculated by assigning di values to different lulc types and multiplying these with the area fraction covered by each type in that subdistrict to evaluate the performance of the method it is applied to the devastating floods in kerala during august 2018 relative risk maps are generated at a subdistrict level at different lead days by combining the hazard generated with corresponding lead days with socioeconomic and topographic vulnerability and exposure these maps are compared with the district wise loss data from kpdna 2018 reports which is used as a proxy for severity of the flood most subdistricts in the highly affected districts like wayanad thrissur ernakulam and kottayam and moderately affected district malappuram are identified as high relative risk areas starting from lead day 15 though losses due to flood is very high in alappuzha the model fails to identify them in some of its subdistricts due to presence of water bodies with very low exposure value the high relative risk areas identified in the risk maps could have been given priorities in flood mitigation and evacuation planning in order decrease the flood losses the risk model can be applied to any different case studies and may suitably be adjusted modified depending on data availability as for example the major issue a new case study area may face are the problems associated with the availability of socio economic data the elevation data and the gridded rainfall data as applied in the present manuscript are mostly available for majority of areas around the globe the socio economic vulnerability component in the present model is a flexible one to consider a lower availability of variables the model is sensitive to the component used for vulnerability analysis and it does not consider intermodal uncertainty across different approaches used for computing vulnerability the proposed model assumes that only local rainfall feeds into the regional flood and does not consider either the upstream hydrology or the memory resulting from near past heavy rainfall such an assumption does not hold true for large watersheds the units considered in the model are rather administrative depending on the availability of socio economic data understanding of flood needs the consideration of hydrological units such as watershed or sub basin future scope of the work lies in addressing these limitations further improvement of this risk based forecast system can be done by using in situ station level or finer resolution rainfall data by combining this method with hydro economic models flood loss can be predicted accurately which is very useful for reduction and assessment of losses during floods often extreme rainfall in hilly regions lead to landslides and thus a landslide module can be added to the present model to further improve it for a multi hazard system another limitation of this framework is combining multiple indicators like hazard vulnerability and exposure using product based combination aggregation though this approach has been popularly used and are efficient for a quick and relative representation it may not be the best metric a solution could be a multi criteria decision making approach which can be considered as a potential area of future research credit statement sg conceptualized the idea and designed the overall algorithms and methodology st performed the formal analysis sk and vh performed the socio economic vulnerability analysis st and sg performed investigation and analysis of results st performed data curation sg and st wrote the manuscript sk and vh reviewed and edited the manuscript sg and sk supervised the work sg and sk did the acquisition of the financial support for the project leading to this publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors thank the department of science and technology for the financial support through sponsored research project dst ccp coe 140 2018 the authors acknowledge india meteorological department imd global ensemble forecast system gefs reforecast and tropical rainfall measuring mission trmm for the rainfall datasets we thank census of india government of india for the demographic dataset nasa version 3 0 shuttle radar topography mission srtm for the dem dataset and ornl daac for the lulc dataset we extend our gratitude to the kerala flood is provided by kerala state disaster management authority ksdma for providing the kerala post disaster needs assessment august 2018 kpdna 2018 report supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103785 appendix supplementary materials image application 1 image application 2 
384,non structural mitigation measures to the globally increasing flood events include forecast based alert generation however the extreme rainfall forecasts are associated with low hit rate high false alarm and spatiotemporal bias which makes it difficult to rely on them further the losses due to flood in a region not only depend on rainfall severity but also on topography socioeconomic conditions and exposure of the region to floods here we introduce a new concept of spatial flood risk mapping and forecasting at weather to medium range based on forecasted hazard embedded with vulnerability topographic and socioeconomic and exposure information we define hazard as the probability of extreme rainfall event during upcoming days given an available weather forecast for the same days as hindcast is used for computation of probabilities hazard contains prior information about the false alarm hit rate and spatiotemporal bias of the forecast vulnerability is calculated by averaging the topographic and socioeconomic indicators and exposure is calculated using a land use land cover map topographic vulnerability is computed with digital elevation model using height above the nearest drainage method and data envelopment analysis is performed to derive the socioeconomic vulnerability based on the demographic census data for a specific region and a specific event the relative flood risk maps are generated at an administrative level e g district subdistrict or village level for india and the high risk areas can be identified from those maps for mitigation the methodology is demonstrated for a very recent extremely severe flood event that happened in kerala india in august 2018 it is evident from the results that the high risk areas forecasted well in advance as high lead time as 15 days match fairly well with the areas which suffered maximum losses because of direct flood graphical abstract image graphical abstract keywords extreme rainfall flood risk weather forecast hazard vulnerability exposure 1 introduction climate change has caused a considerable impact to the global water cycle which lead to changes in seasonal patterns as well as an increase in the frequency of extreme rainfall events oki and kanae 2006 intergovernmental panel on climate change ipcc 2012 reports that this increase in extreme rainfall is statistically significant in many parts of the globe it also states with medium confidence that these changes in extremes are attributed to anthropogenic influences in the indian sub continent as well both empirical methods and model projections have shown an increase in the frequency and magnitude of extreme rainfall events goswami et al 2006 preethi et al 2017 roxy et al 2017 a decrease in the total rainfall and intensification of extremes in the tropics for 21st century has been projected in the ipcc reports seneviratne et al 2012 irrespective of climate change the importance of forecast and understanding its uncertainty have a very high societal relevance increasing extremes in a changing climate further increases its importance extreme rainfall events not only affect our day to day lives but also result in floods and landslides which cause huge loss of lives and property dottori et al 2018 fowler et al 2010 every year a billion people are affected and thousands die because of these extreme events ifrc rcs 2011 according to the national disaster management authority ndma 2008 floods have become a cause of concern because of an increasing trend in flood related losses in india some examples of such extreme events include the heavy rainfall event in mumbai india on 26th july 2005 which recorded 944 mm rainfall in 24 h jenamani et al 2006 the rainfall and resulting flood caused the death of almost 1000 people and an economic loss of about us 100 million kumar et al 2008 uttarakhand in india received almost 340 mm rainfall in a day 375 above daily mean on 17th june 2013 which resulted in severe flash floods dube et al 2014 chennai city experienced a terrible flood during november december 2015 which caused at least 400 deaths economic loss of us 1120 million narasimhan et al 2016 seenirajan et al 2017 a significant amount of research has been conducted in the past decades to understand these events a large proportion of these events could not have been predicted accurately and hence resulted in devastation coumou and rahmstorf 2012 the commonly used alert generation and warning system for flood are mostly based on the rainfall forecast hence an accurate prediction of extreme rainfall at an administrative level is very important for the stakeholders and decision makers however the complex multiscale atmospheric processes responsible for the occurrence of any extreme rainfall event and their inherent variability makes them difficult to predict fritsch et al 1998 coarse resolution dynamical models often fail to predict these extreme rainfall events with accuracy as these have high false alarm low hit rate and spatio temporal biases březková et al 2010 khaladkar et al 2007 selvam 2011 shastri et al 2017 prediction skills can be improved using regional models at a high spatiotemporal resolution but these simulations are computationally very intensive thus are difficult to perform on real time dodla and ratna 2010 very heavy rainfall in a short time span often results in floods when it exceeds the ground absorption capacity and the runoff exceeds the capacity of river system neuendorf et al 2005 this makes mitigation planning difficult flood risk associated with these events are difficult to predict as it not only depends on the complexity of processes related to extreme rainfall but also on the interaction between these events and the geography population infrastructure the preparedness of the region balk et al 2012 flood risk can be defined as a product of hazard vulnerability and exposure ipcc 2012 kron 2005 karmakar et al 2010 which is used for climatological projections in the risk framework for climate applications hazard is defined in ipcc 2012 as the potential occurrence of a natural or human induced physical event that may cause loss of life injury or other health impacts as well as damage and loss to property infrastructure livelihoods service provision and environmental resources calculating flood risk also requires vulnerability which works as a proxy of human environment relationship turner et al 2003 ipcc 2012 defines vulnerability as the propensity or predisposition to be adversely affected in order to get the overall vulnerability of a region various classes of vulnerability are combined together karmakar et al 2010 the exposure ipcc 2012 component of risk is defined as the presence location of people livelihoods environmental services and resources infrastructure or economic social or cultural assets in places that could be adversely affected by physical events and which thereby are subject to potential future harm loss or damage in the changing climate along with an increase in the amount and frequency of extreme rainfall events the exposure of humans to flood is also increasing hirabayashi et al 2013 thus it is considered for flood risk quantification however this approach of defining climatological flood risk cannot be used for event specific flood mitigation here we propose a new methodology to use this concept of flood risk at weather to medium range scale to generate event specific risk maps traditionally the concept of hazard is used at a climate scale where it is defined as the probability of extreme rainfall above a specific threshold and computed from the long term data here we have introduced a concept of hazard which is computed at weather to medium range in the forecast system hazard is computed as the probability of occurrence of an extreme rainfall in any grid given a forecast value flood risk maps are generated by combining hazard with socioeconomic and topographic vulnerability and exposure of a region to demonstrate and evaluate this methodology it is applied to the flood event in kerala india which occurred during august 2018 the following section discusses the case study and the data used the limitations of the state of art weather forecast system is discussed in the section 3 sections 4 and 5 contains the methodology and results respectively and the paper is summarised in the last section 2 case study and data 2 1 case study description kerala flood of august 2018 kerala is a southern coastal state of india spread over an area of 38 863 km2 and is divided into 14 districts and 63 subdistricts fig 1 a apel et al 2009 it is one of the most densely populated indian states with a population of over 33 million 860 people per square kilometre and with a gross domestic product gdp of us 120 billion department of economics and statistics government of kerala http www ecostat kerala gov in index php economy many other human development indicators for kerala are at par with those of developed countries like literacy rate 93 11 life expectancy 77 years and a sex ratio of 1084 women per 1000 men census 2011 it should also be noted that kerala is a coastal state extending like a thin strip along the west coast of india and the grids of state of the art weather forecasting model such as the global ensemble forecast system gefs reforecast version 2 are too coarse spatial resolution of 1 1 for the present one which is used in the study to obtain district and state specific information fig 1b between 1st june and 18th august 2018 the cumulative rainfall in kerala was 42 in excess of the normal average which caused the worst flood in august in the state since 1924 during this period maximum rainfall occurred on 15th august and the daily accumulated rainfall is shown in fig 2 a coastal parts of kerala received extremely heavy rainfall during the event from fig 2a it is evident that thrissur malappuram alappuzha pattanamtitta and kollam districts received the rainfall amount more than 120 mm day whereas in the other parts of kerala the intensity was more than 60 mm day 1259 out of 1664 villages spread across all 14 districts supplementary table s1 were affected kpdna 2018 nearly 341 landslides were reported from 10 districts with idukki being the worst hit district with 143 landslides the seven most affected districts were alappuzha ernakulam idukki kottayam pattanamtitta thrissur and wayanad malappuram and palakkad suffered moderate losses in this flood this affected 5 4 million people displaced 1 4 million people and took 433 lives 22nd may 29th august 2018 kpdna 2018 the district wise total crop transport aquaculture health and heritage losses in million usd as obtained from the kpdna 2018 reports are presented in fig 2b it is interesting to note that there is lack of consistency between the districts with high rainfall and the districts with high loss it may also be possible that the heavy rainfall that occurred at different places during the 1st half of august 2018 is responsible for such an inconsistency it is quite evident that very high loss fig 2b cannot be attributed to the rainfall intensity fig 2a only and there exists lot of other factors related to vulnerability and exposure here in this study we focus on the same for the forecasts of weather to medium range flood risk map according to the kpdna 2018 report though heavy rainfall was forecasted well in advance a lack of planning caused the overflow of reservoirs the late pre emptying of reservoirs is often making flood damage worse in india and elsewhere these problems could have been avoided by the help of a reliable forecast system and proper mitigation plan the method proposed here is applied to the kerala flood of august 2018 for demonstration 2 2 data the data obtained for different modules of the weather to medium range flood risk forecast models are mentioned in the following subsections 2 2 1 hazard the 6 hourly rainfall data at a spatial resolution of 1 1 for a period of 1985 to 2015 from global ensemble forecast system gefs reforecast version 2 is used as the reforecasted rainfall dataset hamill et al 2013 reforecast also called hindcast data are retrospective forecasts for the past using the same model configuration employed for operational forecasts the datasets are generated from an 11 member ensemble forecast every day from 00 utc for a period of december 1984 to present day and is available at a 3 hourly time step for first 8 days horizontal resolution is t254 50 km and 6 hourly time step for 8 16 days t190 70 km at 42 vertical levels the daily gridded rainfall data for the period 1985 to 2015 from the india meteorological department imd at a spatial resolution of 0 25 0 25 is used as the observational dataset pai et al 2015 this dataset is developed based on ground observations from 6995 stations across india using an inverse distance weighing scheme and is available for the period of 1901 2015 pai et al 2015 the data is re gridded to a resolution of 1 1 to match the resolution of forecast data fig 1b shows the grids considered in this study to generate the forecasted hazard for kerala during august 2018 flood event to compare the model performance in forecasting the extreme rainfall event tropical rainfall measurement mission trmm 3 hourly 0 25 0 25 rainfall product is used to generate daily accumulated rainfall for 15th august 2018 the day of maximum rainfall during the flood event we have used the trmm data as the gridded data from imd was not available for 2018 2 2 2 socioeconomic and topographic vulnerability to calculate the socioeconomic vulnerability the subdistrict level demographic and economic information is obtained from the census of india 2011 as per the availability of data from census of india a set of relevant indicators are chosen to appropriately represent the status of socio economy of the region vittal et al 2020 table 1 shows the list of indicators chosen along with corresponding justifications in support of their selection for the topographic information we have procured digital elevation model dem product farr et al 2007 from nasa version 3 0 shuttle radar topography mission srtm at global 1 arc s 30 m resolution which is further used in the height above the nearest drainage hand model to generate the topographic vulnerability 2 2 3 exposure here the land use land cover lulc data is used as a proxy to quantify the exposure component of risk as it directly affects the severity of flood for example but not limited to the land use pattern specifically over urbanized region leads to increasing the severity of flood due to increase in an impervious fraction which eventually increases the exposure of land use to flood over urban hotspots karmakar et al 2010 the decadal 100 m resolution lulc dataset for 2005 from ornl daac roy et al 2016 is used for this purpose which classifies the lulc into 19 classes as provided in supplementary table s2 3 limitations of state of the art weather forecasts the state of the art flood alert generation system uses the forecasted rainfall amount during extreme rainfall events however these forecasts are often associated with very low hit rate and high false alarm along with spatiotemporal biases here we consider the weather to medium range forecasts of extreme rainfall event in kerala on 15th august 2018 at multiple lead times since the meteorological forecasts have significant bias we apply bias correction by scaling maraun 2016 using the hindcast and observed data the bias corrected outputs are presented in fig 3 following the conventional approach bias correction is done for the entire available hindcast dataset but the result is presented only for a single storm even at lead day 1 the model completely fails to produce the magnitude of the extreme rainfall fig 2a over the state of kerala further to this at a lead time of 10 and 15 days the forecasts are not at all showing indications of extremes over kerala and at a lead time of 2 3 days it starts showing a bit of indication of moderate rainfall clearly these are not sufficient for an evacuation action or taking any other precautionary measures we also find that these forecasts are associated with very low hit rate and high false alarms here we define hit rate as the fraction of extremes predicted by the model successfully and false alarm as the fraction of time model predicts a non extreme event as an extreme hit rate and false alarm are calculated as 1 h i t r a t e a a c 2 f a l s e a l a r m b a b where a number of intersects between a and b b number of intersects between a and b c number of intersects between a and b a number of times model forecasts an extreme b number of times observation shows an extreme a number of time model forecasts a non extreme rainfall b number of time observation shows a non extreme rainfall the hit rate and false alarm associated with the forecasts are given in supplementary figure s1 the hit rate always remains around 10 at any lead time with a very slight improvement at a lead time of 1 day false alarm remains as high as 90 it is evident that they are not enough for decision making towards disaster mitigation 4 method here to address the above mentioned problems associated with the uncertain and biased forecasts we propose a novel approach of forecasting flood risk at weather to extended range scale this is in contrast to the conventional approach of flood forecasting which uses bias corrected weather forecasts conventionally the risk to extremes is defined as the product of hazard vulnerability and exposure as chen et al 2015 gusain et al 2020 ipcc 2012 karmakar et al 2010 kron 2005 sahani et al 2019 3 r i s k h a z a r d v u l n e r a b i l i t y e x p o s u r e the concept of risk has been widely used at a climate scale however we propose the same at a weather to medium range the difference in the concept of risks between weather to medium range and climate scales lies in the definition of hazards we define hazard as the probability of an extreme event above a defined threshold given the forecasts for the same day the flood risk at a weather to medium range incorporates weather to medium range forecasts which essentially makes the hazard component dynamic whereas the remaining components of flood risk vulnerability and exposure are considered to be static during extreme events based on the forecasted risk maps generated at a weather to medium range the high risk areas can be identified and targeted first hence a location specific evacuation and flood mitigation can be done well in advance to reduce the losses fig 4 shows a complete flowchart of the methodology used to generate weather to medium range event specific flood risk maps further these maps need to be generated at an administrative eg district subdistrict or village for india level depending upon the data availability for better efficiency in decision making in the present study flood risk is generated at subdistrict level based on the authoritarian decision process and availability of demographic data 4 1 hazard hazard is typically defined as the probability of an extreme event this concept is traditionally used at a climate scale gusain et al 2020 sajjad et al 2020 as for example a hazard associated with 95thpercentile of rainfall is 0 05 here we propose to define hazard at a weather to medium range and define it as the probability of getting an extreme rainfall in any grid given a forecast value this is a dependant of the forecasts and hence with the change in lead time and subsequent forecasts the hazard value at a location gets modified at a location for example for a grid if we define extreme rainfall event as the rainfall above a threshold of 99th percentile the hazard may be defined as 4 haza rd p o o 99 h f where o observed daily rainfall o 99 99th percentile of rainfall observations h forecasted hindcast rainfall with its value denoted as f in order to generate the conditional probability given in eq 4 we first obtain the joint probability of observed and hindcast from the same model which is being used for forecast to generate the joint probability copula is used which creates the multivariate distribution based on the individual marginal distributions dupuis 2007 ghosh 2010 copula does not need the marginal distributions to follow a specific and same distribution this makes copula advantageous over conventional multivariate parametric multivariate distributions like normal log normal the first step in applying the copula based approach is to obtain the marginal distribution functions of associated variables which are the observed rainfall o and hindcast rainfall h as both observational and hindcast rainfall data contain zero rainfall values mixed marginal distributions are used gamma distributions are fitted to the non zero values of rainfall to model the probability mass function of zero and non zero rainfall we apply bernouli trials the cdf of the variables x which stands for both o and h is given by 5 f x x p 1 p g x x x 0 p x 0 where p probability of getting zero rainfall n 1 n n total no of days n 1 no of zero rainfall days gx x cdf of nonzero rainfall obtained by fitting a gamma distribution after generating the marginal distributions archimedean copula is used to generate the bivariate distribution in this case o and h are the two variables for which copula is to be fitted by definition of copula a two dimensional distribution function is given by 6 f o h c f o o f h f where c copula fo and f h marginal distribution functions of o and h in the approach based on copula we consider two variables u fo o and v fh f to be the cdf of o and h respectively where u and v are uniformly distributed random variables with values u and v here a single parametric copula is used with parameter θ ghosh 2010 nelsen 1999 zhang and singh 2006 7 c θ u v 1 u v where is a convex decreasing function copula generator the parameter θ is generated using the relationship between kendall s coefficient of correlation τ and t ghosh 2010 karmakar and simonovic 2009 8 τ 1 4 0 1 t t d t where t u or v kendall s τ calculated using the following equation 9 τ n n 2 1 i j s i g n x i x j y i y j where s i g n 1 f o r x i x j y i y j 0 s i g n 0 f o r x i x j y i y j 0 i j 1 2 n three types of archimedean copulas are used namely gumbel frank and clayton the relationship between τ and θ and the mathematical equations for each of these copulas are given in supplementary table s3 the choice of copula to best fit the distributions is crucial to the above calculations favre et al 2004 the copula having minimum akaike information criterion aic and bayesian information criterion bic score is considered to be the best fitted copula for correct representation of extreme events tail dependence test is important for the selection of copula ghosh 2010 poulin et al 2007 as we are dealing with extreme rainfall events copula having the highest upper tail dependence coefficient is desirable here the upper tail dependence coefficient is computed using a nonparametric estimator cfg caperaa fougeres genest capéraà et al 2000 based on aic bic and upper tail dependence coefficient the best copula is selected the other important criteria is the selection of grids for analysing the forecasts ideally the forecasted and obserevd rainfall should belong to the same grid however there is a high possibility of spatial bias in the forecasts that generates rainfall to a neighbouring place of the area of interest to overcome this we perform three anlyses a considering only the grid of interest for both observations and the forecasts b considering maximum of the observed and forecasted rainfall over a 3 3 box of 9 grids with the grid of interest at the centre c considering maximum of the observed and forecasted rainfall over a 5 5 box of 25 grids with the grid of interst at the centre the hazard values thus generated using eq 4 are in gridded form which are converted into subdistrict level by area weighted average method 4 2 vulnerability the vulnerability component of flood risk is calculated as the average of socioeconomic and topographic vulnerabilities 4 2 1 socioeconomic vulnerability here a measure of a region s susceptibility to flood damage is referred as flood vulnerability which includes a portion of population susceptible to either emotional mental or physical damage in addition the seriousness of current situation and previous experiences with disastrous event may further influence the vulnerability karmakar et al 2010 during a disastrous event the socio economic vulnerability mainly focuses on response reaction and resistance for a population of a region along with the damage caused to the economic sector here the framework proposed by sherly et al 2015 and vittal et al 2020 is implemented to estimate the socio economic vulnerability over the study region the major steps followed which include judicious selection standardization and aggregation of indicators and subsequent ranking of sub districts have been shown in fig 5 a step by step exposition has been provided in the following paragraphs choice of indicator plays an important role in socioeconomic vulnerability quantification the indicators chosen should be relevant justifiable and a good representative of the social and economic condition of the concerned region vittal et al 2020 the set of indicators selected for our study along with their justification are given in table 1 identifying an indicator as positive sensitive or negative adaptive has a significant influence on the overall vulnerability and thus appropriate recognition of such indicators is crucial vittal et al 2020 sharma et al 2020 a positive negative indicator increases decreases the vulnerability and consequently affect the flood risk here the vulnerability indicators are standardized mainly to make the indicators dimensionless which will allow us to compare the different indicators over the study region the method of standardization for an indicator wu et al 2002 karmakar et al 2010 is provided in the equation below 10a v i s t d v i v m i n v m a x v m i n for negative sensitive indicators 10b v i s t d v m a x v i v m a x v m i n for positive adaptive indicators where v i s t d standardized vulnerability indicator of ith subdistrict vi vulnerability indicator of ith subdistrict vmin minimum vulnerability indicator in all the subdistricts vmax maximum vulnerability indicator in all the subdistricts these equations consider both maximum and minimum values in the expression and ensure that the vulnerability values are within a 0 1 interval wu et al 2002 and always non negative karmakar et al 2010 during standardization the indicators were adjusted for their sign which indicates whether each indicator contributes positively or negatively to overall vulnerability the inherent subjectivity involved in the selection of threshold values for different classes or ranges makes it difficult to categorize the vulnerability holand et al 2011 mitchem 2004 uitto 1998 to address this issue there have been different approaches adopted by the researchers ranging from simple averaging karmakar et al 2010 to more complex cluster analysis kok et al 2016 sietz et al 2011 although these approaches are equally efficient for an elegant representation these lack in reducing the subjectivity in selection of weights rygel et al 2006 contrarily data envelopment analysis dea huang et al 2011 sherly et al 2015 vittal et al 2020 wei et al 2004 does not require weight assignment thereby reducing the subjectivity and also introduces a new classification approach with minimal possibility of rank reversal huang et al 2011 wei et al 2004 in addition dea does not make any assumption on the form of the functions as it is a non parametric technique therefore the present study implements dea to aggregate the standardized indicators and subsequently rank each spatial unit known as decision making units dmus i e subdistricts in the present study to use the dea model efficiently the indicators should have very low correlation in order to decorrelate the highly correlated indicators and decrease the dimensionality of these indicators principal component analysis pca is performed principal components pcs explaining 75 of the variability are considered as inputs for the dea following the approach of sherly et al 2015 and vittal et al 2020 a dummy value 1 unity is assigned as output in the dea since it represents the state of the system prior to the occurrence of a hazard hence the socioeconomic vulnerability of each dmu subdistrict is obtained by subtracting the relative efficiency of that subdistrict from unity in our case 2 pcs are considered as input and the banker charnes cooper bcc model banker et al 1984 of dea is used to rank the dmus by calculating their relative efficiencies lower the efficiency lower is the rank and higher is the vulnerability 4 2 2 topographic vulnerability to quantify the flood risk during extreme rainfall events elevation of a region plays a very important role topographic vulnerability accounts for the elevation and hand method rahmati et al 2018 rennó et al 2008 nobre et al 2011 2016 is used to quantify this parameter the model normalises digital elevation model dem values by changing the elevation with respect to sea level into elevation with respect to nearest drainage the advantage of using hand values to calculate topographic vulnerability is that grids with different elevation values but same hand value are considered equally vulnerable a flow chart of the methodology followed to calculate topographic vulnerability at grid level is shown in fig 6 and the steps followed are described considering an example dem in supplementary figure s2 firstly the dem with sinks is used as input for the hand model a sink or depression is an area or a point which has an elevation lower than all its neighbouring area or point rieger 1998 marked in red circles in supplementary figure s2a as it does not have any drainage outlet flow network generation is not possible with dem having sinks depression breaching method martz and garbrecht 1999 is used to connect these sinks and a hydrologically coherent dem is generated supplementary figure s2c the breaching method connects two neighbouring sinks by lowering elevation of some points in the shortest path connecting them flow direction is computed from this hydrologically coherent dem using d8 method tarboron 1997 in this method flow from each grid is assigned to one of its eight neighbours having the steepest slope to generate a local drainage direction ldd the ldd generated using hydrologically coherent dem is called coherent ldd supplementary figure s2d next a flow accumulation for each grid is computed by the adding the number of grids draining into that grid to get an accumulated area map supplementary figure s2e in order to generate a drainage network map channel initiation is done by setting up a threshold of accumulated area set as 10 in the example in the map grids having an area above the threshold value are considered in the drainage network this threshold can be applied manually or automatically by using an accurate drainage network map called mapped stream network msn as input using the ldd with drainage network nearest drainage map is generated and each grid is associated with the grid that it drains into known as the drainage grid an example is shown in supplementary figure s2g where each drainage grid is given a colour and the associated grids are represented as a lighter shade of the same colour the relative height of each grid is calculated as the difference of height of that grid and its drainage grid to generate a hand map supplementary figure s2h to calculate topographic vulnerability for kerala an automated geographic information system gis tool of the hand model developed by rahmati et al al 2018 is used due to the unavailability of an accurate msn an appropriate threshold is to be selected manually by trial and error method as suggested by rahmati et al 2018 here 25 000 is found to an appropriate threshold as higher values fail to give any positive value to the grids in the low lying areas coastal areas and lower values result in a very high density drainage network using this drainage network nearest area map is generated and hand values calculated at a grid level 30 m these gridded hand values are converted to subdistrict level by weighted average method lower the hand value higher is the topographic vulnerability subdistrict level hand values are standardised between 0 and 1 and are subtracted from 1 to obtain the topographic vulnerability 4 3 exposure lulc of an area is used as indicator to quantify exposure of any area to flood lulc is a primary characteristic of any region and decides the soil permeability run off etc following the methodology used by karmakar et al 2010 each lulc category is assigned with a degree of importance di for example the built in area has more pavement and concrete surfaces which increases the run off and is more prone to losses during flood and thus is assigned a higher di value on the other hand grassland like open areas have very low di values since they allow infiltration and decrease run off so suffer lesser losses di values associated with each of the lulc type as described by karmakar et al 2010 are given in table 2 using the di values and the fraction of area they covered by any lulc type in a subdistrict exposure is calculated as 11 e i l 1 n d i l a i l a i where ei exposure of subdistrict i l lulc type dil degree of importance for lulc type l a i l area occupied by lulc type l in subdistrict i ai area of subdistrict i the subdistrict wise exposure values obtained are then standardised between 0 and 1 and exposure maps are generated 5 results 5 1 hazard in the proposed methodology hazard is calculated for different lead times using the rainfall forecast as the conditional probability of extreme rainfall event given the forecasts eq 4 to get the conditional probability joint probability needs to be calculated from the bivariate distribution of observed and forecasted hindcasted rainfall a bivariate copula is used and three archimedean copulas are fitted gumbel frank and clayton in order to find the best fit among these three gumbel is found to be the best fitted for all the grids and at all the lead times as it has the minimum aic and bic values and the maximum upper tail dependence coefficient calculated using the cfg estimator hence gumbel copula is identified as the most suitable and is used to generate the conditional probability of extreme rainfall given the forecasts and to calculate the hazard the daily accumulated rainfall forecasted at lead day 15 forecasted on 1st august 2018 to lead day 1 forecast done on 15th august 2018 is used in eq 4 to forecast hazard at respective lead days for the event that took place on 15th august 2018 here to consider the spatial bias we propose to apply three approaches as mentioned in section 4 1 by considering the rainfall data a only at the grid of interest b which is maximum over 3 3 grid boxes centred on the grid of interest and c which is maximum over 5 5 grid boxes centred on the grid of interest first we calculate the hazard for each grid using the observed and forecasted rainfall of the grid of interest at different lead times supplementary figure s3 for case a the higher hazard areas are not matching with the most affected areas hatched areas until lead day 2 even on lead day 1 only some of the hatched parts are shown to be having a high forecasted hazard value this may be attributed to the spatial bias present in the model which results in forecasting the rainfall in the neighbouring grid the other two analyses case b and c are done by considering the maximum rainfall in a 3 3 box of 9 grids and 5 5 box of 25 grids with the grid of interest at the centre the hazard maps generated using the 3 3 fig 7 and 5 5 boxes supplementary figure s4 shows high values at the areas that suffered maximum losses from lead day 15 this further proves that the rainfall forecast is likely to have a spatial bias in the model which can be overcome by also including the neighbouring grids in the analysis since the 3 3 and 5 5 cases cases b and c do not show much disagreement the maximum rainfall forecasted in a 3 3 box is used to generate the gridded hazard maps for each grid fig 7b 1 6 further which is used to calculate risk hazard for the same event is generated using rainfall forecast from all the 11 ensembles present in the gefs it found that no major difference exists in the hazard values between different ensembles this is because of the consideration of conditional probabilities from the hindcast in the gridded hazard maps from lead day 5 most of the grids start showing very high value which implies our method is able to predict the extreme event the hazard maps generated by this method are able to take into account the model s inability to forecast extreme rainfall magnitude correctly we further use area weighted average method to convert these gridded values to subdistrict scale values supplementary figure s5 the subdistrict wise hazard values are standardised between 0 and 1 and divided in to five categories very low 0 0 2 low 0 2 0 4 medium 0 4 0 6 high 0 6 0 8 and very high 0 8 1 starting from day 15 till lead day 10 the standardized hazard values are showing a low to very low values in most of the affected areas these values start to increase by lead day 5 and show high to very high hazard in most of the affected areas as these subdistrict wise hazard values are obtained based on the method that considers 3 3 grids these show a very little variability amongst adjacent subdistricts hence it is difficult to identify the high risk zones in the area under consideration using these subdistrict wise hazard maps to understand the applicability of the model for other extremes we apply the same to six extreme events that took place during 1985 2015 in the study region supplementary table s4 we present the composites of hazard values with their band from six events at different lead days supplementary figure s6 b i it is quite interesting to note that within lead days of 10 for all the cases of extremes the hazard value comes almost same and this is a good indication of identifying a correct threshold of hazard for a specific grid such an identification of threshold must be done probably with a higher number of extreme events corresponding to different high percentiles we further compute the false alarm ratio by considering the days to have false alarms when they are not extreme days but the model shows a higher hazard with respect to the threshold corresponding to the specific grid and lead day supplementary figure s7 we find still a huge false alarm ratio exists with our proposed post processing approach the false alarm ratio drops a bit at a lead day of 1 such a huge false alarm for the west coast of india during monsoon was also reported in shastri et al 2017 reducing false alarm needs improvements in the weather models and such improvements are not possible using post processing techniques alone for this specific region and the season for a coastal region it also needs finer resolution models to take care of the sea land inerface however given the forecasts these are the best estimates and among the high hazard zones the hotspots are identified with the help of vulnerability and exposure values hence we introduce the concept of risk that considers all three aspects hazard vulnerability and exposure the next subsections present the results obtained from the vulnerability analysis 5 2 vulnerability 5 2 1 socioeconomic vulnerability subdistrict wise socioeconomic vulnerability is computed using dea approach which calculates the relative efficiency of each subdistrict using various vulnerability indicators fig 8 a shows the subdistrict wise socioeconomic vulnerability map of kerala to understand the importance of socio economic vulnerability we overlaid the economic loss map on the vulnerability map showing the highly affected districts with hatching we find that most of the highly affected regions have high socio economic vulnerability the results also show the correctness of the selection of the socioeconomic indicators for this present study 5 2 2 topographic vulnerability the topographic vulnerability values calculated using the hand method and the vulnerability map is given in fig 8b among the highly affected districts alappuzha kottayam and some parts of pattanamtitta and wayanad have very high topographic vulnerability but idukki and eastern parts of pattanamtitta show very low to low vulnerabilities though losses in these areas are high this is because idukki and parts of pattanamtitta lie in western ghats mountain ranges resulting in higher hand values and thus lower topographic vulnerability 5 3 exposure exposure is calculated at subdistrict level as the sum of product of each the di value of a lulc type with the fraction of area covered by each type using eq 11 and exposure maps are generated fig 8c subdistricts with more crop areas and built in areas such as ernakulam kottayam and wayanad are more exposed to flood risks because of a high di value of these lulc types idukki and parts of alappuzha show low exposure due to the presence of more forest barren lands and water bodies which are associated with low di values 5 4 risk subdistrict wise relative risk maps are generated for the extreme rainfall event that took place on 15th august at different lead days by combining hazard vulnerability and exposure using eq 3 fig 9 risk is standardised and is categorised into 5 predefined categories very low 20 low 20 40 medium 40 60 high 60 80 and very high 80 it should be noted that very low here refers to very low relative risk compared to other sub district but at an absolute level they may be still high this risk map helps to prioritize sub district level the action plans by selecting the region with highest relative risk subdistricts with relatively higher relative risk are identified in this process which could have been prioritised for implementation of evacuation strategies planning response and recovery practices and other mitigation comparing the generated relative risk maps fig 9 with the actual loss data fig 2b it is evident that high relative risks in the districts like wayanad thrissur ernakulam and kottayam are forecasted by the model well in advance with a lead time of up to 15 days some subdistricts in malapuram which had reported moderate losses fig 2b are also showing high to very high relative risk throughout the 15 days forecast the predicted relative risk in kannur region is higher in the extended range 15 8 days forecasts because of the higher rainfall prediction in the region during that period this relative risk eventually reduces with lead time as the rainfall forecast improves the model failed to forecasts the high relative risk in some parts of alappuzha district because of the low exposure value despite having a very high vulnerability value fig 8 the low exposure value in that area is attributed to the small water bodies which cover a significant fraction of the total area and these water bodies are assigned with a very low di value 0 1 losses in idukki palakkad and eastern parts of pattanamtitta are mostly attributed to landslides in the western ghats during the event our model does not consider landslides and hence those areas are not identified by the relative flood risk maps as high risk zones the future scope is to consider a landslide model in this framework to further improve the forecasting skill the loss data for the flood is available at district level whereas we are predicting risk at subdistrict level it is evident from fig 9 that in some of the affected districts not all subdistricts are showing a high relative risk this may be due to the fact that the reported loss in a district is dominated by some of the subdistricts in it overall this method is successful in identifying most areas effected by direct flood as high relative risk areas at a lead time of almost 15 days 6 summary extreme rainfall events show an increasing trend in the indian subcontinent and so do the resultant flood events the state of the art rainfall forecast system is usually associated with a very high false alarm low hit rate and spatiotemporal biases in recent years there have been a number of cases where the prediction of extreme rainfall is either spatially or temporally inaccurate or the amount of rainfall predicted is wrong altogether these forecasts were not good enough to be implemented in mitigation planning thus resulting in huge loss of life and property here a new methodology has been proposed which generates relative flood risk maps at weather to medium range as a product of hazard vulnerability and exposure this methodology is applied to the august 2018 kerala floods in india for demonstration the concepts of vulnerability and exposure do not really add to the predictive information but helps to identify the regions with high risk well in advance the flood event during 2018 over kerala showed that all the areas with high flood losses did not necessarily experienced spatially highest amount of rainfall in the state the losses were also governed by the vulnerability and exposure therefore this concept is brought into the forecast system with the proposed approach hazard is defined as the probability of getting an extreme rainfall above 99th percentile in any grid given a forecast value gefs forecast data is used to generate hazard forecast up to 15 lead days in order to find the conditional probability a bivariate copula is fitted to the gefs hindcast and imd observed rainfall data for the period 1985 to 2015 cdf of observed and hindcasted rainfall data is found by fitting a mixed distribution where gamma distribution is used for the non zero values for selection of the best copula three archimedean copulas are considered namely gumbel frank and clayton gumbel is selected as it showed minimum aic bic values and maximum tail dependence among the three archimedean copulas in order to take care of the spatial bias present in the rainfall forecast model the maximum rainfall observed and forecasted in the neighbouring 3 3 box of 9 grids with the grid of interest in centre is considered as the rainfall observed and forecasted respectively for a grid the hazard generated is able to predict extreme rainfall by showing high values but fails to identify areas under high risk due to coarse spatial resolution hence along with the hazard values other local parameters such as the social and economic conditions topography and lulc are also incorporated in terms of vulnerability and exposure in order to find the relative risk of the units in the region of interest vulnerability is defined as average of socioeconomic and topographic vulnerabilities socioeconomic vulnerability describes the socioeconomic conditions of an area from the census of india 2011 data various positive and negative indicators are considered in order to quantify the socioeconomic vulnerability pca is performed to reduce the dimensionality of these indicators and 2 pcs explaining 75 of the variability are used as input for the dea framework bcc model of dea is used to calculate the relative efficiency of the subdistricts in order to rank them socioeconomic vulnerability is computed by subtracting the relative efficiency from 1 topographic vulnerability is calculated using the hand method which uses dem output and generates hand maps where the relative height of each grid with respect to the flow path is considered the smaller hand values show larger vulnerabilities so the value subtracted from 1 is used as the topographic vulnerability exposure for each subdistrict is calculated by assigning di values to different lulc types and multiplying these with the area fraction covered by each type in that subdistrict to evaluate the performance of the method it is applied to the devastating floods in kerala during august 2018 relative risk maps are generated at a subdistrict level at different lead days by combining the hazard generated with corresponding lead days with socioeconomic and topographic vulnerability and exposure these maps are compared with the district wise loss data from kpdna 2018 reports which is used as a proxy for severity of the flood most subdistricts in the highly affected districts like wayanad thrissur ernakulam and kottayam and moderately affected district malappuram are identified as high relative risk areas starting from lead day 15 though losses due to flood is very high in alappuzha the model fails to identify them in some of its subdistricts due to presence of water bodies with very low exposure value the high relative risk areas identified in the risk maps could have been given priorities in flood mitigation and evacuation planning in order decrease the flood losses the risk model can be applied to any different case studies and may suitably be adjusted modified depending on data availability as for example the major issue a new case study area may face are the problems associated with the availability of socio economic data the elevation data and the gridded rainfall data as applied in the present manuscript are mostly available for majority of areas around the globe the socio economic vulnerability component in the present model is a flexible one to consider a lower availability of variables the model is sensitive to the component used for vulnerability analysis and it does not consider intermodal uncertainty across different approaches used for computing vulnerability the proposed model assumes that only local rainfall feeds into the regional flood and does not consider either the upstream hydrology or the memory resulting from near past heavy rainfall such an assumption does not hold true for large watersheds the units considered in the model are rather administrative depending on the availability of socio economic data understanding of flood needs the consideration of hydrological units such as watershed or sub basin future scope of the work lies in addressing these limitations further improvement of this risk based forecast system can be done by using in situ station level or finer resolution rainfall data by combining this method with hydro economic models flood loss can be predicted accurately which is very useful for reduction and assessment of losses during floods often extreme rainfall in hilly regions lead to landslides and thus a landslide module can be added to the present model to further improve it for a multi hazard system another limitation of this framework is combining multiple indicators like hazard vulnerability and exposure using product based combination aggregation though this approach has been popularly used and are efficient for a quick and relative representation it may not be the best metric a solution could be a multi criteria decision making approach which can be considered as a potential area of future research credit statement sg conceptualized the idea and designed the overall algorithms and methodology st performed the formal analysis sk and vh performed the socio economic vulnerability analysis st and sg performed investigation and analysis of results st performed data curation sg and st wrote the manuscript sk and vh reviewed and edited the manuscript sg and sk supervised the work sg and sk did the acquisition of the financial support for the project leading to this publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors thank the department of science and technology for the financial support through sponsored research project dst ccp coe 140 2018 the authors acknowledge india meteorological department imd global ensemble forecast system gefs reforecast and tropical rainfall measuring mission trmm for the rainfall datasets we thank census of india government of india for the demographic dataset nasa version 3 0 shuttle radar topography mission srtm for the dem dataset and ornl daac for the lulc dataset we extend our gratitude to the kerala flood is provided by kerala state disaster management authority ksdma for providing the kerala post disaster needs assessment august 2018 kpdna 2018 report supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103785 appendix supplementary materials image application 1 image application 2 
