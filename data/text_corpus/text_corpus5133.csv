index,text
25665,many multi objective optimization problems in integrated environmental modelling and management involve not only continuous decision variables but also variables like integers and or discrete variables furthermore the optimization problems are often subject to various constraints solving this kind of constrained hybrid problems usually requires a huge number of model evaluations that can be computationally expensive this study presents an algorithm known as multi objective adaptive surrogate modelling based optimization for constrained hybrid problems mo asmoch it incorporates several evolutionary operators to handle different types of decision variables and uses a classification surrogate model to deal with model constraints mo asmoch was evaluated against the widely used nsga ii method on three engineering design problems and three water distribution system design problems with up to 30 dimensions the results showed that mo asmoch is able to obtain nondominated solutions of similar quality as that of nsga ii using much fewer model evaluations keywords multi objective optimization surrogate model constrained hybrid problem nsga ii mo asmo 1 introduction many optimization problems in integrated modelling and management of environmental systems involve not only continuous decision variables but also discrete ones this kind of problems are known as hybrid or mixed variable optimization problems many examples of such problems come from the design of water distribution systems cunha and marques 2020 wang et al 2017 environmental and watershed management ahmadi et al 2013 geng et al 2019 groundwater remediation and others li and hilton 2007 shourian and davoudi 2017 some decision variables in those problems only take on integer values or a specific selection from a set of options for instance categorical variables or variables describing components with standard values in fact hybrid optimization problems are very common in various fields such as engineering design process industry and management science deep et al 2009 the hybrid nature of the decision variables increases the complexity of the search space and the difficulty of finding the optimal solutions wang et al 2021 therefore solving these problems is challenging and often accompanied by a large number of computationally expensive computer model simulations in practice many of the above mentioned optimization problems are inherently multi objective and several disparate and often conflicting criteria have to be considered simultaneously generally in multi objective optimization there is no such a solution that can optimize all objective functions at the same time instead we are looking for trade off or pareto optimal solutions to solve this kind of problems multi objective evolutionary algorithms moeas seem to be the most promising and have attracted a lot of attention over the recent several decades wang et al 2017 such as the nondominated sorting genetic algorithm ii nsga ii deb et al 2002 the nsga iii deb and jain 2013 the pareto archived evolution strategy paes knowles and corne 1999 the strength pareto evolutionary algorithm 2 spea2 zitzler et al 2001 moea based on decomposition moea d zhang and li 2007 although most of the algorithms were originally developed for continuous variable optimization problems they can be suitably modified and extended to handle integer and discrete variables for example since the genetic algorithm can handle different variable types by modifying the sampling crossover and mutation components the nsga ii method arguably the most popular moea has been widely applied to various fields for solving multi objective optimization problems in the presence of discrete variables ahmadi et al 2013 bre and fachinotti 2017 oraei zare et al 2012 wang et al 2015 however when compared to its single objective counterpart multi objective optimization methods suffer from high computational cost basically they require hundreds of thousands of simulation runs to identify the pareto optimal solutions if the computer simulation model is computationally expensive to run the computational burden of using these moeas would become intractable surrogate models which are cheap to run statistical or empirical data driven models can be used to emulate the response surfaces of expensive simulation models therefore many surrogate modelling based optimization methods which use the predictions of the surrogate model to guide the search for promising solutions müller 2016 have been developed and widely used in a variety of fields including environmental modelling and water resources management razavi et al 2012 not surprisingly surrogate models have also been used in combination with various moeas to identify the pareto optimal front with significantly reduced number of original model evaluations for instance nain and deb 2005 integrated the nsga ii with artificial neural network ann and developed the method nsga ii ann there are also many similar surrogate enabled multi objective optimization algorithms akhtar and shoemaker 2016 datta and regis 2016 jourdan et al 2006 kunakote and bureerat 2013 müller 2017 however most of the surrogate based multi objective algorithms have been developed by fitting the response surface surrogates on continuous decision variables there is only a limited number of studies featuring surrogates in the discrete decision space for multi objective problems behzadian et al 2009 brownlee and wright 2015 castelletti et al 2010 di pierro et al 2009 compared to those for single objective problems the recent development of surrogate assisted optimization methods has shown an increasing interest in addressing discrete multi objective optimization problems gu et al 2021a 2021b regis 2021 there still could be room for improvement to make the methods flexible to handle problems with different kinds of variables rather than continuous or discrete variables alone furthermore not a few of these surrogate based multi objective optimization algorithms for discrete problems were not able to handle various model constraints manson et al 2021 whose calculation can be computationally expensive by themselves generally for single objective problems many algorithms deal with model constraints by incorporating them into the objective function via penalty terms and using surrogate models to approximate the modified objective function regis 2011 however the adjustment of the penalty factor is a delicate task müller and woodbury 2017 which could impact the surrogate modelling procedure alternatively surrogate models can be directly used to approximate expensive constraint functions however this way of handling constraints has only been adopted in limited multi objective surrogate based optimization methods brownlee and wright 2015 datta and regis 2016 de winter et al 2021 hussein and deb 2016 regis 2016 singh et al 2014 considering all the points discussed above it is known that there is a lack of research on surrogate modelling based methods that can simultaneously address hybrid optimization problems with various kinds of variables multiple computationally expensive objective functions and model constraints therefore we develop a method called the multi objective adaptive surrogate modelling based optimization for constrained hybrid problems mo asmoch the new mo asmoch method is built from our previously proposed mo asmo method gong et al 2016a which was originally designed to solve continuous optimization problems without considering model constraints except for parameter ranges in this mo asmoch method a truncation procedure and a mapping procedure are used along with the real coded nsga ii to handle integer and discrete variables separate surrogate models are used to approximate different objective functions and model constraints in addition we have also introduced a new adaptive sampling strategy on the original basis to balance the local exploration with the global search of the sample points the rest of the article is organized as follows section 2 gives a detailed description of the proposed mo asmoch method section 3 introduces the test problems including three real world engineering design problems and three water distribution system wds design problems the results showing the performance of the proposed method and its comparison with the nsga ii method are presented in section 4 section 5 provides some discussion and section 6 gives a summary of the paper 2 methodology 2 1 optimization problem description formally this paper is focused on solving the following multi objective optimization problem 1 min f x f 1 x f m x s t g x g 1 x g n x 0 h x h 1 x h k x 0 x i d i ℝ i 1 d where x is a vector with d dimensional decision variables and x i is the i th variable which could be continuous integer or discrete d i is the parameter range if x i is a continuous variable otherwise d i is the finite set of the possible integer or discrete values f x denotes m objective functions g x denotes n inequality constraints and h x denotes k equality constraints assume that they are black box and obtained from a computationally expensive simulation using the original model the objectives are usually conflicting and solving the problem 1 needs to minimize all the objectives simultaneously in the feasible region where the solutions must satisfy all conditions imposed by the constraints generally most of the constraint functions of the optimization problems in the literature are inequality constraints which can be always defined as g x 0 thus a solution is considered as infeasible if at least one constraint violation is larger than zero in addition to the inequality constraints there are also binding constraint if an inequality constraint holds with equality at the optimal point as well as equality constraints which always have to be enforced for a multi objective optimization problem there is no single solution that simultaneously optimizes each objective a solution is called nondominated or pareto optimal if none of the objective function values can be improved without degrading some of the other objective values therefore the goal of solving the problem 1 is to find a set of pareto optimal solutions which is often called the pareto front 2 2 mo asmoch the proposed mo asmoch method extends our previously developed mo asmo method gong et al 2016a which was designed for solving continuous and bound constrained multi objective optimization problems the decision variables handled by the new mo asmoch method can be continuous variables integer variables discrete variables or a mix of them in addition the mo asmoch method can also handle problems containing model constraints which may be computationally expensive by themselves therefore the mo asmoch method aims to be more widely applicable and more flexible than the original mo asmo method to achieve those objectives we have introduced innovations in the following aspects 1 we apply different evolutionary operators to different types of variables for integer and discrete variables the sampling crossover and mutation components of the nsga ii method are modified by introducing a truncation procedure and a mapping procedure 2 we use a classification surrogate model for handling model constraints different from most of the previous studies which used surrogate models to estimate constraint function values the mo asmoch method builds a classifier for the constraints to separate feasible and infeasible solutions basically the use of a classification surrogate model to handle constraints is very rare basudhar et al 2012 beauthier et al 2017 handoko et al 2008 the purpose of doing this rather than quantifying the degree of constraint violations is to deal with two issues one is that estimation of constraint values is difficult especially when the constraint values can span several orders of magnitude the second issue is that approximation errors can make the algorithm discard infeasible solutions which are in fact feasible and can affect the dominance relationship among solutions brownlee and wright 2015 especially when approximating equality or binding constraints 3 we improve the original adaptive sampling strategy of mo asmo to balance the local exploration with the global search of the parameter space a general overview of the mo asmoch method is shown in fig 1 and the main steps are described as follows 1 initialization an initial set of design sites which are points in the decision variable space is generated using a specific design of experiment doe method generally the doe methods tend to distribute the initial design sites uniformly as a prior knowledge about the real response surface is usually unavailable there are many doe methods available however symmetric latin hypercube slh and good lattice points glp can produce the most uniformly scattered samples as suggested by gong et al 2016b to handle the integer and discrete variables we apply a truncation procedure and a mapping procedure for integer variables we round the sampled value to the nearest integer value meaning that a real value is truncated to an integer value for each discrete variable we first sort the available discrete values and then map them to an equivalent number of integer variables from 1 to the number of discrete options we can treat them as integer variables afterwards they are only transformed back to the discrete values when evaluating the objective functions and constraints with the original model assume that we generate an initial sample matrix x t d where t is the sample size x only contains continuous and integer values as the discrete variables have been mapped to integer values after the initial sample sets are generated we transform the integer values of the discrete variables to their original discrete options and get x then perform the original model simulations at these points x to obtain the corresponding objective function values y t m and constraint violations c t n k 2 surrogate model construction build the surrogate models for the objective functions and constraints with the input output pairs in our method a separate surrogate model is built for each objective function to predict continuous values while another surrogate model is constructed for binary classification of constraints there are a wide variety of approximation methods available in the literature which have been applied as surrogate models among them some methods can be used for both regression and classification such as the support vector machines svm the gaussian processes gp the random forests rf the neural networks nn gaussian processes regression gpr has been shown to outperform other methods in some cases gong et al 2015 wang et al 2014 however gpr is not necessarily the best surrogate model for every problem the mo asmoch method has the flexibility of applying any of these surrogate models for approximating the objective functions similarly several classifiers can be used as surrogate models to handle constraints using the classifier enables us to only use one classification surrogate model to separate the points as feasible and infeasible rather than building surrogate models for each constraint function thus the computational burden can be further reduced as training surrogate models can be also time consuming for each objective function i i 1 m a separate surrogate model is fitted on the initial set of design sites x and objective function values y i for constraints we set a value of 1 to a design site if all constraints are satisfied otherwise we set a value of 0 to the design site hence the constraint matrix c is converted to c t 1 then we train a classification surrogate model with x and c in this study we adopt gpr and gaussian processes classification gpc see appendix for a brief description as the default surrogate modelling methods the gp models used for surrogate based optimization usually assume that the input variables of the optimization problem are real valued in cases where the input variables take integer values the most common approach to deal with these variables is to assume that they take real values and then simply round to the closest integer before evaluating the objectives and constraints garrido merchán and hernández lobato 2020 therefore we can directly use x as input to train the surrogate models without any special treatment 3 running the nsga ii algorithm on the surrogate models now the constructed surrogate models are fully substituted for the original simulation model the nsga ii algorithm is run on the surrogate models to obtain approximate pareto optimal solutions the original nsga ii is real coded and uses the simulated binary crossover operator and polynomial mutation during the sampling crossover and mutation processes the same truncation and mapping procedures as described in the first step are applied to deal with integer and discrete variables this means that discrete variables are first mapped to integer variables after sampling crossover and mutation operations have been performed the integer restrictions are satisfied by using the truncation procedure the original nsga ii method can also solve constrained multi objective optimization problems by including a constrained domination principle deb et al 2002 the result of using this principle is that feasible solutions always have better nondomination ranks than infeasible solutions all feasible solutions are ranked according to their nondomination based on the objective function values among infeasible solutions the solution with a smaller constraint violation has a better rank in the mo asmoch method we adapt the principle to make it work on surrogate models for handling constraints as the classification surrogate model is used to separate the solutions into feasible and infeasible solutions the infeasible solutions are also ranked using the fast nondominated sort based on the objective function values ignoring the values of their constraint violations 4 adaptive sampling after running the nsga ii on the surrogate models a part of the points x is selected from the final population obtained in step 3 again transform x to x in the original decision variable space like in step 1 and then the points x are evaluated using the original model to obtain the objective functions y and constraints c the adaptive sampling strategy used in the original mo asmo method is to select a portion of the final solutions e g 20 which have the largest crowding distances this adaptive sampling strategy was constructed on the assumption that the final solutions obtained in step 3 are all nondominated solutions however the assumption could fail in many cases for example the surrogate surfaces are highly multimodal and finding the pareto front is not easy especially when the number of decision variables is large therefore only a part of the final solutions could be nondominated solutions in the proposed mo asmoch method we relax the original assumption and make some modifications let n denotes the population size of the nsga ii method and pct denotes the resampling percentage which represents the percentage of the final population selected for evaluation and updating the surrogate models if the number of nondominated solutions in the final population is larger than n pct we select n pct of them with the largest crowding distances meaning that the points that are too close to each other are discarded otherwise if the number m of nondominated solutions is less than n pct we select all of them and the remaining points are selected by maximizing the minimum distance to already evaluated points more specifically after the nondominated solutions have been selected we compute the distance from each point in the remaining solutions to all already evaluated points and find its nearest neighbour then select the points with the top n pct m largest distances to their nearest neighbours if the distance from a selected point to its nearest neighbour is smaller than a threshold e g 1e 3 we select a new point from a large number of randomly generated points in the whole decision space which is furthest away from all evaluated points with the improved adaptive sampling strategy the local exploitation by solving the approximation optimization problem and the global exploration by diversifying the design sites throughout the decision space and on the pareto front can be better balanced 5 iteration append the input decision variables output objective functions and constraints pairs selected in the last step to the original input output pool x x x y y y c c c go back to step 2 and update the surrogate models with the updated data pairs the step 2 to step 5 are repeated several times to adaptively evolve the surrogate models until convergence or stopping criteria are reached 2 3 performance metrics to comprehensively assess the performance of the proposed mo asmoch method and facilitate the comparison between it and the baseline moea nsga ii both visualization and performance metrics are used in a complementary manner the former one shows the estimated pareto front and the true or referenced pareto front directly and intuitively in the objective space while the latter provides a way to quantitatively evaluate the quality of the nondominated solutions obtained evaluating the approximate pareto optimal solutions estimated by the multi objective optimization algorithm requires the examination of convergence and diversity convergence refers to the accuracy of the achieved approximate pareto optimal points diversity represents the spread of solutions in the objective space ideally the nondominated solutions obtained are expected to lie on the true pareto front and well distributed over the entire front in this study two widely used metrics are adopted namely the generational distance veldhuizen 1999 and the hypervolume zitzler and thiele 1999 the generational distance gd measures the average distance of the approximate pareto optimal points from the true or referenced pareto front and is defined as 2 g d i 1 n d i 2 1 2 n where n is the number of the approximate pareto optimal points and d i is the euclidean distance in objective space between the i th optimal point and the nearest member of the true pareto front fig 2 a shows a schematic of the gd calculated for a two objective minimization problem the ideal value of gd is 0 the hypervolume hv measures the volume enclosed by the union of the polytopes p 1 p 2 p n in the objective space where n is the number of the nondominated solutions obtained and each p i is formed by the solution point an additional selected reference point and the intersection of the hyperplanes arising out of that solution point along with the axes the shaded area shown in fig 2b is the hv of the approximate pareto optimal solutions with respect to the reference point for a two objective minimization problem in this study we calculate the ratio of the hv of the approximate pareto optimal solutions to that of the solutions in the true or referenced pareto front which is denoted as rhv the ideal value of rhv is 1 3 benchmark problems in this study we test the performance of the mo asmoch method on six benchmark problems due to the complexity of the multi objective hybrid optimization problems it is impractical to derive the pareto front analytically therefore synthetic test functions with true pareto front are not available for this kind of problems alternatively we resort to use real world multi objective optimization problems among the six benchmark problems three of which are engineering design problems from a recently introduced multi objective optimization problem suite tanabe and ishibuchi 2020 the other three are wds design problems selected from a library of benchmark design problems of wds wang et al 2015 table 1 gives a summary of the six problems the three engineering design problems were originally constrained single objective real world problems however the constraint functions of the original problems can be simultaneously minimized so tanabe and ishibuchi 2020 formulated the sum of constraint violation values as an additional objective function and the newly formed multi objective problems have no constraint functions for example re1 is the reinforced concrete beam design problem the first objective of the problem as shown in table 1 is to minimize the total cost of concrete and reinforcing steel of the beam the second objective is the sum of the two constraint violations in re1 the three decision variables x 1 x 2 and x 3 represent the area of the reinforcement the width of the beam and the depth of the beam respectively x 1 has a predefined discrete value from 0 2 to 15 and the other two are continuous variables since the real pareto front is unknown the best known approximation of the true pareto front was obtained by running five moeas nsga ii moea d tch moea d pbi ibea and nsga iii for a huge number of objective function evaluations the number of function evaluations was set to 100 000 and 31 independent runs were performed therefore the approximated pareto fronts are used as references in our study please refer to tanabe and ishibuchi 2020 for more details the three wds design problems include a hypothetical network the two loop network tln and two real networks the new york tunnel nyt network and the goyang goy network the library of benchmark design problems of wds was built by collecting various benchmark problems from the literatures each problem in this library is formulated to minimize the total cost associated with pipe components and to maximize the network resilience the mathematical expression of each objective is given as follows 3 min c i 1 n p u c d i l i where c is the total cost monetary units problem dependent np is the number of pipes u c is the unit pipe cost depending on the diameter selected in a specific problem d i is the diameter of pipe i l i is the length of pipe i 4 max i n j 1 n n c j q j h j h j r e q k 1 n r q k h k i 1 n p u p i γ j 1 n n q j h j r e q 5 c j i 1 n p j d i n p j max d i where i n is the network resilience n n is the number of demand nodes c j q j h j and h j r e q are uniformity demand actual head and minimum head of node j n r is the number of reservoirs q k and h k are discharge and actual head of reservoir k n p u is the number of pumps p i is the power of pump i if any γ is the specific weight of water n p j is the number of pipes connected to node j d i is the diameter of pipe i connected to demand node j the variables mentioned for evaluation of network resilience are obtained by running the hydraulic simulation using the epanet 2 software rossman 2000 these wds design problems have explicit constraints which are related to the hydraulic performance of a network for example the nodal pressures at demand nodes and flow velocities in pipes the constraint violations must be computed during the hydraulic simulation and hence are computationally expensive the complexity of these benchmark problems is different according to the size of search space for relatively low dimension problems e g tln used in our study the true pareto front can be obtained through full enumeration for high dimension problems wang et al 2015 ran five moeas amalgam borg nsga ii ε moea and ε nsga ii for a very large number of evaluations up to 2 000 000 for large problems to find the best known approximation to the true pareto front which is similar to tanabe and ishibuchi 2020 the true pareto front of tln and the best known pareto fronts of nyt and goy are used as references in our study interested readers can refer to wang et al 2015 for more details 4 results 4 1 engineering design problems to evaluate the effectiveness and efficiency of the proposed mo asmoch method we compare it with the embedded nsga ii method which is arguably the most popular moea and is also regarded as an industry standard method considering the referenced pareto front of each problem is the best known approximation to the true pareto front obtained by running five moeas for a very large number of evaluations it is no need to use nsga ii with the same large number of model evaluations for comparison to best assess how efficient or effective the mo asmoch method is in comparison with nsga ii we compare the two methods given the same limited computing budget in model evaluations in addition we also compute the efficiency gains which is recommended by razavi et al 2012 to do this we quantify the numbers of original model evaluations the mo asmoch and nsga ii require to obtain the approximate pareto optimal solutions with nearly the same quality after determining the setup of the mo asmoch method we gradually increase the number of model evaluations for nsga ii to get the same quality of nondominated solutions obtained to perform valid comparisons each method is repeated 10 times the final setups of the two methods for the three engineering design problems are shown in table 2 for mo asmoch the population size and number of generations of the embedded nsga ii are set to 100 and the resampling percentage is set to 20 the glp with ranked gram schmidt rgs de correlation is used as the doe method and the default gpr is used as the surrogate model in the mo asmoch approach which follows the recommendations of gong et al 2016a 2016b we have also tested other doe methods and surrogate model types and the results also demonstrate that the selections are appropriate as this study does not focus on the intercomparison of different doe methods and surrogate models we do not elaborate the results here the multi objective optimization results of the three problems are shown in fig 3 and table 3 fig 3 presents a randomly selected set of approximate pareto optimal solutions from the 10 replicates for the mo asmoch and nsga ii respectively table 3 shows the mean and standard deviation of the gd and rhv calculated from the 10 replications of the two methods as illustrated in fig 3 basically the mo asmoch method can perfectly reach the referenced pareto front which is the best known approximation of the true pareto front for the re1 problem there are a few points lying slightly away from the referenced pareto front while all the approximate pareto optimal points are well distributed along the referenced pareto front for the re2 and re3 problems we can see that the referenced pareto fronts are disconnected but the approximate pareto optimal points obtained with mo asmoch are located almost exactly on the referenced pareto fronts fig 3 shows that the mo asmoch method obtains significantly better sets of nondominated objective vectors than the ones obtained by nsga ii table 3 shows that the gd and rhv obtained by mo asmoch are much better than those obtained by nsga ii given the same computational budget moreover the standard deviations of the two metrics obtained by nsga ii is significantly larger than those obtained by mo asmoch indicating the large randomness of nsga ii given a small number of model simulations after increasing the computational budget for nsga ii it can be seen that the diversity of the nondominated solutions obtained by mo asmoch is slightly worse than that of nsga ii large especially for the re3 problem from table 3 we can further know that mo asmoch does a very good job in achieving convergence but performs slightly worse in representing the diversity of the pareto front which is indicated by the rhv metric generally the two metrics of mo asmoch and nsga ii large with the setups in table 2 are very close indicating the quality of nondominated solutions obtained with the two methods is similar in this case we can know that the computational savings achieved through using the mo asmoch method range from 75 to 92 of original model evaluations in addition the standard deviations of the two metrics are very small meaning the conclusions on the effectiveness and efficiency of the proposed method with the current setups are valid 4 2 wds design problems the above three engineering design problems do not contain constraints in this subsection we evaluate the mo asmoch method based on three wds design problems which are constrained multi objective discrete combinatorial optimization problems the dimensionalities of the three wds design problems 8 21 30 are also higher than the engineering design problems the tln problem is repeated 10 times while the other two problems are repeated 5 times as training the surrogate model is time consuming when the number of decision variables is large again we use the same strategy as used in section 4 1 to assess the effectiveness and efficiency of the mo asmoch method the setups of the two methods for the three wds problems are shown in table 4 for these wds design problems we find that the gpr still performs the best among a variety of surrogate model types and the default gpc performs well for the initialization step we find that using already evaluated points in the early generations of nsga ii rather than initial points from a formal doe to initially build the surrogate model can obtain better optimization results for example fig 4 shows the obtained pareto fronts of using the two different initialization modes for the tln problem for both initialization modes 700 design sites are used to build the initial surrogate models the 700 design sites of fig 4a are generated using the glp with rgs de correlation method while the 700 design sites of fig 4b are the last generated 700 points of the nsga ii method with population size of 50 and 20 generations clearly the approximate pareto optimal solutions in fig 4b have better diversity than those in fig 4a although they achieve good convergence the search space is generally very large for the wds design problems with many pipes and available pipe diameter options as such the number of design sites required to uniformly cover the decision variable space could be extremely large especially for higher dimensional problems theoretically a sufficiently large uniformly distributed initial set of design sites is required to construct a surrogate model which can well represent the original response surface however the more design sites used for surrogate model constructing the higher the computational time required in this process which could largely reduce the computational efficiency of the surrogate based method many studies behzadian et al 2009 jourdan et al 2006 yan and minsker 2011 which follow the metamodel embedded evolution framework typically did not incorporate formal does instead they used points produced in the early generations of the evolutionary algorithm to build the initial surrogate model although it is argued razavi et al 2012 that the surrogate model fitted on the initial design sites collected from previous optimization results may not have global accuracy and could be misleading we believe that the surrogate models can be more accurate in a reduced search space by running the nsga ii for a few generations for the nyt and goy problems we also run nsga ii with population size of 50 and 20 generations and then use the last generated 700 points as the initial input of the mo asmoch method different from the tln problem mo asmoch fails to reach the referenced pareto front for the nyt and goy problems as illustrated in figs 5 and 6 for the nyt problem although the approximate pareto front obtained with mo asmoch is not close to the best known pareto front which is obtained by running five moeas with 600000 original model evaluations it is close to the approximate pareto front obtained by running nsga ii with 20000 model evaluations in addition the approximate pareto front of mo asmoch has relatively good diversity it can effectively cover the region with the network resilience between 0 45 and 0 72 but it cannot cover the region with the network resilience less than 0 45 and larger than 0 72 as the referenced pareto front does it for the goy problem the distances between the approximate pareto optimal solutions of mo asmoch and the referenced pareto front are small which is also indicated in table 5 these approximate pareto optimal solutions are reported to have total cost between 0 18 and 0 22 million on average which only cover the region with the network resilience less than 0 65 even if these solutions are not entirely satisfactory from an engineering design standpoint lack of diversity they are still of engineering interest since saving infrastructure capital and operational costs is considered as the primary task in wds design problems with the setups in table 4 both the convergence and diversity metrics of mo asmoch are significantly better than those of nsga ii indicating that the mo asmoch algorithm is as effective but more efficient than the nsga ii method given the same computational budget in fact the nondominated objective vectors obtained by mo asmoch dominate all the nondominated objective vectors obtained by nsga ii in addition the performance metrics of mo asmoch and nsga ii large are very similar as shown in table 5 therefore we can know that the efficiency gains achieved through using the mo asmoch method range from 73 to 90 of original model evaluations 4 3 impact of the mo asmoch setup on optimization results the setup of the mo asmoch method may have significant influence on the optimization performance based on the experience from the mo asmo method gong et al 2016a therefore we evaluate the impact of initial sample size resampling percentage pct and number of iterations iter on optimization result in this subsection when the asmo method was developed wang et al 2014 various values of initial sample size were compared and it came to a conclusion that 15 20 times of the dimension of the problem may be the proper initial sample size however this rule of thumb may not be suitable for other practical problems we generate the initial sample points with different sample sizes using the glp with rgs de correlation method to test the impact of initial sample size on optimization results we start with the initial sample size of 10 times of the dimension and increase the initial sample size by 5 times each time for each initial sample size the pct is set to 0 2 and the iter is set to 5 we find that it is until 30 times that the optimization results are satisfactory when the initial sample size is less than 30 times of the dimension the optimization performance of mo asmoch is poor not shown even for the three relatively simple engineering design problems we want to test if further increasing the initial sample size could improve the optimization performance the two performance metrics with different initial sample sizes for the six test problems are shown in table 6 for the three engineering design problems and the tln problem the mo asmoch method is run for 10 trials for the nyt and goy problems the mo asmoch method is run for 5 trials for relatively simple engineering design problems further increasing the initial sample size could not guarantee the improvement in optimization performance instead the metrics may become slightly worse since they have already been good enough for the three wds design problems further increasing the initial sample size improves both convergence and diversity metrics this is easy to understand since a sufficiently large initial set of design sites is required to construct surrogate models which can well represent the original response surface for high dimensional problems however training the surrogate models become very time consuming when the sample size is large the computational expense associated with surrogate model fitting and the increased model evaluations can directly affect the efficiency of the optimization algorithm consequently the proper initial sample size may be 30 times the number of decision variables the users can increase the initial sample size for high dimensional problems if having adequate computational budget when dealing with computationally expensive problems in practice we would be able to do only a limited number of model evaluations so the number of iterations iter can be determined by computational budget or the total number of model evaluations the initial sample size the population size pop of the embedded nsga ii and the resampling percentage pct generally pop should be large enough to ensure the embedded nsga ii can work well on surrogate models and thus it is usually set to 100 therefore the users only need to specify pct after determining the initial sample size given the limited computational budget we evaluate the impact of pct by comparing the optimization results obtained with different pct values for the six test problems the value of pct is set to 0 1 0 2 0 5 and 1 0 respectively to give a fair comparison the total number of model evaluations is fixed for each test problem therefore the variations of the performance metrics with the pct values can be illustrated as figure 8 in gong et al 2016a similarly our results not shown also indicate the impact of pct is problem dependent for the three engineering problems the influence on the performance metrics is complicated both performance metrics are best when pct is set to 0 1 and 0 2 for re2 and re3 respectively for re1 the performance metrics are best with pct equalling to 1 0 for two of the three wds design problems nyt and goy overall gd increases with the increase of pct whereas rhv decreases with the increase of pct for the tln problem the performance metrics are best when pct is set to 0 5 consequently a small to medium value of pct such as 0 1 or 0 2 is recommended which is also same as gong et al 2016a suggested 5 discussion the proposed mo asmoch method has several new features which includes introducing a truncation procedure and a mapping procedure to the surrogate based optimization process building a separate classification surrogate model for the constraints and modifying the adaptive sampling strategy with these new features mo asmoch can handle constrained hybrid multi objective optimization problems which require computationally expensive model simulations in this section we want to discuss how these new features affect the optimization performance the classification surrogate model is introduced to separate the solutions into feasible and infeasible solutions so the accuracy of the classifier would directly affect the algorithm s performance in optimization we conduct an experiment to show the accuracy of the classification surrogate when used in mo asmoch for the three wds design problems the setups of mo asmoch are same as those in table 6 with initial sample size equalling to 30 times of the number of decision variables after each iteration of constructing the classification surrogate model we perform the 5 fold cross validation and calculate the accuracy scores which measures the fraction of correct predictions the correct prediction means that the set of labels predicted for a sample must exactly match the corresponding true set of labels fig 7 shows the accuracy scores of the default gpc of each iteration for the three wds design problems it is obvious that gpc performs pretty well in all three problems the worst accuracy scores which occur in the nyt problem still exceed 0 8 and the accuracy scores for the other two problems all exceed 0 9 the results demonstrate the effectiveness of using a classifier to separate the solutions for the nyt problem we further find that the accuracy scores of the multi layer perceptron classifier mlpc are better than those of gpc after replacing gpc by mlpc in the mo asmoch method both the gd and rhv improve a little as shown in table 7 a in addition table 7b also shows the impact of the resampling strategy on the optimization performance as described in section 2 2 the assumption of the adaptive sampling strategy used in mo asmo could fail in some cases for example the tln problem in our study the modified version is more reasonable and using it in mo asmoch makes the performance metrics improved it should be noted that the modified adaptive sampling strategy becomes the original one when the assumption does not fail the accuracy of the surrogate model could greatly affect the optimization results which have been stated in many previous studies since the classification surrogate is fairly accurate we want to further test how much the performance would degrade if the surrogate accuracy is low similarly we also perform the 5 fold cross validation to evaluate the accuracy of the surrogate models taking the tln problem as an example the coefficients of determination of the initially fitted gpr models for the two objective functions are shown in fig 8 despite the accuracy of the surrogate model of the second objective function is relatively low the nondominated solutions obtained are very close to the true pareto front when the default gpr model is replaced by the mlp model the accuracy significantly deteriorates as a result the approximate pareto optimal solutions obtained by the mlp based method are poor in both convergence and diversity and they are totally dominated by the approximate pareto optimal solutions obtained by the gpr based method therefore we need to carefully select the surrogate model before using the algorithm unlike the multi objective continuous optimization problems where there are many standard benchmark problems with analytic solutions there are no such standard benchmarks for multi objective mixed variable optimization problems previous studies on multi objective mixed variable problems were often limited to specific applications in which assessing the performance of algorithms relies on intercomparison the 6 test problems used in this study were collected from two benchmark suites tanabe and ishibuchi 2020 wang et al 2015 which have provided the best known pareto front of each benchmark problem therefore we can better assess the effectiveness and efficiency of the proposed method through comparison with the referenced pareto fronts despite having used 6 test problems we admit that the evaluation is still limited but it does not mean that our proposed method is only suitable for the two kinds of problems the mo asmoch method is able to handle problems with continuous integer and ordinal discrete variables in fact the algorithm can be simply modified to handle categorical variables whose values do not possess a natural ordering in this case a popular technique to account for these variables in surrogate modelling is to use a one hot encoding of the categorical variable garrido merchán and hernández lobato 2020 it means that the input of the surrogate model corresponding to a categorical variable is replaced by additional real valued variables one variable per category for example assume that a decision variable is categorical and contains a set of finite categories c red green yellow this variable is replaced with three variables that may take values 1 0 0 0 1 0 and 0 0 1 corresponding to the categories in c then after running the nsga ii on the surrogate models the largest extra variable is set to one and the other two are set equal to zero because there are no such test problems involving categorical variables as the ones used in our study which have the best known approximation to the true pareto front we did not include categorical variables in this study we will test the mo asmoch method on real world problems with categorical variables in our future studies we believe our method is flexible enough to be combined with different simulation models and used in a variety of real world applications compared to previous surrogate based algorithms which can only handle some of the features multi objective mixed variables computationally expensive constraints the proposed mo asmoch method has the following advantages 1 mo asmoch is a surrogate enabled method rather than a surrogate embedded evolutionary method like in behzadian et al 2009 brownlee and wright 2015 jourdan et al 2006 it means that our method is non intrusive and is compatible with many other moeas or multi objective optimizers 2 mo asmoch is compatible with different initialization methods like formal doe methods and using solutions from previous optimization attempts in addition there is no fixed surrogate model type in mo asmoch as different surrogate modelling methods have their own merits for example radial basis function rbf interpolation and artificial neural network ann are the most commonly used surrogate model types in surrogate based optimization studies but we found that using gpr in mo asmoch can produce better optimization results than using ann and rbf in our study despite training gpr is more time consuming especially when the number of design sites is large 3 using a classification surrogate model to classify the constraints as satisfied and not satisfied is easy and has been proved to be accurate to accommodate to the classification surrogate the constrained domination principle of nsga ii is modified by ignoring the magnitude of constraint violations when sorting infeasible solutions although this may cause nsga ii to wander in the infeasible search region for more generations before reaching the feasible region there is no need to worry about the computational burden of running nsga ii with many generations on the surrogate models hence the equality constraints can also be handled despite the advantages there is still room to improve the mo asmoch method it has been widely recognized that the nondominated solutions obtained with the surrogate based optimization methods provide only an approximation of the pareto front and mo asmoch is no exception the convergence and the diversity of the obtained non dominated solutions mainly depend on the performance of the surrogate models if the surrogate models have low accuracy the obtained nondominated solutions could be far from the real pareto front and it is also difficult to capture the whole region of the real pareto front just as shown in fig 8 in this study we also found the quality of the obtained nondominated solutions deteriorates as the dimension of the benchmark problem increases which is mainly reflected as the reduced diversity of the optimal solutions actually we have also tested the fossolo network problem with 58 discrete variables from the wds benchmark suite the obtained nondominated solutions not shown are close to the referenced pareto front but they can only cover a small region toward the left tail of the pareto front gaussian process models are struggling to effectively model high dimensional input output maps zhu et al 2019 and the problem exists in almost all other surrogate modelling approaches razavi et al 2012 the surrogate model built with a limited number of samples might not well represent the original response surface thus missing certain regions of the function space in addition high dimensional problems have an extremely large search space thus the number of design sites needed to reasonably cover the decision space becomes very large training a surrogate model with a large number of samples can be very time consuming and obtaining these samples which requires the equal number of original model evaluations further increases the computational burden therefore surrogate modelling becomes less attractive or even infeasible when dealing with high dimensional problems a possible way to tackle or mitigate this problem is to develop or use advanced machine learning models which excel at modelling high dimensional input output maps liu et al 2017 xi et al 2020 zhu et al 2019 another promising way is to further improve the adaptive sampling strategy so that it can automatically detect and collect enough design sites from the rugged regions we will follow these paths to improve our method for better handling high dimensional optimization problems we would like to investigate the performance of our method in combination with different types of multi objective algorithms furthermore we would like to apply the method to different kinds of real world applications 6 conclusions in this study we developed the novel mo asmoch method for solving computationally expensive multi objective optimization problems whose decision variables can be continuous integer discrete or a mixture of them these problems may also contain model constraints that may require computationally expensive simulation although there are quite a few surrogate based methods which can handle optimization problems with some of these features e g multi objective hybrid variables computationally expensive constraints the optimization problems with all of the three features have been rarely touched by those surrogate modelling based methods to achieve these goals we have extended our previously developed mo asmo method by introducing a truncation procedure and a mapping procedure to the embedded nsga ii building a separate classification surrogate model for the constraints modifying the adaptive sampling procedure to incorporate a global sampling strategy in contrast to previous methods that use the surrogate model to approximate the constraint function value we use a classifier which is built on previously evaluated solutions to separate a new solution to feasible or infeasible therefore there is no need to build surrogate models for each constraint and the equality or binding constraints can be also coped with we compared the mo asmoch method and its embedded nsga ii on 6 problems including three engineering design problems without constraints and three wds design problems with constraints the comparisons were focused on the convergence and diversity of the pareto optimal solutions obtained with the two optimization methods the results show that mo asmoch obtained much better nondominated objective vectors than nsga ii given the same computational budget moreover our proposed method obtained similar nondominated solutions with nsga ii but requires only 5 27 of the original model evaluations used by nsga ii overall the nondominated solutions obtained by mo asmoch have good convergence but are not so widely dispersed compared to the referenced pareto front considering the referenced pareto fronts are obtained by running different moeas for a huge number of model evaluations the evaluation of mo asmoch has actually incorporated the comparisons with other moeas rather than just nsga ii hence the proposed mo asmoch method is very promising for solving constrained multi objective mixed variable optimization problems where the objective and constraint function evaluations are computationally expensive software and data availability the source code of the mo asmoch method which is developed in python language is available at https github com getred mo asmoch the test data of the three engineering design problems are available at https github com ryojitanabe reproblems and the test data of the three wds design problems are available at http www exeter ac uk benchmarks pareto fronts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was jointly supported by the strategic priority research program of the chinese academy of sciences xda2006040104 the national natural science foundation of china 42101046 51979004 and the china postdoctoral science foundation 2019m661714 we really appreciate dr wei gong at beijing normal university for providing the source code of mo asmo and providing constructive comments we also thank the other three reviewers for their constructive and valuable comments which helped to improve the manuscript appendix gaussian process classification here in the appendix we give a brief introduction of gpc which is the default classifier used in the mo asmoch method the gpc implements gaussian processes for probabilistic classification where test predictions take the form of class probabilities so we first review using linear models for binary classification which form the foundation of gpc models for binary classification the labels y 1 and y 0 are usually used to distinguish the two classes the idea for the binary discriminative case is to turn the output of a regression model into a class probability using a response function one of the most popular learning algorithms for classification is the logistic regression which combines the linear model with the logistic function a1 p y x g x τ w g z 1 1 exp z where x is the input vector and y is the class label w is the weight vector and g z is the logistic function as the probability of the two classes must sum to 1 it is obvious that p y 0 x w 1 p y 1 x w given the training data x y the prediction for a test point x is obtained as follows a2 p y 1 x d p y 1 w x p w d d w which integrates the prediction p y 1 w x g x τ w over the posterior distribution of weights the basic idea behind gpc is simple it places a gp prior on a latent function f which is then squashed through a link function logistic function to obtain the probabilistic classification more specifically the linear function f x x τ w from the linear logistic model is replaced by a gaussian process and correspondingly the gaussian prior on the weights is replaced by a gp prior the values of f are not observed and we are also not interested in the values of f but rather in π where π x p y 1 x g f x the probabilistic prediction for a test point x is obtained with two steps a3 p f x y x p f x x f p f x y d f a4 π p y 1 x y x g f p f x y x d f where p f x y p y f p f x p y x is the posterior over the latent variables in the gpr case computation of predictions is straightforward since the relevant integrals are gaussian and could be computed analytically for classification the posterior of the latent function f is not gaussian even for a gp prior because of the non gaussian likelihood thus making the integral analytically intractable therefore gpc approximates the non gaussian posterior with a gaussian based on the laplace approximation the posterior mean and variance for f are express as a5 e f x y x k x x τ k x x 1 f ˆ a6 v f x y x k x x k x x τ k x x w 1 1 k x x where k is the covariance function f ˆ is the one that maximizes p f x y and f ˆ k x x log p y f ˆ w log p y f the detailed implementations for finding f ˆ and calculating w can be found in chapter 3 of rasmussen and williams 2006 
25665,many multi objective optimization problems in integrated environmental modelling and management involve not only continuous decision variables but also variables like integers and or discrete variables furthermore the optimization problems are often subject to various constraints solving this kind of constrained hybrid problems usually requires a huge number of model evaluations that can be computationally expensive this study presents an algorithm known as multi objective adaptive surrogate modelling based optimization for constrained hybrid problems mo asmoch it incorporates several evolutionary operators to handle different types of decision variables and uses a classification surrogate model to deal with model constraints mo asmoch was evaluated against the widely used nsga ii method on three engineering design problems and three water distribution system design problems with up to 30 dimensions the results showed that mo asmoch is able to obtain nondominated solutions of similar quality as that of nsga ii using much fewer model evaluations keywords multi objective optimization surrogate model constrained hybrid problem nsga ii mo asmo 1 introduction many optimization problems in integrated modelling and management of environmental systems involve not only continuous decision variables but also discrete ones this kind of problems are known as hybrid or mixed variable optimization problems many examples of such problems come from the design of water distribution systems cunha and marques 2020 wang et al 2017 environmental and watershed management ahmadi et al 2013 geng et al 2019 groundwater remediation and others li and hilton 2007 shourian and davoudi 2017 some decision variables in those problems only take on integer values or a specific selection from a set of options for instance categorical variables or variables describing components with standard values in fact hybrid optimization problems are very common in various fields such as engineering design process industry and management science deep et al 2009 the hybrid nature of the decision variables increases the complexity of the search space and the difficulty of finding the optimal solutions wang et al 2021 therefore solving these problems is challenging and often accompanied by a large number of computationally expensive computer model simulations in practice many of the above mentioned optimization problems are inherently multi objective and several disparate and often conflicting criteria have to be considered simultaneously generally in multi objective optimization there is no such a solution that can optimize all objective functions at the same time instead we are looking for trade off or pareto optimal solutions to solve this kind of problems multi objective evolutionary algorithms moeas seem to be the most promising and have attracted a lot of attention over the recent several decades wang et al 2017 such as the nondominated sorting genetic algorithm ii nsga ii deb et al 2002 the nsga iii deb and jain 2013 the pareto archived evolution strategy paes knowles and corne 1999 the strength pareto evolutionary algorithm 2 spea2 zitzler et al 2001 moea based on decomposition moea d zhang and li 2007 although most of the algorithms were originally developed for continuous variable optimization problems they can be suitably modified and extended to handle integer and discrete variables for example since the genetic algorithm can handle different variable types by modifying the sampling crossover and mutation components the nsga ii method arguably the most popular moea has been widely applied to various fields for solving multi objective optimization problems in the presence of discrete variables ahmadi et al 2013 bre and fachinotti 2017 oraei zare et al 2012 wang et al 2015 however when compared to its single objective counterpart multi objective optimization methods suffer from high computational cost basically they require hundreds of thousands of simulation runs to identify the pareto optimal solutions if the computer simulation model is computationally expensive to run the computational burden of using these moeas would become intractable surrogate models which are cheap to run statistical or empirical data driven models can be used to emulate the response surfaces of expensive simulation models therefore many surrogate modelling based optimization methods which use the predictions of the surrogate model to guide the search for promising solutions müller 2016 have been developed and widely used in a variety of fields including environmental modelling and water resources management razavi et al 2012 not surprisingly surrogate models have also been used in combination with various moeas to identify the pareto optimal front with significantly reduced number of original model evaluations for instance nain and deb 2005 integrated the nsga ii with artificial neural network ann and developed the method nsga ii ann there are also many similar surrogate enabled multi objective optimization algorithms akhtar and shoemaker 2016 datta and regis 2016 jourdan et al 2006 kunakote and bureerat 2013 müller 2017 however most of the surrogate based multi objective algorithms have been developed by fitting the response surface surrogates on continuous decision variables there is only a limited number of studies featuring surrogates in the discrete decision space for multi objective problems behzadian et al 2009 brownlee and wright 2015 castelletti et al 2010 di pierro et al 2009 compared to those for single objective problems the recent development of surrogate assisted optimization methods has shown an increasing interest in addressing discrete multi objective optimization problems gu et al 2021a 2021b regis 2021 there still could be room for improvement to make the methods flexible to handle problems with different kinds of variables rather than continuous or discrete variables alone furthermore not a few of these surrogate based multi objective optimization algorithms for discrete problems were not able to handle various model constraints manson et al 2021 whose calculation can be computationally expensive by themselves generally for single objective problems many algorithms deal with model constraints by incorporating them into the objective function via penalty terms and using surrogate models to approximate the modified objective function regis 2011 however the adjustment of the penalty factor is a delicate task müller and woodbury 2017 which could impact the surrogate modelling procedure alternatively surrogate models can be directly used to approximate expensive constraint functions however this way of handling constraints has only been adopted in limited multi objective surrogate based optimization methods brownlee and wright 2015 datta and regis 2016 de winter et al 2021 hussein and deb 2016 regis 2016 singh et al 2014 considering all the points discussed above it is known that there is a lack of research on surrogate modelling based methods that can simultaneously address hybrid optimization problems with various kinds of variables multiple computationally expensive objective functions and model constraints therefore we develop a method called the multi objective adaptive surrogate modelling based optimization for constrained hybrid problems mo asmoch the new mo asmoch method is built from our previously proposed mo asmo method gong et al 2016a which was originally designed to solve continuous optimization problems without considering model constraints except for parameter ranges in this mo asmoch method a truncation procedure and a mapping procedure are used along with the real coded nsga ii to handle integer and discrete variables separate surrogate models are used to approximate different objective functions and model constraints in addition we have also introduced a new adaptive sampling strategy on the original basis to balance the local exploration with the global search of the sample points the rest of the article is organized as follows section 2 gives a detailed description of the proposed mo asmoch method section 3 introduces the test problems including three real world engineering design problems and three water distribution system wds design problems the results showing the performance of the proposed method and its comparison with the nsga ii method are presented in section 4 section 5 provides some discussion and section 6 gives a summary of the paper 2 methodology 2 1 optimization problem description formally this paper is focused on solving the following multi objective optimization problem 1 min f x f 1 x f m x s t g x g 1 x g n x 0 h x h 1 x h k x 0 x i d i ℝ i 1 d where x is a vector with d dimensional decision variables and x i is the i th variable which could be continuous integer or discrete d i is the parameter range if x i is a continuous variable otherwise d i is the finite set of the possible integer or discrete values f x denotes m objective functions g x denotes n inequality constraints and h x denotes k equality constraints assume that they are black box and obtained from a computationally expensive simulation using the original model the objectives are usually conflicting and solving the problem 1 needs to minimize all the objectives simultaneously in the feasible region where the solutions must satisfy all conditions imposed by the constraints generally most of the constraint functions of the optimization problems in the literature are inequality constraints which can be always defined as g x 0 thus a solution is considered as infeasible if at least one constraint violation is larger than zero in addition to the inequality constraints there are also binding constraint if an inequality constraint holds with equality at the optimal point as well as equality constraints which always have to be enforced for a multi objective optimization problem there is no single solution that simultaneously optimizes each objective a solution is called nondominated or pareto optimal if none of the objective function values can be improved without degrading some of the other objective values therefore the goal of solving the problem 1 is to find a set of pareto optimal solutions which is often called the pareto front 2 2 mo asmoch the proposed mo asmoch method extends our previously developed mo asmo method gong et al 2016a which was designed for solving continuous and bound constrained multi objective optimization problems the decision variables handled by the new mo asmoch method can be continuous variables integer variables discrete variables or a mix of them in addition the mo asmoch method can also handle problems containing model constraints which may be computationally expensive by themselves therefore the mo asmoch method aims to be more widely applicable and more flexible than the original mo asmo method to achieve those objectives we have introduced innovations in the following aspects 1 we apply different evolutionary operators to different types of variables for integer and discrete variables the sampling crossover and mutation components of the nsga ii method are modified by introducing a truncation procedure and a mapping procedure 2 we use a classification surrogate model for handling model constraints different from most of the previous studies which used surrogate models to estimate constraint function values the mo asmoch method builds a classifier for the constraints to separate feasible and infeasible solutions basically the use of a classification surrogate model to handle constraints is very rare basudhar et al 2012 beauthier et al 2017 handoko et al 2008 the purpose of doing this rather than quantifying the degree of constraint violations is to deal with two issues one is that estimation of constraint values is difficult especially when the constraint values can span several orders of magnitude the second issue is that approximation errors can make the algorithm discard infeasible solutions which are in fact feasible and can affect the dominance relationship among solutions brownlee and wright 2015 especially when approximating equality or binding constraints 3 we improve the original adaptive sampling strategy of mo asmo to balance the local exploration with the global search of the parameter space a general overview of the mo asmoch method is shown in fig 1 and the main steps are described as follows 1 initialization an initial set of design sites which are points in the decision variable space is generated using a specific design of experiment doe method generally the doe methods tend to distribute the initial design sites uniformly as a prior knowledge about the real response surface is usually unavailable there are many doe methods available however symmetric latin hypercube slh and good lattice points glp can produce the most uniformly scattered samples as suggested by gong et al 2016b to handle the integer and discrete variables we apply a truncation procedure and a mapping procedure for integer variables we round the sampled value to the nearest integer value meaning that a real value is truncated to an integer value for each discrete variable we first sort the available discrete values and then map them to an equivalent number of integer variables from 1 to the number of discrete options we can treat them as integer variables afterwards they are only transformed back to the discrete values when evaluating the objective functions and constraints with the original model assume that we generate an initial sample matrix x t d where t is the sample size x only contains continuous and integer values as the discrete variables have been mapped to integer values after the initial sample sets are generated we transform the integer values of the discrete variables to their original discrete options and get x then perform the original model simulations at these points x to obtain the corresponding objective function values y t m and constraint violations c t n k 2 surrogate model construction build the surrogate models for the objective functions and constraints with the input output pairs in our method a separate surrogate model is built for each objective function to predict continuous values while another surrogate model is constructed for binary classification of constraints there are a wide variety of approximation methods available in the literature which have been applied as surrogate models among them some methods can be used for both regression and classification such as the support vector machines svm the gaussian processes gp the random forests rf the neural networks nn gaussian processes regression gpr has been shown to outperform other methods in some cases gong et al 2015 wang et al 2014 however gpr is not necessarily the best surrogate model for every problem the mo asmoch method has the flexibility of applying any of these surrogate models for approximating the objective functions similarly several classifiers can be used as surrogate models to handle constraints using the classifier enables us to only use one classification surrogate model to separate the points as feasible and infeasible rather than building surrogate models for each constraint function thus the computational burden can be further reduced as training surrogate models can be also time consuming for each objective function i i 1 m a separate surrogate model is fitted on the initial set of design sites x and objective function values y i for constraints we set a value of 1 to a design site if all constraints are satisfied otherwise we set a value of 0 to the design site hence the constraint matrix c is converted to c t 1 then we train a classification surrogate model with x and c in this study we adopt gpr and gaussian processes classification gpc see appendix for a brief description as the default surrogate modelling methods the gp models used for surrogate based optimization usually assume that the input variables of the optimization problem are real valued in cases where the input variables take integer values the most common approach to deal with these variables is to assume that they take real values and then simply round to the closest integer before evaluating the objectives and constraints garrido merchán and hernández lobato 2020 therefore we can directly use x as input to train the surrogate models without any special treatment 3 running the nsga ii algorithm on the surrogate models now the constructed surrogate models are fully substituted for the original simulation model the nsga ii algorithm is run on the surrogate models to obtain approximate pareto optimal solutions the original nsga ii is real coded and uses the simulated binary crossover operator and polynomial mutation during the sampling crossover and mutation processes the same truncation and mapping procedures as described in the first step are applied to deal with integer and discrete variables this means that discrete variables are first mapped to integer variables after sampling crossover and mutation operations have been performed the integer restrictions are satisfied by using the truncation procedure the original nsga ii method can also solve constrained multi objective optimization problems by including a constrained domination principle deb et al 2002 the result of using this principle is that feasible solutions always have better nondomination ranks than infeasible solutions all feasible solutions are ranked according to their nondomination based on the objective function values among infeasible solutions the solution with a smaller constraint violation has a better rank in the mo asmoch method we adapt the principle to make it work on surrogate models for handling constraints as the classification surrogate model is used to separate the solutions into feasible and infeasible solutions the infeasible solutions are also ranked using the fast nondominated sort based on the objective function values ignoring the values of their constraint violations 4 adaptive sampling after running the nsga ii on the surrogate models a part of the points x is selected from the final population obtained in step 3 again transform x to x in the original decision variable space like in step 1 and then the points x are evaluated using the original model to obtain the objective functions y and constraints c the adaptive sampling strategy used in the original mo asmo method is to select a portion of the final solutions e g 20 which have the largest crowding distances this adaptive sampling strategy was constructed on the assumption that the final solutions obtained in step 3 are all nondominated solutions however the assumption could fail in many cases for example the surrogate surfaces are highly multimodal and finding the pareto front is not easy especially when the number of decision variables is large therefore only a part of the final solutions could be nondominated solutions in the proposed mo asmoch method we relax the original assumption and make some modifications let n denotes the population size of the nsga ii method and pct denotes the resampling percentage which represents the percentage of the final population selected for evaluation and updating the surrogate models if the number of nondominated solutions in the final population is larger than n pct we select n pct of them with the largest crowding distances meaning that the points that are too close to each other are discarded otherwise if the number m of nondominated solutions is less than n pct we select all of them and the remaining points are selected by maximizing the minimum distance to already evaluated points more specifically after the nondominated solutions have been selected we compute the distance from each point in the remaining solutions to all already evaluated points and find its nearest neighbour then select the points with the top n pct m largest distances to their nearest neighbours if the distance from a selected point to its nearest neighbour is smaller than a threshold e g 1e 3 we select a new point from a large number of randomly generated points in the whole decision space which is furthest away from all evaluated points with the improved adaptive sampling strategy the local exploitation by solving the approximation optimization problem and the global exploration by diversifying the design sites throughout the decision space and on the pareto front can be better balanced 5 iteration append the input decision variables output objective functions and constraints pairs selected in the last step to the original input output pool x x x y y y c c c go back to step 2 and update the surrogate models with the updated data pairs the step 2 to step 5 are repeated several times to adaptively evolve the surrogate models until convergence or stopping criteria are reached 2 3 performance metrics to comprehensively assess the performance of the proposed mo asmoch method and facilitate the comparison between it and the baseline moea nsga ii both visualization and performance metrics are used in a complementary manner the former one shows the estimated pareto front and the true or referenced pareto front directly and intuitively in the objective space while the latter provides a way to quantitatively evaluate the quality of the nondominated solutions obtained evaluating the approximate pareto optimal solutions estimated by the multi objective optimization algorithm requires the examination of convergence and diversity convergence refers to the accuracy of the achieved approximate pareto optimal points diversity represents the spread of solutions in the objective space ideally the nondominated solutions obtained are expected to lie on the true pareto front and well distributed over the entire front in this study two widely used metrics are adopted namely the generational distance veldhuizen 1999 and the hypervolume zitzler and thiele 1999 the generational distance gd measures the average distance of the approximate pareto optimal points from the true or referenced pareto front and is defined as 2 g d i 1 n d i 2 1 2 n where n is the number of the approximate pareto optimal points and d i is the euclidean distance in objective space between the i th optimal point and the nearest member of the true pareto front fig 2 a shows a schematic of the gd calculated for a two objective minimization problem the ideal value of gd is 0 the hypervolume hv measures the volume enclosed by the union of the polytopes p 1 p 2 p n in the objective space where n is the number of the nondominated solutions obtained and each p i is formed by the solution point an additional selected reference point and the intersection of the hyperplanes arising out of that solution point along with the axes the shaded area shown in fig 2b is the hv of the approximate pareto optimal solutions with respect to the reference point for a two objective minimization problem in this study we calculate the ratio of the hv of the approximate pareto optimal solutions to that of the solutions in the true or referenced pareto front which is denoted as rhv the ideal value of rhv is 1 3 benchmark problems in this study we test the performance of the mo asmoch method on six benchmark problems due to the complexity of the multi objective hybrid optimization problems it is impractical to derive the pareto front analytically therefore synthetic test functions with true pareto front are not available for this kind of problems alternatively we resort to use real world multi objective optimization problems among the six benchmark problems three of which are engineering design problems from a recently introduced multi objective optimization problem suite tanabe and ishibuchi 2020 the other three are wds design problems selected from a library of benchmark design problems of wds wang et al 2015 table 1 gives a summary of the six problems the three engineering design problems were originally constrained single objective real world problems however the constraint functions of the original problems can be simultaneously minimized so tanabe and ishibuchi 2020 formulated the sum of constraint violation values as an additional objective function and the newly formed multi objective problems have no constraint functions for example re1 is the reinforced concrete beam design problem the first objective of the problem as shown in table 1 is to minimize the total cost of concrete and reinforcing steel of the beam the second objective is the sum of the two constraint violations in re1 the three decision variables x 1 x 2 and x 3 represent the area of the reinforcement the width of the beam and the depth of the beam respectively x 1 has a predefined discrete value from 0 2 to 15 and the other two are continuous variables since the real pareto front is unknown the best known approximation of the true pareto front was obtained by running five moeas nsga ii moea d tch moea d pbi ibea and nsga iii for a huge number of objective function evaluations the number of function evaluations was set to 100 000 and 31 independent runs were performed therefore the approximated pareto fronts are used as references in our study please refer to tanabe and ishibuchi 2020 for more details the three wds design problems include a hypothetical network the two loop network tln and two real networks the new york tunnel nyt network and the goyang goy network the library of benchmark design problems of wds was built by collecting various benchmark problems from the literatures each problem in this library is formulated to minimize the total cost associated with pipe components and to maximize the network resilience the mathematical expression of each objective is given as follows 3 min c i 1 n p u c d i l i where c is the total cost monetary units problem dependent np is the number of pipes u c is the unit pipe cost depending on the diameter selected in a specific problem d i is the diameter of pipe i l i is the length of pipe i 4 max i n j 1 n n c j q j h j h j r e q k 1 n r q k h k i 1 n p u p i γ j 1 n n q j h j r e q 5 c j i 1 n p j d i n p j max d i where i n is the network resilience n n is the number of demand nodes c j q j h j and h j r e q are uniformity demand actual head and minimum head of node j n r is the number of reservoirs q k and h k are discharge and actual head of reservoir k n p u is the number of pumps p i is the power of pump i if any γ is the specific weight of water n p j is the number of pipes connected to node j d i is the diameter of pipe i connected to demand node j the variables mentioned for evaluation of network resilience are obtained by running the hydraulic simulation using the epanet 2 software rossman 2000 these wds design problems have explicit constraints which are related to the hydraulic performance of a network for example the nodal pressures at demand nodes and flow velocities in pipes the constraint violations must be computed during the hydraulic simulation and hence are computationally expensive the complexity of these benchmark problems is different according to the size of search space for relatively low dimension problems e g tln used in our study the true pareto front can be obtained through full enumeration for high dimension problems wang et al 2015 ran five moeas amalgam borg nsga ii ε moea and ε nsga ii for a very large number of evaluations up to 2 000 000 for large problems to find the best known approximation to the true pareto front which is similar to tanabe and ishibuchi 2020 the true pareto front of tln and the best known pareto fronts of nyt and goy are used as references in our study interested readers can refer to wang et al 2015 for more details 4 results 4 1 engineering design problems to evaluate the effectiveness and efficiency of the proposed mo asmoch method we compare it with the embedded nsga ii method which is arguably the most popular moea and is also regarded as an industry standard method considering the referenced pareto front of each problem is the best known approximation to the true pareto front obtained by running five moeas for a very large number of evaluations it is no need to use nsga ii with the same large number of model evaluations for comparison to best assess how efficient or effective the mo asmoch method is in comparison with nsga ii we compare the two methods given the same limited computing budget in model evaluations in addition we also compute the efficiency gains which is recommended by razavi et al 2012 to do this we quantify the numbers of original model evaluations the mo asmoch and nsga ii require to obtain the approximate pareto optimal solutions with nearly the same quality after determining the setup of the mo asmoch method we gradually increase the number of model evaluations for nsga ii to get the same quality of nondominated solutions obtained to perform valid comparisons each method is repeated 10 times the final setups of the two methods for the three engineering design problems are shown in table 2 for mo asmoch the population size and number of generations of the embedded nsga ii are set to 100 and the resampling percentage is set to 20 the glp with ranked gram schmidt rgs de correlation is used as the doe method and the default gpr is used as the surrogate model in the mo asmoch approach which follows the recommendations of gong et al 2016a 2016b we have also tested other doe methods and surrogate model types and the results also demonstrate that the selections are appropriate as this study does not focus on the intercomparison of different doe methods and surrogate models we do not elaborate the results here the multi objective optimization results of the three problems are shown in fig 3 and table 3 fig 3 presents a randomly selected set of approximate pareto optimal solutions from the 10 replicates for the mo asmoch and nsga ii respectively table 3 shows the mean and standard deviation of the gd and rhv calculated from the 10 replications of the two methods as illustrated in fig 3 basically the mo asmoch method can perfectly reach the referenced pareto front which is the best known approximation of the true pareto front for the re1 problem there are a few points lying slightly away from the referenced pareto front while all the approximate pareto optimal points are well distributed along the referenced pareto front for the re2 and re3 problems we can see that the referenced pareto fronts are disconnected but the approximate pareto optimal points obtained with mo asmoch are located almost exactly on the referenced pareto fronts fig 3 shows that the mo asmoch method obtains significantly better sets of nondominated objective vectors than the ones obtained by nsga ii table 3 shows that the gd and rhv obtained by mo asmoch are much better than those obtained by nsga ii given the same computational budget moreover the standard deviations of the two metrics obtained by nsga ii is significantly larger than those obtained by mo asmoch indicating the large randomness of nsga ii given a small number of model simulations after increasing the computational budget for nsga ii it can be seen that the diversity of the nondominated solutions obtained by mo asmoch is slightly worse than that of nsga ii large especially for the re3 problem from table 3 we can further know that mo asmoch does a very good job in achieving convergence but performs slightly worse in representing the diversity of the pareto front which is indicated by the rhv metric generally the two metrics of mo asmoch and nsga ii large with the setups in table 2 are very close indicating the quality of nondominated solutions obtained with the two methods is similar in this case we can know that the computational savings achieved through using the mo asmoch method range from 75 to 92 of original model evaluations in addition the standard deviations of the two metrics are very small meaning the conclusions on the effectiveness and efficiency of the proposed method with the current setups are valid 4 2 wds design problems the above three engineering design problems do not contain constraints in this subsection we evaluate the mo asmoch method based on three wds design problems which are constrained multi objective discrete combinatorial optimization problems the dimensionalities of the three wds design problems 8 21 30 are also higher than the engineering design problems the tln problem is repeated 10 times while the other two problems are repeated 5 times as training the surrogate model is time consuming when the number of decision variables is large again we use the same strategy as used in section 4 1 to assess the effectiveness and efficiency of the mo asmoch method the setups of the two methods for the three wds problems are shown in table 4 for these wds design problems we find that the gpr still performs the best among a variety of surrogate model types and the default gpc performs well for the initialization step we find that using already evaluated points in the early generations of nsga ii rather than initial points from a formal doe to initially build the surrogate model can obtain better optimization results for example fig 4 shows the obtained pareto fronts of using the two different initialization modes for the tln problem for both initialization modes 700 design sites are used to build the initial surrogate models the 700 design sites of fig 4a are generated using the glp with rgs de correlation method while the 700 design sites of fig 4b are the last generated 700 points of the nsga ii method with population size of 50 and 20 generations clearly the approximate pareto optimal solutions in fig 4b have better diversity than those in fig 4a although they achieve good convergence the search space is generally very large for the wds design problems with many pipes and available pipe diameter options as such the number of design sites required to uniformly cover the decision variable space could be extremely large especially for higher dimensional problems theoretically a sufficiently large uniformly distributed initial set of design sites is required to construct a surrogate model which can well represent the original response surface however the more design sites used for surrogate model constructing the higher the computational time required in this process which could largely reduce the computational efficiency of the surrogate based method many studies behzadian et al 2009 jourdan et al 2006 yan and minsker 2011 which follow the metamodel embedded evolution framework typically did not incorporate formal does instead they used points produced in the early generations of the evolutionary algorithm to build the initial surrogate model although it is argued razavi et al 2012 that the surrogate model fitted on the initial design sites collected from previous optimization results may not have global accuracy and could be misleading we believe that the surrogate models can be more accurate in a reduced search space by running the nsga ii for a few generations for the nyt and goy problems we also run nsga ii with population size of 50 and 20 generations and then use the last generated 700 points as the initial input of the mo asmoch method different from the tln problem mo asmoch fails to reach the referenced pareto front for the nyt and goy problems as illustrated in figs 5 and 6 for the nyt problem although the approximate pareto front obtained with mo asmoch is not close to the best known pareto front which is obtained by running five moeas with 600000 original model evaluations it is close to the approximate pareto front obtained by running nsga ii with 20000 model evaluations in addition the approximate pareto front of mo asmoch has relatively good diversity it can effectively cover the region with the network resilience between 0 45 and 0 72 but it cannot cover the region with the network resilience less than 0 45 and larger than 0 72 as the referenced pareto front does it for the goy problem the distances between the approximate pareto optimal solutions of mo asmoch and the referenced pareto front are small which is also indicated in table 5 these approximate pareto optimal solutions are reported to have total cost between 0 18 and 0 22 million on average which only cover the region with the network resilience less than 0 65 even if these solutions are not entirely satisfactory from an engineering design standpoint lack of diversity they are still of engineering interest since saving infrastructure capital and operational costs is considered as the primary task in wds design problems with the setups in table 4 both the convergence and diversity metrics of mo asmoch are significantly better than those of nsga ii indicating that the mo asmoch algorithm is as effective but more efficient than the nsga ii method given the same computational budget in fact the nondominated objective vectors obtained by mo asmoch dominate all the nondominated objective vectors obtained by nsga ii in addition the performance metrics of mo asmoch and nsga ii large are very similar as shown in table 5 therefore we can know that the efficiency gains achieved through using the mo asmoch method range from 73 to 90 of original model evaluations 4 3 impact of the mo asmoch setup on optimization results the setup of the mo asmoch method may have significant influence on the optimization performance based on the experience from the mo asmo method gong et al 2016a therefore we evaluate the impact of initial sample size resampling percentage pct and number of iterations iter on optimization result in this subsection when the asmo method was developed wang et al 2014 various values of initial sample size were compared and it came to a conclusion that 15 20 times of the dimension of the problem may be the proper initial sample size however this rule of thumb may not be suitable for other practical problems we generate the initial sample points with different sample sizes using the glp with rgs de correlation method to test the impact of initial sample size on optimization results we start with the initial sample size of 10 times of the dimension and increase the initial sample size by 5 times each time for each initial sample size the pct is set to 0 2 and the iter is set to 5 we find that it is until 30 times that the optimization results are satisfactory when the initial sample size is less than 30 times of the dimension the optimization performance of mo asmoch is poor not shown even for the three relatively simple engineering design problems we want to test if further increasing the initial sample size could improve the optimization performance the two performance metrics with different initial sample sizes for the six test problems are shown in table 6 for the three engineering design problems and the tln problem the mo asmoch method is run for 10 trials for the nyt and goy problems the mo asmoch method is run for 5 trials for relatively simple engineering design problems further increasing the initial sample size could not guarantee the improvement in optimization performance instead the metrics may become slightly worse since they have already been good enough for the three wds design problems further increasing the initial sample size improves both convergence and diversity metrics this is easy to understand since a sufficiently large initial set of design sites is required to construct surrogate models which can well represent the original response surface for high dimensional problems however training the surrogate models become very time consuming when the sample size is large the computational expense associated with surrogate model fitting and the increased model evaluations can directly affect the efficiency of the optimization algorithm consequently the proper initial sample size may be 30 times the number of decision variables the users can increase the initial sample size for high dimensional problems if having adequate computational budget when dealing with computationally expensive problems in practice we would be able to do only a limited number of model evaluations so the number of iterations iter can be determined by computational budget or the total number of model evaluations the initial sample size the population size pop of the embedded nsga ii and the resampling percentage pct generally pop should be large enough to ensure the embedded nsga ii can work well on surrogate models and thus it is usually set to 100 therefore the users only need to specify pct after determining the initial sample size given the limited computational budget we evaluate the impact of pct by comparing the optimization results obtained with different pct values for the six test problems the value of pct is set to 0 1 0 2 0 5 and 1 0 respectively to give a fair comparison the total number of model evaluations is fixed for each test problem therefore the variations of the performance metrics with the pct values can be illustrated as figure 8 in gong et al 2016a similarly our results not shown also indicate the impact of pct is problem dependent for the three engineering problems the influence on the performance metrics is complicated both performance metrics are best when pct is set to 0 1 and 0 2 for re2 and re3 respectively for re1 the performance metrics are best with pct equalling to 1 0 for two of the three wds design problems nyt and goy overall gd increases with the increase of pct whereas rhv decreases with the increase of pct for the tln problem the performance metrics are best when pct is set to 0 5 consequently a small to medium value of pct such as 0 1 or 0 2 is recommended which is also same as gong et al 2016a suggested 5 discussion the proposed mo asmoch method has several new features which includes introducing a truncation procedure and a mapping procedure to the surrogate based optimization process building a separate classification surrogate model for the constraints and modifying the adaptive sampling strategy with these new features mo asmoch can handle constrained hybrid multi objective optimization problems which require computationally expensive model simulations in this section we want to discuss how these new features affect the optimization performance the classification surrogate model is introduced to separate the solutions into feasible and infeasible solutions so the accuracy of the classifier would directly affect the algorithm s performance in optimization we conduct an experiment to show the accuracy of the classification surrogate when used in mo asmoch for the three wds design problems the setups of mo asmoch are same as those in table 6 with initial sample size equalling to 30 times of the number of decision variables after each iteration of constructing the classification surrogate model we perform the 5 fold cross validation and calculate the accuracy scores which measures the fraction of correct predictions the correct prediction means that the set of labels predicted for a sample must exactly match the corresponding true set of labels fig 7 shows the accuracy scores of the default gpc of each iteration for the three wds design problems it is obvious that gpc performs pretty well in all three problems the worst accuracy scores which occur in the nyt problem still exceed 0 8 and the accuracy scores for the other two problems all exceed 0 9 the results demonstrate the effectiveness of using a classifier to separate the solutions for the nyt problem we further find that the accuracy scores of the multi layer perceptron classifier mlpc are better than those of gpc after replacing gpc by mlpc in the mo asmoch method both the gd and rhv improve a little as shown in table 7 a in addition table 7b also shows the impact of the resampling strategy on the optimization performance as described in section 2 2 the assumption of the adaptive sampling strategy used in mo asmo could fail in some cases for example the tln problem in our study the modified version is more reasonable and using it in mo asmoch makes the performance metrics improved it should be noted that the modified adaptive sampling strategy becomes the original one when the assumption does not fail the accuracy of the surrogate model could greatly affect the optimization results which have been stated in many previous studies since the classification surrogate is fairly accurate we want to further test how much the performance would degrade if the surrogate accuracy is low similarly we also perform the 5 fold cross validation to evaluate the accuracy of the surrogate models taking the tln problem as an example the coefficients of determination of the initially fitted gpr models for the two objective functions are shown in fig 8 despite the accuracy of the surrogate model of the second objective function is relatively low the nondominated solutions obtained are very close to the true pareto front when the default gpr model is replaced by the mlp model the accuracy significantly deteriorates as a result the approximate pareto optimal solutions obtained by the mlp based method are poor in both convergence and diversity and they are totally dominated by the approximate pareto optimal solutions obtained by the gpr based method therefore we need to carefully select the surrogate model before using the algorithm unlike the multi objective continuous optimization problems where there are many standard benchmark problems with analytic solutions there are no such standard benchmarks for multi objective mixed variable optimization problems previous studies on multi objective mixed variable problems were often limited to specific applications in which assessing the performance of algorithms relies on intercomparison the 6 test problems used in this study were collected from two benchmark suites tanabe and ishibuchi 2020 wang et al 2015 which have provided the best known pareto front of each benchmark problem therefore we can better assess the effectiveness and efficiency of the proposed method through comparison with the referenced pareto fronts despite having used 6 test problems we admit that the evaluation is still limited but it does not mean that our proposed method is only suitable for the two kinds of problems the mo asmoch method is able to handle problems with continuous integer and ordinal discrete variables in fact the algorithm can be simply modified to handle categorical variables whose values do not possess a natural ordering in this case a popular technique to account for these variables in surrogate modelling is to use a one hot encoding of the categorical variable garrido merchán and hernández lobato 2020 it means that the input of the surrogate model corresponding to a categorical variable is replaced by additional real valued variables one variable per category for example assume that a decision variable is categorical and contains a set of finite categories c red green yellow this variable is replaced with three variables that may take values 1 0 0 0 1 0 and 0 0 1 corresponding to the categories in c then after running the nsga ii on the surrogate models the largest extra variable is set to one and the other two are set equal to zero because there are no such test problems involving categorical variables as the ones used in our study which have the best known approximation to the true pareto front we did not include categorical variables in this study we will test the mo asmoch method on real world problems with categorical variables in our future studies we believe our method is flexible enough to be combined with different simulation models and used in a variety of real world applications compared to previous surrogate based algorithms which can only handle some of the features multi objective mixed variables computationally expensive constraints the proposed mo asmoch method has the following advantages 1 mo asmoch is a surrogate enabled method rather than a surrogate embedded evolutionary method like in behzadian et al 2009 brownlee and wright 2015 jourdan et al 2006 it means that our method is non intrusive and is compatible with many other moeas or multi objective optimizers 2 mo asmoch is compatible with different initialization methods like formal doe methods and using solutions from previous optimization attempts in addition there is no fixed surrogate model type in mo asmoch as different surrogate modelling methods have their own merits for example radial basis function rbf interpolation and artificial neural network ann are the most commonly used surrogate model types in surrogate based optimization studies but we found that using gpr in mo asmoch can produce better optimization results than using ann and rbf in our study despite training gpr is more time consuming especially when the number of design sites is large 3 using a classification surrogate model to classify the constraints as satisfied and not satisfied is easy and has been proved to be accurate to accommodate to the classification surrogate the constrained domination principle of nsga ii is modified by ignoring the magnitude of constraint violations when sorting infeasible solutions although this may cause nsga ii to wander in the infeasible search region for more generations before reaching the feasible region there is no need to worry about the computational burden of running nsga ii with many generations on the surrogate models hence the equality constraints can also be handled despite the advantages there is still room to improve the mo asmoch method it has been widely recognized that the nondominated solutions obtained with the surrogate based optimization methods provide only an approximation of the pareto front and mo asmoch is no exception the convergence and the diversity of the obtained non dominated solutions mainly depend on the performance of the surrogate models if the surrogate models have low accuracy the obtained nondominated solutions could be far from the real pareto front and it is also difficult to capture the whole region of the real pareto front just as shown in fig 8 in this study we also found the quality of the obtained nondominated solutions deteriorates as the dimension of the benchmark problem increases which is mainly reflected as the reduced diversity of the optimal solutions actually we have also tested the fossolo network problem with 58 discrete variables from the wds benchmark suite the obtained nondominated solutions not shown are close to the referenced pareto front but they can only cover a small region toward the left tail of the pareto front gaussian process models are struggling to effectively model high dimensional input output maps zhu et al 2019 and the problem exists in almost all other surrogate modelling approaches razavi et al 2012 the surrogate model built with a limited number of samples might not well represent the original response surface thus missing certain regions of the function space in addition high dimensional problems have an extremely large search space thus the number of design sites needed to reasonably cover the decision space becomes very large training a surrogate model with a large number of samples can be very time consuming and obtaining these samples which requires the equal number of original model evaluations further increases the computational burden therefore surrogate modelling becomes less attractive or even infeasible when dealing with high dimensional problems a possible way to tackle or mitigate this problem is to develop or use advanced machine learning models which excel at modelling high dimensional input output maps liu et al 2017 xi et al 2020 zhu et al 2019 another promising way is to further improve the adaptive sampling strategy so that it can automatically detect and collect enough design sites from the rugged regions we will follow these paths to improve our method for better handling high dimensional optimization problems we would like to investigate the performance of our method in combination with different types of multi objective algorithms furthermore we would like to apply the method to different kinds of real world applications 6 conclusions in this study we developed the novel mo asmoch method for solving computationally expensive multi objective optimization problems whose decision variables can be continuous integer discrete or a mixture of them these problems may also contain model constraints that may require computationally expensive simulation although there are quite a few surrogate based methods which can handle optimization problems with some of these features e g multi objective hybrid variables computationally expensive constraints the optimization problems with all of the three features have been rarely touched by those surrogate modelling based methods to achieve these goals we have extended our previously developed mo asmo method by introducing a truncation procedure and a mapping procedure to the embedded nsga ii building a separate classification surrogate model for the constraints modifying the adaptive sampling procedure to incorporate a global sampling strategy in contrast to previous methods that use the surrogate model to approximate the constraint function value we use a classifier which is built on previously evaluated solutions to separate a new solution to feasible or infeasible therefore there is no need to build surrogate models for each constraint and the equality or binding constraints can be also coped with we compared the mo asmoch method and its embedded nsga ii on 6 problems including three engineering design problems without constraints and three wds design problems with constraints the comparisons were focused on the convergence and diversity of the pareto optimal solutions obtained with the two optimization methods the results show that mo asmoch obtained much better nondominated objective vectors than nsga ii given the same computational budget moreover our proposed method obtained similar nondominated solutions with nsga ii but requires only 5 27 of the original model evaluations used by nsga ii overall the nondominated solutions obtained by mo asmoch have good convergence but are not so widely dispersed compared to the referenced pareto front considering the referenced pareto fronts are obtained by running different moeas for a huge number of model evaluations the evaluation of mo asmoch has actually incorporated the comparisons with other moeas rather than just nsga ii hence the proposed mo asmoch method is very promising for solving constrained multi objective mixed variable optimization problems where the objective and constraint function evaluations are computationally expensive software and data availability the source code of the mo asmoch method which is developed in python language is available at https github com getred mo asmoch the test data of the three engineering design problems are available at https github com ryojitanabe reproblems and the test data of the three wds design problems are available at http www exeter ac uk benchmarks pareto fronts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was jointly supported by the strategic priority research program of the chinese academy of sciences xda2006040104 the national natural science foundation of china 42101046 51979004 and the china postdoctoral science foundation 2019m661714 we really appreciate dr wei gong at beijing normal university for providing the source code of mo asmo and providing constructive comments we also thank the other three reviewers for their constructive and valuable comments which helped to improve the manuscript appendix gaussian process classification here in the appendix we give a brief introduction of gpc which is the default classifier used in the mo asmoch method the gpc implements gaussian processes for probabilistic classification where test predictions take the form of class probabilities so we first review using linear models for binary classification which form the foundation of gpc models for binary classification the labels y 1 and y 0 are usually used to distinguish the two classes the idea for the binary discriminative case is to turn the output of a regression model into a class probability using a response function one of the most popular learning algorithms for classification is the logistic regression which combines the linear model with the logistic function a1 p y x g x τ w g z 1 1 exp z where x is the input vector and y is the class label w is the weight vector and g z is the logistic function as the probability of the two classes must sum to 1 it is obvious that p y 0 x w 1 p y 1 x w given the training data x y the prediction for a test point x is obtained as follows a2 p y 1 x d p y 1 w x p w d d w which integrates the prediction p y 1 w x g x τ w over the posterior distribution of weights the basic idea behind gpc is simple it places a gp prior on a latent function f which is then squashed through a link function logistic function to obtain the probabilistic classification more specifically the linear function f x x τ w from the linear logistic model is replaced by a gaussian process and correspondingly the gaussian prior on the weights is replaced by a gp prior the values of f are not observed and we are also not interested in the values of f but rather in π where π x p y 1 x g f x the probabilistic prediction for a test point x is obtained with two steps a3 p f x y x p f x x f p f x y d f a4 π p y 1 x y x g f p f x y x d f where p f x y p y f p f x p y x is the posterior over the latent variables in the gpr case computation of predictions is straightforward since the relevant integrals are gaussian and could be computed analytically for classification the posterior of the latent function f is not gaussian even for a gp prior because of the non gaussian likelihood thus making the integral analytically intractable therefore gpc approximates the non gaussian posterior with a gaussian based on the laplace approximation the posterior mean and variance for f are express as a5 e f x y x k x x τ k x x 1 f ˆ a6 v f x y x k x x k x x τ k x x w 1 1 k x x where k is the covariance function f ˆ is the one that maximizes p f x y and f ˆ k x x log p y f ˆ w log p y f the detailed implementations for finding f ˆ and calculating w can be found in chapter 3 of rasmussen and williams 2006 
25666,the encapsulation of specific management measurements and computing services is rarely reported for surface water edss to promote the efficiency and reduce the complexity of surface water edss development a generic pathway to encapsulate the surface water quality model and its applications as cloud computing services is proposed the web service is implemented under the opengms framework two typical management measurements water assimilative capacity allocation and pollution load reduction planning are encapsulated based on the advanced version of the wasp program an edss instance of the maozhou river shenzhen is developed which provides precise management measurements for cod and nh3 n control management services provided by other models efdc and swmm are also implemented under this novel framework the proposed pathway provides the merits of rapid development easy deployment and maintenance of an advanced edss it presents a good demonstration of coupling advanced wasp programs within edss for receiving water management keywords industrial cloud maozhou river model integration surface water quality model water assimilative capacity web services 1 introduction urban rivers the most accessible water environments are essential for human life many measurements for surface water quality planning and management have been developed around the world to protect the water environment and sustainability such as total pollution load control water function zone division environment carrying capacity layout planning of wastewater effluent outlets water assimilation capacity management and water quality early warning and forecasting the well recognized total maximum daily loads tmdls plan by the usepa lung 2001 mohamoud and zhang 2019 wang et al 2019 and the recent three lines one permit tlop policy by the chinese epa wang et al 2020 are national level instances of management measurements they are all model based simple or complex and computational engeland and alfredsen 2020 wang et al 2016 many efforts have been made by experts and governments to develop these modeling tools for example the water quality analysis simulation program wasp and environmental fluid dynamic code efdc by the usepa have been widely validated and used in a large number of practices in different countries bouchard et al 2017 jiang et al 2019 furthermore in the era of smart cities developing an environmental decision support system edss mcintosh et al 2011 integrated with monitoring modeling planning and control for smart management of urban river water quality has been widely accepted and implemented around the world chen and han 2018 rui et al 2015 integrating these management measures and modeling tools into the edss tang et al 2016 can greatly improve the convenience and efficiency of decision making therefore coupling localized water quality models as operational running components to meet different management requirements is one of the fundamental technologies for surface water edss development ruiz ortiz et al 2019 mckinney and cai 2002 first proposed the concept of combining geographic information systems giss with water resource management models the traditional pathway of model integrations with edss is usually database oriented interaction and has succeeded in many circumstances martin et al 2004 proposed the first water pollution management information system that couples gis and water quality models lee and hyun 2008 integrated wasp7 and efdc with giss on the han river and kyeonggi bay for best management practices bmps in korea a gis database was built to estimate the amount of generated and discharged water pollutants according to the tmdl technical guide peng et al 2011 utilized a similar database oriented pathway to integrate the different types of efdc modeling data into a gis platform in the lower charles river basin in the usa efforts have also been made to integrate usepa swmm liao et al 2012 liu et al 2016 martin et al 2020 however the traditional integration pathways are non modular with less efficient share and reuse capabilities and struggle with system updating edss development requires close cooperation of professional software developers and model experts to learn from each other and clarify the data structure of the platform system the interactive format and calling of model parameters it is challenging to master and re edit the source code of open models liu et al 2018 nevertheless it is worth contemplating how to reduce the threshold of using the professional model improve the calculation efficiency and understand the business operations behind the professional model software it not only strengthens the prediction and simulation ability of the water quality management information system but also improves the universality of the model which is convenient for general technical personnel managers and decision makers liu et al 2018 zeng et al 2020 with the development of cloud computing and web services service oriented design is an emerging trend of model coupling although cloud computing is not a new concept as its commercialization started in approximately 2000 the global use of computers and smartphones has increased the needs of high speed and high capacity computing tasks and cloud computing seeks to address these needs senyo et al 2018 cloud computing often appears in the form of system services which enables ubiquitous convenient on demand network access to a shared pool of configurable computing resources there are three types of cloud computing service models software as a service saas platform as a service paas and infrastructure as a service iaas zissis and lekkas 2012 cloud computing together with the iot and big data has shown great application prospects in manufacturing healthcare aceto et al 2020 and management the widely design and use of cloud computing in industries known as industry cloud has been considered as a common function among industry solutions and the usage with use case scenarios kushida and pingali 2014 under this emerging technical background service oriented interface design for open distributed environmental simulations is receiving increasing attention in the gis area and environmental modeling field zhang et al 2020 a position paper indicates that distributed service oriented models would be more popular in model sharing and application chen et al 2020 there are many kinds of service oriented models applied in different domains goodall et al 2011 presented a service oriented interface design for integrating water resource models o donncha et al 2016 deployed the efdc on a container based computer to provide a solution that saves time in simulation xiao et al 2019 developed a web system for swmm application and zeng et al 2020 proposed a web service framework based swmm for real time urban stormwater management web service oriented encapsulation of efdc and swmm has been recently reported liu et al 2018 however it is curious that no web service oriented software package has been developed to facilitate the application of wasp though it is a standard receiving waterbody environmental management model there is an obvious gap in water quality management practices in the literature jia et al 2001 first integrated the wasp 5 model with gis in a semi tight integration mode which provided strong support for the spatial simulation of surface water quality and water quality management peng et al 2010 completely embedded the wasp 5 water quality model into the gis platform for the first time so that the input and output data of the wasp model were connected with the gis database and could be displayed graphically li and zhang 2012 fully integrated the eutrophication module in wasp 5 with gis and adapted it to a submodule of the gis analysis system making an active attempt to study large scale hydrodynamics and water quality model systems based on gis in addition almost all the wasp model integration reported in the literature is implemented by analyzing and encapsulating the original fortran code of wasp 5 these source codes were developed as early as the 1980s and this model integration pattern is time consuming to some degree few studies have reported the coupling technique for the latest version of the wasp program with the edss platform which hinders the deployment and application of high level automation of the model program in addition during the emerging web service wave there was a lack of service encapsulation for management measurements current research and practice are mostly aimed at the model computing engine itself only a generic model computation service is provided without further encapsulation for the model based management business which is not thorough enough moreover the pathway of forming management measurements into business tools by encapsulating environment models and integrating them with edss has not yet been reported as a result this kind of model integration is not linked to the business level of water environment management planning which still has an existing technical threshold for terminal users or developers of edss with high repeatability of integration work and low efficiency this is apparently not enough in the era of the massive practice of smart cities the technical threshold of developers needs to be liberated it can also be further developed to enable local management agencies to deploy model applications conveniently and efficiently facilitate system upgrades and maintenance and call by different user platforms to fill these technology gaps this paper aims to develop a web service oriented software package encapsulated with the wasp v7 5 program and propose a technical route of integrating water quality management measurements with edss in the form of a web service business tool the web service encapsulation of localized wasp models and advanced cloud computing services will be implemented with the help of open geographic modeling and simulation opengms and related tools zhang et al 2020 as a saas pattern two water quality management measurements water assimilative capacity wac allocation and pollution load reduction plr planning are carried out by cloud computing services which achieve operational edss running a case study is conducted with real edss deployment in the maozhou river shenzhen china in the following section this paper discusses the structure of the wasp model and designs basic algorithms of two typical water quality planning measurements load reduction simulation and wac computing section 3 introduces the proposed process of encapsulating wasp based computing programs as web services deployment on cloud servers service containers and coupling web services with edss systems section 4 shows an edss case study on the maozhou river and presents how the wasp based computing program integrates with the edss system in the form of web services then it discusses the implications of the technical route for other water quality models such as efdc or swmm the advantages and further applications 2 methods as wasp was selected as the major surface water quality model for integration the mechanism and versions were introduced in this section the algorithms of plr and wac were introduced as the typical water quality management measurements selected 2 1 mechanism of wasp model the wasp model is currently one of the most widely used water quality models and has been used in thousands of rivers lakes reservoirs and other bodies of water around the world it performs well for capturing the processes of dissolved oxygen bacteria eutrophication toxic substances and other biochemical processes compared to other water quality models wasp has the characteristics of substantial flexibility high versatility and accurate simulation results peng et al 2010 wool et al 2020 the wasp model consists of two modules the hydrodynamic module dynhyd which is used to simulate hydrodynamic processes such as river flow and velocity and the water quality module wasp which contains the eutrophication model eutro see fig s1 the toxic substance model toxi and the mercury model mercury these models are used to simulate the migration and reaction process of pollutants in water the two modules can operate independently without affecting each other wool et al 2020 the wasp hydrodynamic models are based on a set of transport and transformation equations advective transport is driven by water flow through a specified computational network see fig s2 the main differential equations of the wasp hydrodynamic model are as follows 1 q x a t 0 2 d u x d t u x d u x d x a g a f where q volumetric flow m3 s a cross sectional area m2 u x river velocity m s a g gravitational acceleration along the axis of the channel m2 s a f frictional acceleration m2 s equations 1 and 2 form the basis of the hydrodynamic model dynhyd which is implemented within wasp the water column flow can also connect with other hydrodynamic models alternatively such as epdriv1 or efdc the main differential equation of the wasp water quality model is as follows wan and mao 2015 3 c t x d x c x u x c s l s b s k where c the pollutant concentration at x downstream of the pollutant discharge point when the migration time is t mg l d x diffusion coefficient of pollutants m2 s u x river velocity m s s l source and sink loads of pollutants entering rivers s b boundary pollution load of pollutants s k biochemical reaction items of pollutants compared to the one dimensional solute transport model the wasp model replaces the attenuation term in the one dimensional solute transport model with the sum of the pollution source term s l the boundary load term s b and the biochemical reaction term s k in the wasp model the pollutants are imported as point sources in each segments it is assumed that the pollutant is a steady state emission in each month and is equally distributed in each day the minimum time step of our wasp model which is adopted in environmental management on a large time scale like annual planning 2 2 versions of wasp program wasp initially runs under dos and the code is open source after 2001 it was upgraded to the wasp 6 version running under windows 98 and the wasp7 version was issued in 2005 which matched windows 2000 and xp systems wasp 7 substantially improved pre and post treatment and the operation efficiency was increased by more than ten times more efficient eutrophication and organic pollutant treatment modules have been built chen et al 2006 wasp version 8 released later in 2018 eliminates silent running interfaces and cannot be tightly coupled with third party platforms to complete complex automated customization operations wool et al 2020 the source code is no longer accessible since version 6 0 which caused difficulties for customization and integration with edss to the best of our knowledge there are no reports of coupling wasp with automatic water quality management under higher versions this study integrates the 7 5 version released in 2012 with complete functions its eutrophication module is stable and efficient and suitable for more complex watershed management requirements such as water assimilation capacity management and pollutant load reduction planning it has a clandestine scheduling interface with silent batch operation which provides an opportunity for customization for edss 2 3 two typical model based management measurements from an environmental engineering point of view watershed water quality management refers to many model based applications e g load reduction sewage outfall setting wac allocation ecological water supply etc here we highlight and design the algorithms for plr planning and wac allocation the two most widely used management measurements the plan to control the pollution source loading into the receiving body of water is fundamental to water environment quality management pollution reduction is parameterized by reducing the pollution source intensity of each discharge outlet specifically the pre established pollution reduction scenario data the set pollutant discharge amount of each river section are imported into the model input file the water quality calculation model is called for calculation and the control section under the simulation scenario of emission reduction is obtained the pollutant concentration in the simulated situation can be compared with the pollution control target to evaluate the pollution reduction effect in addition plr is closely related to the issue of pollution load allocation the latter is an optimization problem considering socioeconomic equity among different pollution sources and corresponding regions su et al 2019 this paper only implements plr itself while neglecting the reduction policy the wac provides an essential basis for decision makers to draw the red line of sewage discharge and formulate the abovementioned load reduction plans faulkner 2008 novotny and krenkel 1975 scherer 1975 the calculation of wac is generally based on scientific analysis of the hydrological and water conservancy characteristics natural geographical conditions pollutant discharge methods of the river section etc to calculate how many pollutants the river can tolerate jiang et al 2019 novotny and krenkel 1975 zhang et al 1998 wac is a linkage of water quality models to waste load management measures farhadian et al 2015 both simplified models and advanced models such as wasp can be used for wac calculations after decades of development currently commonly used calculation methods for wac include the analytical formula method trial and error method and systematic optimization method dong et al 2014 the calculation of wac in our case is set as an optimization problem whose optimization goal is to make the characteristic pollutant concentration at the control units close to a given threshold the optimization goal and constraints of wac is shown in equation 6 4 min f c w c s s t w 0 ε c w c s c s 0 02 c s constants where w water assimilation capacity at segment g s c w concentration at the segment section where the pollutant load is w mg l c s concentration threshold at segment mg l the segment section where c w is located has different selection methods and three calculation modes are derived the segment head control method segment end control method and control unit end control method lei 2006 the word segment here means the river segment between two pollutant outlets segment head is the upper outlet of one river segment and segment head control is to make the pollutant concentration at the upper outlet close to the controlling threshold control unit end control means making only the end outlet of a control unit a combination of several continuous segments meets the controlling threshold regardless of the change of water quality in the control unit here we choose the control unit end control in case study as in case study we separated 12 segments into two function areas which named control unit 1 and control unit 2 respectively see fig 6 to realize the automatic calculation of the wac based on the determination of water function zoning and pollution control objectives of each reach of urban rivers this study adopts the definition of the segmental control method and the optimization method of dichotomy based on the wasp model the dichotomy formula is as follows 5 a n 1 a n 1 2 n a 1 n 1 2 where a n the river section pollutant load in the nth iteration calculation t n number of iterations equation 5 is an iteration algorithm to solve the optimization equations of wac as presented at equation 4 the initial pollutant load a 1 launched the calculation of pollutant concentrations at the control section compared to the target concentration under control the formula of a 2 takes the negative sign if calculation results exceed the target otherwise it takes the positive sign the same operation continues until the simulated pollutant concentration is close to the target concentration when the iterative calculation is terminated the pollutant load of the river section is assigned the wac of the river reach section 3 a generic pathway of wasp based web services integrated into edss 3 1 overall technology road map the overall technical road map of the proposed edss integration for surface water quality management can be summarized as follows model control interface management application component web service encapsulation system integration a diagram of the pathway is shown in fig 1 it builds up wasp based saas web services for load reduction and wac computing which provide a core function for an edss system the four key steps are explained in section 3 this framework addresses the three technology and application gaps mentioned before understanding the complete encapsulation from model to business layer comprehending the web service of water environment model and its environmental management business completing the implementation of wasp model application and breaking through the integration problems on its advanced version these are all fulfilled in a unified framework fig 1 as detailed in the illustration in the below the system consists of three layers cloud server layer application server layer and browser layer the cloud server contains wasp program engine and a model container where web services are deployed we take plr and wac as instances of web services each web services has its own computing components a set of encapsulated program package and a model description file mdl which defines the input and output interface of the web service the application server layer can contains different edss systems each edss is able to invoke web services deployed on the cloud server independently through designated scripts the browser layer is open for client users different stakeholders who can upload inputs from their own edss and receives analysis results downloaded from cloud server owning to the flexible architecture associated with the encapsulate pathway service oriented system herein has many benefits in the circumstances of water quality management and planning many stakeholders will participate the regulatory government departments including water affairs bureau environmental protection bureau and urban management bureau consulting institutions such as research academy of environmental sciences universities market subjects including water utilities hydraulic engineering companies and wastewater treatment companies residents and the public they can quickly customize a platform i e edss that meets their own needs by integrating different industry cloud services more points will be systematically summarized on the discussion section to simply in this paper we only demonstrated one instance of edss 3 2 step 1 interface design for wasp model control the wasp model includes three core parts the main calculation program the executive file the input file with suffix inp and the output file with suffix out model control includes interface design for wasp inputs and outputs model template configuration and computing job scheduling 3 2 1 preprocessing and model template configuration model preprocessing includes model conceptualization data collection and calibration it should be well prepared before or concurrently with the system development after setting up a well established wasp model a model input file inp is produced and serves as a template for updating into different modeling scenarios we assume that the preprocessing of the wasp model is done separately and is not covered by the edss functionality users can utilize geographic surveys and geographic information systems gis to obtain spatial information of the objective river including urban river length tributary confluence slope roughness pollution discharge outlet distributions etc time series data can be obtained by historical sensors and survey records including hydrological data e g the average monthly river flow and pollution discharge the monthly average discharge of the pollutant discharge outlet lung 2001 3 2 2 input file structure and input control the preliminary part of a model integration is the analysis and control of input files inp the inp file of wasp contains all the information needed for the model computation engine the file structure of the inp file is shown in fig s3 the corresponding parameters can be divided as fixed i e default and adjustable according to the requirements from model edss users some further classified the adjustable parameters into imperatively adjusted and adjustable liu et al 2018 for a model application the target river reach or watershed is usually fixed due to the administration duty therefore the basic geographic information of the river such as river length tributary confluence slope roughness and pollution discharge outlet distribution can be attributed to fixed parameters thus far it has been very difficult to set up a commonly applicable mechanical model for different locations on the other hand business data are changing in the application such as the average monthly stream flow and the monthly sewage discharge from outlets this information belongs to the adjustable parameters where values in the inp file will update on each modeling scenario here the input control function i e interface of wasp is written in the python language after the application component generates the model scheduling configuration file the input control function reads the designing data replaces the corresponding data card in the inp file and generates a new inp file ready for the process of the wasp computation engine 3 2 3 output control and postprocessing when the calculation of the wasp7 5 model program is complete an output file bmd that stores the concentration data for all parameters is generated although the bmd file is a binary file the data can be extracted as csv files according to the settings before running the wasp program the csv files are named after chosen parameters using days as the time interval and each column saves the pollutant concentration calculation results of a control section see fig s4 the output interface function written in python analyzes these output files csv changes daily data into monthly data and stores the calculation results in the corresponding gis database according to the pollution concentration of the control section the wac of each river section and the simulation effect of load reduction it displays on the smart environmental protection platform i e edss in the form of a table at the same time the system can also use multiple methods of map rendering under gis to visualize the calculation results 3 3 step 2 computing components of management measurements this section introduces the framework data flow and interaction of computing components for load reduction simulation and wac calculation coupled with the wasp model program 3 3 1 plr simulation component coupling with wasp the technical route to realize the automatic calculation of load reduction is shown in fig 2 the process is as follows a platform users such as decision makers determine the load reduction plan and the edss platform generates a configuration file in csv format to operate the wasp model b the computing component reads the above created configuration file and updates the inp file as a new scenario based on the established template see item in section 3 2 through the configuration file csv c executes the wasp computing program exe file d after the wasp program runs the output result files which contain a variety of file formats will be processed by the main program of the calculation component to generate the concentration result file csv format containing the concentration of pollutants in each monitored section an example of the file structure is shown below keysection id keysection name segment number month year conc cod conc ammonia note b2 yutian river 1 1 2018 26 582664516129032 1 4424999999999997 b2 yutian river 1 2 2018 27 705407142857137 1 451371428571429 e the load reduction simulation component reads the concentration content of the monitored section in the csv output file for postprocessing and visualization on a computer with an intel seventh generation core i7 processor and 8 gb of memory load reduction calculations can be performed in approximately 3 min 3 3 2 wac calculation component coupling with wasp the technical route of model coupling for wac calculation is shown in fig 3 the application user determines the pollution control target of the control section and imports the information to the calculation component the output interface function reads the simulated concentrations at the control section and judges whether it is close to the pollution control target given by the user if so the main program outputs the pollution load of each river section as the result of the water assimilative capacity for post processing and visualization otherwise it uses the dichotomy to adjust the pollution load and recalculates and iteration continues during the whole calculation process the program keeps running in silent style which can comprehend fully automatic batch processing generally a wac calculation based on the dichotomy requires 5 6 iterations of calculations it can be completed in approximately 45 60 min on a computer equipped with an intel 7th generation core i7 processor and 8 gb of memory the time consumption is totally acceptable for decision makers to work out monthly or annual management plans 3 4 step 3 web service encapsulation and deployment 3 4 1 application encapsulation by opengms ws framework after finishing the design of the plr simulation component and wac calculation component they are encapsulated as web service packages and uploaded to a specific cloud server which is called the opengms wrapper system opengms ws making it possible to run calculation components in the cloud server opengms ws is a web service container that can be deployed in computer resources and publish geographical or environmental models in the form of web services zhang et al 2019 as shown in fig 4 opengms ws has three modules a service publishing module a resource managing module and a model accessing module the model accessing module aims to access kinds of encapsulated native models in the model repository of the system the resource management module can manage the models in the repository the service publishing module is designed to publish web services related to these models with the help of these modules opengms ws can publish model services related to these native models and help users invoke them on the web with the help of related tools and interfaces the measurement component coupled with the water quality model can be encapsulated as a standardized service package that can be deployed in opengms ws and publish related web services the designed measurement component is packaged as a zip file containing a web service model description language mdl document and the wrapped files the mdl document is the most important file in the service package as it determines how the web service runs the input and output interface are also defined by the mdl document the wrapped files are model files that have standardized interfaces interoperation with the wrapper system after deployment the third party server can call the measurement component through python jsp xml or other languages to calculate and communicate with the input and output interface of the water quality model service third party server users can use the measurement component to complete the corresponding water quality management tasks 3 4 2 the deployment of web services the deployment of web services under opengms ws is straightforward fig s5 it only consists of a file package which means users can easily deploy it on the server without installing anything after extracting the file package the service container can be run by opening the service container console exe file then a management page opens and helps manage web services it is easy for users to deploy web services by uploading the encapsulated package file on the deployment page under the local services menu users can check all of the deployed web services and their running statuses on the items page fig s6 opengms ws provides various ways to invoke deployed web services for instance a python code enables clients to communicate with the service container through an ip address and starts a web service calculation by giving a web service api and uploading an input file when the calculation finishes the web container sends a message to the client with the output results file some of the python code is shown in fig s7 3 5 step 4 linking the web services into edss and system integration 3 5 1 linking the web services into edss the edss can easily invoke web services deployed in the established service container by clicking on the link pointing to a python or xml code file which enables user clients to communicate with the input output interface of the web service users can upload their input data through the page and download the output results after the calculation is done 3 5 2 planning oriented edss design for water quality management edss for river water quality management has a broad definition it can refer to programming and planning zhang et al 2021 real time early warning wang et al 2015 waste load allocation paredes et al 2010 contaminant release response ciolofan et al 2018 jiang et al 2012 policy making zolfagharipoor and ahmadi 2016 etc according to the issues emerging on the target river and the interests of the stakeholders mcintosh et al 2011 herein we mainly focus on the functionality of water quality prediction load reduction simulation and wac allocation monitoring data management and basic information of the river are attached to the core functionality of the edss the light workload left for integrating the web services into edss is a local database development and business design for the end users the database for the cloud computing server only contains historical records that call services from the user while the database on the local application server i e edss stores not only model running records but also details of the model s necessary parameters fig s8 and other operation records it is able to manage parameters for each simulation scenario such as boundary conditions and initial conditions which help edss managers make decisions in terms of simulation results the model database in the local edss server can be separately maintained the information of river segmentations and control sections is shown in tables s1 and s2 the model database also contains average flow data for each month in the simulation time period control section information discharge information and historical discharge information the business database supports user interaction for core operation it contains tables of wac calculation control concentration targets wac output data load reduction simulation control load reduction simulation input and load reduction simulation output data visualization and guis are commonly developed by interactive design with users i e administrators or stakeholders of the river java and php are popular for web application development 4 a edss case of maozhou river water quality management 4 1 overview of maozhou river basin the maozhou river is located in northwestern shenzhen and is one of the five major urban rivers in shenzhen the maozhou river originates from yangtai mountain flows through the shiyan gongming guangming songgang shajing and chang an towns in dongguan city and enters the sea at lingdingyang bay in the pearl river estuary the mainstream of the maozhou river is 31 29 km in length the upstream from yangtai mountain to loucun village via shiyan reservoir is 10 32 km at length and the current is relatively fast because of the hilly area loucun to yangyong river sluice for the middle reaches 9 29 km long and is made up of mostly plain or basin terrain the downstream 11 68 km is the boundary river of dongguan city shenzhen which is a tidal section with flat terrain fig 5 as the largest river in shenzhen city the maozhou river has experienced severe water pollution problems throughout history during the rapid development of the city especially in the middle reaches across the guangming district there are several industrial parks on both sides and a large amount of industrial wastewater is received treated by wwtps and then discharged to the maozhou river nonpoint source pollution from neighboring regions is input into the mainstream of the maozhou river through tributaries and stormwater networks the water quality improvement project has been vigorously carried out in shenzhen since 2016 where the maozhou river is a typical example wang et al 2021 the maozhou river comprehensive treatment project led by power china eco environmental group co ltd was basically completed in 2020 to further guarantee the long term effects of meeting national water quality standards integrated water management to strengthen pollution discharge control the tmdl plan will be executed by power china eco environmental group co ltd and encouraged by the shenzhen municipal government in addition shenzhen ecology and environment intelligent management and control center developed a large platform for environmental management our edss demonstrated in this section belongs to the water environment management system of the whole platform 4 2 system architecture of the edss based on the previously established technique route the maozhou river water quality management platform mrwqm dss was developed it includes fundamental monitoring infrastructures cloud servers and communication links see framework in fig 1 mrwqm dss users can understand whether the urban pollution load exceeds the river s own receiving tolerance limit can simulate the effects of various load reduction programs on river pollution control and can provide decision making support for experts to formulate load reduction strategies for different river sections a parallel job schedule strategy is implemented and it recognizes simultaneous calculation of multiple input files improving data processing efficiency the system is currently deployed in shenzhen epa mrwqm dss currently provides two water management measurements plr simulation and wac calculation both rely on the wasp model based web services packages deployed on opengms and this makes the operational running of these two measurements possible 4 3 construction and configuration of the wasp model the guangming section of the maozhou river is selected as the modeling section which starts at the songbai highway bridge of shiyan reservoir and ends at the yanchuan water quality monitoring station in the middle reaches of the maozhou river considering the distribution of tributaries sewage outlets and water quality monitoring stations in the study area the mainstream of the study area is divided into 12 segments as shown in fig 6 the part of the wasp model input file that records the division of rivers is shown in fig 7 the hydrodynamic module we adopted in this model is dynhyd we set the discharge of mainstream and tributaries according to observation results from the local epa then dynhyd module gave the daily discharge of each segments automatically the wasp water quality parameters are manually calibrated see table s3 the lisonglang section one of the city water quality control sections is selected as the model calibrated section and the water quality parameters are adjusted to match the model results with the measured results to verify the accuracy of the model loucun and yanchuan are selected as model check sections to examine whether the simulated values of ammonia nitrogen nh3 n and chemical oxygen demand cod are consistent with the actual values the comparison result between the simulated value and the measured value is shown in fig 8 and fig 9 the mean relative error of the calibration results is approximately 35 see table s4 for each section except for a few points the model generally reflects the overall trend of nh3 n and cod considering objective factors such as model generalization and insufficient historical data it is believed that the wasp model can be used for environmental capacity planning and load reductions of the maozhou river under similar hydrological conditions 4 4 gui and use cases of mrwqm dss one of the system guis wac management is shown in fig 10 the system gui includes three modules the input module on the top left the table and charts on the bottom left and right and the map in the middle decision maker uploads input csv file by clicking the upload button in the input module then click start calc button to start plr or wac calculation on the top of the interface users can click on wac or lr simulation button to choose plr or wac mode after calculation finished the results will be displayed in tables on the bottom left and charts on the right side users can click cod or nh3 button to check the results of two pollutants based on the edss particular stakeholders of the maozhou river can assign the target concentration under control through the main interface and mrwqm dss will provide feedback on the results of the wac using vivid tables and figures here the wac of cod and nh3 n in different segments of the maozhou river is calculated for january 2018 the middle and upper reaches of the maozhou river are divided into two control units shiyan reservoir loucun is the first control unit and loucun yanchuan is the second control unit the pollutant concentration control targets adopted by the two control units are shown in table 1 under this control target the annual wac of cod and nh3 n at each river section is calculated and compared with the actual cod and nh3 n released that year as shown in table 2 table 2 shows that the calculated wac of cod and nh3 n in each section of the middle and upper reaches of the maozhou river in january 2018 is less than the actual pollutant loads estimated the load of nh3 n is approximately 3 4 times the wac and the load of cod is approximately 1 5 times the wac to ensure that the middle and upper reaches of the maozhou river meet the controlling targets table 2 it is necessary to substantially reduce the amount of cod and nh3 n discharged into the maozhou river in fact with the improvement of the maozhou river s governance including load reduction dredging and interception of drainage the wac has been surplus in 2020 the gui and operation process of the load reduction simulation module are similar 4 5 discussion water quality model maintenance mrwqm dss generalizes the arrangement of discharge outlets along river sides where some outlets nonpoint sources and point sources are combined as one outlet it can be further explored and refined to upgrade the model accuracy only if sufficient source data are ready because the hydrological conditions of the maozhou river have changed in recent years due to comprehensive water treatment projects it is recommended that the system update the basic data inp files in web services once a year to ensure that the calculation results fit the actual situation expansibility and reusability mrwqm dss can be rapidly reused in other watersheds by updating and replacing the raw data files required by the wasp model and the underlying inp files in each case it is necessary for users and model developers to jointly discuss which parameters are set as constants and which parameters need to be provided by the user and open to modify permissions the proposed 4 step framework is also effective for encapsulating web services and integrating edss systems for other water environment models such as swmm and efdc zeng et al 2020 for instance the efdc and swmm models can be used for load reduction simulations reservoir water quality management and storm water management the cloud server can become a national platform ready for services plugged in from local edss since the calculating tasks become heavy surrogate models by artificial intelligence may play a role in improving performance advantages against component based models web component based development aims at decoupling web application code modules and making them reusable and customizable software entities tibermacine and kerdoudi 2010 component based models do have the advantages of easy reusing and lower costs as the components used for building models are already exist negi et al 2015 however in some emerging occasions web service based model is more appreciated abbas et al 2019 indicated that service oriented architecture soa is an evolution of component based software engineering cbse which is truly a paradigm shift from typical software engineering having more focus on reusability and flexibility of services compared with component based integration table s5 which still requires users to have a certain understanding of the structure of the model as users need to combine different components to form their unique model to meet the managing requirements service oriented model is also friendly for users who know nothing about the model structure which enables them to deploy conveniently and efficiently moreover service oriented model is accessible on the web which makes it convenient for service users from different platforms to invoke different service oriented models can share with the same input and output structure making it easier for users to deal with different models and control them with flexibility the water quality management is such a classical occasion where service oriented models are more appreciated as mentioned above it is difficult and complex to design and deploy component based water quality models for different rivers on different edss individually it can be convenient by using service oriented model as users do not need to deploy models on their own systems but a link to invoke models instead it is also easy for users to fit the model with the conditions of rivers to meet their own needs by integrating different web services management implications we think that a province or a city with a good spatial scale for integrated watershed management for china can establish a generic web services center platform for watershed water quality management with the promotion of the three lines one permit policy of the china epa upcoming watershed management can benefit from the proposed dss integration framework furthermore water quality forecasting eutrophication management discharge scheduling and other environmental supervision services can be extended in mrwqm dss on business the saas pattern of model based water quality management is a new trend as an emerging business model it provides stable capital flow as customs booked it however the issue of data security is rising if data used for water quality model services refer to classified data it may be a problem to deploy and store on an open commercial server 5 conclusion to pursue a better model based water environmental management service this work proposes an advanced pathway to encapsulate the water quality model and management applications as cloud computing services water assimilation captions and load reduction are integrated in edss as the two typical model based measurements of urban water environment management taking the maozhou river basin in shenzhen as the study area an edss of maozhou river water quality management is developed this study hopes to provide a valuable reference for the smart water or digital water industry hayward 2020 the proposed four step pathway of integration model control application component service encapsulate dss integration is generic to various applications and scenarios cloud computing services are also understood under the same framework not only for wasp but also for efdc and swmm opengms herein helps to implement the services oriented encapsulation of wasp and its applications it also enriches the ecology of open models e g https github com openmodelingfoundation the edss case on the maozhou river successfully calculates the corresponding wac in different river sections and formulates load reduction plans in a targeted manner to the best of our knowledge this is the first study to realize the customization and operation of the advanced version of the wasp model the most popular 2d receiving water quality model the proposed model coupling and integration technology fundamental computing components and pathway of edss development are all helpful to serve the water related sectors and stakeholders such as the water resources bureau the ecology and environment bureau and the river governance company more efficiently manage the water environment software availability the model package of plr and wac can be download at https zsesys zsest cn webservice packages html contacting jiping jiang jiangjp sustech edu cn for details opengms can be downloaded at github contacting fengyuan zhang zhangfengyuangis 163 com for details a demonstration version of mrwqm dss is accessible at https zsesys zsest cn maozhou edss index html the formal version of maorwqm dss is commercial and deployed in shenzhen epa china it is not accessible from internet due to copyrights and confidentiality agreement declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by national natural science foundation of china 51979136 51779066 science technology and innovation commission of shenzhen jsgg20201103094600001 key area research and development program of guangdong province 2019b110205005 and open project of state key laboratory of urban water resource and environment harbin institute of technology grant no esk201901 thanks to ping an international smart city technology co ltd shenzhen ecology and environment intelligent management and control center power china eco environmental group co ltd for supporting the study we are grateful for constructive comments from yong tian appendix a supplementary data the following is the supplementary data to this article supplementary materials include 8 figures and 5 tables the details of each are as follows diagrams of the eutrophication process in wasp fig s1 model network with advective transport pathways fig s2 structures of the wasp inp file fig s3 structures of the wasp output csv file fig s4 frameworks of opengms fig s5 gui of web service deployment in opengms fig s6 python codes to invoke web services fig s7 er diagrams of the edss database fig s8 river segment data in databases table s1 control unit data in databases table s2 parameters for wasp model of maozhou river table s3 calibration performances of cod and nh3 n table s4 and comparison between component based models and service oriented models table s5 multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105280 
25666,the encapsulation of specific management measurements and computing services is rarely reported for surface water edss to promote the efficiency and reduce the complexity of surface water edss development a generic pathway to encapsulate the surface water quality model and its applications as cloud computing services is proposed the web service is implemented under the opengms framework two typical management measurements water assimilative capacity allocation and pollution load reduction planning are encapsulated based on the advanced version of the wasp program an edss instance of the maozhou river shenzhen is developed which provides precise management measurements for cod and nh3 n control management services provided by other models efdc and swmm are also implemented under this novel framework the proposed pathway provides the merits of rapid development easy deployment and maintenance of an advanced edss it presents a good demonstration of coupling advanced wasp programs within edss for receiving water management keywords industrial cloud maozhou river model integration surface water quality model water assimilative capacity web services 1 introduction urban rivers the most accessible water environments are essential for human life many measurements for surface water quality planning and management have been developed around the world to protect the water environment and sustainability such as total pollution load control water function zone division environment carrying capacity layout planning of wastewater effluent outlets water assimilation capacity management and water quality early warning and forecasting the well recognized total maximum daily loads tmdls plan by the usepa lung 2001 mohamoud and zhang 2019 wang et al 2019 and the recent three lines one permit tlop policy by the chinese epa wang et al 2020 are national level instances of management measurements they are all model based simple or complex and computational engeland and alfredsen 2020 wang et al 2016 many efforts have been made by experts and governments to develop these modeling tools for example the water quality analysis simulation program wasp and environmental fluid dynamic code efdc by the usepa have been widely validated and used in a large number of practices in different countries bouchard et al 2017 jiang et al 2019 furthermore in the era of smart cities developing an environmental decision support system edss mcintosh et al 2011 integrated with monitoring modeling planning and control for smart management of urban river water quality has been widely accepted and implemented around the world chen and han 2018 rui et al 2015 integrating these management measures and modeling tools into the edss tang et al 2016 can greatly improve the convenience and efficiency of decision making therefore coupling localized water quality models as operational running components to meet different management requirements is one of the fundamental technologies for surface water edss development ruiz ortiz et al 2019 mckinney and cai 2002 first proposed the concept of combining geographic information systems giss with water resource management models the traditional pathway of model integrations with edss is usually database oriented interaction and has succeeded in many circumstances martin et al 2004 proposed the first water pollution management information system that couples gis and water quality models lee and hyun 2008 integrated wasp7 and efdc with giss on the han river and kyeonggi bay for best management practices bmps in korea a gis database was built to estimate the amount of generated and discharged water pollutants according to the tmdl technical guide peng et al 2011 utilized a similar database oriented pathway to integrate the different types of efdc modeling data into a gis platform in the lower charles river basin in the usa efforts have also been made to integrate usepa swmm liao et al 2012 liu et al 2016 martin et al 2020 however the traditional integration pathways are non modular with less efficient share and reuse capabilities and struggle with system updating edss development requires close cooperation of professional software developers and model experts to learn from each other and clarify the data structure of the platform system the interactive format and calling of model parameters it is challenging to master and re edit the source code of open models liu et al 2018 nevertheless it is worth contemplating how to reduce the threshold of using the professional model improve the calculation efficiency and understand the business operations behind the professional model software it not only strengthens the prediction and simulation ability of the water quality management information system but also improves the universality of the model which is convenient for general technical personnel managers and decision makers liu et al 2018 zeng et al 2020 with the development of cloud computing and web services service oriented design is an emerging trend of model coupling although cloud computing is not a new concept as its commercialization started in approximately 2000 the global use of computers and smartphones has increased the needs of high speed and high capacity computing tasks and cloud computing seeks to address these needs senyo et al 2018 cloud computing often appears in the form of system services which enables ubiquitous convenient on demand network access to a shared pool of configurable computing resources there are three types of cloud computing service models software as a service saas platform as a service paas and infrastructure as a service iaas zissis and lekkas 2012 cloud computing together with the iot and big data has shown great application prospects in manufacturing healthcare aceto et al 2020 and management the widely design and use of cloud computing in industries known as industry cloud has been considered as a common function among industry solutions and the usage with use case scenarios kushida and pingali 2014 under this emerging technical background service oriented interface design for open distributed environmental simulations is receiving increasing attention in the gis area and environmental modeling field zhang et al 2020 a position paper indicates that distributed service oriented models would be more popular in model sharing and application chen et al 2020 there are many kinds of service oriented models applied in different domains goodall et al 2011 presented a service oriented interface design for integrating water resource models o donncha et al 2016 deployed the efdc on a container based computer to provide a solution that saves time in simulation xiao et al 2019 developed a web system for swmm application and zeng et al 2020 proposed a web service framework based swmm for real time urban stormwater management web service oriented encapsulation of efdc and swmm has been recently reported liu et al 2018 however it is curious that no web service oriented software package has been developed to facilitate the application of wasp though it is a standard receiving waterbody environmental management model there is an obvious gap in water quality management practices in the literature jia et al 2001 first integrated the wasp 5 model with gis in a semi tight integration mode which provided strong support for the spatial simulation of surface water quality and water quality management peng et al 2010 completely embedded the wasp 5 water quality model into the gis platform for the first time so that the input and output data of the wasp model were connected with the gis database and could be displayed graphically li and zhang 2012 fully integrated the eutrophication module in wasp 5 with gis and adapted it to a submodule of the gis analysis system making an active attempt to study large scale hydrodynamics and water quality model systems based on gis in addition almost all the wasp model integration reported in the literature is implemented by analyzing and encapsulating the original fortran code of wasp 5 these source codes were developed as early as the 1980s and this model integration pattern is time consuming to some degree few studies have reported the coupling technique for the latest version of the wasp program with the edss platform which hinders the deployment and application of high level automation of the model program in addition during the emerging web service wave there was a lack of service encapsulation for management measurements current research and practice are mostly aimed at the model computing engine itself only a generic model computation service is provided without further encapsulation for the model based management business which is not thorough enough moreover the pathway of forming management measurements into business tools by encapsulating environment models and integrating them with edss has not yet been reported as a result this kind of model integration is not linked to the business level of water environment management planning which still has an existing technical threshold for terminal users or developers of edss with high repeatability of integration work and low efficiency this is apparently not enough in the era of the massive practice of smart cities the technical threshold of developers needs to be liberated it can also be further developed to enable local management agencies to deploy model applications conveniently and efficiently facilitate system upgrades and maintenance and call by different user platforms to fill these technology gaps this paper aims to develop a web service oriented software package encapsulated with the wasp v7 5 program and propose a technical route of integrating water quality management measurements with edss in the form of a web service business tool the web service encapsulation of localized wasp models and advanced cloud computing services will be implemented with the help of open geographic modeling and simulation opengms and related tools zhang et al 2020 as a saas pattern two water quality management measurements water assimilative capacity wac allocation and pollution load reduction plr planning are carried out by cloud computing services which achieve operational edss running a case study is conducted with real edss deployment in the maozhou river shenzhen china in the following section this paper discusses the structure of the wasp model and designs basic algorithms of two typical water quality planning measurements load reduction simulation and wac computing section 3 introduces the proposed process of encapsulating wasp based computing programs as web services deployment on cloud servers service containers and coupling web services with edss systems section 4 shows an edss case study on the maozhou river and presents how the wasp based computing program integrates with the edss system in the form of web services then it discusses the implications of the technical route for other water quality models such as efdc or swmm the advantages and further applications 2 methods as wasp was selected as the major surface water quality model for integration the mechanism and versions were introduced in this section the algorithms of plr and wac were introduced as the typical water quality management measurements selected 2 1 mechanism of wasp model the wasp model is currently one of the most widely used water quality models and has been used in thousands of rivers lakes reservoirs and other bodies of water around the world it performs well for capturing the processes of dissolved oxygen bacteria eutrophication toxic substances and other biochemical processes compared to other water quality models wasp has the characteristics of substantial flexibility high versatility and accurate simulation results peng et al 2010 wool et al 2020 the wasp model consists of two modules the hydrodynamic module dynhyd which is used to simulate hydrodynamic processes such as river flow and velocity and the water quality module wasp which contains the eutrophication model eutro see fig s1 the toxic substance model toxi and the mercury model mercury these models are used to simulate the migration and reaction process of pollutants in water the two modules can operate independently without affecting each other wool et al 2020 the wasp hydrodynamic models are based on a set of transport and transformation equations advective transport is driven by water flow through a specified computational network see fig s2 the main differential equations of the wasp hydrodynamic model are as follows 1 q x a t 0 2 d u x d t u x d u x d x a g a f where q volumetric flow m3 s a cross sectional area m2 u x river velocity m s a g gravitational acceleration along the axis of the channel m2 s a f frictional acceleration m2 s equations 1 and 2 form the basis of the hydrodynamic model dynhyd which is implemented within wasp the water column flow can also connect with other hydrodynamic models alternatively such as epdriv1 or efdc the main differential equation of the wasp water quality model is as follows wan and mao 2015 3 c t x d x c x u x c s l s b s k where c the pollutant concentration at x downstream of the pollutant discharge point when the migration time is t mg l d x diffusion coefficient of pollutants m2 s u x river velocity m s s l source and sink loads of pollutants entering rivers s b boundary pollution load of pollutants s k biochemical reaction items of pollutants compared to the one dimensional solute transport model the wasp model replaces the attenuation term in the one dimensional solute transport model with the sum of the pollution source term s l the boundary load term s b and the biochemical reaction term s k in the wasp model the pollutants are imported as point sources in each segments it is assumed that the pollutant is a steady state emission in each month and is equally distributed in each day the minimum time step of our wasp model which is adopted in environmental management on a large time scale like annual planning 2 2 versions of wasp program wasp initially runs under dos and the code is open source after 2001 it was upgraded to the wasp 6 version running under windows 98 and the wasp7 version was issued in 2005 which matched windows 2000 and xp systems wasp 7 substantially improved pre and post treatment and the operation efficiency was increased by more than ten times more efficient eutrophication and organic pollutant treatment modules have been built chen et al 2006 wasp version 8 released later in 2018 eliminates silent running interfaces and cannot be tightly coupled with third party platforms to complete complex automated customization operations wool et al 2020 the source code is no longer accessible since version 6 0 which caused difficulties for customization and integration with edss to the best of our knowledge there are no reports of coupling wasp with automatic water quality management under higher versions this study integrates the 7 5 version released in 2012 with complete functions its eutrophication module is stable and efficient and suitable for more complex watershed management requirements such as water assimilation capacity management and pollutant load reduction planning it has a clandestine scheduling interface with silent batch operation which provides an opportunity for customization for edss 2 3 two typical model based management measurements from an environmental engineering point of view watershed water quality management refers to many model based applications e g load reduction sewage outfall setting wac allocation ecological water supply etc here we highlight and design the algorithms for plr planning and wac allocation the two most widely used management measurements the plan to control the pollution source loading into the receiving body of water is fundamental to water environment quality management pollution reduction is parameterized by reducing the pollution source intensity of each discharge outlet specifically the pre established pollution reduction scenario data the set pollutant discharge amount of each river section are imported into the model input file the water quality calculation model is called for calculation and the control section under the simulation scenario of emission reduction is obtained the pollutant concentration in the simulated situation can be compared with the pollution control target to evaluate the pollution reduction effect in addition plr is closely related to the issue of pollution load allocation the latter is an optimization problem considering socioeconomic equity among different pollution sources and corresponding regions su et al 2019 this paper only implements plr itself while neglecting the reduction policy the wac provides an essential basis for decision makers to draw the red line of sewage discharge and formulate the abovementioned load reduction plans faulkner 2008 novotny and krenkel 1975 scherer 1975 the calculation of wac is generally based on scientific analysis of the hydrological and water conservancy characteristics natural geographical conditions pollutant discharge methods of the river section etc to calculate how many pollutants the river can tolerate jiang et al 2019 novotny and krenkel 1975 zhang et al 1998 wac is a linkage of water quality models to waste load management measures farhadian et al 2015 both simplified models and advanced models such as wasp can be used for wac calculations after decades of development currently commonly used calculation methods for wac include the analytical formula method trial and error method and systematic optimization method dong et al 2014 the calculation of wac in our case is set as an optimization problem whose optimization goal is to make the characteristic pollutant concentration at the control units close to a given threshold the optimization goal and constraints of wac is shown in equation 6 4 min f c w c s s t w 0 ε c w c s c s 0 02 c s constants where w water assimilation capacity at segment g s c w concentration at the segment section where the pollutant load is w mg l c s concentration threshold at segment mg l the segment section where c w is located has different selection methods and three calculation modes are derived the segment head control method segment end control method and control unit end control method lei 2006 the word segment here means the river segment between two pollutant outlets segment head is the upper outlet of one river segment and segment head control is to make the pollutant concentration at the upper outlet close to the controlling threshold control unit end control means making only the end outlet of a control unit a combination of several continuous segments meets the controlling threshold regardless of the change of water quality in the control unit here we choose the control unit end control in case study as in case study we separated 12 segments into two function areas which named control unit 1 and control unit 2 respectively see fig 6 to realize the automatic calculation of the wac based on the determination of water function zoning and pollution control objectives of each reach of urban rivers this study adopts the definition of the segmental control method and the optimization method of dichotomy based on the wasp model the dichotomy formula is as follows 5 a n 1 a n 1 2 n a 1 n 1 2 where a n the river section pollutant load in the nth iteration calculation t n number of iterations equation 5 is an iteration algorithm to solve the optimization equations of wac as presented at equation 4 the initial pollutant load a 1 launched the calculation of pollutant concentrations at the control section compared to the target concentration under control the formula of a 2 takes the negative sign if calculation results exceed the target otherwise it takes the positive sign the same operation continues until the simulated pollutant concentration is close to the target concentration when the iterative calculation is terminated the pollutant load of the river section is assigned the wac of the river reach section 3 a generic pathway of wasp based web services integrated into edss 3 1 overall technology road map the overall technical road map of the proposed edss integration for surface water quality management can be summarized as follows model control interface management application component web service encapsulation system integration a diagram of the pathway is shown in fig 1 it builds up wasp based saas web services for load reduction and wac computing which provide a core function for an edss system the four key steps are explained in section 3 this framework addresses the three technology and application gaps mentioned before understanding the complete encapsulation from model to business layer comprehending the web service of water environment model and its environmental management business completing the implementation of wasp model application and breaking through the integration problems on its advanced version these are all fulfilled in a unified framework fig 1 as detailed in the illustration in the below the system consists of three layers cloud server layer application server layer and browser layer the cloud server contains wasp program engine and a model container where web services are deployed we take plr and wac as instances of web services each web services has its own computing components a set of encapsulated program package and a model description file mdl which defines the input and output interface of the web service the application server layer can contains different edss systems each edss is able to invoke web services deployed on the cloud server independently through designated scripts the browser layer is open for client users different stakeholders who can upload inputs from their own edss and receives analysis results downloaded from cloud server owning to the flexible architecture associated with the encapsulate pathway service oriented system herein has many benefits in the circumstances of water quality management and planning many stakeholders will participate the regulatory government departments including water affairs bureau environmental protection bureau and urban management bureau consulting institutions such as research academy of environmental sciences universities market subjects including water utilities hydraulic engineering companies and wastewater treatment companies residents and the public they can quickly customize a platform i e edss that meets their own needs by integrating different industry cloud services more points will be systematically summarized on the discussion section to simply in this paper we only demonstrated one instance of edss 3 2 step 1 interface design for wasp model control the wasp model includes three core parts the main calculation program the executive file the input file with suffix inp and the output file with suffix out model control includes interface design for wasp inputs and outputs model template configuration and computing job scheduling 3 2 1 preprocessing and model template configuration model preprocessing includes model conceptualization data collection and calibration it should be well prepared before or concurrently with the system development after setting up a well established wasp model a model input file inp is produced and serves as a template for updating into different modeling scenarios we assume that the preprocessing of the wasp model is done separately and is not covered by the edss functionality users can utilize geographic surveys and geographic information systems gis to obtain spatial information of the objective river including urban river length tributary confluence slope roughness pollution discharge outlet distributions etc time series data can be obtained by historical sensors and survey records including hydrological data e g the average monthly river flow and pollution discharge the monthly average discharge of the pollutant discharge outlet lung 2001 3 2 2 input file structure and input control the preliminary part of a model integration is the analysis and control of input files inp the inp file of wasp contains all the information needed for the model computation engine the file structure of the inp file is shown in fig s3 the corresponding parameters can be divided as fixed i e default and adjustable according to the requirements from model edss users some further classified the adjustable parameters into imperatively adjusted and adjustable liu et al 2018 for a model application the target river reach or watershed is usually fixed due to the administration duty therefore the basic geographic information of the river such as river length tributary confluence slope roughness and pollution discharge outlet distribution can be attributed to fixed parameters thus far it has been very difficult to set up a commonly applicable mechanical model for different locations on the other hand business data are changing in the application such as the average monthly stream flow and the monthly sewage discharge from outlets this information belongs to the adjustable parameters where values in the inp file will update on each modeling scenario here the input control function i e interface of wasp is written in the python language after the application component generates the model scheduling configuration file the input control function reads the designing data replaces the corresponding data card in the inp file and generates a new inp file ready for the process of the wasp computation engine 3 2 3 output control and postprocessing when the calculation of the wasp7 5 model program is complete an output file bmd that stores the concentration data for all parameters is generated although the bmd file is a binary file the data can be extracted as csv files according to the settings before running the wasp program the csv files are named after chosen parameters using days as the time interval and each column saves the pollutant concentration calculation results of a control section see fig s4 the output interface function written in python analyzes these output files csv changes daily data into monthly data and stores the calculation results in the corresponding gis database according to the pollution concentration of the control section the wac of each river section and the simulation effect of load reduction it displays on the smart environmental protection platform i e edss in the form of a table at the same time the system can also use multiple methods of map rendering under gis to visualize the calculation results 3 3 step 2 computing components of management measurements this section introduces the framework data flow and interaction of computing components for load reduction simulation and wac calculation coupled with the wasp model program 3 3 1 plr simulation component coupling with wasp the technical route to realize the automatic calculation of load reduction is shown in fig 2 the process is as follows a platform users such as decision makers determine the load reduction plan and the edss platform generates a configuration file in csv format to operate the wasp model b the computing component reads the above created configuration file and updates the inp file as a new scenario based on the established template see item in section 3 2 through the configuration file csv c executes the wasp computing program exe file d after the wasp program runs the output result files which contain a variety of file formats will be processed by the main program of the calculation component to generate the concentration result file csv format containing the concentration of pollutants in each monitored section an example of the file structure is shown below keysection id keysection name segment number month year conc cod conc ammonia note b2 yutian river 1 1 2018 26 582664516129032 1 4424999999999997 b2 yutian river 1 2 2018 27 705407142857137 1 451371428571429 e the load reduction simulation component reads the concentration content of the monitored section in the csv output file for postprocessing and visualization on a computer with an intel seventh generation core i7 processor and 8 gb of memory load reduction calculations can be performed in approximately 3 min 3 3 2 wac calculation component coupling with wasp the technical route of model coupling for wac calculation is shown in fig 3 the application user determines the pollution control target of the control section and imports the information to the calculation component the output interface function reads the simulated concentrations at the control section and judges whether it is close to the pollution control target given by the user if so the main program outputs the pollution load of each river section as the result of the water assimilative capacity for post processing and visualization otherwise it uses the dichotomy to adjust the pollution load and recalculates and iteration continues during the whole calculation process the program keeps running in silent style which can comprehend fully automatic batch processing generally a wac calculation based on the dichotomy requires 5 6 iterations of calculations it can be completed in approximately 45 60 min on a computer equipped with an intel 7th generation core i7 processor and 8 gb of memory the time consumption is totally acceptable for decision makers to work out monthly or annual management plans 3 4 step 3 web service encapsulation and deployment 3 4 1 application encapsulation by opengms ws framework after finishing the design of the plr simulation component and wac calculation component they are encapsulated as web service packages and uploaded to a specific cloud server which is called the opengms wrapper system opengms ws making it possible to run calculation components in the cloud server opengms ws is a web service container that can be deployed in computer resources and publish geographical or environmental models in the form of web services zhang et al 2019 as shown in fig 4 opengms ws has three modules a service publishing module a resource managing module and a model accessing module the model accessing module aims to access kinds of encapsulated native models in the model repository of the system the resource management module can manage the models in the repository the service publishing module is designed to publish web services related to these models with the help of these modules opengms ws can publish model services related to these native models and help users invoke them on the web with the help of related tools and interfaces the measurement component coupled with the water quality model can be encapsulated as a standardized service package that can be deployed in opengms ws and publish related web services the designed measurement component is packaged as a zip file containing a web service model description language mdl document and the wrapped files the mdl document is the most important file in the service package as it determines how the web service runs the input and output interface are also defined by the mdl document the wrapped files are model files that have standardized interfaces interoperation with the wrapper system after deployment the third party server can call the measurement component through python jsp xml or other languages to calculate and communicate with the input and output interface of the water quality model service third party server users can use the measurement component to complete the corresponding water quality management tasks 3 4 2 the deployment of web services the deployment of web services under opengms ws is straightforward fig s5 it only consists of a file package which means users can easily deploy it on the server without installing anything after extracting the file package the service container can be run by opening the service container console exe file then a management page opens and helps manage web services it is easy for users to deploy web services by uploading the encapsulated package file on the deployment page under the local services menu users can check all of the deployed web services and their running statuses on the items page fig s6 opengms ws provides various ways to invoke deployed web services for instance a python code enables clients to communicate with the service container through an ip address and starts a web service calculation by giving a web service api and uploading an input file when the calculation finishes the web container sends a message to the client with the output results file some of the python code is shown in fig s7 3 5 step 4 linking the web services into edss and system integration 3 5 1 linking the web services into edss the edss can easily invoke web services deployed in the established service container by clicking on the link pointing to a python or xml code file which enables user clients to communicate with the input output interface of the web service users can upload their input data through the page and download the output results after the calculation is done 3 5 2 planning oriented edss design for water quality management edss for river water quality management has a broad definition it can refer to programming and planning zhang et al 2021 real time early warning wang et al 2015 waste load allocation paredes et al 2010 contaminant release response ciolofan et al 2018 jiang et al 2012 policy making zolfagharipoor and ahmadi 2016 etc according to the issues emerging on the target river and the interests of the stakeholders mcintosh et al 2011 herein we mainly focus on the functionality of water quality prediction load reduction simulation and wac allocation monitoring data management and basic information of the river are attached to the core functionality of the edss the light workload left for integrating the web services into edss is a local database development and business design for the end users the database for the cloud computing server only contains historical records that call services from the user while the database on the local application server i e edss stores not only model running records but also details of the model s necessary parameters fig s8 and other operation records it is able to manage parameters for each simulation scenario such as boundary conditions and initial conditions which help edss managers make decisions in terms of simulation results the model database in the local edss server can be separately maintained the information of river segmentations and control sections is shown in tables s1 and s2 the model database also contains average flow data for each month in the simulation time period control section information discharge information and historical discharge information the business database supports user interaction for core operation it contains tables of wac calculation control concentration targets wac output data load reduction simulation control load reduction simulation input and load reduction simulation output data visualization and guis are commonly developed by interactive design with users i e administrators or stakeholders of the river java and php are popular for web application development 4 a edss case of maozhou river water quality management 4 1 overview of maozhou river basin the maozhou river is located in northwestern shenzhen and is one of the five major urban rivers in shenzhen the maozhou river originates from yangtai mountain flows through the shiyan gongming guangming songgang shajing and chang an towns in dongguan city and enters the sea at lingdingyang bay in the pearl river estuary the mainstream of the maozhou river is 31 29 km in length the upstream from yangtai mountain to loucun village via shiyan reservoir is 10 32 km at length and the current is relatively fast because of the hilly area loucun to yangyong river sluice for the middle reaches 9 29 km long and is made up of mostly plain or basin terrain the downstream 11 68 km is the boundary river of dongguan city shenzhen which is a tidal section with flat terrain fig 5 as the largest river in shenzhen city the maozhou river has experienced severe water pollution problems throughout history during the rapid development of the city especially in the middle reaches across the guangming district there are several industrial parks on both sides and a large amount of industrial wastewater is received treated by wwtps and then discharged to the maozhou river nonpoint source pollution from neighboring regions is input into the mainstream of the maozhou river through tributaries and stormwater networks the water quality improvement project has been vigorously carried out in shenzhen since 2016 where the maozhou river is a typical example wang et al 2021 the maozhou river comprehensive treatment project led by power china eco environmental group co ltd was basically completed in 2020 to further guarantee the long term effects of meeting national water quality standards integrated water management to strengthen pollution discharge control the tmdl plan will be executed by power china eco environmental group co ltd and encouraged by the shenzhen municipal government in addition shenzhen ecology and environment intelligent management and control center developed a large platform for environmental management our edss demonstrated in this section belongs to the water environment management system of the whole platform 4 2 system architecture of the edss based on the previously established technique route the maozhou river water quality management platform mrwqm dss was developed it includes fundamental monitoring infrastructures cloud servers and communication links see framework in fig 1 mrwqm dss users can understand whether the urban pollution load exceeds the river s own receiving tolerance limit can simulate the effects of various load reduction programs on river pollution control and can provide decision making support for experts to formulate load reduction strategies for different river sections a parallel job schedule strategy is implemented and it recognizes simultaneous calculation of multiple input files improving data processing efficiency the system is currently deployed in shenzhen epa mrwqm dss currently provides two water management measurements plr simulation and wac calculation both rely on the wasp model based web services packages deployed on opengms and this makes the operational running of these two measurements possible 4 3 construction and configuration of the wasp model the guangming section of the maozhou river is selected as the modeling section which starts at the songbai highway bridge of shiyan reservoir and ends at the yanchuan water quality monitoring station in the middle reaches of the maozhou river considering the distribution of tributaries sewage outlets and water quality monitoring stations in the study area the mainstream of the study area is divided into 12 segments as shown in fig 6 the part of the wasp model input file that records the division of rivers is shown in fig 7 the hydrodynamic module we adopted in this model is dynhyd we set the discharge of mainstream and tributaries according to observation results from the local epa then dynhyd module gave the daily discharge of each segments automatically the wasp water quality parameters are manually calibrated see table s3 the lisonglang section one of the city water quality control sections is selected as the model calibrated section and the water quality parameters are adjusted to match the model results with the measured results to verify the accuracy of the model loucun and yanchuan are selected as model check sections to examine whether the simulated values of ammonia nitrogen nh3 n and chemical oxygen demand cod are consistent with the actual values the comparison result between the simulated value and the measured value is shown in fig 8 and fig 9 the mean relative error of the calibration results is approximately 35 see table s4 for each section except for a few points the model generally reflects the overall trend of nh3 n and cod considering objective factors such as model generalization and insufficient historical data it is believed that the wasp model can be used for environmental capacity planning and load reductions of the maozhou river under similar hydrological conditions 4 4 gui and use cases of mrwqm dss one of the system guis wac management is shown in fig 10 the system gui includes three modules the input module on the top left the table and charts on the bottom left and right and the map in the middle decision maker uploads input csv file by clicking the upload button in the input module then click start calc button to start plr or wac calculation on the top of the interface users can click on wac or lr simulation button to choose plr or wac mode after calculation finished the results will be displayed in tables on the bottom left and charts on the right side users can click cod or nh3 button to check the results of two pollutants based on the edss particular stakeholders of the maozhou river can assign the target concentration under control through the main interface and mrwqm dss will provide feedback on the results of the wac using vivid tables and figures here the wac of cod and nh3 n in different segments of the maozhou river is calculated for january 2018 the middle and upper reaches of the maozhou river are divided into two control units shiyan reservoir loucun is the first control unit and loucun yanchuan is the second control unit the pollutant concentration control targets adopted by the two control units are shown in table 1 under this control target the annual wac of cod and nh3 n at each river section is calculated and compared with the actual cod and nh3 n released that year as shown in table 2 table 2 shows that the calculated wac of cod and nh3 n in each section of the middle and upper reaches of the maozhou river in january 2018 is less than the actual pollutant loads estimated the load of nh3 n is approximately 3 4 times the wac and the load of cod is approximately 1 5 times the wac to ensure that the middle and upper reaches of the maozhou river meet the controlling targets table 2 it is necessary to substantially reduce the amount of cod and nh3 n discharged into the maozhou river in fact with the improvement of the maozhou river s governance including load reduction dredging and interception of drainage the wac has been surplus in 2020 the gui and operation process of the load reduction simulation module are similar 4 5 discussion water quality model maintenance mrwqm dss generalizes the arrangement of discharge outlets along river sides where some outlets nonpoint sources and point sources are combined as one outlet it can be further explored and refined to upgrade the model accuracy only if sufficient source data are ready because the hydrological conditions of the maozhou river have changed in recent years due to comprehensive water treatment projects it is recommended that the system update the basic data inp files in web services once a year to ensure that the calculation results fit the actual situation expansibility and reusability mrwqm dss can be rapidly reused in other watersheds by updating and replacing the raw data files required by the wasp model and the underlying inp files in each case it is necessary for users and model developers to jointly discuss which parameters are set as constants and which parameters need to be provided by the user and open to modify permissions the proposed 4 step framework is also effective for encapsulating web services and integrating edss systems for other water environment models such as swmm and efdc zeng et al 2020 for instance the efdc and swmm models can be used for load reduction simulations reservoir water quality management and storm water management the cloud server can become a national platform ready for services plugged in from local edss since the calculating tasks become heavy surrogate models by artificial intelligence may play a role in improving performance advantages against component based models web component based development aims at decoupling web application code modules and making them reusable and customizable software entities tibermacine and kerdoudi 2010 component based models do have the advantages of easy reusing and lower costs as the components used for building models are already exist negi et al 2015 however in some emerging occasions web service based model is more appreciated abbas et al 2019 indicated that service oriented architecture soa is an evolution of component based software engineering cbse which is truly a paradigm shift from typical software engineering having more focus on reusability and flexibility of services compared with component based integration table s5 which still requires users to have a certain understanding of the structure of the model as users need to combine different components to form their unique model to meet the managing requirements service oriented model is also friendly for users who know nothing about the model structure which enables them to deploy conveniently and efficiently moreover service oriented model is accessible on the web which makes it convenient for service users from different platforms to invoke different service oriented models can share with the same input and output structure making it easier for users to deal with different models and control them with flexibility the water quality management is such a classical occasion where service oriented models are more appreciated as mentioned above it is difficult and complex to design and deploy component based water quality models for different rivers on different edss individually it can be convenient by using service oriented model as users do not need to deploy models on their own systems but a link to invoke models instead it is also easy for users to fit the model with the conditions of rivers to meet their own needs by integrating different web services management implications we think that a province or a city with a good spatial scale for integrated watershed management for china can establish a generic web services center platform for watershed water quality management with the promotion of the three lines one permit policy of the china epa upcoming watershed management can benefit from the proposed dss integration framework furthermore water quality forecasting eutrophication management discharge scheduling and other environmental supervision services can be extended in mrwqm dss on business the saas pattern of model based water quality management is a new trend as an emerging business model it provides stable capital flow as customs booked it however the issue of data security is rising if data used for water quality model services refer to classified data it may be a problem to deploy and store on an open commercial server 5 conclusion to pursue a better model based water environmental management service this work proposes an advanced pathway to encapsulate the water quality model and management applications as cloud computing services water assimilation captions and load reduction are integrated in edss as the two typical model based measurements of urban water environment management taking the maozhou river basin in shenzhen as the study area an edss of maozhou river water quality management is developed this study hopes to provide a valuable reference for the smart water or digital water industry hayward 2020 the proposed four step pathway of integration model control application component service encapsulate dss integration is generic to various applications and scenarios cloud computing services are also understood under the same framework not only for wasp but also for efdc and swmm opengms herein helps to implement the services oriented encapsulation of wasp and its applications it also enriches the ecology of open models e g https github com openmodelingfoundation the edss case on the maozhou river successfully calculates the corresponding wac in different river sections and formulates load reduction plans in a targeted manner to the best of our knowledge this is the first study to realize the customization and operation of the advanced version of the wasp model the most popular 2d receiving water quality model the proposed model coupling and integration technology fundamental computing components and pathway of edss development are all helpful to serve the water related sectors and stakeholders such as the water resources bureau the ecology and environment bureau and the river governance company more efficiently manage the water environment software availability the model package of plr and wac can be download at https zsesys zsest cn webservice packages html contacting jiping jiang jiangjp sustech edu cn for details opengms can be downloaded at github contacting fengyuan zhang zhangfengyuangis 163 com for details a demonstration version of mrwqm dss is accessible at https zsesys zsest cn maozhou edss index html the formal version of maorwqm dss is commercial and deployed in shenzhen epa china it is not accessible from internet due to copyrights and confidentiality agreement declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by national natural science foundation of china 51979136 51779066 science technology and innovation commission of shenzhen jsgg20201103094600001 key area research and development program of guangdong province 2019b110205005 and open project of state key laboratory of urban water resource and environment harbin institute of technology grant no esk201901 thanks to ping an international smart city technology co ltd shenzhen ecology and environment intelligent management and control center power china eco environmental group co ltd for supporting the study we are grateful for constructive comments from yong tian appendix a supplementary data the following is the supplementary data to this article supplementary materials include 8 figures and 5 tables the details of each are as follows diagrams of the eutrophication process in wasp fig s1 model network with advective transport pathways fig s2 structures of the wasp inp file fig s3 structures of the wasp output csv file fig s4 frameworks of opengms fig s5 gui of web service deployment in opengms fig s6 python codes to invoke web services fig s7 er diagrams of the edss database fig s8 river segment data in databases table s1 control unit data in databases table s2 parameters for wasp model of maozhou river table s3 calibration performances of cod and nh3 n table s4 and comparison between component based models and service oriented models table s5 multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105280 
25667,the computational burden of running a semi distributed hydrological model numerous times such as for optimization applications can be exorbitant this study provides a surrogate model to estimate streamflow nutrient and sediment export under spatially distributed management decisions specifically we surrogate the soil and water assessment tool swat using a modified response matrix rm approach a traditional rm approach applied to swat assumes hydrological responses are approximated by linear functions of management decisions and falls short in accounting for in stream and reservoir processes here we explain and illustrate three key modifications that address interaction effects between co located conservation practices and in stream and reservoir processes affecting nutrient and sediment loads the modified rm approach provides excellent estimation nash sutcliffe efficiency nse 0 95 for streamflow and nutrient export throughout the stream network and provides very good estimation nse 0 85 for sediment export at most though not all points in the stream network keywords surrogate modeling nutrient transport watershed modeling soil and water assessment tool swat management decisions 1 introduction many process based hydrological watershed modeling tools have been developed to evaluate the effectiveness of agricultural conservation practices such as the soil and water assessment tool swat water quality analysis simulation program wasp and storm water management model swmm babbar sebens et al 2015 daniel et al 2011 sinshaw et al 2019 these models simulate a multitude of processes such as runoff infiltration channel and reservoir routing sedimentation and nutrient dynamics due to their complexity watershed models can be computationally expensive for example it can take several minutes to hours to simulate hydrology and nutrient loads with models such as swat and modflow arnold et al 2012 peterson et al 2016 zhang et al 2009 this computational load becomes cumbersome when the model must be run thousands of times such as when searching for optimal management decisions the need for reduced computational burden multiplies further when the watershed model is just one among multiple integrated models within the optimization framework in an integrated modeling framework individual process based models also typically cannot communicate directly given their independent development surrogate models which capture statistical relationships between inputs and outputs can ease the burdens of computation and integration though at some cost of model fidelity razavi et al 2012 various methods such as artificial neural networks ann support vector machines svm and response matrices rm have been used to surrogate process based models cai et al 2015 housh et al 2014 li et al 2021b zhang et al 2009 each with its own limitations for example most applications of ann and svm for surrogating watershed models have only considered a small number of inputs and outputs zhang et al 2009 used ann and svm to surrogate a swat model for parameterization mapping combinations of 16 parameters to model effectiveness at predicting total basin runoff cai et al 2015 used svm to surrogate a swat model to optimize decision making under climate uncertainty mapping four management decisions to four measures of basin scale drought impact training an svm or ann to surrogate spatially distributed decision making e g 50 conservation practices per subwatershed 40 subwatersheds 2000 decision variables and spatially distributed and temporally refined outputs e g 120 months 40 subwatersheds 4800 monthly outputs would require an exorbitant number of simulations the traditional rm method is a suitable surrogate method for generating a large set of spatially distributed and dynamic hydrological responses under spatially distributed agricultural management decisions rm methods approximate hydrological responses as a linear function of a potentially large set of distributed management decisions and have been used to surrogate hydrological models such as groundwater models gorelick 1983 maddock 1972 yoon et al 2021 integrated surface water and groundwater model seo et al 2018 and watershed models gorelick et al 2019 housh et al 2014 2015 shafiee jood et al 2018 although those studies do not explicitly name their approaches as response matrix however their approaches hold the core idea of response matrix assuming that hydrological responses are approximated by linear functions of management decisions the linear nature of the rm method enables the use of linear programming for optimization applications saving considerable computational resources for example housh et al 2014 applied an rm approximation within a mixed integer linear programming model for biofuel development considering more than 10 000 decision variables even in optimization applications that do not use linear programming rm approximations have been adopted for large spatial problems because assumed independence between spatially distributed decisions a condition of linearity provides a path to estimate overall outcomes with substantially fewer simulations for example gorelick et al 2019 and gramig et al 2013 coupled a genetic algorithm with rm approximations that map management decisions to watershed sediment or nutrient loss however watershed modeling applications of the traditional rm method have not explored potentially co located conservation practices and have sparingly addressed the impacts of in stream source model processes in a rare example of treating in stream processes within an rm framework femeena et al 2018 loosely coupled swat landscape outputs with an exponential decay model for in stream nutrient processes the authors suggest that more efforts should be invested when loosely coupling swat results to better consider in stream processes co located practices and in stream processes introduce nonlinearities and interactions between decisions and thus they may reduce the effectiveness of rm based approaches for studies concerning diverse conservation practices or watershed export responses the traditional rm method based on linear approximation must be validated or modified femeena et al 2018 the overall goal of this study is to provide insights from our experience that converts a distributed hydrological model swat to a rm based surrogate model and demonstrate how to surrogate swat in a reasonable way that addresses the interaction effects between co located conservation practices and in stream and reservoir processes affecting nutrient and sediment loads the specific study objectives are to provide a revised rm method compare it with traditional rm and discuss the application and limitations of the revised rm for the present context the key assumption of the rm surrogate model is that given a known weather scenario watershed hydrological responses may be reasonably represented by linear functions of agricultural management decisions the validity of and modifications required for satisfying this assumption are the cornerstones of the discussion in this paper in the sections to follow we first provide relevant background regarding swat and the traditional response matrix method section 2 in section 3 we explore the consequences of co located conservation practices section 3 1 and in stream and reservoir processing sections 3 2 and 3 3 for the validity of the traditional rm method and accordingly illustrate modified rm approaches we also discuss how these findings and adjustments are shaped by our specific research questions and hydrological model i e swat so as to increase the transferability of the process and findings 2 background 2 1 modeling a testbed watershed in swat swat is a semi distributed hydrologic model designed to evaluate and predict the impacts of agricultural management practices on water sediments and pollutants arnold et al 2012 swat discretizes the watershed into user defined subwatersheds and within each subwatershed into hydrologic response units hrus which share a common land use soil type and slope for each hru the model independently 1 implements management practices 2 applies external weather forcing uniform within subwatersheds and 3 simulates surface and sub surface hydrology plant growth nutrient transformations and nutrient and sediment transport in swat2012 hru water nutrient and sediment yields are aggregated at the subwatershed level and routed directly to the subwatershed level stream reach within each subwatershed the stream network is consolidated to a single reach note that for swat the latest version of swat model currently available hru yields are aggregated at the landscape unit lsu level and routed through floodplains into channels before entering the subwatershed level stream reach finally swat simulates flow nutrient and sediment routing as well as simplified water quality transformations for every stream reach here we use a swat2012 model set for the upper sangamon river watershed usrw located in central illinois usa the usrw is predominantly operated for corn and soybean rotation 80 of the total 3680 km2 watershed area and is extensively tile drained usda nass 2019 the lake decatur dam in the downstream half of the watershed see fig 1 supplies municipal and industrial water to the city of decatur and nearby bioethanol producers fitzpatrick et al 1987 sediment deposition in lake decatur reduces active storage impairs water quality and requires costly dredging to remediate residential and bioethanol facility wastewaters are treated by the sanitary district of decatur sdd and discharged downstream of lake decatur notably phosphorus p discharge from sdd constitutes over 70 percent of p loads at the watershed outlet 680 mg p year sdd discharge versus 940 mg p year total watershed export in part due to significant trapping of upstream p loads in the lake decatur reservoir the swat model used in this study is calibrated for flow crop yield sediments nitrate and total phosphorus tp at multiple sites during the period from 2003 to 2012 the daily flow is calibrated at four sites fisher monticello decatur and the watershed outlet flow calibration shows satisfactory model performance moriasi et al 2007 at all sites with nash sutcliffe efficiencies nse 0 65 and percent bias within 10 the model then follows the calibration of annual crop yield for both corn and soy in macon county with a reasonable model performance furthermore the model shows satisfactory performance after the calibration of monthly sediments nitrate and tp at monticello decatur and wyckles bridge downstream of sdd with nse ranging from 0 63 to 0 83 and bias mostly within 10 lastly the model parameters are readjusted with a global run for flow crop yield sediments nitrate and tp at all the calibration sites the model performs reasonably well at these sites in the validation period 2013 2018 2 2 application of the traditional response matrix method due to the nature in which swat discretizes and simulates watershed processes described above there is no interaction between hrus until runoff mixes within the stream reach thus the pre stream portion of swat i e the land surface model processes which proceed routing into and through the stream network is inherently compatible with the independence criterion of a linear model e g a response matrix if the decision variables are modeled at the hru level to reduce computational load in optimization applications a user might desire to aggregate decisions to the subwatershed level that is to decide the percent of each subwatershed on which each conservation practice is applied rather than which practices are applied on each hru housh et al 2014 our study follows housh et al 2014 in this regard the traditional response matrix approach formally presented below following housh et al 2014 takes advantage of hru independence in swat to approximate land surface yields as the product of agricultural practice land area fractions and their siloed impacts that is their impacts when adopted in isolation without other conservation practices housh et al 2014 applied linear production functions to determine the total nitrate and runoff contribution from subwatersheds which follows the same method of the response matrix that is approximating hydrological responses using linear functions of management decisions the primary shortcomings of this traditional approach expanded upon in section 3 are that it 1 does not clearly describe how to account for interaction effects between co located conservation practices and 2 does not account for interaction effects and other sources of nonlinearity originating within the stream network the steps for implementing the traditional rm method are as follows step 1 for each conservation practice under consideration run a swat model for the watershed where the practice is adopted on all agricultural hrus step 2 construct response matrices for each conservation practice hydrologic variable e g nutrients streamflow and month construct a single response matrix where elements along the diagonal are the swat landscape yields for each subwatershed y m t step 3 construct decision vectors for each conservation practice f m where the elements are the area fractions that are allocated to that particular practice at a subwatershed level step 4 calculate the agricultural land area a for each subwatershed or depending on the modeled practice perhaps the total or urban area where appropriate step 5 construct a connectivity matrix w describing the upstream downstream relationships of all subwatersheds with off diagonal elements w i j i j equal to one if subwatershed j is upstream of subwatershed i and zero otherwise w i i 1 i step 6 apply equation 1 to estimate landscape yield during month t across all subwatersheds step 7 apply equation 2 to estimate in stream loads at the outlet of each subwatershed by summing its own yield and all upstream yields because swat models are typically calibrated and analyzed at the monthly scale we present the rm method as a surrogate for monthly swat outputs following housh et al 2014 for a model considering m possible conservation practices in n subwatersheds the subwatershed landscape yield during month t q p ni s r n is calculated as the sum of products of each practice s area allocations and response matrices 1 q t m m d i a g y m t q d i a g a f m t t where the function d i a g converts the vector argument with elements y i o r a i into a diagonal matrix with elements x ii y i o r a i y m t r n are response vectors of swat subwatershed yield outputs for conservation practice m during month t a r n is a vector containing the total agricultural area of each subwatershed and f m r n are decision vectors indicating the fraction of subwatershed areas allocated to conservation practice m under the traditional formulation the in stream loads at the outlet of each subwatershed q p ni s r n are the sum of all upstream landscape yields 2 q t w q t t t where w r n n is a connectivity matrix with off diagonal elements w i j i j equal to one if subwatershed j is upstream of subwatershed i and zero otherwise w i i 1 i equations 1 and 2 are presented for the case of estimating flow q but are applied likewise for any modeled output variable e g phosphorus p nitrogen ni or sediment s this application of the rm method estimates the impact of partial practice adoption in a subwatershed according to the aggregate impact of complete adoption across the whole subwatershed in swat the impacts might be distinct to slopes and soil types i e hrus within the subwatershed therefore the subwatershed level rm estimates most accurately represent a hypothetical swat scenario where equal parts of each hru within a subwatershed adopt a practice for example the rm model decision to apply cover crops on 20 of agricultural land in a subwatershed corresponds to the swat implementation of cover crops on 20 of the land in each agricultural hru in the subwatershed in actual swat use practices are assigned at the hru level and for entire hrus this dissonance cannot be resolved for rm models framed at the subwatershed level since the framework does not contain requisite hru information the trade off for this dissonance at the subwatershed level is the aforementioned benefit of reduced decision variables when optimizing for applications where soil and slope specific decisions are more desirable than reduced decision variables all methods discussed herein may be applied by allowing n to be the number of hrus and using hru level swat outputs moreover all results and discussion of modifications to the rm method presented below remain applicable none are specific to the subwatershed level application fig 2 demonstrates the output specific effectiveness and limitations of the traditional rm method for approximating streamflow nitrate total phosphorus and sediment at the monthly scale at the outlet of the usrw as shown in fig 2a and b the traditional rm method effectively reproduces swat streamflow and nitrate outputs however fig 2c and d illustrate that the traditional rm method does not effectively reproduce swat phosphorus or sediment outputs the sediment nash sutcliffe efficiency nse indicates that the traditional rm surrogates swat far worse than simply the long term mean nse 0 and the percent bias p bias indicates that sediment is vastly overestimated while the phosphorus nse and p bias indicate the traditional rm may be an effective surrogate in general the rm estimates systematically overestimate p load during periods of low flow which in some settings are the most critical periods for nutrient management in section 3 we demonstrate how interactions between co located conservation practices and nonlinearities within the stream network contribute to traditional rm shortcomings and how we modify the rm approach to better approximate swat phosphorus and sediment outputs 3 modifying the response matrix approach in this section we illustrate the primary shortcomings of the traditional rm approach and propose three modifications to the rm approach to better approximate swat simulated phosphorus and sediment export while we provide brief comment on approximating flow and nitrogen we focus the discussion on phosphorus and sediment because these variables are not adequately approximated by the traditional rm approach 3 1 modification 1 dealing with interaction effects between co located conservation practices when multiple conservation measures are applied at the same location the total impact is not likely to equal the sum of its parts though the impacts are typically complementary boreux et al 2013 chaubey et al 2010 illinois environmental protection agency et al 2015 for instance implementing conservation tillage may affect the nutrient reductions achieved from reducing fertilizer applications jarvie et al 2017 to the extent that these interaction effects are captured in swat the traditional rm approach where impacts of siloed practices are simply added would not accurately capture the total impact of co located conservation practices therefore we test a modified the rm approach where each unique combination of conservation practices is simulated to generate unique response matrices for each combination rather than just simulating and generating response matrices for the siloed individual practices likewise the land allocation decision vector of the rm model is reformulated to contain fractions allocated to each possible conservation practice combination under this modification according to the syntax of equation 1 m now represents the set of all possible combinations of conservation practices here we illustrate the interaction effects between conservation practices by comparing traditional response matrix model estimates and swat simulated landscape yield for a case of co located practices the baseline management practices for the usrw are two year corn soybean rotations conventional tillage 207 kg ha diammonium phosphate application preceding corn years no cover crops and no vegetative filter strips we compare estimates for the case of co located filter strips cover crops and fertilizer reduction throughout the entire watershed for this case the traditional rms are derived from swat simulated impacts of siloed filter strips siloed cover crops and siloed fertilizer reduction see scenarios 1 3 in table 1 on the other hand the modified rms would be derived directly from the swat simulated impact of all three practices implemented together scenario 4 in table 1 that is for the reasons discussed in section 2 2 the modified rm estimate exactly matches the landscape yield i e pre stream simulated in swat fig 3 shows the estimated annual sediment and phosphorus reductions at the usrw outlet if the described co located conservation practices had been applied during the years 2003 2018 if there were no interaction effects among conservation practices the traditional rm approach would provide the same estimate as the swat simulation instead fig 3a and c indicate that on average the siloed traditional rm approach overestimates the swat simulated sediment and phosphorus yield reductions by 19 75 percent and 16 6 percent respectively fig 3b and d further reveal that the overestimation is greatest in years with high water yield and thus also high sediment and phosphorus yield the combined impact of filter strips cover crops and fertilizer reductions is thus demonstrably less than the sum of their siloed impacts according to swat simulations and this emergent swat outcome is captured by the modified rm approach but not the traditional approach per fig 2a interaction effects do not seem meaningful for nitrogen perhaps implying that interaction effects in swat are primarily mediated by sediment loss to which phosphorus is more tightly coupled than nitrogen 3 2 modification 2 dealing with impacts of in stream and reservoir processes for flow and nutrient if flow and nutrient processes in each swat stream reach or water body e g reservoir behave as a linear system then the traditional response matrix method can be modified by incorporating linear in stream and water body processing effects within the channel network connectivity matrix see section 2 2 the degree to which a linear approximation holds for these processes dictates the potential effectiveness of any rm based method 3 2 1 swat in stream and reservoir processes for flow and nutrients 3 2 1 1 streamflow for streamflow swat s in stream and water body e g reservoir simulation routines include routing seepage and evaporation swat simulates seepage and evaporation losses as the product of a seepage or evaporation rate coefficient k q mm hr the exposed area i e wetted stream area or surface area a m2 and the residence time in the reach or water body δ t hr that is they share the general form 3 δ q k q a δ t where q is the volume of water in the reach since both residence time and wetted perimeter surface area are nonlinear functions of inflow themselves seepage and evaporation are each the product of rate constants and potentially nonlinear functions of flow 4 δ q k q a δ t k q f 1 q f 2 q the effectiveness of a modified rm method for flow thus depends upon the accuracy of a linear approximation such as 5 q i k q i f 1 i q f 2 q k ˆ q q where k ˆ q i is the aggregate fraction of flow lost due to seepage i 1 and evaporation i 2 at typical conditions for the scale of the present application δq is a vector of water lost m3 from each reach applied for each month of analysis in addition an rm based approach is not able to account for changes in stream or reservoir storage between time steps while assuming zero storage change is likely reasonable for annual scale estimates the assumption is plausibly problematic for monthly scale estimates for the usrw though accurate estimation by the traditional rm approach see fig 2 implies that monthly storage change is negligible and that k ˆ q is a zero vector i e seepage and evaporation are also negligible 3 2 1 2 phosphorus on the other hand while somewhat similarly formulated in swat phosphorus is not adequately estimated by the traditional rm approach for stream reaches swat simulates phosphorus settling mineralization exchange with algae and release from benthos these processes are formulated either as a 1 first order reaction i e δ p k p p where p is the mass of phosphorus in the reach and k p is the rate coefficient or 2 zeroth order reaction with dependence on flow depth or algae concentration i e δ p k p f d e p t h o r δ p k p f a l g a e similar to flow though the amount of phosphorus transformed or exchanged during transport also depends on the residence time within the reach therefore these processes take the generalized forms 6a δ p k p p δ t p 6b o r δ p k p q f 3 d e p t h a l g a e δ t where k p p hr 1 and k p q units vary are the rates of phosphorus lost within the stream for a given process note that in swat phosphorus and sediment transport are completely de coupled once they have reached the stream network while in reality and in swat sediment lost from the landscape exercises greatly influences phosphorus lost from the landscape sediment deposition and erosion have no effect on phosphorus transport in swat though the two are indeed coupled in reality since we aim to surrogate swat and only indirectly to estimate real world loads we do not include sediment as a predictor for in stream phosphorus processing therefore we test a linear regression with one phosphorus dependent term and one flow dependent term we expect that 1 the phosphorus dependent term should increase in magnitude as incoming phosphorus load increases due to the first order reaction basis and 2 the flow dependent term should increase in magnitude as incoming flow decreases due to the inverse relationship between flow and residence time therefore a linear approximation around which to build the modified rm formulation might be 7a δ p k ˆ p p p k ˆ p 1 q q i n v 7b q i n v w q i n v 7c q inv t m m d i a g y m t 1 q d i a g a f m t t where k ˆ p p is the phosphorus yield dependent fraction of phosphorus lost due to settling at typical conditions k ˆ p 1 q mg p l is the aggregate water yield dependent i e flow dependent fraction of phosphorus lost due to algal uptake j 1 algal decomposition j 2 and benthic uptake j 3 at typical conditions q i n v r n are vectors analogous to cumulative residence time at each subwatershed outlet during month t q inv r n are vectors analogous to residence time in each subwatershed in month t and y m t 1 q r n are response vectors for the inverse of water yield in each subwatershed for conservation practice m during month t while this formulation for q inv does not precisely represent the mean residence time i e q inv does not equal q 1 it preserves a linear relationship between the decision variables f m and the estimated output p for water bodies swat2012 simulates settling only with no nutrient transformations as with in stream settling settling in water bodies is formulated as a first order reaction with respect to the body s nutrient concentration and depends on the residence time in the reservoir distinctly the settling rate in water bodies scales linearly with the water body area and the residence time can exceed the model time step therefore settling depends geometrically upon residence time 8 δ p 1 1 k p r δ t δ t a p where k p r 1 hr m2 is the reservoir trapping rate per unit area δ t hr is the model time step therefore with regard to reservoir trapping the effectiveness of a modified rm method depends upon the accuracy of the linear approximation 9 δ p k ˆ p r p where k p r is the fraction of phosphorus trapped by the reservoir at typical conditions notably because the residence time in water bodies is much greater in magnitude and variance than in streams the simulated reservoir trapping may also be much larger in magnitude and span therefore it is not immediately clear whether this approximation at typical conditions will hold well equations 6 10 are generalized forms provided and discussed as the background and justifications for the modifications we make in the following section in section 3 2 2 we evaluate the validity of the phosphorus in stream approximation equation 7 and phosphorus reservoir trapping approximation formulation provided in section 3 2 1 in concert for watersheds where nitrogen is not adequately estimated by the traditional rm approach the same formulation and validation process may be applied as that for phosphorus 3 2 2 incorporation of effects of point sources reservoirs and in stream processes in phosphorus estimation here we present phosphorus response matrix modifications that adjust for 1 point source discharges 2 reservoir trapping and 3 in stream processes the modifications are embedded within the framework of equations 1 and 2 by adding additional sources contributing to subwatershed yield p scaling the phosphorus yield response vectors y m t p adding dependence on the inverse water yield response vectors y m t 1 q and selectively scaling elements of the connectivity matrix w fig 4 illustrates a simple conceptual watershed with the features considered by the modified response matrix approach for phosphorus greater attention is given to reservoir trapping and in stream processes since adding point source impacts is straightforward point source loads are simply added to the subwatershed yield equation 1 where the point sources are located 10 p t m m d i a g y m t q d i a g a f m p t ps p t nps p t ps t t where p t nps r n is the total non point source yield from each subwatershed in month t and p t ps r n is the total point source yield from each subwatershed in month t the inclusion of point sources is trivial methodologically to the point that we include it in fig 2 when comparing traditional rm to swat simulations we evaluate the linear approximation for reservoir trapping equation 9 in the usrw baseline scenario by conducting a linear regression for phosphorus effluent according to phosphorus influent including an intercept term to allow for some minimum trapped load the regression performs very well r squared 0 95 see fig 5 11 p t r e s o u t 1 k ˆ p r 1 p t r e s i n k ˆ p r 2 t t where p t r e s i n and p t r e s o u t are the simulated phosphorus loads into and out of the reservoir at time t and k ˆ p r 1 and k ˆ p r 2 are linear regression coefficients the accuracy of the linear regression indicates that a linear filter compatible with the response matrix approach may reasonably approximate the trapping effect because the trapping efficiency i e k ˆ p r 1 is time invariant it may be incorporated directly within the also time invariant stream connectivity matrix for all phosphorus stream connectivity matrix elements w i j p such that i d and j u where d is the set of stream reaches downstream from the reservoir and u is the set of reaches upstream of the reservoir we set w i j equal to one minus the trapping efficiency rather than 1 as had been before 12 w i j p 1 k ˆ p r 1 i f i d a n d j u 1 o t h e r w i s e i f j i s u p s t r e a m o f i 0 o t h e r w i s e meanwhile the minimum trapped load i e k ˆ p r 1 is subtracted directly from the yield of the downstream subwatershed as if a point sink 13 p t p t nps p t ps p t res t t where the minimum trapped load p t res r n is equal to k ˆ p r 2 for the subwatershed immediately downstream of the reservoir and zero otherwise we evaluate the linear approximation for in stream processing equation 7 in the usrw baseline scenario by conducting linear regressions at each subwatershed outlet for phosphorus effluent according to upstream phosphorus yields and inverse flow yields 14 p t i n k ˆ p p w p t i n k ˆ p 1 q w q i n v t t t where k ˆ p p r n x n and k ˆ p 1 q r n x n are diagonal matrices whose elements are the regression coefficients estimated using swat simulation data i e p yield and streamflow and indicating the fraction of phosphorus lost in a stream and i n r n x n is an identity matrix recall that w is the connectivity matrix accounting for upstream downstream relationships the right hand side of equation 14 is then the p export at each reach broken into a term dependent on upstream landscape p loading and a term dependent on upstream cumulative residence time larger values of k ˆ p p and k ˆ p 1 q indicate that the stream acts as more of a phosphorus sink as with the traditional rm method we apply equations 10 14 at the monthly scale we find again that the regression performs very well r squared 0 95 see fig 6 suggesting that equations 10 and 12 14 constitute an effective modified response matrix approach for phosphorus below we compare swat simulation results for monthly in stream phosphorus loads at the usrw outlet with estimates from both the modified rm formulation and a traditional point source estimate we choose to show the traditional point source estimate rather than the traditional rm estimate because 1 the point source addition method is trivial and 2 the point source load in the usrw is so large that its omission obscures the value of the other modifications first for swat simulations we randomly select one of four conservation practice combinations from table 1 filter strips only cover crops only fertilizer reduction only or all practices together for every agricultural hru in the watershed then for the rm estimates we apply the resultant subwatershed land allocation fractions as described in section 2 2 the rm estimates assume that this fraction of land allocated to a conservation practice is evenly distributed among all hrus within the subwatershed some discrepancy between the rm estimates and swat simulations may be attributed to this allocation distinction and differences in how conservation practices impact yields in different hrus i e on different soils and slopes we measure the surrogate accuracy of the rm approaches by nash sutcliffe efficiency nse and percent bias p bias between the respective rm estimated time series and the simulated time series moriasi et al 2007 we find that the modified rm formulation provides more accurate and less biased estimates nse 0 98 and p bias 0 9 than the traditional point source formulation nse 0 92 and p bias 12 8 fig 7 furthermore the modified rm formulation provides drastically more representative estimates during periods of low flow the difference between the modified rm and traditional point source estimates is most pronounced during the 2011 2012 drought from july 2011 to september 2012 swat simulates 541 950 kg total phosphorus export from the watershed the modified rm estimate is 561 562 kg export 3 6 over estimate while the traditional point source estimate is 772 064 kg export 42 5 over estimate this focused improvement from the modified rm approach during low flows is to be expected the effects of in stream processes and reservoir trapping are greatest when residence time is largest moreover this focused improvement is relevant and important since streams and water bodies receiving significant point source discharge can be most vulnerable to harmful algal blooms during low flow periods harrison et al 2019 jarvie et al 2006 the cumulative impact of improved prediction at low flows is a relatively unbiased estimate for total export over the 16 years rather than the 12 8 over prediction by the traditional point source method we repeat the above evaluation process 100 times for different realizations of randomized conservation practice allocations using modified rm method and compare the results with the same randomized allocations in swat to evaluate the robustness of rm performance under randomized allocations these realizations provide a more comprehensive picture of the modified rm performance and illustrate the impacts of the conservation practice allocation methods discussed above fig 8 presents the mean and range for the performance metrics nse and p bias across the 100 realizations and every subwatershed outlet overall the modified rm method has satisfactory performance nses 0 96 to nearly 1 p bias 15 18 across all subwatersheds except for subwatershed 8 for approximating swat in stream phosphorus loads notably the subwatersheds which perform worst are headwater subwatersheds and consist of relatively few hrus for example subwatershed 8 has only three hrus the lesser performance in these subwatersheds is likely due to the divergence between the rm assumption and swat applied method for allocating conservation practice combinations when there are many upstream hrus the upstream land allocated to each conservation practice combination will in the aggregate consist of similar soils and slope classes despite different precise allocations when there are few upstream hrus the rm estimate may not capture swat sensitivity to which soil type or slope class i e which hru a practice is implemented on therefore when applying the modified or traditional rm method at the subwatershed level it is important to acknowledge that the method allows for targeting specific subwatersheds with conservation practices but not targeting specific hrus within a subwatershed to evaluate the benefits of computational time saved by modified rm approach we recorded the computation time of simulations between original swat and our modified rm approach with the same conservation practice allocations specifically the computation for 100 randomized simulations in swat takes 2 h 20 min while the modified rm method takes about 4 2 min with a single processor in an intel core i7 8700k 64 bit and 32 gb memory windows pc thus the computational time can be reduced by about 3500 with the modified rm approach 3 3 modification 3 dealing with impacts of in stream and reservoir processes for sediment 3 3 1 swat in stream and reservoir processes for sediment sediment transport in swat depends upon flow conditions and sediment supply here sediment supply refers to sediment which has been lost from the landscape during the current model time step as well as all sediment that was previously deposited and still remains within the stream network note also sediment entrainment but not deposition is not controlled by flow conditions and sediment supply simultaneously but rather one or the other is limiting for a given reach at a specific time according to the following logic 1 sediment entrainment may only occur if the stream has sufficient transport capacity as determined by flow conditions for increased suspended sediment concentration 2 any previously deposited sediments which have remained within the reach are entrained first 3 if all previously deposited sediments are exhausted the sediment transport capacity may be met by channel bed and bank erosion but only if the streamflow generates sufficient shear stress upon the streambed bank these interactions between flow and sediment supply and between the event scale and long term accumulation present an obstacle for ascertaining a simple approximation for in stream sediment processes for zones of a stream network where transport capacity consistently exceeds sediment supply we will refer to these as supply constrained reaches would be controlled by a combination of landscape sediment loss and streambed shear stress a function of flow for zones of a stream network where sediment supply consistently exceeds transport capacity we will refer to these as flow constrained reaches would be controlled by a combination of the transport capacity a function of flow distinct from that for shear stress and the sediment deposition rate 3 3 2 use streamflow to estimate in stream sediment loads for surrogate model here we demonstrate the need to depart from an rm based approximation for sediment and offer instead a simple nonlinear approximation based on the underlying model processes in swat for flow constrained reaches within the stream network the most sensible linear approximation and rm formulation for sediment export are according to landscape water yield that is 15 s f 4 q k ˆ s q k ˆ s 1 m m d i a g y m t q d i a g a f m k ˆ s 2 t t where k ˆ s 1 mg l and k ˆ s 2 mg l are coefficients representing effects at typical conditions however the transport capacity can be a highly nonlinear function of flow therefore this approximation may be unlikely to hold for example we select the swat modeling option to use the simplified bagnold model neitsch et al 2011 for transport capacity one of four options where 16 c o n c s e d c h m x c s p v c h p k s p e x p where c o n c s e d c h m x is the maximum sediment concentration ton m3 or kg l c s p and s p e x p are parameters defined by swat modeler and v c h p k is the peak channel velocity m s during the time step a nonlinear function of inflow see eq 7 2 2 3 from swat theory documentation 2009 in the case where non linearities must be incorporated and following the bagnold equation a more representative approximation might be 17a s n f 4 q k ˆ s l i n q n k ˆ s e x p n n t t or perhaps considering the complex relationship between flow volume q here and peak flow velocity v c h p k in the bagnold equation even a polynomial approximation is suitable 17b s n f 4 q k ˆ s 1 k ˆ s 2 q n k ˆ s 3 q n 2 n n t t where k ˆ s l i n k ˆ s e x p k ˆ s 1 k ˆ s 2 and k ˆ s 3 are parameters for the sediment flow relationship at typical conditions however this nonlinear approximation is not compatible with a response matrix formulation and would possibly require the modeler to adjust their use of the surrogate for instance changing the solution method used for optimization on the other hand for supply constrained reaches the most sensible linear approximation and rm formulation likely must account for landscape sediment yield and water yield that is 18 s s f 5 q s k s q q m m d i a g y m t s d i a g a f m k ˆ s q m m d i a g y m t q d i a g a f m t t where k ˆ s q mg l is the streambed sediment contribution per unit of flow at typical conditions however the nonlinearity of streambed erosion processes may make this approximation unlikely to hold as well fig 9 a and b compares the estimates of best performing approximations with estimates of the traditional rm approach for selected illustrative subwatersheds for flow constrained subwatersheds such as shown in figs 9a and 10 b best approximations from equations 15 and 17 respectively the traditional rm approach significantly overestimates the sediment loads while the proposed modifications provide highly accurate estimates r squared 0 9 for example in subwatersheds 33 and 34 the outlet the nse for traditional rm sediment estimates are 0 08 and 1 69 respectively but improve to 0 91 and 0 97 under a flow based approximation evidently swat simulates significant sediment deposition in these zones of the stream network effectively buffering the upstream landscape sediment loss signal jerolmack and paola 2010 romans et al 2016 in some cases e g as shown in fig 9c the flow sediment relationship can also be approximately linear hence the acceptable performance of the response matrix approach presented in equation 15 however in many cases e g as shown in fig 9d the flow sediment relationship is clearly exponential or polynomial and a nonlinear approximation is required in place of the rm approach we apply the potential rm formulations presented in equations 15 and 17 a or b and 18 and compare their performance in estimating sediment load at the monthly scale at all subwatershed outlets over 100 realizations for conservation practice allocations we find that of the 45 usrw subwatersheds 20 subwatersheds are approximated well i e average nse 0 85 by either the linear equation 15 or nonlinear equations 17a and 17b approximations aligned with the bagnold equation see fig 10 for those 20 subwatersheds the linear approximation method is suggested as it may be incorporated into an rm approach 12 subwatersheds are only approximated well by the nonlinear estimates polynomial or power functions these results align with the suggestions of previous studies that due to historical management practices sediment transport in the upper mississippi river basin is generally flow constrained neal and anders 2015 trimble 1999 of the other 13 subwatersheds 6 can be approximated acceptably i e nse 0 5 but not well by one of the flow constrained approximations the remaining 7 subwatersheds cannot be approximated acceptably by any of the formulations offered here i e nse 0 5 the poor performance in sediment estimation seems to go beyond the unaccounted impacts of soil type and slope class discussed in section 3 3 2 as there is no clear trend in performance as upstream hrus increase it appears that some subwatersheds either frequently switch modes between possible dominant controls on swat sediment export not conforming to a single approximation or are generally less amenable to approximations for swat sediment simulations 4 conclusions in this work we show that 1 an accurate rm based approach to swat approximation requires that response matrices be generated for all distinct combinations of conservation practices in order to account for the interaction effects between individual practices 2 a modified rm method especially accounting for in stream and reservoir processes is required to correct estimates for phosphorus export and 3 a departure from rm based approximation is required for accurately estimating sediment instead utilizing a nonlinear flow based estimate for sediment loads we hypothesize based on primary process model understanding that the modifications presented for phosphorus could also adequately correct estimates for flow and nitrogen when necessary we also highlight that when applied for decisions at the subwatershed scale rather than the hru scale the proposed approximations perform best at outlets draining large areas the approximations detailed here provide efficient spatial and dynamic simulations on hydrological responses based on a wide range of spatial applications of agricultural conservation practices excluding sediment the approximations maintain an rm based approach facilitating advantages such as the feasibility of linear programming methods the approximations are especially well suited to integration within a system of systems modeling framework where modelers wish to consider a mixture of non point and point source models li et al 2021a for example the nutrient effluents simulated by wastewater treatment models can be directly added via the rm method to accurately simulate the in stream p load the application here is centered on surrogating the swat model but the process enlightened by the discussion provided here could plausibly be generalized to other semi distributed hydrologic models e g wasp or swmm the revised rm enables a more accurate use of a watershed hydrological model for finding optimal watershed management solutions via 1 classic optimization i e linear programming and nonlinear programming including the suggested nonlinear equation describing the instream and reservoir sediment processes 2 heuristic optimization such as genetic algorithm which uses the revised rm to replace the original simulation model data and code availability the data and codes used for constructing the modified rm are available via github https github com shaobinli modified rm declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the us national science foundation infews t1 award number 1739788 
25667,the computational burden of running a semi distributed hydrological model numerous times such as for optimization applications can be exorbitant this study provides a surrogate model to estimate streamflow nutrient and sediment export under spatially distributed management decisions specifically we surrogate the soil and water assessment tool swat using a modified response matrix rm approach a traditional rm approach applied to swat assumes hydrological responses are approximated by linear functions of management decisions and falls short in accounting for in stream and reservoir processes here we explain and illustrate three key modifications that address interaction effects between co located conservation practices and in stream and reservoir processes affecting nutrient and sediment loads the modified rm approach provides excellent estimation nash sutcliffe efficiency nse 0 95 for streamflow and nutrient export throughout the stream network and provides very good estimation nse 0 85 for sediment export at most though not all points in the stream network keywords surrogate modeling nutrient transport watershed modeling soil and water assessment tool swat management decisions 1 introduction many process based hydrological watershed modeling tools have been developed to evaluate the effectiveness of agricultural conservation practices such as the soil and water assessment tool swat water quality analysis simulation program wasp and storm water management model swmm babbar sebens et al 2015 daniel et al 2011 sinshaw et al 2019 these models simulate a multitude of processes such as runoff infiltration channel and reservoir routing sedimentation and nutrient dynamics due to their complexity watershed models can be computationally expensive for example it can take several minutes to hours to simulate hydrology and nutrient loads with models such as swat and modflow arnold et al 2012 peterson et al 2016 zhang et al 2009 this computational load becomes cumbersome when the model must be run thousands of times such as when searching for optimal management decisions the need for reduced computational burden multiplies further when the watershed model is just one among multiple integrated models within the optimization framework in an integrated modeling framework individual process based models also typically cannot communicate directly given their independent development surrogate models which capture statistical relationships between inputs and outputs can ease the burdens of computation and integration though at some cost of model fidelity razavi et al 2012 various methods such as artificial neural networks ann support vector machines svm and response matrices rm have been used to surrogate process based models cai et al 2015 housh et al 2014 li et al 2021b zhang et al 2009 each with its own limitations for example most applications of ann and svm for surrogating watershed models have only considered a small number of inputs and outputs zhang et al 2009 used ann and svm to surrogate a swat model for parameterization mapping combinations of 16 parameters to model effectiveness at predicting total basin runoff cai et al 2015 used svm to surrogate a swat model to optimize decision making under climate uncertainty mapping four management decisions to four measures of basin scale drought impact training an svm or ann to surrogate spatially distributed decision making e g 50 conservation practices per subwatershed 40 subwatersheds 2000 decision variables and spatially distributed and temporally refined outputs e g 120 months 40 subwatersheds 4800 monthly outputs would require an exorbitant number of simulations the traditional rm method is a suitable surrogate method for generating a large set of spatially distributed and dynamic hydrological responses under spatially distributed agricultural management decisions rm methods approximate hydrological responses as a linear function of a potentially large set of distributed management decisions and have been used to surrogate hydrological models such as groundwater models gorelick 1983 maddock 1972 yoon et al 2021 integrated surface water and groundwater model seo et al 2018 and watershed models gorelick et al 2019 housh et al 2014 2015 shafiee jood et al 2018 although those studies do not explicitly name their approaches as response matrix however their approaches hold the core idea of response matrix assuming that hydrological responses are approximated by linear functions of management decisions the linear nature of the rm method enables the use of linear programming for optimization applications saving considerable computational resources for example housh et al 2014 applied an rm approximation within a mixed integer linear programming model for biofuel development considering more than 10 000 decision variables even in optimization applications that do not use linear programming rm approximations have been adopted for large spatial problems because assumed independence between spatially distributed decisions a condition of linearity provides a path to estimate overall outcomes with substantially fewer simulations for example gorelick et al 2019 and gramig et al 2013 coupled a genetic algorithm with rm approximations that map management decisions to watershed sediment or nutrient loss however watershed modeling applications of the traditional rm method have not explored potentially co located conservation practices and have sparingly addressed the impacts of in stream source model processes in a rare example of treating in stream processes within an rm framework femeena et al 2018 loosely coupled swat landscape outputs with an exponential decay model for in stream nutrient processes the authors suggest that more efforts should be invested when loosely coupling swat results to better consider in stream processes co located practices and in stream processes introduce nonlinearities and interactions between decisions and thus they may reduce the effectiveness of rm based approaches for studies concerning diverse conservation practices or watershed export responses the traditional rm method based on linear approximation must be validated or modified femeena et al 2018 the overall goal of this study is to provide insights from our experience that converts a distributed hydrological model swat to a rm based surrogate model and demonstrate how to surrogate swat in a reasonable way that addresses the interaction effects between co located conservation practices and in stream and reservoir processes affecting nutrient and sediment loads the specific study objectives are to provide a revised rm method compare it with traditional rm and discuss the application and limitations of the revised rm for the present context the key assumption of the rm surrogate model is that given a known weather scenario watershed hydrological responses may be reasonably represented by linear functions of agricultural management decisions the validity of and modifications required for satisfying this assumption are the cornerstones of the discussion in this paper in the sections to follow we first provide relevant background regarding swat and the traditional response matrix method section 2 in section 3 we explore the consequences of co located conservation practices section 3 1 and in stream and reservoir processing sections 3 2 and 3 3 for the validity of the traditional rm method and accordingly illustrate modified rm approaches we also discuss how these findings and adjustments are shaped by our specific research questions and hydrological model i e swat so as to increase the transferability of the process and findings 2 background 2 1 modeling a testbed watershed in swat swat is a semi distributed hydrologic model designed to evaluate and predict the impacts of agricultural management practices on water sediments and pollutants arnold et al 2012 swat discretizes the watershed into user defined subwatersheds and within each subwatershed into hydrologic response units hrus which share a common land use soil type and slope for each hru the model independently 1 implements management practices 2 applies external weather forcing uniform within subwatersheds and 3 simulates surface and sub surface hydrology plant growth nutrient transformations and nutrient and sediment transport in swat2012 hru water nutrient and sediment yields are aggregated at the subwatershed level and routed directly to the subwatershed level stream reach within each subwatershed the stream network is consolidated to a single reach note that for swat the latest version of swat model currently available hru yields are aggregated at the landscape unit lsu level and routed through floodplains into channels before entering the subwatershed level stream reach finally swat simulates flow nutrient and sediment routing as well as simplified water quality transformations for every stream reach here we use a swat2012 model set for the upper sangamon river watershed usrw located in central illinois usa the usrw is predominantly operated for corn and soybean rotation 80 of the total 3680 km2 watershed area and is extensively tile drained usda nass 2019 the lake decatur dam in the downstream half of the watershed see fig 1 supplies municipal and industrial water to the city of decatur and nearby bioethanol producers fitzpatrick et al 1987 sediment deposition in lake decatur reduces active storage impairs water quality and requires costly dredging to remediate residential and bioethanol facility wastewaters are treated by the sanitary district of decatur sdd and discharged downstream of lake decatur notably phosphorus p discharge from sdd constitutes over 70 percent of p loads at the watershed outlet 680 mg p year sdd discharge versus 940 mg p year total watershed export in part due to significant trapping of upstream p loads in the lake decatur reservoir the swat model used in this study is calibrated for flow crop yield sediments nitrate and total phosphorus tp at multiple sites during the period from 2003 to 2012 the daily flow is calibrated at four sites fisher monticello decatur and the watershed outlet flow calibration shows satisfactory model performance moriasi et al 2007 at all sites with nash sutcliffe efficiencies nse 0 65 and percent bias within 10 the model then follows the calibration of annual crop yield for both corn and soy in macon county with a reasonable model performance furthermore the model shows satisfactory performance after the calibration of monthly sediments nitrate and tp at monticello decatur and wyckles bridge downstream of sdd with nse ranging from 0 63 to 0 83 and bias mostly within 10 lastly the model parameters are readjusted with a global run for flow crop yield sediments nitrate and tp at all the calibration sites the model performs reasonably well at these sites in the validation period 2013 2018 2 2 application of the traditional response matrix method due to the nature in which swat discretizes and simulates watershed processes described above there is no interaction between hrus until runoff mixes within the stream reach thus the pre stream portion of swat i e the land surface model processes which proceed routing into and through the stream network is inherently compatible with the independence criterion of a linear model e g a response matrix if the decision variables are modeled at the hru level to reduce computational load in optimization applications a user might desire to aggregate decisions to the subwatershed level that is to decide the percent of each subwatershed on which each conservation practice is applied rather than which practices are applied on each hru housh et al 2014 our study follows housh et al 2014 in this regard the traditional response matrix approach formally presented below following housh et al 2014 takes advantage of hru independence in swat to approximate land surface yields as the product of agricultural practice land area fractions and their siloed impacts that is their impacts when adopted in isolation without other conservation practices housh et al 2014 applied linear production functions to determine the total nitrate and runoff contribution from subwatersheds which follows the same method of the response matrix that is approximating hydrological responses using linear functions of management decisions the primary shortcomings of this traditional approach expanded upon in section 3 are that it 1 does not clearly describe how to account for interaction effects between co located conservation practices and 2 does not account for interaction effects and other sources of nonlinearity originating within the stream network the steps for implementing the traditional rm method are as follows step 1 for each conservation practice under consideration run a swat model for the watershed where the practice is adopted on all agricultural hrus step 2 construct response matrices for each conservation practice hydrologic variable e g nutrients streamflow and month construct a single response matrix where elements along the diagonal are the swat landscape yields for each subwatershed y m t step 3 construct decision vectors for each conservation practice f m where the elements are the area fractions that are allocated to that particular practice at a subwatershed level step 4 calculate the agricultural land area a for each subwatershed or depending on the modeled practice perhaps the total or urban area where appropriate step 5 construct a connectivity matrix w describing the upstream downstream relationships of all subwatersheds with off diagonal elements w i j i j equal to one if subwatershed j is upstream of subwatershed i and zero otherwise w i i 1 i step 6 apply equation 1 to estimate landscape yield during month t across all subwatersheds step 7 apply equation 2 to estimate in stream loads at the outlet of each subwatershed by summing its own yield and all upstream yields because swat models are typically calibrated and analyzed at the monthly scale we present the rm method as a surrogate for monthly swat outputs following housh et al 2014 for a model considering m possible conservation practices in n subwatersheds the subwatershed landscape yield during month t q p ni s r n is calculated as the sum of products of each practice s area allocations and response matrices 1 q t m m d i a g y m t q d i a g a f m t t where the function d i a g converts the vector argument with elements y i o r a i into a diagonal matrix with elements x ii y i o r a i y m t r n are response vectors of swat subwatershed yield outputs for conservation practice m during month t a r n is a vector containing the total agricultural area of each subwatershed and f m r n are decision vectors indicating the fraction of subwatershed areas allocated to conservation practice m under the traditional formulation the in stream loads at the outlet of each subwatershed q p ni s r n are the sum of all upstream landscape yields 2 q t w q t t t where w r n n is a connectivity matrix with off diagonal elements w i j i j equal to one if subwatershed j is upstream of subwatershed i and zero otherwise w i i 1 i equations 1 and 2 are presented for the case of estimating flow q but are applied likewise for any modeled output variable e g phosphorus p nitrogen ni or sediment s this application of the rm method estimates the impact of partial practice adoption in a subwatershed according to the aggregate impact of complete adoption across the whole subwatershed in swat the impacts might be distinct to slopes and soil types i e hrus within the subwatershed therefore the subwatershed level rm estimates most accurately represent a hypothetical swat scenario where equal parts of each hru within a subwatershed adopt a practice for example the rm model decision to apply cover crops on 20 of agricultural land in a subwatershed corresponds to the swat implementation of cover crops on 20 of the land in each agricultural hru in the subwatershed in actual swat use practices are assigned at the hru level and for entire hrus this dissonance cannot be resolved for rm models framed at the subwatershed level since the framework does not contain requisite hru information the trade off for this dissonance at the subwatershed level is the aforementioned benefit of reduced decision variables when optimizing for applications where soil and slope specific decisions are more desirable than reduced decision variables all methods discussed herein may be applied by allowing n to be the number of hrus and using hru level swat outputs moreover all results and discussion of modifications to the rm method presented below remain applicable none are specific to the subwatershed level application fig 2 demonstrates the output specific effectiveness and limitations of the traditional rm method for approximating streamflow nitrate total phosphorus and sediment at the monthly scale at the outlet of the usrw as shown in fig 2a and b the traditional rm method effectively reproduces swat streamflow and nitrate outputs however fig 2c and d illustrate that the traditional rm method does not effectively reproduce swat phosphorus or sediment outputs the sediment nash sutcliffe efficiency nse indicates that the traditional rm surrogates swat far worse than simply the long term mean nse 0 and the percent bias p bias indicates that sediment is vastly overestimated while the phosphorus nse and p bias indicate the traditional rm may be an effective surrogate in general the rm estimates systematically overestimate p load during periods of low flow which in some settings are the most critical periods for nutrient management in section 3 we demonstrate how interactions between co located conservation practices and nonlinearities within the stream network contribute to traditional rm shortcomings and how we modify the rm approach to better approximate swat phosphorus and sediment outputs 3 modifying the response matrix approach in this section we illustrate the primary shortcomings of the traditional rm approach and propose three modifications to the rm approach to better approximate swat simulated phosphorus and sediment export while we provide brief comment on approximating flow and nitrogen we focus the discussion on phosphorus and sediment because these variables are not adequately approximated by the traditional rm approach 3 1 modification 1 dealing with interaction effects between co located conservation practices when multiple conservation measures are applied at the same location the total impact is not likely to equal the sum of its parts though the impacts are typically complementary boreux et al 2013 chaubey et al 2010 illinois environmental protection agency et al 2015 for instance implementing conservation tillage may affect the nutrient reductions achieved from reducing fertilizer applications jarvie et al 2017 to the extent that these interaction effects are captured in swat the traditional rm approach where impacts of siloed practices are simply added would not accurately capture the total impact of co located conservation practices therefore we test a modified the rm approach where each unique combination of conservation practices is simulated to generate unique response matrices for each combination rather than just simulating and generating response matrices for the siloed individual practices likewise the land allocation decision vector of the rm model is reformulated to contain fractions allocated to each possible conservation practice combination under this modification according to the syntax of equation 1 m now represents the set of all possible combinations of conservation practices here we illustrate the interaction effects between conservation practices by comparing traditional response matrix model estimates and swat simulated landscape yield for a case of co located practices the baseline management practices for the usrw are two year corn soybean rotations conventional tillage 207 kg ha diammonium phosphate application preceding corn years no cover crops and no vegetative filter strips we compare estimates for the case of co located filter strips cover crops and fertilizer reduction throughout the entire watershed for this case the traditional rms are derived from swat simulated impacts of siloed filter strips siloed cover crops and siloed fertilizer reduction see scenarios 1 3 in table 1 on the other hand the modified rms would be derived directly from the swat simulated impact of all three practices implemented together scenario 4 in table 1 that is for the reasons discussed in section 2 2 the modified rm estimate exactly matches the landscape yield i e pre stream simulated in swat fig 3 shows the estimated annual sediment and phosphorus reductions at the usrw outlet if the described co located conservation practices had been applied during the years 2003 2018 if there were no interaction effects among conservation practices the traditional rm approach would provide the same estimate as the swat simulation instead fig 3a and c indicate that on average the siloed traditional rm approach overestimates the swat simulated sediment and phosphorus yield reductions by 19 75 percent and 16 6 percent respectively fig 3b and d further reveal that the overestimation is greatest in years with high water yield and thus also high sediment and phosphorus yield the combined impact of filter strips cover crops and fertilizer reductions is thus demonstrably less than the sum of their siloed impacts according to swat simulations and this emergent swat outcome is captured by the modified rm approach but not the traditional approach per fig 2a interaction effects do not seem meaningful for nitrogen perhaps implying that interaction effects in swat are primarily mediated by sediment loss to which phosphorus is more tightly coupled than nitrogen 3 2 modification 2 dealing with impacts of in stream and reservoir processes for flow and nutrient if flow and nutrient processes in each swat stream reach or water body e g reservoir behave as a linear system then the traditional response matrix method can be modified by incorporating linear in stream and water body processing effects within the channel network connectivity matrix see section 2 2 the degree to which a linear approximation holds for these processes dictates the potential effectiveness of any rm based method 3 2 1 swat in stream and reservoir processes for flow and nutrients 3 2 1 1 streamflow for streamflow swat s in stream and water body e g reservoir simulation routines include routing seepage and evaporation swat simulates seepage and evaporation losses as the product of a seepage or evaporation rate coefficient k q mm hr the exposed area i e wetted stream area or surface area a m2 and the residence time in the reach or water body δ t hr that is they share the general form 3 δ q k q a δ t where q is the volume of water in the reach since both residence time and wetted perimeter surface area are nonlinear functions of inflow themselves seepage and evaporation are each the product of rate constants and potentially nonlinear functions of flow 4 δ q k q a δ t k q f 1 q f 2 q the effectiveness of a modified rm method for flow thus depends upon the accuracy of a linear approximation such as 5 q i k q i f 1 i q f 2 q k ˆ q q where k ˆ q i is the aggregate fraction of flow lost due to seepage i 1 and evaporation i 2 at typical conditions for the scale of the present application δq is a vector of water lost m3 from each reach applied for each month of analysis in addition an rm based approach is not able to account for changes in stream or reservoir storage between time steps while assuming zero storage change is likely reasonable for annual scale estimates the assumption is plausibly problematic for monthly scale estimates for the usrw though accurate estimation by the traditional rm approach see fig 2 implies that monthly storage change is negligible and that k ˆ q is a zero vector i e seepage and evaporation are also negligible 3 2 1 2 phosphorus on the other hand while somewhat similarly formulated in swat phosphorus is not adequately estimated by the traditional rm approach for stream reaches swat simulates phosphorus settling mineralization exchange with algae and release from benthos these processes are formulated either as a 1 first order reaction i e δ p k p p where p is the mass of phosphorus in the reach and k p is the rate coefficient or 2 zeroth order reaction with dependence on flow depth or algae concentration i e δ p k p f d e p t h o r δ p k p f a l g a e similar to flow though the amount of phosphorus transformed or exchanged during transport also depends on the residence time within the reach therefore these processes take the generalized forms 6a δ p k p p δ t p 6b o r δ p k p q f 3 d e p t h a l g a e δ t where k p p hr 1 and k p q units vary are the rates of phosphorus lost within the stream for a given process note that in swat phosphorus and sediment transport are completely de coupled once they have reached the stream network while in reality and in swat sediment lost from the landscape exercises greatly influences phosphorus lost from the landscape sediment deposition and erosion have no effect on phosphorus transport in swat though the two are indeed coupled in reality since we aim to surrogate swat and only indirectly to estimate real world loads we do not include sediment as a predictor for in stream phosphorus processing therefore we test a linear regression with one phosphorus dependent term and one flow dependent term we expect that 1 the phosphorus dependent term should increase in magnitude as incoming phosphorus load increases due to the first order reaction basis and 2 the flow dependent term should increase in magnitude as incoming flow decreases due to the inverse relationship between flow and residence time therefore a linear approximation around which to build the modified rm formulation might be 7a δ p k ˆ p p p k ˆ p 1 q q i n v 7b q i n v w q i n v 7c q inv t m m d i a g y m t 1 q d i a g a f m t t where k ˆ p p is the phosphorus yield dependent fraction of phosphorus lost due to settling at typical conditions k ˆ p 1 q mg p l is the aggregate water yield dependent i e flow dependent fraction of phosphorus lost due to algal uptake j 1 algal decomposition j 2 and benthic uptake j 3 at typical conditions q i n v r n are vectors analogous to cumulative residence time at each subwatershed outlet during month t q inv r n are vectors analogous to residence time in each subwatershed in month t and y m t 1 q r n are response vectors for the inverse of water yield in each subwatershed for conservation practice m during month t while this formulation for q inv does not precisely represent the mean residence time i e q inv does not equal q 1 it preserves a linear relationship between the decision variables f m and the estimated output p for water bodies swat2012 simulates settling only with no nutrient transformations as with in stream settling settling in water bodies is formulated as a first order reaction with respect to the body s nutrient concentration and depends on the residence time in the reservoir distinctly the settling rate in water bodies scales linearly with the water body area and the residence time can exceed the model time step therefore settling depends geometrically upon residence time 8 δ p 1 1 k p r δ t δ t a p where k p r 1 hr m2 is the reservoir trapping rate per unit area δ t hr is the model time step therefore with regard to reservoir trapping the effectiveness of a modified rm method depends upon the accuracy of the linear approximation 9 δ p k ˆ p r p where k p r is the fraction of phosphorus trapped by the reservoir at typical conditions notably because the residence time in water bodies is much greater in magnitude and variance than in streams the simulated reservoir trapping may also be much larger in magnitude and span therefore it is not immediately clear whether this approximation at typical conditions will hold well equations 6 10 are generalized forms provided and discussed as the background and justifications for the modifications we make in the following section in section 3 2 2 we evaluate the validity of the phosphorus in stream approximation equation 7 and phosphorus reservoir trapping approximation formulation provided in section 3 2 1 in concert for watersheds where nitrogen is not adequately estimated by the traditional rm approach the same formulation and validation process may be applied as that for phosphorus 3 2 2 incorporation of effects of point sources reservoirs and in stream processes in phosphorus estimation here we present phosphorus response matrix modifications that adjust for 1 point source discharges 2 reservoir trapping and 3 in stream processes the modifications are embedded within the framework of equations 1 and 2 by adding additional sources contributing to subwatershed yield p scaling the phosphorus yield response vectors y m t p adding dependence on the inverse water yield response vectors y m t 1 q and selectively scaling elements of the connectivity matrix w fig 4 illustrates a simple conceptual watershed with the features considered by the modified response matrix approach for phosphorus greater attention is given to reservoir trapping and in stream processes since adding point source impacts is straightforward point source loads are simply added to the subwatershed yield equation 1 where the point sources are located 10 p t m m d i a g y m t q d i a g a f m p t ps p t nps p t ps t t where p t nps r n is the total non point source yield from each subwatershed in month t and p t ps r n is the total point source yield from each subwatershed in month t the inclusion of point sources is trivial methodologically to the point that we include it in fig 2 when comparing traditional rm to swat simulations we evaluate the linear approximation for reservoir trapping equation 9 in the usrw baseline scenario by conducting a linear regression for phosphorus effluent according to phosphorus influent including an intercept term to allow for some minimum trapped load the regression performs very well r squared 0 95 see fig 5 11 p t r e s o u t 1 k ˆ p r 1 p t r e s i n k ˆ p r 2 t t where p t r e s i n and p t r e s o u t are the simulated phosphorus loads into and out of the reservoir at time t and k ˆ p r 1 and k ˆ p r 2 are linear regression coefficients the accuracy of the linear regression indicates that a linear filter compatible with the response matrix approach may reasonably approximate the trapping effect because the trapping efficiency i e k ˆ p r 1 is time invariant it may be incorporated directly within the also time invariant stream connectivity matrix for all phosphorus stream connectivity matrix elements w i j p such that i d and j u where d is the set of stream reaches downstream from the reservoir and u is the set of reaches upstream of the reservoir we set w i j equal to one minus the trapping efficiency rather than 1 as had been before 12 w i j p 1 k ˆ p r 1 i f i d a n d j u 1 o t h e r w i s e i f j i s u p s t r e a m o f i 0 o t h e r w i s e meanwhile the minimum trapped load i e k ˆ p r 1 is subtracted directly from the yield of the downstream subwatershed as if a point sink 13 p t p t nps p t ps p t res t t where the minimum trapped load p t res r n is equal to k ˆ p r 2 for the subwatershed immediately downstream of the reservoir and zero otherwise we evaluate the linear approximation for in stream processing equation 7 in the usrw baseline scenario by conducting linear regressions at each subwatershed outlet for phosphorus effluent according to upstream phosphorus yields and inverse flow yields 14 p t i n k ˆ p p w p t i n k ˆ p 1 q w q i n v t t t where k ˆ p p r n x n and k ˆ p 1 q r n x n are diagonal matrices whose elements are the regression coefficients estimated using swat simulation data i e p yield and streamflow and indicating the fraction of phosphorus lost in a stream and i n r n x n is an identity matrix recall that w is the connectivity matrix accounting for upstream downstream relationships the right hand side of equation 14 is then the p export at each reach broken into a term dependent on upstream landscape p loading and a term dependent on upstream cumulative residence time larger values of k ˆ p p and k ˆ p 1 q indicate that the stream acts as more of a phosphorus sink as with the traditional rm method we apply equations 10 14 at the monthly scale we find again that the regression performs very well r squared 0 95 see fig 6 suggesting that equations 10 and 12 14 constitute an effective modified response matrix approach for phosphorus below we compare swat simulation results for monthly in stream phosphorus loads at the usrw outlet with estimates from both the modified rm formulation and a traditional point source estimate we choose to show the traditional point source estimate rather than the traditional rm estimate because 1 the point source addition method is trivial and 2 the point source load in the usrw is so large that its omission obscures the value of the other modifications first for swat simulations we randomly select one of four conservation practice combinations from table 1 filter strips only cover crops only fertilizer reduction only or all practices together for every agricultural hru in the watershed then for the rm estimates we apply the resultant subwatershed land allocation fractions as described in section 2 2 the rm estimates assume that this fraction of land allocated to a conservation practice is evenly distributed among all hrus within the subwatershed some discrepancy between the rm estimates and swat simulations may be attributed to this allocation distinction and differences in how conservation practices impact yields in different hrus i e on different soils and slopes we measure the surrogate accuracy of the rm approaches by nash sutcliffe efficiency nse and percent bias p bias between the respective rm estimated time series and the simulated time series moriasi et al 2007 we find that the modified rm formulation provides more accurate and less biased estimates nse 0 98 and p bias 0 9 than the traditional point source formulation nse 0 92 and p bias 12 8 fig 7 furthermore the modified rm formulation provides drastically more representative estimates during periods of low flow the difference between the modified rm and traditional point source estimates is most pronounced during the 2011 2012 drought from july 2011 to september 2012 swat simulates 541 950 kg total phosphorus export from the watershed the modified rm estimate is 561 562 kg export 3 6 over estimate while the traditional point source estimate is 772 064 kg export 42 5 over estimate this focused improvement from the modified rm approach during low flows is to be expected the effects of in stream processes and reservoir trapping are greatest when residence time is largest moreover this focused improvement is relevant and important since streams and water bodies receiving significant point source discharge can be most vulnerable to harmful algal blooms during low flow periods harrison et al 2019 jarvie et al 2006 the cumulative impact of improved prediction at low flows is a relatively unbiased estimate for total export over the 16 years rather than the 12 8 over prediction by the traditional point source method we repeat the above evaluation process 100 times for different realizations of randomized conservation practice allocations using modified rm method and compare the results with the same randomized allocations in swat to evaluate the robustness of rm performance under randomized allocations these realizations provide a more comprehensive picture of the modified rm performance and illustrate the impacts of the conservation practice allocation methods discussed above fig 8 presents the mean and range for the performance metrics nse and p bias across the 100 realizations and every subwatershed outlet overall the modified rm method has satisfactory performance nses 0 96 to nearly 1 p bias 15 18 across all subwatersheds except for subwatershed 8 for approximating swat in stream phosphorus loads notably the subwatersheds which perform worst are headwater subwatersheds and consist of relatively few hrus for example subwatershed 8 has only three hrus the lesser performance in these subwatersheds is likely due to the divergence between the rm assumption and swat applied method for allocating conservation practice combinations when there are many upstream hrus the upstream land allocated to each conservation practice combination will in the aggregate consist of similar soils and slope classes despite different precise allocations when there are few upstream hrus the rm estimate may not capture swat sensitivity to which soil type or slope class i e which hru a practice is implemented on therefore when applying the modified or traditional rm method at the subwatershed level it is important to acknowledge that the method allows for targeting specific subwatersheds with conservation practices but not targeting specific hrus within a subwatershed to evaluate the benefits of computational time saved by modified rm approach we recorded the computation time of simulations between original swat and our modified rm approach with the same conservation practice allocations specifically the computation for 100 randomized simulations in swat takes 2 h 20 min while the modified rm method takes about 4 2 min with a single processor in an intel core i7 8700k 64 bit and 32 gb memory windows pc thus the computational time can be reduced by about 3500 with the modified rm approach 3 3 modification 3 dealing with impacts of in stream and reservoir processes for sediment 3 3 1 swat in stream and reservoir processes for sediment sediment transport in swat depends upon flow conditions and sediment supply here sediment supply refers to sediment which has been lost from the landscape during the current model time step as well as all sediment that was previously deposited and still remains within the stream network note also sediment entrainment but not deposition is not controlled by flow conditions and sediment supply simultaneously but rather one or the other is limiting for a given reach at a specific time according to the following logic 1 sediment entrainment may only occur if the stream has sufficient transport capacity as determined by flow conditions for increased suspended sediment concentration 2 any previously deposited sediments which have remained within the reach are entrained first 3 if all previously deposited sediments are exhausted the sediment transport capacity may be met by channel bed and bank erosion but only if the streamflow generates sufficient shear stress upon the streambed bank these interactions between flow and sediment supply and between the event scale and long term accumulation present an obstacle for ascertaining a simple approximation for in stream sediment processes for zones of a stream network where transport capacity consistently exceeds sediment supply we will refer to these as supply constrained reaches would be controlled by a combination of landscape sediment loss and streambed shear stress a function of flow for zones of a stream network where sediment supply consistently exceeds transport capacity we will refer to these as flow constrained reaches would be controlled by a combination of the transport capacity a function of flow distinct from that for shear stress and the sediment deposition rate 3 3 2 use streamflow to estimate in stream sediment loads for surrogate model here we demonstrate the need to depart from an rm based approximation for sediment and offer instead a simple nonlinear approximation based on the underlying model processes in swat for flow constrained reaches within the stream network the most sensible linear approximation and rm formulation for sediment export are according to landscape water yield that is 15 s f 4 q k ˆ s q k ˆ s 1 m m d i a g y m t q d i a g a f m k ˆ s 2 t t where k ˆ s 1 mg l and k ˆ s 2 mg l are coefficients representing effects at typical conditions however the transport capacity can be a highly nonlinear function of flow therefore this approximation may be unlikely to hold for example we select the swat modeling option to use the simplified bagnold model neitsch et al 2011 for transport capacity one of four options where 16 c o n c s e d c h m x c s p v c h p k s p e x p where c o n c s e d c h m x is the maximum sediment concentration ton m3 or kg l c s p and s p e x p are parameters defined by swat modeler and v c h p k is the peak channel velocity m s during the time step a nonlinear function of inflow see eq 7 2 2 3 from swat theory documentation 2009 in the case where non linearities must be incorporated and following the bagnold equation a more representative approximation might be 17a s n f 4 q k ˆ s l i n q n k ˆ s e x p n n t t or perhaps considering the complex relationship between flow volume q here and peak flow velocity v c h p k in the bagnold equation even a polynomial approximation is suitable 17b s n f 4 q k ˆ s 1 k ˆ s 2 q n k ˆ s 3 q n 2 n n t t where k ˆ s l i n k ˆ s e x p k ˆ s 1 k ˆ s 2 and k ˆ s 3 are parameters for the sediment flow relationship at typical conditions however this nonlinear approximation is not compatible with a response matrix formulation and would possibly require the modeler to adjust their use of the surrogate for instance changing the solution method used for optimization on the other hand for supply constrained reaches the most sensible linear approximation and rm formulation likely must account for landscape sediment yield and water yield that is 18 s s f 5 q s k s q q m m d i a g y m t s d i a g a f m k ˆ s q m m d i a g y m t q d i a g a f m t t where k ˆ s q mg l is the streambed sediment contribution per unit of flow at typical conditions however the nonlinearity of streambed erosion processes may make this approximation unlikely to hold as well fig 9 a and b compares the estimates of best performing approximations with estimates of the traditional rm approach for selected illustrative subwatersheds for flow constrained subwatersheds such as shown in figs 9a and 10 b best approximations from equations 15 and 17 respectively the traditional rm approach significantly overestimates the sediment loads while the proposed modifications provide highly accurate estimates r squared 0 9 for example in subwatersheds 33 and 34 the outlet the nse for traditional rm sediment estimates are 0 08 and 1 69 respectively but improve to 0 91 and 0 97 under a flow based approximation evidently swat simulates significant sediment deposition in these zones of the stream network effectively buffering the upstream landscape sediment loss signal jerolmack and paola 2010 romans et al 2016 in some cases e g as shown in fig 9c the flow sediment relationship can also be approximately linear hence the acceptable performance of the response matrix approach presented in equation 15 however in many cases e g as shown in fig 9d the flow sediment relationship is clearly exponential or polynomial and a nonlinear approximation is required in place of the rm approach we apply the potential rm formulations presented in equations 15 and 17 a or b and 18 and compare their performance in estimating sediment load at the monthly scale at all subwatershed outlets over 100 realizations for conservation practice allocations we find that of the 45 usrw subwatersheds 20 subwatersheds are approximated well i e average nse 0 85 by either the linear equation 15 or nonlinear equations 17a and 17b approximations aligned with the bagnold equation see fig 10 for those 20 subwatersheds the linear approximation method is suggested as it may be incorporated into an rm approach 12 subwatersheds are only approximated well by the nonlinear estimates polynomial or power functions these results align with the suggestions of previous studies that due to historical management practices sediment transport in the upper mississippi river basin is generally flow constrained neal and anders 2015 trimble 1999 of the other 13 subwatersheds 6 can be approximated acceptably i e nse 0 5 but not well by one of the flow constrained approximations the remaining 7 subwatersheds cannot be approximated acceptably by any of the formulations offered here i e nse 0 5 the poor performance in sediment estimation seems to go beyond the unaccounted impacts of soil type and slope class discussed in section 3 3 2 as there is no clear trend in performance as upstream hrus increase it appears that some subwatersheds either frequently switch modes between possible dominant controls on swat sediment export not conforming to a single approximation or are generally less amenable to approximations for swat sediment simulations 4 conclusions in this work we show that 1 an accurate rm based approach to swat approximation requires that response matrices be generated for all distinct combinations of conservation practices in order to account for the interaction effects between individual practices 2 a modified rm method especially accounting for in stream and reservoir processes is required to correct estimates for phosphorus export and 3 a departure from rm based approximation is required for accurately estimating sediment instead utilizing a nonlinear flow based estimate for sediment loads we hypothesize based on primary process model understanding that the modifications presented for phosphorus could also adequately correct estimates for flow and nitrogen when necessary we also highlight that when applied for decisions at the subwatershed scale rather than the hru scale the proposed approximations perform best at outlets draining large areas the approximations detailed here provide efficient spatial and dynamic simulations on hydrological responses based on a wide range of spatial applications of agricultural conservation practices excluding sediment the approximations maintain an rm based approach facilitating advantages such as the feasibility of linear programming methods the approximations are especially well suited to integration within a system of systems modeling framework where modelers wish to consider a mixture of non point and point source models li et al 2021a for example the nutrient effluents simulated by wastewater treatment models can be directly added via the rm method to accurately simulate the in stream p load the application here is centered on surrogating the swat model but the process enlightened by the discussion provided here could plausibly be generalized to other semi distributed hydrologic models e g wasp or swmm the revised rm enables a more accurate use of a watershed hydrological model for finding optimal watershed management solutions via 1 classic optimization i e linear programming and nonlinear programming including the suggested nonlinear equation describing the instream and reservoir sediment processes 2 heuristic optimization such as genetic algorithm which uses the revised rm to replace the original simulation model data and code availability the data and codes used for constructing the modified rm are available via github https github com shaobinli modified rm declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the us national science foundation infews t1 award number 1739788 
25668,large scale hydrological models simulate watershed processes with applications in water resources climate change land use and forecast systems the quality of the simulations mainly depends on calibrating optimal sets of watershed parameters a time consuming task that highly demands computational resources from repeated simulations this work aims at performance optimizations on the mgb modelo de grandes bacias hydrological model and the mocom ua multi objective complex evolution calibration method for two watersheds the optimizations target state of the art cpu gpu systems exploiting techniques that include avx 512 vectorization and multi core cpu and many core gpu parallelisms significant speedups of up to 20 cpu were achieved for calibration while the scalability analysis indicated 24 cpu and 65 gpu for simulations with larger problem sizes the roofline analysis confirmed more effective use of the hardware resources and the quantitative accuracy evaluation of the optimized implementations reached maximum relative errors of approximately 6 for discharges and objective functions keywords high performance computing parallel processing vectorization roofline model hydrology models parameterization software availability software optimized mgb hydrological model description mgb code optimized for intel cpus with intel intrinsics and openmp and for nvidia gpus with cuda developer henrique r a freitas system requirements linux ifort pgf90 language fortran 90 size 1 2 mb zipped version availability the optimized mgb code can be freely downloaded from https github com henriquerenno mgb 1 introduction large scale hydrological models are extensively used to simulate watershed processes with applications in water resources climate change land use and forecast systems paiva et al 2011 those models simulate complex nonlinear hydrological processes and contain a set of nonphysical parameters i e not measurable from actual data acquired in field which are embedded in the model to function as conceptual representations that express watershed characteristics thus allowing the model simulated responses to satisfactorily adjust to the real world observed data and simulate the watershed behavior efstratiadis and koutsoyiannis 2010 for instance the watershed discharge several factors affect the efficiency of large scale hydrological models which highly demand computational resources for more accurate simulations of water flows those factors include the continuous advances in gis software and technologies the large availability of high resolution spatial temporal hydrometeorological data such as in situ measurements and remote sensing data the great extent of the regions processed in the simulations and the number of model parameters in calibration manual trial and error selection of the optimal sets of watershed parameters still predominates despite being a time consuming task even for experienced model users computer based automatic methods have recently been considered an effective and efficient option vrugt et al 2003 calibration methods typically employ objective functions which are computed from model simulations as measures to be minimized for the optimization of the sets of parameters implicitly solved as an inverse problem some methods focus on only one objective function such as the search based and evolutionary shuffled complex evolution sce ua method duan et al 1993 commonly employed using hydrological models however single objective methods do not provide solutions that can truly model the behavior of the observed data in hydrological simulations particularly in the extreme situations of wet and dry periods tian et al 2019 in contrast multi objective methods were employed in hydrological modeling to evaluate and capture various properties of the observed data with the critical advantage of estimating high quality parameters that can be generically used in subsequent simulations for more consistent and accurate simulated results vrugt et al 2003 even in extreme events of floods and droughts the multi objective complex evolution mocom ua method yapo et al 1998 has been successfully applied to a variety of environmental applications fagundes et al 2019 geritana et al 2014 islam and déry 2017 tatsumi 2016 tesemma et al 2015 being predominantly used for general purpose global multi objective optimization a variant of the mocom ua method was proposed as an epidemic genetic algorithm ega araújo et al 2013 employed using a hydrological model for small basins tucci 2005 the performance of calibration methods in hydrological applications is a key issue that has been continuously addressed optimization and parallel computing techniques were mainly applied to large scale hydrological models with the purpose of reducing the runtimes for estimating watershed parameters the efficiency of the sce ua method processing up to millions of objective function evaluations was improved by using parallel implementations of the xinanjiang model zhao 1992 designed for cpu openmp and for gpu cuda kan et al 2018 furthermore zhang et al 2016 proposed a double layer parallel approach for a genetic algorithm based method using the dyrim model wang et al 2007 with parallelism in the spatial decomposition of the watershed divided into independent units lower layer and also in simultaneous model simulations with different combinations of parameters upper layer a large number of calibration methods using the swat model arnold et al 1998 were also optimized mpi and python were employed to parallelize the multi objective amalgam calibration method with concurrent evaluations of simulations zhang et al 2013 the python multiprocessing module was used to develop a parallel multi objective calibration tool mameo for multi core processors zhang et al 2012 a framework based on the optimization program sufi2 for the swat cup calibration procedure automatically and transparently submit parallel jobs on multi core cpus rouholahnejad et al 2012 different hydrological models were optimized on either cpu or gpu aiming at performance improvements a simplified version of the lisflood fp model bates et al 2010 reached theoretical speedups with openmp and mpi neal et al 2010 but without processing 1d channel flow and dry checking functions and more recently was optimized to compute 2d water flows and depths on gpu using the cuda framework sarates 2015 openmp was also applied on a grid based fully sequential dependent hydrological model fsdhm to process simulation units based on a layered approach computing overland and channel flows in parallel liu et al 2014 mpi was used in the swat model with a scheme devised to decrease execution times by reducing the overheads of communication wu et al 2013 and in the tribs model vivoni et al 2011 to decompose watersheds formed by triangulated irregular networks tins as directed graphs from drainage network channels where independent sub basins were assigned to mpi processes exchanging data across boundaries the present work targets performance optimizations on the mgb hydrological model collischonn 2001 used for simulations and the mocom ua calibration method based on a double layer approach to execute the mgb model at each iteration the watersheds of the purus brazil and the niger africa rivers were chosen for simulation test cases but only the former for calibration the mgb model implements a numerical method referred to as inertial model that solves the saint venant equations to estimate water height and discharge stream flow along the longitudinal extension of the river in the watershed fan et al 2014 which is useful in many hydrological applications fagundes et al 2020 fleischmann et al 2017 2019 gorgoglione et al 2019 paiva et al 2011 the watershed parameters are used to set initial and boundary conditions for simulations and also to compute hydrometeorological data such as radiation evapotranspiration and water balance in soil those parameters are not directly used to compute water flows with the numerical method but instead to update the variables used as input to the inertial model the computational time requirements of the calibration phase were reduced by the optimizations proposed herein that exploit the ever growing parallel resources of computer systems the mgb model was thoroughly optimized for cpu on shared memory systems with vectorization and multi core parallelism as well as for gpu with many core parallelism implemented as a separate version whereas the mocom ua method was optimized for cpu with openmp modern cpus support vector extensions up to 512 bits current intel cpus for single instruction multiple data simd data parallel strategies compilers are not fully effective in automatic vectorization so hand tuned explicitly vectorized codes execute faster than the auto vectorized equivalents however explicit vectorization requires careful and detailed code analysis low level instructions and more development time mitra et al 2013 the additional availability of multiple cores in cpus ralston 2008 also enables thread parallel strategies in shared memory environments the workload is distributed to multiple cpu cores identified as threads and processed in parallel the openmp standard dagum and menon 1998 executes applications with thread level parallelism via compiler directives that dynamically activate deactivate parallel threads many core gpus provide up to thousands of parallel threads for data parallel strategies with numerical intensive workloads rupp 2016 and are designed with high bandwidth memories porting codes to gpus is usually done with frameworks such as cuda pgi 2020 opencl tompson and schlachter 2012 and openacc wienke et al 2012 openacc is based on compiler directives similar to openmp providing portable codes for different gpu architectures however in most cases the highest performance is achieved with the cuda framework although it requires more effort and development time fully exploiting the hardware resources not only decrease the overall execution times of the mgb model and improve the performance of the calibration but also provide more understanding about the mgb model s computational requirements building efficient and portable applications requires performance analysis tools to help developers exploit the hardware potentials the cache aware roofline model carm ilic et al 2013 lopes 2016 marques et al 2020 identifies the factors that limit an application s performance flops s and visually represents the compute and memory behavior of an application system pair as guidelines for optimizations according to the application s arithmetic intensity flops byte the roofline characterization of the mgb model shows how the hardware resources of the employed architectures are more effectively exploited with the proposed optimizations and allows to evaluate the achieved optimization levels revealing that effective use of the capabilities of each system may improve performance the mixing of explicit vectorization and directive based parallelization proposed as a set of advanced optimizations to exploit most of the capacity of vector resources and shared memory parallelism offers more opportunities for improving the performance of hydrological models with similar characteristics to the authors knowledge the optimizations proposed herein are the first that employ explicit vector instructions with intel intrinsics for hydrological models together with the roofline model to analyze performance improvements a scalability analysis using a miniapp a reliable proxy of the mgb model developed to process larger problem sizes shows the performance gains achievable if large datasets are available miniapps are testbeds that should accurately model the performance bottleneck of full applications stone et al 2012 and also be able to scale the problem size while matching the performance the prediction and evaluation of how the full application is affected by code changes are issues investigated using miniapps lin et al 2015 murai et al 2017 2 methods this section introduces the mgb hydrological model and the mocom ua calibration method which are employed to find the optimal sets of watershed parameters details about the optimizations on the calibration and simulation parts are described by following the double layer approach where mgb model s simulations are simultaneously executed on different cpu cores upper layer and each simulation is also parallelized lower layer 2 1 mgb hydrological model the mgb modelo de grandes bacias hydrological model collischonn 2001 is a fortran code developed at iph ufrgs instituto de pesquisas hidráulicas universidade federal do rio grande do sul in brazil focusing on improving the knowledge of hydrological processes in large scale watersheds simulations of the mgb model generate 1d propagation of water flows in rivers as illustrated in fig 1 which exhibits the elevation z water height h water level y discharge q and length δx of each segment of the river stretch discretization from the numerical scheme detailed in subsection 2 1 1 the mgb model simulates the hydrological cycle computing soil water and energy budgets evapotranspiration interception surface subsurface and groundwater flows in either daily or hourly time steps for the forecast of river discharge analysis of extreme events floods and droughts estimation of the effects of soil and vegetation cover on climate water quality and sediment transport paiva et al 2011 the spatial discretization comprises three hydrological units catchments sub basins and hydrological response units hrus catchments and sub basins are surface regions that contribute water to drainage network segments and to outlet points respectively whereas hrus are regions that combine land use cover soil and slope maps based on user defined thresholds kalcic et al 2015 fig 2 illustrates catchments smallest spatial units delineated for a drainage network of the purus watershed defined from a threshold of accumulated flows equal to 10 000 m2 and generated with the terrahidro computational platform jardim 2017 rosim 2008 the mgb model has been effectively employed in a diverse and large number of applications such as sediment modeling for estimation of soil erosion sediment transport and deposition fagundes et al 2020 modeling of reservoirs as an internal boundary condition for simulation of hydrodynamic processes and their interaction with upstream and downstream floodplains fleischmann et al 2019 analysis of water conflicts in transboundary watersheds to improve the sustainability of water allocation gorgoglione et al 2019 representation of the interactions between hydrology and hydrodynamics mainly infiltration from floodplains into soil to improve model estimates fleischmann et al 2017 and hydrological and hydrodynamic modeling to simulate fluvial processes of wave delay and attenuation backwater effects flood inundation and its effects on flood waves paiva et al 2011 2 1 1 water flow equations and numerical scheme the mgb model simulates the spatial and temporal distribution of water flows in a watershed measured by water height and discharge through an implementation of the inertial simplification fan et al 2014 of the saint venant equations cunge et al 1980 these equations are referred to as continuity 1 and momentum 2 equations where h is water height q is discharge y h z is water level given by the sum of water height h and elevation z g is the acceleration of gravity n is the manning coefficient t is time and x is longitudinal distance 1 h t q x 0 2 q t g h y x g q q n 2 h 7 3 0 equations 1 and 2 are numerically solved with forward in time and centered in space finite difference approximations that integrate through time with initial conditions from soil moisture water volumes and water flows in reservoirs and boundary conditions from accumulated flows muskingum cunge and reference discharges the numerical solution is a scheme of wave propagation commonly employed in rainfall runoff hydrological models the explicit numerical scheme is shown in equations 3 7 where c is the number of catchments of the watershed and i and k are spatial and temporal indexes respectively 3 δ t m i n α δ x g h i k i 1 2 c 4 h i 1 2 k m a x y i k y i 1 k m a x z i z i 1 5 q i 1 2 k 1 q i 1 2 k g δ t h i 1 2 k y i 1 k y i k δ x 1 g δ t q i 1 2 k n 2 h i 1 2 k 7 3 6 h i k 1 h i k δ t δ x q i 1 2 k 1 q i 1 2 k 1 7 y i k 1 z i h i k 1 the time step of each catchment i from equation 3 requires 0 α 1 to avoid numerical instability bates et al 2010 where α 0 7 in the mgb model fan et al 2014 the minimum time step δt corresponds to the maximum water height h i k among all catchments the water height h i 1 2 k at position i 1 2 border is the difference between the maximum water levels y and elevations z at positions i and i 1 as in equation 4 the outflow discharge q i 1 2 k 1 of each catchment for the next time step k 1 is found through equation 5 from the previous discharge q i 1 2 k water height h i 1 2 k and water levels y i 1 k and y i k the water height h i k 1 and level y i k 1 at position i center are obtained from the updated discharges q i 1 2 k 1 and q i 1 2 k 1 as in equation 6 and from the water height h i k 1 as in equation 7 the numerical scheme of equations 3 7 executes m times for all c catchments where m is a constant that depends on the input data so that the computational complexity is o m c the amount of computations required to find the numerical solution at each time step is mostly determined by the number of catchments c as this value can theoretically increase without limits 2 1 2 mgb code structure the mgb model s numerical scheme inertial model is divided into three routines timestep ste discharge dis and continuity con these routines are successively called for each minimum time step δt that satisfies the inertial model s stability and are accumulated up to the mgb model s time step each routine is executed for all iterations of the mgb model a fixed number of steps from the input data processing each catchment to compute the minimum time step in the ste routine equation 3 water height and discharge in the dis routine equations 4 and 5 and water height and level in the con routine equations 6 and 7 as well as water volume and cross sectional area besides the saint venant equations the mgb model computes hydrometeorogical data such as radiation evapotranspiration penman monteith equations shuttleworth 1993 and vertical water balance in soil collischonn 2001 code 1 structure of the mgb model image 1 code 1 provides the mgb model s structure it includes the routines that load rainfall and climate data load rainfall climate data compute hydrometeorogical data calc radiation calc evapotranspiration and calc water balance and execute the inertial model inertial model 2 1 3 profiling of serial execution based on serial executions of the original mgb code compiled with default optimization flags the routines of the inertial model were identified as the most time consuming routines from reports of the gnu s gprof profiling tool those routines accounted for the highest percentage of execution time from either thousands or millions of calls for different simulations taking on average 92 96 of execution time as the main execution bottleneck of the mgb model table 1 provides the profiling details including the average runtime and percentage of execution time of each routine relative to the mgb model s execution time and also the number of calls these values were obtained from simulations using different datasets 2 2 mocom ua calibration method the mocom ua method formulates and implicitly solves the estimation of the sets of model parameters as an inverse problem i e successively generating candidate solutions to minimize multiple objective functions which express the difference between simulated and measured data in this work the calibration employs three objective functions sorribas et al 2013 a the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 computed from observed discharge average observed discharge and simulated discharge b the nse computed from the natural logarithm of the discharges nse log and c the systematic error both nse and nse log are within the range 1 where 1 means the best fit and the systematic error is given as a percentage the mocom ua method is based on determinism to converge to the local global optimal solutions simplex search probability to better explore the search space escaping from local minima random search evolution to improve the solutions population evolution and nonuniqueness of solutions to provide multiple optimal solutions pareto front a population with a fixed number of candidate solutions or independent samples is generated in a multi dimensional search space where each dimension represents a particular model parameter each solution or sample corresponds to a point in the multi dimensional space with coordinates given by the parameter values which are randomly computed from restricted ranges of the domain for the initial iteration each combination of parameters is used in simulations and the objective function values are compared to rank all samples according to the pareto ranking in this ranking a sample with all objective function values greater than the corresponding ones from another sample is considered inferior and identified as dominated otherwise it is nondominated all nondominated samples are ranked 1 then the remaining dominated samples are checked and the new nondominated ones are ranked 2 and so on in the end the worst ranked samples are ranked rmax maximum rank the optimal set of samples sets of parameters is located in the region of the objective space called pareto front which consists of all solutions equally acceptable as optimal solutions in the pareto ranking lower ranked samples are considered superior and located closer to the pareto front the worst ranked samples are improved with simplex operations reflection and contraction using other samples randomly selected according to a probability distribution from all ranks where samples with smaller larger ranks have higher lower probability to be selected the simplex operations require r more samples number of parameters to generate a new sample from the worst ranked sample s w and the centroid s c of the r samples firstly if the new sample s ref from the reflection operation equation 8 is nondominated relative to the r selected samples it replaces s w otherwise the new sample s con from the contraction operation equation 9 replaces it after all worst ranked samples are processed the pareto ranking is recalculated and this repeats until all ranks are equal to 1 8 s r e f 2 s c s w 9 s c o n 0 5 s c s w 3 performance optimization techniques the optimizations selected to boost the performance of the mocom ua method and of the mgb model consist in data parallel and thread parallel strategies vectorization and multi core cpu and many core gpu parallelisms the upper layer employs multi core cpu parallelism openmp to process mgb model simulations in parallel for s samples parameter sets θ i i 1 2 s the lower layer implements different optimizations intrinsics openmp cuda for parallel processing of catchments fig 3 illustrates the parallelization scheme 3 1 simulation mgb model the numerical scheme implemented in the routines of the inertial model processes all catchments in a domain that can be divided into independent subdomains which are updated using only previous water flows this differs from implicit numerical schemes that would require a system of equations to be solved at each time step hoffman 2001 each routine includes a single loop that iterates over the catchments with successive and independent memory accesses defining codes highly suitable for different parallelization strategies 3 1 1 optimizations for cpu the multi core cpu optimizations proposed herein employ explicit vectorization via interoperability between the intel intrinsics library available in c c intel 2020b and the fortran mgb model s code and openmp parallelization openmp provides portable optimized codes but the vectorization requires specific cpu hardware resources avx 512 vector extensions these optimizations are detailed for the ste routine that processes catchments to compute the minimum time step δt openmp threads are created outside the routine in a parallel region for simultaneous independent calls to the inertial model code 2 shows the ste routine optimized with vectorization and thread parallelism the input variables are the height h of the water flow hflow length δx of the river stretch lriver constants α alpha and of gravity g g time step dt minimum time step of each thread minlocaldt thread number th number of threads nth and catchments nc the m512d variables are vectors that store multiple data into the cpu registers for data parallel vector processing firstly the minimum time step minlocaldt of each thread th and the corresponding vectors mindt are initialized with the mgb model s time step largest value from argument dt after the initialization the loop of catchments is processed by the openmp threads and each thread cooperates by executing a number of iterations multiple of 8 nth whereas the remaining iterations are sequentially processed outside the routine the stride of 8 means that 512 bit cpu registers and vector instructions load mul div sqrt min simultaneously process eight 64 bit double precision data computing time steps for catchments in parallel the openmp clause for static scheduling uniformly distributes the workload which is well balanced in the loop iterations that execute similar instructions code 2 ste routine optimized for cpu with intrinsics and openmp image 2 whenever each thread th exits the loop a minimum reduction vector operation reduce finds the minimum time step to be stored into minlocaldt th finally all threads wait at a synchronization barrier to ensure all local minimum time steps are available so the master thread th 0 can update the global minimum time step dt this is the only routine that requires a synchronization mechanism to ensure thread safety and correct behavior with multiple threads the optimizations on the dis and con routines are more complex the dis routine independently updates the discharge q of each catchment the input variables are the elevation z of the river bottom water height h to compute water level y length δx and width of the river stretch manning coefficient n current discharge q minimum time step δt ste routine and reference to the downstream catchment i e the single catchment where water flows the dis routine executes arithmetic and logical vector instructions the latter referred to as mask vector instructions used for logical conditions to concurrently check whether the downstream water flows are present or not these instructions provide vector masks that indicate which catchments update the discharge q i e only where the downstream water flow is present the con routine uses the minimum time step δt ste routine and the discharge q dis routine catchments are also independently processed using as input variables the discharges from the upstream catchments evapotranspiration rainfall length δx of the river stretch mgb model s time step as well as the range positions of precomputed water volumes cross sectional areas and elevations that are stored in nondecreasing order as 2d tables for each catchment this routine updates the water volume cross sectional area water height h and water level y for the next iteration of the inertial model the sum of the upstream discharges of each catchment is initially computed with a sum reduction vector operation by collecting and adding up the discharges from the upstream catchments which were contiguously arranged in memory in the optimized routine for more efficient memory accesses by the vector instructions new water volumes are computed using the upstream discharges and each catchment searches its table of water volumes for the new value the table search retrieves precomputed water volumes cross sectional areas and elevations from ranges with the new value where gather vector instructions collect the values from memory positions of the ranges volume fractions are computed from the new water volumes to interpolate the cross sectional area and water level the boundaries of the ranges are checked with vector masks and the table search shifts either left or right to locate ranges with the new water volumes the search stops when all ranges are found updating the current range positions fig 4 illustrates the relation between catchments and water volume table table rows store precomputed water volumes v for each catchment 1 i c and the eight positions stored into the vectors vpos0 and vpos1 specify the ranges with the current water volumes v i j 1 w i v i j v i 1 j 1 w i 1 v i 1 j 2 v i 2 j 6 w i 2 v i 2 j 7 and so on eight values of water volumes v located in the range vpos0 vpos1 are collected from memory positions using a gather vector instruction gather and compared using a logical vector instruction masks with the volumes w of the catchments whenever the water volumes w are updated the current positions vpos0 independently shift either left or right table columns to the new range for each catchment vector optimizations are fully described in freitas 2021 freitas et al 2020 3 1 2 optimizations for gpu the ste dis and con routines were executed on gpu as cuda kernels with the cuda fortran interface nvidia 2020a supported by the pgi pgf90 compiler pgi 2020 the cuda framework was chosen because we previously applied directive based gpu programming standards openacc on the mgb model freitas and mendes 2019 but that particular optimization decreased the performance mainly caused by overheads associated to the openacc directives that increased runtimes from millions of routine calls not amortized by thousands of loop iterations catchments i e computations were not gpu intensive current nvidia gpus provide thousands of parallel threads which is enough to cover all loop iterations in the routines of the inertial model for the available datasets the cuda kernels assign each thread to one single catchment increasing the degree of parallelism in the cuda implementation the variables of the inertial model were globally declared as device variables in a fortran module with data transferred between host cpu and device gpu memories code 3 illustrates the fortran module with the cuda version of the ste routine which required an explicit minimum reduction operation for the minimum time step δt cuda does not include a minimum reduction operation differently from intel intrinsics library function and openmp directive clause firstly each thread is identified from cuda structures such as the size blockdim x and index blockidx x of the cuda blocks and the thread index inside the cuda block threadidx x for the cuda grid x dimension to be stored into the variable i this variable is an index to each catchment and checked if it is valid i e not larger than the number of catchments nc all valid threads compute time steps in parallel in the ste cuda routine independently storing the values into the localdt array the dt cuda routine executes the minimum reduction operation in one single cuda block for thread synchronization processing only half of the array to simultaneously update the minimum time step between positions i first half and i pow second half where pow2nc is the smallest power of two relative to nc the minimum time steps are kept in the first half of the array and all threads are synchronized cuda function syncthreads this repeats by halving pow at each iteration of the reduction and in the end the global minimum time step is stored into the first position of the localdt array finally localdt 1 updates the global minimum time step dt for the dis and con cuda kernels code 3 ste routine optimized for gpu with cuda fortran image 3 3 2 calibration mocom ua method the mgb model s parameters selected for calibration are shown in table 2 which includes soil parameters that largely affect hydrological responses in wet and dry periods collischonn 2001 the ranges specify multipliers for the parameters relative to the initial value the calibration implementation was optimized on steps that simulate the mgb model with the current set of samples and that evolve the population of samples with simplex operations the former step computes simulated discharges to evaluate the objective functions for each set of parameters in order to update the pareto ranking of all samples the latter step executes simulations for worst ranked samples modified with simplex operations all samples are independent from each other and can be processed in parallel for both steps which highly demand computational resources to execute full mgb model s simulations for the parallel executions of the mgb model to properly work each sample requires separate executions of four routines successively called to a set parameter values b set initial and boundary conditions c simulate the mgb model and d compute objective functions each sample is assigned a new rank only after these routines have been completely executed for all samples code 4 shows the calibration implementation parallelized with openmp for simultaneous mgb model s executions where the input argument s is the number of samples the initial random parameters routine allocates the s samples in the samples population array storing initial random parameter values for each sample after the initialization all samples are independently processed in a loop with the aforementioned routines upper layer simultaneously executing the set parameters set conditions mgb model and objective functions routines for the initial pareto ranking of all samples the loop is parallelized with an openmp directive that assigns each loop iteration sample to one single thread where the number of threads is set to ncores cpu cores the schedule static 1 clause cyclically assigns one loop iteration at a time next the calibration starts and repeats while there are samples to be improved i e there is at least one sample with pareto rank greater than one rmax 1 firstly the pareto ranking routine assigns ranks to the samples and each rank associates a probability to the sample in the samples probability routine the nrmax worst ranked samples are selected and the reflection and contraction simplex operations are executed in the simplex ops routine generating two new samples for each worst ranked sample but only one updates the current population by replacing it the loop that updates the population of samples is also parallelized with openmp the reflection sample is attempted first and if it is a nondominated sample it replaces the current worst ranked sample otherwise the contraction sample replaces it after all worst ranked samples are processed the condition of the calibration loop is rechecked to verify whether it continues or not in the calibration the mgb model executes on either cpu or gpu lower layer whereas all other routines execute only on cpu in addition the random probability generated to select samples for the simplex operations determines how fast worst ranked samples move towards the pareto front so execution times of calibration runs can randomly vary even with the same initial set of samples code 4 calibration procedure of the mocom ua method image 4 the mgb model and the mocom ua method implementations were thoroughly adapted so that simultaneous simulations avoided data race conditions variables used in the calibration needed an extra dimension allocated as the number of parallel threads ncores independently indexed by each thread th in code 4 whereas in other routines such as radiation evapotranspiration and water balance in soil had to be locally defined as private to each thread additionally another implementation considers the parallel processing of more samples from the population not only the worst ranked samples this approach tries to further increase the performance of the current implementation by also selecting samples with pareto rank just below the worst rank rmax i e rmax 1 rmax 2 and so on the purpose of this strategy is to improve more bad samples at the same iteration while still maintaining the calibration quality i e objective functions are similar to the original calibration this proposed approach has the advantage of more efficiently exploiting the hardware resources by executing the calibration loops with the number of openmp threads equal to the available cpu cores the degree of parallelism increases as more samples can be processed in parallel and more parallel threads are simultaneously working reducing core resources that remain idle 3 3 computational testbed the computational testbed employed to conduct the experiments consists of two multi core cpu gpu systems with linux centos 7 5 and intel 19 0 5 nvidia 10 2 and pgi 19 10 software development tools the hardware specification of each system is detailed in table 3 including the processor and memory capabilities of the cpu and gpu architectures the floating point double precision cpu performance and dram bandwidth were experimentally obtained with the intel advisor performance analysis tool intel 2020a both cpu architectures are based on intel s skylake that supports avx 512 vector instructions 512 bits 3 4 input datasets two datasets were used as input to the mgb model providing hydrometeorogical data from the purus and niger rivers the purus river fig 5 a is one of the main tributaries of the amazon river in brazil with drainage area of 370 000 km2 and average discharge of 11 000 m3 s paiva et al 2011 its region is mostly covered by tropical rainforests which provide high annual rainfall rates and the watershed topography ranges from 10 m at the outlet to 581 m at the highest headwater measured from the shuttle radar topography mission srtm 30 m topographic data farr et al 2007 the niger river fig 5b covers the guinea highlands and sahel desert as the largest river in west africa with drainage area of 657 000 km2 fleischmann et al 2017 and average discharge of 900 m3 s mahé et al 2009 the geographical region presents semi arid climate with large seasonal variation in rainfall and river flow zwarts et al 2005 where approximately 40 of the water available from headwaters are lost due to either evaporation or infiltration and also because of the small number of tributaries along the watercourse extension the construction of dams and irrigation systems provides solutions of hydroelectric structures to optimize the scarce water availability the datasets were provided by iph ufrgs as worst case datasets for these regions requiring high computational effort to be processed each dataset contains discrete data for hydrological units of catchments sub basins and hrus spatial for each time step temporal two datasets were used for purus simulation and calibration whereas only one for niger simulation the number of catchments sub basins hrus and time steps are in table 4 4 computational performance results this section presents the runtimes and corresponding speedups ratio between sequential and parallel runtimes of the optimized implementations as measures for performance analysis scalability issues of the simulations are investigated as well as the cpu and gpu roofline characterizations of the mgb model the numerical accuracy of the hydrological results is quantitatively evaluated 4 1 performance on multi core cpus and many core gpus this subsection presents the performance of simulation mgb model and calibration mocom ua method results include both the cpu and gpu performance on each computer system for the available input datasets 4 1 1 simulation the experiments conducted on multi core cpus executed the original nonoptimized mgb model and the optimized version with avx 512 vector instructions and openmp directives the number of openmp threads ranges from one to the total cpu cores available on each multi core cpu disregarding hyper threading as it showed only marginal performance improvements for the performance analysis of the fully optimized mgb model it is also important to evaluate the performance without vectorization i e using only thread parallelism with openmp regarding portability issues as the vectorized code requires avx 512 vector extensions the nonvectorized code can serve as a basis to verify how much the performance increases with additional vectorization fig 6 illustrates the differences in runtime and speedup for the vectorized and nonvectorized mgb model executed with 1 2 4 and 8 openmp threads on system01 for the purus and niger datasets the vectorized mgb model was faster than the nonvectorized version achieving speedup differences of up to 1 86 for 8 openmp threads about 42 higher moreover the runtimes of the nonvectorized version executed with only one openmp thread were lower relative to the original nonoptimized version this probably occurred because openmp directives trigger additional compiler optimizations fig 7 exhibits the runtimes and speedups achieved by the fully optimized mgb model on the multi core cpus of each computer system for the simulation input datasets the single threaded vectorized mgb model reached the highest speedup of 2 71 for the purus dataset on system01 which indicates that the overall performance significantly improved with only vectorization the lowest runtimes for the single threaded version were achieved on system02 due to the more advanced cpu and higher bandwidth memory the performance analysis reported by the intel vtune profiler intel 2020a indicated that the performance of the single threaded vectorized mgb model was limited by how many floating point instructions of the original code were vectorized which did not exceed 65 3 measurements of cpu cycles spent on long latency divisions and on pipeline slots stalled by load store instructions reached up to 27 9 and 25 7 respectively which also hindered performance vectorization and multi core parallelism combined further reduced the runtimes of the mgb model increasing the corresponding speedups although reaching a plateau with only slight changes independently of the number of threads this occurred because the workload catchments processed by the routines of the inertial model consists in thousands of loop iterations not enough for good scalability as each thread is assigned few loop iterations this effect can be noticed by comparing the speedups of the purus and niger cases as the workload of the niger dataset is approximately 2 larger relative to the purus dataset the hardware resources are more efficiently exploited in the former case resulting in higher speedups even though the parallel runtimes across systems were similar the speedups on system01 were higher as the execution of the nonoptimized mgb model on this system was slower so the optimization effects were more pronounced than on system02 the performance of a parallel program can also be theoretically predicted with measures that indicate how much performance improvement can be achieved by increasing the number of threads the amdahl s law quinn 2004 can provide an upper bound on the speedup achievable with the openmp threads mainly depending on the fraction of operations that must be sequentially executed from amdahl s law the highest speedups achievable by the mgb model on system01 and system02 assuming full parallelization of the inertial model are 5 75 and 5 88 respectively these theoretical speedups were computed from the average runtime percentage of the inertial model subsection 2 1 3 relative to the full execution of the original nonoptimized mgb model across the purus and niger datasets on each system for the total cores available the highest speedups of the optimized mgb model on system01 were 5 32 purus and 6 27 niger both with 8 threads whereas on system02 were 4 26 purus and 4 89 niger the former with 4 threads and the latter with 8 threads therefore significant speedups were achieved with the optimizations proposed herein and the case of speedup higher than the theoretical peak niger dataset on system01 probably occurred due to cache effects as more l2 caches faster accesses were available with multi core processing besides executions of the mgb model on multi core cpus the cuda implementations for many core gpus achieved higher speedups for the inertial model computed from serial cpu runtimes and parallel gpu runtimes as gpus work with sets of 32 threads warps the cuda kernels were executed using 256 threads per block multiple of 32 that resulted in the highest speedups for the mgb model fig 8 depicts a comparison between the optimal cpu speedups and the gpu speedups relative to the original cpu runtimes where the inertial model s performance on gpu was higher than on cpu in all cases the runtimes of the ste dis and con routines were significantly reduced with gpu executions of the cuda kernels the average runtimes of the routines for the purus and niger datasets from both gpus were 2 33 s and 1 27 s respectively explaining the noticeably high speedups of the dis and con routines that execute more computations as highlighted for the niger dataset on system01 the performance of the ste and con routines on cpu and gpu was hindered by insufficient floating point operations ste and by the table search that requires many memory movements con not reaching speedups as high as the dis routine which more effectively exploited hardware resources although the runtimes of the routines were considerably small the mgb model s speedups on both the pascal gpu of system01 gpu01 and volta gpu of system02 gpu02 were lower than the speedups on the multi core cpus cpu01 and cpu02 the main cause was the overheads of data transfer between host cpu and device gpu memories and of kernel launches by collecting performance data with the cuda profiling tool the percentage of the mgb model s execution time on gpu spent on data transfer and kernel launches ranged from 68 90 to 71 88 for purus and from 53 79 to 54 57 for niger table 5 the purus dataset required more calls to the cuda api function of kernel launches 4 272 300 than the niger dataset 2 791 500 the mgb model s performance could increase with larger workloads as the available datasets are not gpu friendly so developing more efficient cpu codes is more advantageous than the gpu solution even though the gpus provided enough threads to assign each loop iteration to one single gpu thread significantly reducing the inertial model s runtimes the mgb model did not scale as the overheads increased runtimes decreasing the mgb model s performance 4 1 2 calibration the calibration considers a particular purus dataset that additionally employs a time series of observed discharges which are essential data to compute objective functions to calibrate the watershed parameters the observed discharges are from the municipality of canutama in the amazonas state brazil located at the outlet of the purus watershed all discharge values were collected with a fluvial gauge controlled by the brazilian national water agency or ana agência nacional de águas for the period from january 2nd 2000 to january 7th 2015 the calibration was executed with these optimizations a either vectorization and multi core cpu parallelism or many core gpu parallelism to process catchments simulation and b multi core cpu parallelism to process samples calibration the performance analysis considered three types of execution depending on the selected optimizations a nonoptimized simulation and calibration original code b optimized simulation and c optimized simulation and calibration besides setting the number of samples s 100 as the only input argument the calibration was also configured to limit the evolution of samples to 400 generations this value was obtained from fully executing the nonoptimized original calibration on system01 422 and system02 434 fig 9 exhibits the runtimes and speedups for all execution types identified by codes indicating whether or not the simulation and calibration were optimized the types v0 x for cpu specify nonoptimized calibration v0 and x threads in simulation thus x 0 is the original nonoptimized code and x 1 2 4 8 10 is the simulation optimized with vectorization and x threads the types v1 and v2 are the optimized calibration that assigned samples to cpu threads by selecting from either worst ranked samples v1 or also bad samples with pareto rank greater than one in the same generation v2 in both cases simulations were executed with only vectorization to avoid overheads and performance degradation with nested parallel regions when calling the ste dis and con routines results for the type v2 were obtained from calibration executions that processed bad samples of pareto rank up to rmax 5 achieving the highest speedups the types v0 v1 and v2 for gpu also assigned samples to cpu threads in calibration although simulations were executed on gpu the calibration is a really time consuming task the original nonoptimized calibration v0 0 took more than seven and eight days to reach the limit of 400 generations when executed uninterruptedly on system02 and system01 respectively whenever the dataset is updated for a recalibration computational resources are used for long periods of time directly affecting the calibration usefulness the calibration with optimizations only in simulation v0 x showed considerable performance gains the smallest runtime was 132 073 s 1 d 12 h 41 min 13 s on system01 with 8 threads v0 8 reaching the speedup of 5 78 the use of 10 threads on system02 v0 10 had a small effect on decreasing runtimes as workloads are not large enough to exploit more cpu resources even though the types v0 x reached significant speedups the highest performance levels were achieved with the type v2 on both systems therefore selecting and processing more samples for each generation not only the worst ranked samples rmax as originally established in the mocom ua method resulted in additional performance gains in this case the smallest runtime was 33 738 s 9 h 22 min 18 s on system02 reaching the highest speedup of 19 89 gpu results were similar for all types of optimization and did not exhibit performance gains when compared to the best cpu results gpu runtimes and speedups were comparable to the cpu results obtained from the type v0 2 this behavior was expected as the previous analysis about the performance of the simulations executed on gpu showed that overheads of data transfer and kernel launches degraded performance this effect was more pronounced in calibration which required a large number of simulations as indicated in table 6 4 2 scalability with problem size the available datasets used to simulate real world hydrological scenarios with the mgb model did not provide workloads catchments or loop iterations of sizes large enough to reach optimal scalability even by exploiting most hardware resources of the multi core cpus and many core gpus additional insights on the mgb model s parallel speedups were obtained from a scalability analysis conducted with larger datasets which were designed from replication of parts of the original data as input to a mgb model s miniapp that reproduces the same computations and optimizations while processing workloads of varying sizes more flexible settings larger datasets allow to evaluate how the mgb model s performance scales with problem size the mgb model s miniapp also includes the key ste dis and con routines of the inertial model however all data explicitly computed at each time step in the routines for radiation evapotranspiration and water balance was previously stored into separate files with the original model to be read by the miniapp this avoids full mgb model s simulations so only the critical ste dis and con routines process varying workloads replicated in the loop of each routine as a multiple of the original size fig 10 in logarithmic scale illustrates the runtimes and speedups of the mgb model s miniapp relative to the problem size from executions on cpu and gpu for both datasets the problem sizes range from 1 to 32 the original size of each dataset cpu results are from simulations optimized with vectorization and multi core parallelism using the maximum cores available on each system i e 8 cores on system01 and 10 cores on system02 whereas gpu results are from simulations optimized with cuda cpu speedups increased up to the problem size of 8 for both datasets reaching the highest value of 23 56 for the niger dataset on system01 most speedups seemed not to improve with problem sizes larger than 8 as larger datasets increase accesses to the slower dram and the memory latency is not totally hidden even with more l2 caches available from the cpu cores however the lowest speedup of 8 07 for the purus dataset on system01 problem size of 32 was still a satisfactory result achieved using 8 cpu threads speedups on system02 were higher with the largest problem size because this system provides more cpu cores larger l3 cache and higher bandwidth memories gpu speedups behave differently and kept increasing with the problem size on both gpus although at different rates the gap between speedups from the pascal gpu system01 and volta gpu system02 was largest for the niger dataset with the problem size of 32 where the speedup on the pascal gpu was 22 90 and on the volta gpu was 65 20 2 85 higher this was expected as both the performance and dram bandwidth of the volta gpu are higher relative to the pascal gpu by comparing the cpu and gpu speedups as in fig 11 smaller problem sizes present higher cpu speedups mainly when compared to the pascal gpu this occurs because the overheads of data transfer and kernel launches associated with the mgb model s execution on gpu for small datasets degrade performance furthermore the maximum performance of both cpus 919 5 gflops s is higher than the performance of the pascal gpu which has more impact in small cases than the slower cpu dram bandwidth 68 5 gb s however for larger problem sizes the performance on gpu was higher than on cpu as shown for the problem size of 16 on the volta gpu the scalability analysis of the mgb model s miniapp indicates that if large datasets are available the mgb model s performance will be higher on gpu than on cpu 4 3 cpu and gpu roofline analysis the roofline model provided a visual analysis of the mgb model s performance behavior the cpu roofline characterization includes both the original nonoptimized and optimized versions where only the key routines of the inertial model were analyzed flops and bytes processed by each routine were collected with the intel advisor performance analysis tool intel 2020a fig 12 exhibits the cpu roofline characterization of the ste dis and con routines for each dataset on both systems in all cases the optimized routines moved upwards indicating performance increase in relation to the nonoptimized versions and confirming that vectorization and multi core parallelism more effectively exploited the hardware resources these roofline characterizations of the optimized routines include the performance using all cores on each system which resulted in the highest performance on each roofline characterization the nonoptimized routines were located under the dram roof higher memory levels l1 l2 and l3 caches were not frequently accessed so the routines required more accesses to the slower dram affecting the performance however as all original routines were also located under compute roofs the intrinsics openmp optimizations increased the performance moving routines closer to higher roofs vectorization in the optimized codes decreased the amount of data loaded from memory as more data is kept into the cpu registers the fastest units of data storage in the cpu hardware this optimization approximately doubled the arithmetic intensity of the dis and con routines and together with thread level parallelism moved their performance from under the dram roof closer to the l2 and l3 cache roofs mainly due to the larger availability of l2 caches from multiple cores in contrast the arithmetic intensity of the ste routine did not increase because the bytes transferred through the memory hierarchy became larger in the optimized code possibly due to the thread synchronization of the minimum reduction operation in summary the arithmetic intensity changes were ste 0 58 0 44 dis 0 05 0 13 and con 0 06 0 11 besides cpu roofline characterizations the routines of the inertial model were also characterized with gpu roofline models from the nvidia gpus pascal and volta flops and bytes for the gpu roofline characterization were collected with the cuda component of the performance api papi dongarra et al 2001 a widely used interface that can access hardware performance counters from cpu and gpu architectures the performance counters selected to collect flops include double precision single precision and special mathematical functions floating point operations whereas the ones selected to compute transferred bytes include load and store memory transactions per memory request fig 13 exhibits the gpu roofline characterization of the optimized ste dis and con routines the arithmetic intensity of each routine on gpu ste 1 10 dis 1 31 and con 0 15 is different from the corresponding one on cpu because cpu and gpu architectures work differently the performance levels were nearly the same on both gpus for each dataset showing that performance improvements achieved with cuda were independent of processor and memory gpu capabilities almost all cases from routine dataset pairs remained under the dram roof global memory with the exception of the con routine that more often executes the table search and keeps data in cache the niger dataset requires more floating point operations thus resulting in higher performance performance results obtained from intel advisor cpu and from papi gpu are shown in fig 14 the mgb model s gpu performance was not computed because the performance counters were collected only for the routines executed on gpu as cuda kernels not the full mgb model the gpu performance was higher for the niger dataset more floating point operations and the only case with similar cpu and gpu performance levels was the ste routine for the purus dataset regardless of the system the highlighted values show the biggest cpu and gpu performance differences that occurred with the dis routine largest number of computations for the niger dataset more effectively exploiting gpu resources and reaching more than 400 gflops s the cpu performance for the purus dataset exhibited the same pattern on both systems but the larger niger dataset showed higher cpu performance on system02 this system provides larger l3 cache and more cpu cores with faster caches reducing dram accesses for larger workloads that affect performance 4 4 accuracy of hydrological results the vectorization with intel intrinsics in the optimized cpu version of the mgb model demanded extensive tests to detect which vector instructions for low level compiler instructions precisely simulated the original mgb model numerical variations accumulated at each time step throughout simulations mainly due to how fortran treats single precision and double precision constants produced outputs differing in the order of either hundreds or thousands between the nonoptimized and optimized implementations affecting the accuracy nevertheless the optimized mgb model was carefully implemented to solve data type conversion issues and final simulation outputs from both nonoptimized and optimized cpu versions matched exactly in contrast the gpu mgb model s version optimized with cuda did not produce the same simulation outputs as the original nonoptimized cpu mgb model reference although results showed only minor variations the mathematical functions implemented in the nvidia cuda math library may differ from cpu implementations so results are not expected to exactly match for a given input nvidia 2020b although various sets of pgi compiler flags related to floating point operations were tested on gpu trying to simulate the intel compiler s arithmetic behavior on cpu all attempts were unsuccessful fig 15 illustrates differences between simulation outputs discharge and water height at the purus watershed s outlet obtained from the original nonoptimized cpu and optimized gpu versions of the mgb model the time series of simulated discharges and water heights were practically the same so gpu simulation outputs almost exactly agreed with cpu outputs mean and maximum relative errors for discharges were 4 18 10 5 and 2 46 10 4 respectively whereas for water heights were 2 51 10 5 and 1 53 10 4 however for the niger dataset simulation outputs presented lower accuracy as shown in fig 16 the quantitative error analysis resulted in larger absolute and relative errors mainly for discharge values reaching mean and maximum relative errors of 0 59 and 6 06 for discharges whereas 0 36 and 3 60 for water heights the niger dataset was more sensitive to differences from intermediate computations for the analysis of calibration results the accuracy was measured by comparing minimum values of the objective functions from each type of optimization with minimum values from the original nonoptimized implementation as samples processed in the calibration are randomly selected calibration results of the optimized implementations are expected to be different from the original results table 7 exhibits minimum values of the objective functions and relative errors for each type of optimization compared to the type v0 0 original nonoptimized implementation all values were computed as the average between results from the calibration executed on system01 and system02 except for the type v0 10 available only from system02 the largest relative errors occurred for the type v2 on cpu reaching 5 97 3 24 and 1 52 for the coefficients nse and nse log and systematic error err respectively these small errors indicate that calibration results were acceptable and validated the optimized implementations 5 conclusions cpu and gpu optimizations were investigated using the mgb hydrological model and mocom ua calibration method widely used by the hydrology community in brazil the calibration performance was improved with runtimes reduced from week long to few hour periods reaching speedups close to 20 on cpu with a proposed approach thus increasing the usefulness in hydrological applications vectorization intel intrinsics and thread parallelism openmp on cpu and also data parallelism cuda on gpu were employed for optimized simulation and calibration based on a double layer approach with experiments conducted on state of the art cpu gpu systems for real world datasets vectorization of the mgb model s key routines achieved significant speedups from approximately two thirds of vectorized code with intrinsic vector functions which maximize the use of vector resources cpu speedups of the fully optimized mgb model vectorization and thread parallelism were close to the theoretical peak from amdahl s law or even higher cache effects this performance level would not be reached by implicit vectorization via compiler flags the runtimes of the routines were reduced to less than 3 s with cuda on gpu but the mgb model s performance was hindered by overheads of data transfer and kernel launches too many routine calls besides the workloads of the available datasets are not gpu friendly not enough computations so cpu optimizations provided a more appropriate solution the scalability analysis indicated that the workloads of the available datasets were not enough for good scalability speedups of approximately 24 and 65 were achieved on cpu and gpu respectively for problem sizes of at least 8 larger than the original workloads the cpu roofline analysis confirmed that the proposed optimizations more effectively exploited the available cpu resources whereas the gpu roofline analysis showed that executing the mgb model on nvidia gpus resulted in different arithmetic intensity and performance relative to cpu in particular one routine achieved performance almost 8 higher than on cpu as cuda kernels were significantly faster than optimized cpu codes a quantitative analysis of the time series of discharges and water heights showed small relative errors with the largest error of 6 06 although in this case the mean relative error was only 0 59 comparison between minimum objective functions from the nonoptimized and optimized calibration implementations also resulted in acceptable accuracy with the largest relative error of 5 97 for one objective function overall this investigation proved that modern systems can be effectively exploited by carefully deploying parallelization techniques adjusting applications to the advanced capabilities of currently available hardware as possible routes for future work the roofline analysis can be extended to the mgb model s miniapp for deeper understanding of how optimizations exploit hardware resources for larger problem sizes the cpu and gpu optimizations can also be employed in another version of the mgb model that simulates hydrological processes for continental scale datasets the iph research group is currently designing datasets that cover the entire south america which can be approximately 8 larger than available datasets declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are grateful to walter collischonn from iph ufrgs for the availability of the mgb hydrological model and datasets this work was financially supported by the conselho nacional de desenvolvimento científico e tecnológico cnpq brazil grant number 140 687 2 016 5 and by portuguese national funds through fundação para a ciência e a tecnologia fct under projects uidb 50 021 2 020 and ptdc cci com 31 901 2 017 
25668,large scale hydrological models simulate watershed processes with applications in water resources climate change land use and forecast systems the quality of the simulations mainly depends on calibrating optimal sets of watershed parameters a time consuming task that highly demands computational resources from repeated simulations this work aims at performance optimizations on the mgb modelo de grandes bacias hydrological model and the mocom ua multi objective complex evolution calibration method for two watersheds the optimizations target state of the art cpu gpu systems exploiting techniques that include avx 512 vectorization and multi core cpu and many core gpu parallelisms significant speedups of up to 20 cpu were achieved for calibration while the scalability analysis indicated 24 cpu and 65 gpu for simulations with larger problem sizes the roofline analysis confirmed more effective use of the hardware resources and the quantitative accuracy evaluation of the optimized implementations reached maximum relative errors of approximately 6 for discharges and objective functions keywords high performance computing parallel processing vectorization roofline model hydrology models parameterization software availability software optimized mgb hydrological model description mgb code optimized for intel cpus with intel intrinsics and openmp and for nvidia gpus with cuda developer henrique r a freitas system requirements linux ifort pgf90 language fortran 90 size 1 2 mb zipped version availability the optimized mgb code can be freely downloaded from https github com henriquerenno mgb 1 introduction large scale hydrological models are extensively used to simulate watershed processes with applications in water resources climate change land use and forecast systems paiva et al 2011 those models simulate complex nonlinear hydrological processes and contain a set of nonphysical parameters i e not measurable from actual data acquired in field which are embedded in the model to function as conceptual representations that express watershed characteristics thus allowing the model simulated responses to satisfactorily adjust to the real world observed data and simulate the watershed behavior efstratiadis and koutsoyiannis 2010 for instance the watershed discharge several factors affect the efficiency of large scale hydrological models which highly demand computational resources for more accurate simulations of water flows those factors include the continuous advances in gis software and technologies the large availability of high resolution spatial temporal hydrometeorological data such as in situ measurements and remote sensing data the great extent of the regions processed in the simulations and the number of model parameters in calibration manual trial and error selection of the optimal sets of watershed parameters still predominates despite being a time consuming task even for experienced model users computer based automatic methods have recently been considered an effective and efficient option vrugt et al 2003 calibration methods typically employ objective functions which are computed from model simulations as measures to be minimized for the optimization of the sets of parameters implicitly solved as an inverse problem some methods focus on only one objective function such as the search based and evolutionary shuffled complex evolution sce ua method duan et al 1993 commonly employed using hydrological models however single objective methods do not provide solutions that can truly model the behavior of the observed data in hydrological simulations particularly in the extreme situations of wet and dry periods tian et al 2019 in contrast multi objective methods were employed in hydrological modeling to evaluate and capture various properties of the observed data with the critical advantage of estimating high quality parameters that can be generically used in subsequent simulations for more consistent and accurate simulated results vrugt et al 2003 even in extreme events of floods and droughts the multi objective complex evolution mocom ua method yapo et al 1998 has been successfully applied to a variety of environmental applications fagundes et al 2019 geritana et al 2014 islam and déry 2017 tatsumi 2016 tesemma et al 2015 being predominantly used for general purpose global multi objective optimization a variant of the mocom ua method was proposed as an epidemic genetic algorithm ega araújo et al 2013 employed using a hydrological model for small basins tucci 2005 the performance of calibration methods in hydrological applications is a key issue that has been continuously addressed optimization and parallel computing techniques were mainly applied to large scale hydrological models with the purpose of reducing the runtimes for estimating watershed parameters the efficiency of the sce ua method processing up to millions of objective function evaluations was improved by using parallel implementations of the xinanjiang model zhao 1992 designed for cpu openmp and for gpu cuda kan et al 2018 furthermore zhang et al 2016 proposed a double layer parallel approach for a genetic algorithm based method using the dyrim model wang et al 2007 with parallelism in the spatial decomposition of the watershed divided into independent units lower layer and also in simultaneous model simulations with different combinations of parameters upper layer a large number of calibration methods using the swat model arnold et al 1998 were also optimized mpi and python were employed to parallelize the multi objective amalgam calibration method with concurrent evaluations of simulations zhang et al 2013 the python multiprocessing module was used to develop a parallel multi objective calibration tool mameo for multi core processors zhang et al 2012 a framework based on the optimization program sufi2 for the swat cup calibration procedure automatically and transparently submit parallel jobs on multi core cpus rouholahnejad et al 2012 different hydrological models were optimized on either cpu or gpu aiming at performance improvements a simplified version of the lisflood fp model bates et al 2010 reached theoretical speedups with openmp and mpi neal et al 2010 but without processing 1d channel flow and dry checking functions and more recently was optimized to compute 2d water flows and depths on gpu using the cuda framework sarates 2015 openmp was also applied on a grid based fully sequential dependent hydrological model fsdhm to process simulation units based on a layered approach computing overland and channel flows in parallel liu et al 2014 mpi was used in the swat model with a scheme devised to decrease execution times by reducing the overheads of communication wu et al 2013 and in the tribs model vivoni et al 2011 to decompose watersheds formed by triangulated irregular networks tins as directed graphs from drainage network channels where independent sub basins were assigned to mpi processes exchanging data across boundaries the present work targets performance optimizations on the mgb hydrological model collischonn 2001 used for simulations and the mocom ua calibration method based on a double layer approach to execute the mgb model at each iteration the watersheds of the purus brazil and the niger africa rivers were chosen for simulation test cases but only the former for calibration the mgb model implements a numerical method referred to as inertial model that solves the saint venant equations to estimate water height and discharge stream flow along the longitudinal extension of the river in the watershed fan et al 2014 which is useful in many hydrological applications fagundes et al 2020 fleischmann et al 2017 2019 gorgoglione et al 2019 paiva et al 2011 the watershed parameters are used to set initial and boundary conditions for simulations and also to compute hydrometeorological data such as radiation evapotranspiration and water balance in soil those parameters are not directly used to compute water flows with the numerical method but instead to update the variables used as input to the inertial model the computational time requirements of the calibration phase were reduced by the optimizations proposed herein that exploit the ever growing parallel resources of computer systems the mgb model was thoroughly optimized for cpu on shared memory systems with vectorization and multi core parallelism as well as for gpu with many core parallelism implemented as a separate version whereas the mocom ua method was optimized for cpu with openmp modern cpus support vector extensions up to 512 bits current intel cpus for single instruction multiple data simd data parallel strategies compilers are not fully effective in automatic vectorization so hand tuned explicitly vectorized codes execute faster than the auto vectorized equivalents however explicit vectorization requires careful and detailed code analysis low level instructions and more development time mitra et al 2013 the additional availability of multiple cores in cpus ralston 2008 also enables thread parallel strategies in shared memory environments the workload is distributed to multiple cpu cores identified as threads and processed in parallel the openmp standard dagum and menon 1998 executes applications with thread level parallelism via compiler directives that dynamically activate deactivate parallel threads many core gpus provide up to thousands of parallel threads for data parallel strategies with numerical intensive workloads rupp 2016 and are designed with high bandwidth memories porting codes to gpus is usually done with frameworks such as cuda pgi 2020 opencl tompson and schlachter 2012 and openacc wienke et al 2012 openacc is based on compiler directives similar to openmp providing portable codes for different gpu architectures however in most cases the highest performance is achieved with the cuda framework although it requires more effort and development time fully exploiting the hardware resources not only decrease the overall execution times of the mgb model and improve the performance of the calibration but also provide more understanding about the mgb model s computational requirements building efficient and portable applications requires performance analysis tools to help developers exploit the hardware potentials the cache aware roofline model carm ilic et al 2013 lopes 2016 marques et al 2020 identifies the factors that limit an application s performance flops s and visually represents the compute and memory behavior of an application system pair as guidelines for optimizations according to the application s arithmetic intensity flops byte the roofline characterization of the mgb model shows how the hardware resources of the employed architectures are more effectively exploited with the proposed optimizations and allows to evaluate the achieved optimization levels revealing that effective use of the capabilities of each system may improve performance the mixing of explicit vectorization and directive based parallelization proposed as a set of advanced optimizations to exploit most of the capacity of vector resources and shared memory parallelism offers more opportunities for improving the performance of hydrological models with similar characteristics to the authors knowledge the optimizations proposed herein are the first that employ explicit vector instructions with intel intrinsics for hydrological models together with the roofline model to analyze performance improvements a scalability analysis using a miniapp a reliable proxy of the mgb model developed to process larger problem sizes shows the performance gains achievable if large datasets are available miniapps are testbeds that should accurately model the performance bottleneck of full applications stone et al 2012 and also be able to scale the problem size while matching the performance the prediction and evaluation of how the full application is affected by code changes are issues investigated using miniapps lin et al 2015 murai et al 2017 2 methods this section introduces the mgb hydrological model and the mocom ua calibration method which are employed to find the optimal sets of watershed parameters details about the optimizations on the calibration and simulation parts are described by following the double layer approach where mgb model s simulations are simultaneously executed on different cpu cores upper layer and each simulation is also parallelized lower layer 2 1 mgb hydrological model the mgb modelo de grandes bacias hydrological model collischonn 2001 is a fortran code developed at iph ufrgs instituto de pesquisas hidráulicas universidade federal do rio grande do sul in brazil focusing on improving the knowledge of hydrological processes in large scale watersheds simulations of the mgb model generate 1d propagation of water flows in rivers as illustrated in fig 1 which exhibits the elevation z water height h water level y discharge q and length δx of each segment of the river stretch discretization from the numerical scheme detailed in subsection 2 1 1 the mgb model simulates the hydrological cycle computing soil water and energy budgets evapotranspiration interception surface subsurface and groundwater flows in either daily or hourly time steps for the forecast of river discharge analysis of extreme events floods and droughts estimation of the effects of soil and vegetation cover on climate water quality and sediment transport paiva et al 2011 the spatial discretization comprises three hydrological units catchments sub basins and hydrological response units hrus catchments and sub basins are surface regions that contribute water to drainage network segments and to outlet points respectively whereas hrus are regions that combine land use cover soil and slope maps based on user defined thresholds kalcic et al 2015 fig 2 illustrates catchments smallest spatial units delineated for a drainage network of the purus watershed defined from a threshold of accumulated flows equal to 10 000 m2 and generated with the terrahidro computational platform jardim 2017 rosim 2008 the mgb model has been effectively employed in a diverse and large number of applications such as sediment modeling for estimation of soil erosion sediment transport and deposition fagundes et al 2020 modeling of reservoirs as an internal boundary condition for simulation of hydrodynamic processes and their interaction with upstream and downstream floodplains fleischmann et al 2019 analysis of water conflicts in transboundary watersheds to improve the sustainability of water allocation gorgoglione et al 2019 representation of the interactions between hydrology and hydrodynamics mainly infiltration from floodplains into soil to improve model estimates fleischmann et al 2017 and hydrological and hydrodynamic modeling to simulate fluvial processes of wave delay and attenuation backwater effects flood inundation and its effects on flood waves paiva et al 2011 2 1 1 water flow equations and numerical scheme the mgb model simulates the spatial and temporal distribution of water flows in a watershed measured by water height and discharge through an implementation of the inertial simplification fan et al 2014 of the saint venant equations cunge et al 1980 these equations are referred to as continuity 1 and momentum 2 equations where h is water height q is discharge y h z is water level given by the sum of water height h and elevation z g is the acceleration of gravity n is the manning coefficient t is time and x is longitudinal distance 1 h t q x 0 2 q t g h y x g q q n 2 h 7 3 0 equations 1 and 2 are numerically solved with forward in time and centered in space finite difference approximations that integrate through time with initial conditions from soil moisture water volumes and water flows in reservoirs and boundary conditions from accumulated flows muskingum cunge and reference discharges the numerical solution is a scheme of wave propagation commonly employed in rainfall runoff hydrological models the explicit numerical scheme is shown in equations 3 7 where c is the number of catchments of the watershed and i and k are spatial and temporal indexes respectively 3 δ t m i n α δ x g h i k i 1 2 c 4 h i 1 2 k m a x y i k y i 1 k m a x z i z i 1 5 q i 1 2 k 1 q i 1 2 k g δ t h i 1 2 k y i 1 k y i k δ x 1 g δ t q i 1 2 k n 2 h i 1 2 k 7 3 6 h i k 1 h i k δ t δ x q i 1 2 k 1 q i 1 2 k 1 7 y i k 1 z i h i k 1 the time step of each catchment i from equation 3 requires 0 α 1 to avoid numerical instability bates et al 2010 where α 0 7 in the mgb model fan et al 2014 the minimum time step δt corresponds to the maximum water height h i k among all catchments the water height h i 1 2 k at position i 1 2 border is the difference between the maximum water levels y and elevations z at positions i and i 1 as in equation 4 the outflow discharge q i 1 2 k 1 of each catchment for the next time step k 1 is found through equation 5 from the previous discharge q i 1 2 k water height h i 1 2 k and water levels y i 1 k and y i k the water height h i k 1 and level y i k 1 at position i center are obtained from the updated discharges q i 1 2 k 1 and q i 1 2 k 1 as in equation 6 and from the water height h i k 1 as in equation 7 the numerical scheme of equations 3 7 executes m times for all c catchments where m is a constant that depends on the input data so that the computational complexity is o m c the amount of computations required to find the numerical solution at each time step is mostly determined by the number of catchments c as this value can theoretically increase without limits 2 1 2 mgb code structure the mgb model s numerical scheme inertial model is divided into three routines timestep ste discharge dis and continuity con these routines are successively called for each minimum time step δt that satisfies the inertial model s stability and are accumulated up to the mgb model s time step each routine is executed for all iterations of the mgb model a fixed number of steps from the input data processing each catchment to compute the minimum time step in the ste routine equation 3 water height and discharge in the dis routine equations 4 and 5 and water height and level in the con routine equations 6 and 7 as well as water volume and cross sectional area besides the saint venant equations the mgb model computes hydrometeorogical data such as radiation evapotranspiration penman monteith equations shuttleworth 1993 and vertical water balance in soil collischonn 2001 code 1 structure of the mgb model image 1 code 1 provides the mgb model s structure it includes the routines that load rainfall and climate data load rainfall climate data compute hydrometeorogical data calc radiation calc evapotranspiration and calc water balance and execute the inertial model inertial model 2 1 3 profiling of serial execution based on serial executions of the original mgb code compiled with default optimization flags the routines of the inertial model were identified as the most time consuming routines from reports of the gnu s gprof profiling tool those routines accounted for the highest percentage of execution time from either thousands or millions of calls for different simulations taking on average 92 96 of execution time as the main execution bottleneck of the mgb model table 1 provides the profiling details including the average runtime and percentage of execution time of each routine relative to the mgb model s execution time and also the number of calls these values were obtained from simulations using different datasets 2 2 mocom ua calibration method the mocom ua method formulates and implicitly solves the estimation of the sets of model parameters as an inverse problem i e successively generating candidate solutions to minimize multiple objective functions which express the difference between simulated and measured data in this work the calibration employs three objective functions sorribas et al 2013 a the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 computed from observed discharge average observed discharge and simulated discharge b the nse computed from the natural logarithm of the discharges nse log and c the systematic error both nse and nse log are within the range 1 where 1 means the best fit and the systematic error is given as a percentage the mocom ua method is based on determinism to converge to the local global optimal solutions simplex search probability to better explore the search space escaping from local minima random search evolution to improve the solutions population evolution and nonuniqueness of solutions to provide multiple optimal solutions pareto front a population with a fixed number of candidate solutions or independent samples is generated in a multi dimensional search space where each dimension represents a particular model parameter each solution or sample corresponds to a point in the multi dimensional space with coordinates given by the parameter values which are randomly computed from restricted ranges of the domain for the initial iteration each combination of parameters is used in simulations and the objective function values are compared to rank all samples according to the pareto ranking in this ranking a sample with all objective function values greater than the corresponding ones from another sample is considered inferior and identified as dominated otherwise it is nondominated all nondominated samples are ranked 1 then the remaining dominated samples are checked and the new nondominated ones are ranked 2 and so on in the end the worst ranked samples are ranked rmax maximum rank the optimal set of samples sets of parameters is located in the region of the objective space called pareto front which consists of all solutions equally acceptable as optimal solutions in the pareto ranking lower ranked samples are considered superior and located closer to the pareto front the worst ranked samples are improved with simplex operations reflection and contraction using other samples randomly selected according to a probability distribution from all ranks where samples with smaller larger ranks have higher lower probability to be selected the simplex operations require r more samples number of parameters to generate a new sample from the worst ranked sample s w and the centroid s c of the r samples firstly if the new sample s ref from the reflection operation equation 8 is nondominated relative to the r selected samples it replaces s w otherwise the new sample s con from the contraction operation equation 9 replaces it after all worst ranked samples are processed the pareto ranking is recalculated and this repeats until all ranks are equal to 1 8 s r e f 2 s c s w 9 s c o n 0 5 s c s w 3 performance optimization techniques the optimizations selected to boost the performance of the mocom ua method and of the mgb model consist in data parallel and thread parallel strategies vectorization and multi core cpu and many core gpu parallelisms the upper layer employs multi core cpu parallelism openmp to process mgb model simulations in parallel for s samples parameter sets θ i i 1 2 s the lower layer implements different optimizations intrinsics openmp cuda for parallel processing of catchments fig 3 illustrates the parallelization scheme 3 1 simulation mgb model the numerical scheme implemented in the routines of the inertial model processes all catchments in a domain that can be divided into independent subdomains which are updated using only previous water flows this differs from implicit numerical schemes that would require a system of equations to be solved at each time step hoffman 2001 each routine includes a single loop that iterates over the catchments with successive and independent memory accesses defining codes highly suitable for different parallelization strategies 3 1 1 optimizations for cpu the multi core cpu optimizations proposed herein employ explicit vectorization via interoperability between the intel intrinsics library available in c c intel 2020b and the fortran mgb model s code and openmp parallelization openmp provides portable optimized codes but the vectorization requires specific cpu hardware resources avx 512 vector extensions these optimizations are detailed for the ste routine that processes catchments to compute the minimum time step δt openmp threads are created outside the routine in a parallel region for simultaneous independent calls to the inertial model code 2 shows the ste routine optimized with vectorization and thread parallelism the input variables are the height h of the water flow hflow length δx of the river stretch lriver constants α alpha and of gravity g g time step dt minimum time step of each thread minlocaldt thread number th number of threads nth and catchments nc the m512d variables are vectors that store multiple data into the cpu registers for data parallel vector processing firstly the minimum time step minlocaldt of each thread th and the corresponding vectors mindt are initialized with the mgb model s time step largest value from argument dt after the initialization the loop of catchments is processed by the openmp threads and each thread cooperates by executing a number of iterations multiple of 8 nth whereas the remaining iterations are sequentially processed outside the routine the stride of 8 means that 512 bit cpu registers and vector instructions load mul div sqrt min simultaneously process eight 64 bit double precision data computing time steps for catchments in parallel the openmp clause for static scheduling uniformly distributes the workload which is well balanced in the loop iterations that execute similar instructions code 2 ste routine optimized for cpu with intrinsics and openmp image 2 whenever each thread th exits the loop a minimum reduction vector operation reduce finds the minimum time step to be stored into minlocaldt th finally all threads wait at a synchronization barrier to ensure all local minimum time steps are available so the master thread th 0 can update the global minimum time step dt this is the only routine that requires a synchronization mechanism to ensure thread safety and correct behavior with multiple threads the optimizations on the dis and con routines are more complex the dis routine independently updates the discharge q of each catchment the input variables are the elevation z of the river bottom water height h to compute water level y length δx and width of the river stretch manning coefficient n current discharge q minimum time step δt ste routine and reference to the downstream catchment i e the single catchment where water flows the dis routine executes arithmetic and logical vector instructions the latter referred to as mask vector instructions used for logical conditions to concurrently check whether the downstream water flows are present or not these instructions provide vector masks that indicate which catchments update the discharge q i e only where the downstream water flow is present the con routine uses the minimum time step δt ste routine and the discharge q dis routine catchments are also independently processed using as input variables the discharges from the upstream catchments evapotranspiration rainfall length δx of the river stretch mgb model s time step as well as the range positions of precomputed water volumes cross sectional areas and elevations that are stored in nondecreasing order as 2d tables for each catchment this routine updates the water volume cross sectional area water height h and water level y for the next iteration of the inertial model the sum of the upstream discharges of each catchment is initially computed with a sum reduction vector operation by collecting and adding up the discharges from the upstream catchments which were contiguously arranged in memory in the optimized routine for more efficient memory accesses by the vector instructions new water volumes are computed using the upstream discharges and each catchment searches its table of water volumes for the new value the table search retrieves precomputed water volumes cross sectional areas and elevations from ranges with the new value where gather vector instructions collect the values from memory positions of the ranges volume fractions are computed from the new water volumes to interpolate the cross sectional area and water level the boundaries of the ranges are checked with vector masks and the table search shifts either left or right to locate ranges with the new water volumes the search stops when all ranges are found updating the current range positions fig 4 illustrates the relation between catchments and water volume table table rows store precomputed water volumes v for each catchment 1 i c and the eight positions stored into the vectors vpos0 and vpos1 specify the ranges with the current water volumes v i j 1 w i v i j v i 1 j 1 w i 1 v i 1 j 2 v i 2 j 6 w i 2 v i 2 j 7 and so on eight values of water volumes v located in the range vpos0 vpos1 are collected from memory positions using a gather vector instruction gather and compared using a logical vector instruction masks with the volumes w of the catchments whenever the water volumes w are updated the current positions vpos0 independently shift either left or right table columns to the new range for each catchment vector optimizations are fully described in freitas 2021 freitas et al 2020 3 1 2 optimizations for gpu the ste dis and con routines were executed on gpu as cuda kernels with the cuda fortran interface nvidia 2020a supported by the pgi pgf90 compiler pgi 2020 the cuda framework was chosen because we previously applied directive based gpu programming standards openacc on the mgb model freitas and mendes 2019 but that particular optimization decreased the performance mainly caused by overheads associated to the openacc directives that increased runtimes from millions of routine calls not amortized by thousands of loop iterations catchments i e computations were not gpu intensive current nvidia gpus provide thousands of parallel threads which is enough to cover all loop iterations in the routines of the inertial model for the available datasets the cuda kernels assign each thread to one single catchment increasing the degree of parallelism in the cuda implementation the variables of the inertial model were globally declared as device variables in a fortran module with data transferred between host cpu and device gpu memories code 3 illustrates the fortran module with the cuda version of the ste routine which required an explicit minimum reduction operation for the minimum time step δt cuda does not include a minimum reduction operation differently from intel intrinsics library function and openmp directive clause firstly each thread is identified from cuda structures such as the size blockdim x and index blockidx x of the cuda blocks and the thread index inside the cuda block threadidx x for the cuda grid x dimension to be stored into the variable i this variable is an index to each catchment and checked if it is valid i e not larger than the number of catchments nc all valid threads compute time steps in parallel in the ste cuda routine independently storing the values into the localdt array the dt cuda routine executes the minimum reduction operation in one single cuda block for thread synchronization processing only half of the array to simultaneously update the minimum time step between positions i first half and i pow second half where pow2nc is the smallest power of two relative to nc the minimum time steps are kept in the first half of the array and all threads are synchronized cuda function syncthreads this repeats by halving pow at each iteration of the reduction and in the end the global minimum time step is stored into the first position of the localdt array finally localdt 1 updates the global minimum time step dt for the dis and con cuda kernels code 3 ste routine optimized for gpu with cuda fortran image 3 3 2 calibration mocom ua method the mgb model s parameters selected for calibration are shown in table 2 which includes soil parameters that largely affect hydrological responses in wet and dry periods collischonn 2001 the ranges specify multipliers for the parameters relative to the initial value the calibration implementation was optimized on steps that simulate the mgb model with the current set of samples and that evolve the population of samples with simplex operations the former step computes simulated discharges to evaluate the objective functions for each set of parameters in order to update the pareto ranking of all samples the latter step executes simulations for worst ranked samples modified with simplex operations all samples are independent from each other and can be processed in parallel for both steps which highly demand computational resources to execute full mgb model s simulations for the parallel executions of the mgb model to properly work each sample requires separate executions of four routines successively called to a set parameter values b set initial and boundary conditions c simulate the mgb model and d compute objective functions each sample is assigned a new rank only after these routines have been completely executed for all samples code 4 shows the calibration implementation parallelized with openmp for simultaneous mgb model s executions where the input argument s is the number of samples the initial random parameters routine allocates the s samples in the samples population array storing initial random parameter values for each sample after the initialization all samples are independently processed in a loop with the aforementioned routines upper layer simultaneously executing the set parameters set conditions mgb model and objective functions routines for the initial pareto ranking of all samples the loop is parallelized with an openmp directive that assigns each loop iteration sample to one single thread where the number of threads is set to ncores cpu cores the schedule static 1 clause cyclically assigns one loop iteration at a time next the calibration starts and repeats while there are samples to be improved i e there is at least one sample with pareto rank greater than one rmax 1 firstly the pareto ranking routine assigns ranks to the samples and each rank associates a probability to the sample in the samples probability routine the nrmax worst ranked samples are selected and the reflection and contraction simplex operations are executed in the simplex ops routine generating two new samples for each worst ranked sample but only one updates the current population by replacing it the loop that updates the population of samples is also parallelized with openmp the reflection sample is attempted first and if it is a nondominated sample it replaces the current worst ranked sample otherwise the contraction sample replaces it after all worst ranked samples are processed the condition of the calibration loop is rechecked to verify whether it continues or not in the calibration the mgb model executes on either cpu or gpu lower layer whereas all other routines execute only on cpu in addition the random probability generated to select samples for the simplex operations determines how fast worst ranked samples move towards the pareto front so execution times of calibration runs can randomly vary even with the same initial set of samples code 4 calibration procedure of the mocom ua method image 4 the mgb model and the mocom ua method implementations were thoroughly adapted so that simultaneous simulations avoided data race conditions variables used in the calibration needed an extra dimension allocated as the number of parallel threads ncores independently indexed by each thread th in code 4 whereas in other routines such as radiation evapotranspiration and water balance in soil had to be locally defined as private to each thread additionally another implementation considers the parallel processing of more samples from the population not only the worst ranked samples this approach tries to further increase the performance of the current implementation by also selecting samples with pareto rank just below the worst rank rmax i e rmax 1 rmax 2 and so on the purpose of this strategy is to improve more bad samples at the same iteration while still maintaining the calibration quality i e objective functions are similar to the original calibration this proposed approach has the advantage of more efficiently exploiting the hardware resources by executing the calibration loops with the number of openmp threads equal to the available cpu cores the degree of parallelism increases as more samples can be processed in parallel and more parallel threads are simultaneously working reducing core resources that remain idle 3 3 computational testbed the computational testbed employed to conduct the experiments consists of two multi core cpu gpu systems with linux centos 7 5 and intel 19 0 5 nvidia 10 2 and pgi 19 10 software development tools the hardware specification of each system is detailed in table 3 including the processor and memory capabilities of the cpu and gpu architectures the floating point double precision cpu performance and dram bandwidth were experimentally obtained with the intel advisor performance analysis tool intel 2020a both cpu architectures are based on intel s skylake that supports avx 512 vector instructions 512 bits 3 4 input datasets two datasets were used as input to the mgb model providing hydrometeorogical data from the purus and niger rivers the purus river fig 5 a is one of the main tributaries of the amazon river in brazil with drainage area of 370 000 km2 and average discharge of 11 000 m3 s paiva et al 2011 its region is mostly covered by tropical rainforests which provide high annual rainfall rates and the watershed topography ranges from 10 m at the outlet to 581 m at the highest headwater measured from the shuttle radar topography mission srtm 30 m topographic data farr et al 2007 the niger river fig 5b covers the guinea highlands and sahel desert as the largest river in west africa with drainage area of 657 000 km2 fleischmann et al 2017 and average discharge of 900 m3 s mahé et al 2009 the geographical region presents semi arid climate with large seasonal variation in rainfall and river flow zwarts et al 2005 where approximately 40 of the water available from headwaters are lost due to either evaporation or infiltration and also because of the small number of tributaries along the watercourse extension the construction of dams and irrigation systems provides solutions of hydroelectric structures to optimize the scarce water availability the datasets were provided by iph ufrgs as worst case datasets for these regions requiring high computational effort to be processed each dataset contains discrete data for hydrological units of catchments sub basins and hrus spatial for each time step temporal two datasets were used for purus simulation and calibration whereas only one for niger simulation the number of catchments sub basins hrus and time steps are in table 4 4 computational performance results this section presents the runtimes and corresponding speedups ratio between sequential and parallel runtimes of the optimized implementations as measures for performance analysis scalability issues of the simulations are investigated as well as the cpu and gpu roofline characterizations of the mgb model the numerical accuracy of the hydrological results is quantitatively evaluated 4 1 performance on multi core cpus and many core gpus this subsection presents the performance of simulation mgb model and calibration mocom ua method results include both the cpu and gpu performance on each computer system for the available input datasets 4 1 1 simulation the experiments conducted on multi core cpus executed the original nonoptimized mgb model and the optimized version with avx 512 vector instructions and openmp directives the number of openmp threads ranges from one to the total cpu cores available on each multi core cpu disregarding hyper threading as it showed only marginal performance improvements for the performance analysis of the fully optimized mgb model it is also important to evaluate the performance without vectorization i e using only thread parallelism with openmp regarding portability issues as the vectorized code requires avx 512 vector extensions the nonvectorized code can serve as a basis to verify how much the performance increases with additional vectorization fig 6 illustrates the differences in runtime and speedup for the vectorized and nonvectorized mgb model executed with 1 2 4 and 8 openmp threads on system01 for the purus and niger datasets the vectorized mgb model was faster than the nonvectorized version achieving speedup differences of up to 1 86 for 8 openmp threads about 42 higher moreover the runtimes of the nonvectorized version executed with only one openmp thread were lower relative to the original nonoptimized version this probably occurred because openmp directives trigger additional compiler optimizations fig 7 exhibits the runtimes and speedups achieved by the fully optimized mgb model on the multi core cpus of each computer system for the simulation input datasets the single threaded vectorized mgb model reached the highest speedup of 2 71 for the purus dataset on system01 which indicates that the overall performance significantly improved with only vectorization the lowest runtimes for the single threaded version were achieved on system02 due to the more advanced cpu and higher bandwidth memory the performance analysis reported by the intel vtune profiler intel 2020a indicated that the performance of the single threaded vectorized mgb model was limited by how many floating point instructions of the original code were vectorized which did not exceed 65 3 measurements of cpu cycles spent on long latency divisions and on pipeline slots stalled by load store instructions reached up to 27 9 and 25 7 respectively which also hindered performance vectorization and multi core parallelism combined further reduced the runtimes of the mgb model increasing the corresponding speedups although reaching a plateau with only slight changes independently of the number of threads this occurred because the workload catchments processed by the routines of the inertial model consists in thousands of loop iterations not enough for good scalability as each thread is assigned few loop iterations this effect can be noticed by comparing the speedups of the purus and niger cases as the workload of the niger dataset is approximately 2 larger relative to the purus dataset the hardware resources are more efficiently exploited in the former case resulting in higher speedups even though the parallel runtimes across systems were similar the speedups on system01 were higher as the execution of the nonoptimized mgb model on this system was slower so the optimization effects were more pronounced than on system02 the performance of a parallel program can also be theoretically predicted with measures that indicate how much performance improvement can be achieved by increasing the number of threads the amdahl s law quinn 2004 can provide an upper bound on the speedup achievable with the openmp threads mainly depending on the fraction of operations that must be sequentially executed from amdahl s law the highest speedups achievable by the mgb model on system01 and system02 assuming full parallelization of the inertial model are 5 75 and 5 88 respectively these theoretical speedups were computed from the average runtime percentage of the inertial model subsection 2 1 3 relative to the full execution of the original nonoptimized mgb model across the purus and niger datasets on each system for the total cores available the highest speedups of the optimized mgb model on system01 were 5 32 purus and 6 27 niger both with 8 threads whereas on system02 were 4 26 purus and 4 89 niger the former with 4 threads and the latter with 8 threads therefore significant speedups were achieved with the optimizations proposed herein and the case of speedup higher than the theoretical peak niger dataset on system01 probably occurred due to cache effects as more l2 caches faster accesses were available with multi core processing besides executions of the mgb model on multi core cpus the cuda implementations for many core gpus achieved higher speedups for the inertial model computed from serial cpu runtimes and parallel gpu runtimes as gpus work with sets of 32 threads warps the cuda kernels were executed using 256 threads per block multiple of 32 that resulted in the highest speedups for the mgb model fig 8 depicts a comparison between the optimal cpu speedups and the gpu speedups relative to the original cpu runtimes where the inertial model s performance on gpu was higher than on cpu in all cases the runtimes of the ste dis and con routines were significantly reduced with gpu executions of the cuda kernels the average runtimes of the routines for the purus and niger datasets from both gpus were 2 33 s and 1 27 s respectively explaining the noticeably high speedups of the dis and con routines that execute more computations as highlighted for the niger dataset on system01 the performance of the ste and con routines on cpu and gpu was hindered by insufficient floating point operations ste and by the table search that requires many memory movements con not reaching speedups as high as the dis routine which more effectively exploited hardware resources although the runtimes of the routines were considerably small the mgb model s speedups on both the pascal gpu of system01 gpu01 and volta gpu of system02 gpu02 were lower than the speedups on the multi core cpus cpu01 and cpu02 the main cause was the overheads of data transfer between host cpu and device gpu memories and of kernel launches by collecting performance data with the cuda profiling tool the percentage of the mgb model s execution time on gpu spent on data transfer and kernel launches ranged from 68 90 to 71 88 for purus and from 53 79 to 54 57 for niger table 5 the purus dataset required more calls to the cuda api function of kernel launches 4 272 300 than the niger dataset 2 791 500 the mgb model s performance could increase with larger workloads as the available datasets are not gpu friendly so developing more efficient cpu codes is more advantageous than the gpu solution even though the gpus provided enough threads to assign each loop iteration to one single gpu thread significantly reducing the inertial model s runtimes the mgb model did not scale as the overheads increased runtimes decreasing the mgb model s performance 4 1 2 calibration the calibration considers a particular purus dataset that additionally employs a time series of observed discharges which are essential data to compute objective functions to calibrate the watershed parameters the observed discharges are from the municipality of canutama in the amazonas state brazil located at the outlet of the purus watershed all discharge values were collected with a fluvial gauge controlled by the brazilian national water agency or ana agência nacional de águas for the period from january 2nd 2000 to january 7th 2015 the calibration was executed with these optimizations a either vectorization and multi core cpu parallelism or many core gpu parallelism to process catchments simulation and b multi core cpu parallelism to process samples calibration the performance analysis considered three types of execution depending on the selected optimizations a nonoptimized simulation and calibration original code b optimized simulation and c optimized simulation and calibration besides setting the number of samples s 100 as the only input argument the calibration was also configured to limit the evolution of samples to 400 generations this value was obtained from fully executing the nonoptimized original calibration on system01 422 and system02 434 fig 9 exhibits the runtimes and speedups for all execution types identified by codes indicating whether or not the simulation and calibration were optimized the types v0 x for cpu specify nonoptimized calibration v0 and x threads in simulation thus x 0 is the original nonoptimized code and x 1 2 4 8 10 is the simulation optimized with vectorization and x threads the types v1 and v2 are the optimized calibration that assigned samples to cpu threads by selecting from either worst ranked samples v1 or also bad samples with pareto rank greater than one in the same generation v2 in both cases simulations were executed with only vectorization to avoid overheads and performance degradation with nested parallel regions when calling the ste dis and con routines results for the type v2 were obtained from calibration executions that processed bad samples of pareto rank up to rmax 5 achieving the highest speedups the types v0 v1 and v2 for gpu also assigned samples to cpu threads in calibration although simulations were executed on gpu the calibration is a really time consuming task the original nonoptimized calibration v0 0 took more than seven and eight days to reach the limit of 400 generations when executed uninterruptedly on system02 and system01 respectively whenever the dataset is updated for a recalibration computational resources are used for long periods of time directly affecting the calibration usefulness the calibration with optimizations only in simulation v0 x showed considerable performance gains the smallest runtime was 132 073 s 1 d 12 h 41 min 13 s on system01 with 8 threads v0 8 reaching the speedup of 5 78 the use of 10 threads on system02 v0 10 had a small effect on decreasing runtimes as workloads are not large enough to exploit more cpu resources even though the types v0 x reached significant speedups the highest performance levels were achieved with the type v2 on both systems therefore selecting and processing more samples for each generation not only the worst ranked samples rmax as originally established in the mocom ua method resulted in additional performance gains in this case the smallest runtime was 33 738 s 9 h 22 min 18 s on system02 reaching the highest speedup of 19 89 gpu results were similar for all types of optimization and did not exhibit performance gains when compared to the best cpu results gpu runtimes and speedups were comparable to the cpu results obtained from the type v0 2 this behavior was expected as the previous analysis about the performance of the simulations executed on gpu showed that overheads of data transfer and kernel launches degraded performance this effect was more pronounced in calibration which required a large number of simulations as indicated in table 6 4 2 scalability with problem size the available datasets used to simulate real world hydrological scenarios with the mgb model did not provide workloads catchments or loop iterations of sizes large enough to reach optimal scalability even by exploiting most hardware resources of the multi core cpus and many core gpus additional insights on the mgb model s parallel speedups were obtained from a scalability analysis conducted with larger datasets which were designed from replication of parts of the original data as input to a mgb model s miniapp that reproduces the same computations and optimizations while processing workloads of varying sizes more flexible settings larger datasets allow to evaluate how the mgb model s performance scales with problem size the mgb model s miniapp also includes the key ste dis and con routines of the inertial model however all data explicitly computed at each time step in the routines for radiation evapotranspiration and water balance was previously stored into separate files with the original model to be read by the miniapp this avoids full mgb model s simulations so only the critical ste dis and con routines process varying workloads replicated in the loop of each routine as a multiple of the original size fig 10 in logarithmic scale illustrates the runtimes and speedups of the mgb model s miniapp relative to the problem size from executions on cpu and gpu for both datasets the problem sizes range from 1 to 32 the original size of each dataset cpu results are from simulations optimized with vectorization and multi core parallelism using the maximum cores available on each system i e 8 cores on system01 and 10 cores on system02 whereas gpu results are from simulations optimized with cuda cpu speedups increased up to the problem size of 8 for both datasets reaching the highest value of 23 56 for the niger dataset on system01 most speedups seemed not to improve with problem sizes larger than 8 as larger datasets increase accesses to the slower dram and the memory latency is not totally hidden even with more l2 caches available from the cpu cores however the lowest speedup of 8 07 for the purus dataset on system01 problem size of 32 was still a satisfactory result achieved using 8 cpu threads speedups on system02 were higher with the largest problem size because this system provides more cpu cores larger l3 cache and higher bandwidth memories gpu speedups behave differently and kept increasing with the problem size on both gpus although at different rates the gap between speedups from the pascal gpu system01 and volta gpu system02 was largest for the niger dataset with the problem size of 32 where the speedup on the pascal gpu was 22 90 and on the volta gpu was 65 20 2 85 higher this was expected as both the performance and dram bandwidth of the volta gpu are higher relative to the pascal gpu by comparing the cpu and gpu speedups as in fig 11 smaller problem sizes present higher cpu speedups mainly when compared to the pascal gpu this occurs because the overheads of data transfer and kernel launches associated with the mgb model s execution on gpu for small datasets degrade performance furthermore the maximum performance of both cpus 919 5 gflops s is higher than the performance of the pascal gpu which has more impact in small cases than the slower cpu dram bandwidth 68 5 gb s however for larger problem sizes the performance on gpu was higher than on cpu as shown for the problem size of 16 on the volta gpu the scalability analysis of the mgb model s miniapp indicates that if large datasets are available the mgb model s performance will be higher on gpu than on cpu 4 3 cpu and gpu roofline analysis the roofline model provided a visual analysis of the mgb model s performance behavior the cpu roofline characterization includes both the original nonoptimized and optimized versions where only the key routines of the inertial model were analyzed flops and bytes processed by each routine were collected with the intel advisor performance analysis tool intel 2020a fig 12 exhibits the cpu roofline characterization of the ste dis and con routines for each dataset on both systems in all cases the optimized routines moved upwards indicating performance increase in relation to the nonoptimized versions and confirming that vectorization and multi core parallelism more effectively exploited the hardware resources these roofline characterizations of the optimized routines include the performance using all cores on each system which resulted in the highest performance on each roofline characterization the nonoptimized routines were located under the dram roof higher memory levels l1 l2 and l3 caches were not frequently accessed so the routines required more accesses to the slower dram affecting the performance however as all original routines were also located under compute roofs the intrinsics openmp optimizations increased the performance moving routines closer to higher roofs vectorization in the optimized codes decreased the amount of data loaded from memory as more data is kept into the cpu registers the fastest units of data storage in the cpu hardware this optimization approximately doubled the arithmetic intensity of the dis and con routines and together with thread level parallelism moved their performance from under the dram roof closer to the l2 and l3 cache roofs mainly due to the larger availability of l2 caches from multiple cores in contrast the arithmetic intensity of the ste routine did not increase because the bytes transferred through the memory hierarchy became larger in the optimized code possibly due to the thread synchronization of the minimum reduction operation in summary the arithmetic intensity changes were ste 0 58 0 44 dis 0 05 0 13 and con 0 06 0 11 besides cpu roofline characterizations the routines of the inertial model were also characterized with gpu roofline models from the nvidia gpus pascal and volta flops and bytes for the gpu roofline characterization were collected with the cuda component of the performance api papi dongarra et al 2001 a widely used interface that can access hardware performance counters from cpu and gpu architectures the performance counters selected to collect flops include double precision single precision and special mathematical functions floating point operations whereas the ones selected to compute transferred bytes include load and store memory transactions per memory request fig 13 exhibits the gpu roofline characterization of the optimized ste dis and con routines the arithmetic intensity of each routine on gpu ste 1 10 dis 1 31 and con 0 15 is different from the corresponding one on cpu because cpu and gpu architectures work differently the performance levels were nearly the same on both gpus for each dataset showing that performance improvements achieved with cuda were independent of processor and memory gpu capabilities almost all cases from routine dataset pairs remained under the dram roof global memory with the exception of the con routine that more often executes the table search and keeps data in cache the niger dataset requires more floating point operations thus resulting in higher performance performance results obtained from intel advisor cpu and from papi gpu are shown in fig 14 the mgb model s gpu performance was not computed because the performance counters were collected only for the routines executed on gpu as cuda kernels not the full mgb model the gpu performance was higher for the niger dataset more floating point operations and the only case with similar cpu and gpu performance levels was the ste routine for the purus dataset regardless of the system the highlighted values show the biggest cpu and gpu performance differences that occurred with the dis routine largest number of computations for the niger dataset more effectively exploiting gpu resources and reaching more than 400 gflops s the cpu performance for the purus dataset exhibited the same pattern on both systems but the larger niger dataset showed higher cpu performance on system02 this system provides larger l3 cache and more cpu cores with faster caches reducing dram accesses for larger workloads that affect performance 4 4 accuracy of hydrological results the vectorization with intel intrinsics in the optimized cpu version of the mgb model demanded extensive tests to detect which vector instructions for low level compiler instructions precisely simulated the original mgb model numerical variations accumulated at each time step throughout simulations mainly due to how fortran treats single precision and double precision constants produced outputs differing in the order of either hundreds or thousands between the nonoptimized and optimized implementations affecting the accuracy nevertheless the optimized mgb model was carefully implemented to solve data type conversion issues and final simulation outputs from both nonoptimized and optimized cpu versions matched exactly in contrast the gpu mgb model s version optimized with cuda did not produce the same simulation outputs as the original nonoptimized cpu mgb model reference although results showed only minor variations the mathematical functions implemented in the nvidia cuda math library may differ from cpu implementations so results are not expected to exactly match for a given input nvidia 2020b although various sets of pgi compiler flags related to floating point operations were tested on gpu trying to simulate the intel compiler s arithmetic behavior on cpu all attempts were unsuccessful fig 15 illustrates differences between simulation outputs discharge and water height at the purus watershed s outlet obtained from the original nonoptimized cpu and optimized gpu versions of the mgb model the time series of simulated discharges and water heights were practically the same so gpu simulation outputs almost exactly agreed with cpu outputs mean and maximum relative errors for discharges were 4 18 10 5 and 2 46 10 4 respectively whereas for water heights were 2 51 10 5 and 1 53 10 4 however for the niger dataset simulation outputs presented lower accuracy as shown in fig 16 the quantitative error analysis resulted in larger absolute and relative errors mainly for discharge values reaching mean and maximum relative errors of 0 59 and 6 06 for discharges whereas 0 36 and 3 60 for water heights the niger dataset was more sensitive to differences from intermediate computations for the analysis of calibration results the accuracy was measured by comparing minimum values of the objective functions from each type of optimization with minimum values from the original nonoptimized implementation as samples processed in the calibration are randomly selected calibration results of the optimized implementations are expected to be different from the original results table 7 exhibits minimum values of the objective functions and relative errors for each type of optimization compared to the type v0 0 original nonoptimized implementation all values were computed as the average between results from the calibration executed on system01 and system02 except for the type v0 10 available only from system02 the largest relative errors occurred for the type v2 on cpu reaching 5 97 3 24 and 1 52 for the coefficients nse and nse log and systematic error err respectively these small errors indicate that calibration results were acceptable and validated the optimized implementations 5 conclusions cpu and gpu optimizations were investigated using the mgb hydrological model and mocom ua calibration method widely used by the hydrology community in brazil the calibration performance was improved with runtimes reduced from week long to few hour periods reaching speedups close to 20 on cpu with a proposed approach thus increasing the usefulness in hydrological applications vectorization intel intrinsics and thread parallelism openmp on cpu and also data parallelism cuda on gpu were employed for optimized simulation and calibration based on a double layer approach with experiments conducted on state of the art cpu gpu systems for real world datasets vectorization of the mgb model s key routines achieved significant speedups from approximately two thirds of vectorized code with intrinsic vector functions which maximize the use of vector resources cpu speedups of the fully optimized mgb model vectorization and thread parallelism were close to the theoretical peak from amdahl s law or even higher cache effects this performance level would not be reached by implicit vectorization via compiler flags the runtimes of the routines were reduced to less than 3 s with cuda on gpu but the mgb model s performance was hindered by overheads of data transfer and kernel launches too many routine calls besides the workloads of the available datasets are not gpu friendly not enough computations so cpu optimizations provided a more appropriate solution the scalability analysis indicated that the workloads of the available datasets were not enough for good scalability speedups of approximately 24 and 65 were achieved on cpu and gpu respectively for problem sizes of at least 8 larger than the original workloads the cpu roofline analysis confirmed that the proposed optimizations more effectively exploited the available cpu resources whereas the gpu roofline analysis showed that executing the mgb model on nvidia gpus resulted in different arithmetic intensity and performance relative to cpu in particular one routine achieved performance almost 8 higher than on cpu as cuda kernels were significantly faster than optimized cpu codes a quantitative analysis of the time series of discharges and water heights showed small relative errors with the largest error of 6 06 although in this case the mean relative error was only 0 59 comparison between minimum objective functions from the nonoptimized and optimized calibration implementations also resulted in acceptable accuracy with the largest relative error of 5 97 for one objective function overall this investigation proved that modern systems can be effectively exploited by carefully deploying parallelization techniques adjusting applications to the advanced capabilities of currently available hardware as possible routes for future work the roofline analysis can be extended to the mgb model s miniapp for deeper understanding of how optimizations exploit hardware resources for larger problem sizes the cpu and gpu optimizations can also be employed in another version of the mgb model that simulates hydrological processes for continental scale datasets the iph research group is currently designing datasets that cover the entire south america which can be approximately 8 larger than available datasets declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors are grateful to walter collischonn from iph ufrgs for the availability of the mgb hydrological model and datasets this work was financially supported by the conselho nacional de desenvolvimento científico e tecnológico cnpq brazil grant number 140 687 2 016 5 and by portuguese national funds through fundação para a ciência e a tecnologia fct under projects uidb 50 021 2 020 and ptdc cci com 31 901 2 017 
25669,a simple algorithm is provided for randomly sampling a set of n 1 weights such that their sum is constrained to be equal to one analogous to randomly subdividing a pie into n 1 slices where the probability distribution of slice volumes are identically distributed the cumulative density and probability density functions of the random weights are provided the algorithmic implementation for the random number sampling are made available this algorithm has potential applications in calibration uncertainty analysis and sensitivity analysis of environmental models three example applications are provided to demonstrate the efficiency and superiority of the proposed method compared to alternative sampling methods keywords weight sampling identical distributions random number generation constrained random numbers 1 introduction there are many scenarios in optimization sensitivity analysis and uncertainty analysis of environmental models where it is desirable to randomly sample n 1 quantities w i such that the sum of quantities is constrained to be equal to a constant here set to 1 for simplicity 1 i 1 n 1 w i 1 this problem is equivalent to the problem of sampling n randomly sized slices of apple pie one slice for each of your friends while the remaining last slice n 1 is for yourself we assume that you do not favour any of your friends i e sampling distribution of all slices is identical the order you hand out the slices does not matter the samples are independent and you will eat all leftovers yourself all slices sum up to one examples from environmental modelling include applications where one wants to randomly partition unity into classes where class membership is assumed to have equal likelihood for instance one may wish to randomly sample the soil texture triangle which characterizes soils by percent silt sand or clay pozdniakov et al 2019 in ecological modelling one may wish to randomly generate distributions of biomass compositions by percent lipids carbohydrates and proteins summative weights are used in environmental modeling when various alternative options are blended mixed these options could be for example several precipitation station datasets surrounding a domain of interest instead of forcing the model with individual station datasets one could use the weighted average of the datasets piotrowski et al 2019 montanari and di baldassarre 2013 weights also need to be sampled in multi model averaging approaches such as in arsenault et al 2015 or to weight multiple performance criteria in decision making applications ganji et al 2016 hyde and maier 2006 hyde et al 2004 another example are so called blended models where the model is defined as the weighted average of multiple models mai et al 2020 to conserve mass or otherwise respect constraints in these examples the weights are required to sum up to 1 many optimization algorithms similarly depend upon uniform sampling of parameter space when model parameters are constrained as expressed in equation 1 it is useful to have the ability to sample independently and identically while respecting constraints upon parameters while it is straightforward to sample weights for n 1 by sampling from a uniform distribution and then determining w 2 as 1 w 1 this simple approach is not easily extendable to n greater than 1 two naïve approaches are for example a to sample random numbers iteratively by reducing the sampling range in each step such that the overall sum can not be exceeded see section 4 for algorithmic details or b drawing n samples from the uniform distribution and then either discard the entire sample in case the sum is already exceeding 1 otherwise keeping the sample and assigning the remainder to 1 to the n 1 th weight these approaches however can introduce a bias in the expected values of weights when using approach a i e w 0 25 0 25 0 25 0 25 is more likely to be generated than w 0 0 0 1 for n 3 this bias is due to the fact that a large last weight here w 4 requires all previous weights to be small while a large first weight does not have that restriction naïve approach b on the other hand does not draw biased samples but is significantly more computationally expensive than approach a or other methods these limitations will be demonstrated in detail later in this work see section 5 and figs 5 6 and 7 therein this challenge of sampling unbiased weights has been addressed in the computational task scheduling literature bini and buttazzo 2005 and the statistics literature moeini et al 2011 the first algorithmic implementation to this problem we could locate is the uunifast sampling algorithm first proposed by bini and buttazzo 2005 another subsequent solution is found by moeini et al 2011 who do not cite bini and buttazzo 2005 and is thus presumably independent of bini and buttazzo 2005 a key difference between these solutions is that bini and buttazzo 2005 provides a readily available sampling algorithm uunifast while moeini et al 2011 only describes the mathematical underpinning of an approach for sampling n random numbers from the uniform distribution u 0 1 and transforming them into n 1 weights as reported in griffin et al 2020 an alternate method for sampling from a uniform distribution is available within an open source matlab code stafford 2006 but no derivation accompanies the tool the methodology used is unpublished and not readily transferable to applications outside matlab the cdfs pdfs and inverse cdfs of the resultant weight distributions are not reported griffin et al 2020 proposed a new and more general sampling algorithm called the dirichlet rescale algorithm drs that replaces the uunifast algorithm bini and buttazzo 2005 and can sample a vector of n 1 outputs that sum to u not just 1 with each individual output value in the range of potentially different upper and lower bounds the mathematical presentation in these publications is quite advanced and as noted by bini and buttazzo 2005 there are some hidden pitfalls with the implementation of such a sampling algorithm these pitfalls include the biased sampling of the weights some weight combinations are more likely to appear than others and the poor computational efficiency of some implementations bini and buttazzo 2005 none of the aforementioned methods has been published in an environmental modelling related journal which has apparently hampered their practical application in this field for example none of the 867 publications that cite bini and buttazzo 2005 status june 2021 described applications in the environmental sciences the earlier mentioned examples of environmental modelling publications that require the sampling of weights pozdniakov et al 2019 piotrowski et al 2019 montanari and di baldassarre 2013 arsenault et al 2015 ganji et al 2016 hyde and maier 2006 hyde et al 2004 do not describe their sampling method explicitly and do not cite the methods introduced by bini and buttazzo 2005 moeini et al 2011 stafford 2006 or griffin et al 2020 as such some of these previous studies might not sample correctly the work presented here provides an alternate independently obtained mathematical derivation of the approach of bini and buttazzo 2005 and moeini et al 2011 it presents the full derivation of pdfs and cdfs and outlines a straightforward and replicable algorithm for determining unbiased weights which satisfy equation 1 the method has recently been used by mai et al 2020 to enable the estimation of model structural sensitivities through the weighting of discrete model process options continuously for simulating process level hydrologic fluxes the method also found its application in chlumsky et al 2021 to allow for a simultaneous calibration of model parameters and model structure where the weights are used to define the latter this work aims to bring the proper sampling of weights to the attention of environmental modellers by providing not only the mathematical underpinning section 2 the resulting explicit method to sample the weights section 3 and algorithmic validity checks of the proposed sampling method section 4 but also by presenting a few example applications requiring the sampling of weights section 5 the algorithms for the weight sampling are provided on github in multiple programming languages such that they can easily be used or translated into other programming languages 2 methodology for the case of two weights summing to one we can consider the constrained sampling problem as trying to uniformly sample a position along the one dimensional line from x 0 to 1 where the distance to the left vertex x 0 is given by w 1 and the distance to the right vertex x 1 is given by w 2 as depicted in fig 1 a from simple geometry or equation 1 w 2 1 w 1 by definition because we want to uniformly sample the domain the probability of being at any one position x along the line is equal i e f x 1 and since w 1 is interpreted as the distance from x 0 w 1 is equal to x and therefore f w 1 is a uniform distribution over the interval from 0 to 1 note that the same holds for w 2 as w 2 1 x 1 w 1 meaning f w 2 is also a uniform distribution for n greater than 1 the same general approach is valid we can consider the sampling problem as equivalent to uniformly sampling a single position w 1 w 2 w n 1 from a regular n simplex which is a n dimensional symmetric geometric shape with n edges connecting n nodes the simplex we use here is defined as the hypervolume defined by all points w 1 w 2 w n 1 where w i is greater than zero for all i such that the sum of the coordinates is less than or equal to 1 for n 1 the simplex is the line from above for n 2 the simplex is the right triangle of fig 1b for n 3 the simplex is a tetrahedron and so on we wish to uniformly sample points from the hypervolume of each simplex this is the reasoning for the scaling of the simplex a vertex corresponds to the case where one weight is equal to one and all others are zero the uniform sampling strategy may be visualized readily for n 2 as shown in fig 1b uniform sampling corresponds to even point density of randomly selected points within this triangle without position bias it is worthwhile to note that for n 2 each position can be uniquely identified using a single 2d coordinate w 1 w 2 i e there are only two degrees of freedom and the final weight may always be defined by knowing the other coordinates e g w 3 1 w 1 w 2 this holds true in higher dimensions as well with reference to fig 1b we here note that we can determine a uniform sampling strategy by recognizing that the cumulative distribution function of each weight should be equal to the relative area swept from the edge i to a distance of w i from the edge of the triangle regardless which edge it sweeps from i e 2 f 2 w i 1 v 2 0 w i 1 1 x d x 1 1 w i 2 where f 2 is the cumulative distribution function and v n 1 n is the hyper dimensional volume of an n simplex with unit length edges which in this case is the area of the triangle here l x 1 1 x is the length of the sweeping edge expressed as a function of x where the 1 is used to indicate the length of the edge at x 0 the probability distribution function of w i is given by the derivative of f 2 w i 3 f 2 w i d f 2 d w i 2 1 w i this can be repeated for higher dimensions for example for n 3 4 f 3 w i 1 v 3 0 w i a x d x where a x a 0 1 x 2 is the area of the sweeping face at a distance x from side i which is a triangle with side lengths that scale with 1 x and a maximum area a 0 the area of the surface corresponding to x 0 note that this area is equivalent to the hypervolume v 2 note also that the edge l 0 was the same as v 1 giving 5 f 3 w i v 2 v 3 0 w i 1 x 2 d x 1 1 w i 3 differentiating with respect to w i provides the probability density function for w i 6 f 3 w i d f 3 d w i 3 1 w i 2 note that the sweeping face was a line of length proportional to 1 x in 2d the area of an equilateral triangle a 3 x with side length proportional to 1 x in 3d and will be the volume of a pyramid with side length decreasing as 1 x in 4d i e a 4 x a 4 0 1 x 3 this generalizes to higher dimensions as a n x a n 0 1 x n 1 leading to the following integral 7 f n w i v n 1 v n 0 w i a n x a n 0 d x n 0 w i 1 x n 1 d x this in turn leads to a general expression for both the cdf and pdf of the weights 8 f n w i 1 1 w i n 9 f n w i n 1 w i n 1 note that these distributions more heavily lean towards smaller values of w i as n increases which has a clear interpretation for instance it should be expected that the mean weight of all sampled weights for n 1 3 is 1 3 and the mean for n 1 7 should be 1 7 i e a uniform weighting this can be shown to be true for any n i e 10 w 0 1 w f n w d w n 0 1 w 1 w n 1 d w 1 n 1 3 sampling of course these distribution functions cannot be sampled independently once w 1 is sampled for instance the other weights w 2 w 3 etc are constrained to a single plane slicing through the n simplex the appropriate strategy for such conditional sampling recognizes that once the first weight is sampled the determination of the n 1 remaining weights may be done by sampling from the distribution f n 1 w then scaling the weights by 1 w 1 analogous to sampling the remainder of the pie into n 1 slices this can be repeated from n down to 1 we can define from equation 8 above the following sampling function or inverse cdf enabling us to generate a random number sampled from the distribution f n w given a random number r sampled from a uniform distribution in the range from 0 to 1 11 s n r 1 1 r 1 n our sampling strategy then can be summarized as follows for each set of weights needed first generate a vector of random numbers r 1 r 2 r n from a uniform distribution between 0 and 1 the corresponding vector of weights w 1 w 2 w n 1 can then be calculated using 12 w 1 s n r 1 w 2 1 w 1 s n 1 r 2 w 3 1 w 1 w 2 s n 2 r 3 w j 1 i 1 j 1 w i s n j 1 r j w n 1 1 i 1 n w i 4 demonstration of validity of proposed sampling method a first sampling experiment is performed using m 10 000 independent samples of weights drawn fig 2 the sampling is tested using n 1 3 and n 1 5 following the sampling strategy in equation 12 fig 2 demonstrates that the histograms of the sampled weights are all identical and matching the analytical pdf f 2 w i and f 4 w i depending if 3 or 5 weights are sampled a second numerical exercise is used to confirm that the approximated and analytical pdf also match when the n repl 100 independent samplings are performed while increasing the number of samples m from 100 to 50 000 fig 3 the results show that the mean absolute error between the continuous analytical pdf eq 9 and the approximated pdf are converging to zero with an increasing number of samples drawn m 10 000 proves to be a sufficient number of samples to yield a mean absolute error mae of below 0 06 for all 100 repetitions of the sampling experiment the uncertainty of the error can be approximated through the width of the boxplots and is decreasing to zero with an increasing number of samples the fact that there are no differences between the error statistics of the weights w i compare fig 3a c and d h shows that the weights are i independent and ii all weights are converging to the same analytical pdf i is proven by the fact that all weights show the same pattern of convergence while ii is demonstrated through the convergence to zero for large sample sizes as a counter example fig 4 compares the proposed sampling strategy a c with a naïve sampling e g where the first weight w i is uniformly sampled from the unit interval w 1 u 0 1 the second weight is uniformly sampled from the remaining range w 2 u 0 1 w 1 while the third weight w 3 is set to be the remainder such that the three weights sum up to 1 0 w 3 1 w 1 w 2 the results show that the naïvely sampled weights do not follow the same distribution the sampling strategy favors large weights for w 1 and small weights for w 2 and w 3 which leads to a non uniform sampling of the parameter space this is depicted in fig 4d and h showing the ternary plot of three sampled weights for the proposed method and the naïve method respectively it clearly demonstrates that the proposed method samples the domain uniform while the naïve approach preferentially samples large weights for w 1 and low weights for w 2 and w 3 lower right corner of triangle the proposed method hence is clearly beneficial when for example random samples of the soil texture are drawn by interpreting the weight as the fraction of sand w 1 silt w 2 and clay w 3 the naïve approach samples more sandy soils while only a rare amount of clay and silty soils the proposed method prevents this 5 examples of application of proposed sampling method the following three sections are to demonstrate examples where the sampling of summative weights is required in real world environmental modelling applications these examples are to demonstrate that the proposed sampling is addressing the limitations pitfalls stated in the introduction the proposed method is more efficient compared to a state of the practice exclusion based method sec 5 1 is superior to simple biased sampling approaches sec 5 2 and can be used to avoid sampling artefacts when optimal weights are obtained through automatic calibration sec 5 3 5 1 example 1 efficiency of proposed sampling a state of the practice method to draw samples of any distribution is the so called accept reject sampling which samples points within a unit hypercube and rejects samples that do not follow this distribution robert and casella 2005 in terms of the sampling of summative weights problem this translates into sampling points within a square n 1 3 or cube n 1 4 and withdraw samples that are outside a triangle or tetrahedron respectively even though this method guarantees that the samples are distributed uniformly within the desired simplex triangle tetrahedron the amount of samples withdrawn increases drastically with increasing dimensions in case of summative weights the volume of the unit hypercubes is 1 and the volume of the desired n simplex is 1 n when n 1 weights are to be sampled eq 2 this results in only 1 n of the samples drawn within a hypercube being accepted fig 5a shows that all samples drawn with the proposed method are feasible and uniformly distributed within a ternary plot while fig 5b shows that 50 of the samples drawn with the accept reject method are invalid when n 1 3 weights are sampled fig 5c shows the non linear decrease of efficiency here valid vs invalid samples in case n 1 10 weights need to be sampled only 1 9 100 0 00028 of the samples drawn are valid in other words only around 3 out of 1 000 000 samples are valid on the contrary the efficiency of the proposed sampling is 100 independent of the number of weights since the method is constructed to generate samples filling the n simplex uniformly without rejection 5 2 example 2 sampling of porosity of soil classes a real world example of summative weights is to sample soil textures i e sample the sand silt and clay content of a mineral soil as for example done by pozdniakov et al 2019 these three contents need to sum up to 1 the soil textures are used to define soil classes sampled soil textures are often visualized using the soil triangle which is a ternary plot as shown in fig 6 a c and e g for demonstration purposes the exemplary soil class sandy clay from the usda soil triangle which is of a triangular shape itself will be sampled from the soil class sandy clay contains soils with sand contents between 45 and 65 clay contents between 35 and 55 and silt contents up to 20 soil textures are often used in hydrologic models and land surface schemes to derive soil properties such as porosity θ s in using pedo transfer functions for instance cosby et al 1984 obtained the following pedo transfer function for porosity 13 θ s 0 126 c s a n d 48 9 where c sand represents the percentage of sand in a given soil fig 6 shows random samples drawn from the soil class sandy clay using the proposed a c and the naïve sampling approach e f the difference between the three panels for each approach is solely the labeling of the three axis i e the random assignment of which weight corresponds to which soil component it is expected that this assignment should not matter which is clearly the case for the proposed sampling method fig 6d however in the naïve sampling approach the corresponding distribution of calculated porosity depends on which weight is assigned to sand and therefore the leads to a calculated porosity distribution which is problematically sensitive to the order of sampling as shown in figure 4h 5 3 example 3 calibration of blended model options so called blended models are defined as the weighted average of multiple models or model components mai et al 2020 to conserve mass or otherwise respect physical constraints the weights are required to sum up to 1 we here use a manufactured example blended model f defined as the weighted average of three components 14 f x p w w 1 sin p 0 x w 2 sin p 1 x w 3 p 2 exp p 3 x where w denotes the weights of the three components p are model component specific parameters and x is an independent variable such as time or space this model has been contrived to keep the discussion of results straightforward however these results extend to much more complicated environmental models with summative constraints on their calibrated parameters a real world problem may now be to find the optimal model parameters p and weights w to fit a set of observations chlumsky et al 2021 model parameters would be drawn for example from a uniform distribution given an upper and lower physical limit of the model parameters the weights need to be sampled such that they sum up to 1 and the sampling distribution should be identical so as to not favour one model component over another one fig 7 a shows the results of such a calibration in case a naïve sampling is used to find the optimal weights and parameters while fig 7b shows the same experiment but using the proposed sampling for the weights one vector of synthetic observations at 5000 points f x was derived using a fixed set of parameters and weights for all experiments and trials the optimum of the objective function is hence known to be 0 in this synthetic experiment in total 50 independent trials have been performed for each of the two experiments the fixed computational budget dynamically dimensioned search dds tolson and shoemaker 2007 algorithm was used for automatic calibration a budget of 100 function evaluations was used in each trial the distribution of the best objective function value based on the 50 trials fig 7c demonstrates that the proposed sampling is more likely to find better objective function values after 100 iterations the sampling approach described here will be similarly useful for other optimization algorithms based upon stochastic search such as simulated annealing kirkpatrick et al 1983 or shuffled complex evolution sce duan et al 1992 algorithms but not for deterministic gradient based optimization methods as they are not based on random sampling 6 conclusions a simple and effective strategy for sampling n 1 summative weights from identical distributions is derived and demonstrated to ensure appropriate methods are known to environmental modellers the method is shown to be effective and perfectly unbiased for multiple values of n three example applications demonstrate the computational efficiency and superiority of this method compared to other sampling approaches the approach may be used any time where uniform random sampling of variables with a summation constraint is needed such problems are found in a wide variety of calibration uncertainty analysis and sensitivity analysis exercises python and r implementations of the sampling algorithm of equation 12 are freely available on github under https github com julemai piesharedistribution mai et al 2020 we recommend readers that require their weights to sum up to values other than 1 or requiring additional lower and upper bounds for the weights to look into the sampling method introduced by griffin et al 2020 as it addresses these additional constraints besides these limitations i e fixed upper lower bounds and the sum being 1 the authors are not aware of any other limitation or challenge associated with the use of the sampling method proposed declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was undertaken thanks primarily to funding from the canada first research excellence fund provided to the global water futures gwf project and the integrated modeling program for canada impc 
25669,a simple algorithm is provided for randomly sampling a set of n 1 weights such that their sum is constrained to be equal to one analogous to randomly subdividing a pie into n 1 slices where the probability distribution of slice volumes are identically distributed the cumulative density and probability density functions of the random weights are provided the algorithmic implementation for the random number sampling are made available this algorithm has potential applications in calibration uncertainty analysis and sensitivity analysis of environmental models three example applications are provided to demonstrate the efficiency and superiority of the proposed method compared to alternative sampling methods keywords weight sampling identical distributions random number generation constrained random numbers 1 introduction there are many scenarios in optimization sensitivity analysis and uncertainty analysis of environmental models where it is desirable to randomly sample n 1 quantities w i such that the sum of quantities is constrained to be equal to a constant here set to 1 for simplicity 1 i 1 n 1 w i 1 this problem is equivalent to the problem of sampling n randomly sized slices of apple pie one slice for each of your friends while the remaining last slice n 1 is for yourself we assume that you do not favour any of your friends i e sampling distribution of all slices is identical the order you hand out the slices does not matter the samples are independent and you will eat all leftovers yourself all slices sum up to one examples from environmental modelling include applications where one wants to randomly partition unity into classes where class membership is assumed to have equal likelihood for instance one may wish to randomly sample the soil texture triangle which characterizes soils by percent silt sand or clay pozdniakov et al 2019 in ecological modelling one may wish to randomly generate distributions of biomass compositions by percent lipids carbohydrates and proteins summative weights are used in environmental modeling when various alternative options are blended mixed these options could be for example several precipitation station datasets surrounding a domain of interest instead of forcing the model with individual station datasets one could use the weighted average of the datasets piotrowski et al 2019 montanari and di baldassarre 2013 weights also need to be sampled in multi model averaging approaches such as in arsenault et al 2015 or to weight multiple performance criteria in decision making applications ganji et al 2016 hyde and maier 2006 hyde et al 2004 another example are so called blended models where the model is defined as the weighted average of multiple models mai et al 2020 to conserve mass or otherwise respect constraints in these examples the weights are required to sum up to 1 many optimization algorithms similarly depend upon uniform sampling of parameter space when model parameters are constrained as expressed in equation 1 it is useful to have the ability to sample independently and identically while respecting constraints upon parameters while it is straightforward to sample weights for n 1 by sampling from a uniform distribution and then determining w 2 as 1 w 1 this simple approach is not easily extendable to n greater than 1 two naïve approaches are for example a to sample random numbers iteratively by reducing the sampling range in each step such that the overall sum can not be exceeded see section 4 for algorithmic details or b drawing n samples from the uniform distribution and then either discard the entire sample in case the sum is already exceeding 1 otherwise keeping the sample and assigning the remainder to 1 to the n 1 th weight these approaches however can introduce a bias in the expected values of weights when using approach a i e w 0 25 0 25 0 25 0 25 is more likely to be generated than w 0 0 0 1 for n 3 this bias is due to the fact that a large last weight here w 4 requires all previous weights to be small while a large first weight does not have that restriction naïve approach b on the other hand does not draw biased samples but is significantly more computationally expensive than approach a or other methods these limitations will be demonstrated in detail later in this work see section 5 and figs 5 6 and 7 therein this challenge of sampling unbiased weights has been addressed in the computational task scheduling literature bini and buttazzo 2005 and the statistics literature moeini et al 2011 the first algorithmic implementation to this problem we could locate is the uunifast sampling algorithm first proposed by bini and buttazzo 2005 another subsequent solution is found by moeini et al 2011 who do not cite bini and buttazzo 2005 and is thus presumably independent of bini and buttazzo 2005 a key difference between these solutions is that bini and buttazzo 2005 provides a readily available sampling algorithm uunifast while moeini et al 2011 only describes the mathematical underpinning of an approach for sampling n random numbers from the uniform distribution u 0 1 and transforming them into n 1 weights as reported in griffin et al 2020 an alternate method for sampling from a uniform distribution is available within an open source matlab code stafford 2006 but no derivation accompanies the tool the methodology used is unpublished and not readily transferable to applications outside matlab the cdfs pdfs and inverse cdfs of the resultant weight distributions are not reported griffin et al 2020 proposed a new and more general sampling algorithm called the dirichlet rescale algorithm drs that replaces the uunifast algorithm bini and buttazzo 2005 and can sample a vector of n 1 outputs that sum to u not just 1 with each individual output value in the range of potentially different upper and lower bounds the mathematical presentation in these publications is quite advanced and as noted by bini and buttazzo 2005 there are some hidden pitfalls with the implementation of such a sampling algorithm these pitfalls include the biased sampling of the weights some weight combinations are more likely to appear than others and the poor computational efficiency of some implementations bini and buttazzo 2005 none of the aforementioned methods has been published in an environmental modelling related journal which has apparently hampered their practical application in this field for example none of the 867 publications that cite bini and buttazzo 2005 status june 2021 described applications in the environmental sciences the earlier mentioned examples of environmental modelling publications that require the sampling of weights pozdniakov et al 2019 piotrowski et al 2019 montanari and di baldassarre 2013 arsenault et al 2015 ganji et al 2016 hyde and maier 2006 hyde et al 2004 do not describe their sampling method explicitly and do not cite the methods introduced by bini and buttazzo 2005 moeini et al 2011 stafford 2006 or griffin et al 2020 as such some of these previous studies might not sample correctly the work presented here provides an alternate independently obtained mathematical derivation of the approach of bini and buttazzo 2005 and moeini et al 2011 it presents the full derivation of pdfs and cdfs and outlines a straightforward and replicable algorithm for determining unbiased weights which satisfy equation 1 the method has recently been used by mai et al 2020 to enable the estimation of model structural sensitivities through the weighting of discrete model process options continuously for simulating process level hydrologic fluxes the method also found its application in chlumsky et al 2021 to allow for a simultaneous calibration of model parameters and model structure where the weights are used to define the latter this work aims to bring the proper sampling of weights to the attention of environmental modellers by providing not only the mathematical underpinning section 2 the resulting explicit method to sample the weights section 3 and algorithmic validity checks of the proposed sampling method section 4 but also by presenting a few example applications requiring the sampling of weights section 5 the algorithms for the weight sampling are provided on github in multiple programming languages such that they can easily be used or translated into other programming languages 2 methodology for the case of two weights summing to one we can consider the constrained sampling problem as trying to uniformly sample a position along the one dimensional line from x 0 to 1 where the distance to the left vertex x 0 is given by w 1 and the distance to the right vertex x 1 is given by w 2 as depicted in fig 1 a from simple geometry or equation 1 w 2 1 w 1 by definition because we want to uniformly sample the domain the probability of being at any one position x along the line is equal i e f x 1 and since w 1 is interpreted as the distance from x 0 w 1 is equal to x and therefore f w 1 is a uniform distribution over the interval from 0 to 1 note that the same holds for w 2 as w 2 1 x 1 w 1 meaning f w 2 is also a uniform distribution for n greater than 1 the same general approach is valid we can consider the sampling problem as equivalent to uniformly sampling a single position w 1 w 2 w n 1 from a regular n simplex which is a n dimensional symmetric geometric shape with n edges connecting n nodes the simplex we use here is defined as the hypervolume defined by all points w 1 w 2 w n 1 where w i is greater than zero for all i such that the sum of the coordinates is less than or equal to 1 for n 1 the simplex is the line from above for n 2 the simplex is the right triangle of fig 1b for n 3 the simplex is a tetrahedron and so on we wish to uniformly sample points from the hypervolume of each simplex this is the reasoning for the scaling of the simplex a vertex corresponds to the case where one weight is equal to one and all others are zero the uniform sampling strategy may be visualized readily for n 2 as shown in fig 1b uniform sampling corresponds to even point density of randomly selected points within this triangle without position bias it is worthwhile to note that for n 2 each position can be uniquely identified using a single 2d coordinate w 1 w 2 i e there are only two degrees of freedom and the final weight may always be defined by knowing the other coordinates e g w 3 1 w 1 w 2 this holds true in higher dimensions as well with reference to fig 1b we here note that we can determine a uniform sampling strategy by recognizing that the cumulative distribution function of each weight should be equal to the relative area swept from the edge i to a distance of w i from the edge of the triangle regardless which edge it sweeps from i e 2 f 2 w i 1 v 2 0 w i 1 1 x d x 1 1 w i 2 where f 2 is the cumulative distribution function and v n 1 n is the hyper dimensional volume of an n simplex with unit length edges which in this case is the area of the triangle here l x 1 1 x is the length of the sweeping edge expressed as a function of x where the 1 is used to indicate the length of the edge at x 0 the probability distribution function of w i is given by the derivative of f 2 w i 3 f 2 w i d f 2 d w i 2 1 w i this can be repeated for higher dimensions for example for n 3 4 f 3 w i 1 v 3 0 w i a x d x where a x a 0 1 x 2 is the area of the sweeping face at a distance x from side i which is a triangle with side lengths that scale with 1 x and a maximum area a 0 the area of the surface corresponding to x 0 note that this area is equivalent to the hypervolume v 2 note also that the edge l 0 was the same as v 1 giving 5 f 3 w i v 2 v 3 0 w i 1 x 2 d x 1 1 w i 3 differentiating with respect to w i provides the probability density function for w i 6 f 3 w i d f 3 d w i 3 1 w i 2 note that the sweeping face was a line of length proportional to 1 x in 2d the area of an equilateral triangle a 3 x with side length proportional to 1 x in 3d and will be the volume of a pyramid with side length decreasing as 1 x in 4d i e a 4 x a 4 0 1 x 3 this generalizes to higher dimensions as a n x a n 0 1 x n 1 leading to the following integral 7 f n w i v n 1 v n 0 w i a n x a n 0 d x n 0 w i 1 x n 1 d x this in turn leads to a general expression for both the cdf and pdf of the weights 8 f n w i 1 1 w i n 9 f n w i n 1 w i n 1 note that these distributions more heavily lean towards smaller values of w i as n increases which has a clear interpretation for instance it should be expected that the mean weight of all sampled weights for n 1 3 is 1 3 and the mean for n 1 7 should be 1 7 i e a uniform weighting this can be shown to be true for any n i e 10 w 0 1 w f n w d w n 0 1 w 1 w n 1 d w 1 n 1 3 sampling of course these distribution functions cannot be sampled independently once w 1 is sampled for instance the other weights w 2 w 3 etc are constrained to a single plane slicing through the n simplex the appropriate strategy for such conditional sampling recognizes that once the first weight is sampled the determination of the n 1 remaining weights may be done by sampling from the distribution f n 1 w then scaling the weights by 1 w 1 analogous to sampling the remainder of the pie into n 1 slices this can be repeated from n down to 1 we can define from equation 8 above the following sampling function or inverse cdf enabling us to generate a random number sampled from the distribution f n w given a random number r sampled from a uniform distribution in the range from 0 to 1 11 s n r 1 1 r 1 n our sampling strategy then can be summarized as follows for each set of weights needed first generate a vector of random numbers r 1 r 2 r n from a uniform distribution between 0 and 1 the corresponding vector of weights w 1 w 2 w n 1 can then be calculated using 12 w 1 s n r 1 w 2 1 w 1 s n 1 r 2 w 3 1 w 1 w 2 s n 2 r 3 w j 1 i 1 j 1 w i s n j 1 r j w n 1 1 i 1 n w i 4 demonstration of validity of proposed sampling method a first sampling experiment is performed using m 10 000 independent samples of weights drawn fig 2 the sampling is tested using n 1 3 and n 1 5 following the sampling strategy in equation 12 fig 2 demonstrates that the histograms of the sampled weights are all identical and matching the analytical pdf f 2 w i and f 4 w i depending if 3 or 5 weights are sampled a second numerical exercise is used to confirm that the approximated and analytical pdf also match when the n repl 100 independent samplings are performed while increasing the number of samples m from 100 to 50 000 fig 3 the results show that the mean absolute error between the continuous analytical pdf eq 9 and the approximated pdf are converging to zero with an increasing number of samples drawn m 10 000 proves to be a sufficient number of samples to yield a mean absolute error mae of below 0 06 for all 100 repetitions of the sampling experiment the uncertainty of the error can be approximated through the width of the boxplots and is decreasing to zero with an increasing number of samples the fact that there are no differences between the error statistics of the weights w i compare fig 3a c and d h shows that the weights are i independent and ii all weights are converging to the same analytical pdf i is proven by the fact that all weights show the same pattern of convergence while ii is demonstrated through the convergence to zero for large sample sizes as a counter example fig 4 compares the proposed sampling strategy a c with a naïve sampling e g where the first weight w i is uniformly sampled from the unit interval w 1 u 0 1 the second weight is uniformly sampled from the remaining range w 2 u 0 1 w 1 while the third weight w 3 is set to be the remainder such that the three weights sum up to 1 0 w 3 1 w 1 w 2 the results show that the naïvely sampled weights do not follow the same distribution the sampling strategy favors large weights for w 1 and small weights for w 2 and w 3 which leads to a non uniform sampling of the parameter space this is depicted in fig 4d and h showing the ternary plot of three sampled weights for the proposed method and the naïve method respectively it clearly demonstrates that the proposed method samples the domain uniform while the naïve approach preferentially samples large weights for w 1 and low weights for w 2 and w 3 lower right corner of triangle the proposed method hence is clearly beneficial when for example random samples of the soil texture are drawn by interpreting the weight as the fraction of sand w 1 silt w 2 and clay w 3 the naïve approach samples more sandy soils while only a rare amount of clay and silty soils the proposed method prevents this 5 examples of application of proposed sampling method the following three sections are to demonstrate examples where the sampling of summative weights is required in real world environmental modelling applications these examples are to demonstrate that the proposed sampling is addressing the limitations pitfalls stated in the introduction the proposed method is more efficient compared to a state of the practice exclusion based method sec 5 1 is superior to simple biased sampling approaches sec 5 2 and can be used to avoid sampling artefacts when optimal weights are obtained through automatic calibration sec 5 3 5 1 example 1 efficiency of proposed sampling a state of the practice method to draw samples of any distribution is the so called accept reject sampling which samples points within a unit hypercube and rejects samples that do not follow this distribution robert and casella 2005 in terms of the sampling of summative weights problem this translates into sampling points within a square n 1 3 or cube n 1 4 and withdraw samples that are outside a triangle or tetrahedron respectively even though this method guarantees that the samples are distributed uniformly within the desired simplex triangle tetrahedron the amount of samples withdrawn increases drastically with increasing dimensions in case of summative weights the volume of the unit hypercubes is 1 and the volume of the desired n simplex is 1 n when n 1 weights are to be sampled eq 2 this results in only 1 n of the samples drawn within a hypercube being accepted fig 5a shows that all samples drawn with the proposed method are feasible and uniformly distributed within a ternary plot while fig 5b shows that 50 of the samples drawn with the accept reject method are invalid when n 1 3 weights are sampled fig 5c shows the non linear decrease of efficiency here valid vs invalid samples in case n 1 10 weights need to be sampled only 1 9 100 0 00028 of the samples drawn are valid in other words only around 3 out of 1 000 000 samples are valid on the contrary the efficiency of the proposed sampling is 100 independent of the number of weights since the method is constructed to generate samples filling the n simplex uniformly without rejection 5 2 example 2 sampling of porosity of soil classes a real world example of summative weights is to sample soil textures i e sample the sand silt and clay content of a mineral soil as for example done by pozdniakov et al 2019 these three contents need to sum up to 1 the soil textures are used to define soil classes sampled soil textures are often visualized using the soil triangle which is a ternary plot as shown in fig 6 a c and e g for demonstration purposes the exemplary soil class sandy clay from the usda soil triangle which is of a triangular shape itself will be sampled from the soil class sandy clay contains soils with sand contents between 45 and 65 clay contents between 35 and 55 and silt contents up to 20 soil textures are often used in hydrologic models and land surface schemes to derive soil properties such as porosity θ s in using pedo transfer functions for instance cosby et al 1984 obtained the following pedo transfer function for porosity 13 θ s 0 126 c s a n d 48 9 where c sand represents the percentage of sand in a given soil fig 6 shows random samples drawn from the soil class sandy clay using the proposed a c and the naïve sampling approach e f the difference between the three panels for each approach is solely the labeling of the three axis i e the random assignment of which weight corresponds to which soil component it is expected that this assignment should not matter which is clearly the case for the proposed sampling method fig 6d however in the naïve sampling approach the corresponding distribution of calculated porosity depends on which weight is assigned to sand and therefore the leads to a calculated porosity distribution which is problematically sensitive to the order of sampling as shown in figure 4h 5 3 example 3 calibration of blended model options so called blended models are defined as the weighted average of multiple models or model components mai et al 2020 to conserve mass or otherwise respect physical constraints the weights are required to sum up to 1 we here use a manufactured example blended model f defined as the weighted average of three components 14 f x p w w 1 sin p 0 x w 2 sin p 1 x w 3 p 2 exp p 3 x where w denotes the weights of the three components p are model component specific parameters and x is an independent variable such as time or space this model has been contrived to keep the discussion of results straightforward however these results extend to much more complicated environmental models with summative constraints on their calibrated parameters a real world problem may now be to find the optimal model parameters p and weights w to fit a set of observations chlumsky et al 2021 model parameters would be drawn for example from a uniform distribution given an upper and lower physical limit of the model parameters the weights need to be sampled such that they sum up to 1 and the sampling distribution should be identical so as to not favour one model component over another one fig 7 a shows the results of such a calibration in case a naïve sampling is used to find the optimal weights and parameters while fig 7b shows the same experiment but using the proposed sampling for the weights one vector of synthetic observations at 5000 points f x was derived using a fixed set of parameters and weights for all experiments and trials the optimum of the objective function is hence known to be 0 in this synthetic experiment in total 50 independent trials have been performed for each of the two experiments the fixed computational budget dynamically dimensioned search dds tolson and shoemaker 2007 algorithm was used for automatic calibration a budget of 100 function evaluations was used in each trial the distribution of the best objective function value based on the 50 trials fig 7c demonstrates that the proposed sampling is more likely to find better objective function values after 100 iterations the sampling approach described here will be similarly useful for other optimization algorithms based upon stochastic search such as simulated annealing kirkpatrick et al 1983 or shuffled complex evolution sce duan et al 1992 algorithms but not for deterministic gradient based optimization methods as they are not based on random sampling 6 conclusions a simple and effective strategy for sampling n 1 summative weights from identical distributions is derived and demonstrated to ensure appropriate methods are known to environmental modellers the method is shown to be effective and perfectly unbiased for multiple values of n three example applications demonstrate the computational efficiency and superiority of this method compared to other sampling approaches the approach may be used any time where uniform random sampling of variables with a summation constraint is needed such problems are found in a wide variety of calibration uncertainty analysis and sensitivity analysis exercises python and r implementations of the sampling algorithm of equation 12 are freely available on github under https github com julemai piesharedistribution mai et al 2020 we recommend readers that require their weights to sum up to values other than 1 or requiring additional lower and upper bounds for the weights to look into the sampling method introduced by griffin et al 2020 as it addresses these additional constraints besides these limitations i e fixed upper lower bounds and the sum being 1 the authors are not aware of any other limitation or challenge associated with the use of the sampling method proposed declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was undertaken thanks primarily to funding from the canada first research excellence fund provided to the global water futures gwf project and the integrated modeling program for canada impc 
