index,text
25390,predicting future land change is crucial in anticipating societal and environmental impacts and informing responses at different scales we designed an integrated high resolution land change model and forecasted australia s land change for the years 2020 2025 and 2030 for cropland forest grassland and built up land uses using cloud based and high performance computing a spatially explicit set of drivers was fed into a random forest classifier to generate 30 m per class suitability layers for the country which were then used for allocating land use the model was validated against 2015 data then land use was projected until 2030 accuracy at the national level was 94 forecasts showed increases in grassland and built up areas and decreases in forest and cropland our modelling framework expands the current capabilities of large scale land change models and provides a first of its kind multiclass land forecast for australia that can inform land policy at multiple scales in australia keywords land use change integrated model forecast random forest google earth engine data availability the code and the generated outputs are available for exploration and download 1 introduction land use and land cover hereafter simply land use transformations over large areas occur in response to changes in both global and local conditions song et al 2018 the nature conservancy 2018 modifications to land use hereafter simply land change cause a diverse range of direct and indirect impacts on land productivity climate hydrology biogeochemical cycling natural habitats and species diversity and other ecosystem services foley et al 2005 güneralp et al 2013 shukla et al 2019 song et al 2018 further transformative land change is expected to occur in response to future changes in human population climate diets commodity prices social norms financial incentives and land use policies planning regulation and governance alexander et al 2016a allan et al 2017 bizer 2005 gao and bryan 2017 hurtt et al 2011 riggio et al 2020 united nations 2017 understanding the likely impact of these variables on the type extent location and pace of land change at high resolution and over large scales is essential for managing and mitigating land change impacts bayer et al 2021 land change models are commonly used for analysing complex interactions between drivers of change and for representing the influence of these drivers on present and future land use grundy et al 2016 ren et al 2019 verburg et al 2016 despite the multiple theoretical frameworks and algorithms for modelling land change alexander et al 2016b prestele et al 2016 verburg et al 2013b in their most basic form land change models translate land system behaviour i e drivers and responses into mathematical equations chen et al 2020 doelman et al 2018 heck et al 2018 these equations aim to spatially represent the location and extent of current and future land use at different thematic and spatiotemporal resolutions doelman et al 2018 verburg et al 2019 wolff et al 2018 at the continental and global scale land change models e g landshift alcamo and schaldach 2006 globiom valin et al 2013 image stehfest et al 2014 clumondo van vliet and verburg 2018 gcam calvin et al 2019 magpie dietrich et al 2019 sleuth zhou et al 2019 and luto bryan et al 2016 are generally calibrated at coarse spatial resolution median pixel resolution 624 m van vliet et al 2016 with a single snapshot of land use e g pijanowski et al 2002 and or using a single land use type e g liang et al 2021 these limitations hinder the ability of models to capture and represent present and future land transformation processes operating at finer spatial scales than the modelling unit e g small patches landscape mosaics or highly heterogeneous biophysical regions all of which result in species loss due to habitat fragmentation franklin et al 2013 gillespie et al 2008 van vliet et al 2016 willis and whittaker 2002 global land change models are also limited in their ability to account for path dependencies and legacy effects of land change aspinall and hill 2007 azadi et al 2017 that are crucial for representing subtle or gradual temporal change such as cropping grazing rotations prestele et al 2016 and in the ability to represent multiple land use trajectories furthermore land management at local scales requires granular and spatially and temporally explicit multiclass land use information for ensuring that land dynamics are correctly represented within governmental jurisdictions hertel et al 2019 ren et al 2019 van vliet et al 2019 land change models relying solely on one modelling approach are able to capture and represent some aspects of the system whereas hybrid or integrated approaches are better suited for incorporating the complex non linear interactions at different spatial and temporal scales among drivers and the changes themselves de freitas et al 2018 gounaridis et al 2019 coupling regression based or machine learning algorithms with spatial characteristics of the neighbourhood hewitt et al 2014 roodposhti et al 2019 land change history vazquez quintero et al 2016 and other influential spatio temporal biophysical parameters in the modelling structure can better capture the complex land system dynamics and increase confidence in the outputs azadi et al 2017 pongratz et al 2018 machine learning algorithms have gained increased attention in land change modelling as they can handle large structured and unstructured datasets and unlike common statistical methods do not require a priori assumptions about the distribution and relationships among the variables motesharrei et al 2016 van vliet et al 2016 verburg et al 2011 2013a machine learning based land change models are commonly coupled with cellular automata and markov chain models e g hagenauer et al 2019 hsieh 2009 in a two stage process these approaches calculate land potential or suitability by employing a machine learning model that is later coupled with an allocation algorithm such as cellular automata for modelling land change multiclass models that incorporate these drivers as explanatory variables typically either use only the land change history van duynhoven and dragićević 2019 land change history and neighbourhood characteristics cao et al 2019 or neighbourhood characteristics and spatial drivers of change van vliet and verburg 2018 at the global or continental scale only a handful of models include in their structure all four key drivers of land change land change history neighbourhood accessibility here defined as the proximity to infrastructure and other land classes roodposhti et al 2019 2020 and spatially explicit environmental variables e g xing et al 2020 in addition the calibration of these models was either spatially coarse grained temporally restricted to a few or no historical time steps primarily centered on a small spatial extent or projecting single land use changes olmedo et al 2018 prestele et al 2016 moving towards a new generation of continental scale high resolution land change models requires the access to advanced computing infrastructure that allows the rapid processing of several variables and can operate in parallel on millions of pixels wang and yuan 2020 zhang et al 2023 zhuang et al 2020 furthermore standardized transparent and integrated approaches employing comprehensive sets of spatiotemporal drivers of land change at varying resolutions are required in order to better account for uncertainty in future land change projections bayer et al 2021 liu et al 2017 popp et al 2017 prestele et al 2016 verburg et al 2019 here we present an integrated continental scale high spatial resolution land change model we used a combination of a cloud based google earth engine gee gorelick et al 2017 and local high performance computing environments for training validating and forecasting land change at continental scale for australia nationwide land clearing driven by agricultural activities has shaped the current continental landscape historical land change has been closely linked to the introduction and changes in policies that have promoted and prevented the clearing rhodes et al 2017 simmons et al 2018a 2018b state based policies e g vegetation management act vma 1999 have further added complexity in how land has changed across australia simmons et al 2018a 2018b 2018c other land changes have also influenced forest dynamics and land configuration more broadly urban expansion has led to forest losses and irreversible land change across the country calderón loor et al 2021 expected increases in australia s population to around 26 30 million by 2030 nsw government and department of planning 2021 coupled with low population densities economic growth and incentives for infrastructure construction will influence the rates and course of urban expansion chen et al 2020 mcdonald et al 2013 seto et al 2012 land change modelling in the country is limited to monothematic coarse resolution models e g bryan et al 2016 ye et al 2019 which highlights the need for integrated models that can provide a comprehensive analysis of land dynamics at high granularity we used gee for developing individual suitability layers for four land use classes cropland forest grassland and built up land using a random forest rf algorithm historical land change information from 1985 to 2010 calderón loor et al 2021 was used for training the model and for calculating the historical and future land demand up to 2030 using compositional regression of past trends allocation was an iterative process based on the maximum likelihood approach that ensured all areal land demands were met we assessed the accuracy of our land change model against observed land use in 2015 and demonstrate the applicability of the model by producing short term continental scale land change forecasts at 5 year time steps from 2020 to 2030 2 methods 2 1 study area and land use data our land change model was calibrated and validated for the entire australian continent with an area of 7 688 287 km2 the country spans diverse climatic ecological and socio economic zones lesslie and mewett 2018 thackway et al 2013 two long term spatially explicit land use datasets are available for the country at 250 m resolution from 2001 to 2015 lymburner et al 2015 and at 30 m resolution from 1985 to 2015 at 5 year time steps calderón loor et al 2021 because of its granularity consistency and robustness we used the latter as our reference historical land use data each of the seven spatial land use layers contained around 8 5 billion pixels classified land use into six classes cropland forest grassland built up water and other we focussed on modelling land change dynamics between cropland forest grassland and built up land use classes water and other land use classes were considered to be constant over time by applying a mask to these areas two main reasons drove this decision a short forecast time window i e 10 years was not long enough for observing significant changes in these classes and the lack of dynamic predictor variables for modelling changes in these land classes 2 2 land change modelling the modelling was divided into three main components consolidation of the drivers of change generation of suitability layers and pixel allocation fig 1 2 2 1 drivers of change a comprehensive set of predictors at different spatiotemporal resolution was used to produce the 30 m individual suitability layers these predictors can be grouped into four main categories including historical path dependencies h neighbourhood dynamics n accessibility a and environmental factors influencing suitability e table 1 h variables provide a plausible set of land change trajectories based on past patterns cao et al 2019 zhang et al 2020 n variables represent the proportion of each land class in a specified neighbourhood three spatial windows were defined 3 3 9 9 and 81 81 pixels to capture the relationships e g attraction and repulsion hewitt et al 2014 between the state of the centre pixel and the state of its surroundings at different spatial scales at t 0 a variables captured the influence of the proximity of certain land use classes and infrastructure networks on land changes roodposhti et al 2020 lastly e variables capture a diverse array of spatially explicit factors influencing land use suitability noszczyk 2019 van vliet and verburg 2018 a similar dataset has been employed by marcos martinez et al 2018 and ye et al 2019 for analysing forest cover dynamics in australia where c cropland f forest g grassland b built up h historical path dependency drivers n neighbourhood drivers a accessibility drivers e environmental drivers sources 1 calderón loor et al 2021 2 commonwealth department of the environment 2014 3 xu and hutchinson 2011 4 viscarra rossel et al 2014 5 facebook connectivity lab and center for international earth science information network ciesin columbia university 2016 6 olson et al 2001 7 farr et al 2007 the complete set of variables was uploaded to gee were n variables and distances were calculated at each time step see the shared code and supplementary materials data processing and modelling specifications variables were resampled to the same pixel resolution i e 30 m discrete variables i e state id statistical area level id and ecoregion id where included as factors during the training process 2 2 2 land use suitability modelling we used random forest rf regression models for generating independent probability based suitability layers rf is a machine learning algorithm that constructs a user specified number of decision trees the outputs of each individual decision tree are averaged for calculating the individual suitability layers denisko and hoffman 2018 gounaridis et al 2019 each continental scale suitability layer is a continuous probability surface at 30 m with values from 0 to 100 where higher probabilities represent a higher confidence of a pixel to transition to a certain land use class we used the 2010 land use from calderón loor et al 2021 as our dependent variable whereas the historical land use data from 1985 to 2005 i e five historical time steps were used as part of the predictor variables for training the land change model i e h variables four rf algorithms one for each land use type were trained using gee for each rf model we removed the least significant variables i e where the mean accumulation of the impurity decrease within each tree was 0 1 from the training set to avoid computational limits in gee the complete list of variables used for training the rf algorithms is listed in table 1 this fine tuning process was conducted locally using the scikit learn python package pedregosa et al 2011 training points were generated following a stratified random approach the strata were defined according to the number of historical land transitions from 1985 to 2005 for each pixel i e no temporal change i e 0 historical transitions medium change i e 1 3 historical transitions and high land change i e 3 historical transitions inside each stratum approximately 25 000 points were randomly located summing up to a total of 75 000 80 000 points in each land use the number of trees for each rf model was set to 150 and the number of variables per split mtry was equal to the square root of the number of predictors following the hyperparameters used by calderón loor et al 2021 the trained rf models were used for producing individual suitability layers for the year 2015 h n and a distances to land use classes variables were updated and used for the suitability prediction using the generated suitability layers a maximum likelihood approach unconstrained by future land use area demand was used for producing a map for the year 2015 and later employed for testing the accuracy of this simpler allocation approach prior to introducing land use area demand constraints see supplementary materials 2 2 3 demand calculation and allocation to allocate land use to pixels we started by calculating the areal land demand for each land use type at statistical area level 2 sa2 sa2s are non overlaying functional areas that represent a community that interacts together socially and economically and are defined under the australian statistical geography standard asgs abs 2016 observed land transitions were used to calculate historical demand i e for the period 1985 2015 in gee land demand forecasts were calculated for 2020 2025 and 2030 using a time series compositional regression leininger et al 2013 for each sa2 a regression model with compositional response was fitted using the year as the independent variable and the constrained proportions of the four land use classes at each time step as the dependent variables a model with compositional response assumes that a composition y all its elements add 1 is a linear function of different explanatory variables x 0 x 1 x p eq 1 mateu figueras and pawlowsky glahn 2008 van den boogaart k et al 2021a eq 1 y ˆ i 0 p x i ʘ b i y n φ d y ˆ σ ε where n φ d y ˆ σ ε stands for the normal distribution of the simplex of y for simplicity purposes the logratio transformation of eq 1 can help to represent the model as a multivariate linear regression model eq 2 eq 2 y ˆ i 0 p x i b i y n d 1 y ˆ σ ε where b 0 b 1 b p are the slopes and the residual covariance matrix σ ε compositional regression was performed using the compositions package van den boogaart k et al 2021b in r r core team 2021 an adjustment of forecasted land demand was made prior to the allocation process to avoid exceeding the maximum number of suitable pixels of each land use type at sa2 level using the following heuristic at each time step the maximum forecasted demand could not be higher than 1 5 times the number of pixels with a suitability 40 pixels were allocated in a two step process first land use persistence was modelled by transferring the land use type of each pixel from t0 into t1 then the land use area from the initial allocated pixels in t1 was compared with the calculated or observed demand for the same time step an automated iterative process was then employed for allocating pixels into under allocated land use classes by removing pixels from overallocated land use classes pixels with the lowest suitability for the over allocated land use class were allocated into a new under allocated land use class with the highest suitability the allocation process ended when demands were satisfied for all land use classes 2 3 accuracy assessment the land use map output for the year 2015 was compared against a validation land use map for 2015 and used to calculate the accuracy of the model at different spatial scales national state and sa2 level a confusion matrix was used for calculating the overall producer and user accuracy oa sum of the pixels that were correctly classified divided by the total number of pixels pa and ua respectively as well as the omission and commission errors oe 100 pa and ce 100 ua respectively of the land change model at national level olofsson et al 2013 2014 to provide a better representation of accuracy at different spatial scales the oa was also calculated at state and sa2 levels furthermore the ability of the land change model for modelling different land use classes was evaluated by producing receiver operating characteristics roc plots at state level conceived as a probability curve roc plots present the false positive rate fpr versus true positive rate tpr for summarizing the ability of the land change model to distinguish between land use classes pontius jr and schneider 2001 the area under the curve auc was computed for each land use class and then micro averaged to account for the imbalance in the dataset for calculating the multi class auc at state level safitri et al 2021 wu et al 2022 lastly we calculated the sensitivity s 1 i e the proportion of pixels correctly classified as having changed eq 3 specificity s 2 i e the proportion of pixels correctly classified as having not changed eq 4 and prevalence θ that is the proportion of pixels that changed between 2010 and 2015 eq 5 calderón loor et al 2021 foody 2010 table 2 we isolated the a pixels i e total hits and calculated the proportion of pixels that changed and were predicted to the right land class 3 results 3 1 suitability and simulated land use four independent suitability layers were simulated for 2015 fig 2 each map presents the likelihood of each land use class to be present in any given pixel in general the suitability layers present high concordance with the actual distribution of each land use class using the generated suitability layers as inputs for the allocation algorithm a simulated 2015 map was produced fig 3 a visual comparison between the reference and the predicted map in addition to visual inspections of selected sites demonstrates the high degree of similarity between the maps and the accuracy of the model in predicting the location and extent of land change readers can visualize the produced layers in the following gee app https mioash users earthengine app view australialandchangeforecast overall accuracy of the final simulated 2015 map was estimated at 93 8 in comparison to the accuracy of the unconstrained maximum likelihood approach which was 90 see supplementary materials grassland and forest classes presented the lowest commission and omission errors of 4 and 9 respectively conversely cropland and built up areas showed the highest commission and omission errors both exceeding 14 table 3 confusion occurred between cropland and grassland whereas built up areas were incorrectly simulated as cropland forest and grassland in similar proportions at the national level sensitivity s1 was estimated to 69 4 specificity s2 was 94 7 and the prevalence θ was 3 2 the proportion of changing pixels between 2010 and 2015 for which the correct land class was predicted was 84 6 where pa producer accuracy ua user accuracy ce commission errors oe omission errors at state level the oa for 2015 varied between 86 3 and 98 7 in new south wales and south australia respectively average oa accuracies at sa2 level presented the same patterns as the oa at state level furthermore around 95 of the sa2s had oa accuracies 70 across states table 4 fig 4 see also supplementary data 1 auc at state level varied between 0 91 and 0 97 higher errors i e lower true positive and higher false positive rates were found in new south wales queensland and victoria in general forest presented the highest auc at the land use level across states whereas croplands showed the lowest auc largely because of the lower true positive rates in the australian capital territory and northern territory of 0 65 and 0 69 respectively grasslands had lower false positive rates across states fig 5 in general the accuracy of the unconstrained maximum likelihood allocation model at national and local level were significantly lower than the ones from the integrated model see supplementary materials 3 2 short term land change forecasting our land demand forecasting projected an increase of 18 7 in built up areas by 2030 compared to 2015 followed by a 3 4 increase in grassland conversely croplands and forests were forecast to reduce their extent by 8 4 and 7 respectively built up areas expanded by 68 3 54 4 and 36 8 in tasmania south australia and the australian capital territory respectively forest losses increased by 14 6 4 4 and 4 in queensland the northern territory and new south wales respectively fig 6 fig 7 shows the result of the three land use change forecasts for 2030 and the land use trajectories of the four selected focal areas the focal areas show different patterns built up expansion was projected to be more pronounced around melbourne fig 7a forest contraction was projected to occur in areas with recent high rates of deforestation i e queensland fig 7c on the other hand fig 7b and d illustrate more stable land use patterns 4 discussion we have developed and evaluated an integrated multiclass high resolution land change model and forecasted short term land use change for australia to 2030 the high overall accuracy at national state and sa2 level combined with the high auc values at state and land use level support the validity of the integrated land change model for producing short term continental scale land use forecasts at high spatial resolution the suitability and land use forecast dataset is freely available for exploration and download at https mioash users earthengine app view australialandchangeforecast the full code used is freely available at https github com mioash australia lu model 4 1 integrated land change modelling land change is a process where the complex interaction between the drivers of change impact on the pace direction and magnitude of change of multiple land use classes van vliet et al 2016 verburg et al 2019 our integrated high resolution multiclass model incorporates multiple factors influencing land change at different scales for calculating land use suitability and subsequently allocating land use to pixels in order to meet forecasted areal demands suitability layers were generated in gee by combining the land use of each pixel at each of five historical time steps current land use in the neighbourhood ranging from immediate to distant with different neighbourhood windows accessibility information and environmental drivers of change the combination of these variables proved to be sufficient for accurately predicting the spatial distribution of future multiclass land use contrary to other modelling frameworks where the neighbourhood accessibility or other variables need to be parameterized through an iterative and time intensive manual process hewitt et al 2015 roodposhti et al 2020 our integrated model assigned the corresponding weights to each one of these variables in an automated machine learning process furthermore we used a cloud based geospatial platform i e gee and high performance computing to process train and generate per class suitability layers and forecast land change at the continental level in weeks the continental extent of analysis as well as the spatial thematic and temporal resolution of our integrated multiclass model expands the current capabilities of most regional and global land change models projecting single land use types mostly urban e g huang et al 2019 calibrated using a single time stamp or at coarse spatial resolution e g liu et al 2017 lotze campen et al 2008 stehfest et al 2014 van vliet and verburg 2018 and focused at local scales e g cao et al 2019 van duynhoven and dragićević 2019 concurrently these first of their kind outputs can be used as inputs for land management at local areas without having to rely on downscaling approaches that could introduce further uncertainty in the initial and future conditions of the land system van vliet et al 2016 verburg et al 2019 4 2 land change model accuracy we have made uncertainty estimates available at different spatial scales i e national state and sa2 levels and thematic levels i e by land use types for highlighting areas and land use classes for which the model has higher skill in simulating land change as well as areas where its skill was lower our integrated model had a national overall accuracy of 93 8 and a local oa 85 across sa2s and states the sensitivity of the model for predicting land change was 69 4 with 84 6 of the changing pixels being correctly predicted furthermore at the level of land use type the ua and pa were 90 for forest and grassland whereas ua and pa for cropland and built up areas were 82 around 10 of cropland 8 of forest and 6 of built up areas were misclassified as grasslands in 2015 built up areas were incorrectly allocated in other land use classes at a rate of around 5 7 across states cropland presented the lowest auc particularly in the australian capital territory and the northern territory of 0 65 and 0 69 respectively these two states are the ones with the lowest fraction of cropland areas in australia 0 4 hence less suitable areas for this land use may have been predicted negatively impacting the model s performance in these regions fig 4 displays the spatial variation of the oa of the integrated model at the sa2 level low oa was generally found in areas in which the uncertainty of the reference land cover dataset was high corresponding to jurisdictions where grassland cropland and forest dominate the landscape see calderón loor et al 2021 in contrast while the maximum likelihood allocation approach achieved a national oa of 90 state and sa2 level the accuracies were highly variable reaching as low as 1 in sa2s where land changes were not captured by the model without having knowledge of the demand see supplementary materials this highlights the robustness of our integrated modelling where both land demand and pixel allocation are appropriately modelled leading to higher accuracies at all spatial scales a closer look at the four focal areas fig 3 shows the relatively high level of agreement between the simulated map and the 2015 validation map nonetheless some patterns can be distinguished in these areas first land use persistence i e where there is no change is accurately simulated due to the first step of the land use allocation method of transferring land use in year t to year t 1 for instance fig 3c and d shows the relatively low discrepancies between the simulated map and the 2015 land use map around southwest cairns and perth city and its hinterland respectively second misallocation is found in highly heterogeneous landscapes e g in the forest grassland cropland mosaic in the northern tablelands of new south wales fig 3c lastly built up land expansion patterns are in general well defined and accurately modelled in areas with relatively high increases in urban area such as west melbourne fig 3a spatio temporal inconsistencies in the underlying reference land change layers i e 1985 2015 are likely to be among the main factors influencing the sensitivity and allocation disagreements land use maps generated by calderón loor et al 2021 did not include any post processing and so include classification errors and spatiotemporal inconsistencies that affect our land use allocation accuracy foody 2010 reported a drop in the sensitivity of 14 when the accuracy of the reference layer was 95 showing the influence that the quality of the ground truth data has on the outputs similarly our model s sensitivity to predict land change may have been influenced by the resistance to change imposed by our modelling strategy and the calculated prevalence i e 3 2 divergences in the generated suitability layers are another potential contributing factor to the allocation mismatches for instance distances to certain land use classes might be exaggerating the suitability of neighbouring pixels to belong to that land use type e g pixels in the vicinity of built up areas receive higher suitability values for being transformed into built up areas over time uncertainty generated by error in the underlying land use dataset needs to be better quantified and its effects more comprehensively assessed in future work for instance sandler and rashford 2018 and olofsson et al 2020 propose methods for correcting and mitigating misclassification errors in the reference layers that could be applied when using imperfect reference land cover datasets nonetheless the calculated auc and sensitivity values indicate that our land change model performs considerably better than a random classifier pontius jr and schneider 2001 and therefore can be used for land use change forecasting 4 3 short term land change forecasting impacts and implications we used a compositional linear regression model for forecasting land demand until 2030 see supplementary data 2 the compositional time series demand model using time as the independent variable and the historical constrained proportions of the four land use classes as the dependent variables was able to incorporate the interactions among land use classes and to capture the observed past trends for projecting land use change to the best of our knowledge multiclass land use trajectories have not been estimated at this resolution and extent before and this assessment provides the most comprehensive depiction of continental scale future land change dynamics forecast land changes exhibit a variety of patterns across australia at the national level our short term forecast suggests that following recent trends cropland and forest areas decline through to 2030 whereas grassland and built up areas expand across the country in contrast forecasts from the ipcc s 6th assessment report ar6 byers et al 2022 suggest that forest cropland and built up areas will expand up to 2030 at 4 289 2989 and 127 km2 yr 1 respectively whereas pasture will experience a decline in area of 21 245 km2 yr 1 differences in the direction of the change might be related to any of the following factors how the different land classes were defined e g pasture vs grassland differences in initial conditions spatial resolution ar6 forecasts are only available at the country level model type and variables and the considered scenarios bau vs pathways from our study cropland encroachment a reduction of 8 4 from the initial 2015 cropland area around 3573 km2 yr 1 will be shared among the states supporting most of the cropland area i e new south wales queensland south australia and western australia fig 7b increases or no change in cropland area are expected in the other states non modelled drivers such as changes in agricultural land use intensification global crop and livestock demand policy and regulation urbanisation and commodity prices may lead to further decline in cropping area bryan et al 2016 heck et al 2018 lesslie and mewett 2018 marcos martinez et al 2017 forest losses 8250 km2 yr 1 at the national level occurred across all states except victoria with deforestation happening at higher rates in queensland 5800 km2 yr 1 the northern territory 940 km2 yr 1 and new south wales 700 km2 yr 1 forest areas will be mainly replaced by grassland which is expanding at a rate of 13 000 km2 yr 1 comparable historical trends have been reported by evans 2016 and calderón loor et al 2021 suggesting that besides wildfire agriculture e g cropland expansion and extensive cattle production will remain one of the main drivers of deforestation curtis et al 2018 similar drivers and annual deforestation rates have been recorded in new south wales over the past 40 years nsw government and department of planning 2021 where historical annual deforestation rates have been estimated to be as high as 600 km2 yr 1 which is aligned with our forecasted trends for the state 705 km2 yr 1 fig 7c likewise built up land expansion is expected to drive deforestation and land change in australia total built up areas for 2030 are forecast to reach 17 840 km2 an increase of 19 over the study period with increases ranging from 7 to 68 3 in new south wales and tasmania respectively for example fig 7a illustrated how urban expansion in melbourne will continue especially to the north and west in areas with low population densities rahnama et al 2020 exerting pressure on grassland and cropland areas understanding built up land dynamics is essential for assessing the direct and indirect impacts of urban expansion on ecosystems and the environment via habitat loss and fragmentation carbon losses and hydrological changes celik et al 2019 chen et al 2020 colsaet et al 2018 curtis et al 2018 song et al 2018 soonsawad et al 2022 the nature conservancy 2018 for instance using melbourne s 2030 urban growth boundary ugb department of infrastructure 2002 hahs and mcdonnell 2006 estimated a potential loss of nearly 550 indigenous plant species over the next 100 years caused by projected urban expansion results from this study can further enable spatially explicit integrated assessments of the impact of urbanization on key biodiversity areas eken et al 2004 and on biodiversity in general at high resolution kapitza et al 2021 rogers et al 2021 4 4 limitations although our model has reached a high degree of accuracy and has proven to be useful for forecasting land changes at continental scale some considerations need to be taken into account our modelling approach relies on the quality and availability of the input data such as historical land cover maps population estimations and climatic data intrinsic uncertainties errors or biases in these variables can propagate through the modelling process alexander et al 2016b burnicki et al 2007 liu and chun 2009 we encourage future users of this methodology make use of updated validated and harmonized datasets as well as conducting sensitivity analyses to assess the impact of data uncertainty on the results the temporal resolution of the forecasts 5 year time steps may not account for certain dynamic or nonlinear processes or feedbacks that occur at shorter time scales such as policy implementation or the effects of climatic extremes or natural disasters this constraint could be addressed by using extended historical land change at high temporal resolution such as annual or sub annual time step as part of the training and validation process and by generating dynamically updated land change forecasts at the same time steps however this would also increase the computational demand of the modelling approach therefore a trade off between temporal resolution and computational feasibility should be considered similarly the flexibility of our modelling framework should be assessed for its ability to accommodate various policy scenarios or interventions that could affect land change outcomes this limitation could be addressed by incorporating the capability to test and evaluate alternative policy options and their effects on land change although we have used our model for short term forecasting this modelling framework could be also suitable for longer term land use projections and scenario analysis long term projections will require the updating of the suitability layers at each time step so the number of suitable pixels for each land use is properly adjusted and available to meet the forecasted land use area demands under different scenarios e g the shared socioeconomic pathways or ssps o neill et al 2014 furthermore additional variables would need to be considered e g agricultural commodity demands from domestic and international markets and certain dynamic drivers such as population and climate will need to be dynamically updated reflecting how their values will change under future scenarios and how the changes in these variables will influence land use suitability thus mitigating the uncertainty of long term projections friedlingstein 2015 gao et al 2016 holman et al 2017 prestele et al 2016 simmons et al 2018b white and engelen 1997 5 conclusion modelling land change dynamics at continental scale and high spatial resolution requires an integrated modelling framework our integrated continental scale model for projecting multiclass land use change at high spatial resolution employed a two step approach for generating per class land use suitability layers based on maximum likelihood used for allocating land use change in an iterative process our approach leverages the power of cloud based and high performance computing platforms to produce a detailed representation of land change at the continental scale the whole of australia we found that the combination of diverse variables representing land change history neighbourhood accessibility and environmental drivers of change were able to achieve a high overall accuracy of 93 8 at the national level land use areal demand was forecasted and presented from 2020 until 2030 at 5 year time steps for each sa2 using a compositional regression analysis and allocated at 30 m pixel level using a maximum likelihood heuristic increases in demand occurred for grassland 13 000 km2 yr 1 and built up 220 km2 yr 1 land while decreases occurred for forest 9210 km2 yr 1 and cropland 4238 km2 yr 1 these first of their kind future land use maps for australia provide a comprehensive high resolution product representing likely short term land use trajectories that can be used for assessing the impacts of land use change and supporting improved land management planning and governance in australia funding this research was supported by the funding from the deakin university postgraduate research scholarship 0000019085 and the australian research council discovery dp170104795 funded by the australian government declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105749 
25390,predicting future land change is crucial in anticipating societal and environmental impacts and informing responses at different scales we designed an integrated high resolution land change model and forecasted australia s land change for the years 2020 2025 and 2030 for cropland forest grassland and built up land uses using cloud based and high performance computing a spatially explicit set of drivers was fed into a random forest classifier to generate 30 m per class suitability layers for the country which were then used for allocating land use the model was validated against 2015 data then land use was projected until 2030 accuracy at the national level was 94 forecasts showed increases in grassland and built up areas and decreases in forest and cropland our modelling framework expands the current capabilities of large scale land change models and provides a first of its kind multiclass land forecast for australia that can inform land policy at multiple scales in australia keywords land use change integrated model forecast random forest google earth engine data availability the code and the generated outputs are available for exploration and download 1 introduction land use and land cover hereafter simply land use transformations over large areas occur in response to changes in both global and local conditions song et al 2018 the nature conservancy 2018 modifications to land use hereafter simply land change cause a diverse range of direct and indirect impacts on land productivity climate hydrology biogeochemical cycling natural habitats and species diversity and other ecosystem services foley et al 2005 güneralp et al 2013 shukla et al 2019 song et al 2018 further transformative land change is expected to occur in response to future changes in human population climate diets commodity prices social norms financial incentives and land use policies planning regulation and governance alexander et al 2016a allan et al 2017 bizer 2005 gao and bryan 2017 hurtt et al 2011 riggio et al 2020 united nations 2017 understanding the likely impact of these variables on the type extent location and pace of land change at high resolution and over large scales is essential for managing and mitigating land change impacts bayer et al 2021 land change models are commonly used for analysing complex interactions between drivers of change and for representing the influence of these drivers on present and future land use grundy et al 2016 ren et al 2019 verburg et al 2016 despite the multiple theoretical frameworks and algorithms for modelling land change alexander et al 2016b prestele et al 2016 verburg et al 2013b in their most basic form land change models translate land system behaviour i e drivers and responses into mathematical equations chen et al 2020 doelman et al 2018 heck et al 2018 these equations aim to spatially represent the location and extent of current and future land use at different thematic and spatiotemporal resolutions doelman et al 2018 verburg et al 2019 wolff et al 2018 at the continental and global scale land change models e g landshift alcamo and schaldach 2006 globiom valin et al 2013 image stehfest et al 2014 clumondo van vliet and verburg 2018 gcam calvin et al 2019 magpie dietrich et al 2019 sleuth zhou et al 2019 and luto bryan et al 2016 are generally calibrated at coarse spatial resolution median pixel resolution 624 m van vliet et al 2016 with a single snapshot of land use e g pijanowski et al 2002 and or using a single land use type e g liang et al 2021 these limitations hinder the ability of models to capture and represent present and future land transformation processes operating at finer spatial scales than the modelling unit e g small patches landscape mosaics or highly heterogeneous biophysical regions all of which result in species loss due to habitat fragmentation franklin et al 2013 gillespie et al 2008 van vliet et al 2016 willis and whittaker 2002 global land change models are also limited in their ability to account for path dependencies and legacy effects of land change aspinall and hill 2007 azadi et al 2017 that are crucial for representing subtle or gradual temporal change such as cropping grazing rotations prestele et al 2016 and in the ability to represent multiple land use trajectories furthermore land management at local scales requires granular and spatially and temporally explicit multiclass land use information for ensuring that land dynamics are correctly represented within governmental jurisdictions hertel et al 2019 ren et al 2019 van vliet et al 2019 land change models relying solely on one modelling approach are able to capture and represent some aspects of the system whereas hybrid or integrated approaches are better suited for incorporating the complex non linear interactions at different spatial and temporal scales among drivers and the changes themselves de freitas et al 2018 gounaridis et al 2019 coupling regression based or machine learning algorithms with spatial characteristics of the neighbourhood hewitt et al 2014 roodposhti et al 2019 land change history vazquez quintero et al 2016 and other influential spatio temporal biophysical parameters in the modelling structure can better capture the complex land system dynamics and increase confidence in the outputs azadi et al 2017 pongratz et al 2018 machine learning algorithms have gained increased attention in land change modelling as they can handle large structured and unstructured datasets and unlike common statistical methods do not require a priori assumptions about the distribution and relationships among the variables motesharrei et al 2016 van vliet et al 2016 verburg et al 2011 2013a machine learning based land change models are commonly coupled with cellular automata and markov chain models e g hagenauer et al 2019 hsieh 2009 in a two stage process these approaches calculate land potential or suitability by employing a machine learning model that is later coupled with an allocation algorithm such as cellular automata for modelling land change multiclass models that incorporate these drivers as explanatory variables typically either use only the land change history van duynhoven and dragićević 2019 land change history and neighbourhood characteristics cao et al 2019 or neighbourhood characteristics and spatial drivers of change van vliet and verburg 2018 at the global or continental scale only a handful of models include in their structure all four key drivers of land change land change history neighbourhood accessibility here defined as the proximity to infrastructure and other land classes roodposhti et al 2019 2020 and spatially explicit environmental variables e g xing et al 2020 in addition the calibration of these models was either spatially coarse grained temporally restricted to a few or no historical time steps primarily centered on a small spatial extent or projecting single land use changes olmedo et al 2018 prestele et al 2016 moving towards a new generation of continental scale high resolution land change models requires the access to advanced computing infrastructure that allows the rapid processing of several variables and can operate in parallel on millions of pixels wang and yuan 2020 zhang et al 2023 zhuang et al 2020 furthermore standardized transparent and integrated approaches employing comprehensive sets of spatiotemporal drivers of land change at varying resolutions are required in order to better account for uncertainty in future land change projections bayer et al 2021 liu et al 2017 popp et al 2017 prestele et al 2016 verburg et al 2019 here we present an integrated continental scale high spatial resolution land change model we used a combination of a cloud based google earth engine gee gorelick et al 2017 and local high performance computing environments for training validating and forecasting land change at continental scale for australia nationwide land clearing driven by agricultural activities has shaped the current continental landscape historical land change has been closely linked to the introduction and changes in policies that have promoted and prevented the clearing rhodes et al 2017 simmons et al 2018a 2018b state based policies e g vegetation management act vma 1999 have further added complexity in how land has changed across australia simmons et al 2018a 2018b 2018c other land changes have also influenced forest dynamics and land configuration more broadly urban expansion has led to forest losses and irreversible land change across the country calderón loor et al 2021 expected increases in australia s population to around 26 30 million by 2030 nsw government and department of planning 2021 coupled with low population densities economic growth and incentives for infrastructure construction will influence the rates and course of urban expansion chen et al 2020 mcdonald et al 2013 seto et al 2012 land change modelling in the country is limited to monothematic coarse resolution models e g bryan et al 2016 ye et al 2019 which highlights the need for integrated models that can provide a comprehensive analysis of land dynamics at high granularity we used gee for developing individual suitability layers for four land use classes cropland forest grassland and built up land using a random forest rf algorithm historical land change information from 1985 to 2010 calderón loor et al 2021 was used for training the model and for calculating the historical and future land demand up to 2030 using compositional regression of past trends allocation was an iterative process based on the maximum likelihood approach that ensured all areal land demands were met we assessed the accuracy of our land change model against observed land use in 2015 and demonstrate the applicability of the model by producing short term continental scale land change forecasts at 5 year time steps from 2020 to 2030 2 methods 2 1 study area and land use data our land change model was calibrated and validated for the entire australian continent with an area of 7 688 287 km2 the country spans diverse climatic ecological and socio economic zones lesslie and mewett 2018 thackway et al 2013 two long term spatially explicit land use datasets are available for the country at 250 m resolution from 2001 to 2015 lymburner et al 2015 and at 30 m resolution from 1985 to 2015 at 5 year time steps calderón loor et al 2021 because of its granularity consistency and robustness we used the latter as our reference historical land use data each of the seven spatial land use layers contained around 8 5 billion pixels classified land use into six classes cropland forest grassland built up water and other we focussed on modelling land change dynamics between cropland forest grassland and built up land use classes water and other land use classes were considered to be constant over time by applying a mask to these areas two main reasons drove this decision a short forecast time window i e 10 years was not long enough for observing significant changes in these classes and the lack of dynamic predictor variables for modelling changes in these land classes 2 2 land change modelling the modelling was divided into three main components consolidation of the drivers of change generation of suitability layers and pixel allocation fig 1 2 2 1 drivers of change a comprehensive set of predictors at different spatiotemporal resolution was used to produce the 30 m individual suitability layers these predictors can be grouped into four main categories including historical path dependencies h neighbourhood dynamics n accessibility a and environmental factors influencing suitability e table 1 h variables provide a plausible set of land change trajectories based on past patterns cao et al 2019 zhang et al 2020 n variables represent the proportion of each land class in a specified neighbourhood three spatial windows were defined 3 3 9 9 and 81 81 pixels to capture the relationships e g attraction and repulsion hewitt et al 2014 between the state of the centre pixel and the state of its surroundings at different spatial scales at t 0 a variables captured the influence of the proximity of certain land use classes and infrastructure networks on land changes roodposhti et al 2020 lastly e variables capture a diverse array of spatially explicit factors influencing land use suitability noszczyk 2019 van vliet and verburg 2018 a similar dataset has been employed by marcos martinez et al 2018 and ye et al 2019 for analysing forest cover dynamics in australia where c cropland f forest g grassland b built up h historical path dependency drivers n neighbourhood drivers a accessibility drivers e environmental drivers sources 1 calderón loor et al 2021 2 commonwealth department of the environment 2014 3 xu and hutchinson 2011 4 viscarra rossel et al 2014 5 facebook connectivity lab and center for international earth science information network ciesin columbia university 2016 6 olson et al 2001 7 farr et al 2007 the complete set of variables was uploaded to gee were n variables and distances were calculated at each time step see the shared code and supplementary materials data processing and modelling specifications variables were resampled to the same pixel resolution i e 30 m discrete variables i e state id statistical area level id and ecoregion id where included as factors during the training process 2 2 2 land use suitability modelling we used random forest rf regression models for generating independent probability based suitability layers rf is a machine learning algorithm that constructs a user specified number of decision trees the outputs of each individual decision tree are averaged for calculating the individual suitability layers denisko and hoffman 2018 gounaridis et al 2019 each continental scale suitability layer is a continuous probability surface at 30 m with values from 0 to 100 where higher probabilities represent a higher confidence of a pixel to transition to a certain land use class we used the 2010 land use from calderón loor et al 2021 as our dependent variable whereas the historical land use data from 1985 to 2005 i e five historical time steps were used as part of the predictor variables for training the land change model i e h variables four rf algorithms one for each land use type were trained using gee for each rf model we removed the least significant variables i e where the mean accumulation of the impurity decrease within each tree was 0 1 from the training set to avoid computational limits in gee the complete list of variables used for training the rf algorithms is listed in table 1 this fine tuning process was conducted locally using the scikit learn python package pedregosa et al 2011 training points were generated following a stratified random approach the strata were defined according to the number of historical land transitions from 1985 to 2005 for each pixel i e no temporal change i e 0 historical transitions medium change i e 1 3 historical transitions and high land change i e 3 historical transitions inside each stratum approximately 25 000 points were randomly located summing up to a total of 75 000 80 000 points in each land use the number of trees for each rf model was set to 150 and the number of variables per split mtry was equal to the square root of the number of predictors following the hyperparameters used by calderón loor et al 2021 the trained rf models were used for producing individual suitability layers for the year 2015 h n and a distances to land use classes variables were updated and used for the suitability prediction using the generated suitability layers a maximum likelihood approach unconstrained by future land use area demand was used for producing a map for the year 2015 and later employed for testing the accuracy of this simpler allocation approach prior to introducing land use area demand constraints see supplementary materials 2 2 3 demand calculation and allocation to allocate land use to pixels we started by calculating the areal land demand for each land use type at statistical area level 2 sa2 sa2s are non overlaying functional areas that represent a community that interacts together socially and economically and are defined under the australian statistical geography standard asgs abs 2016 observed land transitions were used to calculate historical demand i e for the period 1985 2015 in gee land demand forecasts were calculated for 2020 2025 and 2030 using a time series compositional regression leininger et al 2013 for each sa2 a regression model with compositional response was fitted using the year as the independent variable and the constrained proportions of the four land use classes at each time step as the dependent variables a model with compositional response assumes that a composition y all its elements add 1 is a linear function of different explanatory variables x 0 x 1 x p eq 1 mateu figueras and pawlowsky glahn 2008 van den boogaart k et al 2021a eq 1 y ˆ i 0 p x i ʘ b i y n φ d y ˆ σ ε where n φ d y ˆ σ ε stands for the normal distribution of the simplex of y for simplicity purposes the logratio transformation of eq 1 can help to represent the model as a multivariate linear regression model eq 2 eq 2 y ˆ i 0 p x i b i y n d 1 y ˆ σ ε where b 0 b 1 b p are the slopes and the residual covariance matrix σ ε compositional regression was performed using the compositions package van den boogaart k et al 2021b in r r core team 2021 an adjustment of forecasted land demand was made prior to the allocation process to avoid exceeding the maximum number of suitable pixels of each land use type at sa2 level using the following heuristic at each time step the maximum forecasted demand could not be higher than 1 5 times the number of pixels with a suitability 40 pixels were allocated in a two step process first land use persistence was modelled by transferring the land use type of each pixel from t0 into t1 then the land use area from the initial allocated pixels in t1 was compared with the calculated or observed demand for the same time step an automated iterative process was then employed for allocating pixels into under allocated land use classes by removing pixels from overallocated land use classes pixels with the lowest suitability for the over allocated land use class were allocated into a new under allocated land use class with the highest suitability the allocation process ended when demands were satisfied for all land use classes 2 3 accuracy assessment the land use map output for the year 2015 was compared against a validation land use map for 2015 and used to calculate the accuracy of the model at different spatial scales national state and sa2 level a confusion matrix was used for calculating the overall producer and user accuracy oa sum of the pixels that were correctly classified divided by the total number of pixels pa and ua respectively as well as the omission and commission errors oe 100 pa and ce 100 ua respectively of the land change model at national level olofsson et al 2013 2014 to provide a better representation of accuracy at different spatial scales the oa was also calculated at state and sa2 levels furthermore the ability of the land change model for modelling different land use classes was evaluated by producing receiver operating characteristics roc plots at state level conceived as a probability curve roc plots present the false positive rate fpr versus true positive rate tpr for summarizing the ability of the land change model to distinguish between land use classes pontius jr and schneider 2001 the area under the curve auc was computed for each land use class and then micro averaged to account for the imbalance in the dataset for calculating the multi class auc at state level safitri et al 2021 wu et al 2022 lastly we calculated the sensitivity s 1 i e the proportion of pixels correctly classified as having changed eq 3 specificity s 2 i e the proportion of pixels correctly classified as having not changed eq 4 and prevalence θ that is the proportion of pixels that changed between 2010 and 2015 eq 5 calderón loor et al 2021 foody 2010 table 2 we isolated the a pixels i e total hits and calculated the proportion of pixels that changed and were predicted to the right land class 3 results 3 1 suitability and simulated land use four independent suitability layers were simulated for 2015 fig 2 each map presents the likelihood of each land use class to be present in any given pixel in general the suitability layers present high concordance with the actual distribution of each land use class using the generated suitability layers as inputs for the allocation algorithm a simulated 2015 map was produced fig 3 a visual comparison between the reference and the predicted map in addition to visual inspections of selected sites demonstrates the high degree of similarity between the maps and the accuracy of the model in predicting the location and extent of land change readers can visualize the produced layers in the following gee app https mioash users earthengine app view australialandchangeforecast overall accuracy of the final simulated 2015 map was estimated at 93 8 in comparison to the accuracy of the unconstrained maximum likelihood approach which was 90 see supplementary materials grassland and forest classes presented the lowest commission and omission errors of 4 and 9 respectively conversely cropland and built up areas showed the highest commission and omission errors both exceeding 14 table 3 confusion occurred between cropland and grassland whereas built up areas were incorrectly simulated as cropland forest and grassland in similar proportions at the national level sensitivity s1 was estimated to 69 4 specificity s2 was 94 7 and the prevalence θ was 3 2 the proportion of changing pixels between 2010 and 2015 for which the correct land class was predicted was 84 6 where pa producer accuracy ua user accuracy ce commission errors oe omission errors at state level the oa for 2015 varied between 86 3 and 98 7 in new south wales and south australia respectively average oa accuracies at sa2 level presented the same patterns as the oa at state level furthermore around 95 of the sa2s had oa accuracies 70 across states table 4 fig 4 see also supplementary data 1 auc at state level varied between 0 91 and 0 97 higher errors i e lower true positive and higher false positive rates were found in new south wales queensland and victoria in general forest presented the highest auc at the land use level across states whereas croplands showed the lowest auc largely because of the lower true positive rates in the australian capital territory and northern territory of 0 65 and 0 69 respectively grasslands had lower false positive rates across states fig 5 in general the accuracy of the unconstrained maximum likelihood allocation model at national and local level were significantly lower than the ones from the integrated model see supplementary materials 3 2 short term land change forecasting our land demand forecasting projected an increase of 18 7 in built up areas by 2030 compared to 2015 followed by a 3 4 increase in grassland conversely croplands and forests were forecast to reduce their extent by 8 4 and 7 respectively built up areas expanded by 68 3 54 4 and 36 8 in tasmania south australia and the australian capital territory respectively forest losses increased by 14 6 4 4 and 4 in queensland the northern territory and new south wales respectively fig 6 fig 7 shows the result of the three land use change forecasts for 2030 and the land use trajectories of the four selected focal areas the focal areas show different patterns built up expansion was projected to be more pronounced around melbourne fig 7a forest contraction was projected to occur in areas with recent high rates of deforestation i e queensland fig 7c on the other hand fig 7b and d illustrate more stable land use patterns 4 discussion we have developed and evaluated an integrated multiclass high resolution land change model and forecasted short term land use change for australia to 2030 the high overall accuracy at national state and sa2 level combined with the high auc values at state and land use level support the validity of the integrated land change model for producing short term continental scale land use forecasts at high spatial resolution the suitability and land use forecast dataset is freely available for exploration and download at https mioash users earthengine app view australialandchangeforecast the full code used is freely available at https github com mioash australia lu model 4 1 integrated land change modelling land change is a process where the complex interaction between the drivers of change impact on the pace direction and magnitude of change of multiple land use classes van vliet et al 2016 verburg et al 2019 our integrated high resolution multiclass model incorporates multiple factors influencing land change at different scales for calculating land use suitability and subsequently allocating land use to pixels in order to meet forecasted areal demands suitability layers were generated in gee by combining the land use of each pixel at each of five historical time steps current land use in the neighbourhood ranging from immediate to distant with different neighbourhood windows accessibility information and environmental drivers of change the combination of these variables proved to be sufficient for accurately predicting the spatial distribution of future multiclass land use contrary to other modelling frameworks where the neighbourhood accessibility or other variables need to be parameterized through an iterative and time intensive manual process hewitt et al 2015 roodposhti et al 2020 our integrated model assigned the corresponding weights to each one of these variables in an automated machine learning process furthermore we used a cloud based geospatial platform i e gee and high performance computing to process train and generate per class suitability layers and forecast land change at the continental level in weeks the continental extent of analysis as well as the spatial thematic and temporal resolution of our integrated multiclass model expands the current capabilities of most regional and global land change models projecting single land use types mostly urban e g huang et al 2019 calibrated using a single time stamp or at coarse spatial resolution e g liu et al 2017 lotze campen et al 2008 stehfest et al 2014 van vliet and verburg 2018 and focused at local scales e g cao et al 2019 van duynhoven and dragićević 2019 concurrently these first of their kind outputs can be used as inputs for land management at local areas without having to rely on downscaling approaches that could introduce further uncertainty in the initial and future conditions of the land system van vliet et al 2016 verburg et al 2019 4 2 land change model accuracy we have made uncertainty estimates available at different spatial scales i e national state and sa2 levels and thematic levels i e by land use types for highlighting areas and land use classes for which the model has higher skill in simulating land change as well as areas where its skill was lower our integrated model had a national overall accuracy of 93 8 and a local oa 85 across sa2s and states the sensitivity of the model for predicting land change was 69 4 with 84 6 of the changing pixels being correctly predicted furthermore at the level of land use type the ua and pa were 90 for forest and grassland whereas ua and pa for cropland and built up areas were 82 around 10 of cropland 8 of forest and 6 of built up areas were misclassified as grasslands in 2015 built up areas were incorrectly allocated in other land use classes at a rate of around 5 7 across states cropland presented the lowest auc particularly in the australian capital territory and the northern territory of 0 65 and 0 69 respectively these two states are the ones with the lowest fraction of cropland areas in australia 0 4 hence less suitable areas for this land use may have been predicted negatively impacting the model s performance in these regions fig 4 displays the spatial variation of the oa of the integrated model at the sa2 level low oa was generally found in areas in which the uncertainty of the reference land cover dataset was high corresponding to jurisdictions where grassland cropland and forest dominate the landscape see calderón loor et al 2021 in contrast while the maximum likelihood allocation approach achieved a national oa of 90 state and sa2 level the accuracies were highly variable reaching as low as 1 in sa2s where land changes were not captured by the model without having knowledge of the demand see supplementary materials this highlights the robustness of our integrated modelling where both land demand and pixel allocation are appropriately modelled leading to higher accuracies at all spatial scales a closer look at the four focal areas fig 3 shows the relatively high level of agreement between the simulated map and the 2015 validation map nonetheless some patterns can be distinguished in these areas first land use persistence i e where there is no change is accurately simulated due to the first step of the land use allocation method of transferring land use in year t to year t 1 for instance fig 3c and d shows the relatively low discrepancies between the simulated map and the 2015 land use map around southwest cairns and perth city and its hinterland respectively second misallocation is found in highly heterogeneous landscapes e g in the forest grassland cropland mosaic in the northern tablelands of new south wales fig 3c lastly built up land expansion patterns are in general well defined and accurately modelled in areas with relatively high increases in urban area such as west melbourne fig 3a spatio temporal inconsistencies in the underlying reference land change layers i e 1985 2015 are likely to be among the main factors influencing the sensitivity and allocation disagreements land use maps generated by calderón loor et al 2021 did not include any post processing and so include classification errors and spatiotemporal inconsistencies that affect our land use allocation accuracy foody 2010 reported a drop in the sensitivity of 14 when the accuracy of the reference layer was 95 showing the influence that the quality of the ground truth data has on the outputs similarly our model s sensitivity to predict land change may have been influenced by the resistance to change imposed by our modelling strategy and the calculated prevalence i e 3 2 divergences in the generated suitability layers are another potential contributing factor to the allocation mismatches for instance distances to certain land use classes might be exaggerating the suitability of neighbouring pixels to belong to that land use type e g pixels in the vicinity of built up areas receive higher suitability values for being transformed into built up areas over time uncertainty generated by error in the underlying land use dataset needs to be better quantified and its effects more comprehensively assessed in future work for instance sandler and rashford 2018 and olofsson et al 2020 propose methods for correcting and mitigating misclassification errors in the reference layers that could be applied when using imperfect reference land cover datasets nonetheless the calculated auc and sensitivity values indicate that our land change model performs considerably better than a random classifier pontius jr and schneider 2001 and therefore can be used for land use change forecasting 4 3 short term land change forecasting impacts and implications we used a compositional linear regression model for forecasting land demand until 2030 see supplementary data 2 the compositional time series demand model using time as the independent variable and the historical constrained proportions of the four land use classes as the dependent variables was able to incorporate the interactions among land use classes and to capture the observed past trends for projecting land use change to the best of our knowledge multiclass land use trajectories have not been estimated at this resolution and extent before and this assessment provides the most comprehensive depiction of continental scale future land change dynamics forecast land changes exhibit a variety of patterns across australia at the national level our short term forecast suggests that following recent trends cropland and forest areas decline through to 2030 whereas grassland and built up areas expand across the country in contrast forecasts from the ipcc s 6th assessment report ar6 byers et al 2022 suggest that forest cropland and built up areas will expand up to 2030 at 4 289 2989 and 127 km2 yr 1 respectively whereas pasture will experience a decline in area of 21 245 km2 yr 1 differences in the direction of the change might be related to any of the following factors how the different land classes were defined e g pasture vs grassland differences in initial conditions spatial resolution ar6 forecasts are only available at the country level model type and variables and the considered scenarios bau vs pathways from our study cropland encroachment a reduction of 8 4 from the initial 2015 cropland area around 3573 km2 yr 1 will be shared among the states supporting most of the cropland area i e new south wales queensland south australia and western australia fig 7b increases or no change in cropland area are expected in the other states non modelled drivers such as changes in agricultural land use intensification global crop and livestock demand policy and regulation urbanisation and commodity prices may lead to further decline in cropping area bryan et al 2016 heck et al 2018 lesslie and mewett 2018 marcos martinez et al 2017 forest losses 8250 km2 yr 1 at the national level occurred across all states except victoria with deforestation happening at higher rates in queensland 5800 km2 yr 1 the northern territory 940 km2 yr 1 and new south wales 700 km2 yr 1 forest areas will be mainly replaced by grassland which is expanding at a rate of 13 000 km2 yr 1 comparable historical trends have been reported by evans 2016 and calderón loor et al 2021 suggesting that besides wildfire agriculture e g cropland expansion and extensive cattle production will remain one of the main drivers of deforestation curtis et al 2018 similar drivers and annual deforestation rates have been recorded in new south wales over the past 40 years nsw government and department of planning 2021 where historical annual deforestation rates have been estimated to be as high as 600 km2 yr 1 which is aligned with our forecasted trends for the state 705 km2 yr 1 fig 7c likewise built up land expansion is expected to drive deforestation and land change in australia total built up areas for 2030 are forecast to reach 17 840 km2 an increase of 19 over the study period with increases ranging from 7 to 68 3 in new south wales and tasmania respectively for example fig 7a illustrated how urban expansion in melbourne will continue especially to the north and west in areas with low population densities rahnama et al 2020 exerting pressure on grassland and cropland areas understanding built up land dynamics is essential for assessing the direct and indirect impacts of urban expansion on ecosystems and the environment via habitat loss and fragmentation carbon losses and hydrological changes celik et al 2019 chen et al 2020 colsaet et al 2018 curtis et al 2018 song et al 2018 soonsawad et al 2022 the nature conservancy 2018 for instance using melbourne s 2030 urban growth boundary ugb department of infrastructure 2002 hahs and mcdonnell 2006 estimated a potential loss of nearly 550 indigenous plant species over the next 100 years caused by projected urban expansion results from this study can further enable spatially explicit integrated assessments of the impact of urbanization on key biodiversity areas eken et al 2004 and on biodiversity in general at high resolution kapitza et al 2021 rogers et al 2021 4 4 limitations although our model has reached a high degree of accuracy and has proven to be useful for forecasting land changes at continental scale some considerations need to be taken into account our modelling approach relies on the quality and availability of the input data such as historical land cover maps population estimations and climatic data intrinsic uncertainties errors or biases in these variables can propagate through the modelling process alexander et al 2016b burnicki et al 2007 liu and chun 2009 we encourage future users of this methodology make use of updated validated and harmonized datasets as well as conducting sensitivity analyses to assess the impact of data uncertainty on the results the temporal resolution of the forecasts 5 year time steps may not account for certain dynamic or nonlinear processes or feedbacks that occur at shorter time scales such as policy implementation or the effects of climatic extremes or natural disasters this constraint could be addressed by using extended historical land change at high temporal resolution such as annual or sub annual time step as part of the training and validation process and by generating dynamically updated land change forecasts at the same time steps however this would also increase the computational demand of the modelling approach therefore a trade off between temporal resolution and computational feasibility should be considered similarly the flexibility of our modelling framework should be assessed for its ability to accommodate various policy scenarios or interventions that could affect land change outcomes this limitation could be addressed by incorporating the capability to test and evaluate alternative policy options and their effects on land change although we have used our model for short term forecasting this modelling framework could be also suitable for longer term land use projections and scenario analysis long term projections will require the updating of the suitability layers at each time step so the number of suitable pixels for each land use is properly adjusted and available to meet the forecasted land use area demands under different scenarios e g the shared socioeconomic pathways or ssps o neill et al 2014 furthermore additional variables would need to be considered e g agricultural commodity demands from domestic and international markets and certain dynamic drivers such as population and climate will need to be dynamically updated reflecting how their values will change under future scenarios and how the changes in these variables will influence land use suitability thus mitigating the uncertainty of long term projections friedlingstein 2015 gao et al 2016 holman et al 2017 prestele et al 2016 simmons et al 2018b white and engelen 1997 5 conclusion modelling land change dynamics at continental scale and high spatial resolution requires an integrated modelling framework our integrated continental scale model for projecting multiclass land use change at high spatial resolution employed a two step approach for generating per class land use suitability layers based on maximum likelihood used for allocating land use change in an iterative process our approach leverages the power of cloud based and high performance computing platforms to produce a detailed representation of land change at the continental scale the whole of australia we found that the combination of diverse variables representing land change history neighbourhood accessibility and environmental drivers of change were able to achieve a high overall accuracy of 93 8 at the national level land use areal demand was forecasted and presented from 2020 until 2030 at 5 year time steps for each sa2 using a compositional regression analysis and allocated at 30 m pixel level using a maximum likelihood heuristic increases in demand occurred for grassland 13 000 km2 yr 1 and built up 220 km2 yr 1 land while decreases occurred for forest 9210 km2 yr 1 and cropland 4238 km2 yr 1 these first of their kind future land use maps for australia provide a comprehensive high resolution product representing likely short term land use trajectories that can be used for assessing the impacts of land use change and supporting improved land management planning and governance in australia funding this research was supported by the funding from the deakin university postgraduate research scholarship 0000019085 and the australian research council discovery dp170104795 funded by the australian government declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105749 
25391,fortran software named heavy was developed to simulate gravity change due to water storage change in modflow groundwater models heavy is compatible with modflow 2005 and modflow nwt models using the layer property flow or upstream weighting packages all of the necessary information for the gravity calculation the geometry of the model cells the storage coefficient and head change is present within the existing modflow model files and no additional information is necessary gravity change is calculated at each time step for each layer at user specified locations or at a grid of hypothetical positions across the model the software has been validated using analytical gravity solutions and three example modflow models are included for demonstration heavy leverages the input output routines from modflow and is orders of magnitude faster than previous efforts using interpreted languages such as python or matlab the objective of the software is to facilitate repeat microgravity field measurements for groundwater flow model calibration keywords modflow hydrogeophysics gravity repeat microgravity data availability the link to download code is included in the manuscipt 1 introduction groundwater flow model evaluation and calibration depends on comparing model output with observations of head streamflow and other field data that is history matching one relatively new observation type is repeat microgravity which provides unique information about change in groundwater storage by measuring the change in acceleration due to earth s gravity as water mass is removed from or recharged into a gravity meter s region of sensitivity the force of gravity decreases or increases the region of sensitivity is generally large extending to a radial distance about 10 times the distance between the gravity meter and the water table in this way repeat microgravity provides spatially averaged observations at a scale relevant to groundwater models with grid cell sizes of hundreds of meters most transient groundwater modeling studies focus on the simulation of groundwater level head changes several packages are available for extracting and comparing simulated head from models and observed groundwater level measurements for the purpose of model evaluation and calibration hyd hanson and leake 1999 oc harbaugh et al 2000 hob hill et al 2000 in this paper we present a gravity observation postprocessor similar to the head observation package hob for the purpose of simulating gravity change at observation locations in turn simulated gravity change can be compared to repeat microgravity field measurements observations to calculate an objective function for model evaluation and calibration repeat microgravity is a natural fit for groundwater model analysis because all of the information necessary for the gravity calculation model geometry head change and storage properties exists in the model itself with no additional parameters several recent studies have demonstrated the value of terrestrial repeat microgravity observations for groundwater flow modeling and monitoring these data can be used directly to map storage change carruth et al 2018 to estimate specific yield i e the relation between storage and groundwater level change at collocated monitoring wells pool and eychaner 1995 or to estimate model parameters through inversion kennedy et al 2016 2021a adoption of the repeat microgravity method is advancing with improvements in field methods kennedy et al 2021b and new quantum ménoret et al 2018 and microelectronic carbone et al 2020 sensors although gravity measurements from space e g nasa s gravity recovery and climate experiment satellites can also be used for groundwater model calibration and verification sun et al 2012 rateb et al 2020 the spatial scale of satellite derived storage changes are too coarse to be relevant to the cell by cell calculations presented here to facilitate the use of repeat microgravity field measurements for groundwater flow model calibration and in particular the use of automated parameter estimation tools such as pest white et al 2020 we present fortran source code for a modflow forward gravity post processor only one optional additional file is needed beyond the standard modflow files to specify observation locations the software is made available as a compiled windows executable and the source code is readily compiled for use on linux and or high performance computing machines 2 methods 2 1 overview of gravity equations the simplest approximation for forward modeling gravity change from terrestrial water storage change is the horizontal infinite slab bouguer slab approximation 1 g 2 π g ρ h where g is the gravitational constant 6 6743 10 11 m3 kg 1 s 2 ρ is the density of the slab where water storage change occurs for water often assumed to be 1 0 g cm 3 and h is the slab thickness gravity change is calculated by considering the change in density from one time period to another for hydrologic applications where water content change is uniform throughout the slab possible examples include a water table moving up or down in an unconfined aquifer or soil water content changes at the land surface ρ and h are related 2 d ρ h where d is the equivalent thickness of free standing water in other words for a given amount of water storage change if the slab thickness h increases the density decreases the slab represents a horizontal layer of porous media if the media is fully saturated or desaturated owing to water storage changes the density ρ represents the aquifer effective porosity or specific yield the bouguer slab approximation can be improved by considering the three dimensional nature of water storage changes if storage changes occur at the land surface they can be represented as a thin layer spread across the local topography halloran 2022 storage changes in subsurface aquifers can be simulated by three dimensional flow models such as modflow harbaugh et al 2000 and in turn used to simulate the resultant change in gravity at the land surface or any other location known as forward modeling forward gravity modeling from modflow simulated storage change involves calculating the sum of the gravitational attraction of each model cell relative to a specific location such as an observation location where gravity measurements are made many different gravity analytical solutions exist for different mass geometry including for rectangular prisms forsberg 1984 point masses torge 1989 spherical prisms uieda et al 2016 and wedge shaped prisms halloran 2022 here we build on the work of leirião et al 2009 who presented a metric f 2 based on the dimensions of a rectangular model cell relative to the gravimeter position to determine the appropriate gravity formula 3 f 2 r 2 d r 2 where r is the distance from the gravity meter to the prism center and d r 2 δ x 2 δ y 2 δ z 2 is a measure of the prism size defined by the length of the three sides δ x δ y and δ z fig 1 leirião et al 2009 recommended that the prism formula also known as the forsberg 1984 equation can be used if f 2 is less than 4 4 δ g γ p s y x log y r y log x r z tan 1 x y z d x p 2 x p 1 y p 2 y p 1 z p 2 z p 1 where γ is the gravitational constant ρs y is the cell density change water density times specific yield and x p 1 x p 2 y p 1 y p 2 z p 1 z p 2 are the cartesian distances between the cell corners and the gravity station for large distances between the model cell and gravity station the difference between the integration bounds in equation 4 can become small and the formula is unstable for values of f 2 greater than 81 leirião et al 2009 recommends the point mass formula 5 δ g γ p s y δ h l i h l i h 2 x 2 y 2 3 2 δ x δ y where h is the average hydraulic head for unconfined layer cells or cell midpoint for confined layer cells δ h is the change in head between time steps and l i is the vertical distance between the microgravity instrument at location i and the hydraulic head elevation for intermediate values of f 2 between 4 and 81 leirião et al 2009 recommends an approximation of equation 2 macmillan 1958 for computational efficiency in our testing however there was little performance benefit from using the macmillan 1958 formula as compared to the more accurate prism formula and the latter was used for f 2 values less than 81 the forward gravity calculation equations 4 and 5 is carried out for each model cell in each layer each gravity meter position and each time step the horizontal geometry of each cell is defined by the modflow model and the vertical geometry by the confined or unconfined nature of the cell fig 1 the simulated gravity change value at an observation location is the sum of all model cells at a given time step relative to the sum of all cells at the initial time step 2 2 gravity from modflow output heavy kennedy and larsen 2023 leverages existing fortran input output routines for modflow harbaugh 2005 which ensures compatibility with model files and output from modflow 2005 and modflow nwt executables provided by the u s geological survey and other sources heavy should also run with other models using compatible name files and head output e g modflow ohwm boyce et al 2020 boyce 2022 a microsoft windows executable is provided in the project repository the software can be compiled using gfortran or intel compilers or with the pymake python program https github com modflowpy pymake speed of the compiled executable was similar with either the gfortran or intel compiler heavy reads model properties from the upstream weighting package upw or layer property flow lpf package the gravity change caused by water storage change in any subsurface media porous media fracture flow in hard rock aquifers flow in karst aquifers can be simulated by heavy as long as the upw or lpf packages are appropriate other flow packages such as block centered flow package bcf or hydrogeologic unit flow huf are not supported the water retained outside the modflow flow packages upw lpf is not included in the gravity calculation for example the unsaturated zone flow uzf lake lak and streamflow routing sfr packages can be used but their effect on gravity is ignored like modflow heavy reads model file names from a name nam file this can be the same name file used for modflow but must include a hvy file that specifies the output file number and observation locations if not using the g option heavy is compatible with all methods for specifying model parameters including zones and multiplier arrays heavy depends only on model geometry head change and storage properties other properties such as hydraulic conductivity are ignored head change is read for all time steps present in the modflow binary head file specified in the heavy name file time steps must be specified in the output control file provided to modflow when carrying out the gravity calculation for a given model cell heavy first determines if the cell is confined in modflow cells with constant transmissivity or unconfined variable transmissivity based on the head elevation relative to the top cell coordinate fig 1 and the convertible layer flag if the cell is in a layer that is not convertible and or the head elevation is higher than the cell it is considered confined for the purpose of the gravity calculation in this case the cell geometry is defined by the model coordinates and the density change within the cell is calculated from the head change and specific storage expressed as storage capacity in modflow for that model cell density change refers to the change in the mass of water within a modflow model cell water density change from salinity or temperature change is not considered nor is aquifer compressibility if the head elevation is within a cell in a convertible layer the cell is considered unconfined and the geometry for the purpose of the gravity calculation is defined by the beginning and ending head assuming this volume drains or fills completely within the time step fig 1 the density change within the cell is calculated from the head change and specific yield if beginning and ending head are below the bottom of a cell the gravity response for that cell is not calculated heavy accommodates models that can have the water table pass through multiple layers that is convertible layers that switch between confined and unconfined fig 2 the appropriate storage parameter specific yield or specific storage is selected based on the head elevation relative to the top of the cell described above any cells that transition from confined to unconfined during a time step or from unconfined to confined are treated as unconfined for that time step because the cumulative gravity calculation is usually dominated by unconfined storage by using the l command line option table 1 the calculation can be limited to a single layer thereby reducing computation time by default heavy reports the change in gravitational attraction of each layer individually and the cumulative change gravity observation locations in the hvy file are specified in model coordinates to facilitate rapid analysis without needing to specify observation locations by using the g command line option heavy can generate an evenly spaced grid of gravity stations across the model domain in this case any observation locations in the hvy file are ignored the z coordinate of the gravity station is the elevation of the encompassing cell from the model top dataset the user must verify model top represents the land surface additional command line options specify the number of observation locations in the x and y directions and allow the user to specify locations over a subset of the model domain see the user s guide for more information kennedy and larsen 2023 heavy output is written to a txt file with one row per observation location per time step each row includes the simulated gravity change for each layer in microgal and the total change for all layers this output can be directly integrated in an automated parameter estimation workflow such as pest white et al 2020 by writing an appropriate pest instruction file or can be further processed such as by creating a unique identifier for each observation at each time step before use in parameter estimation two python programs are included for visualization using matplotlib one program g animation py outputs a time series animation of simulated gravity change when the g option is used with heavy to generate a grid of gravity stations the second program g timeseries py creates a time series plot showing gravity change at all model cells 3 results 3 1 comparison with analytical solutions several test cases are included with heavy using the q command line option table 1 first the individual subroutines for the prism equation 4 and point mass equation 5 formulas are tested the prism formula solutions are calculated for a single prism representing a horizontal layer 1 10 m5 square and for the same layer discretized into 10 10 m prisms agreement with the bouguer slab approximation is good with less than or equal to 0 008 μga l difference table 2 the point mass solution is calculated for a single prism 20 20 20 m 100 m below an observation locations this solution agrees well 0 037 μga l difference with newton s law of gravitation solution equation 5 for a sphere of equivalent volume and density change table 2 the difference is likely related to the short distance 100 m between the observation location and the prism relative to the size of the prism 20 m square 3 2 modflow examples three transient modflow models with input and output head files are included in the software release kennedy and larsen 2023 a single layer modflow nwt model of the all american canal in southeast california usa simulates the dissipation of a groundwater mound following the concrete lining of the canal to reduce seepage losses wildermuth and kennedy 2022 model cell size is 250 m and the simulation spanned the years 1941 2020 this model was used to demonstrate model calibration using pest white et al 2020 with a field repeat microgravity dataset kennedy et al 2021a large rapid groundwater level and gravity decreases were observed after the canal was lined fig 3 for software validation heavy output was compared to equivalent gravity forward modeling routines implemented independently in python modflow files were read with flopy bakker et al 2022 and the gravity calculation carried out with code included in the software release the differences between python and heavy simulation table 2 are small compared to the accuracy with which gravity changes were measured 5 10 μga l in the study area using absolute gravimetry a nine layer modflow 2005 model covering an 80 km by 160 km region of the rio grande valley near albuquerque new mexico demonstrates layer property flow and complex use of zones and arrays to define layer properties myers and friesz 2019 the top four model layers are convertible and the bottom five are confined model cell size was 300 m and the simulation period spanned from predevelopment to future conditions 1900 2050 groundwater flow occurs primarily from mountain block and mountain front recharge seepage from the rio grande and groundwater pumping the first two of these processes are mostly steady state and do not cause large storage changes whereas pumping and changes in pumping results in large storage and gravity changes in and around albuquerque fig 4 a in other parts of the model storage and gravity changes are near zero a 4 layer modflow nwt model from northwest arizona demonstrates upstream weighting and unconfined storage in multiple layers knight 2020a the model was constructed to simulate future pumping scenarios of up to 2 3 106 m3 per year including the extent and magnitude of groundwater storage change knight 2020b model cell size is 300 m and the model simulated flow between the years 1976 2140 model calibration and parameter estimation using groundwater levels was carried out with pest white et al 2020 storage changes in this model occur in response to groundwater pumping but pumping volumes storage change and gravity change are much smaller than in the rio grande valley model the complex pattern of predicted gravity change calculated over a subregion of the model using the g option fig 4b reflects the distribution of storage properties in the groundwater model which in turn were based on a detailed airborne electromagnetic survey ball 2020 simulations such as these are useful for planning gravity data collection by targeting areas with high and low expected gravity change 4 discussion much progress has been made in recent decades to incorporate automated parameter estimation for groundwater models reducing the need for expert local knowledge and trial and error parameter adjustments anderson et al 2015 repeat microgravity data are natural candidates for model calibration because predicted values are obtained directly from the model without requiring a petrophysical model to relate measurements to model derived values gravity change is a direct measurement of storage change a property of interest for groundwater flow modeling rather than a surrogate property e g electrical conductivity gravity measurements for comparison with heavy simulations can be standalone absolute gravity measurements or relative gravity measurements between stations crossley et al 2013 kennedy et al 2021b often relative and absolute gravity measurements are combined using least squares network adjustment hwang et al 2002 these surveys are typically carried out at discrete seasonal or annual intervals the measurement precision of gravity change i e between surveys ranges from about 0 024 to 0 14 m of water using the bouguer slab approximation to convert acceleration to meters of water 41 9 μga l 1 m of water assuming the density of water is 1 0 gc m 3 1 μga l 10 nm s 2 alternatively much more precise 1 μga l uncertainty continuous daily hourly or finer data can be collected using superconducting or spring based meters crossley et al 2013 these deployments are less common and typically require alternating current ac power but can be especially useful where groundwater storage is rapidly changing such as at artificial recharge facilities kennedy et al 2014 2016 when using field data i e a limited amount of data in sparse locations computation time for most real world groundwater models is on the order of seconds and is usually much faster than solving the groundwater flow model when generating gravity change over a grid of stations using the g command line option table 1 computation time can be much longer owing to the large number of floating point calculations that must be carried out m model cells n time steps i gravity stations however this analysis at a large number of stations on a grid is usually only carried out once or a few times for model screening and not repeatedly as with parameter estimation where gravity values need only to be predicted at measurement locations depth to water relative to model cell size is important to avoid stair stepping in the gravity residual kennedy et al 2021a if this value represented by f equation 3 is too small an artificial pattern is induced in the gravity forward calculation by the discretization of the modflow model fig 5 at present the user must evaluate the significance of this effect for their model a mitigating factor is that the repeat microgravity method depends only on the magnitude of gravity changes between time steps thus a model that may produce a stair step gravity field at individual time steps may also produce a smoothly changing gravity time series at an individual station because the stair step pattern is static automatic head interpolation could prevent this effect an alternative to predicting gravity change by forward modeling is to convert gravity change to one dimensional storage change using the bouguer slab approximation this conversion from acceleration to equivalent thickness of water is similar to that made for satellite data from the grace mission rateb et al 2020 for the all american canal model fig 3 with significant mounding from recharge but relatively high depth to water greater than 30 m the departure from the infinite slab approximation was small two percent or less for the full three dimensional forward modeled signal kennedy et al 2021a for other models three dimensional effects have been significant e g kennedy et al 2016 one disadvantage to the converting gravity change to storage change approach is that vertically integrated storage change across all model layers is not typically included in model output and must be calculated another is that the spatial scale of the gravity measurement must be considered a storage change estimate is obtained at the measurement location but at how many modflow cells should that apply and with what weighting 5 conclusion a fortran modflow post processor named heavy was developed to simulate the change in gravitational attraction at user defined locations or at a grid of locations the software is intended to facilitate the use of field data collected using microgravity instruments absolute relative superconducting quantum and other technologies for model calibration using automated tools such as pest white et al 2020 several improvements are possible for heavy these include accommodating unstructured grids and triangular discretization made possible in modflow 6 more detailed discretization leads to longer run times but two solutions are possible first the gravity calculation is easily parallelized and multi core pcs or high performance computing can be applied at present heavy runs on a single core second for cells within the model that don t change position i e confined layers but not unconfined layers where the cell coordinates depend on head the gravity effect of any one cell is simply s i j k mass change where s i j k is a sensitivity that is calculated once for each gravimeter location model cell pair and can be stored and reused for future time steps third heavy does not consider subsidence but gravity measurements at the land surface are affected by changes in elevation caused by subsidence or uplift this effect could be included for models that simulate subsidence caused by groundwater withdrawals finally unsaturated zone storage is ignored in heavy but gravity measurements are affected by changes in storage in the unsaturated zone full integration into the modflow source code compiling the modflow and heavy codes in a single executable could provide an avenue for these updates to be applied to heavy gravity change maps figs 3 and 4 produced by heavy represent a low pass filtered view of model storage change primarily in the uppermost unconfined aquifer as such they may be useful for model evaluation such as identifying cells that go dry apart from the comparison with gravity data the power of such maps is that they show a quantity that can be measured at the land surface therefore they can be used as a screening tool to determine where gravity data can be collected to verify or refute model assumptions software availability heavy code a compiled windows executable and user guide are available at https doi org 10 5066 p9iihxn3 disclaimer any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25391,fortran software named heavy was developed to simulate gravity change due to water storage change in modflow groundwater models heavy is compatible with modflow 2005 and modflow nwt models using the layer property flow or upstream weighting packages all of the necessary information for the gravity calculation the geometry of the model cells the storage coefficient and head change is present within the existing modflow model files and no additional information is necessary gravity change is calculated at each time step for each layer at user specified locations or at a grid of hypothetical positions across the model the software has been validated using analytical gravity solutions and three example modflow models are included for demonstration heavy leverages the input output routines from modflow and is orders of magnitude faster than previous efforts using interpreted languages such as python or matlab the objective of the software is to facilitate repeat microgravity field measurements for groundwater flow model calibration keywords modflow hydrogeophysics gravity repeat microgravity data availability the link to download code is included in the manuscipt 1 introduction groundwater flow model evaluation and calibration depends on comparing model output with observations of head streamflow and other field data that is history matching one relatively new observation type is repeat microgravity which provides unique information about change in groundwater storage by measuring the change in acceleration due to earth s gravity as water mass is removed from or recharged into a gravity meter s region of sensitivity the force of gravity decreases or increases the region of sensitivity is generally large extending to a radial distance about 10 times the distance between the gravity meter and the water table in this way repeat microgravity provides spatially averaged observations at a scale relevant to groundwater models with grid cell sizes of hundreds of meters most transient groundwater modeling studies focus on the simulation of groundwater level head changes several packages are available for extracting and comparing simulated head from models and observed groundwater level measurements for the purpose of model evaluation and calibration hyd hanson and leake 1999 oc harbaugh et al 2000 hob hill et al 2000 in this paper we present a gravity observation postprocessor similar to the head observation package hob for the purpose of simulating gravity change at observation locations in turn simulated gravity change can be compared to repeat microgravity field measurements observations to calculate an objective function for model evaluation and calibration repeat microgravity is a natural fit for groundwater model analysis because all of the information necessary for the gravity calculation model geometry head change and storage properties exists in the model itself with no additional parameters several recent studies have demonstrated the value of terrestrial repeat microgravity observations for groundwater flow modeling and monitoring these data can be used directly to map storage change carruth et al 2018 to estimate specific yield i e the relation between storage and groundwater level change at collocated monitoring wells pool and eychaner 1995 or to estimate model parameters through inversion kennedy et al 2016 2021a adoption of the repeat microgravity method is advancing with improvements in field methods kennedy et al 2021b and new quantum ménoret et al 2018 and microelectronic carbone et al 2020 sensors although gravity measurements from space e g nasa s gravity recovery and climate experiment satellites can also be used for groundwater model calibration and verification sun et al 2012 rateb et al 2020 the spatial scale of satellite derived storage changes are too coarse to be relevant to the cell by cell calculations presented here to facilitate the use of repeat microgravity field measurements for groundwater flow model calibration and in particular the use of automated parameter estimation tools such as pest white et al 2020 we present fortran source code for a modflow forward gravity post processor only one optional additional file is needed beyond the standard modflow files to specify observation locations the software is made available as a compiled windows executable and the source code is readily compiled for use on linux and or high performance computing machines 2 methods 2 1 overview of gravity equations the simplest approximation for forward modeling gravity change from terrestrial water storage change is the horizontal infinite slab bouguer slab approximation 1 g 2 π g ρ h where g is the gravitational constant 6 6743 10 11 m3 kg 1 s 2 ρ is the density of the slab where water storage change occurs for water often assumed to be 1 0 g cm 3 and h is the slab thickness gravity change is calculated by considering the change in density from one time period to another for hydrologic applications where water content change is uniform throughout the slab possible examples include a water table moving up or down in an unconfined aquifer or soil water content changes at the land surface ρ and h are related 2 d ρ h where d is the equivalent thickness of free standing water in other words for a given amount of water storage change if the slab thickness h increases the density decreases the slab represents a horizontal layer of porous media if the media is fully saturated or desaturated owing to water storage changes the density ρ represents the aquifer effective porosity or specific yield the bouguer slab approximation can be improved by considering the three dimensional nature of water storage changes if storage changes occur at the land surface they can be represented as a thin layer spread across the local topography halloran 2022 storage changes in subsurface aquifers can be simulated by three dimensional flow models such as modflow harbaugh et al 2000 and in turn used to simulate the resultant change in gravity at the land surface or any other location known as forward modeling forward gravity modeling from modflow simulated storage change involves calculating the sum of the gravitational attraction of each model cell relative to a specific location such as an observation location where gravity measurements are made many different gravity analytical solutions exist for different mass geometry including for rectangular prisms forsberg 1984 point masses torge 1989 spherical prisms uieda et al 2016 and wedge shaped prisms halloran 2022 here we build on the work of leirião et al 2009 who presented a metric f 2 based on the dimensions of a rectangular model cell relative to the gravimeter position to determine the appropriate gravity formula 3 f 2 r 2 d r 2 where r is the distance from the gravity meter to the prism center and d r 2 δ x 2 δ y 2 δ z 2 is a measure of the prism size defined by the length of the three sides δ x δ y and δ z fig 1 leirião et al 2009 recommended that the prism formula also known as the forsberg 1984 equation can be used if f 2 is less than 4 4 δ g γ p s y x log y r y log x r z tan 1 x y z d x p 2 x p 1 y p 2 y p 1 z p 2 z p 1 where γ is the gravitational constant ρs y is the cell density change water density times specific yield and x p 1 x p 2 y p 1 y p 2 z p 1 z p 2 are the cartesian distances between the cell corners and the gravity station for large distances between the model cell and gravity station the difference between the integration bounds in equation 4 can become small and the formula is unstable for values of f 2 greater than 81 leirião et al 2009 recommends the point mass formula 5 δ g γ p s y δ h l i h l i h 2 x 2 y 2 3 2 δ x δ y where h is the average hydraulic head for unconfined layer cells or cell midpoint for confined layer cells δ h is the change in head between time steps and l i is the vertical distance between the microgravity instrument at location i and the hydraulic head elevation for intermediate values of f 2 between 4 and 81 leirião et al 2009 recommends an approximation of equation 2 macmillan 1958 for computational efficiency in our testing however there was little performance benefit from using the macmillan 1958 formula as compared to the more accurate prism formula and the latter was used for f 2 values less than 81 the forward gravity calculation equations 4 and 5 is carried out for each model cell in each layer each gravity meter position and each time step the horizontal geometry of each cell is defined by the modflow model and the vertical geometry by the confined or unconfined nature of the cell fig 1 the simulated gravity change value at an observation location is the sum of all model cells at a given time step relative to the sum of all cells at the initial time step 2 2 gravity from modflow output heavy kennedy and larsen 2023 leverages existing fortran input output routines for modflow harbaugh 2005 which ensures compatibility with model files and output from modflow 2005 and modflow nwt executables provided by the u s geological survey and other sources heavy should also run with other models using compatible name files and head output e g modflow ohwm boyce et al 2020 boyce 2022 a microsoft windows executable is provided in the project repository the software can be compiled using gfortran or intel compilers or with the pymake python program https github com modflowpy pymake speed of the compiled executable was similar with either the gfortran or intel compiler heavy reads model properties from the upstream weighting package upw or layer property flow lpf package the gravity change caused by water storage change in any subsurface media porous media fracture flow in hard rock aquifers flow in karst aquifers can be simulated by heavy as long as the upw or lpf packages are appropriate other flow packages such as block centered flow package bcf or hydrogeologic unit flow huf are not supported the water retained outside the modflow flow packages upw lpf is not included in the gravity calculation for example the unsaturated zone flow uzf lake lak and streamflow routing sfr packages can be used but their effect on gravity is ignored like modflow heavy reads model file names from a name nam file this can be the same name file used for modflow but must include a hvy file that specifies the output file number and observation locations if not using the g option heavy is compatible with all methods for specifying model parameters including zones and multiplier arrays heavy depends only on model geometry head change and storage properties other properties such as hydraulic conductivity are ignored head change is read for all time steps present in the modflow binary head file specified in the heavy name file time steps must be specified in the output control file provided to modflow when carrying out the gravity calculation for a given model cell heavy first determines if the cell is confined in modflow cells with constant transmissivity or unconfined variable transmissivity based on the head elevation relative to the top cell coordinate fig 1 and the convertible layer flag if the cell is in a layer that is not convertible and or the head elevation is higher than the cell it is considered confined for the purpose of the gravity calculation in this case the cell geometry is defined by the model coordinates and the density change within the cell is calculated from the head change and specific storage expressed as storage capacity in modflow for that model cell density change refers to the change in the mass of water within a modflow model cell water density change from salinity or temperature change is not considered nor is aquifer compressibility if the head elevation is within a cell in a convertible layer the cell is considered unconfined and the geometry for the purpose of the gravity calculation is defined by the beginning and ending head assuming this volume drains or fills completely within the time step fig 1 the density change within the cell is calculated from the head change and specific yield if beginning and ending head are below the bottom of a cell the gravity response for that cell is not calculated heavy accommodates models that can have the water table pass through multiple layers that is convertible layers that switch between confined and unconfined fig 2 the appropriate storage parameter specific yield or specific storage is selected based on the head elevation relative to the top of the cell described above any cells that transition from confined to unconfined during a time step or from unconfined to confined are treated as unconfined for that time step because the cumulative gravity calculation is usually dominated by unconfined storage by using the l command line option table 1 the calculation can be limited to a single layer thereby reducing computation time by default heavy reports the change in gravitational attraction of each layer individually and the cumulative change gravity observation locations in the hvy file are specified in model coordinates to facilitate rapid analysis without needing to specify observation locations by using the g command line option heavy can generate an evenly spaced grid of gravity stations across the model domain in this case any observation locations in the hvy file are ignored the z coordinate of the gravity station is the elevation of the encompassing cell from the model top dataset the user must verify model top represents the land surface additional command line options specify the number of observation locations in the x and y directions and allow the user to specify locations over a subset of the model domain see the user s guide for more information kennedy and larsen 2023 heavy output is written to a txt file with one row per observation location per time step each row includes the simulated gravity change for each layer in microgal and the total change for all layers this output can be directly integrated in an automated parameter estimation workflow such as pest white et al 2020 by writing an appropriate pest instruction file or can be further processed such as by creating a unique identifier for each observation at each time step before use in parameter estimation two python programs are included for visualization using matplotlib one program g animation py outputs a time series animation of simulated gravity change when the g option is used with heavy to generate a grid of gravity stations the second program g timeseries py creates a time series plot showing gravity change at all model cells 3 results 3 1 comparison with analytical solutions several test cases are included with heavy using the q command line option table 1 first the individual subroutines for the prism equation 4 and point mass equation 5 formulas are tested the prism formula solutions are calculated for a single prism representing a horizontal layer 1 10 m5 square and for the same layer discretized into 10 10 m prisms agreement with the bouguer slab approximation is good with less than or equal to 0 008 μga l difference table 2 the point mass solution is calculated for a single prism 20 20 20 m 100 m below an observation locations this solution agrees well 0 037 μga l difference with newton s law of gravitation solution equation 5 for a sphere of equivalent volume and density change table 2 the difference is likely related to the short distance 100 m between the observation location and the prism relative to the size of the prism 20 m square 3 2 modflow examples three transient modflow models with input and output head files are included in the software release kennedy and larsen 2023 a single layer modflow nwt model of the all american canal in southeast california usa simulates the dissipation of a groundwater mound following the concrete lining of the canal to reduce seepage losses wildermuth and kennedy 2022 model cell size is 250 m and the simulation spanned the years 1941 2020 this model was used to demonstrate model calibration using pest white et al 2020 with a field repeat microgravity dataset kennedy et al 2021a large rapid groundwater level and gravity decreases were observed after the canal was lined fig 3 for software validation heavy output was compared to equivalent gravity forward modeling routines implemented independently in python modflow files were read with flopy bakker et al 2022 and the gravity calculation carried out with code included in the software release the differences between python and heavy simulation table 2 are small compared to the accuracy with which gravity changes were measured 5 10 μga l in the study area using absolute gravimetry a nine layer modflow 2005 model covering an 80 km by 160 km region of the rio grande valley near albuquerque new mexico demonstrates layer property flow and complex use of zones and arrays to define layer properties myers and friesz 2019 the top four model layers are convertible and the bottom five are confined model cell size was 300 m and the simulation period spanned from predevelopment to future conditions 1900 2050 groundwater flow occurs primarily from mountain block and mountain front recharge seepage from the rio grande and groundwater pumping the first two of these processes are mostly steady state and do not cause large storage changes whereas pumping and changes in pumping results in large storage and gravity changes in and around albuquerque fig 4 a in other parts of the model storage and gravity changes are near zero a 4 layer modflow nwt model from northwest arizona demonstrates upstream weighting and unconfined storage in multiple layers knight 2020a the model was constructed to simulate future pumping scenarios of up to 2 3 106 m3 per year including the extent and magnitude of groundwater storage change knight 2020b model cell size is 300 m and the model simulated flow between the years 1976 2140 model calibration and parameter estimation using groundwater levels was carried out with pest white et al 2020 storage changes in this model occur in response to groundwater pumping but pumping volumes storage change and gravity change are much smaller than in the rio grande valley model the complex pattern of predicted gravity change calculated over a subregion of the model using the g option fig 4b reflects the distribution of storage properties in the groundwater model which in turn were based on a detailed airborne electromagnetic survey ball 2020 simulations such as these are useful for planning gravity data collection by targeting areas with high and low expected gravity change 4 discussion much progress has been made in recent decades to incorporate automated parameter estimation for groundwater models reducing the need for expert local knowledge and trial and error parameter adjustments anderson et al 2015 repeat microgravity data are natural candidates for model calibration because predicted values are obtained directly from the model without requiring a petrophysical model to relate measurements to model derived values gravity change is a direct measurement of storage change a property of interest for groundwater flow modeling rather than a surrogate property e g electrical conductivity gravity measurements for comparison with heavy simulations can be standalone absolute gravity measurements or relative gravity measurements between stations crossley et al 2013 kennedy et al 2021b often relative and absolute gravity measurements are combined using least squares network adjustment hwang et al 2002 these surveys are typically carried out at discrete seasonal or annual intervals the measurement precision of gravity change i e between surveys ranges from about 0 024 to 0 14 m of water using the bouguer slab approximation to convert acceleration to meters of water 41 9 μga l 1 m of water assuming the density of water is 1 0 gc m 3 1 μga l 10 nm s 2 alternatively much more precise 1 μga l uncertainty continuous daily hourly or finer data can be collected using superconducting or spring based meters crossley et al 2013 these deployments are less common and typically require alternating current ac power but can be especially useful where groundwater storage is rapidly changing such as at artificial recharge facilities kennedy et al 2014 2016 when using field data i e a limited amount of data in sparse locations computation time for most real world groundwater models is on the order of seconds and is usually much faster than solving the groundwater flow model when generating gravity change over a grid of stations using the g command line option table 1 computation time can be much longer owing to the large number of floating point calculations that must be carried out m model cells n time steps i gravity stations however this analysis at a large number of stations on a grid is usually only carried out once or a few times for model screening and not repeatedly as with parameter estimation where gravity values need only to be predicted at measurement locations depth to water relative to model cell size is important to avoid stair stepping in the gravity residual kennedy et al 2021a if this value represented by f equation 3 is too small an artificial pattern is induced in the gravity forward calculation by the discretization of the modflow model fig 5 at present the user must evaluate the significance of this effect for their model a mitigating factor is that the repeat microgravity method depends only on the magnitude of gravity changes between time steps thus a model that may produce a stair step gravity field at individual time steps may also produce a smoothly changing gravity time series at an individual station because the stair step pattern is static automatic head interpolation could prevent this effect an alternative to predicting gravity change by forward modeling is to convert gravity change to one dimensional storage change using the bouguer slab approximation this conversion from acceleration to equivalent thickness of water is similar to that made for satellite data from the grace mission rateb et al 2020 for the all american canal model fig 3 with significant mounding from recharge but relatively high depth to water greater than 30 m the departure from the infinite slab approximation was small two percent or less for the full three dimensional forward modeled signal kennedy et al 2021a for other models three dimensional effects have been significant e g kennedy et al 2016 one disadvantage to the converting gravity change to storage change approach is that vertically integrated storage change across all model layers is not typically included in model output and must be calculated another is that the spatial scale of the gravity measurement must be considered a storage change estimate is obtained at the measurement location but at how many modflow cells should that apply and with what weighting 5 conclusion a fortran modflow post processor named heavy was developed to simulate the change in gravitational attraction at user defined locations or at a grid of locations the software is intended to facilitate the use of field data collected using microgravity instruments absolute relative superconducting quantum and other technologies for model calibration using automated tools such as pest white et al 2020 several improvements are possible for heavy these include accommodating unstructured grids and triangular discretization made possible in modflow 6 more detailed discretization leads to longer run times but two solutions are possible first the gravity calculation is easily parallelized and multi core pcs or high performance computing can be applied at present heavy runs on a single core second for cells within the model that don t change position i e confined layers but not unconfined layers where the cell coordinates depend on head the gravity effect of any one cell is simply s i j k mass change where s i j k is a sensitivity that is calculated once for each gravimeter location model cell pair and can be stored and reused for future time steps third heavy does not consider subsidence but gravity measurements at the land surface are affected by changes in elevation caused by subsidence or uplift this effect could be included for models that simulate subsidence caused by groundwater withdrawals finally unsaturated zone storage is ignored in heavy but gravity measurements are affected by changes in storage in the unsaturated zone full integration into the modflow source code compiling the modflow and heavy codes in a single executable could provide an avenue for these updates to be applied to heavy gravity change maps figs 3 and 4 produced by heavy represent a low pass filtered view of model storage change primarily in the uppermost unconfined aquifer as such they may be useful for model evaluation such as identifying cells that go dry apart from the comparison with gravity data the power of such maps is that they show a quantity that can be measured at the land surface therefore they can be used as a screening tool to determine where gravity data can be collected to verify or refute model assumptions software availability heavy code a compiled windows executable and user guide are available at https doi org 10 5066 p9iihxn3 disclaimer any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25392,remote sensing data and model simulations of precipitation complemented the traditional surface measurements and offer unprecedented coverage on a global scale however the substantial heterogeneity among data products has hindered this unique opportunity to obtain a robust quantification of the climatologic properties of precipitation globally herein we present precipe a package developed in r with reproducible science as our standard precipe provides functions to download explore process and visualize a database of 24 precipitation data sets at monthly time step and 0 25 resolution 20cr v3 chirps v2 0 cmap cmorph cpc global cru ts v4 06 em earth era 20c era5 ghcn v2 gldas clsm gldas noah gldas vic gpcc v2020 gpcp v2 3 gpm imergm v06 mswep v2 8 ncep doe r2 ncep ncar r1 persiann cdr prec l terraclimate trmm 3b43 v7 udel v5 01 keywords multi source data data set validation global precipitation satellite remote sensing reanalyses hydroclimatology data availability the precipe package is publicly available 1 introduction it is common practice to describe and quantify the water cycle focusing on its four major components evaporation precipitation runoff and total water storage harding et al 2011 out of these four components precipitation has been extensively researched because it is the primary factor determining water availability across several spatiotemporal scales trenberth and zhang 2018 accurate estimates of precipitation climatologies are crucial for water resource management marques et al 2022 water related engineering design and long term agricultural policy making bezner kerr et al 2022 notwithstanding to date a comprehensive network of ground stations remains elusive due to practical economical or political reasons vargas godoy et al 2021 when ground observations are unavailable we may rely on data from different sources such as satellite remote sensing model simulations and reanalyses regardless of the source having a good grasp of the uncertainty of the estimates becomes imperative consequently using different data products from various ideally independent sources is the most appropriate direction for current research and operational needs although precipitation understanding has improved dramatically due to the vast amount of different data sources nowadays their information has not been comprehensive enough due to substantial uncertainty between sources with biases reaching as much as 300 fekete et al 2004 such uncertainty could be partially attributed to the intrinsic heterogeneity of multiple aspects from data distribution to end product specifications e g spatial resolution time step measuring units file format etc therefore we find ourselves with a broad spectrum of data renditions a research matter on its own sun et al 2018 and a homogenization pre processing hindrance the latter i e data preparation is acknowledged to be an often unavoidable and rather time consuming step of the analysis young et al 2017 some data distributors mitigate the above challenges by facilitating different tools for extraction e g https www earthdata nasa gov exploration e g https climexp knmi nl and in some cases visualization e g https giovanni gsfc nasa gov however being online services they are heavily oriented toward graphic user interfaces are limited to elementary operations for exploratory data analysis and allow for simultaneous analysis of at most two data sets at a time undeniably a broader more inclusive framework integrating multiple data sets is still missing tailored software is available to deal with the processing and analyze large amounts of data efficiently namely the climate data operators cdo schulzweida 2022 and climate data analysis tools cdat williams et al 2009 these packages provide data cleaning analysis and visualization tools while working with precipitation data can be challenging and time consuming these or similar alternative software allow researchers to automate and streamline the data analysis process for reproducibility nevertheless a significant limitation of tools like cdo and cdat is their incompatibility with windows the dominant desktop operating system globally it could be argued that installing both cdo and cdat in windows is possible however the installation is done through the windows subsystem for linux wsl which provides a gnu linux environment including command line tools and utilities on windows singh 2020 moreover cdat is staged for deprecation and cease of support around the end of the calendar year 2023 over the last decade r an open source programming language r core team 2023 has continuously increased its presence until it acquired a central role in hydrological research and the operational practice of hydrology slater et al 2019 the r hydrological community has grown significantly in the last decade with applications or packages that involve data retrieval and pre processing from hydrological and meteorological sources hydrograph and spatial analysis functions and tools for process based and stochastic modeling nevertheless more often than not these packages are still developed around one data set or one data provider e g easyclimate to access high resolution daily climate data for europe cruz alonso et al 2023 or dataretrieval for the us geological survey usgs national water information system decicco et al 2022 tools centered packages tend to be more comprehensive and require more generic inputs e g envoutliers identifies outliers in environmental time series data čampulová et al 2022 and cosmos generates univariate multivariate non gaussian time series and random fields for environmental and hydroclimatic processes papalexiou et al 2021 these latter kinds of packages give the users more flexibility on the account they are to deal with data gathering and pre processing on their own despite r flourishing in hydrology the previously mentioned support supplied by data providers focuses more on different programming languages like matlab or python on that account addressing both data preparation time consumption and the lack of a comprehensive r based alternative we introduce precipe we acknowledge that various yet exclusive data retrieval packages are already available see albers et al 2022 moreover while claiming to provide all available data sets would be fraudulent we can ascertain that precipe provides a ready for analysis homogenized database with products from various sources the precipe package database consists of 24 data sets at monthly time step and 0 25 resolution these are derived from gauge satellite reanalysis and hydrological model forcing precipitation products furthermore precipe offers additional processing tools to subset the record length and spatial coverage crop data based using shapefiles and various graphical aesthetics for visualization and exploratory data analysis the package can be downloaded from the cran repository or from https github com mirovago precipe 2 methods 2 1 data the precipe package offers a database of 24 precipitation data sets homogenized to common a spatial 0 25 and temporal monthly resolution these include seven gauge based products cpc global xie et al 2010 cru ts v4 06 harris et al 2020 em earth tang et al 2022 ghcn v2 peterson and vose 1997 gpcc v2020 schneider et al 2011 prec l chen et al 2002 and udel v5 01 willmott and matsuura 2001 eight satellite based products chirps v2 0 funk et al 2015 cmap xie and arkin 1997 cmorph joyce et al 2004 gpcp v2 3 adler et al 2018 gpm imergm v06 huffman et al 2019 mswep v2 8 beck et al 2019 persiann cdr ashouri et al 2015 and trmm 3b43 v7 huffman et al 2010 five reanalysis products 20cr v3 slivinski et al 2019 era 20c poli et al 2016 era5 hersbach et al 2020 ncep ncar r1 kalnay et al 1996 and ncep doe r2 kanamitsu et al 2002 four hydrological model forcing products gldas clsm v2 0 rodell et al 2004 gldas noah v2 0 rodell et al 2004 gldas vic v2 0 rodell et al 2004 and terraclimate abatzoglou et al 2018 their native specifications as well as download links to their original providers and their respective references are detailed in tables 1 2 3 and 4 respectively if multiple distributions were available the one closest to the target spatiotemporal resolution was chosen to minimize the pre processing uncertainty remapping data from one spatial or temporal resolution to another can result in information loss when processing data from higher to lower resolution hence these uncertainties magnitudes depend on source data quality and are proportional to the times we manipulate data consequently the less pre processing we have to perform on the data the less uncertainty we introduce overall the package focuses on three fronts formatting homogenization and storage to begin with most providers either natively have data in the network common data form netcdf format or offer the option to download in that format in the same fashion we chose the netcdf format for our database the gpm huffman et al 2019 and trmm huffman et al 2010 data sets use the hierarchical data format hdf instead trmm data is in hdf4 format and was reformatted into netcdf using the conversion toolkit from the hdf group https hdfeos org gpm is in hdf5 format and no direct conversion tool was available thus we extracted the values and stored them in netcdf files using r note that no reprojection or manipulation of any kind took place at this stage once all data sets were in netcdf files if there were multiple files per data set i e one file per day month or year we merged them in time into a single netcdf file using climate data operators cdo schulzweida 2022 then data homogenization addressed the variable type total precipitation tp the measuring units millimeters mm the temporal resolution monthly and the spatial resolution 0 25 if the providers offered both precipitation rate and total precipitation total precipitation files were downloaded to minimize data tampering we converted the precipitation rate from mm day or kg m 2 s into total precipitation mm else we just converted the units of total precipitation where needed e g m into mm subsequently daily data was aggregated into monthly thereafter spatial remapping was performed using cdo when regridding coarser than 0 25 resolution data the remapnn operator was used for nearest neighbor interpolation otherwise the gridboxmean operator would be used for regridding via area weighted averaging accounting for the area of each grid cell in proportion to the total area being averaged plus remapnn when 0 25 is not divisible by the original resolution arguably nearest neighbor interpolation potentially leads to abrupt changes in the values of the remapped data when used to fill in missing data however if used simply for regridding we do not introduce any significant artifacts as evinced by differences of less than 0 01 in total precipitation volume between raw and remapped data finally the database has been deposited in a public zenodo repository under the following naming convention data set variable units coverage start date end date resolution time step nc e g gpcp v2 3 adler et al 2018 would be gpcp tp mm global 197901 202205 025 monthly nc 2 2 package design designed with reproducible science in mind the precipe package facilitates the download exploration visualization and analysis of multiple precipitation data products across various spatiotemporal scales the general workflow is as follows 1 direct download of a single multiple or all data sets available in the precipe database is done via the download data function which has two arguments data name and destination the data name argument is set to all by default but the users can specify the name s of their interest 20cr chirps cmap cmorph cpc cru ts em earth era20c era5 ghcn gldas clsm gldas noah gldas vic gpcc gpcp gpm imerg mswep ncep doe ncep ncar persiann precl terraclimate trmm 3b43 and or udel the destination argument is set to by default i e the current working directory by replacing it for your project folder the downloaded files will be stored in your project folder instead 2 data processing functions are built upon the raster package hijmans et al 2022 with the additional advantage that saving data will do so in a netcdf format compatible with cdo currently precipe offers spatial subsetting by either a bounding box or an irregular polygon via shapefile besides temporal upscaling from monthly to yearly scale offers basic statistical options such as maximum minimum median average and sum last but not least the make ts function computes the area weighted average of each time step be it monthly or annual to transform the raster into a time series comma separated values csv file 3 prompt and aesthetic visualization is available at any stage of analysis the precipe graphical framework allows the user to explore and present analysis results of its data via maps time series curves boxplots histograms and heat maps it is important to note that the above mentioned graphical framework is based on the ggplot2 package wickham et al 2022 as such the outputs are easily adjusted to suit the user s needs using the grammar of graphics the precipe package is publicly available in the comprehensive r archive network cran at https cran r project org package precipe more experienced users may find all the functions source code at https github com mirovago precipe and can easily modify them to fit the user specific needs if needed 3 case study the user friendly accessibility that precipe provides makes analysis reproducibility as simple as following a recipe in this introductory recipe we downloaded the entire precipe database section 2 1 using the download data function we then subsetted the downloaded data to the 1981 2020 period using the subset time function and cropped it within the administrative borders of czechia via the crop data in conjunction with a shapefile provided by the database of global administrative areas gadm we then generated time series using the make ts function the time series were generated by computing the area weighted average of all the grid cells of interest and the values were stored in data table objects with four columns date value and name type the last two are mainly used for graphical aesthetics note that storing the time series in data table objects enables further calculations with ease herein we calculated the sum min max median and mean of our monthly data by year in order to visually assess the similarities and discrepancies between data sources using the plot line function fig 1 it is evident at first glance that even limiting the data record to just 40 years a line plot is not the best graphical aesthetic to represent our data due to the high clustering and overlapping of lines fig 1a upscaling into annual time steps it is easier to observe that while there is considerable variability between different products fig 1b c e and f there is higher agreement in measuring high precipitation fig 1d to validate data using local observations or one of the downloaded data sets as the reference we can assess their correlation and variance through taylor diagrams using the plot taylor function fig 2 we used data from the czech hydrometeorological institute chmi to validate the database in this case study as expected observational data from gauge based and satellite based products are highly correlated with the chmi reference with most of their correlation coefficients above 0 95 and 0 9 respectively fig 2 in terms of variance we observe that the hydrological model forcing data exhibits almost identical locations on the diagram in contrast reanalysis data are the most scattered of all four data sources from this quick inspection we can say gpcc v2020 data estimates are the closest to our validation reference while ncep ncar r1 and ncep doe r2 are the most inconsistent lowest correlation and highest variance the former is presumably due to gpcc v2020 likely drawing data by the same network of stations chmi oversees becker et al 2013 the latter might be an artifact produced by the coarse native resolution of ncep reanalyses compared to the area of czechia i e the area of two grid cells approximately 86632 km 2 fully covers czechia 78867 km 2 moreover ncep ncar r1 is a first generation reanalysis that uses antiquated data assimilation and model kalnay et al 1996 ncep doe r2 is a direct update that fixed some errors and updated the parameterizations of ncep ncar r1 kanamitsu et al 2002 nonetheless it did not address other limitations like higher horizontal and vertical resolution direct assimilation of radiances proper use of ssm i data and assimilation of rainfall data further insight into the validation of our data sets can be reckoned with by looking into their correlation and variance across different seasons fig 3 this time we looked only into eight data sets camp cpc global era 20c era5 gldas clsm v2 0 gldas noah v2 0 gpcc v2020 and trmm 3b43 v7 using high correlation to chmi as a preliminary filter we selected the two best data sets from each source visualizing the data sets correlation by season we discover that the best agreement with the chmi reference occurs during fall where most data sets have a correlation above 0 95 normalized standard deviation around 1 and centered root mean square error 0 5 out of the selected data sets era 20c has the lowest correlation regardless of the season with its correlation further away from the rest dropping to 0 8 in summer observations assimilated by era 20c include surface pressure from the international surface pressure databank compo et al 2015 as well as from icoads woodruff et al 2011 and surface winds over the oceans from icoads upper air and satellite data are omitted poli et al 2016 due to the limited observations used era 20c does not provide the best estimate since 1979 when major advancements in the observing system occurred with the dawn of the satellite era another point of interest is that trmm 3b43 v7 correlation is consistent across seasons but its variance visibly increases in winter trmm precipitation radar algorithm has been reported to underestimate precipitation at higher latitudes 40 n chen and li 2016 in conjunction with winter precipitation characteristics i e lighter rain events snow and mixed phase precipitation would explain larger biases in the winter season maggioni et al 2016 the toolbox that precipe offers can also be applied to assess changes in precipitation regimes we selected four data sets gpcc v2020 era5 gldas noah v2 0 and mswep v2 8 we divided the time series in two 20 year periods 1981 2000 and 2001 2020 and examined their empirical distribution to do so we used the plot density function fig 4 we identify two common traits a density peak around 50 mm for the first 20 year period and a general widening of the density curve toward higher precipitation across the selected data sets in the last 20 years a particular distinction in the distribution of era5 compared with the other data sets is that the density peak is shorter for both periods a shorter density peak and a bigger area under the density curve to the right of said peak indicate that era5 precipitation estimates are higher than those of the other data sets in line with our findings through the empirical distribution of era5 overestimation of precipitation has previously been identified across different regions hassler and lauer 2021 a different approach to analyzing changes in precipitation regimes is to explore their spatial patterns we computed the median monthly precipitation at each grid cell for two 20 year periods and then portrayed them using the plot map function fig 5 the maps show that no drastic changes in spatial patterns took place between 1981 2000 and 2001 2020 except for a slight increase in precipitation in relatively uniform manner intercomparing data sets we observe a common high precipitation center located around the šumava mountains on the southwestern border of czechia while the empirical distribution of precipitation estimates already pointed at era5 overestimating precipitation it is now observable and perhaps more easily conveyed on the maps that era5 estimates are overall higher than the rest contrarily we can see the lack of spatial contrast between high and low precipitation in gldas noah v2 0 estimates appearing as more homogeneous color maps conversely this artifact is due to precipitation underestimation compared to the other data sets e g xue et al 2013 we found explicitly higher estimates around the sudetic šumava and ore mountains supporting previous reports that in era5 too much precipitation can occur on the leeward side of an orographic barrier lavers et al 2022 more details about further functions including a simpler and fully reproducible example can be found at https cran r project org web packages precipe vignettes precipe html 4 conclusions the precipe package provides a common starting point for the hydrology scientific community through its homogenized database by encompassing widely used products from multiple sources and establishing a common ground from which to start analysis precipe guarantees a fully reproducible framework for precipitation research its versatility to export data at any processing stage in netcdf raster or csv data table facilitates a seamless transition for the user into different r packages like cosmos papalexiou 2018 csa markonis et al 2021 and somspace markonis and strnad 2020 for further analysis the precipe package constitutes a valuable resource for academics government agencies and private sector professionals because it provides a consistent and transparent approach to precipitation research through precipe these users can easily access and analyze precipitation data from multiple sources visualize various characteristics of precipitation climatology and seamlessly transition into different r packages for further analysis overall the precipe package is a powerful tool that can benefit anyone involved in precipitation research from novice researchers to seasoned experts herein we have exemplified some of its key capabilities and showcased the ease of accessibility for the user to visualize various characteristics of precipitation climatology we aim to provide through precipe an open access database and toolbox that the hydrology community could adopt for a more consistent and reproducible science the latter aspect will be strengthened in coming years under the following roadmap implementation of probabilistic significance estimation for slopes changes in 2023 and uncertainty quantification functions in 2024 at the same time we plan to develop an evaporation twin package of precipe with similar functionalities and integrate them together into a holistic framework for the study of terrestrial water cycle as suggested by vargas godoy and markonis 2023 to this end we invite all scientists involved in precipitation hydroclimatology research to actively contribute with their suggestions additions and requests for future versions of precipe glossary 20c 20th century 20cr 20th century reanalysis cdat climate data analysis tools cdo climate data operators chirps climate hazards group infrared precipitation with station data chmi czech hydrometeorological institute clsm catchment land surface model cmap climate prediction center merged analysis of precipitation cmorph climate prediction center morphing method cpc climate prediction center cru ts university of east anglia climatic research unit time series doe department of energy em earth ensemble meteorological dataset for planet earth era european centre for medium range weather forecasts re analysis gadm database of global administrative areas ghcn global historical climatology network gldas global land data assimilation system gpcc global precipitation climatology centre gpcp global precipitation climatology project gpm global precipitation measurement icoads international comprehensive ocean atmosphere data set imerg integrated multi satellite retrievals for global precipitation measurement mswep multi source weighted ensemble precipitation ncar national center for atmospheric research ncep national centers for environmental prediction noaa national oceanic and atmospheric administration noah national centers for environmental prediction oregon state university air force hydrology lab persiann precipitation estimation from remotely sensed information using artificial neural networks prec l precipitation reconstruction over land ssm i special sensor microwave imager trmm tropical rainfall measuring mission udel university of delaware usgs united states geological survey vic variable infiltration capacity credit authorship contribution statement mijael rodrigo vargas godoy conceptualization software data curation writing original draft visualization yannis markonis conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was carried out within the project investigation of terrestrial hydrological cycle acceleration ithaca funded by the czech science foundation grant 22 33266m mrvg was supported by the faculty of environmental sciences czech university of life sciences prague internal grant 2023b0001 chimera charting hydrological intensification of atmospheric fluxes in reanalysis data appendix case study r code 
25392,remote sensing data and model simulations of precipitation complemented the traditional surface measurements and offer unprecedented coverage on a global scale however the substantial heterogeneity among data products has hindered this unique opportunity to obtain a robust quantification of the climatologic properties of precipitation globally herein we present precipe a package developed in r with reproducible science as our standard precipe provides functions to download explore process and visualize a database of 24 precipitation data sets at monthly time step and 0 25 resolution 20cr v3 chirps v2 0 cmap cmorph cpc global cru ts v4 06 em earth era 20c era5 ghcn v2 gldas clsm gldas noah gldas vic gpcc v2020 gpcp v2 3 gpm imergm v06 mswep v2 8 ncep doe r2 ncep ncar r1 persiann cdr prec l terraclimate trmm 3b43 v7 udel v5 01 keywords multi source data data set validation global precipitation satellite remote sensing reanalyses hydroclimatology data availability the precipe package is publicly available 1 introduction it is common practice to describe and quantify the water cycle focusing on its four major components evaporation precipitation runoff and total water storage harding et al 2011 out of these four components precipitation has been extensively researched because it is the primary factor determining water availability across several spatiotemporal scales trenberth and zhang 2018 accurate estimates of precipitation climatologies are crucial for water resource management marques et al 2022 water related engineering design and long term agricultural policy making bezner kerr et al 2022 notwithstanding to date a comprehensive network of ground stations remains elusive due to practical economical or political reasons vargas godoy et al 2021 when ground observations are unavailable we may rely on data from different sources such as satellite remote sensing model simulations and reanalyses regardless of the source having a good grasp of the uncertainty of the estimates becomes imperative consequently using different data products from various ideally independent sources is the most appropriate direction for current research and operational needs although precipitation understanding has improved dramatically due to the vast amount of different data sources nowadays their information has not been comprehensive enough due to substantial uncertainty between sources with biases reaching as much as 300 fekete et al 2004 such uncertainty could be partially attributed to the intrinsic heterogeneity of multiple aspects from data distribution to end product specifications e g spatial resolution time step measuring units file format etc therefore we find ourselves with a broad spectrum of data renditions a research matter on its own sun et al 2018 and a homogenization pre processing hindrance the latter i e data preparation is acknowledged to be an often unavoidable and rather time consuming step of the analysis young et al 2017 some data distributors mitigate the above challenges by facilitating different tools for extraction e g https www earthdata nasa gov exploration e g https climexp knmi nl and in some cases visualization e g https giovanni gsfc nasa gov however being online services they are heavily oriented toward graphic user interfaces are limited to elementary operations for exploratory data analysis and allow for simultaneous analysis of at most two data sets at a time undeniably a broader more inclusive framework integrating multiple data sets is still missing tailored software is available to deal with the processing and analyze large amounts of data efficiently namely the climate data operators cdo schulzweida 2022 and climate data analysis tools cdat williams et al 2009 these packages provide data cleaning analysis and visualization tools while working with precipitation data can be challenging and time consuming these or similar alternative software allow researchers to automate and streamline the data analysis process for reproducibility nevertheless a significant limitation of tools like cdo and cdat is their incompatibility with windows the dominant desktop operating system globally it could be argued that installing both cdo and cdat in windows is possible however the installation is done through the windows subsystem for linux wsl which provides a gnu linux environment including command line tools and utilities on windows singh 2020 moreover cdat is staged for deprecation and cease of support around the end of the calendar year 2023 over the last decade r an open source programming language r core team 2023 has continuously increased its presence until it acquired a central role in hydrological research and the operational practice of hydrology slater et al 2019 the r hydrological community has grown significantly in the last decade with applications or packages that involve data retrieval and pre processing from hydrological and meteorological sources hydrograph and spatial analysis functions and tools for process based and stochastic modeling nevertheless more often than not these packages are still developed around one data set or one data provider e g easyclimate to access high resolution daily climate data for europe cruz alonso et al 2023 or dataretrieval for the us geological survey usgs national water information system decicco et al 2022 tools centered packages tend to be more comprehensive and require more generic inputs e g envoutliers identifies outliers in environmental time series data čampulová et al 2022 and cosmos generates univariate multivariate non gaussian time series and random fields for environmental and hydroclimatic processes papalexiou et al 2021 these latter kinds of packages give the users more flexibility on the account they are to deal with data gathering and pre processing on their own despite r flourishing in hydrology the previously mentioned support supplied by data providers focuses more on different programming languages like matlab or python on that account addressing both data preparation time consumption and the lack of a comprehensive r based alternative we introduce precipe we acknowledge that various yet exclusive data retrieval packages are already available see albers et al 2022 moreover while claiming to provide all available data sets would be fraudulent we can ascertain that precipe provides a ready for analysis homogenized database with products from various sources the precipe package database consists of 24 data sets at monthly time step and 0 25 resolution these are derived from gauge satellite reanalysis and hydrological model forcing precipitation products furthermore precipe offers additional processing tools to subset the record length and spatial coverage crop data based using shapefiles and various graphical aesthetics for visualization and exploratory data analysis the package can be downloaded from the cran repository or from https github com mirovago precipe 2 methods 2 1 data the precipe package offers a database of 24 precipitation data sets homogenized to common a spatial 0 25 and temporal monthly resolution these include seven gauge based products cpc global xie et al 2010 cru ts v4 06 harris et al 2020 em earth tang et al 2022 ghcn v2 peterson and vose 1997 gpcc v2020 schneider et al 2011 prec l chen et al 2002 and udel v5 01 willmott and matsuura 2001 eight satellite based products chirps v2 0 funk et al 2015 cmap xie and arkin 1997 cmorph joyce et al 2004 gpcp v2 3 adler et al 2018 gpm imergm v06 huffman et al 2019 mswep v2 8 beck et al 2019 persiann cdr ashouri et al 2015 and trmm 3b43 v7 huffman et al 2010 five reanalysis products 20cr v3 slivinski et al 2019 era 20c poli et al 2016 era5 hersbach et al 2020 ncep ncar r1 kalnay et al 1996 and ncep doe r2 kanamitsu et al 2002 four hydrological model forcing products gldas clsm v2 0 rodell et al 2004 gldas noah v2 0 rodell et al 2004 gldas vic v2 0 rodell et al 2004 and terraclimate abatzoglou et al 2018 their native specifications as well as download links to their original providers and their respective references are detailed in tables 1 2 3 and 4 respectively if multiple distributions were available the one closest to the target spatiotemporal resolution was chosen to minimize the pre processing uncertainty remapping data from one spatial or temporal resolution to another can result in information loss when processing data from higher to lower resolution hence these uncertainties magnitudes depend on source data quality and are proportional to the times we manipulate data consequently the less pre processing we have to perform on the data the less uncertainty we introduce overall the package focuses on three fronts formatting homogenization and storage to begin with most providers either natively have data in the network common data form netcdf format or offer the option to download in that format in the same fashion we chose the netcdf format for our database the gpm huffman et al 2019 and trmm huffman et al 2010 data sets use the hierarchical data format hdf instead trmm data is in hdf4 format and was reformatted into netcdf using the conversion toolkit from the hdf group https hdfeos org gpm is in hdf5 format and no direct conversion tool was available thus we extracted the values and stored them in netcdf files using r note that no reprojection or manipulation of any kind took place at this stage once all data sets were in netcdf files if there were multiple files per data set i e one file per day month or year we merged them in time into a single netcdf file using climate data operators cdo schulzweida 2022 then data homogenization addressed the variable type total precipitation tp the measuring units millimeters mm the temporal resolution monthly and the spatial resolution 0 25 if the providers offered both precipitation rate and total precipitation total precipitation files were downloaded to minimize data tampering we converted the precipitation rate from mm day or kg m 2 s into total precipitation mm else we just converted the units of total precipitation where needed e g m into mm subsequently daily data was aggregated into monthly thereafter spatial remapping was performed using cdo when regridding coarser than 0 25 resolution data the remapnn operator was used for nearest neighbor interpolation otherwise the gridboxmean operator would be used for regridding via area weighted averaging accounting for the area of each grid cell in proportion to the total area being averaged plus remapnn when 0 25 is not divisible by the original resolution arguably nearest neighbor interpolation potentially leads to abrupt changes in the values of the remapped data when used to fill in missing data however if used simply for regridding we do not introduce any significant artifacts as evinced by differences of less than 0 01 in total precipitation volume between raw and remapped data finally the database has been deposited in a public zenodo repository under the following naming convention data set variable units coverage start date end date resolution time step nc e g gpcp v2 3 adler et al 2018 would be gpcp tp mm global 197901 202205 025 monthly nc 2 2 package design designed with reproducible science in mind the precipe package facilitates the download exploration visualization and analysis of multiple precipitation data products across various spatiotemporal scales the general workflow is as follows 1 direct download of a single multiple or all data sets available in the precipe database is done via the download data function which has two arguments data name and destination the data name argument is set to all by default but the users can specify the name s of their interest 20cr chirps cmap cmorph cpc cru ts em earth era20c era5 ghcn gldas clsm gldas noah gldas vic gpcc gpcp gpm imerg mswep ncep doe ncep ncar persiann precl terraclimate trmm 3b43 and or udel the destination argument is set to by default i e the current working directory by replacing it for your project folder the downloaded files will be stored in your project folder instead 2 data processing functions are built upon the raster package hijmans et al 2022 with the additional advantage that saving data will do so in a netcdf format compatible with cdo currently precipe offers spatial subsetting by either a bounding box or an irregular polygon via shapefile besides temporal upscaling from monthly to yearly scale offers basic statistical options such as maximum minimum median average and sum last but not least the make ts function computes the area weighted average of each time step be it monthly or annual to transform the raster into a time series comma separated values csv file 3 prompt and aesthetic visualization is available at any stage of analysis the precipe graphical framework allows the user to explore and present analysis results of its data via maps time series curves boxplots histograms and heat maps it is important to note that the above mentioned graphical framework is based on the ggplot2 package wickham et al 2022 as such the outputs are easily adjusted to suit the user s needs using the grammar of graphics the precipe package is publicly available in the comprehensive r archive network cran at https cran r project org package precipe more experienced users may find all the functions source code at https github com mirovago precipe and can easily modify them to fit the user specific needs if needed 3 case study the user friendly accessibility that precipe provides makes analysis reproducibility as simple as following a recipe in this introductory recipe we downloaded the entire precipe database section 2 1 using the download data function we then subsetted the downloaded data to the 1981 2020 period using the subset time function and cropped it within the administrative borders of czechia via the crop data in conjunction with a shapefile provided by the database of global administrative areas gadm we then generated time series using the make ts function the time series were generated by computing the area weighted average of all the grid cells of interest and the values were stored in data table objects with four columns date value and name type the last two are mainly used for graphical aesthetics note that storing the time series in data table objects enables further calculations with ease herein we calculated the sum min max median and mean of our monthly data by year in order to visually assess the similarities and discrepancies between data sources using the plot line function fig 1 it is evident at first glance that even limiting the data record to just 40 years a line plot is not the best graphical aesthetic to represent our data due to the high clustering and overlapping of lines fig 1a upscaling into annual time steps it is easier to observe that while there is considerable variability between different products fig 1b c e and f there is higher agreement in measuring high precipitation fig 1d to validate data using local observations or one of the downloaded data sets as the reference we can assess their correlation and variance through taylor diagrams using the plot taylor function fig 2 we used data from the czech hydrometeorological institute chmi to validate the database in this case study as expected observational data from gauge based and satellite based products are highly correlated with the chmi reference with most of their correlation coefficients above 0 95 and 0 9 respectively fig 2 in terms of variance we observe that the hydrological model forcing data exhibits almost identical locations on the diagram in contrast reanalysis data are the most scattered of all four data sources from this quick inspection we can say gpcc v2020 data estimates are the closest to our validation reference while ncep ncar r1 and ncep doe r2 are the most inconsistent lowest correlation and highest variance the former is presumably due to gpcc v2020 likely drawing data by the same network of stations chmi oversees becker et al 2013 the latter might be an artifact produced by the coarse native resolution of ncep reanalyses compared to the area of czechia i e the area of two grid cells approximately 86632 km 2 fully covers czechia 78867 km 2 moreover ncep ncar r1 is a first generation reanalysis that uses antiquated data assimilation and model kalnay et al 1996 ncep doe r2 is a direct update that fixed some errors and updated the parameterizations of ncep ncar r1 kanamitsu et al 2002 nonetheless it did not address other limitations like higher horizontal and vertical resolution direct assimilation of radiances proper use of ssm i data and assimilation of rainfall data further insight into the validation of our data sets can be reckoned with by looking into their correlation and variance across different seasons fig 3 this time we looked only into eight data sets camp cpc global era 20c era5 gldas clsm v2 0 gldas noah v2 0 gpcc v2020 and trmm 3b43 v7 using high correlation to chmi as a preliminary filter we selected the two best data sets from each source visualizing the data sets correlation by season we discover that the best agreement with the chmi reference occurs during fall where most data sets have a correlation above 0 95 normalized standard deviation around 1 and centered root mean square error 0 5 out of the selected data sets era 20c has the lowest correlation regardless of the season with its correlation further away from the rest dropping to 0 8 in summer observations assimilated by era 20c include surface pressure from the international surface pressure databank compo et al 2015 as well as from icoads woodruff et al 2011 and surface winds over the oceans from icoads upper air and satellite data are omitted poli et al 2016 due to the limited observations used era 20c does not provide the best estimate since 1979 when major advancements in the observing system occurred with the dawn of the satellite era another point of interest is that trmm 3b43 v7 correlation is consistent across seasons but its variance visibly increases in winter trmm precipitation radar algorithm has been reported to underestimate precipitation at higher latitudes 40 n chen and li 2016 in conjunction with winter precipitation characteristics i e lighter rain events snow and mixed phase precipitation would explain larger biases in the winter season maggioni et al 2016 the toolbox that precipe offers can also be applied to assess changes in precipitation regimes we selected four data sets gpcc v2020 era5 gldas noah v2 0 and mswep v2 8 we divided the time series in two 20 year periods 1981 2000 and 2001 2020 and examined their empirical distribution to do so we used the plot density function fig 4 we identify two common traits a density peak around 50 mm for the first 20 year period and a general widening of the density curve toward higher precipitation across the selected data sets in the last 20 years a particular distinction in the distribution of era5 compared with the other data sets is that the density peak is shorter for both periods a shorter density peak and a bigger area under the density curve to the right of said peak indicate that era5 precipitation estimates are higher than those of the other data sets in line with our findings through the empirical distribution of era5 overestimation of precipitation has previously been identified across different regions hassler and lauer 2021 a different approach to analyzing changes in precipitation regimes is to explore their spatial patterns we computed the median monthly precipitation at each grid cell for two 20 year periods and then portrayed them using the plot map function fig 5 the maps show that no drastic changes in spatial patterns took place between 1981 2000 and 2001 2020 except for a slight increase in precipitation in relatively uniform manner intercomparing data sets we observe a common high precipitation center located around the šumava mountains on the southwestern border of czechia while the empirical distribution of precipitation estimates already pointed at era5 overestimating precipitation it is now observable and perhaps more easily conveyed on the maps that era5 estimates are overall higher than the rest contrarily we can see the lack of spatial contrast between high and low precipitation in gldas noah v2 0 estimates appearing as more homogeneous color maps conversely this artifact is due to precipitation underestimation compared to the other data sets e g xue et al 2013 we found explicitly higher estimates around the sudetic šumava and ore mountains supporting previous reports that in era5 too much precipitation can occur on the leeward side of an orographic barrier lavers et al 2022 more details about further functions including a simpler and fully reproducible example can be found at https cran r project org web packages precipe vignettes precipe html 4 conclusions the precipe package provides a common starting point for the hydrology scientific community through its homogenized database by encompassing widely used products from multiple sources and establishing a common ground from which to start analysis precipe guarantees a fully reproducible framework for precipitation research its versatility to export data at any processing stage in netcdf raster or csv data table facilitates a seamless transition for the user into different r packages like cosmos papalexiou 2018 csa markonis et al 2021 and somspace markonis and strnad 2020 for further analysis the precipe package constitutes a valuable resource for academics government agencies and private sector professionals because it provides a consistent and transparent approach to precipitation research through precipe these users can easily access and analyze precipitation data from multiple sources visualize various characteristics of precipitation climatology and seamlessly transition into different r packages for further analysis overall the precipe package is a powerful tool that can benefit anyone involved in precipitation research from novice researchers to seasoned experts herein we have exemplified some of its key capabilities and showcased the ease of accessibility for the user to visualize various characteristics of precipitation climatology we aim to provide through precipe an open access database and toolbox that the hydrology community could adopt for a more consistent and reproducible science the latter aspect will be strengthened in coming years under the following roadmap implementation of probabilistic significance estimation for slopes changes in 2023 and uncertainty quantification functions in 2024 at the same time we plan to develop an evaporation twin package of precipe with similar functionalities and integrate them together into a holistic framework for the study of terrestrial water cycle as suggested by vargas godoy and markonis 2023 to this end we invite all scientists involved in precipitation hydroclimatology research to actively contribute with their suggestions additions and requests for future versions of precipe glossary 20c 20th century 20cr 20th century reanalysis cdat climate data analysis tools cdo climate data operators chirps climate hazards group infrared precipitation with station data chmi czech hydrometeorological institute clsm catchment land surface model cmap climate prediction center merged analysis of precipitation cmorph climate prediction center morphing method cpc climate prediction center cru ts university of east anglia climatic research unit time series doe department of energy em earth ensemble meteorological dataset for planet earth era european centre for medium range weather forecasts re analysis gadm database of global administrative areas ghcn global historical climatology network gldas global land data assimilation system gpcc global precipitation climatology centre gpcp global precipitation climatology project gpm global precipitation measurement icoads international comprehensive ocean atmosphere data set imerg integrated multi satellite retrievals for global precipitation measurement mswep multi source weighted ensemble precipitation ncar national center for atmospheric research ncep national centers for environmental prediction noaa national oceanic and atmospheric administration noah national centers for environmental prediction oregon state university air force hydrology lab persiann precipitation estimation from remotely sensed information using artificial neural networks prec l precipitation reconstruction over land ssm i special sensor microwave imager trmm tropical rainfall measuring mission udel university of delaware usgs united states geological survey vic variable infiltration capacity credit authorship contribution statement mijael rodrigo vargas godoy conceptualization software data curation writing original draft visualization yannis markonis conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was carried out within the project investigation of terrestrial hydrological cycle acceleration ithaca funded by the czech science foundation grant 22 33266m mrvg was supported by the faculty of environmental sciences czech university of life sciences prague internal grant 2023b0001 chimera charting hydrological intensification of atmospheric fluxes in reanalysis data appendix case study r code 
25393,real time simulation is crucial in time critical situations within the environmental modeling domain the advancement of web technologies has facilitated the sharing and integration of various resources over the internet web based real time modeling has gained increasing attention however interoperability remains a challenge when integrating environmental models and real time observations in a web environment this paper proposes an interoperable and service oriented approach for real time modeling by coupling the open geospatial consortium ogc web processing service wps and sensorthings application programming interface api the sensorthings api provides a unified way to interconnect environmental sensors data and applications the ogc wps defines standard operations for accessing online processes we extended ogc wps to support the message queuing telemetry transport mqtt protocol and integrated it with sensorthings api this integration enables real time access to sensor observations and time series simulation results a prototype system using the storm water management model swmm demonstrates the applicability and effectiveness of our approach keywords environmental models real time simulation ogc wps sensorthings api interoperability data availability data will be made available on request 1 introduction environmental models play a significant role in geographical and environmental simulations chen et al 2020 granell et al 2013 laniak et al 2013 ma et al 2019 real time modeling and simulation are vital in time critical situations such as disaster risk response shangguan et al 2019 real time model calibration ma et al 2022 and urban stormwater management zeng et al 2021 the rapid development of the internet of things iot is driving the increased deployment of sensors for environmental monitoring a y sun et al 2019 iot connects various devices applications and networks to the cloud making sensor observations accessible in real time in addition the web based approach has become prevalent in sharing environmental models chen et al 2020 maghami et al 2023 qin et al 2023 sun et al 2019 yue et al 2020 zhang et al 2022 due to its cross platform capability wide availability and reusability there is great potential to enable real time modeling by coupling iot technologies and environmental models over the internet it allows environmental models instantly access continuous observations and facilitates the efficient and timely delivery of simulation results many complex environmental models such as time marching models use a numerical time stepping procedure to simulate time varying phenomena zhang et al 2020 these models often follow multiple simulation phases and perform time step computations goodall et al 2011 peckham et al 2013 vanecek and roger 2014 in a web environment there are two main approaches to provide time series input data one approach is to use archived time series data as input and perform time step computation in a loop inside the model the other approach is to support time step based data exchange at runtime real time modeling requires the latter approach with continuous observations to drive the time step computations currently several web based approaches already support real time modeling or dynamic interactions gao et al 2019 herle and blankenbach 2018 zeng et al 2021 zeng et al 2021 designed and implemented an swmm based service for real time urban stormwater management to address the issue of complex and dynamic interaction gao et al 2019 deployed the environmental models as custom made web services using the websocket protocol in their work web interfaces and transmitted data encoding were customized limiting interoperability and reusability interoperability is critical for accessing and integrating web services belete et al 2017 zhang et al 2020 the open geospatial consortium ogc is a major organization that works on developing interoperable geospatial web services standards the ogc web processing service wps provides a general approach to making traditional analysis functions accessible through standard web interfaces it has gained increasing attention for its ability to share environmental models qiao et al 2019 zhang et al 2019 2020 c zhang et al 2019 for instance zhang et al 2020 extended wps with four key considerations maintaining state data representation model description and model execution this allows environmental models to perform time step calculations on the web some works have integrated wps and sensor observations for environmental monitoring and real time geoprocessing stasch et al 2018 coupled sensor observation service sos bröring et al 2012 and wps to enable online geoprocessing in water dam monitoring the sos uses the http protocol for communication and follows the request response pattern which requires clients to access observations in a loop herle and blankenbach 2018 enhanced wps with the mqtt protocol which allows observation data to automatically feed into wps however their work depends on a particular tool named geopipes and does not follow a sensor observation standard the ogc sensorthings api liang et al 2016 provides a unified way to interconnect iot devices data and applications over the web it offers flexible standardized interfaces for retrieving observations from heterogeneous iot sensor systems the sensorthings service also includes an mqtt extension to support the publish subscribe communication pattern we propose an interoperable and service oriented approach for real time modeling by coupling ogc wps and sensorthings api integrating the sensorthings api and wps can take advantage of both standards and improve the interoperability of web based real time modeling this paper is organized as follows section 2 introduces the proposed approach in section 3 we demonstrate the implementation of the prototype system and a use case section 4 discusses the contribution of the proposed method as well as its advantages and limitations finally we present our conclusion in section 5 2 design 2 1 key considerations for interoperability between ogc wps and sensorthings api the sensorthings api is designed specifically for resource constrained iot devices and follows the rest representational state transfer principles it uses efficient json encoding and supports the mqtt protocol mqtt message queuing telemetry transport is a standard messaging protocol for the iot and is already widely used in various industries it is designed as a lightweight open and straightforward publish subscribe messaging transport the asynchronous delivery of messages distinguishes the publish subscribe pattern from the request response practice the former can subscribe to persistent observations while ogc wps only supports request response patterns limiting its usage in real time we proposed a method to extend ogc wps and integrate it with sensorthings api based on the mqtt protocol the observations from iot sensor systems can be ingested into environmental models in real time to improve the interoperability and reusability key issues including the communication protocol data exchange and execution workflow are considered as shown in fig 1 regarding the communication protocol we designed the role of wps in such integration and the method to extend the wps conceptual model regarding data exchange we proposed a wps compliant method to describe data inputs and outputs of model services as domain specific descriptions are an essential part of web services jiang et al 2017 yue et al 2016 f f zhang et al 2019 in addition we designed the interoperable representation of topics and messages for mutual understanding between ogc wps and sensorthings api regarding execution the collaborative execution workflow between wps and sensorthings api is designed 2 2 the role of ogc wps and sensorthings api the publish subscribe pattern decouples the client that sends a message i e the publisher from the clients that receive the messages i e the subscribers a third component called a broker handles communication between them it accepts incoming messages from publishers filters them and distributes the messages of interest to subscribers in sensorthings api a client can send an mqtt subscription request to the sensorthings server when the server receives matching observations from third parties or sensors it immediately pushes them to the appropriate clients which can be various browsers or other applications the wps server is responsible for receiving inputs performing model execution and publishing the outputs as available resources on the web firstly the wps server must handle interactions between the observations and environmental models this requires the wps server to have a subscription function secondly the real time visualization of model simulation results is also crucial one model instance can generate several data streams the wps server should have the publish function to meet such requirements the wps server plays both roles of subscriber and publisher it submits a subscription to the broker sensorthings server once such observations are available the sensorthings server publishes them to the wps server when the model performs one time step computation the wps server can immediately publish the result to a specific broker which can also be the sensorthings server 2 3 extensions to ogc wps conceptual model ogc wps provides a core conceptual model that defines the basic requirements for a web based processing service müller and pross 2015 it can specify a wps in different architectures such as rest and soap simple object access protocol in wps specification a process is a function that calculates outputs for a given set of data inputs müller and pross 2015 a wps server provides access to computational processing services a processing job will be created for particular process execution the client can monitor and control the job using a job id as shown in fig 2 environmental models represent the functioning of various processes we added subscription and publication functions to the job component the job can subscribe to and receive observations from the mqtt broker it runs the model step by step and publishes the results to the mqtt broker using job id as an identifier it requires the wps server to handle the mqtt protocol 2 4 data transmission using both mqtt and http protocols mqtt is an iot interaction protocol that can transmit data between sensors and applications http hyper text transfer protocol is applied by the world wide web www and is widely used for data transmission among web based applications such as wps as illustrated in fig 2 mqtt and http protocols work together to transmit data between server side models and heterogeneous clients there are two distinct ways for data transmission in wps müller and pross 2015 1 by value and 2 by reference small or atomic data such as integers and short strings are typically submitted by value large data inputs such as web accessible data or services are usually supplied by references the wps server is responsible for fetching the referenced data for model execution in a real time modeling scenario environmental observations are continuous inputs that can be provided as data services the service information including the address of the mqtt broker subscription topic and authentication information if required is submitted by the http request after receiving the request the wps server will subscribe to the particular mqtt broker using the specified subscription topic e g an observed property from a sensor the wps server can receive observations continuously and publish the results of time step computation immediately using the mqtt protocol the clients can access the final results using an http request after the model execution 2 5 data representation and description the wps specification makes no assumptions about the encoding of the transmitted data müller and pross 2015 without further information it is difficult for the receiver to automatically detect the semantics of nested inputs the data format description that includes encoding attributes is necessary to decode the transmitted data ogc wps specifies a processofferings document to describe processes in the wps server it contains several elements that represent data inputs including their mimetype encoding and schema such information can guide the encoding and decoding of the transmitted data wps 2 0 allows nested structures which enhances data description capabilities however the processofferings document still has limitations in distinguishing the inputs used for different model phases such as initialization and time step computations a specific metadata element with a unique title name i e iotype can indicate the correspondence between execution phases and inputs outputs zhang et al 2020 they used nested description structures to represent multiple dimensions of inputs and outputs our approach also uses nested descriptions with specific metadata elements to indicate the inputs outputs that support mqtt protocol fig 3 shows the data description structure that can represent the complex semantics of the model services the input output elements with the type runtime describe the exchange data at runtime which is used for time step based computations they may contain three nested children element quantity and time which describe the value location and time dimensions respectively zhang et al 2020 in our extensions they may also have a nested child identified by mqtt the mqtt element indicates that the input is supposed to be from an mqtt broker it contains at least two nested children the address of the mqtt broker and the topic our approach allows the wps server to publish time series outputs to an mqtt broker this requires clients to provide the mqtt address as shown in fig 3 in the description document an output child with the type mqtt is nested into an output with the type runtime it indicates that the wps server can publish this output to an mqtt broker the mqtt address is provided in the input element with the same identifier as the output in this way the wps server can publish different outcomes to corresponding mqtt brokers 2 6 interoperable topics and message in the publish subscribe pattern the structure design of the topics and messages affects the feasibility and interoperability of information exchange among different components wps and sensorthings api use different data models to represent time series data we propose a mapping method for data exchange between them which maintains equivalence in both semantic content and hierarchical information level the wps specification uses a basic data model that supports standard or non standard data formats for inputs and outputs müller and pross 2015 as discussed in section 2 5 the nested structure can represent the three dimensions of transmitted data value location and time the model service can use such structured data as inputs the semantic mapping between the sensing entities from sensorthings api and the data model used in wps is shown in fig 4 the rectangles with gray backgrounds represent sensing entities and those with white backgrounds represent elements from wps the dotted lines represent the mapping relationship a thing is an object of the physical or information world that can be identified and integrated into communication networks a model instance corresponds to a thing a virtual object a datastream is a collection of observations that measure the same phenomenon from the same thing a thing may have zero to many datastreams the datastream entities correspond to the model inputs outputs an output of a model instance can be treated as a datastream generated by this model such as the outlet runoff in a runoff simulation model the model consumes datastreams from other things as inputs an observation instance is classified by its event time e g resulttime and phenonmenontime featureofinterest observedproperty and the procedure used the phenomenontime and observedproperty correspond to the time and quantity dimensions of model input output respectively an observation observes one and only one featureofinterest which corresponds to the location dimension the job component receives observations from the sensorthings server and maps them to time step based model inputs it also transforms the outputs of time step based computation to observations that comply with sensorthings api third party clients that subscribe to these results can receive the observations in standard formats 2 7 interoperable execution workflow using the standard operations fig 5 shows the interoperable execution workflow for coupling ogc wps and sensorthings api the interactions among the clients wps server and the sensorthings server are based on standard operations the sensorthings api uses rest web service style and clients can efficiently perform create update and delete actions on the sensorthings entity types the wps execute operation allows the client to start the environmental models on the server when the wps server receives the execute request it initializes the model and creates an asynchronous processing job the jobid is the identifier used by clients to interact with one specific model instance on the server the wps server also subscribes to the sensorthings server using sensorthings api when the wps server receives the appropriate data from the sensorthings server it performs a time step based computation and then waits for input data for the following step computation the result of time step computation is delivered to the sensorthings service by creating observation entities using sensorthings api clients can then assess the simulation results in real time 3 implementation 3 1 prototype implementation some open source frameworks already facilitate geospatial process deployment following the wps specification such as pywps https pywps org and 52 north javaps project https github com 52north javaps javaps features a pluggable architecture for process and data encodings in our previous work zhang et al 2020 we extended javaps to publish environmental models it supports running the models following the initialization time step based calculation and finish phases we further extended javaps to support the mqtt protocol we also developed a project named wps4m https github com geoprocessing wps4m to facilitate the deployment of environmental models wps4m is an external model repository that can be configured into the javaps platform fig 6 shows the core extensions to the javaps project and its relationship with wps4m in the javaps project the engine component is responsible for job creation and monitoring the job controls the execution of a specific process instance consistent with the wps conceptual model fig 7 shows how the job manages the model execution workflow after extensions the original job in javaps uses execute method to invoke the processes our updated version uses the execute method to initialize the model the performstep method advances the time step computations while the subscribe and publish methods subscribe to real time observations and publish time series results at runtime if mqtt inputs or outputs exist the job checks for them and invokes the subscribe or publish method the job maintains a data container named inputcache when it receives observations from the sensorthings server it pushes the data into this container the job can also receive observations from clients with performstep requests once new data arrives the job checks whether it is enough to run the current time step computation since the step time is not always the same as the interval of observations this container acts as a data cache if mqtt outputs are generated the results of the time step computation are published to the sensorthings server using the mqtt protocol once if the time step computation doesn t reach the end time of the simulation the job waits for new data and performs the following time step computation the eclipse paho https www eclipse org paho project provides an open source and client side implementation of mqtt which we use to perform mqtt subscription and publication in our software the 52 north delivers a sensorthings api implementation project https github com 52north sensorweb server sta which is open source and is used as the mqtt broker the extended javaps project is available on github https github com geoprocessing javaps git mqtt branch 3 2 case study showcase 1 deploying swmm5 using our approach this paper uses the storm water management model version 5 swmm5 as a case study to evaluate the feasibility of the proposed approach and prototype system swmm5 is a rainfall runoff simulation model that can simulate the hydrologic and hydraulic states of the study areas such as the sub catchment runoff and water levels in system links gironás et al 2010 the particular hydrological objects named rain gages provide precipitation data for this model traditionally rain gages rely on archived rainfall data as input swmm5 only requires a single file containing all relevant information e g study area descriptions and simulation options to execute upon successful completion an output file containing time series results is generated the united states environmental protection agency provides an open source implementation of the swmm5 model in c programming language called epa swmm5 open water analytics also developed an enhanced version of epa swmm5 called owa swmm5 https github com openwateranalytics stormwater management model the computation engine of owa swmm5 exposed several methods that other programming languages can access in addition custom routines can be performed between each time step computation swmm4j https github com geoprocessing swmm4j git reuses owa swmm5 source codes through the java native interface jni it exposes three essential methods initialize performstep and finish the performstep method sets the current precipitation of rain gages and advances the time step computation swmm4j is integrated into the javaps through the wps4m project and can therefore be accessed as a model service using our approach the relationship among javaps wps4m swmm4j and owa swmm5 is shown in fig 6 2 case study area fig 8 shows the study area and the model schema used in the use case this use case tamaddun 2021 which includes the model parameters and data precipitation is derived from the hydroshare platform hydroshare is a powerful web based platform designed for managing and sharing diverse types of environmental data and models horsburgh et al 2016 the study area covers the scotts level branch watershed located in baltimore county maryland the length of this entire watershed is about 4 2 miles and its area spans approximately 1 83 square miles in this use case tamaddun 2021 the study area is divided into six sub catchments w150 w160 w170 w180 w240 w260 the conveyance system consists of storm sewer conduits r30 r80 r110 r130 junctions j49 j41 j44 j38 and one outlet one rain gauge provides the precipitation data and is assigned to all sub catchments in our implementation this rain gage is treated as a virtual iot thing that generates rainfall data and publishes them to the sensorthings server swwm5 can use the precipitation data directly from the sensor web the swmm5 instance is treated as a virtual thing and the expected outputs are treated as datastreams entities we assume that users are interested in real time runoff from the outlet and the water level in pipeline r110 fig 9 shows the related entities of interest used in this use case 3 a walkthrough example we provide a workthrough example to demonstrate how to use developed swmm5 service it covers the model description model initialization time step based computation and real time access to the simulation results a model description the model description is crucial for model discovery and use in a web environment in the ogc wps standard the describeprocess request allows clients to obtain a detailed description of the specific model including the required inputs and their supposed outputs fig 10 shows the description segments of the swmm5 service the rectangles represent the elements in the description structure the nearby name is the identifier of the element it indicates that this model requires an input file i e inpfile for model initialization and precipitation data for time step based computation i e precipitation the precipitation input can have more than one input depending on the number of sensors that provide precipitation data clients can submit precipitation data by combining children elements i e time location and value or by using an mqtt broker i e mqttinput the inputs also include the simulation s start time end time and time step the wps server will publish the simulation results to the mqtt broker i e outputmqtt b model initialization fig 11 shows some xml fragments in an execute request that contains the necessary input parameters the inpfile is transmitted by reference while the starttime endtime and timestep are sent by value a sensorthings service provides the precipitation data for time step based computation the url is the address of sensorthings service and the topic is the target data stream datastream raingage when the wps server receives the execute request it initializes the model and subscribes to the sensorthings server using the specified broker address tcp 127 0 0 1 1883 and topic v1 1 datastream raingage the outputmqtt tcp 127 0 0 1 1884 v1 1 things refers to a sensorthings service the wps server creates a thing entity by specifying the property of iot id as jobid it also creates several datastream entities including datastream outlet runoff and datastream r110 waterlevel which are associated with the thing jobid entity c time step based computation once the sensors or other applications publish observations to the sensorthings server it will perform filtering and push the required observations to the wps server when the server receives the observations from datastream raingage it will calculate the hydrologic and hydraulic states of the study areas including the outlet runoff and the r100 water level the result of time step computation will be delivered to the sensorthings service by creating observation entities using sensorthings api for example the outlet runoff will be published to the sensorthings server using the topic thing jobid datastream outlet runoff and the water level of r110 will be published using the thing jobid datastream r110 waterlevel as the topic d real time access to the simulation result any client can retrieve or subscribe to the result of time step based computation using the jobid and the sensorthings api the clients can obtain the available datastreams using the request pattern service root uri things jobid for example in our swmm5 scenario the available datastreams include the datastream outlet runoff and datastream r110 waterlevel then the client can use the request service root uri datastreams r110 waterlevel to obtain the historical water level of r110 the client also can subscribe to this data stream using the mqtt protocol in this way the clients can receive the simulation results and visualize them at runtime the job will cancel the subscription and write the result files after the loop the client can use the getresult operation to retrieve the full results of the simulation the additional finish operation allows users to stop the simulation and trigger the cancellation operation fig 12 shows the outlet runoff and the water level in conduit r110 4 discussion interoperability is a vital consideration when it comes to sharing and integrating web services the ogc aims to make geospatial information and services fair findable accessible interoperable and reusable our approach is based on two widely used ogc standards wps and sensorthings api which provides an interoperable solution for coupling environmental models and sensor observations over the internet 1 interoperable sharing of environmental models using ogc wps the web based sharing of environmental models has attracted increasing attention it enables the accessibility of heterogeneous models across the internet although the wps is already commonly used in the geospatial domain it has some limitations in handling multiple execution phases and time step computations of models yue et al 2015 zhang et al 2020 users submit all necessary data and parameters to the wps server via an execute request and there is no data exchange during the execution some works have extended ogc wps to support continuous or real time geoprocessing herle and blankenbach 2018 stasch et al 2018 however unlike traditional gis geoprocessing functions complex environmental models are often run following multiple simulation phases and based on time step computations goodall et al 2011 peckham et al 2013 vanecek and roger 2014 while existing works provide real time observations to server side models additional operations are still needed for the initialization and finish of the model services in addition herle and blankenbach 2018 used wps 1 0 for real time geoprocessing the wps 1 0 only supports synchronous execution and implements the http post xml and http get kvp encodings while the latest version wps 2 0 comprises both synchronous and asynchronous execution and supports rest interfaces in our previous work zhang et al 2020 we extended ogc wps 2 0 to run environmental models on the web following the model execution phases i e initialization time step calculations and finish the execute operation is used to initiate the model execution process the performstep operation is added to support time step interactions between the clients and the server side models using the http protocol however this approach has limitations in using real time observations from iot directly this paper extends the previous approach by integrating iot observations into environmental models for real time modeling when the simulation s start time is earlier than the latest subscription data clients can use the performstep operation to provide necessary observations we extended wps to share environmental models that bridge the traditional environmental domain to the geospatial domain interoperably it provides valuable insights for the further development of the wps standard integrating environmental models and traditional gis is a continuous study topic clark 1998 goodchild et al 1992 ng et al 2018 santoalla et al 2018 yue et al 2015 our approach presents a promising way to couple these two domains based on the wps standard 2 interoperable access to sensor observations using sensorthings api the widespread deployment of iot devices has significantly improved the capabilities of environmental monitoring interoperability is crucial in connecting iot devices observations and various applications over the internet ogc has already developed standards to discover access and task iot resources for example the sensor observation service sos and the sensorthings api can both retrieve sensor descriptions and observations by standardized requests the sos is an earlier standard that relies on older technologies such as xml and soap for data encoding it only supports request response patterns for data exchange among different applications which usually requires specific clients to retrieve sensor observations in a loop for example yue et al 2015 designed a particular data component for integrating sensor observations from sos into environmental models sensorthings api is a relatively new standard that follows modern web design principles jacoby and usländer 2020 such as restful services and json as a payload format it also supports the mqtt protocol which uses a publish subscribe pattern for data exchange the sensor observations can be delivered to subscribers as soon as they are produced herle and blankenbach 2018 used a customized geomqtt to enhance the ogc wps interfaces with mqtt support for real time geoprocessing while we accessed the sensor observations using the sensorthings api which takes the benefits of this standard and offers an interoperable solution 3 interoperable data exchange when coupling ogc wps and sensorthings api this paper proposes a web based and interoperable approach for real time modeling by coupling ogc wps and sensorthings api this approach enables instant and continuous model simulation while observations are produced it also facilitates the efficient and timely delivery of outcomes without installing desktop software it is helpful for timely warning in environmental monitoring and simulation scenarios the proposed approach can be easily applied to other environmental models firstly the users need to publish the models as wps compliant services using our approach we extended the 52 north javaps project and developed a wps4m project to facilitate the model deployment secondly the environmental sensors should be connected to the iot servers that follow the sensorthings api standard then data exchange among iot devices environmental models and various clients is based on standardized operations ogc wps and sensorthings api work collaboratively in a loose coupling way the users submit the broker i e sensorthings server information e g address and topic to the wps server using standard wps requests the server will automatically subscribe and retrieve the necessary observations for the environmental models using sensorthings api an information model mapping method is proposed to help mutual understanding between wps and sensorthings api any sensorthings compliant clients can visualize the real time simulation over the internet a variety of legacy resources can be reused 4 current limitations and future work numerical time marching models typically run based on specific time steps which may differ from the time intervals of iot measurements in such a situation data interpolation is necessary before time step based computations the wps engine can perform the interpolation function in future work we will investigate such interpolation methods the proposed method allows users to translate the arguments to the model service when they start and supports time series data exchange at runtime however when the used sensor is broken the server side model will continue to wait for sensor observations in this case the clients can use the performstep operation and manually send the necessary observations from nearby sensors to the models however it is better to allow the clients to update the model arguments such as subscribing to other sensor observations in addition the real time calibration of model parameters is essential for accurate simulation and prediction ma et al 2022 in future work we will extend ogc wps further to allow users to update the model parameters and perform real time calibration during its execution 5 conclusion in time critical situations real time modeling requires real time observations we propose a novel approach for real time modeling that utilizes web services and iot technologies interoperably this paper extends wps to support the publish subscribe communication pattern the wps and sensorthings api work collaboratively based on the mqtt protocol we also propose a mapping method between the information models of these two standards to demonstrate the applicability of our methods we developed a prototype system and used the swmm5 model example with our approach the environmental models can consume observations directly from iot devices furthermore the time step based outputs of environmental models are accessible at runtime software availability 1 program name javaps mqtt extension developer mingda zhang year first available 2022 hardware requirements pc server system requirements windows linux software requirements 52 north javaps eclipse paho 1 2 program language java 12 program size 5m javaps project with mqtt extension availability https github com geoprocessing javaps mqtt extension branch license apache license 2 0 2 program name wps4m developer mingda zhang year first available 2019 hardware requirements pc server system requirements windows linux software requirements 52 north javaps javaps mqtt extension program language java 12 program size 350 kb availability https github com geoprocessing wps4m license apache license 2 0 3 program name swmm4j developer mingda zhang year first available 2019 hardware requirements pc server system requirements windows linux software requirements owa swmm5 program language java 12 program size 64 kb availability https github com geoprocessing swmm4j license apache license 2 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the editors and anonymous reviewers for their very constructive comments that help improve the quality of the paper this work was supported by the natural science foundation of hubei province no 2022cfb119 the national natural science foundation of china no 41901313 and 42001330 and the open project program of hubei key laboratory of regional development and environment response hubei university no 2020 b 002 
25393,real time simulation is crucial in time critical situations within the environmental modeling domain the advancement of web technologies has facilitated the sharing and integration of various resources over the internet web based real time modeling has gained increasing attention however interoperability remains a challenge when integrating environmental models and real time observations in a web environment this paper proposes an interoperable and service oriented approach for real time modeling by coupling the open geospatial consortium ogc web processing service wps and sensorthings application programming interface api the sensorthings api provides a unified way to interconnect environmental sensors data and applications the ogc wps defines standard operations for accessing online processes we extended ogc wps to support the message queuing telemetry transport mqtt protocol and integrated it with sensorthings api this integration enables real time access to sensor observations and time series simulation results a prototype system using the storm water management model swmm demonstrates the applicability and effectiveness of our approach keywords environmental models real time simulation ogc wps sensorthings api interoperability data availability data will be made available on request 1 introduction environmental models play a significant role in geographical and environmental simulations chen et al 2020 granell et al 2013 laniak et al 2013 ma et al 2019 real time modeling and simulation are vital in time critical situations such as disaster risk response shangguan et al 2019 real time model calibration ma et al 2022 and urban stormwater management zeng et al 2021 the rapid development of the internet of things iot is driving the increased deployment of sensors for environmental monitoring a y sun et al 2019 iot connects various devices applications and networks to the cloud making sensor observations accessible in real time in addition the web based approach has become prevalent in sharing environmental models chen et al 2020 maghami et al 2023 qin et al 2023 sun et al 2019 yue et al 2020 zhang et al 2022 due to its cross platform capability wide availability and reusability there is great potential to enable real time modeling by coupling iot technologies and environmental models over the internet it allows environmental models instantly access continuous observations and facilitates the efficient and timely delivery of simulation results many complex environmental models such as time marching models use a numerical time stepping procedure to simulate time varying phenomena zhang et al 2020 these models often follow multiple simulation phases and perform time step computations goodall et al 2011 peckham et al 2013 vanecek and roger 2014 in a web environment there are two main approaches to provide time series input data one approach is to use archived time series data as input and perform time step computation in a loop inside the model the other approach is to support time step based data exchange at runtime real time modeling requires the latter approach with continuous observations to drive the time step computations currently several web based approaches already support real time modeling or dynamic interactions gao et al 2019 herle and blankenbach 2018 zeng et al 2021 zeng et al 2021 designed and implemented an swmm based service for real time urban stormwater management to address the issue of complex and dynamic interaction gao et al 2019 deployed the environmental models as custom made web services using the websocket protocol in their work web interfaces and transmitted data encoding were customized limiting interoperability and reusability interoperability is critical for accessing and integrating web services belete et al 2017 zhang et al 2020 the open geospatial consortium ogc is a major organization that works on developing interoperable geospatial web services standards the ogc web processing service wps provides a general approach to making traditional analysis functions accessible through standard web interfaces it has gained increasing attention for its ability to share environmental models qiao et al 2019 zhang et al 2019 2020 c zhang et al 2019 for instance zhang et al 2020 extended wps with four key considerations maintaining state data representation model description and model execution this allows environmental models to perform time step calculations on the web some works have integrated wps and sensor observations for environmental monitoring and real time geoprocessing stasch et al 2018 coupled sensor observation service sos bröring et al 2012 and wps to enable online geoprocessing in water dam monitoring the sos uses the http protocol for communication and follows the request response pattern which requires clients to access observations in a loop herle and blankenbach 2018 enhanced wps with the mqtt protocol which allows observation data to automatically feed into wps however their work depends on a particular tool named geopipes and does not follow a sensor observation standard the ogc sensorthings api liang et al 2016 provides a unified way to interconnect iot devices data and applications over the web it offers flexible standardized interfaces for retrieving observations from heterogeneous iot sensor systems the sensorthings service also includes an mqtt extension to support the publish subscribe communication pattern we propose an interoperable and service oriented approach for real time modeling by coupling ogc wps and sensorthings api integrating the sensorthings api and wps can take advantage of both standards and improve the interoperability of web based real time modeling this paper is organized as follows section 2 introduces the proposed approach in section 3 we demonstrate the implementation of the prototype system and a use case section 4 discusses the contribution of the proposed method as well as its advantages and limitations finally we present our conclusion in section 5 2 design 2 1 key considerations for interoperability between ogc wps and sensorthings api the sensorthings api is designed specifically for resource constrained iot devices and follows the rest representational state transfer principles it uses efficient json encoding and supports the mqtt protocol mqtt message queuing telemetry transport is a standard messaging protocol for the iot and is already widely used in various industries it is designed as a lightweight open and straightforward publish subscribe messaging transport the asynchronous delivery of messages distinguishes the publish subscribe pattern from the request response practice the former can subscribe to persistent observations while ogc wps only supports request response patterns limiting its usage in real time we proposed a method to extend ogc wps and integrate it with sensorthings api based on the mqtt protocol the observations from iot sensor systems can be ingested into environmental models in real time to improve the interoperability and reusability key issues including the communication protocol data exchange and execution workflow are considered as shown in fig 1 regarding the communication protocol we designed the role of wps in such integration and the method to extend the wps conceptual model regarding data exchange we proposed a wps compliant method to describe data inputs and outputs of model services as domain specific descriptions are an essential part of web services jiang et al 2017 yue et al 2016 f f zhang et al 2019 in addition we designed the interoperable representation of topics and messages for mutual understanding between ogc wps and sensorthings api regarding execution the collaborative execution workflow between wps and sensorthings api is designed 2 2 the role of ogc wps and sensorthings api the publish subscribe pattern decouples the client that sends a message i e the publisher from the clients that receive the messages i e the subscribers a third component called a broker handles communication between them it accepts incoming messages from publishers filters them and distributes the messages of interest to subscribers in sensorthings api a client can send an mqtt subscription request to the sensorthings server when the server receives matching observations from third parties or sensors it immediately pushes them to the appropriate clients which can be various browsers or other applications the wps server is responsible for receiving inputs performing model execution and publishing the outputs as available resources on the web firstly the wps server must handle interactions between the observations and environmental models this requires the wps server to have a subscription function secondly the real time visualization of model simulation results is also crucial one model instance can generate several data streams the wps server should have the publish function to meet such requirements the wps server plays both roles of subscriber and publisher it submits a subscription to the broker sensorthings server once such observations are available the sensorthings server publishes them to the wps server when the model performs one time step computation the wps server can immediately publish the result to a specific broker which can also be the sensorthings server 2 3 extensions to ogc wps conceptual model ogc wps provides a core conceptual model that defines the basic requirements for a web based processing service müller and pross 2015 it can specify a wps in different architectures such as rest and soap simple object access protocol in wps specification a process is a function that calculates outputs for a given set of data inputs müller and pross 2015 a wps server provides access to computational processing services a processing job will be created for particular process execution the client can monitor and control the job using a job id as shown in fig 2 environmental models represent the functioning of various processes we added subscription and publication functions to the job component the job can subscribe to and receive observations from the mqtt broker it runs the model step by step and publishes the results to the mqtt broker using job id as an identifier it requires the wps server to handle the mqtt protocol 2 4 data transmission using both mqtt and http protocols mqtt is an iot interaction protocol that can transmit data between sensors and applications http hyper text transfer protocol is applied by the world wide web www and is widely used for data transmission among web based applications such as wps as illustrated in fig 2 mqtt and http protocols work together to transmit data between server side models and heterogeneous clients there are two distinct ways for data transmission in wps müller and pross 2015 1 by value and 2 by reference small or atomic data such as integers and short strings are typically submitted by value large data inputs such as web accessible data or services are usually supplied by references the wps server is responsible for fetching the referenced data for model execution in a real time modeling scenario environmental observations are continuous inputs that can be provided as data services the service information including the address of the mqtt broker subscription topic and authentication information if required is submitted by the http request after receiving the request the wps server will subscribe to the particular mqtt broker using the specified subscription topic e g an observed property from a sensor the wps server can receive observations continuously and publish the results of time step computation immediately using the mqtt protocol the clients can access the final results using an http request after the model execution 2 5 data representation and description the wps specification makes no assumptions about the encoding of the transmitted data müller and pross 2015 without further information it is difficult for the receiver to automatically detect the semantics of nested inputs the data format description that includes encoding attributes is necessary to decode the transmitted data ogc wps specifies a processofferings document to describe processes in the wps server it contains several elements that represent data inputs including their mimetype encoding and schema such information can guide the encoding and decoding of the transmitted data wps 2 0 allows nested structures which enhances data description capabilities however the processofferings document still has limitations in distinguishing the inputs used for different model phases such as initialization and time step computations a specific metadata element with a unique title name i e iotype can indicate the correspondence between execution phases and inputs outputs zhang et al 2020 they used nested description structures to represent multiple dimensions of inputs and outputs our approach also uses nested descriptions with specific metadata elements to indicate the inputs outputs that support mqtt protocol fig 3 shows the data description structure that can represent the complex semantics of the model services the input output elements with the type runtime describe the exchange data at runtime which is used for time step based computations they may contain three nested children element quantity and time which describe the value location and time dimensions respectively zhang et al 2020 in our extensions they may also have a nested child identified by mqtt the mqtt element indicates that the input is supposed to be from an mqtt broker it contains at least two nested children the address of the mqtt broker and the topic our approach allows the wps server to publish time series outputs to an mqtt broker this requires clients to provide the mqtt address as shown in fig 3 in the description document an output child with the type mqtt is nested into an output with the type runtime it indicates that the wps server can publish this output to an mqtt broker the mqtt address is provided in the input element with the same identifier as the output in this way the wps server can publish different outcomes to corresponding mqtt brokers 2 6 interoperable topics and message in the publish subscribe pattern the structure design of the topics and messages affects the feasibility and interoperability of information exchange among different components wps and sensorthings api use different data models to represent time series data we propose a mapping method for data exchange between them which maintains equivalence in both semantic content and hierarchical information level the wps specification uses a basic data model that supports standard or non standard data formats for inputs and outputs müller and pross 2015 as discussed in section 2 5 the nested structure can represent the three dimensions of transmitted data value location and time the model service can use such structured data as inputs the semantic mapping between the sensing entities from sensorthings api and the data model used in wps is shown in fig 4 the rectangles with gray backgrounds represent sensing entities and those with white backgrounds represent elements from wps the dotted lines represent the mapping relationship a thing is an object of the physical or information world that can be identified and integrated into communication networks a model instance corresponds to a thing a virtual object a datastream is a collection of observations that measure the same phenomenon from the same thing a thing may have zero to many datastreams the datastream entities correspond to the model inputs outputs an output of a model instance can be treated as a datastream generated by this model such as the outlet runoff in a runoff simulation model the model consumes datastreams from other things as inputs an observation instance is classified by its event time e g resulttime and phenonmenontime featureofinterest observedproperty and the procedure used the phenomenontime and observedproperty correspond to the time and quantity dimensions of model input output respectively an observation observes one and only one featureofinterest which corresponds to the location dimension the job component receives observations from the sensorthings server and maps them to time step based model inputs it also transforms the outputs of time step based computation to observations that comply with sensorthings api third party clients that subscribe to these results can receive the observations in standard formats 2 7 interoperable execution workflow using the standard operations fig 5 shows the interoperable execution workflow for coupling ogc wps and sensorthings api the interactions among the clients wps server and the sensorthings server are based on standard operations the sensorthings api uses rest web service style and clients can efficiently perform create update and delete actions on the sensorthings entity types the wps execute operation allows the client to start the environmental models on the server when the wps server receives the execute request it initializes the model and creates an asynchronous processing job the jobid is the identifier used by clients to interact with one specific model instance on the server the wps server also subscribes to the sensorthings server using sensorthings api when the wps server receives the appropriate data from the sensorthings server it performs a time step based computation and then waits for input data for the following step computation the result of time step computation is delivered to the sensorthings service by creating observation entities using sensorthings api clients can then assess the simulation results in real time 3 implementation 3 1 prototype implementation some open source frameworks already facilitate geospatial process deployment following the wps specification such as pywps https pywps org and 52 north javaps project https github com 52north javaps javaps features a pluggable architecture for process and data encodings in our previous work zhang et al 2020 we extended javaps to publish environmental models it supports running the models following the initialization time step based calculation and finish phases we further extended javaps to support the mqtt protocol we also developed a project named wps4m https github com geoprocessing wps4m to facilitate the deployment of environmental models wps4m is an external model repository that can be configured into the javaps platform fig 6 shows the core extensions to the javaps project and its relationship with wps4m in the javaps project the engine component is responsible for job creation and monitoring the job controls the execution of a specific process instance consistent with the wps conceptual model fig 7 shows how the job manages the model execution workflow after extensions the original job in javaps uses execute method to invoke the processes our updated version uses the execute method to initialize the model the performstep method advances the time step computations while the subscribe and publish methods subscribe to real time observations and publish time series results at runtime if mqtt inputs or outputs exist the job checks for them and invokes the subscribe or publish method the job maintains a data container named inputcache when it receives observations from the sensorthings server it pushes the data into this container the job can also receive observations from clients with performstep requests once new data arrives the job checks whether it is enough to run the current time step computation since the step time is not always the same as the interval of observations this container acts as a data cache if mqtt outputs are generated the results of the time step computation are published to the sensorthings server using the mqtt protocol once if the time step computation doesn t reach the end time of the simulation the job waits for new data and performs the following time step computation the eclipse paho https www eclipse org paho project provides an open source and client side implementation of mqtt which we use to perform mqtt subscription and publication in our software the 52 north delivers a sensorthings api implementation project https github com 52north sensorweb server sta which is open source and is used as the mqtt broker the extended javaps project is available on github https github com geoprocessing javaps git mqtt branch 3 2 case study showcase 1 deploying swmm5 using our approach this paper uses the storm water management model version 5 swmm5 as a case study to evaluate the feasibility of the proposed approach and prototype system swmm5 is a rainfall runoff simulation model that can simulate the hydrologic and hydraulic states of the study areas such as the sub catchment runoff and water levels in system links gironás et al 2010 the particular hydrological objects named rain gages provide precipitation data for this model traditionally rain gages rely on archived rainfall data as input swmm5 only requires a single file containing all relevant information e g study area descriptions and simulation options to execute upon successful completion an output file containing time series results is generated the united states environmental protection agency provides an open source implementation of the swmm5 model in c programming language called epa swmm5 open water analytics also developed an enhanced version of epa swmm5 called owa swmm5 https github com openwateranalytics stormwater management model the computation engine of owa swmm5 exposed several methods that other programming languages can access in addition custom routines can be performed between each time step computation swmm4j https github com geoprocessing swmm4j git reuses owa swmm5 source codes through the java native interface jni it exposes three essential methods initialize performstep and finish the performstep method sets the current precipitation of rain gages and advances the time step computation swmm4j is integrated into the javaps through the wps4m project and can therefore be accessed as a model service using our approach the relationship among javaps wps4m swmm4j and owa swmm5 is shown in fig 6 2 case study area fig 8 shows the study area and the model schema used in the use case this use case tamaddun 2021 which includes the model parameters and data precipitation is derived from the hydroshare platform hydroshare is a powerful web based platform designed for managing and sharing diverse types of environmental data and models horsburgh et al 2016 the study area covers the scotts level branch watershed located in baltimore county maryland the length of this entire watershed is about 4 2 miles and its area spans approximately 1 83 square miles in this use case tamaddun 2021 the study area is divided into six sub catchments w150 w160 w170 w180 w240 w260 the conveyance system consists of storm sewer conduits r30 r80 r110 r130 junctions j49 j41 j44 j38 and one outlet one rain gauge provides the precipitation data and is assigned to all sub catchments in our implementation this rain gage is treated as a virtual iot thing that generates rainfall data and publishes them to the sensorthings server swwm5 can use the precipitation data directly from the sensor web the swmm5 instance is treated as a virtual thing and the expected outputs are treated as datastreams entities we assume that users are interested in real time runoff from the outlet and the water level in pipeline r110 fig 9 shows the related entities of interest used in this use case 3 a walkthrough example we provide a workthrough example to demonstrate how to use developed swmm5 service it covers the model description model initialization time step based computation and real time access to the simulation results a model description the model description is crucial for model discovery and use in a web environment in the ogc wps standard the describeprocess request allows clients to obtain a detailed description of the specific model including the required inputs and their supposed outputs fig 10 shows the description segments of the swmm5 service the rectangles represent the elements in the description structure the nearby name is the identifier of the element it indicates that this model requires an input file i e inpfile for model initialization and precipitation data for time step based computation i e precipitation the precipitation input can have more than one input depending on the number of sensors that provide precipitation data clients can submit precipitation data by combining children elements i e time location and value or by using an mqtt broker i e mqttinput the inputs also include the simulation s start time end time and time step the wps server will publish the simulation results to the mqtt broker i e outputmqtt b model initialization fig 11 shows some xml fragments in an execute request that contains the necessary input parameters the inpfile is transmitted by reference while the starttime endtime and timestep are sent by value a sensorthings service provides the precipitation data for time step based computation the url is the address of sensorthings service and the topic is the target data stream datastream raingage when the wps server receives the execute request it initializes the model and subscribes to the sensorthings server using the specified broker address tcp 127 0 0 1 1883 and topic v1 1 datastream raingage the outputmqtt tcp 127 0 0 1 1884 v1 1 things refers to a sensorthings service the wps server creates a thing entity by specifying the property of iot id as jobid it also creates several datastream entities including datastream outlet runoff and datastream r110 waterlevel which are associated with the thing jobid entity c time step based computation once the sensors or other applications publish observations to the sensorthings server it will perform filtering and push the required observations to the wps server when the server receives the observations from datastream raingage it will calculate the hydrologic and hydraulic states of the study areas including the outlet runoff and the r100 water level the result of time step computation will be delivered to the sensorthings service by creating observation entities using sensorthings api for example the outlet runoff will be published to the sensorthings server using the topic thing jobid datastream outlet runoff and the water level of r110 will be published using the thing jobid datastream r110 waterlevel as the topic d real time access to the simulation result any client can retrieve or subscribe to the result of time step based computation using the jobid and the sensorthings api the clients can obtain the available datastreams using the request pattern service root uri things jobid for example in our swmm5 scenario the available datastreams include the datastream outlet runoff and datastream r110 waterlevel then the client can use the request service root uri datastreams r110 waterlevel to obtain the historical water level of r110 the client also can subscribe to this data stream using the mqtt protocol in this way the clients can receive the simulation results and visualize them at runtime the job will cancel the subscription and write the result files after the loop the client can use the getresult operation to retrieve the full results of the simulation the additional finish operation allows users to stop the simulation and trigger the cancellation operation fig 12 shows the outlet runoff and the water level in conduit r110 4 discussion interoperability is a vital consideration when it comes to sharing and integrating web services the ogc aims to make geospatial information and services fair findable accessible interoperable and reusable our approach is based on two widely used ogc standards wps and sensorthings api which provides an interoperable solution for coupling environmental models and sensor observations over the internet 1 interoperable sharing of environmental models using ogc wps the web based sharing of environmental models has attracted increasing attention it enables the accessibility of heterogeneous models across the internet although the wps is already commonly used in the geospatial domain it has some limitations in handling multiple execution phases and time step computations of models yue et al 2015 zhang et al 2020 users submit all necessary data and parameters to the wps server via an execute request and there is no data exchange during the execution some works have extended ogc wps to support continuous or real time geoprocessing herle and blankenbach 2018 stasch et al 2018 however unlike traditional gis geoprocessing functions complex environmental models are often run following multiple simulation phases and based on time step computations goodall et al 2011 peckham et al 2013 vanecek and roger 2014 while existing works provide real time observations to server side models additional operations are still needed for the initialization and finish of the model services in addition herle and blankenbach 2018 used wps 1 0 for real time geoprocessing the wps 1 0 only supports synchronous execution and implements the http post xml and http get kvp encodings while the latest version wps 2 0 comprises both synchronous and asynchronous execution and supports rest interfaces in our previous work zhang et al 2020 we extended ogc wps 2 0 to run environmental models on the web following the model execution phases i e initialization time step calculations and finish the execute operation is used to initiate the model execution process the performstep operation is added to support time step interactions between the clients and the server side models using the http protocol however this approach has limitations in using real time observations from iot directly this paper extends the previous approach by integrating iot observations into environmental models for real time modeling when the simulation s start time is earlier than the latest subscription data clients can use the performstep operation to provide necessary observations we extended wps to share environmental models that bridge the traditional environmental domain to the geospatial domain interoperably it provides valuable insights for the further development of the wps standard integrating environmental models and traditional gis is a continuous study topic clark 1998 goodchild et al 1992 ng et al 2018 santoalla et al 2018 yue et al 2015 our approach presents a promising way to couple these two domains based on the wps standard 2 interoperable access to sensor observations using sensorthings api the widespread deployment of iot devices has significantly improved the capabilities of environmental monitoring interoperability is crucial in connecting iot devices observations and various applications over the internet ogc has already developed standards to discover access and task iot resources for example the sensor observation service sos and the sensorthings api can both retrieve sensor descriptions and observations by standardized requests the sos is an earlier standard that relies on older technologies such as xml and soap for data encoding it only supports request response patterns for data exchange among different applications which usually requires specific clients to retrieve sensor observations in a loop for example yue et al 2015 designed a particular data component for integrating sensor observations from sos into environmental models sensorthings api is a relatively new standard that follows modern web design principles jacoby and usländer 2020 such as restful services and json as a payload format it also supports the mqtt protocol which uses a publish subscribe pattern for data exchange the sensor observations can be delivered to subscribers as soon as they are produced herle and blankenbach 2018 used a customized geomqtt to enhance the ogc wps interfaces with mqtt support for real time geoprocessing while we accessed the sensor observations using the sensorthings api which takes the benefits of this standard and offers an interoperable solution 3 interoperable data exchange when coupling ogc wps and sensorthings api this paper proposes a web based and interoperable approach for real time modeling by coupling ogc wps and sensorthings api this approach enables instant and continuous model simulation while observations are produced it also facilitates the efficient and timely delivery of outcomes without installing desktop software it is helpful for timely warning in environmental monitoring and simulation scenarios the proposed approach can be easily applied to other environmental models firstly the users need to publish the models as wps compliant services using our approach we extended the 52 north javaps project and developed a wps4m project to facilitate the model deployment secondly the environmental sensors should be connected to the iot servers that follow the sensorthings api standard then data exchange among iot devices environmental models and various clients is based on standardized operations ogc wps and sensorthings api work collaboratively in a loose coupling way the users submit the broker i e sensorthings server information e g address and topic to the wps server using standard wps requests the server will automatically subscribe and retrieve the necessary observations for the environmental models using sensorthings api an information model mapping method is proposed to help mutual understanding between wps and sensorthings api any sensorthings compliant clients can visualize the real time simulation over the internet a variety of legacy resources can be reused 4 current limitations and future work numerical time marching models typically run based on specific time steps which may differ from the time intervals of iot measurements in such a situation data interpolation is necessary before time step based computations the wps engine can perform the interpolation function in future work we will investigate such interpolation methods the proposed method allows users to translate the arguments to the model service when they start and supports time series data exchange at runtime however when the used sensor is broken the server side model will continue to wait for sensor observations in this case the clients can use the performstep operation and manually send the necessary observations from nearby sensors to the models however it is better to allow the clients to update the model arguments such as subscribing to other sensor observations in addition the real time calibration of model parameters is essential for accurate simulation and prediction ma et al 2022 in future work we will extend ogc wps further to allow users to update the model parameters and perform real time calibration during its execution 5 conclusion in time critical situations real time modeling requires real time observations we propose a novel approach for real time modeling that utilizes web services and iot technologies interoperably this paper extends wps to support the publish subscribe communication pattern the wps and sensorthings api work collaboratively based on the mqtt protocol we also propose a mapping method between the information models of these two standards to demonstrate the applicability of our methods we developed a prototype system and used the swmm5 model example with our approach the environmental models can consume observations directly from iot devices furthermore the time step based outputs of environmental models are accessible at runtime software availability 1 program name javaps mqtt extension developer mingda zhang year first available 2022 hardware requirements pc server system requirements windows linux software requirements 52 north javaps eclipse paho 1 2 program language java 12 program size 5m javaps project with mqtt extension availability https github com geoprocessing javaps mqtt extension branch license apache license 2 0 2 program name wps4m developer mingda zhang year first available 2019 hardware requirements pc server system requirements windows linux software requirements 52 north javaps javaps mqtt extension program language java 12 program size 350 kb availability https github com geoprocessing wps4m license apache license 2 0 3 program name swmm4j developer mingda zhang year first available 2019 hardware requirements pc server system requirements windows linux software requirements owa swmm5 program language java 12 program size 64 kb availability https github com geoprocessing swmm4j license apache license 2 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the editors and anonymous reviewers for their very constructive comments that help improve the quality of the paper this work was supported by the natural science foundation of hubei province no 2022cfb119 the national natural science foundation of china no 41901313 and 42001330 and the open project program of hubei key laboratory of regional development and environment response hubei university no 2020 b 002 
25394,future climate projections are made with global numerical models whose spatial resolution often exceed 100s of km2 these scales are too large to resolve many weather events leaving a gap between the climate information needed to understand the impact of climate change on many human activities and the information that can be provided by global models regional climate projections generated using statistical downscaling methods can provide an essential bridge between global climate models and the high spatial resolution data needed as the demand for localized climate information continues to grow new software tools are necessary to provide downscaled climate information in this article we describe ccdownscaling a software package that provides multiple statistical climate downscaling methods to the station scale including the self organizing maps method ccdownscaling includes several evaluation metrics for assessing the skill of downscaled climate information in various applications and we demonstrate these features on an example dataset keywords statistical downscaling self organizing maps climate change random forest software python data availability the data used in this article are available at https zenodo org record 6506677 data and https zenodo org record 7305359 code 1 introduction general circulation models gcms provide estimates for the state of the earth s climate under a range of future greenhouse gas emissions scenarios the gcms typically run on horizontal spatial grid scales ranging from 0 7 0 7 degrees 2 to 2 5 2 5 degrees 2 taylor et al 2012 priestley et al 2020 these spatial scales are too large to capture many weather events that are crucial for understanding the impacts of climate change on human populations radić and clarke 2011 taylor et al 2012 therefore it is often necessary to regionally generate downscaled climate information to the spatial scales required for the applications of interest maraun et al 2010 precipitation is of particular interest and importance for projecting impacts of climate change on human activity unfortunately precipitation remains a challenge for gcms in many regions for example the tropics where the modeled precipitation can struggle to match even the annual mean precipitation koutroulis et al 2016 yang and huang 2022 as we move to higher spatial and temporal resolutions the characteristics of precipitation become an even larger challenge for most gcms to accurately reproduce with gcm errors often being larger than size of projected changes zamani et al 2020 almazroui et al 2021 even in the regions where gcms do capture the precipitation dynamics at the desired spatial scales the amounts of precipitation represent an areal average on the model grid thereby missing the characterization of localized extreme low or high precipitation events gervais et al 2014 these characteristics create the so called drizzle effect in gcms and result in projections with too many days of moderate precipitation and too few periods with no or extreme high precipitation stephens et al 2010 mehran et al 2014 koutroulis et al 2016 correcting these model biases remains a crucial endeavor for determining accurate estimates of the impacts of climate change especially for accurately predicting changes to the frequency and severity of droughts and flooding camici et al 2014 quintero et al 2018 ahmadalipour et al 2017 statistical downscaling of climate information is one option for reaching the desired spatial scale resolution and for bridging the gap between the information provided by the gcms and that needed by users of climate information robinson and finkelstein 1991 fowler et al 2007 statistical downscaling methods use empirical relationships between the outputs of gcms and more localized i e weather station data to create quantitative models for scaling the given state of climate from the synoptic scale to local environments this approach of incorporating gcm outputs and estimates of localized conditions can be used to produce future climate scenarios under different radiative forcing scenarios such as the shared socioeconomic pathway ssp scenarios produced by the intergovernmental panel on climate change gidden et al 2019 numerous methods have been used for statistical downscaling varying widely in complexity some of the earlier downscaling methods include bias correction approaches to amend scale and produce outputs based on the results from gcms that apply to the local conditions karl et al 1990 murphy 1999 subsequent approaches based on a range of statistical and machine learning techniques include the use of artificial neural networks hewitson and crane 1996 ahmed et al 2015 hernanz et al 2022 clustering hewitson and crane 2006 wang et al 2013 stochastic weather generators wilks 1999 kilsby et al 2007 and constructed analogs abatzoglou and brown 2012 pierce et al 2014 these methods have improved the availability and reliability of downscaled climate projections but the choice of downscaling method can have a significant impact on the results wang et al 2017 while downscaling is a widely used approach for estimating climate information at given locales maraun and widmann 2018 there is a dearth of easy to use software for producing and evaluating tailored downscaled climate projections some existing options include the statistical downscaling model sdsm wilby and dawson 2013 which uses a conditional weather generator approach and the downscaler software package bedia et al 2020 that provides several bias correction linear regression and analog methods the sdsm provides only a single downscaling approach thereby limiting its adaptability to diverse and broad applications other software packages such as the r package musica exist and provide tools for validation of statistical downscaling methods at multiple time scales hanel et al 2017 as the need for regional climate information increases there is demand to develop easy to implement numerical methods to generate regionally downscaled climate projections one objective of this study is to describe the ccdownscaling climate change downscaling software package that provides a framework for incorporating user defined variables to generate climate projections at the station level notably ccdownscaling includes the popular and theoretically sound self organizing map som downscaling method which was not previously available in any publicly available software an additional objective is to couple the provided downscaling methods with a framework for evaluating the skill of the generated climate projections the package is written in python because the language is widely used in both the atmospheric science and machine learning communities making it an ideal choice for wide distribution and application given that applications of climate projections differ widely and apply to diverse environments it is crucial to establish evaluation metrics for the downscaled products in short the goals of this paper are to describe an easy to use python package implementing the som algorithm as well as other downscaling methods describe a set of metrics for evaluating the reliability of downscaling methods included in the python package demonstrate the use of the methods and metrics in the ccdownscaling package on an example downscaling use case 1 1 description of the ccdownscaling package the ccdownscaling package is an open source and freely available implementation of a number of downscaling approaches designed for downscaling from gcm grid scale to station locations the code has also been designed to easily accommodate new methods as desired by future users ccdownscaling package provides a framework for using many common machine learning tools as downscaling methods with the goal to allow users to leverage existing and ongoing advances in machine learning when approaching downscaling problems the package allows users to leverage the powerful scikit learn pedregosa et al 2011 and tensorflow abadi et al 2015 machine learning libraries to use common machine learning approaches for downscaling in addition the ccdownscaling package provides several point based downscaling methods as well as metrics for evaluating the skill of different methods on several variables important for different downscaling applications the som downscaling method initially described in hewitson and crane 2006 is not currently available in any publicly available software package we have incorporated the som downscaling in ccdownscaling within a flexible framework in doing so we demonstrate how ccdownscaling can be easily extended to new machine learning methods to allow for future additions to the downscaling package and integrate with commonly used existing machine learning frameworks this will allow for easy integration of future machine learning techniques using the scikit learn package pedregosa et al 2011 we provide a number of additional machine learning algorithms for comparison as downscaling methods including random forest and multiple linear regression models the ccdownscaling package contains three primary components pre processing tools downscaling methods and evaluation metrics the pre processing tools include methods for variable selection and dimension reduction section 3 tools for selecting specific patterns of train and test data section 2 3 the downscaling methods section 4 provide a range of approaches for addressing the challenges of downscaling for different climate regimes and variables and finally the evaluation metrics section 5 provide tools for assessing the skill of the downscaling methods and inputs an example showing the use of each of these components is provided in the form of a python script and jupyter notebook with the software package 2 downscaling for climate change with ccdownscaling while gcms are our best tools for overall assessment of future climate scenarios there are many important impacts of climate change that are not well specified in gcms extreme precipitation events for example are often underrepresented in gcms and are a major potential climate change impact fig 1 gcm data often has significant biases compared to the observed values in the historical period correcting these biases is important before projecting to future scenarios downscaling can provide a method for bias correction ensuring that the distribution of values matches the observed distribution statistical downscaling is important to provide a bridge between the best available gcms and information needed to make critical decisions on costs and adaptations to climate change these include providing inputs to downstream applications including crop yield modeling and human comfort metrics charles et al 2017 dahl et al 2019 downscaling of gcm simulations to the scales desired for these applications needs to replicate the conditions resulting from climate and provide information on variability due to the weather and local factors in this section we describe how the ccdownscaling package can be used for downscaling describing both the methods for downscaling and the diagnostics included in ccdownscaling to evaluate the success of the downscaling based on key factors e g statistical distribution behavior of extremes temporal correlation important to the chosen situation 2 1 example case downscaling is generally useful to answer specific questions that cannot be adequately addressed by global models in this paper we will demonstrate the use of the ccdownscaling package to answer three such questions for a chosen location using data from o hare airport near chicago illinois we will look at the frequency of days with a maximum temperature above the historical 90 t h percentile the change in the average rainfall and change in frequency of precipitation above 20 mm per day to carry out this downscaling we use precipitation and daily maximum temperature from the global summary of day gsod national climatic data center 2020 dataset for o hare airport in chicago illinois reanalysis data is taken from the national center for environmental prediction ncep kalnay et al 1996 reanalysis 2 for relative humidity at 850 hpa air temperature 850 hpa geopotential height 500 hpa sea level pressure slp surface and zonal and meridional wind components 700 hpa these predictors were selected to capture the synoptic environment of the region more information on the variables selection process can be found in section 3 this data is used to train the downscaling methods included in the ccdownscaling software package and to evaluate the results using the various metrics described below results shown below for this example and the required data sets and python code are provided in the github repository https github com drewpolasky ccdownscaling the period from 1976 to 1999 is used to train the downscaling methods which are then tested on the years from 2000 to 2005 in the following sections we discuss the methods and evaluation metrics in the context of this example case the example use case is provided as both a python script and jupyter notebook in the package repository both provide the same code with the jupyter notebook providing a more interactive format with the package installed it can either be run directly in python from the ohare example py file or the jupyter notebook of the same name located in the example folder the data in netcdf format needed to run the example can be downloaded from https zenodo org record 7817799 2 2 reproducing climate variability a downscaling method must be able to demonstrate that it can reproduce the variability of the observed climate from the input data this is particularly important in cases such as precipitation where the variability is not well described in the gcms stephens et al 2010 the ccdownscaling package includes several metrics to evaluate the ability of a downscaling method to reproduce the existing climate variability including probability density function pdf skill score kolmogorov smirnov testing and seasonal tests see section 5 for more details on these methods these tests are carried out on an independent test set to help separate from the data used to train the downscaling model for our example case we use the final 6 years of the 30 year period 2000 2005 as our test set fig 2 2 3 adapting to new climate conditions a key challenge for statistical downscaling comes from trying to make predictions for future climate scenarios where we are expecting the conditions to be significantly different from those we observe today a good downscaling technique must therefore be able to adapt to changes in the underlying climate that go beyond the data it was trained on evaluating downscaling methods on this criteria requires some creativity since the observational records typical cover smaller changes in climate than those we expect under most climate change scenarios gulev et al 2021 to address this difficulty we implement two evaluation methods in the ccdownscaling package for the first method the input data is ranked over a given time period and split such that the highest or lowest time periods are in the test set for example we select the six hottest years from 1976 to 2005 at o hare airport and use those years for the test set training on the other years in this case the six wettest years averaged 0 95 mm per day above the remaining years an increase of 40 to evaluate the different downscaling methods we plot their performance on the biased train and test sets to see how much of the change in precipitation they capture fig 3 selecting the train and test sets for this method can be done with the select max target years function the second method takes advantage of the differences in climate between seasons to explore the ability of the methods to shift to new climates we train a model on one season e g spring and evaluate that same model on another season e g summer in the o hare example the summer is an average of 12 7 c warmer than the spring with 18 5 more precipitation providing a more extreme test case than selecting the warmest set of years this method requires tuning for individual locations when selecting the train and test season dates for the o hare example we use march april may mam for spring june july august jja for summer other regions such as tropical or monsoon environments would require different date selections this is customizable by setting the train and test dates in the select season train test function these two functions for splitting train and test data are provided in addition to two more traditional techniques a simple split taking the most recent years for the test set and a k fold cross validation split which forms a number of train test split sets by dividing the available data into a given number k of segments each of these segments is held out as the test data while the downscaling model is trained on the remaining data this is especially useful in areas with limited input data as a smaller test set size can be used while still maintaining a robust estimate of model skill fushiki 2011 many of the changes projected under most climate change scenarios represent climates that have not been previously experienced posing a challenge for training and verifying statistical downscaling models using these methods we are able to test the ability of the downscaling methods to adapt to different circumstances to those they were trained on in the example jupyter notebook examples for each of these splitting methods can be found in section 2 2 4 considering extremes one of the most critical pieces for understanding the impact of climate change is understanding changes in frequency and severity of extreme events katz and brown 1992 these events are also more difficult for gcms to represent accurately than changes to the average of a given variable increasing the need for downscaling methods that can capture such events kysel et al 2002 knutti and sedláček 2013 in the ccdownscaling package we consider two forms of extreme events percentage based and absolute percentage based events are defined by the frequency of exceeding a given percentile of the observed data for a given location for example at o hare airport the 90 t h percentile for the training data of 1976 1999 is 30 6 c and we can calculate the number of days in the train and test sets that fall above this threshold absolute metrics look at values that have specific meanings for impacts for example many crops suffer from decreased growth rates above certain temperature thresholds schlenker and roberts 2009 found that average daily temperature of above 39 c for corn and 30 c for soy caused yields to decline rapidly ccdownscaling provides tools for assessing both of these types of metrics through the included climdex module which implements the 27 etccdi climate indices peterson 2005 these indices cover a range of temperature and precipitation based metrics to provide an overview of the changes in key aspects of a region s climate a full list of the indices can be found in appendix a and an example use of these metrics can be found in section 9 of the example jupyter notebook 2 5 comparing downscaling methods with ccdownscaling the different downscaling methods provided by the ccdownscaling package have different strengths and weaknesses when answering different downscaling questions demonstrations of these differences can be found in the results in section 5 the som method see section 4 1 for more details on the method is well suited to assessing changes in the frequency of events and does a good job of recreating past climate variability ning et al 2012 it can however struggle to respond to large changes in the underlying climate and cannot extrapolate to new extreme values hewitson and crane 2006 it is better suited to analyses of how frequently an extreme event may occur in a future climate by estimating the frequency of synoptic conditions that have historically created extreme events rather than estimating the maximum severity of events polasky et al 2021 the random forest method section 4 2 is highly adaptable and generally does a better job of matching individual days than many of the other methods he et al 2016 however it can also overfit to the data and result in too many days with moderate values this effect can be particularly acute for precipitation downscaling with random forest models often producing to many days of moderate precipitation and not enough dry days or days of extreme rainfall fig 3b quantile mapping section 4 3 is generally very good at reproducing the historical climate variability but often struggles to generalize to new climates especially for more complex variables such as precipitation zhao et al 2017 quantile mapping is also vulnerable to variance inflation where the variance in the downscaled output is higher than the variance in the observed data over a given area maraun 2013 cannon et al 2015 because the variance at low resolutions tends to be lower than the variance at high resolutions in mapping between the two distributions the marginal difference in value is corrected but the local variability in values is not included leading to an overly strong spatial correlation between observed locations without further correction this leads to spatial averages that overproduce extremes at either end of the distribution especially for variables such as precipitation that have lower spatial correlations 3 variable selection appropriate variable selection for downscaling is of critical importance for training a reliable and accurate downscaling model najafi et al 2011 hammami et al 2012 teegavarapu and goly 2018 there are a large number of variables that could potentially be useful when downscaling for most downscaling targets there are a large number of atmospheric variables that correlate in complex ways in the case of precipitation for example useful variables might include relative humidity sea level pressure or other variables related to humidity circulation patterns or convective activity charles et al 1999 timbal et al 2008 maraun et al 2010 the selection of variables for downscaling must also take into account the representation of those variables in the gcms many potentially useful variables are not reliably simulated in gcms making them likely to be poor choices for a downscaling model cavazos and hewitson 2005 teegavarapu and goly 2018 in most cases expert opinion is likely to be useful in selecting final sets of predictors that make physical sense for the climatology of the downscaling target region it can then be useful to compare to the variables that objective methods identify as the most useful these objective methods can also be used to select a subset of the initial predictor set to improve model speed with minimal drops in performance ccdownscaling includes three methods for variable selection sliced inverse regression sir li 1991 and principal component analysis pca hotelling 1933 and random forest rf breiman 2001 different methods for variable selection can make different identifications of the most important variables so it can be helpful to run multiple techniques and look for common variables between the different approaches ccdownscaling makes comparing these methods straightforward all of these methods are implemented in the variable selection code and an example can be found in the example folder and jupyter notebook the relative importance rankings for an initial set of five predictors at five pressure levels from these three methods can be found in table 1 the six input variables are air temperature air geopotential height hgt relative humidity rhum sea level pressure slp surface only meridional wind speed vwnd and zonal wind speed uwnd two of the three methods sir and pca also serve as dimension reduction techniques these method can be used to transform the high dimension input data to lower dimension constructed features that maintain as much of the original information as possible ma and zhu 2013 this can be used to construct a set of new inputs that are a combination of the original inputs and capture as much as possible of the variance of the downscaling target these new variables can be used as inputs to the downscaling methods reducing the computational requirements by having fewer input variables while still retaining the original information 4 downscaling methods the ccdownscaling package incorporates several downscaling methods all conforming to a common framework for integration into downscaling workflows the application programming interface api mirrors the scikit learn setup for machine learning models which allows the easy integration of scikit learn methods and ease of use for those already familiar with scikit learn this approach allows for the development testing and comparison of different downscaling methods in a shared framework as displayed in fig 4 the usage of the methods is demonstrated in sections 4 and 5 of the example jupyter notebook all of the downscaling methods in the package are initially trained from reanalysis data once a model has been trained it can then be used with gcm data to provide downscaled estimates for future climate scenarios 4 1 som downscaling self organizing maps soms are an unsupervised machine learning method for mapping a complex set of inputs onto a two dimensional map of nodes each representing a typical pattern observed in the input data kohonen 1990 for downscaling soms can be used to identify characteristic synoptic scale weather patterns and relate those patterns to the observed local conditions hewitson and crane 2006 the som can then be used with gcm projections to explore changes to the frequency of these patterns in future climate scenarios gibson et al 2016 soms have been broadly used for downscaling particularly of precipitation in regions such as south africa hewitson and crane 2006 florida sinha et al 2018 and the midwest united states polasky et al 2021 the som method begins by creating a set of nodes arranged in a two dimensional grid each of these nodes is defined by a vector matching the size of one case of training data to train the som each element of the training data in our example case downscaling for o hare airport this data comes from the ncep reanalysis taking a 5 5 grid point window around the station is compared to the som nodes the node whose vector is nearest in euclidean distance to the training element is selected as the best match unit bmu 1 b m u m i n w v i t where w v is the weight vector for node v i is the input dataset and t is the index of the training element the bmu vector is then incrementally updated towards the training element as are the neighboring nodes to the bmu each node weight vector is updated 2 w v s 1 w v s θ u v α i t w v s where s is the current iteration of the training α is the learning rate of the som and θ is the neighborhood function governing how much the update effects the nodes near to the bmu the value of θ decreases exponentially the distance of the node to bmu adjusting the neighboring nodes in addition to the bmu has the effect of sorting similar nodes to be near to one another in the map the overall update rate is governed by α with each successive pass through the dataset α is decreased to more rapidly converge to a stable map once the som has been trained each day in the training data can be placed on the map by finding the bmu for each node of the som this gives a set of days corresponding to that pattern the station observations for those days can be combined to create a probability function of local values for each som node to create new downscaled projections gcm data can be mapped onto the som matching each day to its bmu the probability function of the downscaling target variable can then be sampled from producing the downscaled value for that day by iterating through the days included in the gcm data sets we produce a daily downscaled time series for the given location the soms were implemented in python using the tensorflow library abadi et al 2015 adapted from the open source tensorflow som project gorman 2019 the som method has the advantage of providing insight into the weather patterns giving rise to specific downscaled outcomes through the patterns detected by the individual nodes of the som in the o hare airport example the highest precipitation nodes fall in the center of the top two rows of the som fig 5 the color gradient represents the frequency of that nodes being the bmu in the training data while the number in the box is the average precipitation for the days falling on that node two of these nodes for example the node at row 4 column 5 4 1 and node 3 4 both have high precipitation but very different temperature patterns fig 6 node 3 4 represents a warm summer day with temperatures between 17 c in the south of the region around o hare and 11 c in the north at 850 hpa the high precipitation likely corresponds to summer type convection the 4 1 node is colder with a strong southwest northeast temperature gradient and the precipitation is likely driven by mid latitude cyclones 4 2 scikit learn downscaling methods scikit learn is a widely used machine learning library for python that provides a wide range of machine learning tools pedregosa et al 2011 these methods have been adapted for use as downscaling tools in ccdownscaling as demonstrated for a random forest model for the o hare airport example random forests are a widely used machine learning approach that have been successfully applied to a wide range of problems breiman 2001 their adaptability and ease of use have led to random forest being a go to method in machine learning biau and scornet 2016 random forests have been used for downscaling temperature and precipitation in a variety of locations hutengs and vohland 2016 sa adi et al 2017 pang et al 2017 polasky et al 2021 the ccdownscaling package provides a framework for extending scikit learn provided methods to better suit downscaling problems an example of this functionality is given in two step random forest model which adapts the standard random forest from scikit learn to have a classifier to initially split dry precipitation days then a second regressor model to predict the amount of precipitation for the wet days this model addresses issues of the random forest producing too many days of middling precipitation and too few dry days similar to the undownscaled gcms and generally outperforms a basic random forest for precipitation metrics section 5 4 3 quantile mapping quantile mapping is a commonly used approach for downscaling especially for temperature maraun 2013 han et al 2019 unlike the other methods included in this software package quantile mapping uses only a single variable as input typically the value being downscaled i e daily precipitation from the model for downscaling precipitation it takes the approach of comparing the difference in value between the quantile ranks of an initial and final distribution a transformation is calculated between these ranks which can then be applied to a new input distribution to create a downscaled output that better matches the observed distribution of events quantile mapping is effective at bias correcting gcms outputs but quantile mapping relies on the fields of the target variable from the gcms this can be an issue particularly in the case of precipitation where the representation of the processes underlying the output values are not properly resolved by the gcms other methods which can make use of variables that are better captured in the gcms are likely to yield superior results of the downscaling in these circumstances zhao et al 2017 nonetheless quantile mapping provides a straightforward bias correction approach and is a useful point of comparison for other downscaling methods wood et al 2004 5 validation methods in addition to providing a range of downscaling methods ccdownscaling also provides a number of different evaluation metrics to compare methods and assess the suitability of a downscaling output for a given task depending on the goals of the downscaling and even the use case different downscaling applications may best be assessed using different evaluation criteria for example estimating drought conditions will require accurate estimation of average precipitation and temperature over longer time frames while projecting for flooding situations will require accurate representation of large precipitation events and temporal correlation for multi day events the implementation of many validation methods in ccdownscaling is thus a key part of its broad applicability in this section we describe a number of evaluation metrics and show the results of these metrics on the o hare airport example for a range of downscaling methods examples of these metrics can be found in sections 7 and 8 of the example jupyter notebook 5 1 som training metrics the som method includes two specialized training metrics quantization and topological error these are commonly used metrics for assessing the training characteristics of a model setup and should be used when tuning the hyperparameters controllable parameters that define the setup and training characteristics of the model quantization error qe refers to the average distance between the day vectors assigned to a node and the characteristic vector of the node kohonen 1990 smaller values of qe represent reduced spread within each cluster as the size of the map increases this value will naturally decrease as days are increasingly subdivided between clusters a common tactic for settling on a map size is to look for the elbow where the decrease in qe slows down with the increase in map size céréghino and park 2009 in fig 7 this occurs at the 5 7 map size and that map size was selected for further analyses topographical error te is calculated as the percentage of input vectors whose second best matching units are adjacent to their best match units this is a measure of how well the topology of the original dataset is being preserved in the lower dimension space of the som uriarte and martín 2005 te generally increases as the size of the map increases with more opportunities for non adjacent nodes to match an input vector fig 8 these two metrics combine to help choose an optimal size for the som balancing the gains in specificity against the increasing complexity of the model an example demonstrating both of these methods can be found in som training example py in the example folder of the package 5 2 distribution tests one of the key characteristics for a downscaling method is determining the skill of a downscaling approach is how well it reproduces the historical distribution of events precipitation in particular is commonly spread over too many days with small amounts of precipitation in gcms projections reducing the number of dry days and large precipitation events fig 2 these metrics are especially valuable when trying to understand the shifts in climate and the frequency of different event types under climate change scenarios perkins et al 2013 polasky et al 2021 we implement two common distribution tests in ccdownscaling pdf skill score and the kolmogorov smirnov test the pdf skill score measures the similarity between two probability density functions pdfs by calculating the minimum value between observed and modeled counts within each bin of a histogram to measure the shared area within the distribution 3 s s c o r e 1 n m i n z m z o where s score is the skill score z m and z o are the frequency of modeled and observed values in a given bin respectively and n is the number of bins used to calculate the pdf perkins et al 2007 pdf skill score provides an easily interpretable score for the similarity between the observed and downscaled distribution of values the kolmogorov smirnov ks test provides a non parametric statistical test to evaluate the likelihood that two samples are drawn from the same underlying distribution massey 1951 the ks test provides both a test statistic which can be used to assess the similarity of two distributions where smaller values are more similar and a probability that the given distributions are drawn from the same underlying probability distribution tables 2 and 3 show the ks scores and probabilities for the different methods for the o hare airport example for precipitation and maximum air temperature respectively in most cases pdf score and ks test statistic are strongly correlated brands et al 2012 an exception occurs when the distribution includes a large number of near zero values as in the case of daily precipitation in this instance pdf score can struggle due to the kernel density smoothing applied in the calculation brands et al 2012 the effect of this difference can be seen in the o hare airport example for temperature the som and random forest methods score similarly to one another in both pdf score and ks statistic table 3 by contrast for precipitation the pdf statistic for the two methods is similar while the som far outperforms the random forest on ks statistic the two part random forest achieves a ks statistic score much closer to that of the som primarily by correcting the number of days with no precipitation compared to the single random forest visually we can see that the som much better matches the observed distribution fig 2 indicating that the ks test statistic better represents the relative skill of the two downscaling methods for precipitation 5 3 error metrics error metrics are a common method for evaluating model skill evaluating the day by day differences in downscaled and observed values for the testing period these direct measures of difference are most useful when comparing methods that seek to recreate the specific conditions on each individual day such as the random forest model much of the goal with downscaling and with any climate modeling is to understand the full range and frequency of events occurring rather than to predict the correct event on the specific day for this reason the error metrics are often less useful for assessing downscaling model skill than the distribution tests nonetheless these measures can provide useful comparisons especially between similar classes of models and so we have made several error metrics available in ccdownscaling these error metrics should only be applied when comparing downscaled outputs calculated from reanalysis data to the ground truth observations data sets coming from gcms are unlikely to match the daily variation of the historical records and the results may not represent the documented conditions commonly used error metrics such as mean squared error and bias are included in the package the results for the o hare airport example can be found in table 2 for daily precipitation and table 3 for maximum temperature the results for som and linear regression models for temperature show the differences between the error and distribution metrics the som outperforms the linear model on the pdf and ks scores as it provides a better performance of matching the overall distribution of events but scores worse in terms of rmse and bias because it does not match the specific day to day variations as well as the linear model a similar approach can be taken by plotting the observed and downscaled data against one another and calculating a linear regression fig 9 this provides a clear visual representation of how well the downscaled data matches the observations but suffers from the same drawbacks as the error metrics mentioned above 5 4 extreme events performance on extreme events is a key goal for many downscaling applications wigley 1985 many extreme event definitions will depend on the specifics of the question to be answered but we include some common extreme event indices in the ccdownscaling package in addition to being potentially useful values for questions about future climate scenarios these indices can be a useful metric for determining the skill of the different downscaling methods using the warmest wettest year split described in section 2 4 we calculate the maximum temperature precipitation climdex values for the som rf and qmap methods tables 4 and 5 for precipitation while the rf method does the best job of the three at adapting the to higher average precipitation the poor performance on the high and low ends of the distribution are evident in the very low values for 95 t h percentile r95p 99 t h percentile r99p and consecutive dry days cdd the differences are not as pronounced for the max temperature indices but here qmap clearly outperforms the other two methods for most of the indices 5 5 autocorrelation autocorrelation provides a metric for temporal similarity within time series data it is calculated by taking the correlation between a timeseries and a time lagged copy of the same timeseries for a number of different time lags this can be particularly important for applications such as flooding where understanding the likelihood of multi day large rainfall events is crucial for projecting the frequency of major flooding events in a given climate scenario in the case of o hare airport the autocorrelation of both the observed and downscaled datasets falls off rapidly with most of the downscaled methods apart from the som and qmap overestimating the observed correlation at a lag of one day fig 10 6 concluding remarks the accelerating need for reliable highly localized data for climate change scenarios has led to the development of software packages such as ccdownscaling that can readily and reliably provide information tailored to individual sites or regions ccdownscaling is a new software package providing options for downscaling approaches and tools in a manner that the user can readily apply and iterate upon to meet the needs and requirements of the desired downscaled climate change projections it is freely available and open source to be easily used in the creation of new downscaled climate data for a range of potential uses ccdownscaling provides som based downscaling along with the extension of traditional machine learning tools to downscaling we demonstrate the capabilities of the package using an example station located at the chicago o hare airport in illinois this example serves to demonstrate the use of the software package but will not cover many issues that will come up for a real downscaling use case including limited data availability complex terrain or changes in land cover typically we have found the som method performs well for precipitation downscaling while the random forest and qmap methods often perform well for temperature variables but these results can vary based on the meteorology and available data for a given location in addition to the downscaling methods the package includes a variety of metrics for assessing the skill and utility of downscaled outputs these tools along with the example given provide a framework for creating and evaluating new downscaled products tailored to a given location and use case declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests jenni l evans reports financial support was provided by national science foundation acknowledgments thanks to zach moon for assistance in making the code more usable and daniel polasky for helpful comments on the manuscript we also thank two anonymous reviewers for their helpful comments on this manuscript thanks as well to the penn state institute for computational and data sciences for computational resources and support this work was supported by the national science foundation united states under grant 1639342 appendix a climdex indices the 27 climdex indices represent a standard set of values for precipitation maximum temperature and minimum temperatures to describe changes in climate with a focus on extreme events created by the expert team on climate change detection and indices etccdi http etccdi pacificclimate org indices shtml these values provide an overview of mean and extreme changes that will be commonly useful to many applications for maximum and minimum temperature as well as precipitation see table a 1 appendix b package setup installation instructions for the ccdownscaling package can be found on the github along with the code for the example use case in the form of both a python script and jupyter notebook both jupyter notebook and script contain the same code the jupyter notebook is included to provide a more readable and interactive interface for the example to set up the environment for ccdownscaling we recommend using conda for ease of installation a conda environment yml file with the list of requirements all the downscaling models in ccdownscaling conform to a common api making the addition of new methods to a downscaling use case straightforward the jupyter notebook in the example folder gives an example of the main pieces of the ccdownscaling package section 2 shows an example for using three different train test splits section 5 demonstrates training several different models all called using the same format where the model is created then trained fit and final downscaling output is generated predict the outputs from these different methods can then be passed into any of the analysis methods section 6 demonstrates some of the som specific analysis plots including heatmaps and variable grids for the som nodes section 7 then shows the more universal evaluation metrics computing skill score values for each of the methods section 8 makes the plots for comparing the different methods and finally section 9 calculates the climdex values for each output 
25394,future climate projections are made with global numerical models whose spatial resolution often exceed 100s of km2 these scales are too large to resolve many weather events leaving a gap between the climate information needed to understand the impact of climate change on many human activities and the information that can be provided by global models regional climate projections generated using statistical downscaling methods can provide an essential bridge between global climate models and the high spatial resolution data needed as the demand for localized climate information continues to grow new software tools are necessary to provide downscaled climate information in this article we describe ccdownscaling a software package that provides multiple statistical climate downscaling methods to the station scale including the self organizing maps method ccdownscaling includes several evaluation metrics for assessing the skill of downscaled climate information in various applications and we demonstrate these features on an example dataset keywords statistical downscaling self organizing maps climate change random forest software python data availability the data used in this article are available at https zenodo org record 6506677 data and https zenodo org record 7305359 code 1 introduction general circulation models gcms provide estimates for the state of the earth s climate under a range of future greenhouse gas emissions scenarios the gcms typically run on horizontal spatial grid scales ranging from 0 7 0 7 degrees 2 to 2 5 2 5 degrees 2 taylor et al 2012 priestley et al 2020 these spatial scales are too large to capture many weather events that are crucial for understanding the impacts of climate change on human populations radić and clarke 2011 taylor et al 2012 therefore it is often necessary to regionally generate downscaled climate information to the spatial scales required for the applications of interest maraun et al 2010 precipitation is of particular interest and importance for projecting impacts of climate change on human activity unfortunately precipitation remains a challenge for gcms in many regions for example the tropics where the modeled precipitation can struggle to match even the annual mean precipitation koutroulis et al 2016 yang and huang 2022 as we move to higher spatial and temporal resolutions the characteristics of precipitation become an even larger challenge for most gcms to accurately reproduce with gcm errors often being larger than size of projected changes zamani et al 2020 almazroui et al 2021 even in the regions where gcms do capture the precipitation dynamics at the desired spatial scales the amounts of precipitation represent an areal average on the model grid thereby missing the characterization of localized extreme low or high precipitation events gervais et al 2014 these characteristics create the so called drizzle effect in gcms and result in projections with too many days of moderate precipitation and too few periods with no or extreme high precipitation stephens et al 2010 mehran et al 2014 koutroulis et al 2016 correcting these model biases remains a crucial endeavor for determining accurate estimates of the impacts of climate change especially for accurately predicting changes to the frequency and severity of droughts and flooding camici et al 2014 quintero et al 2018 ahmadalipour et al 2017 statistical downscaling of climate information is one option for reaching the desired spatial scale resolution and for bridging the gap between the information provided by the gcms and that needed by users of climate information robinson and finkelstein 1991 fowler et al 2007 statistical downscaling methods use empirical relationships between the outputs of gcms and more localized i e weather station data to create quantitative models for scaling the given state of climate from the synoptic scale to local environments this approach of incorporating gcm outputs and estimates of localized conditions can be used to produce future climate scenarios under different radiative forcing scenarios such as the shared socioeconomic pathway ssp scenarios produced by the intergovernmental panel on climate change gidden et al 2019 numerous methods have been used for statistical downscaling varying widely in complexity some of the earlier downscaling methods include bias correction approaches to amend scale and produce outputs based on the results from gcms that apply to the local conditions karl et al 1990 murphy 1999 subsequent approaches based on a range of statistical and machine learning techniques include the use of artificial neural networks hewitson and crane 1996 ahmed et al 2015 hernanz et al 2022 clustering hewitson and crane 2006 wang et al 2013 stochastic weather generators wilks 1999 kilsby et al 2007 and constructed analogs abatzoglou and brown 2012 pierce et al 2014 these methods have improved the availability and reliability of downscaled climate projections but the choice of downscaling method can have a significant impact on the results wang et al 2017 while downscaling is a widely used approach for estimating climate information at given locales maraun and widmann 2018 there is a dearth of easy to use software for producing and evaluating tailored downscaled climate projections some existing options include the statistical downscaling model sdsm wilby and dawson 2013 which uses a conditional weather generator approach and the downscaler software package bedia et al 2020 that provides several bias correction linear regression and analog methods the sdsm provides only a single downscaling approach thereby limiting its adaptability to diverse and broad applications other software packages such as the r package musica exist and provide tools for validation of statistical downscaling methods at multiple time scales hanel et al 2017 as the need for regional climate information increases there is demand to develop easy to implement numerical methods to generate regionally downscaled climate projections one objective of this study is to describe the ccdownscaling climate change downscaling software package that provides a framework for incorporating user defined variables to generate climate projections at the station level notably ccdownscaling includes the popular and theoretically sound self organizing map som downscaling method which was not previously available in any publicly available software an additional objective is to couple the provided downscaling methods with a framework for evaluating the skill of the generated climate projections the package is written in python because the language is widely used in both the atmospheric science and machine learning communities making it an ideal choice for wide distribution and application given that applications of climate projections differ widely and apply to diverse environments it is crucial to establish evaluation metrics for the downscaled products in short the goals of this paper are to describe an easy to use python package implementing the som algorithm as well as other downscaling methods describe a set of metrics for evaluating the reliability of downscaling methods included in the python package demonstrate the use of the methods and metrics in the ccdownscaling package on an example downscaling use case 1 1 description of the ccdownscaling package the ccdownscaling package is an open source and freely available implementation of a number of downscaling approaches designed for downscaling from gcm grid scale to station locations the code has also been designed to easily accommodate new methods as desired by future users ccdownscaling package provides a framework for using many common machine learning tools as downscaling methods with the goal to allow users to leverage existing and ongoing advances in machine learning when approaching downscaling problems the package allows users to leverage the powerful scikit learn pedregosa et al 2011 and tensorflow abadi et al 2015 machine learning libraries to use common machine learning approaches for downscaling in addition the ccdownscaling package provides several point based downscaling methods as well as metrics for evaluating the skill of different methods on several variables important for different downscaling applications the som downscaling method initially described in hewitson and crane 2006 is not currently available in any publicly available software package we have incorporated the som downscaling in ccdownscaling within a flexible framework in doing so we demonstrate how ccdownscaling can be easily extended to new machine learning methods to allow for future additions to the downscaling package and integrate with commonly used existing machine learning frameworks this will allow for easy integration of future machine learning techniques using the scikit learn package pedregosa et al 2011 we provide a number of additional machine learning algorithms for comparison as downscaling methods including random forest and multiple linear regression models the ccdownscaling package contains three primary components pre processing tools downscaling methods and evaluation metrics the pre processing tools include methods for variable selection and dimension reduction section 3 tools for selecting specific patterns of train and test data section 2 3 the downscaling methods section 4 provide a range of approaches for addressing the challenges of downscaling for different climate regimes and variables and finally the evaluation metrics section 5 provide tools for assessing the skill of the downscaling methods and inputs an example showing the use of each of these components is provided in the form of a python script and jupyter notebook with the software package 2 downscaling for climate change with ccdownscaling while gcms are our best tools for overall assessment of future climate scenarios there are many important impacts of climate change that are not well specified in gcms extreme precipitation events for example are often underrepresented in gcms and are a major potential climate change impact fig 1 gcm data often has significant biases compared to the observed values in the historical period correcting these biases is important before projecting to future scenarios downscaling can provide a method for bias correction ensuring that the distribution of values matches the observed distribution statistical downscaling is important to provide a bridge between the best available gcms and information needed to make critical decisions on costs and adaptations to climate change these include providing inputs to downstream applications including crop yield modeling and human comfort metrics charles et al 2017 dahl et al 2019 downscaling of gcm simulations to the scales desired for these applications needs to replicate the conditions resulting from climate and provide information on variability due to the weather and local factors in this section we describe how the ccdownscaling package can be used for downscaling describing both the methods for downscaling and the diagnostics included in ccdownscaling to evaluate the success of the downscaling based on key factors e g statistical distribution behavior of extremes temporal correlation important to the chosen situation 2 1 example case downscaling is generally useful to answer specific questions that cannot be adequately addressed by global models in this paper we will demonstrate the use of the ccdownscaling package to answer three such questions for a chosen location using data from o hare airport near chicago illinois we will look at the frequency of days with a maximum temperature above the historical 90 t h percentile the change in the average rainfall and change in frequency of precipitation above 20 mm per day to carry out this downscaling we use precipitation and daily maximum temperature from the global summary of day gsod national climatic data center 2020 dataset for o hare airport in chicago illinois reanalysis data is taken from the national center for environmental prediction ncep kalnay et al 1996 reanalysis 2 for relative humidity at 850 hpa air temperature 850 hpa geopotential height 500 hpa sea level pressure slp surface and zonal and meridional wind components 700 hpa these predictors were selected to capture the synoptic environment of the region more information on the variables selection process can be found in section 3 this data is used to train the downscaling methods included in the ccdownscaling software package and to evaluate the results using the various metrics described below results shown below for this example and the required data sets and python code are provided in the github repository https github com drewpolasky ccdownscaling the period from 1976 to 1999 is used to train the downscaling methods which are then tested on the years from 2000 to 2005 in the following sections we discuss the methods and evaluation metrics in the context of this example case the example use case is provided as both a python script and jupyter notebook in the package repository both provide the same code with the jupyter notebook providing a more interactive format with the package installed it can either be run directly in python from the ohare example py file or the jupyter notebook of the same name located in the example folder the data in netcdf format needed to run the example can be downloaded from https zenodo org record 7817799 2 2 reproducing climate variability a downscaling method must be able to demonstrate that it can reproduce the variability of the observed climate from the input data this is particularly important in cases such as precipitation where the variability is not well described in the gcms stephens et al 2010 the ccdownscaling package includes several metrics to evaluate the ability of a downscaling method to reproduce the existing climate variability including probability density function pdf skill score kolmogorov smirnov testing and seasonal tests see section 5 for more details on these methods these tests are carried out on an independent test set to help separate from the data used to train the downscaling model for our example case we use the final 6 years of the 30 year period 2000 2005 as our test set fig 2 2 3 adapting to new climate conditions a key challenge for statistical downscaling comes from trying to make predictions for future climate scenarios where we are expecting the conditions to be significantly different from those we observe today a good downscaling technique must therefore be able to adapt to changes in the underlying climate that go beyond the data it was trained on evaluating downscaling methods on this criteria requires some creativity since the observational records typical cover smaller changes in climate than those we expect under most climate change scenarios gulev et al 2021 to address this difficulty we implement two evaluation methods in the ccdownscaling package for the first method the input data is ranked over a given time period and split such that the highest or lowest time periods are in the test set for example we select the six hottest years from 1976 to 2005 at o hare airport and use those years for the test set training on the other years in this case the six wettest years averaged 0 95 mm per day above the remaining years an increase of 40 to evaluate the different downscaling methods we plot their performance on the biased train and test sets to see how much of the change in precipitation they capture fig 3 selecting the train and test sets for this method can be done with the select max target years function the second method takes advantage of the differences in climate between seasons to explore the ability of the methods to shift to new climates we train a model on one season e g spring and evaluate that same model on another season e g summer in the o hare example the summer is an average of 12 7 c warmer than the spring with 18 5 more precipitation providing a more extreme test case than selecting the warmest set of years this method requires tuning for individual locations when selecting the train and test season dates for the o hare example we use march april may mam for spring june july august jja for summer other regions such as tropical or monsoon environments would require different date selections this is customizable by setting the train and test dates in the select season train test function these two functions for splitting train and test data are provided in addition to two more traditional techniques a simple split taking the most recent years for the test set and a k fold cross validation split which forms a number of train test split sets by dividing the available data into a given number k of segments each of these segments is held out as the test data while the downscaling model is trained on the remaining data this is especially useful in areas with limited input data as a smaller test set size can be used while still maintaining a robust estimate of model skill fushiki 2011 many of the changes projected under most climate change scenarios represent climates that have not been previously experienced posing a challenge for training and verifying statistical downscaling models using these methods we are able to test the ability of the downscaling methods to adapt to different circumstances to those they were trained on in the example jupyter notebook examples for each of these splitting methods can be found in section 2 2 4 considering extremes one of the most critical pieces for understanding the impact of climate change is understanding changes in frequency and severity of extreme events katz and brown 1992 these events are also more difficult for gcms to represent accurately than changes to the average of a given variable increasing the need for downscaling methods that can capture such events kysel et al 2002 knutti and sedláček 2013 in the ccdownscaling package we consider two forms of extreme events percentage based and absolute percentage based events are defined by the frequency of exceeding a given percentile of the observed data for a given location for example at o hare airport the 90 t h percentile for the training data of 1976 1999 is 30 6 c and we can calculate the number of days in the train and test sets that fall above this threshold absolute metrics look at values that have specific meanings for impacts for example many crops suffer from decreased growth rates above certain temperature thresholds schlenker and roberts 2009 found that average daily temperature of above 39 c for corn and 30 c for soy caused yields to decline rapidly ccdownscaling provides tools for assessing both of these types of metrics through the included climdex module which implements the 27 etccdi climate indices peterson 2005 these indices cover a range of temperature and precipitation based metrics to provide an overview of the changes in key aspects of a region s climate a full list of the indices can be found in appendix a and an example use of these metrics can be found in section 9 of the example jupyter notebook 2 5 comparing downscaling methods with ccdownscaling the different downscaling methods provided by the ccdownscaling package have different strengths and weaknesses when answering different downscaling questions demonstrations of these differences can be found in the results in section 5 the som method see section 4 1 for more details on the method is well suited to assessing changes in the frequency of events and does a good job of recreating past climate variability ning et al 2012 it can however struggle to respond to large changes in the underlying climate and cannot extrapolate to new extreme values hewitson and crane 2006 it is better suited to analyses of how frequently an extreme event may occur in a future climate by estimating the frequency of synoptic conditions that have historically created extreme events rather than estimating the maximum severity of events polasky et al 2021 the random forest method section 4 2 is highly adaptable and generally does a better job of matching individual days than many of the other methods he et al 2016 however it can also overfit to the data and result in too many days with moderate values this effect can be particularly acute for precipitation downscaling with random forest models often producing to many days of moderate precipitation and not enough dry days or days of extreme rainfall fig 3b quantile mapping section 4 3 is generally very good at reproducing the historical climate variability but often struggles to generalize to new climates especially for more complex variables such as precipitation zhao et al 2017 quantile mapping is also vulnerable to variance inflation where the variance in the downscaled output is higher than the variance in the observed data over a given area maraun 2013 cannon et al 2015 because the variance at low resolutions tends to be lower than the variance at high resolutions in mapping between the two distributions the marginal difference in value is corrected but the local variability in values is not included leading to an overly strong spatial correlation between observed locations without further correction this leads to spatial averages that overproduce extremes at either end of the distribution especially for variables such as precipitation that have lower spatial correlations 3 variable selection appropriate variable selection for downscaling is of critical importance for training a reliable and accurate downscaling model najafi et al 2011 hammami et al 2012 teegavarapu and goly 2018 there are a large number of variables that could potentially be useful when downscaling for most downscaling targets there are a large number of atmospheric variables that correlate in complex ways in the case of precipitation for example useful variables might include relative humidity sea level pressure or other variables related to humidity circulation patterns or convective activity charles et al 1999 timbal et al 2008 maraun et al 2010 the selection of variables for downscaling must also take into account the representation of those variables in the gcms many potentially useful variables are not reliably simulated in gcms making them likely to be poor choices for a downscaling model cavazos and hewitson 2005 teegavarapu and goly 2018 in most cases expert opinion is likely to be useful in selecting final sets of predictors that make physical sense for the climatology of the downscaling target region it can then be useful to compare to the variables that objective methods identify as the most useful these objective methods can also be used to select a subset of the initial predictor set to improve model speed with minimal drops in performance ccdownscaling includes three methods for variable selection sliced inverse regression sir li 1991 and principal component analysis pca hotelling 1933 and random forest rf breiman 2001 different methods for variable selection can make different identifications of the most important variables so it can be helpful to run multiple techniques and look for common variables between the different approaches ccdownscaling makes comparing these methods straightforward all of these methods are implemented in the variable selection code and an example can be found in the example folder and jupyter notebook the relative importance rankings for an initial set of five predictors at five pressure levels from these three methods can be found in table 1 the six input variables are air temperature air geopotential height hgt relative humidity rhum sea level pressure slp surface only meridional wind speed vwnd and zonal wind speed uwnd two of the three methods sir and pca also serve as dimension reduction techniques these method can be used to transform the high dimension input data to lower dimension constructed features that maintain as much of the original information as possible ma and zhu 2013 this can be used to construct a set of new inputs that are a combination of the original inputs and capture as much as possible of the variance of the downscaling target these new variables can be used as inputs to the downscaling methods reducing the computational requirements by having fewer input variables while still retaining the original information 4 downscaling methods the ccdownscaling package incorporates several downscaling methods all conforming to a common framework for integration into downscaling workflows the application programming interface api mirrors the scikit learn setup for machine learning models which allows the easy integration of scikit learn methods and ease of use for those already familiar with scikit learn this approach allows for the development testing and comparison of different downscaling methods in a shared framework as displayed in fig 4 the usage of the methods is demonstrated in sections 4 and 5 of the example jupyter notebook all of the downscaling methods in the package are initially trained from reanalysis data once a model has been trained it can then be used with gcm data to provide downscaled estimates for future climate scenarios 4 1 som downscaling self organizing maps soms are an unsupervised machine learning method for mapping a complex set of inputs onto a two dimensional map of nodes each representing a typical pattern observed in the input data kohonen 1990 for downscaling soms can be used to identify characteristic synoptic scale weather patterns and relate those patterns to the observed local conditions hewitson and crane 2006 the som can then be used with gcm projections to explore changes to the frequency of these patterns in future climate scenarios gibson et al 2016 soms have been broadly used for downscaling particularly of precipitation in regions such as south africa hewitson and crane 2006 florida sinha et al 2018 and the midwest united states polasky et al 2021 the som method begins by creating a set of nodes arranged in a two dimensional grid each of these nodes is defined by a vector matching the size of one case of training data to train the som each element of the training data in our example case downscaling for o hare airport this data comes from the ncep reanalysis taking a 5 5 grid point window around the station is compared to the som nodes the node whose vector is nearest in euclidean distance to the training element is selected as the best match unit bmu 1 b m u m i n w v i t where w v is the weight vector for node v i is the input dataset and t is the index of the training element the bmu vector is then incrementally updated towards the training element as are the neighboring nodes to the bmu each node weight vector is updated 2 w v s 1 w v s θ u v α i t w v s where s is the current iteration of the training α is the learning rate of the som and θ is the neighborhood function governing how much the update effects the nodes near to the bmu the value of θ decreases exponentially the distance of the node to bmu adjusting the neighboring nodes in addition to the bmu has the effect of sorting similar nodes to be near to one another in the map the overall update rate is governed by α with each successive pass through the dataset α is decreased to more rapidly converge to a stable map once the som has been trained each day in the training data can be placed on the map by finding the bmu for each node of the som this gives a set of days corresponding to that pattern the station observations for those days can be combined to create a probability function of local values for each som node to create new downscaled projections gcm data can be mapped onto the som matching each day to its bmu the probability function of the downscaling target variable can then be sampled from producing the downscaled value for that day by iterating through the days included in the gcm data sets we produce a daily downscaled time series for the given location the soms were implemented in python using the tensorflow library abadi et al 2015 adapted from the open source tensorflow som project gorman 2019 the som method has the advantage of providing insight into the weather patterns giving rise to specific downscaled outcomes through the patterns detected by the individual nodes of the som in the o hare airport example the highest precipitation nodes fall in the center of the top two rows of the som fig 5 the color gradient represents the frequency of that nodes being the bmu in the training data while the number in the box is the average precipitation for the days falling on that node two of these nodes for example the node at row 4 column 5 4 1 and node 3 4 both have high precipitation but very different temperature patterns fig 6 node 3 4 represents a warm summer day with temperatures between 17 c in the south of the region around o hare and 11 c in the north at 850 hpa the high precipitation likely corresponds to summer type convection the 4 1 node is colder with a strong southwest northeast temperature gradient and the precipitation is likely driven by mid latitude cyclones 4 2 scikit learn downscaling methods scikit learn is a widely used machine learning library for python that provides a wide range of machine learning tools pedregosa et al 2011 these methods have been adapted for use as downscaling tools in ccdownscaling as demonstrated for a random forest model for the o hare airport example random forests are a widely used machine learning approach that have been successfully applied to a wide range of problems breiman 2001 their adaptability and ease of use have led to random forest being a go to method in machine learning biau and scornet 2016 random forests have been used for downscaling temperature and precipitation in a variety of locations hutengs and vohland 2016 sa adi et al 2017 pang et al 2017 polasky et al 2021 the ccdownscaling package provides a framework for extending scikit learn provided methods to better suit downscaling problems an example of this functionality is given in two step random forest model which adapts the standard random forest from scikit learn to have a classifier to initially split dry precipitation days then a second regressor model to predict the amount of precipitation for the wet days this model addresses issues of the random forest producing too many days of middling precipitation and too few dry days similar to the undownscaled gcms and generally outperforms a basic random forest for precipitation metrics section 5 4 3 quantile mapping quantile mapping is a commonly used approach for downscaling especially for temperature maraun 2013 han et al 2019 unlike the other methods included in this software package quantile mapping uses only a single variable as input typically the value being downscaled i e daily precipitation from the model for downscaling precipitation it takes the approach of comparing the difference in value between the quantile ranks of an initial and final distribution a transformation is calculated between these ranks which can then be applied to a new input distribution to create a downscaled output that better matches the observed distribution of events quantile mapping is effective at bias correcting gcms outputs but quantile mapping relies on the fields of the target variable from the gcms this can be an issue particularly in the case of precipitation where the representation of the processes underlying the output values are not properly resolved by the gcms other methods which can make use of variables that are better captured in the gcms are likely to yield superior results of the downscaling in these circumstances zhao et al 2017 nonetheless quantile mapping provides a straightforward bias correction approach and is a useful point of comparison for other downscaling methods wood et al 2004 5 validation methods in addition to providing a range of downscaling methods ccdownscaling also provides a number of different evaluation metrics to compare methods and assess the suitability of a downscaling output for a given task depending on the goals of the downscaling and even the use case different downscaling applications may best be assessed using different evaluation criteria for example estimating drought conditions will require accurate estimation of average precipitation and temperature over longer time frames while projecting for flooding situations will require accurate representation of large precipitation events and temporal correlation for multi day events the implementation of many validation methods in ccdownscaling is thus a key part of its broad applicability in this section we describe a number of evaluation metrics and show the results of these metrics on the o hare airport example for a range of downscaling methods examples of these metrics can be found in sections 7 and 8 of the example jupyter notebook 5 1 som training metrics the som method includes two specialized training metrics quantization and topological error these are commonly used metrics for assessing the training characteristics of a model setup and should be used when tuning the hyperparameters controllable parameters that define the setup and training characteristics of the model quantization error qe refers to the average distance between the day vectors assigned to a node and the characteristic vector of the node kohonen 1990 smaller values of qe represent reduced spread within each cluster as the size of the map increases this value will naturally decrease as days are increasingly subdivided between clusters a common tactic for settling on a map size is to look for the elbow where the decrease in qe slows down with the increase in map size céréghino and park 2009 in fig 7 this occurs at the 5 7 map size and that map size was selected for further analyses topographical error te is calculated as the percentage of input vectors whose second best matching units are adjacent to their best match units this is a measure of how well the topology of the original dataset is being preserved in the lower dimension space of the som uriarte and martín 2005 te generally increases as the size of the map increases with more opportunities for non adjacent nodes to match an input vector fig 8 these two metrics combine to help choose an optimal size for the som balancing the gains in specificity against the increasing complexity of the model an example demonstrating both of these methods can be found in som training example py in the example folder of the package 5 2 distribution tests one of the key characteristics for a downscaling method is determining the skill of a downscaling approach is how well it reproduces the historical distribution of events precipitation in particular is commonly spread over too many days with small amounts of precipitation in gcms projections reducing the number of dry days and large precipitation events fig 2 these metrics are especially valuable when trying to understand the shifts in climate and the frequency of different event types under climate change scenarios perkins et al 2013 polasky et al 2021 we implement two common distribution tests in ccdownscaling pdf skill score and the kolmogorov smirnov test the pdf skill score measures the similarity between two probability density functions pdfs by calculating the minimum value between observed and modeled counts within each bin of a histogram to measure the shared area within the distribution 3 s s c o r e 1 n m i n z m z o where s score is the skill score z m and z o are the frequency of modeled and observed values in a given bin respectively and n is the number of bins used to calculate the pdf perkins et al 2007 pdf skill score provides an easily interpretable score for the similarity between the observed and downscaled distribution of values the kolmogorov smirnov ks test provides a non parametric statistical test to evaluate the likelihood that two samples are drawn from the same underlying distribution massey 1951 the ks test provides both a test statistic which can be used to assess the similarity of two distributions where smaller values are more similar and a probability that the given distributions are drawn from the same underlying probability distribution tables 2 and 3 show the ks scores and probabilities for the different methods for the o hare airport example for precipitation and maximum air temperature respectively in most cases pdf score and ks test statistic are strongly correlated brands et al 2012 an exception occurs when the distribution includes a large number of near zero values as in the case of daily precipitation in this instance pdf score can struggle due to the kernel density smoothing applied in the calculation brands et al 2012 the effect of this difference can be seen in the o hare airport example for temperature the som and random forest methods score similarly to one another in both pdf score and ks statistic table 3 by contrast for precipitation the pdf statistic for the two methods is similar while the som far outperforms the random forest on ks statistic the two part random forest achieves a ks statistic score much closer to that of the som primarily by correcting the number of days with no precipitation compared to the single random forest visually we can see that the som much better matches the observed distribution fig 2 indicating that the ks test statistic better represents the relative skill of the two downscaling methods for precipitation 5 3 error metrics error metrics are a common method for evaluating model skill evaluating the day by day differences in downscaled and observed values for the testing period these direct measures of difference are most useful when comparing methods that seek to recreate the specific conditions on each individual day such as the random forest model much of the goal with downscaling and with any climate modeling is to understand the full range and frequency of events occurring rather than to predict the correct event on the specific day for this reason the error metrics are often less useful for assessing downscaling model skill than the distribution tests nonetheless these measures can provide useful comparisons especially between similar classes of models and so we have made several error metrics available in ccdownscaling these error metrics should only be applied when comparing downscaled outputs calculated from reanalysis data to the ground truth observations data sets coming from gcms are unlikely to match the daily variation of the historical records and the results may not represent the documented conditions commonly used error metrics such as mean squared error and bias are included in the package the results for the o hare airport example can be found in table 2 for daily precipitation and table 3 for maximum temperature the results for som and linear regression models for temperature show the differences between the error and distribution metrics the som outperforms the linear model on the pdf and ks scores as it provides a better performance of matching the overall distribution of events but scores worse in terms of rmse and bias because it does not match the specific day to day variations as well as the linear model a similar approach can be taken by plotting the observed and downscaled data against one another and calculating a linear regression fig 9 this provides a clear visual representation of how well the downscaled data matches the observations but suffers from the same drawbacks as the error metrics mentioned above 5 4 extreme events performance on extreme events is a key goal for many downscaling applications wigley 1985 many extreme event definitions will depend on the specifics of the question to be answered but we include some common extreme event indices in the ccdownscaling package in addition to being potentially useful values for questions about future climate scenarios these indices can be a useful metric for determining the skill of the different downscaling methods using the warmest wettest year split described in section 2 4 we calculate the maximum temperature precipitation climdex values for the som rf and qmap methods tables 4 and 5 for precipitation while the rf method does the best job of the three at adapting the to higher average precipitation the poor performance on the high and low ends of the distribution are evident in the very low values for 95 t h percentile r95p 99 t h percentile r99p and consecutive dry days cdd the differences are not as pronounced for the max temperature indices but here qmap clearly outperforms the other two methods for most of the indices 5 5 autocorrelation autocorrelation provides a metric for temporal similarity within time series data it is calculated by taking the correlation between a timeseries and a time lagged copy of the same timeseries for a number of different time lags this can be particularly important for applications such as flooding where understanding the likelihood of multi day large rainfall events is crucial for projecting the frequency of major flooding events in a given climate scenario in the case of o hare airport the autocorrelation of both the observed and downscaled datasets falls off rapidly with most of the downscaled methods apart from the som and qmap overestimating the observed correlation at a lag of one day fig 10 6 concluding remarks the accelerating need for reliable highly localized data for climate change scenarios has led to the development of software packages such as ccdownscaling that can readily and reliably provide information tailored to individual sites or regions ccdownscaling is a new software package providing options for downscaling approaches and tools in a manner that the user can readily apply and iterate upon to meet the needs and requirements of the desired downscaled climate change projections it is freely available and open source to be easily used in the creation of new downscaled climate data for a range of potential uses ccdownscaling provides som based downscaling along with the extension of traditional machine learning tools to downscaling we demonstrate the capabilities of the package using an example station located at the chicago o hare airport in illinois this example serves to demonstrate the use of the software package but will not cover many issues that will come up for a real downscaling use case including limited data availability complex terrain or changes in land cover typically we have found the som method performs well for precipitation downscaling while the random forest and qmap methods often perform well for temperature variables but these results can vary based on the meteorology and available data for a given location in addition to the downscaling methods the package includes a variety of metrics for assessing the skill and utility of downscaled outputs these tools along with the example given provide a framework for creating and evaluating new downscaled products tailored to a given location and use case declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests jenni l evans reports financial support was provided by national science foundation acknowledgments thanks to zach moon for assistance in making the code more usable and daniel polasky for helpful comments on the manuscript we also thank two anonymous reviewers for their helpful comments on this manuscript thanks as well to the penn state institute for computational and data sciences for computational resources and support this work was supported by the national science foundation united states under grant 1639342 appendix a climdex indices the 27 climdex indices represent a standard set of values for precipitation maximum temperature and minimum temperatures to describe changes in climate with a focus on extreme events created by the expert team on climate change detection and indices etccdi http etccdi pacificclimate org indices shtml these values provide an overview of mean and extreme changes that will be commonly useful to many applications for maximum and minimum temperature as well as precipitation see table a 1 appendix b package setup installation instructions for the ccdownscaling package can be found on the github along with the code for the example use case in the form of both a python script and jupyter notebook both jupyter notebook and script contain the same code the jupyter notebook is included to provide a more readable and interactive interface for the example to set up the environment for ccdownscaling we recommend using conda for ease of installation a conda environment yml file with the list of requirements all the downscaling models in ccdownscaling conform to a common api making the addition of new methods to a downscaling use case straightforward the jupyter notebook in the example folder gives an example of the main pieces of the ccdownscaling package section 2 shows an example for using three different train test splits section 5 demonstrates training several different models all called using the same format where the model is created then trained fit and final downscaling output is generated predict the outputs from these different methods can then be passed into any of the analysis methods section 6 demonstrates some of the som specific analysis plots including heatmaps and variable grids for the som nodes section 7 then shows the more universal evaluation metrics computing skill score values for each of the methods section 8 makes the plots for comparing the different methods and finally section 9 calculates the climdex values for each output 
