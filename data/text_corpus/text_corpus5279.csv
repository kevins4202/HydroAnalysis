index,text
26395,identifying the offshore forcing and breaching conditions that lead to marine inundation is of high importance for risk management this task cannot be conducted by using a numerical hydrodynamic model due to its high computation time cost of several minutes or even hours in the present study we show how the random forest rf classification technique can approximate the numerical model to explore these critical conditions we focus on the bouchôleurs site which is located on the french atlantic coast and exposed to overflow processes an iterative strategy is developed for selecting the numerical simulations a total of 200 to train the rf model the sensitivity to the input parameters is studied using permutation based importance measures and extended versions of the partial dependence plots the results highlight the key interplay among the high tide level the surge peak and the phase difference and the complex role of the breaching location keywords coastal flooding overflow random forest classification probability variable importance analysis partial dependence plot 1 introduction recent storm events such as katrina in 2005 and xynthia in 2010 illustrate the present day coastal damages and injuries that can affect coastal areas in both cyclonic and non cyclonic environments katrina was one of the most powerful hurricanes ever to be registered in the atlantic and led to more than 1 500 deaths and damages of approximately 80 billion usd blake 2007 whereas xynthia was a mid latitude storm that severely hit low lying coasts in the central part of the bay of biscay on 27 28 february 2010 and led to more than 40 deaths and one billion euros of material damages see e g vinet et al 2012 from a statistical point of view the wave heights that were generated during the xynthia event could not be considered extremes see e g bertin et al 2012 however what makes this event rare is the combination of a spring tide with a large storm surge enhanced by young wind waves that reached its maximum near the tide peak this illustrates the importance of gaining insights into and a better understanding of the combination of offshore forcing conditions water levels tide peak storm duration wave characteristics that lead to inundation or not a systematic exploration of these conditions is of primary importance for risk management based on multiple objectives as discussed by idier et al 2013 1 for crisis management purposes since this knowledge can be used as input for constraining forecast and early warning systems see an example of such systems in uk stansby et al 2013 2 for prevention purposes since this knowledge can be used to support the return period calculation in determining flooding risk see e g gouldby et al 2014 3 for enhancing risk culture by improving risk awareness and preparedness regarding various possible offshore scenarios in addition to offshore conditions coastal defence failures such as failures of artificial structures such as dikes or breaching of sand dune systems might also influence the inundation characteristics spatial extent inland water height flooding time of arrival etc as illustrated by the 1953 north sea 2005 katrina and 2010 xynthia flooding events therefore a deep understanding of the breaching characteristics spatial location along the coast width etc that increase the inundation likelihood is also desirable for clarity we use the generic term breaching to refer to the failure of an artificial coastal defence or the breach of natural systems such as dunes the task of detecting and analysing such critical conditions in the coastal risk domain offshore and related to coastal defence failure shares the same objectives as the analytic approach for scenario discovery which aims at coping with deep uncertainty e g groves and lempert 2007 kwakkel et al 2013 scenario discovery analysis relies on exploration and analysis of numerical simulations the objective is threefold exhaustively sampling the space that is spanned by the input parameters analysing the consequences via numerical simulations and identifying regions of interest in the input space however in the application of such a systematic search to the coastal domain multiple difficulties are encountered which are at the core of the present study first coastal flooding assessment is typically supported by numerical simulations which might have very large computational time cost typically of several minutes to hours per simulation depending on the process complexity that is taken into account by the model such a computational burden often restricts the analysis to a limited number of input configurations termed as scenarios in the following even when computing architectures are used meta modelling approaches aka surrogate modelling see e g castelletti et al 2012 razavi et al 2012 can efficiently handle the difficulty that is related to computational time cost by replacing the computationally expensive numerical code by a costless to evaluate statistical emulator approximation which is built using a limited number of computer experiments typically a few hundreds the meta model based strategy proved to be very efficient when combined with adaptive sampling strategies a reduction by a factor 20 40 of the total number of necessary computationally expensive simulations was achieved in the case that is described by rohmer and idier 2012 a second difficulty is related to the nature of the coastal processes which are usually controlled by thresholds as an illustration let us consider marine flooding that is induced by overflow processes if the water level at the coast which results from storm surge and tide characteristics is lower than a specified threshold flooding cannot occur the water height at any given location inland remains zero otherwise provided that the water level slightly increases and exceeds a specified threshold overflow induced inundation can occur and inland locations may be flooded in other words the water height inland remains zero until a specific combination of conditions surge amplitude tide amplitude phase difference etc that lead to a flood at this location are met in these situations the functional mathematical relationship between the offshore conditions and the response is expected to be highly non linear and standard meta modelling techniques might fail gaussian process polynomial chaos expansion etc see e g jin et al 2001 among the available possible options see e g razavi et al 2012 an alternative procedure is to use a classification strategy to identify the conditions that lead to inundation class 1 and those that do not class 0 as proposed for instance by bourinet et al 2011 in the field of structural safety and tested by rohmer et al 2016 in the field of marine flooding finally a third difficulty is related to the number of offshore conditions that depend on the considered physical parameters tide atmospheric storm surge wave height wave direction wave period etc this complicates the detection of the critical conditions the larger the number of such conditions the broader the input space and the more tedious the exploration this also makes the visualisation of the potentially complex classification rules difficult variable importance analysis see e g galelli et al 2014 can overcome this issue by carefully selecting the most important parameters regarding the occurrence of the considered event different statistical techniques have been proposed in the literature for dealing with these three difficulties see e g kuhn and johnson 2013 in the present study we utilize a meta modelling strategy that is based on the random forest denoted rf technique breiman 2001 to approximate computationally expensive numerical simulators first difficulty using a limited number of pre calculated simulations typically 100 200 many studies have shown the high performance of rf in different domains of application ecology cutler et al 2007 water quality brooks et al 2016 wildfires rodrigues and de la riva 2014 etc to overcome the second difficulty the rf model is used as a classifier to predict whether a set of forcing and breaching conditions will lead to inundation class 1 or not class 0 at a given spatial location of the considered study site regardless of the type of input parameters i e continuous or categorical with possible correlation among them the third difficulty is handled by means of the variable importance measures that are provided by the rf model wei et al 2015 the objective of the present study is to address the following questions q1 to what extent is the rf model a valid approximation of the true computationally expensive numerical simulator q2 how can the scenarios defined by values of the input parameters that should be simulated to construct the rf model be efficiently selected q3 how can the rf model help to discover and cast light on the critical conditions forcing and breaching that lead to marine inundation for this purpose we take advantage of recent advances that are related to the implementation of this technique in the r software r developement core team r 2017 1 the numerically efficient version of the rf model that was developed by wright and ziegler 2016 implementation in the r package ranger 2 the estimate of the classification probability that was proposed by malley et al 2012 implementation in the r package ranger 3 the feature selection algorithm that was developed by kursa and rudnicki 2010 implementation in the r package boruta and 4 the visualisation tools that were proposed by goldstein et al 2015 for exploring the dependencies using the individual condition expectation curves implementation in the r package icebox the paper is organized as follows first we describe the overall strategy and the associated methods for exploring the offshore forcing and breaching conditions that lead to marine inundation sect 2 second we describe the application case at the bouchôleurs site which is located along the french atlantic coast sect 3 third we apply the proposed strategy sect 4 and address research questions q1 q3 by discussing the added value and the limitations of the rf based analysis from a risk analysis perspective sect 5 the list of acronyms is provided in appendix a 2 methods in this section we first describe the overall strategy that is applied in the present study to explore and analyse the scenarios that lead to inundation sect 2 1 then we describe the key ingredients of the strategy sect 2 2 2 6 2 1 strategy description an overview of the strategy is provided in fig 1 a preliminary and necessary step aims at setting up and validating a hydrodynamic model for simulating the flooding processes for the site of interest on this basis the steps are as follows step 1 a limited number n 0 of scenarios are first selected in the domain of forcing and breaching conditions x these scenarios are used as inputs of the numerical modelling chain described in sect 3 3 using the techniques that are described in sect 2 3 1 step 2 the time evolution of the inland water height denoted h is computed over the considered study area for each of the considered input conditions a class indicator y is defined and labelled as 1 if the maximum value of h over time exceeds zero at the considered spatial location and 0 otherwise set d x i y i i 1 n0 constitutes the initial training dataset based on which the classification rules are learned using the random forest rf classification approach see sect 2 2 step 3 to increase the accuracy of the rf classification model an iterative procedure is proposed for selecting in an adaptive manner the input scenarios that should be run this is based on the evaluation of the classification probability p see sect 2 3 2 step 4 once the number n of affordable numerical simulations has been reached typically 100 200 the predictive capability of the rf model should be assessed see sect 2 4 step 5 once the performance of the rf model has been validated the analysis of the classification rules that lead to marine inundation can be performed this is done by following a two step approach 1 study of the importance influence of each input parameter using variable importance analysis see sect 2 5 2 exploration of the classification rules as functions of the key most important input parameters see sect 2 6 2 2 random forest based classification based on the set of n training data d x i y i i 1 n the objective of setting up a classification model classifier is to identify the rules for assigning a new yet unseen input scenario into class 1 or 0 without resorting to a computationally expensive numerical simulation a basic classification model can rely on the decision tree technique breiman et al 1984 a decision tree is recursively built using binary partitioning of the domain space into regions that are increasingly homogeneous with respect to the class variable as an illustration fig 2 schematically depicts a tree model for binary classification class label 0 or 1 the homogeneous regions correspond to the nodes the splitting process aims at subdividing one parent node into two sub nodes which should be as homogenous as possible node homogeneity is measured by the gini impurity index which is denoted as g breiman et al 1984 and defined as follows 1 g 1 p 1 2 p 0 2 where p k n k n is the fraction of the n k samples from the class with k 0 1 the decrease of g is calculated as g parent g split1 g split2 after splitting the parent node into two sub nodes using a cut off value c see fig 2 at each step of the construction the algorithm selects the value of c that results in the largest decrease of g this process is performed until sub division no longer decreases the gini index or a minimum node size is reached typically a size of 1 for classification random forest builds on the same principles as decision tree classification models but extends them by adding a random character to the construction process at two levels 1 each tree is constructed using a different bootstrap sample of the data breiman 2001 which enables the instability issue of classical decision tree i e the tree structure and the resulting predicted classes might not be stable when new samples are provided to be overcome 2 each node is split using the best among a subset of input parameters that are randomly chosen at that node which enables situations of highly correlated input parameters to be considered cutler et al 2007 fig 3 schematically depicts the main steps of the rf algorithm which are as follows step 1 draw n tree bootstrap samples d j e g using random sampling with replacement from the original training data d x i y i i 1 n step 2 for each of the bootstrap samples d j grow a classification tree by splitting until all the observations in each terminal node come from the same class using the gini index that is defined in eq 1 at each node rather than choosing the best split among all predictors input parameters choose it among m try randomly selected predictors for each tree j the classification error is measured by the misclassification rate which is denoted err j and defined as follows 2 e r r j 1 d j c i d j c i y i y ˆ i where d j c is the set of samples that are not used for the construction of the considered classification decision tree and constitutes the set of out of bag oob samples such that d j c d j d d j c is the size of the jth oob set y i is the ith class indicator within the jth oob set and y ˆ i is the corresponding class prediction using the jth classification tree step 3 predict new data by aggregating the predictions of the n tree trees the class of the yet unseen input parameters is estimated as the class with the maximum number of occurrences votes among the different trees a class probability probability of belonging to class 1 or 0 can be estimated by either relying on a voting based approach by counting the proportion of the considered class that is estimated using the n tree trees or using a regression based rf model as described by malley et al 2012 2 3 selection of the training data multiple strategies for selecting the training dataset will be used to learn the classification rules we propose selecting the scenarios of forcing and breaching parameters that provide the most information regarding the accuracy defined as 1 err of the rf model to do so a two step approach is proposed see fig 1 1 select an initial set of scenarios typically 25 50 to set up an initial rf model sect 2 3 1 2 sequentially select new input scenarios which should a priori increase the classification accuracy and build at each step a new rf model sect 2 3 2 the sequential procedure is conducted until the maximum number of affordable computationally expensive simulations typically 100 200 has been reached 2 3 1 step 1 initial selection the initial step can rely on the same principles as described by camus et al 2011 first many say 500 input parameters that describe the scenarios are generated using for instance a quasi random sequence such as the sobol sequence sobol 1976 to ensure an optimal exploration of the input space then the objective is to select the elements within this first dataset that are the most dissimilar the notion of dissimilarity is based on the notion of distance which should be carefully chosen when mixed types of variables are present as in our case i e we deal with both classical continuous variables such as the value of a surge peak the phase difference between tide and surge temporal signals etc as defined in sect 3 2 and categorical discrete variables related to the breaching location scenario in our case see further details in sect 3 a possible option is the gower distance gower 1971 which is defined as follows consider x i which is the ith vector of p input parameters and x i k which is the kth input parameter the gower distance d i j between two input vectors x i and x j is 3 d i j k 1 p d i j k p where d i j k is a dissimilarity measure that is adapted to the nature of the kth input parameter for continuous variables d i j k x i k x j k r k where r k is the range of values of the kth parameter for categorical variables d i j k 1 if x i x j and 0 otherwise once the dissimilarity matrix whose elements are pairwise dissimilarities distances between observations in the dataset has been estimated a clustering technique see e g kaufman and rousseeuw 1990 can be applied to select a limited number of scenarios a possible option is the k medoids also named partitioning around medoids pam algorithm which was developed by kaufman and rousseeuw 1990 chapter 2 the pam algorithm is based on the search for k representative objects named medoids among the observations within the dataset a medoid is an object within a cluster for which the average dissimilarity between it and all the other members of the cluster is minimal geometrically it corresponds to the most centrally located point in the cluster after finding a set of k medoids k clusters are constructed by assigning each observation to the nearest medoid the pam algorithm is less sensitive to noise and outliers compared to the popular k means algorithm because it uses medoids as cluster centres instead of means which are used in k means once the initial selection has been performed at the centroids of the clusters e g of the order of a few tens of input scenarios the water height can be computed using the numerical model over the area of interest step 1 in fig 1 2 3 2 step 2 adaptive selection based on the initial set of numerical model results a first rf model can be set up and used to estimate the inundation probability denoted as p for any new scenario configuration x i e the probability that y belongs to class 1 given x with the regression based procedure that is described by malley et al 2012 when p is close to 1 0 the classification of y is provided by the rf model with high confidence when p is close to 0 5 the classification of y is provided by the rf model with less confidence the next step is to select an additional input scenario where the uncertainty of the classification is the largest i e where the rf classification is the least confident with p close to 0 5 this is equivalent to selecting the next scenario in which p 1 p reaches a maximum value which proved to be a more numerically stable criterion in practice candidates for the input scenarios are randomly generated e g using quasi random sequences sobol 1976 for each of these candidates p 1 p is evaluated using the rf model and the candidate for which this criterion is maximum is selected as the new input configuration of the numerical model once a new input scenario has been selected and the corresponding inland water height has been computed a new rf model can be set up and the same procedure can be performed until the maximum number of affordable simulations in terms of computation time cost is reached typically 100 200 2 4 validation of the rf prediction at the end of the iterative selection procedure the predictive capability of the rf model should be assessed i e the capability of the rf model to correctly predict the class of new yet unseen input configurations this can be done using either a set of new simulations or cross validation procedures hastie et al 2009 two performance criteria are used the first is the accuracy denoted by acc and defined as the ratio of the number of correctly classified events by the rf model to the total number of events i e the capability of the rf model to predict the true class given the values of the forcing conditions the second performance criterion aims at measuring the extent to which the rf model can distinguish between the classes i e the ability of the rf model to rank the events with y 1 i e sea water reaches the considered spatial location relative to those with y 0 i e sea water does not reach the considered spatial location this criterion is related to the capability of minimising the number of false alarms which correspond to events that belong to class 0 that are classified as belonging to class 1 this can be assessed using the receiver operating characteristic roc curve metz 1978 which relates the true positive rate tpr defined as the ratio of the number of events that are predicted by rf as having y 1 that actually have y 1 to the total number of positive events which have y 1 and the false positive rate fpr defined as the ratio of the number of events that are predicted by the rf model as having y 1 but which actually have y 0 to the total number of negative events which have y 0 at various probability threshold settings the closer the curve is to the left hand corner of the roc space the better the classification the area between the first bisector and the roc curve denoted as auc allows the performance of the rf model to be quantified 2 5 variable importance and selection once validated the obtained rf model can be used to assess the influences of the forcing and breaching parameters on the model output i e the variable importances in the rf approach the importance of a variable input parameter is defined as its impact on the predictive capability of the rf model which is measured by the misclassification rate err eq 2 different approaches for defining a variable importance measure have been proposed in the literature e g wei et al 2015 we focus on the permutation variable importance measure which aims at comparing the misclassification rate before and after randomly permuting the values of a specific input variable as follows 4 v i e r r j i e r r j where e r r j i is the misclassification rate of the jth tree that is constructed by permuting the values of the ith input variable equation 4 can be used to define a global importance measure for each input variable by averaging over all trees i e by computing the mean of the n tree vi values similarly a standard deviation value can also be computed although this importance measure is useful for ranking the variables in terms of influence no straightforward cut off value exists for identifying variables of negligible importance a possible approach is to rely on the boruta algorithm which was developed by kursa and rudnicki 2010 it is based on the z score which is defined as the ratio of the mean to the standard deviation of the n tree vi values the main strategy is to define d new input variables which are termed shadow values by copying the existing ones and randomly shuffling their values for each new random permutation of the shadow values z scores are computed for each of the d original input variable plus the shadow ones by construction the shadow variables have no influence however their z score although low is non zero due to the random shuffling process the statistics of these shadow z scores can then be used as reference cut off values to decide whether a given input variable is important or not 2 6 visualisation of the classification rules partial dependence plots pdps were introduced by friedman 2001 to facilitate the diagnosis of the functional relationship between the input variable of interest and the predicted response which is denoted as f ˆ for any machine learning technique in the classification case f ˆ can be chosen as the logarithm of the odds log odds which is defined as the ratio of the probability of belonging to class 1 inundation to that of belonging to class 0 absence of inundation a positive log odds indicates that the inundation event is the most likely the probability of class 1 is superior to that of class 0 consider n evaluations y ˆ i f ˆ x i given n input vectors of p input parameters x i x i 1 x i 2 x i p i 1 n in the univariate case the pdp for the jth input parameter corresponds to the plot of the following average function ϕ x 5 ϕ x 1 n i 1 n f ˆ x i 1 x i 2 x i j 1 x x i j 1 x i p 1 n i 1 n f ˆ x i c x where x i c is the set x i 1 x i 2 x i j 1 x i j 1 x i p the pdp provides insights into the influence of the jth input parameter over the range of x values on the set of model predictions y ˆ i i 1 n after the impact of all other input parameters has been averaged out goldstein et al 2015 proposed extending this visualisation approach by plotting not only the average partial effect but also all the f ˆ x i c x curves against the x values for each fixed value of x c these plots are called the individual conditional expectation plots ice plots the main advantage of this procedure is that it provides a broader insight into the variants of the conditional relationships instead of focusing only on the average effect 3 case study in this section we describe the setting of the study site and the main physical processes that are related to marine flooding sect 3 1 the offshore forcing and breaching conditions which are denoted as x sect 3 2 and 3 3 and the numerical model that is used to simulate marine flooding sect 3 4 3 1 site description assets and main processes the case study focuses on the bouchôleurs district and the natural reserve of yves marshland they are located on the atlantic coast of france see fig 4 a near the city of la rochelle in a low lying natural coastal marsh and dune system in front of the strait pertuis d antioche between ré island and oléron island fig 4b bouchôleurs is a district of approximately 700 inhabitants of châtelaillon city the tide is macro tidal with a maximum tidal range of 6 68 m at aix island french hydrographic and oceanographic service shom 2014a more details on the site can be found in andré 2013 muller et al 2016 the main offshore drivers of storm induced coastal floods are the tide storm surges and waves the study site is located inside the pertuis charentais fig 4a therefore it is protected from the direct action of waves however it is affected by the indirect action of waves through two processes regional wave setup bertin et al 2015 and sea surface roughness young waves lead to an increase of atmospheric storm surge through the increase of sea roughness bertin et al 2012 thus the main hydrodynamic drivers to be taken into account are the tide and the storm surge which results from the contribution of the regional wave set up and the atmospheric storm surge while the direct effects of waves can be neglected at the bouchôleurs site this also implies that overflow is the main flood process at this site and that a shallow water model can be used to reproduce the flood see sect 3 4 this site was hit by the storm xynthia 27 28 february 2010 which was characterised by a high storm surge 1 5 m at la rochelle tide gauge that was in phase with a high spring tide it led to the inundation of several parts of low lying areas thereby causing large economic losses and losses of life see e g pedreros et al 2010 bertin et al 2012 flooding at the study site was mainly induced by overflow processes which were exacerbated by the damages to the coastal dune system erosion and breaching over the yves marsh dune see e g muller et al 2016 overflow starts on the study site when the total water level at the coast is higher than the minimum topographic level along the coast which reaches 3 2 m ign69 general levelling of france in the absence of breaching however exceeding the topographic level of 3 2 m ign69 at the coast does not necessarily imply that the sea water will propagate to any specific inland location other parameters such as topography and land cover and the associated friction play key roles in the flood propagation inland thus the water height h at a given spatial location remains zero until specific combinations of conditions surge amplitude tide amplitude phase difference between surge peak and high tide storm duration etc are met for the water to propagate inland the purpose of rf based analysis is to explore them this analysis is performed at four specific points of the studied area these spatial locations correspond to four different assets see fig 4b which are selected because they are either particularly vulnerable to flooding or key elements in the case of evacuation asset a1 the camping asset a2 a potential key intersection road crossing for evacuation asset a3 the holiday camp asset a4 the north part of the yves marshland 3 2 setting up the forcing scenarios the physical forcing conditions are time series of tide and surge in this paper the time series are described by assumed shapes using the scalar parameters that are shown in fig 5 see also table 1 the tide is simplified by assuming a sinusoidal signal that is parameterised by its high tide level t see fig 5 top the surge signal is assumed to be described by a triangular model as schematically described in fig 5 bottom using four parameters namely the peak amplitude s the phase difference t 0 between the surge peak and the high tide the time duration of the rising part t and the time duration of the falling part t the ranges of variation for the parameters table 1 are selected based on the following assumptions the range of variation for t fig 6 a is defined by the highest and lowest predicted high tide levels that are reported at the aix island tide gauge the closest gauge to the study site relative to vertical datum ign69 the highest tide value is provided in shom 2014a the lowest tide value is obtained by extracting the smallest value of the high tide that is predicted with the software shomar 2014 shom 2013 over a time span that is larger than the 18 6 year lunar nodal cycle the range of variation for s is based on the analysis of tide gauge measurements at la rochelle la pallice this tide gauge is located further from the site compared to the aix island gauge fig 4a but provides a much longer time series decades than that of the aix island years shom 2014b with differences of only a few centimetres in common time spans the surge time series is derived from the difference between the hourly total water level records and the predicted tide time series estimated by harmonic analysis using the utide software that was developed by codiga 2011 the lower bound of s is determined as the threshold above which the surge data appear to be well modelled by a generalised pareto distribution within the setting of extreme value analysis see e g bulteau et al 2015 this is done using a combination of the mean residual life plot and plots of parameters that are estimated using a range of thresholds threshold stability plots see coles 2001 the upper bound of s is defined as the largest observed value 1 5 m xynthia event plus a margin of 1 0 m to cover a very large range of situations i e broader than that defined by the observations this value is considered to represent extreme situations fig 6b but is plausible and associated with a return period value of about 500 years calculated using a generalised pareto distribution and maximum likelihood estimator see e g bulteau et al 2015 for the details on the procedure the range of variation for t 0 is determined based on the analysis of the surge time series fig 6c the range of variation for t and t is constrained by the tide period 3 3 setting up the breaching scenarios in the domain of coastal flood modelling a breach is usually accounted for using the following characteristics its spatial location its width and length its topographic level after failure the failure duration in our study ten locations of breaches are defined locations b1b6 fig 4b are assumed to be related to the breaching of the dune systems they are selected based on historical observations during the xynthia event see muller et al 2016 and references therein in addition four hard coastal defence breaches are considered at locations b7b10 fig 4b the scarcity of local historical data on failures of hard coastal defences dikes makes it hard to rely on past observed breaches in parameterising them thus these coastal defence breaches were assumed and selected to be close to possible vulnerable assets a1 a3 in terms of the spatial dimensions of the breaches the length is assumed constant 25 m while the width is not explicitly described see the description of breach modelling in sect 3 4 the topographic level after failure is parameterised as a fraction r erosion rate of the initial crest level relative to the coast defence foot for instance r 50 accounts for a topographic level of 50 of the initial crest level the failure duration i e the duration between the breach initiation and the end of the event is assumed to be 120 and 15 min for dune systems and coastal defences respectively the current analysis is focused on the impacts of the breaching location and the topographic erosion rate r table 1 3 4 setting up the numerical model in the present study we use the numerical simulator named model for applications at regional scale 1 1 available at http wwz ifremer fr mars3d le modele faq acces au code mars lazure and dumas 2008 this finite difference model solves the shallow water equations and was originally designed to compute regional coastal hydrodynamics tide and storm induced water level and currents to account for the specificities of local coastal flooding processes the functionalities of the mars were extended this extension is called mars flood first a new functionality f1 for representing the hydraulic processes around connections such as culverts and weirs was added this has been done by using the capability of the mars to impose source and sink terms on the domain which are computed as follows 1 the inflow and outflow discharge are deduced from the continuity and energy equations between the upstream section and the downstream section of the considered connection 2 the type of flow depends on whether the hydraulic connection fully flows or not and whether the flow is controlled by the inlet or outlet the second new functionality f2 deals with the inclusion of the breaching effect in the model the breaching processes are modelled as a weir with a water surface moving down linearly from dyke crest to the bottom the breach location is represented by two mesh elements across its width when the flow goes from the coast to the inland area one source term is implemented on the side of the coast and at the same time one sink term is imposed on the side of the land with a symmetric value of that discharge finally when using this model for coastal flood modelling the parameters that are related to the wet dry interface dynamics must be carefully chosen and the locations of topographic features road wall dike dune etc must be very precise as these features have a significant impact on flood propagation a grid model with a spatial resolution of 25 25 m2 and a total of 39 188 mesh cells is used the digital elevation model dem is based on bathymetric and topographic surveys including lidar as described by muller et al 2016 and accounts for the topographic features road wall dike railway dune etc which influence the flood propagation the land cover effect on the flood is taken into account by using a spatially varying friction coefficient in the present study the classification of the 2006 corine land cover data is used to set up a spatial grid of manning friction coefficients based on the study of bunya et al 2010 see fig 7 the hydraulic connections e g the hydraulic culverts below the roads dike railway etc see fig 4b are also identified and taken into account in the model the model was validated by comparing the numerical model results with the observations of water heights and flood extension sogreah 2011 during the xynthia event the offshore boundary conditions for the xynthia event are obtained from a dynamical downscaled model of wave tide and storm surge which provides the total water level resulting from tide atmospheric storm surge and regional wave set up the modelling of the offshore conditions is described in paris and pedreros 2012 the six breaches that were observed during the xynthia event are also taken into account the water heights that are computed with the numerical model are compared with the 75 local observations of water marks and a mean error a root mean square error and a correlation coefficient of 13 cm 25 cm and 86 respectively are obtained given the values of the input parameters offshore and breaching conditions a single model run takes approximately 30 60 min of computation using a single computer unit cpu over a time duration of four tidal cycles as an illustration fig 8 provides the spatial extent of the inundation i e where the maximum value of the inland water height over time is non zero under different forcing and breaching scenarios fig 8a illustrates the extent to which the flooded area can drastically change when the surge tide levels are modified by only a few tens of centimetres fig 8b illustrates that the location of the breach can have a strong impact on the spatial extent of the flooding hence it can influence whether a given asset can be hit or not by the flooding in particular assets 3 and 4 4 application in this section we apply the strategy that is described in sect 2 1 to explore and identify the forcing and breaching scenarios that lead to overflow induced inundation at each of the four assets of the bouchôleurs site fig 4 first we set up an rf model at the four locations using the adaptive sampling strategy sect 4 1 we assess the validity of the assumption of replacing the true numerical simulator by the rf model in sect 4 2 research questions q1 and q2 to answer research question q3 we use the rf model in a post treatment manner to identify the most important input parameters for describing the forcing and breaching scenarios sect 4 3 finally we explore the key dependencies between the likelihood of the overflow induced inundation and the input parameters sect 4 4 4 1 iterative selection an rf classification model is set up for each of the four locations of interest fig 4 each output of the numerical simulations is classified as 1 if the maximum value of h exceeds zero at the considered asset and 0 otherwise an initial set of 50 scenarios is selected using the dissimilarity clustering approach which is described in sect 2 3 1 and implemented with the pam algorithm preliminary tests not shown were carried out to estimate the optimal number of trees n tree and value of parameter m try that lead to minimum oob misclassification rates on this basis an initial rf model is constructed using the optimal values of n tree 1 000 and m try 5 and 150 new scenarios are iteratively selected by using the probabilistic information estimated using the regression based procedure of malley et al 2012 as described in sect 2 3 2 a total of 200 numerical simulations are performed fig 9 provides the scatter and bivariate density plots of the selected scenarios for asset 4 considering the offshore conditions this shows that large values of t and s tend to be preferably selected while for t 0 values near 0 are preferably selected the tendencies of t and t are more difficult to analyse the variability of the results is large this suggests a lower sensitivity of the model to these parameters considering the breaching conditions locations b 1 and 10 which have large erosion rates seem to be preferably selected these tendencies are also identified for the other assets not shown and should be confirmed in sect 4 3 using the rf based variable importance analysis 4 2 performance analysis the predictive capability of the adaptively constructed rf model is assessed using a set of 500 new numerical simulations which are randomly selected using the latin hypercube sampling lhs strategy mckay et al 1979 by construction these new results are independent from the training dataset that is used for the rf construction two performance criteria are used fig 10 provides the accuracy denoted as acc evolution that is calculated using this validation test set as a function of the number of input scenarios the performance of the adaptive approach is compared to that using purely random selection of the input scenarios conducted 100 times the comparison with the mean and the 95 confidence envelope of the accuracy indicator confirms the highly satisfactory performance of the adaptively constructed rf model fig 10 shows that the adaptive strategy enables a higher level of accuracy to be reached compared to the random strategy acc reaches values that are above the upper bound of the random procedure the second performance criterion aims at measuring the extent to which the rf model can distinguish between the classes inundation and absence of inundation it is assessed by means of roc curves fig 11 depicts them in which the initial before adaptive construction and final after adaptive construction rf models are considered for each asset for all considered assets the performance appears to be improved the final curves are closer to the left hand corner finally the area denoted as auc between the first bisector and the roc curve is calculated using this validation test set as a function of the number of input scenarios fig 12 similar to acc the performance of the adaptive approach is compared to that of a purely random selection of the input scenarios conducted 100 times fig 12 shows that the adaptive strategy enables higher performance levels to be reached compared to the random strategy such that auc reaches values that are above the 95 confidence upper bound of the random procedure for assets 1 2 and 4 and above the mean curve for asset 3 the adaptive strategy is applied to 200 input scenarios for which acc 85 and auc 90 for the four assets 4 3 identification of the most important parameters in the setting of the study site the primary drivers of overflow induced inundation are known to be the high tide level t the surge peak s and the phase difference t 0 sect 3 1 however the quantification of the relative importance of each of these parameters together with those that describe the storm duration and breaching remains unclear using the adaptively constructed rf model the boruta algorithm described in sect 2 5 kursa and rudnicki 2010 can provide useful insights for identifying the important parameters fig 13 shows the z score values for the forcing and breaching conditions for each asset several observations are made as expected the high tide level t has a large influence and is identified as the most important parameter for all assets the second most important parameter is either the surge peak s or the phase difference t 0 this result can be understood from the perspective of past events the xynthia 2010 storm as recalled in the introduction exemplifies the consequences of the combination of high s and a spring tide that is in phase with the surge signal t 0 0 in contrast to the xynthia 2010 storm the lothar and martin 1999 2 2 http tempetes meteofrance fr lothar le 26 decembre 1999 html and quinten 2009 3 3 http tempetes meteofrance fr tempete quinten du 09 au 10 fevrier 2009 html storms had smaller impacts on the atlantic coasts since they were not in phase with high tide despite the moderate to large surge peaks up to 1 m for assets 1 and 2 see fig 13 top only three forcing parameters appear to influence the likelihood of inundation s t and t 0 for asset 3 surge temporal characteristics also play a role although of lower influence than s t and t 0 for asset 4 all forcing and breaching conditions appear to have an influence the storm surge parameter t duration of the storm surge amplitude increase appears to have a negligible effect for assets 1 3 the corresponding boxplots are lower than those of the shadow variables and a small but non negligible effect for asset 4 the breaching scenarios location b and erosion rate r only impact the fourth asset the importance of the location appears to be of the same importance than the storm surge amplitude whereas the erosion rate has a moderate influence and is very close to the reference values of the shadow parameters a more detailed analysis is provided in sect 4 4 2 4 4 exploration of the dependencies based on the variable importance analysis of sect 4 3 we explored in more detail two specific dependencies to do so we generated a new set of 2 500 lhs random input conditions based on which the individual conditional expectation ice plots were estimated 4 4 1 role of the high tide level first the role of the most important parameter namely t as identified in sect 4 3 is analysed fig 14 depicts the evolution of the ice curves for the four assets we observe clearly increasing behaviour of the partial dependence plot pdp in yellow for each asset but with threshold effects for moderate values of t less than 1 0 1 5 m depending on the considered asset all ice curves in grey remain negative thereby indicating that regardless of the values of the other input parameters inundation is impossible below this threshold which is specific to the considered asset location for assets 1 and 2 see the locations in fig 4 the pdp becomes positive inundation is the most likely event for large t values of 2 75 m depending on the considered asset and plateaus above this value the examination of ice curves highlights that combination with the other input parameters can increase the likelihood of inundation for values that are below t 2 75 m down to 1 5 m for assets 3 and 4 the pdp never becomes positive which means that the tide does not lead to inundation on average i e after the impacts of all other input variables have been averaged out see the definition of pdp in sect 2 6 however the examination of the ice curves clearly highlights that the other input parameters influence whether the partial log odds can become positive i e for which marine inundation becomes more likely we now aim at better characterising how the combined contribution of the three other important parameters influences the ice curves that are related to the high tide level t namely the phase difference t 0 the surge peak s and to a lesser extent the time duration of the storm surge decrease t this is done by selecting the set of scenarios that lead to an ice curve that is above the pdp i e those that enhance the inundation likelihood relative to the average partial effect of t increase of the partial log odds fig 15 depicts the density probability distributions of the 1 306 selected scenarios out of 2 500 for the four assets each density distribution is based on a kernel fitting of the histogram using a gaussian type kernel function with automatic selection of the bandwidth using silverman s rule of thumb silverman 1986 page 48 eq 3 31 fig 15 middle indicates the quasi symmetric role of t 0 for assets 1 3 with a mode of approximately 0 and a large proportion of selected scenarios falling in the interval 3 to 3 hours however this tendency is not observed for asset 4 for which the distribution s mode is approximately 2 hours the physical processes that underlie this phenomenon should be better analysed in future work fig 15 left shows that s should preferably reach very high values with a mode of 2 25 m which is above the largest value of the observed surge see fig 6b however moderate values that are of the order of past events such as xynthia s 1 5 m should not be excluded the dispersion of the density distribution for low s values remains large finally the density distribution in fig 15 right shows where the mode is located for large values of t which indicates that flooding is more likely for storm surges of long time duration at assets 3 and 4 4 4 2 role of the breaching scenarios the influence of the breaching scenarios location b and erosion rate r on the inundation likelihood is analysed at asset 4 we first compute the ice plots for the considered set of 2 500 new input scenarios fig 16a shows the corresponding ice plots together with the pdp note that to facilitate visualisation all the plots were scaled so that they reached zero for breaching location b 10 centred option of the icebox package fig 16 a outlines an abrupt shift in the partial log odds for breaching location b 6 i e for the breaching location that is closest to the asset location on the same coastline see fig 4 for b 6 most partial log odds appear to be less than zero which means that flooding is less likely relative to the response at b 10 for b 6 the pdp is greater than zero which means that flooding is more likely however according to the dispersion of the individual ice curves some input scenarios tend to influence the departure from this average tendency i e the combinations of the other input parameters can lead to large changes in the partial log odds we now focus on the role of b together with the other parameters t s t 0 r and select only the ice curves for which the maximum partial log odds is positive i e flooding is the most likely event a total of 446 scenarios were filtered out of the initial set of 2 500 randomly generated scenarios the corresponding density distributions are reported in fig 16b f several observations are made the selected scenarios are associated with large values of t with a mode of approximately 3 0 m fig 16b and relatively moderate dispersion around it a large range of possible values of s could contribute to the inundation likelihood as shown by the dispersion around the mode of 2 25 m fig 16c the distribution of t 0 is approximately symmetric for most of the cases in the range 3 to 3 hours fig 16d the density distribution of r is less straightforward to interpret although the inundation seems to be mostly induced by a high topographic erosion rate r close to 1 according to fig 16e the density distribution of the breaching location fig 16f indicates that two breaching locations may favour inundation namely b 2 southernmost part of the site fig 4b and b 10 the northernmost part of the site interestingly this suggests that the most influential breaching scenario is not necessarily the one that is located the closest to the point of interest the presence of both modes is not distinct the shape of the distribution in fig 16f is quasi flat which suggests that other locations may also be important and may indicate that the distance of the asset to the breach is not the main driver and other factors play a major role dune shape crest and foot topographic level and control of the local topography in the vicinity of the breach on the water flow among others more advanced modelling studies are necessary to obtain deeper insights into those physical processes see the discussion in sect 5 2 5 discussion in this section we discuss the added value sect 5 1 in terms of research questions q1 q3 which were described in the introduction and the limitations sect 5 2 of the rf based approach for marine flooding assessment and prevention 5 1 added value three research questions were outlined in the introduction q1 q3 the first one concerns the validity of using the rf model in place of the true computationally expensive numerical simulator regarding this issue the analysis that was conducted in sect 4 using an independent set of simulations shows the very satisfactory performance level of the constructed classification rf model with an accuracy and an area under the roc curve that are larger than 90 and 95 respectively for assets 1 and 2 and larger than 85 and 90 respectively for assets 3 and 4 this provides good confidence in the quality of the rf model for the bouchôleurs case furthermore the comparison of procedures for the rf set up iterative or random see figs 10 and 12 provides satisfactory indications that the method can efficiently select the simulation scenarios research question q2 for constructing the rf model therefore the rf based analysis provides a costless to evaluate estimator of the possible inundation at the considered assets without resorting to computationally expensive numerical simulations here of cpu time of 30 60 minutes this can be a key ingredient in facilitating the implementation of probabilistic coastal flooding hazard and risk assessment which often requires a very large number of forcing conditions that are randomly generated from multivariate extreme value models for instance 314 000 stochastic realisations which were representative of 10 000 years were used by rueda et al 2016 the rf based rapid evaluation can also be a key element in the implementation of an early warning system as discussed by idier et al 2013 for this purpose the level of error although satisfactory for our exploration analysis sect 4 3 and 4 4 should be even smaller in particular this can be achieved by further optimising the false alarm rate versus the true alarm rate for instance by using alternative performance metrics such as the area under the roc curve for the rf construction as shown by janitza et al 2013 considering the third highlighted research question q3 sect 4 2 4 3 show how the rf based analysis enables improvement in the understanding of the local conditions that lead to marine flooding for the key assets of the bouchôleurs site the identification of the important variables and the exploration of the dependencies enables us to do the following 1 confirm the physical interpretation of the effect of the tide surge interplay on the overflow inundation it shows that the surge peak that is in quasi phase with the high tide with an absolute phase difference of no larger than 3 hours plays a tremendous role in flooding as exemplified by the recent xynthia event 2 show the moderate influence of the surge s time evolution t and t 3 outline the complex relationship between the breaching parameters and the likelihood of inundation in particular it shows that the most influential breaching location is not necessarily the one that is the closest to the point of interest indeed the spatial variability of the dune structure crest and the topography which controls the flow propagation inland may play major roles 5 2 limitations one pillar of any data driven approach such as the rf model in our case is the dependency of the prediction quality on the availability of a validated numerical model in other words rf predictions do not reflect the uncertainty in the modelling assumptions in the bouchôleurs case this corresponds more specifically to the assumptions on the physical processes and to the manner in which the offshore and breaching conditions are parameterised as explained in sect 3 flooding is assumed to be mainly driven by overflow processes hence wave induced overtopping effects are neglected in the bouchôleurs case this assumption is justified because the waves break far offshore bertin et al 2015 in our modelling strategy both the tide and the total storm surge are taken into account thus the main effect of waves on this site i e wave set up is taken into account through the input conditions and shallow water models can be used to reproduce the overflow induced flood however the overtopping contribution even if negligible in the study area is not strictly equal to zero with a high water level the waves even if strongly damped can still lead to small overtopping this was the case in the north of the study site during the xynthia event thus the presented modelling experiment accounts for the main processes in our case but uncertainties could be reduced by including this minor wave contribution since the rf based procedure does not constrain the choice of the type of simulator future work can concentrate on more advanced modelling procedures such as the one that was used by le roy et al 2015 to simulate overtopping induced flooding regarding the model set up the heights of the main structures that control the flood are properly taken into account in the digital terrain model of grid size of 25 25 m2 and non uniform friction coefficients are used to account for the land cover types see fig 7 however this approach does not enable us to describe individual buildings although the efficiency of this approach has been proven see e g mugica et al 2016 on the arcachon basin in france the presence of buildings may have a significant influence on the local water height differences of tens of centimetres are often observed on each side of a flooded building a higher resolution grid could be used this would improve the local water level prediction but not significantly modify the overall flood prediction e g flood extent finally the last specific characteristic of the model set up on the bouchôleurs sites is the hydraulic connections the way in which these connections are taken into account provides realistic results see sect 3 4 and studies in other contexts such as mugica et al 2016 and can be considered state of the art for shallow water models such as the telemac simulator which is described in edf drd 2010 the offshore forcing conditions are parameterised using an ideal tide signal and a triangular surge signal although this assumption remains a satisfactory first order approximation as adopted by other authors see e g poelhekke et al 2016 other storm surge models may be adopted such as the one that was proposed by wahl et al 2011 which better reflects the complexity of the sea level signal but requires more parameters a total of 25 in this situation the rf based variable importance analysis would play an important role in identifying the most influential input parameters regarding the breaching processes a limited number of scenarios are predefined these predefined scenarios are based on observations after the xynthia event 6 breaches in the coastal dune see e g muller et al 2016 plus the assumed locations of 4 breaches of hard coastal defences the width and erosion time scale are assumed based on the type of coastal defence coastal defence or dune thus the failure dynamics is simplified while these assumptions on the location and temporal dynamics are justified when exploring the main purpose of the present study analysis of the rf applicability some areas of improvement should be considered for defence failure modelling for hard defences and morphodynamic modelling for dunes according to the literature a possible modelling approach is to rely on analytical formulas for predicting the likelihood of hard coastal defence failure however these formulas do not provide any information on the spatial and temporal dynamics of the failure see e g eurotop 2016 thus these formulas are not sufficient for estimating the length height and width of a failure or their temporal evolution thus advanced geotechnical models should be used in terms of dune breaches morphodynamic models such as the x beach simulator mc call et al 2010 are promising for reproducing the dune breaches that are induced by overflow on such areas muller et al 2016 however they are very time consuming the issue of accounting for more realistic spatial and temporal characteristics of the breaches is challenging and outside the scope of this paper 5 3 summary and future work in the present study we took advantage of a suite of recently developed r packages namely ranger boruta and icebox to implement rf models and exploit them to detect and explore the critical offshore forcing and breaching conditions which might lead to marine inundation the key features of the rf based analysis are as follows 1 the use of only a limited number of computationally expensive numerical simulations 200 in our application case 2 the low computation time cost 3 the capability of tackling mixed types of input parameters continuous such as high tide level and categorical such as the scenarios of breaching position 4 the capability of tackling many potentially correlated input parameters by means of variable importance analysis the application to a real case bouchôleurs west of france shows that the rf based approach can be useful for marine flooding assessment and prevention however the manner in which we set up the rf model only partly fulfils practical needs which leaves room for improvement first the quality of the prediction of the rf model strongly depends on the validity of the assumptions that underlie the numerical model a broader range of flooding situations should be investigated in the future we studied the case of overflow in the bouchôleurs case in particular for overtopping flooding cases as modelled by le roy et al 2015 at gâvres france and for hydro morphological processes during breaching as modelled by muller et al 2016 second the rf models were set up at spatial locations of interest this strategy may not be appropriate when one is interested in the inundation likelihood over the whole area i e at all spatial locations of the map in fig 4 which corresponds to thousands of pixels when considering only the urban areas the feasibility of multivariate extensions of the rf model as described by segal and xiao 2011 is worth investigating in the future finally for visualisation purposes the use of individual conditional expectation plots iceplot proved to be useful for the exploration of classification rules although approaches are available in icebox package for handling interactions among the parameters they are limited in their ability to fully explore these complex relationships dimensional reduction by projection as proposed by welling 2015 and implemented in the r package forest floor may be effective in overcoming this problem acknowledgements the authors thank the brgm funded dev probsub project for contributions to the financial support of the present work tide gauge data were provided by the french hydrographic and oceanographic service shom through refmar website http refmar shom fr we thank ifremer french research institute for exploitation of the sea for sharing with us the mars code we are also very grateful to the three anonymous reviewers for their comments which led to improving the manuscript appendix acronyms acronym description acc accuracy performance criterion auc area under the roc curve err misclassification rate eq 2 ice plot individual conditional expectation plot lhs latin hypercube sampling oob out of bag p classification probability evaluated using rf pam partitioning around medoids pdp partial dependence plot rf random forest model roc receiver operator curve vi variable importance of rf eq 4 
26395,identifying the offshore forcing and breaching conditions that lead to marine inundation is of high importance for risk management this task cannot be conducted by using a numerical hydrodynamic model due to its high computation time cost of several minutes or even hours in the present study we show how the random forest rf classification technique can approximate the numerical model to explore these critical conditions we focus on the bouchôleurs site which is located on the french atlantic coast and exposed to overflow processes an iterative strategy is developed for selecting the numerical simulations a total of 200 to train the rf model the sensitivity to the input parameters is studied using permutation based importance measures and extended versions of the partial dependence plots the results highlight the key interplay among the high tide level the surge peak and the phase difference and the complex role of the breaching location keywords coastal flooding overflow random forest classification probability variable importance analysis partial dependence plot 1 introduction recent storm events such as katrina in 2005 and xynthia in 2010 illustrate the present day coastal damages and injuries that can affect coastal areas in both cyclonic and non cyclonic environments katrina was one of the most powerful hurricanes ever to be registered in the atlantic and led to more than 1 500 deaths and damages of approximately 80 billion usd blake 2007 whereas xynthia was a mid latitude storm that severely hit low lying coasts in the central part of the bay of biscay on 27 28 february 2010 and led to more than 40 deaths and one billion euros of material damages see e g vinet et al 2012 from a statistical point of view the wave heights that were generated during the xynthia event could not be considered extremes see e g bertin et al 2012 however what makes this event rare is the combination of a spring tide with a large storm surge enhanced by young wind waves that reached its maximum near the tide peak this illustrates the importance of gaining insights into and a better understanding of the combination of offshore forcing conditions water levels tide peak storm duration wave characteristics that lead to inundation or not a systematic exploration of these conditions is of primary importance for risk management based on multiple objectives as discussed by idier et al 2013 1 for crisis management purposes since this knowledge can be used as input for constraining forecast and early warning systems see an example of such systems in uk stansby et al 2013 2 for prevention purposes since this knowledge can be used to support the return period calculation in determining flooding risk see e g gouldby et al 2014 3 for enhancing risk culture by improving risk awareness and preparedness regarding various possible offshore scenarios in addition to offshore conditions coastal defence failures such as failures of artificial structures such as dikes or breaching of sand dune systems might also influence the inundation characteristics spatial extent inland water height flooding time of arrival etc as illustrated by the 1953 north sea 2005 katrina and 2010 xynthia flooding events therefore a deep understanding of the breaching characteristics spatial location along the coast width etc that increase the inundation likelihood is also desirable for clarity we use the generic term breaching to refer to the failure of an artificial coastal defence or the breach of natural systems such as dunes the task of detecting and analysing such critical conditions in the coastal risk domain offshore and related to coastal defence failure shares the same objectives as the analytic approach for scenario discovery which aims at coping with deep uncertainty e g groves and lempert 2007 kwakkel et al 2013 scenario discovery analysis relies on exploration and analysis of numerical simulations the objective is threefold exhaustively sampling the space that is spanned by the input parameters analysing the consequences via numerical simulations and identifying regions of interest in the input space however in the application of such a systematic search to the coastal domain multiple difficulties are encountered which are at the core of the present study first coastal flooding assessment is typically supported by numerical simulations which might have very large computational time cost typically of several minutes to hours per simulation depending on the process complexity that is taken into account by the model such a computational burden often restricts the analysis to a limited number of input configurations termed as scenarios in the following even when computing architectures are used meta modelling approaches aka surrogate modelling see e g castelletti et al 2012 razavi et al 2012 can efficiently handle the difficulty that is related to computational time cost by replacing the computationally expensive numerical code by a costless to evaluate statistical emulator approximation which is built using a limited number of computer experiments typically a few hundreds the meta model based strategy proved to be very efficient when combined with adaptive sampling strategies a reduction by a factor 20 40 of the total number of necessary computationally expensive simulations was achieved in the case that is described by rohmer and idier 2012 a second difficulty is related to the nature of the coastal processes which are usually controlled by thresholds as an illustration let us consider marine flooding that is induced by overflow processes if the water level at the coast which results from storm surge and tide characteristics is lower than a specified threshold flooding cannot occur the water height at any given location inland remains zero otherwise provided that the water level slightly increases and exceeds a specified threshold overflow induced inundation can occur and inland locations may be flooded in other words the water height inland remains zero until a specific combination of conditions surge amplitude tide amplitude phase difference etc that lead to a flood at this location are met in these situations the functional mathematical relationship between the offshore conditions and the response is expected to be highly non linear and standard meta modelling techniques might fail gaussian process polynomial chaos expansion etc see e g jin et al 2001 among the available possible options see e g razavi et al 2012 an alternative procedure is to use a classification strategy to identify the conditions that lead to inundation class 1 and those that do not class 0 as proposed for instance by bourinet et al 2011 in the field of structural safety and tested by rohmer et al 2016 in the field of marine flooding finally a third difficulty is related to the number of offshore conditions that depend on the considered physical parameters tide atmospheric storm surge wave height wave direction wave period etc this complicates the detection of the critical conditions the larger the number of such conditions the broader the input space and the more tedious the exploration this also makes the visualisation of the potentially complex classification rules difficult variable importance analysis see e g galelli et al 2014 can overcome this issue by carefully selecting the most important parameters regarding the occurrence of the considered event different statistical techniques have been proposed in the literature for dealing with these three difficulties see e g kuhn and johnson 2013 in the present study we utilize a meta modelling strategy that is based on the random forest denoted rf technique breiman 2001 to approximate computationally expensive numerical simulators first difficulty using a limited number of pre calculated simulations typically 100 200 many studies have shown the high performance of rf in different domains of application ecology cutler et al 2007 water quality brooks et al 2016 wildfires rodrigues and de la riva 2014 etc to overcome the second difficulty the rf model is used as a classifier to predict whether a set of forcing and breaching conditions will lead to inundation class 1 or not class 0 at a given spatial location of the considered study site regardless of the type of input parameters i e continuous or categorical with possible correlation among them the third difficulty is handled by means of the variable importance measures that are provided by the rf model wei et al 2015 the objective of the present study is to address the following questions q1 to what extent is the rf model a valid approximation of the true computationally expensive numerical simulator q2 how can the scenarios defined by values of the input parameters that should be simulated to construct the rf model be efficiently selected q3 how can the rf model help to discover and cast light on the critical conditions forcing and breaching that lead to marine inundation for this purpose we take advantage of recent advances that are related to the implementation of this technique in the r software r developement core team r 2017 1 the numerically efficient version of the rf model that was developed by wright and ziegler 2016 implementation in the r package ranger 2 the estimate of the classification probability that was proposed by malley et al 2012 implementation in the r package ranger 3 the feature selection algorithm that was developed by kursa and rudnicki 2010 implementation in the r package boruta and 4 the visualisation tools that were proposed by goldstein et al 2015 for exploring the dependencies using the individual condition expectation curves implementation in the r package icebox the paper is organized as follows first we describe the overall strategy and the associated methods for exploring the offshore forcing and breaching conditions that lead to marine inundation sect 2 second we describe the application case at the bouchôleurs site which is located along the french atlantic coast sect 3 third we apply the proposed strategy sect 4 and address research questions q1 q3 by discussing the added value and the limitations of the rf based analysis from a risk analysis perspective sect 5 the list of acronyms is provided in appendix a 2 methods in this section we first describe the overall strategy that is applied in the present study to explore and analyse the scenarios that lead to inundation sect 2 1 then we describe the key ingredients of the strategy sect 2 2 2 6 2 1 strategy description an overview of the strategy is provided in fig 1 a preliminary and necessary step aims at setting up and validating a hydrodynamic model for simulating the flooding processes for the site of interest on this basis the steps are as follows step 1 a limited number n 0 of scenarios are first selected in the domain of forcing and breaching conditions x these scenarios are used as inputs of the numerical modelling chain described in sect 3 3 using the techniques that are described in sect 2 3 1 step 2 the time evolution of the inland water height denoted h is computed over the considered study area for each of the considered input conditions a class indicator y is defined and labelled as 1 if the maximum value of h over time exceeds zero at the considered spatial location and 0 otherwise set d x i y i i 1 n0 constitutes the initial training dataset based on which the classification rules are learned using the random forest rf classification approach see sect 2 2 step 3 to increase the accuracy of the rf classification model an iterative procedure is proposed for selecting in an adaptive manner the input scenarios that should be run this is based on the evaluation of the classification probability p see sect 2 3 2 step 4 once the number n of affordable numerical simulations has been reached typically 100 200 the predictive capability of the rf model should be assessed see sect 2 4 step 5 once the performance of the rf model has been validated the analysis of the classification rules that lead to marine inundation can be performed this is done by following a two step approach 1 study of the importance influence of each input parameter using variable importance analysis see sect 2 5 2 exploration of the classification rules as functions of the key most important input parameters see sect 2 6 2 2 random forest based classification based on the set of n training data d x i y i i 1 n the objective of setting up a classification model classifier is to identify the rules for assigning a new yet unseen input scenario into class 1 or 0 without resorting to a computationally expensive numerical simulation a basic classification model can rely on the decision tree technique breiman et al 1984 a decision tree is recursively built using binary partitioning of the domain space into regions that are increasingly homogeneous with respect to the class variable as an illustration fig 2 schematically depicts a tree model for binary classification class label 0 or 1 the homogeneous regions correspond to the nodes the splitting process aims at subdividing one parent node into two sub nodes which should be as homogenous as possible node homogeneity is measured by the gini impurity index which is denoted as g breiman et al 1984 and defined as follows 1 g 1 p 1 2 p 0 2 where p k n k n is the fraction of the n k samples from the class with k 0 1 the decrease of g is calculated as g parent g split1 g split2 after splitting the parent node into two sub nodes using a cut off value c see fig 2 at each step of the construction the algorithm selects the value of c that results in the largest decrease of g this process is performed until sub division no longer decreases the gini index or a minimum node size is reached typically a size of 1 for classification random forest builds on the same principles as decision tree classification models but extends them by adding a random character to the construction process at two levels 1 each tree is constructed using a different bootstrap sample of the data breiman 2001 which enables the instability issue of classical decision tree i e the tree structure and the resulting predicted classes might not be stable when new samples are provided to be overcome 2 each node is split using the best among a subset of input parameters that are randomly chosen at that node which enables situations of highly correlated input parameters to be considered cutler et al 2007 fig 3 schematically depicts the main steps of the rf algorithm which are as follows step 1 draw n tree bootstrap samples d j e g using random sampling with replacement from the original training data d x i y i i 1 n step 2 for each of the bootstrap samples d j grow a classification tree by splitting until all the observations in each terminal node come from the same class using the gini index that is defined in eq 1 at each node rather than choosing the best split among all predictors input parameters choose it among m try randomly selected predictors for each tree j the classification error is measured by the misclassification rate which is denoted err j and defined as follows 2 e r r j 1 d j c i d j c i y i y ˆ i where d j c is the set of samples that are not used for the construction of the considered classification decision tree and constitutes the set of out of bag oob samples such that d j c d j d d j c is the size of the jth oob set y i is the ith class indicator within the jth oob set and y ˆ i is the corresponding class prediction using the jth classification tree step 3 predict new data by aggregating the predictions of the n tree trees the class of the yet unseen input parameters is estimated as the class with the maximum number of occurrences votes among the different trees a class probability probability of belonging to class 1 or 0 can be estimated by either relying on a voting based approach by counting the proportion of the considered class that is estimated using the n tree trees or using a regression based rf model as described by malley et al 2012 2 3 selection of the training data multiple strategies for selecting the training dataset will be used to learn the classification rules we propose selecting the scenarios of forcing and breaching parameters that provide the most information regarding the accuracy defined as 1 err of the rf model to do so a two step approach is proposed see fig 1 1 select an initial set of scenarios typically 25 50 to set up an initial rf model sect 2 3 1 2 sequentially select new input scenarios which should a priori increase the classification accuracy and build at each step a new rf model sect 2 3 2 the sequential procedure is conducted until the maximum number of affordable computationally expensive simulations typically 100 200 has been reached 2 3 1 step 1 initial selection the initial step can rely on the same principles as described by camus et al 2011 first many say 500 input parameters that describe the scenarios are generated using for instance a quasi random sequence such as the sobol sequence sobol 1976 to ensure an optimal exploration of the input space then the objective is to select the elements within this first dataset that are the most dissimilar the notion of dissimilarity is based on the notion of distance which should be carefully chosen when mixed types of variables are present as in our case i e we deal with both classical continuous variables such as the value of a surge peak the phase difference between tide and surge temporal signals etc as defined in sect 3 2 and categorical discrete variables related to the breaching location scenario in our case see further details in sect 3 a possible option is the gower distance gower 1971 which is defined as follows consider x i which is the ith vector of p input parameters and x i k which is the kth input parameter the gower distance d i j between two input vectors x i and x j is 3 d i j k 1 p d i j k p where d i j k is a dissimilarity measure that is adapted to the nature of the kth input parameter for continuous variables d i j k x i k x j k r k where r k is the range of values of the kth parameter for categorical variables d i j k 1 if x i x j and 0 otherwise once the dissimilarity matrix whose elements are pairwise dissimilarities distances between observations in the dataset has been estimated a clustering technique see e g kaufman and rousseeuw 1990 can be applied to select a limited number of scenarios a possible option is the k medoids also named partitioning around medoids pam algorithm which was developed by kaufman and rousseeuw 1990 chapter 2 the pam algorithm is based on the search for k representative objects named medoids among the observations within the dataset a medoid is an object within a cluster for which the average dissimilarity between it and all the other members of the cluster is minimal geometrically it corresponds to the most centrally located point in the cluster after finding a set of k medoids k clusters are constructed by assigning each observation to the nearest medoid the pam algorithm is less sensitive to noise and outliers compared to the popular k means algorithm because it uses medoids as cluster centres instead of means which are used in k means once the initial selection has been performed at the centroids of the clusters e g of the order of a few tens of input scenarios the water height can be computed using the numerical model over the area of interest step 1 in fig 1 2 3 2 step 2 adaptive selection based on the initial set of numerical model results a first rf model can be set up and used to estimate the inundation probability denoted as p for any new scenario configuration x i e the probability that y belongs to class 1 given x with the regression based procedure that is described by malley et al 2012 when p is close to 1 0 the classification of y is provided by the rf model with high confidence when p is close to 0 5 the classification of y is provided by the rf model with less confidence the next step is to select an additional input scenario where the uncertainty of the classification is the largest i e where the rf classification is the least confident with p close to 0 5 this is equivalent to selecting the next scenario in which p 1 p reaches a maximum value which proved to be a more numerically stable criterion in practice candidates for the input scenarios are randomly generated e g using quasi random sequences sobol 1976 for each of these candidates p 1 p is evaluated using the rf model and the candidate for which this criterion is maximum is selected as the new input configuration of the numerical model once a new input scenario has been selected and the corresponding inland water height has been computed a new rf model can be set up and the same procedure can be performed until the maximum number of affordable simulations in terms of computation time cost is reached typically 100 200 2 4 validation of the rf prediction at the end of the iterative selection procedure the predictive capability of the rf model should be assessed i e the capability of the rf model to correctly predict the class of new yet unseen input configurations this can be done using either a set of new simulations or cross validation procedures hastie et al 2009 two performance criteria are used the first is the accuracy denoted by acc and defined as the ratio of the number of correctly classified events by the rf model to the total number of events i e the capability of the rf model to predict the true class given the values of the forcing conditions the second performance criterion aims at measuring the extent to which the rf model can distinguish between the classes i e the ability of the rf model to rank the events with y 1 i e sea water reaches the considered spatial location relative to those with y 0 i e sea water does not reach the considered spatial location this criterion is related to the capability of minimising the number of false alarms which correspond to events that belong to class 0 that are classified as belonging to class 1 this can be assessed using the receiver operating characteristic roc curve metz 1978 which relates the true positive rate tpr defined as the ratio of the number of events that are predicted by rf as having y 1 that actually have y 1 to the total number of positive events which have y 1 and the false positive rate fpr defined as the ratio of the number of events that are predicted by the rf model as having y 1 but which actually have y 0 to the total number of negative events which have y 0 at various probability threshold settings the closer the curve is to the left hand corner of the roc space the better the classification the area between the first bisector and the roc curve denoted as auc allows the performance of the rf model to be quantified 2 5 variable importance and selection once validated the obtained rf model can be used to assess the influences of the forcing and breaching parameters on the model output i e the variable importances in the rf approach the importance of a variable input parameter is defined as its impact on the predictive capability of the rf model which is measured by the misclassification rate err eq 2 different approaches for defining a variable importance measure have been proposed in the literature e g wei et al 2015 we focus on the permutation variable importance measure which aims at comparing the misclassification rate before and after randomly permuting the values of a specific input variable as follows 4 v i e r r j i e r r j where e r r j i is the misclassification rate of the jth tree that is constructed by permuting the values of the ith input variable equation 4 can be used to define a global importance measure for each input variable by averaging over all trees i e by computing the mean of the n tree vi values similarly a standard deviation value can also be computed although this importance measure is useful for ranking the variables in terms of influence no straightforward cut off value exists for identifying variables of negligible importance a possible approach is to rely on the boruta algorithm which was developed by kursa and rudnicki 2010 it is based on the z score which is defined as the ratio of the mean to the standard deviation of the n tree vi values the main strategy is to define d new input variables which are termed shadow values by copying the existing ones and randomly shuffling their values for each new random permutation of the shadow values z scores are computed for each of the d original input variable plus the shadow ones by construction the shadow variables have no influence however their z score although low is non zero due to the random shuffling process the statistics of these shadow z scores can then be used as reference cut off values to decide whether a given input variable is important or not 2 6 visualisation of the classification rules partial dependence plots pdps were introduced by friedman 2001 to facilitate the diagnosis of the functional relationship between the input variable of interest and the predicted response which is denoted as f ˆ for any machine learning technique in the classification case f ˆ can be chosen as the logarithm of the odds log odds which is defined as the ratio of the probability of belonging to class 1 inundation to that of belonging to class 0 absence of inundation a positive log odds indicates that the inundation event is the most likely the probability of class 1 is superior to that of class 0 consider n evaluations y ˆ i f ˆ x i given n input vectors of p input parameters x i x i 1 x i 2 x i p i 1 n in the univariate case the pdp for the jth input parameter corresponds to the plot of the following average function ϕ x 5 ϕ x 1 n i 1 n f ˆ x i 1 x i 2 x i j 1 x x i j 1 x i p 1 n i 1 n f ˆ x i c x where x i c is the set x i 1 x i 2 x i j 1 x i j 1 x i p the pdp provides insights into the influence of the jth input parameter over the range of x values on the set of model predictions y ˆ i i 1 n after the impact of all other input parameters has been averaged out goldstein et al 2015 proposed extending this visualisation approach by plotting not only the average partial effect but also all the f ˆ x i c x curves against the x values for each fixed value of x c these plots are called the individual conditional expectation plots ice plots the main advantage of this procedure is that it provides a broader insight into the variants of the conditional relationships instead of focusing only on the average effect 3 case study in this section we describe the setting of the study site and the main physical processes that are related to marine flooding sect 3 1 the offshore forcing and breaching conditions which are denoted as x sect 3 2 and 3 3 and the numerical model that is used to simulate marine flooding sect 3 4 3 1 site description assets and main processes the case study focuses on the bouchôleurs district and the natural reserve of yves marshland they are located on the atlantic coast of france see fig 4 a near the city of la rochelle in a low lying natural coastal marsh and dune system in front of the strait pertuis d antioche between ré island and oléron island fig 4b bouchôleurs is a district of approximately 700 inhabitants of châtelaillon city the tide is macro tidal with a maximum tidal range of 6 68 m at aix island french hydrographic and oceanographic service shom 2014a more details on the site can be found in andré 2013 muller et al 2016 the main offshore drivers of storm induced coastal floods are the tide storm surges and waves the study site is located inside the pertuis charentais fig 4a therefore it is protected from the direct action of waves however it is affected by the indirect action of waves through two processes regional wave setup bertin et al 2015 and sea surface roughness young waves lead to an increase of atmospheric storm surge through the increase of sea roughness bertin et al 2012 thus the main hydrodynamic drivers to be taken into account are the tide and the storm surge which results from the contribution of the regional wave set up and the atmospheric storm surge while the direct effects of waves can be neglected at the bouchôleurs site this also implies that overflow is the main flood process at this site and that a shallow water model can be used to reproduce the flood see sect 3 4 this site was hit by the storm xynthia 27 28 february 2010 which was characterised by a high storm surge 1 5 m at la rochelle tide gauge that was in phase with a high spring tide it led to the inundation of several parts of low lying areas thereby causing large economic losses and losses of life see e g pedreros et al 2010 bertin et al 2012 flooding at the study site was mainly induced by overflow processes which were exacerbated by the damages to the coastal dune system erosion and breaching over the yves marsh dune see e g muller et al 2016 overflow starts on the study site when the total water level at the coast is higher than the minimum topographic level along the coast which reaches 3 2 m ign69 general levelling of france in the absence of breaching however exceeding the topographic level of 3 2 m ign69 at the coast does not necessarily imply that the sea water will propagate to any specific inland location other parameters such as topography and land cover and the associated friction play key roles in the flood propagation inland thus the water height h at a given spatial location remains zero until specific combinations of conditions surge amplitude tide amplitude phase difference between surge peak and high tide storm duration etc are met for the water to propagate inland the purpose of rf based analysis is to explore them this analysis is performed at four specific points of the studied area these spatial locations correspond to four different assets see fig 4b which are selected because they are either particularly vulnerable to flooding or key elements in the case of evacuation asset a1 the camping asset a2 a potential key intersection road crossing for evacuation asset a3 the holiday camp asset a4 the north part of the yves marshland 3 2 setting up the forcing scenarios the physical forcing conditions are time series of tide and surge in this paper the time series are described by assumed shapes using the scalar parameters that are shown in fig 5 see also table 1 the tide is simplified by assuming a sinusoidal signal that is parameterised by its high tide level t see fig 5 top the surge signal is assumed to be described by a triangular model as schematically described in fig 5 bottom using four parameters namely the peak amplitude s the phase difference t 0 between the surge peak and the high tide the time duration of the rising part t and the time duration of the falling part t the ranges of variation for the parameters table 1 are selected based on the following assumptions the range of variation for t fig 6 a is defined by the highest and lowest predicted high tide levels that are reported at the aix island tide gauge the closest gauge to the study site relative to vertical datum ign69 the highest tide value is provided in shom 2014a the lowest tide value is obtained by extracting the smallest value of the high tide that is predicted with the software shomar 2014 shom 2013 over a time span that is larger than the 18 6 year lunar nodal cycle the range of variation for s is based on the analysis of tide gauge measurements at la rochelle la pallice this tide gauge is located further from the site compared to the aix island gauge fig 4a but provides a much longer time series decades than that of the aix island years shom 2014b with differences of only a few centimetres in common time spans the surge time series is derived from the difference between the hourly total water level records and the predicted tide time series estimated by harmonic analysis using the utide software that was developed by codiga 2011 the lower bound of s is determined as the threshold above which the surge data appear to be well modelled by a generalised pareto distribution within the setting of extreme value analysis see e g bulteau et al 2015 this is done using a combination of the mean residual life plot and plots of parameters that are estimated using a range of thresholds threshold stability plots see coles 2001 the upper bound of s is defined as the largest observed value 1 5 m xynthia event plus a margin of 1 0 m to cover a very large range of situations i e broader than that defined by the observations this value is considered to represent extreme situations fig 6b but is plausible and associated with a return period value of about 500 years calculated using a generalised pareto distribution and maximum likelihood estimator see e g bulteau et al 2015 for the details on the procedure the range of variation for t 0 is determined based on the analysis of the surge time series fig 6c the range of variation for t and t is constrained by the tide period 3 3 setting up the breaching scenarios in the domain of coastal flood modelling a breach is usually accounted for using the following characteristics its spatial location its width and length its topographic level after failure the failure duration in our study ten locations of breaches are defined locations b1b6 fig 4b are assumed to be related to the breaching of the dune systems they are selected based on historical observations during the xynthia event see muller et al 2016 and references therein in addition four hard coastal defence breaches are considered at locations b7b10 fig 4b the scarcity of local historical data on failures of hard coastal defences dikes makes it hard to rely on past observed breaches in parameterising them thus these coastal defence breaches were assumed and selected to be close to possible vulnerable assets a1 a3 in terms of the spatial dimensions of the breaches the length is assumed constant 25 m while the width is not explicitly described see the description of breach modelling in sect 3 4 the topographic level after failure is parameterised as a fraction r erosion rate of the initial crest level relative to the coast defence foot for instance r 50 accounts for a topographic level of 50 of the initial crest level the failure duration i e the duration between the breach initiation and the end of the event is assumed to be 120 and 15 min for dune systems and coastal defences respectively the current analysis is focused on the impacts of the breaching location and the topographic erosion rate r table 1 3 4 setting up the numerical model in the present study we use the numerical simulator named model for applications at regional scale 1 1 available at http wwz ifremer fr mars3d le modele faq acces au code mars lazure and dumas 2008 this finite difference model solves the shallow water equations and was originally designed to compute regional coastal hydrodynamics tide and storm induced water level and currents to account for the specificities of local coastal flooding processes the functionalities of the mars were extended this extension is called mars flood first a new functionality f1 for representing the hydraulic processes around connections such as culverts and weirs was added this has been done by using the capability of the mars to impose source and sink terms on the domain which are computed as follows 1 the inflow and outflow discharge are deduced from the continuity and energy equations between the upstream section and the downstream section of the considered connection 2 the type of flow depends on whether the hydraulic connection fully flows or not and whether the flow is controlled by the inlet or outlet the second new functionality f2 deals with the inclusion of the breaching effect in the model the breaching processes are modelled as a weir with a water surface moving down linearly from dyke crest to the bottom the breach location is represented by two mesh elements across its width when the flow goes from the coast to the inland area one source term is implemented on the side of the coast and at the same time one sink term is imposed on the side of the land with a symmetric value of that discharge finally when using this model for coastal flood modelling the parameters that are related to the wet dry interface dynamics must be carefully chosen and the locations of topographic features road wall dike dune etc must be very precise as these features have a significant impact on flood propagation a grid model with a spatial resolution of 25 25 m2 and a total of 39 188 mesh cells is used the digital elevation model dem is based on bathymetric and topographic surveys including lidar as described by muller et al 2016 and accounts for the topographic features road wall dike railway dune etc which influence the flood propagation the land cover effect on the flood is taken into account by using a spatially varying friction coefficient in the present study the classification of the 2006 corine land cover data is used to set up a spatial grid of manning friction coefficients based on the study of bunya et al 2010 see fig 7 the hydraulic connections e g the hydraulic culverts below the roads dike railway etc see fig 4b are also identified and taken into account in the model the model was validated by comparing the numerical model results with the observations of water heights and flood extension sogreah 2011 during the xynthia event the offshore boundary conditions for the xynthia event are obtained from a dynamical downscaled model of wave tide and storm surge which provides the total water level resulting from tide atmospheric storm surge and regional wave set up the modelling of the offshore conditions is described in paris and pedreros 2012 the six breaches that were observed during the xynthia event are also taken into account the water heights that are computed with the numerical model are compared with the 75 local observations of water marks and a mean error a root mean square error and a correlation coefficient of 13 cm 25 cm and 86 respectively are obtained given the values of the input parameters offshore and breaching conditions a single model run takes approximately 30 60 min of computation using a single computer unit cpu over a time duration of four tidal cycles as an illustration fig 8 provides the spatial extent of the inundation i e where the maximum value of the inland water height over time is non zero under different forcing and breaching scenarios fig 8a illustrates the extent to which the flooded area can drastically change when the surge tide levels are modified by only a few tens of centimetres fig 8b illustrates that the location of the breach can have a strong impact on the spatial extent of the flooding hence it can influence whether a given asset can be hit or not by the flooding in particular assets 3 and 4 4 application in this section we apply the strategy that is described in sect 2 1 to explore and identify the forcing and breaching scenarios that lead to overflow induced inundation at each of the four assets of the bouchôleurs site fig 4 first we set up an rf model at the four locations using the adaptive sampling strategy sect 4 1 we assess the validity of the assumption of replacing the true numerical simulator by the rf model in sect 4 2 research questions q1 and q2 to answer research question q3 we use the rf model in a post treatment manner to identify the most important input parameters for describing the forcing and breaching scenarios sect 4 3 finally we explore the key dependencies between the likelihood of the overflow induced inundation and the input parameters sect 4 4 4 1 iterative selection an rf classification model is set up for each of the four locations of interest fig 4 each output of the numerical simulations is classified as 1 if the maximum value of h exceeds zero at the considered asset and 0 otherwise an initial set of 50 scenarios is selected using the dissimilarity clustering approach which is described in sect 2 3 1 and implemented with the pam algorithm preliminary tests not shown were carried out to estimate the optimal number of trees n tree and value of parameter m try that lead to minimum oob misclassification rates on this basis an initial rf model is constructed using the optimal values of n tree 1 000 and m try 5 and 150 new scenarios are iteratively selected by using the probabilistic information estimated using the regression based procedure of malley et al 2012 as described in sect 2 3 2 a total of 200 numerical simulations are performed fig 9 provides the scatter and bivariate density plots of the selected scenarios for asset 4 considering the offshore conditions this shows that large values of t and s tend to be preferably selected while for t 0 values near 0 are preferably selected the tendencies of t and t are more difficult to analyse the variability of the results is large this suggests a lower sensitivity of the model to these parameters considering the breaching conditions locations b 1 and 10 which have large erosion rates seem to be preferably selected these tendencies are also identified for the other assets not shown and should be confirmed in sect 4 3 using the rf based variable importance analysis 4 2 performance analysis the predictive capability of the adaptively constructed rf model is assessed using a set of 500 new numerical simulations which are randomly selected using the latin hypercube sampling lhs strategy mckay et al 1979 by construction these new results are independent from the training dataset that is used for the rf construction two performance criteria are used fig 10 provides the accuracy denoted as acc evolution that is calculated using this validation test set as a function of the number of input scenarios the performance of the adaptive approach is compared to that using purely random selection of the input scenarios conducted 100 times the comparison with the mean and the 95 confidence envelope of the accuracy indicator confirms the highly satisfactory performance of the adaptively constructed rf model fig 10 shows that the adaptive strategy enables a higher level of accuracy to be reached compared to the random strategy acc reaches values that are above the upper bound of the random procedure the second performance criterion aims at measuring the extent to which the rf model can distinguish between the classes inundation and absence of inundation it is assessed by means of roc curves fig 11 depicts them in which the initial before adaptive construction and final after adaptive construction rf models are considered for each asset for all considered assets the performance appears to be improved the final curves are closer to the left hand corner finally the area denoted as auc between the first bisector and the roc curve is calculated using this validation test set as a function of the number of input scenarios fig 12 similar to acc the performance of the adaptive approach is compared to that of a purely random selection of the input scenarios conducted 100 times fig 12 shows that the adaptive strategy enables higher performance levels to be reached compared to the random strategy such that auc reaches values that are above the 95 confidence upper bound of the random procedure for assets 1 2 and 4 and above the mean curve for asset 3 the adaptive strategy is applied to 200 input scenarios for which acc 85 and auc 90 for the four assets 4 3 identification of the most important parameters in the setting of the study site the primary drivers of overflow induced inundation are known to be the high tide level t the surge peak s and the phase difference t 0 sect 3 1 however the quantification of the relative importance of each of these parameters together with those that describe the storm duration and breaching remains unclear using the adaptively constructed rf model the boruta algorithm described in sect 2 5 kursa and rudnicki 2010 can provide useful insights for identifying the important parameters fig 13 shows the z score values for the forcing and breaching conditions for each asset several observations are made as expected the high tide level t has a large influence and is identified as the most important parameter for all assets the second most important parameter is either the surge peak s or the phase difference t 0 this result can be understood from the perspective of past events the xynthia 2010 storm as recalled in the introduction exemplifies the consequences of the combination of high s and a spring tide that is in phase with the surge signal t 0 0 in contrast to the xynthia 2010 storm the lothar and martin 1999 2 2 http tempetes meteofrance fr lothar le 26 decembre 1999 html and quinten 2009 3 3 http tempetes meteofrance fr tempete quinten du 09 au 10 fevrier 2009 html storms had smaller impacts on the atlantic coasts since they were not in phase with high tide despite the moderate to large surge peaks up to 1 m for assets 1 and 2 see fig 13 top only three forcing parameters appear to influence the likelihood of inundation s t and t 0 for asset 3 surge temporal characteristics also play a role although of lower influence than s t and t 0 for asset 4 all forcing and breaching conditions appear to have an influence the storm surge parameter t duration of the storm surge amplitude increase appears to have a negligible effect for assets 1 3 the corresponding boxplots are lower than those of the shadow variables and a small but non negligible effect for asset 4 the breaching scenarios location b and erosion rate r only impact the fourth asset the importance of the location appears to be of the same importance than the storm surge amplitude whereas the erosion rate has a moderate influence and is very close to the reference values of the shadow parameters a more detailed analysis is provided in sect 4 4 2 4 4 exploration of the dependencies based on the variable importance analysis of sect 4 3 we explored in more detail two specific dependencies to do so we generated a new set of 2 500 lhs random input conditions based on which the individual conditional expectation ice plots were estimated 4 4 1 role of the high tide level first the role of the most important parameter namely t as identified in sect 4 3 is analysed fig 14 depicts the evolution of the ice curves for the four assets we observe clearly increasing behaviour of the partial dependence plot pdp in yellow for each asset but with threshold effects for moderate values of t less than 1 0 1 5 m depending on the considered asset all ice curves in grey remain negative thereby indicating that regardless of the values of the other input parameters inundation is impossible below this threshold which is specific to the considered asset location for assets 1 and 2 see the locations in fig 4 the pdp becomes positive inundation is the most likely event for large t values of 2 75 m depending on the considered asset and plateaus above this value the examination of ice curves highlights that combination with the other input parameters can increase the likelihood of inundation for values that are below t 2 75 m down to 1 5 m for assets 3 and 4 the pdp never becomes positive which means that the tide does not lead to inundation on average i e after the impacts of all other input variables have been averaged out see the definition of pdp in sect 2 6 however the examination of the ice curves clearly highlights that the other input parameters influence whether the partial log odds can become positive i e for which marine inundation becomes more likely we now aim at better characterising how the combined contribution of the three other important parameters influences the ice curves that are related to the high tide level t namely the phase difference t 0 the surge peak s and to a lesser extent the time duration of the storm surge decrease t this is done by selecting the set of scenarios that lead to an ice curve that is above the pdp i e those that enhance the inundation likelihood relative to the average partial effect of t increase of the partial log odds fig 15 depicts the density probability distributions of the 1 306 selected scenarios out of 2 500 for the four assets each density distribution is based on a kernel fitting of the histogram using a gaussian type kernel function with automatic selection of the bandwidth using silverman s rule of thumb silverman 1986 page 48 eq 3 31 fig 15 middle indicates the quasi symmetric role of t 0 for assets 1 3 with a mode of approximately 0 and a large proportion of selected scenarios falling in the interval 3 to 3 hours however this tendency is not observed for asset 4 for which the distribution s mode is approximately 2 hours the physical processes that underlie this phenomenon should be better analysed in future work fig 15 left shows that s should preferably reach very high values with a mode of 2 25 m which is above the largest value of the observed surge see fig 6b however moderate values that are of the order of past events such as xynthia s 1 5 m should not be excluded the dispersion of the density distribution for low s values remains large finally the density distribution in fig 15 right shows where the mode is located for large values of t which indicates that flooding is more likely for storm surges of long time duration at assets 3 and 4 4 4 2 role of the breaching scenarios the influence of the breaching scenarios location b and erosion rate r on the inundation likelihood is analysed at asset 4 we first compute the ice plots for the considered set of 2 500 new input scenarios fig 16a shows the corresponding ice plots together with the pdp note that to facilitate visualisation all the plots were scaled so that they reached zero for breaching location b 10 centred option of the icebox package fig 16 a outlines an abrupt shift in the partial log odds for breaching location b 6 i e for the breaching location that is closest to the asset location on the same coastline see fig 4 for b 6 most partial log odds appear to be less than zero which means that flooding is less likely relative to the response at b 10 for b 6 the pdp is greater than zero which means that flooding is more likely however according to the dispersion of the individual ice curves some input scenarios tend to influence the departure from this average tendency i e the combinations of the other input parameters can lead to large changes in the partial log odds we now focus on the role of b together with the other parameters t s t 0 r and select only the ice curves for which the maximum partial log odds is positive i e flooding is the most likely event a total of 446 scenarios were filtered out of the initial set of 2 500 randomly generated scenarios the corresponding density distributions are reported in fig 16b f several observations are made the selected scenarios are associated with large values of t with a mode of approximately 3 0 m fig 16b and relatively moderate dispersion around it a large range of possible values of s could contribute to the inundation likelihood as shown by the dispersion around the mode of 2 25 m fig 16c the distribution of t 0 is approximately symmetric for most of the cases in the range 3 to 3 hours fig 16d the density distribution of r is less straightforward to interpret although the inundation seems to be mostly induced by a high topographic erosion rate r close to 1 according to fig 16e the density distribution of the breaching location fig 16f indicates that two breaching locations may favour inundation namely b 2 southernmost part of the site fig 4b and b 10 the northernmost part of the site interestingly this suggests that the most influential breaching scenario is not necessarily the one that is located the closest to the point of interest the presence of both modes is not distinct the shape of the distribution in fig 16f is quasi flat which suggests that other locations may also be important and may indicate that the distance of the asset to the breach is not the main driver and other factors play a major role dune shape crest and foot topographic level and control of the local topography in the vicinity of the breach on the water flow among others more advanced modelling studies are necessary to obtain deeper insights into those physical processes see the discussion in sect 5 2 5 discussion in this section we discuss the added value sect 5 1 in terms of research questions q1 q3 which were described in the introduction and the limitations sect 5 2 of the rf based approach for marine flooding assessment and prevention 5 1 added value three research questions were outlined in the introduction q1 q3 the first one concerns the validity of using the rf model in place of the true computationally expensive numerical simulator regarding this issue the analysis that was conducted in sect 4 using an independent set of simulations shows the very satisfactory performance level of the constructed classification rf model with an accuracy and an area under the roc curve that are larger than 90 and 95 respectively for assets 1 and 2 and larger than 85 and 90 respectively for assets 3 and 4 this provides good confidence in the quality of the rf model for the bouchôleurs case furthermore the comparison of procedures for the rf set up iterative or random see figs 10 and 12 provides satisfactory indications that the method can efficiently select the simulation scenarios research question q2 for constructing the rf model therefore the rf based analysis provides a costless to evaluate estimator of the possible inundation at the considered assets without resorting to computationally expensive numerical simulations here of cpu time of 30 60 minutes this can be a key ingredient in facilitating the implementation of probabilistic coastal flooding hazard and risk assessment which often requires a very large number of forcing conditions that are randomly generated from multivariate extreme value models for instance 314 000 stochastic realisations which were representative of 10 000 years were used by rueda et al 2016 the rf based rapid evaluation can also be a key element in the implementation of an early warning system as discussed by idier et al 2013 for this purpose the level of error although satisfactory for our exploration analysis sect 4 3 and 4 4 should be even smaller in particular this can be achieved by further optimising the false alarm rate versus the true alarm rate for instance by using alternative performance metrics such as the area under the roc curve for the rf construction as shown by janitza et al 2013 considering the third highlighted research question q3 sect 4 2 4 3 show how the rf based analysis enables improvement in the understanding of the local conditions that lead to marine flooding for the key assets of the bouchôleurs site the identification of the important variables and the exploration of the dependencies enables us to do the following 1 confirm the physical interpretation of the effect of the tide surge interplay on the overflow inundation it shows that the surge peak that is in quasi phase with the high tide with an absolute phase difference of no larger than 3 hours plays a tremendous role in flooding as exemplified by the recent xynthia event 2 show the moderate influence of the surge s time evolution t and t 3 outline the complex relationship between the breaching parameters and the likelihood of inundation in particular it shows that the most influential breaching location is not necessarily the one that is the closest to the point of interest indeed the spatial variability of the dune structure crest and the topography which controls the flow propagation inland may play major roles 5 2 limitations one pillar of any data driven approach such as the rf model in our case is the dependency of the prediction quality on the availability of a validated numerical model in other words rf predictions do not reflect the uncertainty in the modelling assumptions in the bouchôleurs case this corresponds more specifically to the assumptions on the physical processes and to the manner in which the offshore and breaching conditions are parameterised as explained in sect 3 flooding is assumed to be mainly driven by overflow processes hence wave induced overtopping effects are neglected in the bouchôleurs case this assumption is justified because the waves break far offshore bertin et al 2015 in our modelling strategy both the tide and the total storm surge are taken into account thus the main effect of waves on this site i e wave set up is taken into account through the input conditions and shallow water models can be used to reproduce the overflow induced flood however the overtopping contribution even if negligible in the study area is not strictly equal to zero with a high water level the waves even if strongly damped can still lead to small overtopping this was the case in the north of the study site during the xynthia event thus the presented modelling experiment accounts for the main processes in our case but uncertainties could be reduced by including this minor wave contribution since the rf based procedure does not constrain the choice of the type of simulator future work can concentrate on more advanced modelling procedures such as the one that was used by le roy et al 2015 to simulate overtopping induced flooding regarding the model set up the heights of the main structures that control the flood are properly taken into account in the digital terrain model of grid size of 25 25 m2 and non uniform friction coefficients are used to account for the land cover types see fig 7 however this approach does not enable us to describe individual buildings although the efficiency of this approach has been proven see e g mugica et al 2016 on the arcachon basin in france the presence of buildings may have a significant influence on the local water height differences of tens of centimetres are often observed on each side of a flooded building a higher resolution grid could be used this would improve the local water level prediction but not significantly modify the overall flood prediction e g flood extent finally the last specific characteristic of the model set up on the bouchôleurs sites is the hydraulic connections the way in which these connections are taken into account provides realistic results see sect 3 4 and studies in other contexts such as mugica et al 2016 and can be considered state of the art for shallow water models such as the telemac simulator which is described in edf drd 2010 the offshore forcing conditions are parameterised using an ideal tide signal and a triangular surge signal although this assumption remains a satisfactory first order approximation as adopted by other authors see e g poelhekke et al 2016 other storm surge models may be adopted such as the one that was proposed by wahl et al 2011 which better reflects the complexity of the sea level signal but requires more parameters a total of 25 in this situation the rf based variable importance analysis would play an important role in identifying the most influential input parameters regarding the breaching processes a limited number of scenarios are predefined these predefined scenarios are based on observations after the xynthia event 6 breaches in the coastal dune see e g muller et al 2016 plus the assumed locations of 4 breaches of hard coastal defences the width and erosion time scale are assumed based on the type of coastal defence coastal defence or dune thus the failure dynamics is simplified while these assumptions on the location and temporal dynamics are justified when exploring the main purpose of the present study analysis of the rf applicability some areas of improvement should be considered for defence failure modelling for hard defences and morphodynamic modelling for dunes according to the literature a possible modelling approach is to rely on analytical formulas for predicting the likelihood of hard coastal defence failure however these formulas do not provide any information on the spatial and temporal dynamics of the failure see e g eurotop 2016 thus these formulas are not sufficient for estimating the length height and width of a failure or their temporal evolution thus advanced geotechnical models should be used in terms of dune breaches morphodynamic models such as the x beach simulator mc call et al 2010 are promising for reproducing the dune breaches that are induced by overflow on such areas muller et al 2016 however they are very time consuming the issue of accounting for more realistic spatial and temporal characteristics of the breaches is challenging and outside the scope of this paper 5 3 summary and future work in the present study we took advantage of a suite of recently developed r packages namely ranger boruta and icebox to implement rf models and exploit them to detect and explore the critical offshore forcing and breaching conditions which might lead to marine inundation the key features of the rf based analysis are as follows 1 the use of only a limited number of computationally expensive numerical simulations 200 in our application case 2 the low computation time cost 3 the capability of tackling mixed types of input parameters continuous such as high tide level and categorical such as the scenarios of breaching position 4 the capability of tackling many potentially correlated input parameters by means of variable importance analysis the application to a real case bouchôleurs west of france shows that the rf based approach can be useful for marine flooding assessment and prevention however the manner in which we set up the rf model only partly fulfils practical needs which leaves room for improvement first the quality of the prediction of the rf model strongly depends on the validity of the assumptions that underlie the numerical model a broader range of flooding situations should be investigated in the future we studied the case of overflow in the bouchôleurs case in particular for overtopping flooding cases as modelled by le roy et al 2015 at gâvres france and for hydro morphological processes during breaching as modelled by muller et al 2016 second the rf models were set up at spatial locations of interest this strategy may not be appropriate when one is interested in the inundation likelihood over the whole area i e at all spatial locations of the map in fig 4 which corresponds to thousands of pixels when considering only the urban areas the feasibility of multivariate extensions of the rf model as described by segal and xiao 2011 is worth investigating in the future finally for visualisation purposes the use of individual conditional expectation plots iceplot proved to be useful for the exploration of classification rules although approaches are available in icebox package for handling interactions among the parameters they are limited in their ability to fully explore these complex relationships dimensional reduction by projection as proposed by welling 2015 and implemented in the r package forest floor may be effective in overcoming this problem acknowledgements the authors thank the brgm funded dev probsub project for contributions to the financial support of the present work tide gauge data were provided by the french hydrographic and oceanographic service shom through refmar website http refmar shom fr we thank ifremer french research institute for exploitation of the sea for sharing with us the mars code we are also very grateful to the three anonymous reviewers for their comments which led to improving the manuscript appendix acronyms acronym description acc accuracy performance criterion auc area under the roc curve err misclassification rate eq 2 ice plot individual conditional expectation plot lhs latin hypercube sampling oob out of bag p classification probability evaluated using rf pam partitioning around medoids pdp partial dependence plot rf random forest model roc receiver operator curve vi variable importance of rf eq 4 
26396,physically based river water quality models are valuable tools for river basin management and planning however their long computational times pose many difficulties for applications that involve a large number of model iterations this paper addresses this problem by developing a faster surrogate conceptual model based on the detailed reference models the hydrodynamic information and water quality process equations from different detailed models are considered as ensembles in the developed model the model conceptualizes rivers using cascades of reservoirs and lumps the advection diffusion and physico biochemical processes we tested the model by comparing its performance for the molse neet river belgium with two popular reference models namely mike 11 and infoworks rs results show that the conceptual model performs equally well as the reference models but with simulation time 104 times faster the successful testing of this model opens a development avenue towards problem solving in the context of water quality control and management keywords calibration conceptual river water quality model coriwaq infoworks rs linear reservoir mike 11 validation software availability name conceptual river water quality coriwaq developers ingrid keupers thanh thuy nguyen and patrick willems primary contact patrick willems department of civil engineering hydraulics section kasteelpark arenberg 40 bus 2448 3001 heverlee belgium 32 16 32 16 58 patrick willems kuleuven be hardware required general purpose computer we recommend using a high speed processor and at least 4 gb of ram software required matlab a c compiler and dhi matlab toolbox programming language matlab and c availability upon request or through the following website of the hydraulics section ku leuven www kuleuven be hydr pwtools htm 1 introduction clean water is an increasing concern to our society because poor water quality both directly and indirectly causes many water related diseases united nations development programme 2006 announced that the people with water related diseases contribute to 50 of hospitalized patients in the world this type of diseases leads to nearly 20 of all deaths under 5 years old children who unicef 2009 to keep our surface waters clean it is crucial to effectively control and manage its quality which requires quantitative knowledge on water quantity and quality status both in time and space information on the water quality status can be obtained from monitoring systems shrestha and kazama 2008 haji gholizadeh et al 2016 however such systems are expensive and usually insufficient to cover the high spatio temporal variability of water quality variables bartley et al 2012 lessels and bishop 2015 an efficient alternative solution is the use of mathematical water quality wq models such models give decision makers insights in the cause effect relationships and guidance in the design of water resources management strategies gwp 2013 by comparing simulation results of different scenarios decision makers can easily derive evidence based decisions many wq models have been developed ranging from lumped conceptual models to more detailed physically based models from models describing single components of the integrated water system to holistic models chapra 2008 however simulations obtained by different models do not always lead to similar findings this is because different models have different model structures which are based on specific hypotheses and tested for selected case studies because of data limitations validation of the complete model structure is in most cases impossible for this reason it would be useful to take into account in the decision making process the uncertainty in the model results as a consequence of the model structure assumptions lindenschmidt et al 2007 nguyen and willems 2016 at current in most model applications just one software package is applied e g swat neitsch et al 2011 hec 5q willey 1986 or infoworks icm innovyze 2012 which involves one fixed model structure moreover the wq modules in most of the packages only accept the hydrodynamic results of their own packages as input the infoworks package solely allows to import networks and results from hec ras meanwhile mike11 eco lab dhi 2011 permits integration with other hydrodynamic models albeit only after modifications in the source code this inflexibility does not allow a complete analysis of the uncertainty in the wq simulation results therefore we plead for the development and application of decision support tools that involve a flexible model structure including different model structures and that allows to modify and combine physico biochemical processes from different physically based models river wq models as many types of models may involve a detailed representation of the modelled system detailed physically based models or a simplified one simplified models simplified modelling involves the use of a lumped conceptual model that is based on a lumping of the processes in space and conceptualized representation of the most dominant processes cox 2003 for instance tomcat and qual2e simulate physico biochemical processes with non uniform and steady flow at daily scale warn 1987 brown and barnwell 1987 with the advantage of short computational times these models allow implementation of sensitivity analysis parameter optimization and uncertainty analysis e g qual2kw pelletier et al 2006 lumped conceptual river wq models have the disadvantage that their processes and parameters are less explicit hence are less directly linked to measurable system properties meanwhile the detailed physically based models e g mike 11 dhi 2011 infoworks rs innovyze 2012 calhidra 3 0 cardona et al 2011 simulate the hydrodynamic advection diffusion and physico biochemical processes the long simulation time poses difficulties on applications that involve huge number of model runs iterations and or long term simulations such as model uncertainty analysis auto calibration real time control optimization etc the difficulties are larger for applications in larger river catchments for instance a water quality simulation for a 1 year period at a time step of 1 h for a river system with the total rivers length of 139 km and cross sections at distances of approximately 50 m takes about 4 days on a standard pc keupers and willems 2017 emulation modelling is known as a low order approximation of the detailed physically based models to reduce their computational complexity castelletti et al 2012 in this manner the most relevant variables are taken into account in the emulator the variables are identified by data based or structure based approaches in the data based approach the variables can be selected by a statistical measure of input output relationship e g partial mutual information bowden et al 2005 li et al 2015 and minimum redundancy maximum relevance peng et al 2005 hejazi and cai 2009 in the structure based approach a model formulation is derived for each possible combination of the variable replacement by constants e g crout et al 2009 reichert et al 2011 machac et al 2016 the model performances are evaluated by criteria such as residual sum of squares akaike s information criterion and bayesian information criterion the advantage of the former approach is that it has much less computational cost however the ignorance of physical mechanisms can lead to low accuracies when the sample size is small therefore this paper opts for the structure based approach but after simplification of existing model structures in order to reduce the computational times considering the above mentioned needs of a flexible physically based but reduced complexity model structure with reduced computational time this paper builds further on the recent advances by the development of the flexible version of the conceptual river water quality model coriwaq conceptual river water quality coriwaq is a hybrid of conceptual and physically based models to obtain more accurate simulations than the traditional lumped conceptual models but with shorter computational times than detailed physically based reference models accordingly hydrodynamic information for coriwaq is obtained from detailed physically based models the equations used to simulate the biochemical transformation processes in coriwaq are taken the same as in the physically based models the hydrodynamic characteristics in the coriwaq process equations are directly taken from the detailed full hydrodynamic reference models after applying correction factors considering the same inputs as the detailed reference models the lumped model is implemented for the motions of determinant concentrations the advection and diffusion processes along river segments are conceptualized using a reservoir type approach the physico biochemical processes along the river segments are presented by a set of equations with the incoming concentrations at the first cross sections and outcoming concentrations at the last cross sections of the corresponding river segments with this approach each river segment is characterized by one time series for each hydraulic characteristic and wq variable this approach has been widely used to transform precipitation to runoff pedersen et al 1980 chow et al 1988 weiler et al 2003 chetan and sudheer 2006 nourani et al 2009 groundwater recharge to discharge peters et al 2003 and for river or tidal river or sewer hydraulics wolfs et al 2015 meert et al 2016 wolfs and willems 2017 however there are only few studies on the application of lumped conceptual models for river wq modelling e g whitehead et al 1997 willems and berlamont 2002 radwan et al 2003 2004 willems 2008 two detailed physically based models implemented in the software packages mike 11 and infoworks rs hereafter denoted shortly as rs with different numerical schemes and different equations to simulate biochemical transformations are selected as reference models this research is a follow up of the initial coriwaq developments based on mike 11 coriwaq mike11 by keupers and willems 2017 coriwaq mike11 was applied to simulate the influence of combined sewage system overflow on river wq keupers et al 2015 and analyze the global analysis sensitivity of wq parameters keupers and willems 2015 in this paper coriwaq has been extended for infoworks rs coriwaq rs the developed model is tested by simulating the water quality for the molse neet river in belgium and by comparing its results with those obtained in both mike 11 and rs in the calibration stage the coriwaq parameters are calibrated to minimize the misfit between the wq output of coriwaq and the reference models in the validation stage the calibrated parameters are used to reproduce the wq variables and compare with those of the spatially more detailed reference models the calibration and validation are complemented within different flow conditions i e wet mean and dry conditions finally how robust the coriwaq model is to changes in the process settings is also tested 2 methodology 2 1 conceptual water quality model coriwaq in coriwaq rivers are internally divided into segments depending on the location of pollutant sources interested locations e g wq measurement stations as well as the maximum and minimum segment lengths set the location of the pollutant sources are defined in the detailed reference models the interested locations and the values of the maximum and minimum segment lengths are to be specified by the user where the latter can be different for different river reaches to satisfy the assumption that the hydraulic conditions do not vary that much along the river segments that correspond to each reservoir the maximum length of these segments should not be too large for rivers in which the flows are more uniform larger values can be given to the maximum segment length as for the minimum length this one should be high enough to sufficiently limit the conceptual model computational times it moreover should be larger than the smallest distances between 2 successive cross sections otherwise the coriwaq model would have the same spatial resolution as the reference models however the minimum segment length should be lower than the smallest distance between 2 successive pollutant sources based on the defined river segments the different wq processes i e advection diffusion and physico biochemical processes are in coriwaq modelled in a lumped way per segment to set up calibrate and validate the model the hydrodynamic hd and wq results from the reference models need to be provided in this study the hd inputs are obtained from the mike 11 and rs full hd reference models 2 1 1 processes 2 1 1 1 advection diffusion the motions of pollutants along the river are described by the advection diffusion ad equation this equation is in wq models typically solved by numerical schemes based on finite differences for example mike 11 uses an implicit finite difference scheme and a correction term to reduce the third order truncation error dhi 2011 rs employs the smooth molecular surface triangulator smart scheme gaskell and lau 1988 these methods require a huge number of repetitive calculations to achieve good approximations for the wq variables instead of solving the ad equation with such numerical methods willems and berlamont 2002 proposed to conceptualize the motion of pollutant determinants along waterways by routing through a cascade of linear reservoirs where each reservoir represents a river segment eq 1 1 c o u t a d t e 1 k c o u t t δ t 1 e 1 k c i n t δ where c o u t a d t is the determinant concentration water temperature at the segment outlet at time t c o u t t δ t is the determinant concentration water temperature at the outlet at previous calculation time step e g δ t 1 hour c i n t δ is the delayed determinant concentration water temperature at the segment inlet eq 1 has 2 parameters namely the recession constant k and the advective time delay δ k δ is the total residence time of the determinant in the river segment the advective time delay presents the advection motion of determinants along the reservoir and depends on the length of the reservoir and the flow velocity because the velocity is spatially and temporally variable along the rivers δ is estimated by dividing the segment length by the mean velocity where the spatial mean velocity is computed from the hd model results the water depth is calculated as the mean of the hydraulic radius in mike 11 or the hydraulic water depth in rs at all calculation nodes within the corresponding river segment 2 1 1 2 physico biochemical transformations with the available input the simulated physico biochemical transformation considers 7 wq variables in mike 11 water temperature t do bod5 ammonium nh4 n nitrate no3 n ortho phosphorus op p and particulate phosphorus pp p and 7 wq variables in rs t do bod5 nh4 n nitrite no2 n no3 n and organic nitrogen on n these wq variables are simulated in coriwaq within dissolved oxygen organic matter nitrogen and phosphorus solely in mike 11 cycles similar to those in the reference models mike 11 and rs the detailed information about these transformation processes as well as the equations can be found in dhi 2011 and innovyze 2012 below we present the main differences in physico biochemical transformation simulation between mike 11 and rs the first difference is the assumptions concerning the reaerated oxygen the reaerated oxygen is assumed to be inversely proportional to the water depth in rs while it is directly proportional to the water surface slope in mike 11 only rs allows to control the reaerated oxygen at the hydraulic structures by applying reaeration coefficients at the hydraulic structures secondly mike 11 considers the diurnal variations of water temperature photosynthesis and respiration sediment oxygen demand deposition and re suspension of particulate phosphorus release and absorption of dissolved phosphorus deposition and re suspension of suspended organic matter rs does not account for these processes if inputs on phytoplankton and sediment concentration are not provided which was the case in this study thirdly the on n is considered separately from bod5 and nitrification is divided in 2 steps i e from nh4 n to no2 n and from no2 n to no3 n in rs the denitrification only occurs when the do concentration is lower than 5 of the saturated do concentration meanwhile the on is not simulated as a wq variable in mike 11 it is considered inclusive in bod5 the nitrification goes directly from nh4 n to no3 n and denitrification happens simultaneously with the nitrification after the ad and biochemical transformation processes are simulated the concentration at the outlet of each conceptual reservoir is estimated as below eq 2 2 c o u t t e 1 k c o u t t δ t 1 e 1 k c i n t δ δ c δ t k δ where δ c δ t is the change of determinant concentration in one time unit due to physico biochemical processes the net effect of physico biochemical transformation processes on the wq variables is represented by reduction factors f reduc determined as the ratio between the wq variables at the outlet of the river segments considering the combined ad and physico biochemical processes concadwq and those considering only the ad process concad eq 3 f reduc can be larger or smaller than 1 f reduc is larger than 1 when the production processes are stronger than the consumption processes and vice versa 3 f r e d u c c o n c a d w q c o n c a d 2 1 2 calibration parameters model parameters are often calibrated to observed data however the uncertainties in wq modelling are dominated by the model input and model structure uncertainties van der perk 1997 håkanson 2000 van griensven and meixner 2006 improvement of the model simulation results hence should involve changes to the model parameters the model input data and the model structures for the proposed hybrid model coriwaq this procedure is tested by calibrating the coriwaq parameters to the wq simulation results of the spatially more detailed reference models i e mike 11 and rs models in this study input uncertainties and uncertainties in the results of the detailed reference models are not considered in this study coriwaq inputs are considered identical to the ones in the reference models and the reference models are assumed well calibrated or at least representative to the real river wq conditions previous section explained how the parameter values e g water depth residence time and velocity in the conceptual model can be initially assessed however due to the spatial lumping involved in the conceptual model the estimated parameter values of the wq processes may differ from the parameter values leading to the most accurate coriwaq results therefore correction factors are introduced which allow adjustment of the conceptual model based on the simulation results of the detailed models f tr correction factor to the estimated residence time f depth correction factor to the water depth f velocity only for coriwaq mike 11 correction factor to the velocity the residence time and the water depth influence the wq variables directly or indirectly through the physico biochemical processes in mike 11 the water depth directly affects the water temperature nh4 n bod5 pp p and do concentrations through heat transfer re suspension deposition photosynthesis respiration sod and reaeration processes in rs the water depth directly affects the do concentration and the water temperature through the reaeration and heat transfer processes respectively in both models water temperature is the primary wq variable and governs all other wq variables this shows that the water depth influences all wq variables indirectly through the water temperature the residence time directly influences all wq variables through the ad process therefore f tr was firstly calibrated followed by the calibration of the factors f depth and f velocity all these correction factors are given the value 1 as initial value no3 n is only indirectly affected by the water depth through nitrification therefore the no3 n output from mike 11 was selected as the wq variable for calibration of f tr because the bod5 and pp p concentrations are very sensitive not only to the water depth but also to the water velocity coriwaq mike 11 has f velocity as an additional correction factor both bod5 and pp p were chosen on the basis of the f depth and f velocity calibration in rs on n starts the nitrogen cycle and impairs the concentrations of nh4 n no2 n and no3 n through the nitrification process in other words on n is the primary wq variable of the nitrogen cycle the hydrolysis of on is indirectly influenced only by the water depth through the water temperature and was therefore used for the f tr calibration in rs for the f depth calibration water temperature is most sensitive to the water depth in rs and was selected for that calibration the correction factors were auto calibrated by the least squares method applied to the coriwaq f reduc coriwaq versus reference model f reduc theor time series results because the water depth and the velocity govern the bod5 and pp p concentration simultaneously latin hypercube sampling lhs was applied for the calibration considering 250 set values for the truncated standard normal distributions non negative values with a mean of 1 and a standard deviation of 1 1 considered for f depth and f velocity in coriwaq mike 11 for f depth in coriwaq rs and f tr in both models 250 values with equal interval were selected in the range 0 2 2 these ranges are based on the meaning of the correction factors and the experienced sensitivity of the model outputs to these factors 2 2 implementation for mike 11 and infoworks rs 2 2 1 mike 11 for implementing coriwaq associated with mike 11 coriwaq requires the mike 11 s hd and wq setup and simulation result files fig 1 the setup files include information on the river branches calculation nodes hydraulic structures mike 11 s network file geometrical data for the cross sections such as the wetted area hydraulic radius top width additional flooded area and resistance factor processed cross section file location and type of the model boundaries and the sources boundary file model input data time series file and wq parameters eco lab parameters file the hd results file contains model results on water levels discharges and velocities at each calculation node in mike 11 the hydraulic radius is used as proxy for water depth to simulate physical processes such as the heat transfer reaeration and deposition time series of hydraulic radius at each cross section were obtained from time series of water levels and the relationships between water level and hydraulic radius from the exported processed cross section file we developed a matlab code to automatically read and compute all this information input and calibration data from the mike 11 files calibrate the coriwaq parameters and conduct the conceptual model simulations and post processing river segments between emission locations boundaries or other interested locations were defined as reservoirs fig 2 the reservoir lengths were set following the range from minimum to maximum lengths chosen if the length of a reservoir was smaller than the minimum length the selected emission locations to divide the river in n segments were relocated if the length of a reservoir was larger than the maximum length the river segment was divided into 2 until its length was smaller than the maximum length the length of each reservoir was redefined by the distance between the nearest flow calculation nodes of the starting and ending chainages in the reference model the nodes used for calculating the water surface slope and the water depth were located between the nearest water level calculation nodes of the starting and ending chainages the water depth and velocity were obtained by averaging in space at the corresponding calculation nodes while the water surface slope was computed by distance weighed average of the slopes between these calculation nodes 2 2 2 infoworks rs for implementation of coriwaq based on rs the required data files also include hd and wq setup and result files this time exported in csv format fig 1 the setup files include information on river sections links connectivities spills rectangular conduits junction nodes and boundary nodes network file and values of the wq parameters wq file the rs results on water levels and wet areas at calculation nodes combined with geometric data at each river section were used to calculate the water depths these water depths were used to simulate wq processes such as heat transfer and reaeration as for mike 11 our code automatically reads and computes all the required information input and calibration data from the rs files implements the reservoir determination calibrates the coriwaq parameters and conducts the conceptual model simulations and post processing similar to mike 11 the reservoir determination was based on the locations of emission locations selected as the boundaries for the river segments and additional interested locations fig 2 if a reservoir length was smaller than the minimum length the emissions were relocated in rs if the reservoir length was larger than the maximum length the river segment was divided into more reservoirs with the lengths close to maximum length the chainages of the new reservoirs were defined based on the calculation nodes that result in reservoir lengths close to the calculated values different from mike 11 where the water level and flow calculation nodes are alternating they are overlapping in rs the selected calculation nodes for each reservoir thus were determined by the starting and ending chainages respectively 2 3 performance testing model validation was conducted by a split sample approach where the model was evaluated for the data period not considered during calibration the evaluation was based on the nash sutcliffe coefficient nse nash and sutcliffe 1970 eq 4 applied to the differences between the coriwaq and the reference model simulation results for the different wq variables nse can vary from negative infinity to 1 the higher the nse values are the better coriwaq performs 4 n s e j 1 i 1 n s i j o i j 2 i 1 n o i j o j 2 where n is the number of time steps s ij is the coriwaq simulation result at the outlet of reservoir j and at time i o ij is the simulation result of reference model at the outlet of reservoir j at time i o j is the average simulation result of reference model at the outlet of reservoir j in this study the coriwaq parameters were calibrated for each of three years with different flow conditions including wet mean and dry years in this way the influence of the flow conditions on the parameters could be assessed the calibrated values for f tr and f depth at the different reservoirs corresponding to each calibration year were plotted in comparative boxplots after calibration the models were validated for three other years representing wet mean and dry years the parameters that show the best model performance for these 3 validation years were selected for further performance testing beside this validation for different flow conditions the performance of coriwaq with different values of the wq process parameters was also investigated this was done by running simulations in both coriwaq and the reference models for a selected year wq parameter values in each simulation were defined by increasing and reducing each wq parameter value with 10 while keeping the values of other wq parameters fixed the coriwaq performances under such changing conditions were again evaluated based on the changes in nse values calculated for all river reservoirs and the seven wq variables 3 case study the molse neet river is located in the north east of the grote nete catchment with a sub catchment area of approximately 24 54 km2 fig 3 a the length of the river from the upstream location to the confluence downstream is 11 5 km the geometry of the molse neet river is determined by cross sections along the river approximately every 50 m and at the hydraulic structures besides geometric data the detailed reference models consider hydraulic structures along the network such as culverts and bridges which locally influence the flow characteristics the emission sources to the molse neet river include wastewater from domestic households industries and agriculture nguyen and willems 2016 these pollutant sources were considered as 13 point sources along the river in rs and mike 11 for some point sources the confluences with the river are located very closely these were merged together the parameters for the ad and biochemical transformation processes in mike 11 and rs were obtained from previous research nguyen and willems 2014 in that research calibration was done for heat transfer rate and reaeration rate coefficient at hydraulic structures to wq sampling data for the two years 2006 and 2007 the values of other wq parameters were selected from recommended values for similar rivers 4 results and discussions coriwaq mike 11 and coriwaq rs were implemented for the molse neet river using same pollutant sources processes and parameters as in the reference models besides the boundaries and point sources the number of reservoirs were defined by the location of wq measurement station 333 000 6 km from the upstream boundary given that the water level variations along the molse neet river do not vary that much over distances shorter than 500 m this length was chosen as the maximum length for the case study given that in the detailed reference models cross sections are implemented at approximately every 50 m this length was chosen as the minimum river segment length as a result the molse neet river was finally characterized by 30 reservoirs in coriwaq mike 11 and 29 reservoirs in coriwaq rs a scheme of the simplified river network model in coriwaq mike 11 is shown in fig 3b as an example with the results of the hydrological model pdm the rainfall runoff from the upstream sub catchment from 2000 to 2010 is shown in fig 4 comparing the runoff among these 11 years the period 2000 2003 can be considered as wet 2004 2008 with mean climate conditions and 2009 2010 as dry the three years 2001 2008 and 2009 including wet mean and dry conditions were selected for calibration the three years 2002 2006 and 2010 corresponding to wet mean and dry conditions were chosen for validation the simulation of each year for the case study i e 29 30 reservoirs with a time step of 1 h takes approximately 8 s on a laptop with intel core i7 3520m processor and installed memory ram 8 gb meanwhile the corresponding simulations in the reference models can take 18 h in rs with time step of 10 s or 43 h in mike 11 with time step of 15 s note that rs and mike 11 need shorter time steps to avoid model instabilities 4 1 calibration results water temperature is the driving force of many physico biochemical processes along rivers meanwhile the do concentration is the consequence of most physico biochemical processes and an important variable to indicate the wq state of a river therefore these two variables were selected as examples of the coriwaq mike 11 and coriwaq rs calibration results for the year 2008 which can be seen as a normal year fig 5 the calibration results of other wq variables are shown in appendix a both coriwaq mike 11 and coriwaq rs simulate water temperature very well with their simulation results almost overlapping with those of the corresponding mike 11 and rs models fig 5a and c when comparison is made with the observations mike 11 and coriwaq mike 11 capture the low water temperature better than rs and coriwaq rs do concentrations are also simulated well in coriwaq with small differences between them and the simulation results in mike 11 and rs fig 5b and d note that the diurnal variations in do concentration due to photosynthesis and respiration are simulated in mike 11 and coriwaq mike 11 therefore the ranges of do concentrations are in mike 11 and coriwaq mike 11 wider than in rs and coriwaq rs the consideration of the photosynthesis and respiration processes may also be one of the reasons why the do concentrations in mike 11 and coriwaq mike 11 match to the observed values better than in rs and coriwaq rs in general coriwaq mike 11 derives simulation results very close to the reference model results for water temperature no3 n nh4 n op p and pp p with rounded nse values of 1 at most of the reservoirs for all 3 years the model also simulates bod5 very well with all nse values higher than 0 89 do concentrations upstream and downstream along the river also match well with the mike 11 results with nse higher than 0 8 however do does not show good results at the central region of the river nse values are small at these locations and even negative at reservoir06 and reservoir07 for all 3 years this inaccuracy is expected to be the result of the inability of the model to take the influence of local steep water surface slopes into account in mike 11 the steep slopes upstream and downstream of hydraulic structures make reaerated oxygen very high and soar the do concentration at these locations this phenomenon is clearly shown at hydraulic structure knw30 at km 2 178 from the upstream boundary where the do concentration increases with 0 88 mg l on average however this influence of the hydraulic structures is eliminated by using an average value for the water surface slopes over space in coriwaq mike 11 coriwaq rs does simulate water temperature no3 n nh4 n and on n accurately with rounded nse values equal to 1 at the majority of the reservoirs for all 3 years the model also performs very well for do with all nse values larger than 0 77 at all reservoirs for all three years except at reservoir09 after calibration for the wet year 2001 due to the lumped approach per reservoir or river reach the underestimation in do concentration is propagated from upstream reservoirs to downstream ones a point source with high do concentration is located at the chainage 3 635 km upstream end of reservoir10 the effluent from this point source increases the do concentration in the river through the dilution effect and mitigates the influence of previous reservoirs for bod5 the results are good with high nse values the smallest nse value is 0 68 at the downstream end of reservoir28 after calibration for the dry year 2009 the nse values for bod5 concentration decrease from 1 at the upstream boundary of the river to 0 78 at the downstream end the no2 n calibration is not good for any of the three calibration years with overestimations that propagate from upstream to downstream no2 n concentrations increase through nitrification from nh4 n and decrease through nitrification to no3 n the overestimations in no2 n concentrations can be caused by overestimation of the nitrification rate from nh4 n to no2 n for the wet year 2001 20 reservoirs show nse value smaller than 0 70 while this is the case for 9 reservoirs for the mean year 2008 and 5 reservoirs for the dry year 2009 4 1 1 residence time factor ftr the comparative boxplots in fig 6 a show the calibrated values of f tr at reservoirs corresponding to mike 11 and rs the figure shows insignificant differences in f tr due to different flow conditions in coriwaq mike 11 f tr has values of 1 05 for the median in all three calibration years their interquartile ranges corresponding to 25 and 75 percentiles are similar with lengths of around 0 12 the f tr values show similar ranges i e 0 75 1 42 and 0 75 1 47 for the years 2008 and 2009 but a smaller range length of 0 89 for the year 2001 among the reservoirs reservoir23 has the highest f tr s values of 1 834 1 761 and 1 944 for the years 2008 2001 and 2009 the residence time along sub divisions of this segment is longer and more variable in both time and space than the one that is lumped in space in comparison with other reservoirs reservoir23 has smaller estimated residence times which also means that the time for nitrification from nh4 n to no3 n is short meanwhile the results of the reference model indicates a larger increase of no3 n concentrations hence high values of f tr need to be applied for this reservoir to minimize the differences between the 2 models in simulating changes in no3 n concentration due to the lumped approach overall f tr in coriwaq rs is smaller and less variable than the one in coriwaq mike 11 rs allows to set the culverts with the invert level lower than the river bed level meanwhile the invert level of culverts are not able to be lower than the river bed level in mike 11 to avoid generating cross sections with irregular shapes the cross sections at the upstream and downstream ends of the culverts obtain the elevation of the culvert culvert cross sections the additional cross sections taking into account the sediment deposition layer are added before and after the culvert cross sections these modifications increase the temporal and spatial variations of velocity along the segments where the culverts are located because of this the mean velocity along the segment does not represent the segment well and a factor had to be applied to obtain a more representative velocity value in coriwaq the correction factor is applied for tr which is calculated by the ratio between the segment length and the mean velocity the value of f tr is highest for the year 2009 with a median value of 1 03 in coriwaq rs the f tr values for the 3 years have similar 25 75 interquartile ranges with range lengths around 0 12 apart from outliers f tr at reservoirs varies in the same range of 0 88 1 15 for the years 2001 and 2009 while the upper quartile value is higher for the year 2008 i e 1 4 outlying f tr values occur for reservoir18 for the years 2001 and 2009 the reason for this outlier is again the lumped approach in comparison with the coriwaq mike 11 results the outlier is occurring at a different segment due to the difference in hd simulation results between mike 11 and rs and the wq variable selected for the f tr calibration 4 1 2 water depth factor fdepth for the water depth factor f depth median values of 1 01 are obtained for coriwaq mike 11 in all three calibration years fig 6b this means that the water depth is estimated very well by that model the 25 75 interquartile range length varies between 0 04 for the year 2009 and 0 06 0 07 for the years 2008 and 2001 some outliers are however noted the year 2001 has extremely high f depth values at reservoir09 and reservoir23 and at reservoir02 and reservoir23 for the year 2009 the f depth is always highest at reservoir23 the f depth is calibrated after f tr calibration so f depth includes the error in the residence time estimation the comparison between the detailed and lumped models is implemented for the ratio of residence time to water depth the variation in geometry of cross sections and the distribution of hydraulic structures along the river segments corresponding to the reservoirs09 reservoir02 and reservoir23 make the ratios along sub divisions of these reservoirs more variable in both time and space meanwhile the lumped model which considers mean residence times and water depths deals with lower values of that ratio in comparison with other reservoirs the reservoirs mentioned have high total residence times while values of f reduc coriwaq of bod5 and pp p concentrations are larger than the corresponding values of f reduc theor the increase in water depths after multiplying the estimated values with the calibrated f depth decreases the changing rates and results in lower differences between f reduc coriwaq and f reduc theor the range of f depth in coriwaq rs is similar to the one in coriwaq mike11 for coriwaq rs the water depth is estimated well with f depth values close to 1 i e 1 01 for the year 2008 1 03 for 2001 and 0 98 for 2009 their interquartile ranges are moreover very small with smallest range length of 0 05 for the year 2008 only one outlier extremely high value of f depth is obtained for all 3 years at reservoir23 the ratios of residence time to water depth along sub divisions of this segment are smaller and less variable in both time and space than the ones in the lumped model in comparison with other reservoirs reservoir23 has smaller ratio of residence time to water depth while the ratio of water temperature at the outlet to water temperature at the inlet is lower therefore high values of f depth need to be applied for this reservoir to minimize the differences between the two models in simulating changes in water temperature note that the water depth is estimated as the hydraulic radius in mike 11 while it is computed as the hydraulic depth in infoworks rs 4 1 3 velocity factor fvelocity for coriwaq mike 11 the velocity factor f velocity values at the 25 50 and 75 percentiles are equal to 1 in all three calibration years fig 6c however the values of f velocity in the upstream are highly different from 1 the highest value of 4 9 is observed at the reservoir09 with the calibration data of year 2009 the smallest value of 0 2 is obtained at the reservoir08 and reservoir02 with the calibration data of years 2008 and 2009 respectively this means that the mean velocity in space can be considered as a good representation for the river segments where the spatial variation of velocity is small there are many hydraulic structures in the upstream part that strongly affect the flow regime the resuspension and deposition of particulate organic matter and phosphorus are affected by such changes in flow velocity the total effects of velocity along river segments on the deposition resuspension can be higher or smaller than the mean velocity the total resuspension of particulate organic matter and phosphorus are larger than the estimated ones with the mean velocity in space at the reservoirs where the f velocity factor is larger than 1 meanwhile the total deposition of particulate organic matter and phosphorus are higher at the reservoirs where f velocity is smaller than 1 4 2 performance testing results after calibration the coriwaq models were validated considering different values of the conceptual model parameters these values were obtained in the calibration stage using independent simulation periods presenting wet mean and dry flow conditions years 2002 2006 and 2010 after that different wq process parameters in the reference models were applied to test how robust the coriwaq model is to changes in the process settings hence to check whether the conceptual models has not been over tuned for the correction factors 4 2 1 different time periods flow conditions the water samples at the wq observation station 333 000 were taken at different time instances however the specific time during the day was not mentioned for all measurements therefore the water temperature and determinant concentrations simulated in coriwaq and the reference models were averaged during the measurement days fig 7 a c e and fig 8 a c e show that the coriwaq mike 11 and coriwaq rs models can simulate water temperature approximately as good as the corresponding reference models with model results lying on the bisectors for all 3 years 2002 2006 and 2010 additionally the different values of coriwaq mike 11 parameters regardless to the flow conditions do not impair the water temperature validation results as for do the simulated do concentrations in coriwaq mike 11 are smaller than the ones in mike 11 at all times fig 7b d f because the f depth factor is calibrated to bod5 and pp p concentrations there exists a bias in the simulated do concentrations for different flow conditions the bias is the smallest for the wet year 2002 and largest for the dry year 2010 the systematic deviations are hypothesized to be the result of the underestimating reaerated oxygen due to the averaging of the water surface slopes see the earlier discussion on this in comparison with the f depth values for the years 2008 and 2001 the values for dry year 2009 are smaller and result in higher reaerated oxygen this is the reason why the parameters for the dry year 2009 are the best ones mitigating the underestimation in reaeration and shortening the differences between do concentrations in coriwaq mike 11 and mike 11 results for coriwaq rs the do concentrations are calibrated not as good as the water temperature and with the dry year 2010 showing the best match with the rs results fig 8b d f the differences in calibrated coriwaq rs parameters due to the flow conditions propagate to significant differences in results the parameters of the mean year 2008 issue the closest results for the years 2002 and 2006 while the parameters of the dry year 2009 result in overestimations of the do concentrations for all three validation years the f depth values are smallest for the year 2009 where they issue higher reaerated oxygen especially for the wet year 2002 with higher water depths the influence of f depth on do concentrations is stronger with higher concentration values in coriwaq than in rs the parameters for the dry year produce slightly better results for the do concentrations in coriwaq mike 11 meanwhile they result in high overestimation of do concentrations for the wet year 2002 in coriwaq rs the differences in the performance of coriwaq mike 11 and coriwaq rs in simulating do concentrations regardless to flow conditions are caused by their approaches to model oxygen reaeration as mentioned in section 2 1 1 2 for other wq variables the influence of flow conditions on their concentrations are insignificant appendix b it is noted in figs 7 and 8 and appendix b that the influence of the uncertainties in the input model structures and wq parameters on the model results is much higher than the influence of the errors due to model structure simplification especially for the do concentrations the mike 11 simulation results vary within a range similar to the observation data i e 7 2 9 7 9 2 and 7 9 8 mg l corresponding to the years 2002 2006 and 2010 without the consideration of photosynthesis and respiration in the oxygen cycle the ranges of do concentrations are smaller in the rs results i e 7 8 2 7 4 8 8 and 7 9 9 4 mg l respectively for the same years 2002 2006 and 2010 4 2 2 different wq parameters values for the evaluation of the coriwaq results after changes in the wq parameter values the simulations were implemented both in the conceptual and reference models given the high similarity in the coriwaq rs and coriwaq mike11 results results are only shown here for the rs results the mean year 2008 showed the best model performance for all 3 validation years and was selected for this analysis the smallest nse values among the 29 reservoirs of coriwaq rs are given in table 1 the wq parameters that result in significant changes in nse values i e the changes are larger than 0 2 were highlighted in the table when the wq parameters are increased or decreased with 10 the differences between coriwaq rs and rs results for water temperature no3 n nh4 n and on n are stable and very small among these wq variables water temperature has the smallest nse value of 0 996 the differences in bod5 results between coriwaq rs and rs remain with nse values between 0 7 and 0 8 when wq parameters change the f reduc theor factor calculated for bod5 varies from values smaller than 1 to higher than 1 this is opposed to the coriwaq mike 11 results where f reduc theor values are only smaller than 1 the latter result is because bod decay is involved as the only process this indicates that the reference model rs does not only consider organic matter decay but also other processes increasing dissolved organic matter to confirm that the inaccuracy in simulating bod5 is not due to the error in the f tr calibration a correction was applied to the residence time factor for the bod5 concentrations in year 2008 the nse values obtained for the bod5 concentrations at the outlet of the reservoirs increase this increase is highest at reservoir28 from 0 78 to 0 89 however the values of f reduc coriwaq are always smaller than 1 also note that there was lack of knowledge about the detailed process implementation in the reference detailed rs model the nse values for the do concentration are stable and always higher than 0 8 except at reservoir09 for an increase of 10 in f air the differences between the conceptual and reference model results are not so large with a mean difference of 0 168 mg l however the do concentration at this location is rather stable with a small standard deviation i e 0 169 mg l also leading to a very low nse value of 0 019 the increase in nse value when decreasing f air and the decrease when increasing f air confirm once more that the lumped approach cannot describe the do concentration with local remarkable changes very well the nse values for the no2 n concentrations are always small in these sensitivity simulations as was the case after model calibration except for the simulations with k no2 the nse value increases to 0 024 when k no2 decreases with 10 and reduces to 0 531 when k no2 increases with 10 the increase decrease in nse is due to the decreasing increasing influence of the no2 n no3 n transformation in rs and coriwaq rs when k no2 increases decreases in other words these nse changes are indicative for whether the nitrification amount from no2 n to no3 n is stronger or less strong in rs than in coriwaq rs this shows the existence of uncertainty in simulating the 3 step nitrification process in the conceptual model based on the rs model it is concluded that coriwaq performs very well with the simulation results remaining close for the different wq parameter sets tested except for no2 n changes in the nitrification process parameters do not affect the performance of the other wq variables significantly with unchanged nse values in this case study the no2 n concentrations are so small that the errors in the no2 n concentration in coriwaq do not influence conclusions regarding the river water quality state in terms of concentrations of no2 n or the other wq variables 5 conclusions and recommendations coriwaq was constructed successfully for the ad and biochemical process modelling in mike 11 and infoworks rs the model performs very well for the simulated wq variables the conceptual model development is supported by the reading of selected information in the files exported from the reference models and the auto calibration of the conceptual model parameters simulation of coriwaq on a laptop with intel core i7 3520m processor and installed memory ram 8 gb resulted in a reduction of the simulation time with a factor 104 for coriwaq in comparison with the detailed models the calibration of coriwaq was based on few primary wq variables no3 n bod5 and pp p for mike 11 and on n and water temperature for infoworks rs the calibration values of the coriwaq correction parameters i e f velocity solely coriwaq mike 11 f tr and f depth close to 1 indicate that the approach to estimate velocity residence time and water depth for river segments performed very well the use of a relationship between reaeration rate and water surface slope in mike 11 eco lab leads to overestimated do concentrations by applying a threshold to the water surface slope this disadvantage can be eliminated nguyen and willems 2016 therefore although the coriwaq mike 11 model performs better for the dry year the mean year should be selected to calibrate the conceptual model the coriwaq results present the total effect of hydraulic characteristics on the determinant concentrations along each river segment therefore the calibrated parameters of the conceptual model corresponding to the detailed reference model e g infoworks rs were used when combining or replacing physico biochemical processes with other detailed models e g adding photosynthesis process as in mike 11 eco lab it is noted that different hydrodynamic models can estimate the hydraulic terms distinctively e g water depth by hydraulic radius in mike 11 and hydraulic water depth in infoworks rs therefore the discrepancies between the simulation results should take these differences into account the equations to model water temperature photosynthesis and respiration in mike 11 should be implemented in coriwaq to improve the accuracy of do simulations in coriwaq rs moreover the uncertainty in estimating the determinant concentrations from pollutant sources model structure and wq parameters should be investigated the performance testing of the coriwaq model under different flow conditions and wq parameters resulted in the same range of nse values at the locations where the variation in do concentrations is small the high value of the transfer velocity at 20 c results in low nse values for the do concentration the absolute errors in the do concentrations are however limited coriwaq rs shows poor performance for no2 n due to the lack of knowledge about the detailed 3 step nitrification implemented in the reference rs model more investigation is required to improve the performance of coriwaq for rs in general the conceptual model results for water temperature and determinant concentrations are close to the ones simulated by the more detailed models mike 11 and infoworks rs except for no2 n in infoworks rs the good performance of the coriwaq model for both mike 11 and infoworks rs suggests that the model is qualified to implement further studies on uncertainty analysis impact analysis of scenarios such as changes in climate change landuse and management practices the current version of coriwaq uses hydrodynamic model results and physico biochemical processes which are included in the two software packages mike 11 and infoworks rs the flexibility is still limited at the level that users only can select the model structure corresponding to mike 11 or infoworks rs the user interface allows to import the data from mike 11 in the specific formats e g nwk res11 and infoworks rs in the format of csv files the conceptual model allows to implement model structure uncertainty analysis and construct or add new model structures by exchanging or editing the equations to simulate the physico biochemical processes the equations are added or changed in the c library and they need to be implemented in the matlab code which is used to write the c code the coming studies will involve further extension of coriwaq with more existing wq model structures and the physico biochemical process options should be shown in the graphical user interface coriwaq with a fully flexible model structure allows to implement model structure uncertainty analysis based on an ensemble of model structures therefore coriwaq is not an extension of the reference models or existing river water quality modelling packages it rather allows applications of use complementary to the reference models particularly applications that demand small model computational times whereas in this study the hd inputs were obtained from the full hd reference models these models may be replaced by conceptual water quantity models and integrated with the coriwaq model conceptual river water quantity modelling has been recently tested by wolfs et al 2015 to replace the detailed river hydrodynamic modelling for the same software packages as considered in this study mike 11 and infoworks rs with this replacement a flexible holistic conceptual river water model would be developed additionally the coriwaq parameters have been calibrated separately i e f tr and f depth or instantaneously f depth and f velocity based on the dependency of wq variables to the hydraulic characteristics in fact the wq variables are still related to the hydraulic characteristics directly or indirectly to generalize the calibration approach for different model structures a global calibration should be given as an option to the users moreover as the coriwaq model s performance has been tested with different flow conditions and wq parameter sets independently further investigations should consider these two factors concurrently acknowledgements this study has been made possible by a doctoral scholarship by the ku leuven interfaculty council for development co operation iro to the first author and a phd fellowship grant of research foundation flanders fwo for the second author the province of antwerp and the flemish environment agency vmm are gratefully acknowledged for providing the initial hydrological and hydrodynamic river models the measurement data on cross sections structures and water quality along the rivers in the molse neet catchment finally dhi and innovyze are responsively acknowledged for the provision of the licenses for the mike 11 and infoworks rs software packages appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 014 appendix a calibration results at the location of station 333 000 for the year 2008 image 1 appendix b scatter plots of the detailed reference model results and coriwaq results or observed data at observation station 333 000 along the molse neet river after application of different calibrated parameter sets image 2 
26396,physically based river water quality models are valuable tools for river basin management and planning however their long computational times pose many difficulties for applications that involve a large number of model iterations this paper addresses this problem by developing a faster surrogate conceptual model based on the detailed reference models the hydrodynamic information and water quality process equations from different detailed models are considered as ensembles in the developed model the model conceptualizes rivers using cascades of reservoirs and lumps the advection diffusion and physico biochemical processes we tested the model by comparing its performance for the molse neet river belgium with two popular reference models namely mike 11 and infoworks rs results show that the conceptual model performs equally well as the reference models but with simulation time 104 times faster the successful testing of this model opens a development avenue towards problem solving in the context of water quality control and management keywords calibration conceptual river water quality model coriwaq infoworks rs linear reservoir mike 11 validation software availability name conceptual river water quality coriwaq developers ingrid keupers thanh thuy nguyen and patrick willems primary contact patrick willems department of civil engineering hydraulics section kasteelpark arenberg 40 bus 2448 3001 heverlee belgium 32 16 32 16 58 patrick willems kuleuven be hardware required general purpose computer we recommend using a high speed processor and at least 4 gb of ram software required matlab a c compiler and dhi matlab toolbox programming language matlab and c availability upon request or through the following website of the hydraulics section ku leuven www kuleuven be hydr pwtools htm 1 introduction clean water is an increasing concern to our society because poor water quality both directly and indirectly causes many water related diseases united nations development programme 2006 announced that the people with water related diseases contribute to 50 of hospitalized patients in the world this type of diseases leads to nearly 20 of all deaths under 5 years old children who unicef 2009 to keep our surface waters clean it is crucial to effectively control and manage its quality which requires quantitative knowledge on water quantity and quality status both in time and space information on the water quality status can be obtained from monitoring systems shrestha and kazama 2008 haji gholizadeh et al 2016 however such systems are expensive and usually insufficient to cover the high spatio temporal variability of water quality variables bartley et al 2012 lessels and bishop 2015 an efficient alternative solution is the use of mathematical water quality wq models such models give decision makers insights in the cause effect relationships and guidance in the design of water resources management strategies gwp 2013 by comparing simulation results of different scenarios decision makers can easily derive evidence based decisions many wq models have been developed ranging from lumped conceptual models to more detailed physically based models from models describing single components of the integrated water system to holistic models chapra 2008 however simulations obtained by different models do not always lead to similar findings this is because different models have different model structures which are based on specific hypotheses and tested for selected case studies because of data limitations validation of the complete model structure is in most cases impossible for this reason it would be useful to take into account in the decision making process the uncertainty in the model results as a consequence of the model structure assumptions lindenschmidt et al 2007 nguyen and willems 2016 at current in most model applications just one software package is applied e g swat neitsch et al 2011 hec 5q willey 1986 or infoworks icm innovyze 2012 which involves one fixed model structure moreover the wq modules in most of the packages only accept the hydrodynamic results of their own packages as input the infoworks package solely allows to import networks and results from hec ras meanwhile mike11 eco lab dhi 2011 permits integration with other hydrodynamic models albeit only after modifications in the source code this inflexibility does not allow a complete analysis of the uncertainty in the wq simulation results therefore we plead for the development and application of decision support tools that involve a flexible model structure including different model structures and that allows to modify and combine physico biochemical processes from different physically based models river wq models as many types of models may involve a detailed representation of the modelled system detailed physically based models or a simplified one simplified models simplified modelling involves the use of a lumped conceptual model that is based on a lumping of the processes in space and conceptualized representation of the most dominant processes cox 2003 for instance tomcat and qual2e simulate physico biochemical processes with non uniform and steady flow at daily scale warn 1987 brown and barnwell 1987 with the advantage of short computational times these models allow implementation of sensitivity analysis parameter optimization and uncertainty analysis e g qual2kw pelletier et al 2006 lumped conceptual river wq models have the disadvantage that their processes and parameters are less explicit hence are less directly linked to measurable system properties meanwhile the detailed physically based models e g mike 11 dhi 2011 infoworks rs innovyze 2012 calhidra 3 0 cardona et al 2011 simulate the hydrodynamic advection diffusion and physico biochemical processes the long simulation time poses difficulties on applications that involve huge number of model runs iterations and or long term simulations such as model uncertainty analysis auto calibration real time control optimization etc the difficulties are larger for applications in larger river catchments for instance a water quality simulation for a 1 year period at a time step of 1 h for a river system with the total rivers length of 139 km and cross sections at distances of approximately 50 m takes about 4 days on a standard pc keupers and willems 2017 emulation modelling is known as a low order approximation of the detailed physically based models to reduce their computational complexity castelletti et al 2012 in this manner the most relevant variables are taken into account in the emulator the variables are identified by data based or structure based approaches in the data based approach the variables can be selected by a statistical measure of input output relationship e g partial mutual information bowden et al 2005 li et al 2015 and minimum redundancy maximum relevance peng et al 2005 hejazi and cai 2009 in the structure based approach a model formulation is derived for each possible combination of the variable replacement by constants e g crout et al 2009 reichert et al 2011 machac et al 2016 the model performances are evaluated by criteria such as residual sum of squares akaike s information criterion and bayesian information criterion the advantage of the former approach is that it has much less computational cost however the ignorance of physical mechanisms can lead to low accuracies when the sample size is small therefore this paper opts for the structure based approach but after simplification of existing model structures in order to reduce the computational times considering the above mentioned needs of a flexible physically based but reduced complexity model structure with reduced computational time this paper builds further on the recent advances by the development of the flexible version of the conceptual river water quality model coriwaq conceptual river water quality coriwaq is a hybrid of conceptual and physically based models to obtain more accurate simulations than the traditional lumped conceptual models but with shorter computational times than detailed physically based reference models accordingly hydrodynamic information for coriwaq is obtained from detailed physically based models the equations used to simulate the biochemical transformation processes in coriwaq are taken the same as in the physically based models the hydrodynamic characteristics in the coriwaq process equations are directly taken from the detailed full hydrodynamic reference models after applying correction factors considering the same inputs as the detailed reference models the lumped model is implemented for the motions of determinant concentrations the advection and diffusion processes along river segments are conceptualized using a reservoir type approach the physico biochemical processes along the river segments are presented by a set of equations with the incoming concentrations at the first cross sections and outcoming concentrations at the last cross sections of the corresponding river segments with this approach each river segment is characterized by one time series for each hydraulic characteristic and wq variable this approach has been widely used to transform precipitation to runoff pedersen et al 1980 chow et al 1988 weiler et al 2003 chetan and sudheer 2006 nourani et al 2009 groundwater recharge to discharge peters et al 2003 and for river or tidal river or sewer hydraulics wolfs et al 2015 meert et al 2016 wolfs and willems 2017 however there are only few studies on the application of lumped conceptual models for river wq modelling e g whitehead et al 1997 willems and berlamont 2002 radwan et al 2003 2004 willems 2008 two detailed physically based models implemented in the software packages mike 11 and infoworks rs hereafter denoted shortly as rs with different numerical schemes and different equations to simulate biochemical transformations are selected as reference models this research is a follow up of the initial coriwaq developments based on mike 11 coriwaq mike11 by keupers and willems 2017 coriwaq mike11 was applied to simulate the influence of combined sewage system overflow on river wq keupers et al 2015 and analyze the global analysis sensitivity of wq parameters keupers and willems 2015 in this paper coriwaq has been extended for infoworks rs coriwaq rs the developed model is tested by simulating the water quality for the molse neet river in belgium and by comparing its results with those obtained in both mike 11 and rs in the calibration stage the coriwaq parameters are calibrated to minimize the misfit between the wq output of coriwaq and the reference models in the validation stage the calibrated parameters are used to reproduce the wq variables and compare with those of the spatially more detailed reference models the calibration and validation are complemented within different flow conditions i e wet mean and dry conditions finally how robust the coriwaq model is to changes in the process settings is also tested 2 methodology 2 1 conceptual water quality model coriwaq in coriwaq rivers are internally divided into segments depending on the location of pollutant sources interested locations e g wq measurement stations as well as the maximum and minimum segment lengths set the location of the pollutant sources are defined in the detailed reference models the interested locations and the values of the maximum and minimum segment lengths are to be specified by the user where the latter can be different for different river reaches to satisfy the assumption that the hydraulic conditions do not vary that much along the river segments that correspond to each reservoir the maximum length of these segments should not be too large for rivers in which the flows are more uniform larger values can be given to the maximum segment length as for the minimum length this one should be high enough to sufficiently limit the conceptual model computational times it moreover should be larger than the smallest distances between 2 successive cross sections otherwise the coriwaq model would have the same spatial resolution as the reference models however the minimum segment length should be lower than the smallest distance between 2 successive pollutant sources based on the defined river segments the different wq processes i e advection diffusion and physico biochemical processes are in coriwaq modelled in a lumped way per segment to set up calibrate and validate the model the hydrodynamic hd and wq results from the reference models need to be provided in this study the hd inputs are obtained from the mike 11 and rs full hd reference models 2 1 1 processes 2 1 1 1 advection diffusion the motions of pollutants along the river are described by the advection diffusion ad equation this equation is in wq models typically solved by numerical schemes based on finite differences for example mike 11 uses an implicit finite difference scheme and a correction term to reduce the third order truncation error dhi 2011 rs employs the smooth molecular surface triangulator smart scheme gaskell and lau 1988 these methods require a huge number of repetitive calculations to achieve good approximations for the wq variables instead of solving the ad equation with such numerical methods willems and berlamont 2002 proposed to conceptualize the motion of pollutant determinants along waterways by routing through a cascade of linear reservoirs where each reservoir represents a river segment eq 1 1 c o u t a d t e 1 k c o u t t δ t 1 e 1 k c i n t δ where c o u t a d t is the determinant concentration water temperature at the segment outlet at time t c o u t t δ t is the determinant concentration water temperature at the outlet at previous calculation time step e g δ t 1 hour c i n t δ is the delayed determinant concentration water temperature at the segment inlet eq 1 has 2 parameters namely the recession constant k and the advective time delay δ k δ is the total residence time of the determinant in the river segment the advective time delay presents the advection motion of determinants along the reservoir and depends on the length of the reservoir and the flow velocity because the velocity is spatially and temporally variable along the rivers δ is estimated by dividing the segment length by the mean velocity where the spatial mean velocity is computed from the hd model results the water depth is calculated as the mean of the hydraulic radius in mike 11 or the hydraulic water depth in rs at all calculation nodes within the corresponding river segment 2 1 1 2 physico biochemical transformations with the available input the simulated physico biochemical transformation considers 7 wq variables in mike 11 water temperature t do bod5 ammonium nh4 n nitrate no3 n ortho phosphorus op p and particulate phosphorus pp p and 7 wq variables in rs t do bod5 nh4 n nitrite no2 n no3 n and organic nitrogen on n these wq variables are simulated in coriwaq within dissolved oxygen organic matter nitrogen and phosphorus solely in mike 11 cycles similar to those in the reference models mike 11 and rs the detailed information about these transformation processes as well as the equations can be found in dhi 2011 and innovyze 2012 below we present the main differences in physico biochemical transformation simulation between mike 11 and rs the first difference is the assumptions concerning the reaerated oxygen the reaerated oxygen is assumed to be inversely proportional to the water depth in rs while it is directly proportional to the water surface slope in mike 11 only rs allows to control the reaerated oxygen at the hydraulic structures by applying reaeration coefficients at the hydraulic structures secondly mike 11 considers the diurnal variations of water temperature photosynthesis and respiration sediment oxygen demand deposition and re suspension of particulate phosphorus release and absorption of dissolved phosphorus deposition and re suspension of suspended organic matter rs does not account for these processes if inputs on phytoplankton and sediment concentration are not provided which was the case in this study thirdly the on n is considered separately from bod5 and nitrification is divided in 2 steps i e from nh4 n to no2 n and from no2 n to no3 n in rs the denitrification only occurs when the do concentration is lower than 5 of the saturated do concentration meanwhile the on is not simulated as a wq variable in mike 11 it is considered inclusive in bod5 the nitrification goes directly from nh4 n to no3 n and denitrification happens simultaneously with the nitrification after the ad and biochemical transformation processes are simulated the concentration at the outlet of each conceptual reservoir is estimated as below eq 2 2 c o u t t e 1 k c o u t t δ t 1 e 1 k c i n t δ δ c δ t k δ where δ c δ t is the change of determinant concentration in one time unit due to physico biochemical processes the net effect of physico biochemical transformation processes on the wq variables is represented by reduction factors f reduc determined as the ratio between the wq variables at the outlet of the river segments considering the combined ad and physico biochemical processes concadwq and those considering only the ad process concad eq 3 f reduc can be larger or smaller than 1 f reduc is larger than 1 when the production processes are stronger than the consumption processes and vice versa 3 f r e d u c c o n c a d w q c o n c a d 2 1 2 calibration parameters model parameters are often calibrated to observed data however the uncertainties in wq modelling are dominated by the model input and model structure uncertainties van der perk 1997 håkanson 2000 van griensven and meixner 2006 improvement of the model simulation results hence should involve changes to the model parameters the model input data and the model structures for the proposed hybrid model coriwaq this procedure is tested by calibrating the coriwaq parameters to the wq simulation results of the spatially more detailed reference models i e mike 11 and rs models in this study input uncertainties and uncertainties in the results of the detailed reference models are not considered in this study coriwaq inputs are considered identical to the ones in the reference models and the reference models are assumed well calibrated or at least representative to the real river wq conditions previous section explained how the parameter values e g water depth residence time and velocity in the conceptual model can be initially assessed however due to the spatial lumping involved in the conceptual model the estimated parameter values of the wq processes may differ from the parameter values leading to the most accurate coriwaq results therefore correction factors are introduced which allow adjustment of the conceptual model based on the simulation results of the detailed models f tr correction factor to the estimated residence time f depth correction factor to the water depth f velocity only for coriwaq mike 11 correction factor to the velocity the residence time and the water depth influence the wq variables directly or indirectly through the physico biochemical processes in mike 11 the water depth directly affects the water temperature nh4 n bod5 pp p and do concentrations through heat transfer re suspension deposition photosynthesis respiration sod and reaeration processes in rs the water depth directly affects the do concentration and the water temperature through the reaeration and heat transfer processes respectively in both models water temperature is the primary wq variable and governs all other wq variables this shows that the water depth influences all wq variables indirectly through the water temperature the residence time directly influences all wq variables through the ad process therefore f tr was firstly calibrated followed by the calibration of the factors f depth and f velocity all these correction factors are given the value 1 as initial value no3 n is only indirectly affected by the water depth through nitrification therefore the no3 n output from mike 11 was selected as the wq variable for calibration of f tr because the bod5 and pp p concentrations are very sensitive not only to the water depth but also to the water velocity coriwaq mike 11 has f velocity as an additional correction factor both bod5 and pp p were chosen on the basis of the f depth and f velocity calibration in rs on n starts the nitrogen cycle and impairs the concentrations of nh4 n no2 n and no3 n through the nitrification process in other words on n is the primary wq variable of the nitrogen cycle the hydrolysis of on is indirectly influenced only by the water depth through the water temperature and was therefore used for the f tr calibration in rs for the f depth calibration water temperature is most sensitive to the water depth in rs and was selected for that calibration the correction factors were auto calibrated by the least squares method applied to the coriwaq f reduc coriwaq versus reference model f reduc theor time series results because the water depth and the velocity govern the bod5 and pp p concentration simultaneously latin hypercube sampling lhs was applied for the calibration considering 250 set values for the truncated standard normal distributions non negative values with a mean of 1 and a standard deviation of 1 1 considered for f depth and f velocity in coriwaq mike 11 for f depth in coriwaq rs and f tr in both models 250 values with equal interval were selected in the range 0 2 2 these ranges are based on the meaning of the correction factors and the experienced sensitivity of the model outputs to these factors 2 2 implementation for mike 11 and infoworks rs 2 2 1 mike 11 for implementing coriwaq associated with mike 11 coriwaq requires the mike 11 s hd and wq setup and simulation result files fig 1 the setup files include information on the river branches calculation nodes hydraulic structures mike 11 s network file geometrical data for the cross sections such as the wetted area hydraulic radius top width additional flooded area and resistance factor processed cross section file location and type of the model boundaries and the sources boundary file model input data time series file and wq parameters eco lab parameters file the hd results file contains model results on water levels discharges and velocities at each calculation node in mike 11 the hydraulic radius is used as proxy for water depth to simulate physical processes such as the heat transfer reaeration and deposition time series of hydraulic radius at each cross section were obtained from time series of water levels and the relationships between water level and hydraulic radius from the exported processed cross section file we developed a matlab code to automatically read and compute all this information input and calibration data from the mike 11 files calibrate the coriwaq parameters and conduct the conceptual model simulations and post processing river segments between emission locations boundaries or other interested locations were defined as reservoirs fig 2 the reservoir lengths were set following the range from minimum to maximum lengths chosen if the length of a reservoir was smaller than the minimum length the selected emission locations to divide the river in n segments were relocated if the length of a reservoir was larger than the maximum length the river segment was divided into 2 until its length was smaller than the maximum length the length of each reservoir was redefined by the distance between the nearest flow calculation nodes of the starting and ending chainages in the reference model the nodes used for calculating the water surface slope and the water depth were located between the nearest water level calculation nodes of the starting and ending chainages the water depth and velocity were obtained by averaging in space at the corresponding calculation nodes while the water surface slope was computed by distance weighed average of the slopes between these calculation nodes 2 2 2 infoworks rs for implementation of coriwaq based on rs the required data files also include hd and wq setup and result files this time exported in csv format fig 1 the setup files include information on river sections links connectivities spills rectangular conduits junction nodes and boundary nodes network file and values of the wq parameters wq file the rs results on water levels and wet areas at calculation nodes combined with geometric data at each river section were used to calculate the water depths these water depths were used to simulate wq processes such as heat transfer and reaeration as for mike 11 our code automatically reads and computes all the required information input and calibration data from the rs files implements the reservoir determination calibrates the coriwaq parameters and conducts the conceptual model simulations and post processing similar to mike 11 the reservoir determination was based on the locations of emission locations selected as the boundaries for the river segments and additional interested locations fig 2 if a reservoir length was smaller than the minimum length the emissions were relocated in rs if the reservoir length was larger than the maximum length the river segment was divided into more reservoirs with the lengths close to maximum length the chainages of the new reservoirs were defined based on the calculation nodes that result in reservoir lengths close to the calculated values different from mike 11 where the water level and flow calculation nodes are alternating they are overlapping in rs the selected calculation nodes for each reservoir thus were determined by the starting and ending chainages respectively 2 3 performance testing model validation was conducted by a split sample approach where the model was evaluated for the data period not considered during calibration the evaluation was based on the nash sutcliffe coefficient nse nash and sutcliffe 1970 eq 4 applied to the differences between the coriwaq and the reference model simulation results for the different wq variables nse can vary from negative infinity to 1 the higher the nse values are the better coriwaq performs 4 n s e j 1 i 1 n s i j o i j 2 i 1 n o i j o j 2 where n is the number of time steps s ij is the coriwaq simulation result at the outlet of reservoir j and at time i o ij is the simulation result of reference model at the outlet of reservoir j at time i o j is the average simulation result of reference model at the outlet of reservoir j in this study the coriwaq parameters were calibrated for each of three years with different flow conditions including wet mean and dry years in this way the influence of the flow conditions on the parameters could be assessed the calibrated values for f tr and f depth at the different reservoirs corresponding to each calibration year were plotted in comparative boxplots after calibration the models were validated for three other years representing wet mean and dry years the parameters that show the best model performance for these 3 validation years were selected for further performance testing beside this validation for different flow conditions the performance of coriwaq with different values of the wq process parameters was also investigated this was done by running simulations in both coriwaq and the reference models for a selected year wq parameter values in each simulation were defined by increasing and reducing each wq parameter value with 10 while keeping the values of other wq parameters fixed the coriwaq performances under such changing conditions were again evaluated based on the changes in nse values calculated for all river reservoirs and the seven wq variables 3 case study the molse neet river is located in the north east of the grote nete catchment with a sub catchment area of approximately 24 54 km2 fig 3 a the length of the river from the upstream location to the confluence downstream is 11 5 km the geometry of the molse neet river is determined by cross sections along the river approximately every 50 m and at the hydraulic structures besides geometric data the detailed reference models consider hydraulic structures along the network such as culverts and bridges which locally influence the flow characteristics the emission sources to the molse neet river include wastewater from domestic households industries and agriculture nguyen and willems 2016 these pollutant sources were considered as 13 point sources along the river in rs and mike 11 for some point sources the confluences with the river are located very closely these were merged together the parameters for the ad and biochemical transformation processes in mike 11 and rs were obtained from previous research nguyen and willems 2014 in that research calibration was done for heat transfer rate and reaeration rate coefficient at hydraulic structures to wq sampling data for the two years 2006 and 2007 the values of other wq parameters were selected from recommended values for similar rivers 4 results and discussions coriwaq mike 11 and coriwaq rs were implemented for the molse neet river using same pollutant sources processes and parameters as in the reference models besides the boundaries and point sources the number of reservoirs were defined by the location of wq measurement station 333 000 6 km from the upstream boundary given that the water level variations along the molse neet river do not vary that much over distances shorter than 500 m this length was chosen as the maximum length for the case study given that in the detailed reference models cross sections are implemented at approximately every 50 m this length was chosen as the minimum river segment length as a result the molse neet river was finally characterized by 30 reservoirs in coriwaq mike 11 and 29 reservoirs in coriwaq rs a scheme of the simplified river network model in coriwaq mike 11 is shown in fig 3b as an example with the results of the hydrological model pdm the rainfall runoff from the upstream sub catchment from 2000 to 2010 is shown in fig 4 comparing the runoff among these 11 years the period 2000 2003 can be considered as wet 2004 2008 with mean climate conditions and 2009 2010 as dry the three years 2001 2008 and 2009 including wet mean and dry conditions were selected for calibration the three years 2002 2006 and 2010 corresponding to wet mean and dry conditions were chosen for validation the simulation of each year for the case study i e 29 30 reservoirs with a time step of 1 h takes approximately 8 s on a laptop with intel core i7 3520m processor and installed memory ram 8 gb meanwhile the corresponding simulations in the reference models can take 18 h in rs with time step of 10 s or 43 h in mike 11 with time step of 15 s note that rs and mike 11 need shorter time steps to avoid model instabilities 4 1 calibration results water temperature is the driving force of many physico biochemical processes along rivers meanwhile the do concentration is the consequence of most physico biochemical processes and an important variable to indicate the wq state of a river therefore these two variables were selected as examples of the coriwaq mike 11 and coriwaq rs calibration results for the year 2008 which can be seen as a normal year fig 5 the calibration results of other wq variables are shown in appendix a both coriwaq mike 11 and coriwaq rs simulate water temperature very well with their simulation results almost overlapping with those of the corresponding mike 11 and rs models fig 5a and c when comparison is made with the observations mike 11 and coriwaq mike 11 capture the low water temperature better than rs and coriwaq rs do concentrations are also simulated well in coriwaq with small differences between them and the simulation results in mike 11 and rs fig 5b and d note that the diurnal variations in do concentration due to photosynthesis and respiration are simulated in mike 11 and coriwaq mike 11 therefore the ranges of do concentrations are in mike 11 and coriwaq mike 11 wider than in rs and coriwaq rs the consideration of the photosynthesis and respiration processes may also be one of the reasons why the do concentrations in mike 11 and coriwaq mike 11 match to the observed values better than in rs and coriwaq rs in general coriwaq mike 11 derives simulation results very close to the reference model results for water temperature no3 n nh4 n op p and pp p with rounded nse values of 1 at most of the reservoirs for all 3 years the model also simulates bod5 very well with all nse values higher than 0 89 do concentrations upstream and downstream along the river also match well with the mike 11 results with nse higher than 0 8 however do does not show good results at the central region of the river nse values are small at these locations and even negative at reservoir06 and reservoir07 for all 3 years this inaccuracy is expected to be the result of the inability of the model to take the influence of local steep water surface slopes into account in mike 11 the steep slopes upstream and downstream of hydraulic structures make reaerated oxygen very high and soar the do concentration at these locations this phenomenon is clearly shown at hydraulic structure knw30 at km 2 178 from the upstream boundary where the do concentration increases with 0 88 mg l on average however this influence of the hydraulic structures is eliminated by using an average value for the water surface slopes over space in coriwaq mike 11 coriwaq rs does simulate water temperature no3 n nh4 n and on n accurately with rounded nse values equal to 1 at the majority of the reservoirs for all 3 years the model also performs very well for do with all nse values larger than 0 77 at all reservoirs for all three years except at reservoir09 after calibration for the wet year 2001 due to the lumped approach per reservoir or river reach the underestimation in do concentration is propagated from upstream reservoirs to downstream ones a point source with high do concentration is located at the chainage 3 635 km upstream end of reservoir10 the effluent from this point source increases the do concentration in the river through the dilution effect and mitigates the influence of previous reservoirs for bod5 the results are good with high nse values the smallest nse value is 0 68 at the downstream end of reservoir28 after calibration for the dry year 2009 the nse values for bod5 concentration decrease from 1 at the upstream boundary of the river to 0 78 at the downstream end the no2 n calibration is not good for any of the three calibration years with overestimations that propagate from upstream to downstream no2 n concentrations increase through nitrification from nh4 n and decrease through nitrification to no3 n the overestimations in no2 n concentrations can be caused by overestimation of the nitrification rate from nh4 n to no2 n for the wet year 2001 20 reservoirs show nse value smaller than 0 70 while this is the case for 9 reservoirs for the mean year 2008 and 5 reservoirs for the dry year 2009 4 1 1 residence time factor ftr the comparative boxplots in fig 6 a show the calibrated values of f tr at reservoirs corresponding to mike 11 and rs the figure shows insignificant differences in f tr due to different flow conditions in coriwaq mike 11 f tr has values of 1 05 for the median in all three calibration years their interquartile ranges corresponding to 25 and 75 percentiles are similar with lengths of around 0 12 the f tr values show similar ranges i e 0 75 1 42 and 0 75 1 47 for the years 2008 and 2009 but a smaller range length of 0 89 for the year 2001 among the reservoirs reservoir23 has the highest f tr s values of 1 834 1 761 and 1 944 for the years 2008 2001 and 2009 the residence time along sub divisions of this segment is longer and more variable in both time and space than the one that is lumped in space in comparison with other reservoirs reservoir23 has smaller estimated residence times which also means that the time for nitrification from nh4 n to no3 n is short meanwhile the results of the reference model indicates a larger increase of no3 n concentrations hence high values of f tr need to be applied for this reservoir to minimize the differences between the 2 models in simulating changes in no3 n concentration due to the lumped approach overall f tr in coriwaq rs is smaller and less variable than the one in coriwaq mike 11 rs allows to set the culverts with the invert level lower than the river bed level meanwhile the invert level of culverts are not able to be lower than the river bed level in mike 11 to avoid generating cross sections with irregular shapes the cross sections at the upstream and downstream ends of the culverts obtain the elevation of the culvert culvert cross sections the additional cross sections taking into account the sediment deposition layer are added before and after the culvert cross sections these modifications increase the temporal and spatial variations of velocity along the segments where the culverts are located because of this the mean velocity along the segment does not represent the segment well and a factor had to be applied to obtain a more representative velocity value in coriwaq the correction factor is applied for tr which is calculated by the ratio between the segment length and the mean velocity the value of f tr is highest for the year 2009 with a median value of 1 03 in coriwaq rs the f tr values for the 3 years have similar 25 75 interquartile ranges with range lengths around 0 12 apart from outliers f tr at reservoirs varies in the same range of 0 88 1 15 for the years 2001 and 2009 while the upper quartile value is higher for the year 2008 i e 1 4 outlying f tr values occur for reservoir18 for the years 2001 and 2009 the reason for this outlier is again the lumped approach in comparison with the coriwaq mike 11 results the outlier is occurring at a different segment due to the difference in hd simulation results between mike 11 and rs and the wq variable selected for the f tr calibration 4 1 2 water depth factor fdepth for the water depth factor f depth median values of 1 01 are obtained for coriwaq mike 11 in all three calibration years fig 6b this means that the water depth is estimated very well by that model the 25 75 interquartile range length varies between 0 04 for the year 2009 and 0 06 0 07 for the years 2008 and 2001 some outliers are however noted the year 2001 has extremely high f depth values at reservoir09 and reservoir23 and at reservoir02 and reservoir23 for the year 2009 the f depth is always highest at reservoir23 the f depth is calibrated after f tr calibration so f depth includes the error in the residence time estimation the comparison between the detailed and lumped models is implemented for the ratio of residence time to water depth the variation in geometry of cross sections and the distribution of hydraulic structures along the river segments corresponding to the reservoirs09 reservoir02 and reservoir23 make the ratios along sub divisions of these reservoirs more variable in both time and space meanwhile the lumped model which considers mean residence times and water depths deals with lower values of that ratio in comparison with other reservoirs the reservoirs mentioned have high total residence times while values of f reduc coriwaq of bod5 and pp p concentrations are larger than the corresponding values of f reduc theor the increase in water depths after multiplying the estimated values with the calibrated f depth decreases the changing rates and results in lower differences between f reduc coriwaq and f reduc theor the range of f depth in coriwaq rs is similar to the one in coriwaq mike11 for coriwaq rs the water depth is estimated well with f depth values close to 1 i e 1 01 for the year 2008 1 03 for 2001 and 0 98 for 2009 their interquartile ranges are moreover very small with smallest range length of 0 05 for the year 2008 only one outlier extremely high value of f depth is obtained for all 3 years at reservoir23 the ratios of residence time to water depth along sub divisions of this segment are smaller and less variable in both time and space than the ones in the lumped model in comparison with other reservoirs reservoir23 has smaller ratio of residence time to water depth while the ratio of water temperature at the outlet to water temperature at the inlet is lower therefore high values of f depth need to be applied for this reservoir to minimize the differences between the two models in simulating changes in water temperature note that the water depth is estimated as the hydraulic radius in mike 11 while it is computed as the hydraulic depth in infoworks rs 4 1 3 velocity factor fvelocity for coriwaq mike 11 the velocity factor f velocity values at the 25 50 and 75 percentiles are equal to 1 in all three calibration years fig 6c however the values of f velocity in the upstream are highly different from 1 the highest value of 4 9 is observed at the reservoir09 with the calibration data of year 2009 the smallest value of 0 2 is obtained at the reservoir08 and reservoir02 with the calibration data of years 2008 and 2009 respectively this means that the mean velocity in space can be considered as a good representation for the river segments where the spatial variation of velocity is small there are many hydraulic structures in the upstream part that strongly affect the flow regime the resuspension and deposition of particulate organic matter and phosphorus are affected by such changes in flow velocity the total effects of velocity along river segments on the deposition resuspension can be higher or smaller than the mean velocity the total resuspension of particulate organic matter and phosphorus are larger than the estimated ones with the mean velocity in space at the reservoirs where the f velocity factor is larger than 1 meanwhile the total deposition of particulate organic matter and phosphorus are higher at the reservoirs where f velocity is smaller than 1 4 2 performance testing results after calibration the coriwaq models were validated considering different values of the conceptual model parameters these values were obtained in the calibration stage using independent simulation periods presenting wet mean and dry flow conditions years 2002 2006 and 2010 after that different wq process parameters in the reference models were applied to test how robust the coriwaq model is to changes in the process settings hence to check whether the conceptual models has not been over tuned for the correction factors 4 2 1 different time periods flow conditions the water samples at the wq observation station 333 000 were taken at different time instances however the specific time during the day was not mentioned for all measurements therefore the water temperature and determinant concentrations simulated in coriwaq and the reference models were averaged during the measurement days fig 7 a c e and fig 8 a c e show that the coriwaq mike 11 and coriwaq rs models can simulate water temperature approximately as good as the corresponding reference models with model results lying on the bisectors for all 3 years 2002 2006 and 2010 additionally the different values of coriwaq mike 11 parameters regardless to the flow conditions do not impair the water temperature validation results as for do the simulated do concentrations in coriwaq mike 11 are smaller than the ones in mike 11 at all times fig 7b d f because the f depth factor is calibrated to bod5 and pp p concentrations there exists a bias in the simulated do concentrations for different flow conditions the bias is the smallest for the wet year 2002 and largest for the dry year 2010 the systematic deviations are hypothesized to be the result of the underestimating reaerated oxygen due to the averaging of the water surface slopes see the earlier discussion on this in comparison with the f depth values for the years 2008 and 2001 the values for dry year 2009 are smaller and result in higher reaerated oxygen this is the reason why the parameters for the dry year 2009 are the best ones mitigating the underestimation in reaeration and shortening the differences between do concentrations in coriwaq mike 11 and mike 11 results for coriwaq rs the do concentrations are calibrated not as good as the water temperature and with the dry year 2010 showing the best match with the rs results fig 8b d f the differences in calibrated coriwaq rs parameters due to the flow conditions propagate to significant differences in results the parameters of the mean year 2008 issue the closest results for the years 2002 and 2006 while the parameters of the dry year 2009 result in overestimations of the do concentrations for all three validation years the f depth values are smallest for the year 2009 where they issue higher reaerated oxygen especially for the wet year 2002 with higher water depths the influence of f depth on do concentrations is stronger with higher concentration values in coriwaq than in rs the parameters for the dry year produce slightly better results for the do concentrations in coriwaq mike 11 meanwhile they result in high overestimation of do concentrations for the wet year 2002 in coriwaq rs the differences in the performance of coriwaq mike 11 and coriwaq rs in simulating do concentrations regardless to flow conditions are caused by their approaches to model oxygen reaeration as mentioned in section 2 1 1 2 for other wq variables the influence of flow conditions on their concentrations are insignificant appendix b it is noted in figs 7 and 8 and appendix b that the influence of the uncertainties in the input model structures and wq parameters on the model results is much higher than the influence of the errors due to model structure simplification especially for the do concentrations the mike 11 simulation results vary within a range similar to the observation data i e 7 2 9 7 9 2 and 7 9 8 mg l corresponding to the years 2002 2006 and 2010 without the consideration of photosynthesis and respiration in the oxygen cycle the ranges of do concentrations are smaller in the rs results i e 7 8 2 7 4 8 8 and 7 9 9 4 mg l respectively for the same years 2002 2006 and 2010 4 2 2 different wq parameters values for the evaluation of the coriwaq results after changes in the wq parameter values the simulations were implemented both in the conceptual and reference models given the high similarity in the coriwaq rs and coriwaq mike11 results results are only shown here for the rs results the mean year 2008 showed the best model performance for all 3 validation years and was selected for this analysis the smallest nse values among the 29 reservoirs of coriwaq rs are given in table 1 the wq parameters that result in significant changes in nse values i e the changes are larger than 0 2 were highlighted in the table when the wq parameters are increased or decreased with 10 the differences between coriwaq rs and rs results for water temperature no3 n nh4 n and on n are stable and very small among these wq variables water temperature has the smallest nse value of 0 996 the differences in bod5 results between coriwaq rs and rs remain with nse values between 0 7 and 0 8 when wq parameters change the f reduc theor factor calculated for bod5 varies from values smaller than 1 to higher than 1 this is opposed to the coriwaq mike 11 results where f reduc theor values are only smaller than 1 the latter result is because bod decay is involved as the only process this indicates that the reference model rs does not only consider organic matter decay but also other processes increasing dissolved organic matter to confirm that the inaccuracy in simulating bod5 is not due to the error in the f tr calibration a correction was applied to the residence time factor for the bod5 concentrations in year 2008 the nse values obtained for the bod5 concentrations at the outlet of the reservoirs increase this increase is highest at reservoir28 from 0 78 to 0 89 however the values of f reduc coriwaq are always smaller than 1 also note that there was lack of knowledge about the detailed process implementation in the reference detailed rs model the nse values for the do concentration are stable and always higher than 0 8 except at reservoir09 for an increase of 10 in f air the differences between the conceptual and reference model results are not so large with a mean difference of 0 168 mg l however the do concentration at this location is rather stable with a small standard deviation i e 0 169 mg l also leading to a very low nse value of 0 019 the increase in nse value when decreasing f air and the decrease when increasing f air confirm once more that the lumped approach cannot describe the do concentration with local remarkable changes very well the nse values for the no2 n concentrations are always small in these sensitivity simulations as was the case after model calibration except for the simulations with k no2 the nse value increases to 0 024 when k no2 decreases with 10 and reduces to 0 531 when k no2 increases with 10 the increase decrease in nse is due to the decreasing increasing influence of the no2 n no3 n transformation in rs and coriwaq rs when k no2 increases decreases in other words these nse changes are indicative for whether the nitrification amount from no2 n to no3 n is stronger or less strong in rs than in coriwaq rs this shows the existence of uncertainty in simulating the 3 step nitrification process in the conceptual model based on the rs model it is concluded that coriwaq performs very well with the simulation results remaining close for the different wq parameter sets tested except for no2 n changes in the nitrification process parameters do not affect the performance of the other wq variables significantly with unchanged nse values in this case study the no2 n concentrations are so small that the errors in the no2 n concentration in coriwaq do not influence conclusions regarding the river water quality state in terms of concentrations of no2 n or the other wq variables 5 conclusions and recommendations coriwaq was constructed successfully for the ad and biochemical process modelling in mike 11 and infoworks rs the model performs very well for the simulated wq variables the conceptual model development is supported by the reading of selected information in the files exported from the reference models and the auto calibration of the conceptual model parameters simulation of coriwaq on a laptop with intel core i7 3520m processor and installed memory ram 8 gb resulted in a reduction of the simulation time with a factor 104 for coriwaq in comparison with the detailed models the calibration of coriwaq was based on few primary wq variables no3 n bod5 and pp p for mike 11 and on n and water temperature for infoworks rs the calibration values of the coriwaq correction parameters i e f velocity solely coriwaq mike 11 f tr and f depth close to 1 indicate that the approach to estimate velocity residence time and water depth for river segments performed very well the use of a relationship between reaeration rate and water surface slope in mike 11 eco lab leads to overestimated do concentrations by applying a threshold to the water surface slope this disadvantage can be eliminated nguyen and willems 2016 therefore although the coriwaq mike 11 model performs better for the dry year the mean year should be selected to calibrate the conceptual model the coriwaq results present the total effect of hydraulic characteristics on the determinant concentrations along each river segment therefore the calibrated parameters of the conceptual model corresponding to the detailed reference model e g infoworks rs were used when combining or replacing physico biochemical processes with other detailed models e g adding photosynthesis process as in mike 11 eco lab it is noted that different hydrodynamic models can estimate the hydraulic terms distinctively e g water depth by hydraulic radius in mike 11 and hydraulic water depth in infoworks rs therefore the discrepancies between the simulation results should take these differences into account the equations to model water temperature photosynthesis and respiration in mike 11 should be implemented in coriwaq to improve the accuracy of do simulations in coriwaq rs moreover the uncertainty in estimating the determinant concentrations from pollutant sources model structure and wq parameters should be investigated the performance testing of the coriwaq model under different flow conditions and wq parameters resulted in the same range of nse values at the locations where the variation in do concentrations is small the high value of the transfer velocity at 20 c results in low nse values for the do concentration the absolute errors in the do concentrations are however limited coriwaq rs shows poor performance for no2 n due to the lack of knowledge about the detailed 3 step nitrification implemented in the reference rs model more investigation is required to improve the performance of coriwaq for rs in general the conceptual model results for water temperature and determinant concentrations are close to the ones simulated by the more detailed models mike 11 and infoworks rs except for no2 n in infoworks rs the good performance of the coriwaq model for both mike 11 and infoworks rs suggests that the model is qualified to implement further studies on uncertainty analysis impact analysis of scenarios such as changes in climate change landuse and management practices the current version of coriwaq uses hydrodynamic model results and physico biochemical processes which are included in the two software packages mike 11 and infoworks rs the flexibility is still limited at the level that users only can select the model structure corresponding to mike 11 or infoworks rs the user interface allows to import the data from mike 11 in the specific formats e g nwk res11 and infoworks rs in the format of csv files the conceptual model allows to implement model structure uncertainty analysis and construct or add new model structures by exchanging or editing the equations to simulate the physico biochemical processes the equations are added or changed in the c library and they need to be implemented in the matlab code which is used to write the c code the coming studies will involve further extension of coriwaq with more existing wq model structures and the physico biochemical process options should be shown in the graphical user interface coriwaq with a fully flexible model structure allows to implement model structure uncertainty analysis based on an ensemble of model structures therefore coriwaq is not an extension of the reference models or existing river water quality modelling packages it rather allows applications of use complementary to the reference models particularly applications that demand small model computational times whereas in this study the hd inputs were obtained from the full hd reference models these models may be replaced by conceptual water quantity models and integrated with the coriwaq model conceptual river water quantity modelling has been recently tested by wolfs et al 2015 to replace the detailed river hydrodynamic modelling for the same software packages as considered in this study mike 11 and infoworks rs with this replacement a flexible holistic conceptual river water model would be developed additionally the coriwaq parameters have been calibrated separately i e f tr and f depth or instantaneously f depth and f velocity based on the dependency of wq variables to the hydraulic characteristics in fact the wq variables are still related to the hydraulic characteristics directly or indirectly to generalize the calibration approach for different model structures a global calibration should be given as an option to the users moreover as the coriwaq model s performance has been tested with different flow conditions and wq parameter sets independently further investigations should consider these two factors concurrently acknowledgements this study has been made possible by a doctoral scholarship by the ku leuven interfaculty council for development co operation iro to the first author and a phd fellowship grant of research foundation flanders fwo for the second author the province of antwerp and the flemish environment agency vmm are gratefully acknowledged for providing the initial hydrological and hydrodynamic river models the measurement data on cross sections structures and water quality along the rivers in the molse neet catchment finally dhi and innovyze are responsively acknowledged for the provision of the licenses for the mike 11 and infoworks rs software packages appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 014 appendix a calibration results at the location of station 333 000 for the year 2008 image 1 appendix b scatter plots of the detailed reference model results and coriwaq results or observed data at observation station 333 000 along the molse neet river after application of different calibrated parameter sets image 2 
26397,simulations from climate models require bias correction prior to use in impact assessments or for statistical or dynamic downscaling to finer scales there are a number of different approaches to bias correction although most of these focus on a single variable for a particular location another limitation is that often corrections are only applied for one time scale of interest for example daily or monthly aggregated simulations despite evidence of different bias structures existing at different time scales recent works have sought to address each of these limitations and have led to the development of the multivariate recursive nesting bias correction mrnbc and multivariate recursive quantile matching nested bias correction mrqnbc methods an open source software toolkit in the r statistical computing environment has been developed to provide access to these methods several applications of the software are demonstrated in this paper along with information about the capabilities of the software keywords bias correction r package persistence multivariate distribution based quantile matching nested bias correction 1 introduction general circulation models gcms are becoming increasingly sophisticated with improvements in resolution and the range of processes that are represented as a result in many cases gcms are now more accurately referred to as earth system models esms because of the number of processes that can be simulated despite these improvements and overall confidence in the representation of large scale responses such as the global temperature sensitivity there remain a number of biases in gcm simulations particularly with respect to the hydrological cycle dynamic downscaling using regional climate models rcms can improve some of these biases because their finer resolutions allow topography to be more accurately represented and at the finest resolutions these models are now considered convection permitting however in many cases significant biases can persist either from the driving gcm or the rcm itself when gcm or rcm simulations are used in statistical downscaling approaches or directly for impact assessments bias correction of the variables of interest is required mehrotra and sharma 2006 2010 there is also an increasing interest in the need to correct gcm biases in the lateral boundary conditions used to downscale to finer resolutions using appropriately chosen rcms rocheta et al 2017 traditionally bias correction has focussed on correcting the representation of individual variables over a single time scale of interest e g daily or monthly data the underlying idea behind any bias correction approach is to identify the bias in a statistic or quantile for the current climate and correct the future climate under the assumption that the bias does not change over time daily or monthly standardization forms the most basic bias correction and is used to correct for systematic biases in the mean and variances of gcm simulations wilby et al 2004 nonparametric bias correction approaches include quantile matching correction factors and transfer functions based approaches e g arnell and reynard 1996 chen et al 2013 chiew and mcmahon 2002 teutschbein and seibert 2013 mpelasoka and chiew 2009 ines and hansen 2006 li et al 2010 piani et al 2010 wood et al 2004 these approaches address biases in the overall distribution of gcm simulations e g cayan et al 2008 li et al 2010 teutschbein and seibert 2013 maurer and hidalgo 2008 a variation of quantile matching named equidistant quantile matching eqm has been proposed by li et al 2010 analogous approaches have also been proposed to correct biases in the frequency spectrum of variables of interest nguyen et al 2016 2017 commonly used bias correction approaches generally consider a single time scale e g day month or year and do not consider the biases in persistence attributes when the bias corrected variables are aggregated averaged to longer time scales for example daily to monthly seasonal or annual observed and bias corrected statistics can be quite different johnson and sharma 2012 proposed the idea of nesting multiple time scales including a persistence correction in the standard bias correction procedure this was named nested bias correction nbc as the nesting was found to create artifacts in some of the statistics of the bias corrected series mehrotra and sharma 2012 proposed multiple repeats of the nested bias correction procedure to minimise the biases at all time scales this modification was termed recursive nested bias correction rnbc one of the criticisms of bias correction is that it is generally applied to each variable separately mehrotra and sharma 2015 2016 vrac and friederichs 2015 li et al 2014 as a result although it improves the statistics of each variable the physical dependencies between different variables are overlooked colette et al 2012 maraun 2013 for water resources impact assessments bias corrected time series of a number of different variables is often needed in catchment modelling for example precipitation and temperature potential evapotranspiration etc and statistical downscaling requires a number of bias corrected upper air variables a related problem can arise with poor representation of spatial correlations if variables are corrected separately for different locations hnilica et al 2016 hanel et al 2017 to address these problems multivariate bias correction approaches have been proposed piani and haerter 2012 proposed a bias correction approach to simultaneously correct temperature and precipitation this was achieved by correcting one time series e g precipitation conditionally to the bias corrected values of the other variable s time series e g temperature copula based methods have also been proposed to consider the joint dependence between variables or the spatial dependence across grids mao et al 2015 vrac and friederichs 2015 mehrotra and sharma 2015 proposed a parametric multivariate extension whilst a multivariate and multi timescale extension of quantile matching based nonparametric bias correction alternatives was suggested by mehrotra and sharma 2016 the latter approach corrects biases in probability space as well as the more routine distribution corrections the bias corrected simulations are shown to have the correct dependence between variables or locations as well as improved persistence structures and distributions over multiple time scales the mathematical relationships used in bias correction are developed based on historical and current climate observations and are applied in a future climate under the assumption of stationarity over time salvi et al 2016 the stationary bias assumption is questionable nahar et al 2017 buser et al 2009 ehret et al 2012 but efforts to improve on the assumption still need further development different researchers have recognised this issue and have suggested possible solutions grillakis et al 2016 provide a review of a few of these approaches in the context of bias correction while multivariate bias correction approach is attractive the multivariate setup requires estimation of additional parameters extremely large matrices and complex mathematical formulations making it inaccessible to practitioners wishing to use such methods for climate change impact assessments keeping in view these aspects a multivariate bias correction mbc software package has been developed in the r statistical computing environment the package includes both multivariate recursive nesting bias correction mrnbc and multivariate quantile matching recursive nesting bias correction mrqnbc approaches mehrotra and sharma 2015 2016 and makes it simple to implement both these approaches in a fairly simple manner this paper describes the software package and provides simple examples of its applications 2 multivariate bias correction the multivariate modelling of mehrotra and sharma 2015 2016 corrects the raw gcm simulations at pre defined time scales to match the observed distributional and persistence attributes at each of these time scales while we do not claim that the proposed multivariate modelling will keep the physical relationship among the climate variable intact it is certainly a better choice than the univariate bias correction option especially when dependence biases between the multiple variables of interest are present future gcm simulations have the same corrections applied which allows for changes in the statistical properties over time but corrects for biases assuming that the biases are stationary and smaller than the magnitude of changes that are projected chen et al 2015 the approach first applies a univariate bias correction at each time scale to match the observed statistical distributional attributes these univariate bias corrected time series are subsequently adjusted to reproduce the observed auto and cross dependence attributes at each time scale more details on the structure of the multivariate bias correction models are discussed in salas 1980 and mehrotra and sharma 2015 2016 and only a few key points related to multivariate and multi timescale aspects of mrnbc and mrqnbc approaches are discussed here in mbc we describe the main statistical attributes by mean and standard deviation or distribution and the dependence attributes by the lag 0 and lag 1 auto and cross correlations at four selected bias correction time scales daily monthly quarterly and annual the statistical attributes and time scales selected are arbitrary and the approach presented here could accommodate more generic representations of statistical attributes as well as time scales johnson and sharma 2012 mehrotra and sharma 2012 2015 the bias correction approach works in stages from univariate to multivariate and from one time scale to another at each time step it first corrects for the biases in statistics distribution of the individual variables once all variables are corrected for the distributional biases these are further corrected for the time and across variables dependence biases using a multivariate autoregressive model bias corrected time series is aggregated averaged to the next time scale and same procedure is repeated the multivariate component includes two auto regressive models the first has constant parameters over time and is used to represent the daily and annual time series whilst the second model uses periodic parameters to represent the monthly and seasonal characteristics salas 1980 3 multivariate bias correction package the multivariate bias correction mbc package for the r statistical software includes both multivariate bias correction approaches namely mrnbc and mrqnbc this section provides the general details of the implementation of the bias correction methods in the r package data requirements and form of the outputs from the package 3 1 general modelling philosophy mbc provides bias corrected climate model simulations which match observed statistics and then uses the correction factors for future simulations to demonstrate the fidelity of any bias correction method it is optimal to test the method using a split sample approach with data from the historical period used to estimate the bias properties and then test the bias corrections on a second sample of historical data borrowing from hydrological literature these two periods are referred to as calibration and verification here the general idea then is to divide the historical data into two or more periods to test and compare bias correction method performance once the best bias correction approach has been determined then the full historical record can be used to estimate the bias properties which are then applied to future simulations the verification stage can be thought as of pseudo future data and thus in the following section future is used as a generic term to refer to the simulations that are being corrected using bias statistics from another period of time there is often a mismatch between the spatial scale of climate model simulations and traditional meteorological observations e g rain gauge or temperature measurements therefore it is generally recommended that gridded data products are used to calculate the bias in climate model simulations alternatively reanalysis data may also be taken as the observation data set this is particularly relevant when upper air variables require bias correction prior to use in a downscaling scheme in what follows observations thus refer to a gridded data product derived either from station data or reanalysis raw simulations are those taken directly from a climate model and the corrected simulations are the products outputs of the bias correction although in the previous discussions the mrnbc and mrqnbc were motivated by the requirements for multiple climate variables the cross dependence that is corrected in both methods can also refer to spatial correlations of a single climate variable or some combination of both multi variate and spatial dependence the mathematical formulations are the same in either case so the methods are sufficiently flexible to address the important dependence structures for any particular problem for the treatment of zero values in the observed and modelled time series a very small value uniform random values between 0 and one multiplied by a small value 0 0001 and the value itself is added to the time series before the implementation of mbc cannon et al 2015 vrac et al 2016 cannon 2017 this procedure while practically has no effect on the actual values overcomes the problem of zeros in the time series 3 2 bias correction framework this section describes the general process that is required for bias correction using the mrnbc and mrqnbc approaches full details and relevant equations are available in mehrotra and sharma 2012 2015 and 2016 the univariate corrections for both methods are applied first with the multivariate corrections applied as the second step there are some differences in the univariate corrections for the mrnbc and mrqnbc due to the differences in the underlying correction philosophies parametric vs non parametric respectively fig 1 shows the correction flow chart 3 2 1 step 1 calculate observed and model statistics the required bias corrections statistics are calculated for the observed and gcm current and future climates for all variables and all locations using the daily time series this is done using the data falling within a moving window of pre specified width for example 31 days centred on the current day of interest rajagopalan and lall 1999 sharma and lall 1999 the required statistics are daily mean and standard deviation as well as the lag 0 and lag 1 auto and cross correlations matrices across the variables 3 2 2 step 2 correct current and future climate model statistics for individual variables for the mrnbc approach the biases in the raw current and future gcm simulations are corrected first for the mean by subtracting the current climate gcm mean and adding the observed mean this time series is then centred and the standard deviation of the residuals is corrected by dividing by the current climate gcm standard deviation and multiplying by the observed standard deviation the time series is then rescaled by adding back the mean which was removed when the time series was centred for the mrqnbc approach the commonly used quantile matching method is implemented empirical cumulative distribution functions cdfs are calculated for the observed data as well as the current and future gcm simulations for a given value in the future climate gcm simulations its cumulative probability is found from the cdf the difference in the values from the observed cdf and gcm current climate cdf for this cumulative probability is also calculated this difference is used to correct the future gcm value the process is repeated for the full future time series 3 2 3 step 3 correcting for auto and cross dependence the corrected time series from step 2 are standardised this residual time series is then bias corrected for a day t lag 1 and lag 0 auto and cross correlations the correction is based on a standard multivariate autoregressive model as discussed in mehrotra and sharma 2015 2016 the corrected residual time series is then rescaled by the mean and standard deviation 3 2 4 step 4 aggregate and correct longer time scales after correction at the daily time scale the time series is aggregated to longer time scales and steps 1 to 3 are repeated at each time scale note that for monthly and seasonal time scales the parameter estimation procedure is slightly different from what is used at daily and annual time scales for some variables the transformation to longer time scales is a simple averaging process whilst for other variables for example precipitation and evapotranspiration aggregation to a longer time scale involves summation 3 2 5 step 5 final bias correction steps a weighting factor can be derived to summarise the correction required at each time scale the raw gcm daily time series is multiplied by the weighting factor from each time scale to obtain the final bias corrected time series if the recursive scheme of mehrotra and sharma 2012 is required then the bias corrected time series is again treated as a raw gcm input and the process from step 1 to step 5 is repeated multiple times 3 3 mbc details mbc is implemented in a r shell and allows variants of mrnbc and mrqnbc bias correction approaches to be applied in a fairly simple manner 3 3 1 input data the package requires all general information on the modelling choices to be provided in the basic dat file in addition four data files need to be prepared these include observed and raw data files for calibration as well as verification period it is not necessary to have equal length of data or start date for raw and observed file either for calibration or verification periods the package also allows having different number of days in a month for example gcm simulations can have 28 days in february while observed data follows a leap year format as discussed above spatial dependence across multiple locations can be corrected instead of the cross dependence of multiple climate variables it is also fairly straightforward to use the package with three files observed and gcm rcm current and future climates raw data files which is the usual case with gcm rcm output in this case the observed verification period file will be same as observed calibration period file in this set up the observations can be used to compare the change in each variable in the future verification period compared to the historical climate i e by comparing observations with bias corrected future simulations the user is first required to pick either the mrnbc or mrqnbc correction options the user then has a choice of which statistics and time scales should be corrected choices for the bias statistics include mean mean and sd or full distribution for mrqnbc lag1 auto correlation lag0 and lag1 cross correlation the options for time nesting include daily monthly seasonal annual and tri annual the package also allows flexibility of applying bias correction either to daily or to monthly time series users are allowed to define their own seasons in addition to the names of the four data files the basic dat file also requires information about the number of years of data number of variables width of the moving window used to correct the daily data the number of repeats in the recursive procedure physical lower and upper limits on the variables whether data consider leap years or not and the split of calendar months across the seasons being modelled all the information is provided in a free format separated by spaces at present the package allows for a maximum of 150 years of daily data 30 variables 12 seasons and 31 day moving window 3 3 2 package outputs upon successful completion of the program 6 output files are generated two files provide the bias corrected time series for the current and future periods there are four summary results files which provide relevant statistics on the observed raw and bias corrected data for the current and future climates containing important statistics of 1 observed and raw data for calibration 2 observed and raw data for verification 3 observed and bias corrected data for calibration and 4 observed and bias corrected data for verification time periods as mentioned above for gcm rcm future climate data corrections the observed verification file would be same as the observed calibration file summary statistics include the means standard deviations skewness lag1 and lag2 auto correlations when multiple variables or locations are corrected then auto and lag1 cross correlations are also computed the package allows the users to look at raw and bias corrected statistics either in the form of a table or as plots at multiple time scales of interest finally the package also provides plots of the empirical cumulative probability distributions of the observed and raw and observed and bias corrected time series 4 presentation of results three sample data sets have been included with the package to provide guidance to users on the different options in the package the first dataset has synthetically generated daily time series for 7 variables that could represent typical atmospheric variables used in downscaling it also includes daily rainfall as one of the variable in order to show the capability of the packages in reproducing the number of wet dry days in the application demonstrated here the mrnbc bias correction approach has been used this example also demonstrates the use of unequal lengths of time series for calibration and verification periods the second dataset demonstrates an application with equal lengths of observed and gcm data for calibration current and verification time periods it uses 7 atmospheric variables and mrnbc as the bias correction approach the third datasets uses observed and ar1 model simulated monthly rainfall at 15 locations over sydney region for this final example the mrqnbc approach is used to correct spatio temporal dependence in the rainfall simulations 4 1 first dataset the first dataset consists of 7 synthetic daily time series that are representative of reanalysis data and raw gcm simulations these include geopotential heights at 925 and 700 hpa temperature depression at 500 hpa u wind at 850 hpa north south gradient of mean sea level pressure thickness of equivalent potential temperature at 500 850 hpa and precipitation the important feature of the 7th variable of the dataset precipitation is that it demonstrates the features of the bias correction for a time series that is highly skewed with many zero values the time series have been divided into two parts with unequal data lengths and different data lengths have been used to represent the availability of reanalysis and gcm simulations the dates for the years are arbitrary and used for illustration purposes and to demonstrate the ability of the software to handle leap years or fixed number of days in a month 66 years of daily data from 1881 to 1946 is used for model calibration whereas another subset of 70 years from 1947 to 2016 is used for model verification likewise a subset of 63 years of raw gcm data from 1891 to 1953 is used for model calibration and of 61 years from 1954 to 2014 is used for model verification the nested multivariate bias correction model has been used with the bias correction applied for daily monthly seasonal and annual time scales for all atmospheric variables average while for rainfall summation option at aggregated time scales is selected three seasons in a year have been chosen as shown in the information provided in the basic dat input file table 1 the number of seasons and their definition is arbitrary in this example and used for illustration purpose only for this example the lag0 cross and lag1 auto dependence options are selected table 1 presents the details of basic dat file used for this dataset the statistics for the calibration and verification periods are presented in tables 2 and 3 the scatter plots of statistics and distribution plots of time series of raw and bias corrected data for calibration and verification periods are presented in figs 2 and 3 respectively the bias correction approach performs well in reproducing the statistics of the reanalysis data in the gcm simulations at all time scales during calibration period table 2 and fig 2 it also reproduces the time distribution of variable at all selected time scales fig 3 some biases in the statistics during verification period are noted although lag1 cross correlations and skewness are not modelled explicitly the bias correction improves their representation in the corrected time series table 2 and fig 3 the observed rainfall time series exhibits very different number of wet days 34 as against the raw time series 76 for both calibration and verification time periods after bias correction these are matched with the observations 4 2 second dataset the second dataset includes four files of equal lengths with daily records of 7 atmospheric variables averaged over sydney australia obtained from the national center for environmental prediction ncep reanalysis2 data provided by the noaa cires climate diagnostics center boulder colorado usa from their web site at http www cdc noaa gov these variables include geopotential height at 925 hpa temperature depression at 700 and 500 hpa equivalent potential temperature at 500 hpa u and v winds at 500 hpa and north south gradient of mean sea level pressure likewise daily output of csiro s mk3 0 a2 gcm for these variables for the same time period is obtained from the atmospheric research division of the csiro australia a subset of 30 years of data from 1950 to 1979 is considered for model calibration while the remaining 30 years from 1980 to 2009 is used for the model verification the gcm data has fixed 28 days in february for all years whilst the reanalysis data follows the usual leap year format the basic information about the data start and end years number of years of data file names number of variables and type of bias correction model are given in basic dat file in a simple text format the bias correction model selected is a multivariate recursive nested bias correction mrnbc model with the option of bias correction in mean standard deviation lag1 auto and lag0 cross correlations at daily monthly and annual time scales four seasons in a year are considered more details on the information included in the basic dat file are provided in table 4 upon successful completion of the bias correction procedure four result files containing a few important statistics of the raw and bias corrected data are created tables 5 and 6 provide the snapshots of a part of these files for raw and bias corrected data for mean standard deviation and auto correlation statistics for calibration and verification periods respectively raw data tables 5a and 6a exhibits some biases in these statistics the bias correction model provides a near perfect fit for the calibration period and a reasonably good fit for the verification period similarly fig 4 provides scatter plots of scaled means standard deviations lag1 autocorrelation lag0 cross correlations and lag1 cross correlations of raw and bias corrected time series for these two periods for a good match all points should lie close to diagonal the model does a good job in reproducing these statistics during the verification period albeit with some scatter for some variables fig 5 presents empirical distribution plots of daily monthly seasonal and annual time series of reanalysis and raw and bias corrected gcm data for calibration and verification time periods for a selected variable specifically temperature depression at 700 hpa temperature depression is the difference of dewpoint and air temperature at that particular pressure level here again the model performs well at all time scales during calibration however exhibits some biases at longer time scales during verification the biases noted during verification period are a function of the differences in the behaviour of the observed and raw time series during calibration and verification time periods mrnbc like any other bias correction model works on the assumption that the biases are stationary and corrects the verification time series for the biases observed in the calibration time period as seen in these results the stationary bias assumption is questionable nahar et al 2017 buser et al 2009 ehret et al 2012 but efforts to improve on the assumption still need further development 4 3 third dataset the third dataset consists of observed and model simulated monthly rainfall time series 70 years of observed rainfall records from 1921 to 1990 at 15 locations around sydney is used to generate synthetic rainfall time series using an ar1 model this dataset does not directly relate to climate model simulations but is provided to demonstrate the capability of the bias correction model to correct for biases in any model data set as the generated rainfall comes from a univariate model with order one temporal dependence it is not expected to reproduce the observed spatio temporal dependence in the simulations two sample realisations of monthly rainfall each 70 years in length are generated these synthetic rainfall sequences are then corrected using the mrqnbc model with one realisation used to calibration of the bias correction model compared to the observed rainfall data the second synthetic series is then corrected in the verification time period the observed rainfall is used both for calibration as well as to assess the skill of the bias correction over the verification period bias correction is applied at monthly seasonal and annual time scale two seasons in a year are considered and since the variable being considered is rainfall the time aggregation option is also activated the structure of basic dat file used in this example is presented in table 7 while a few basic statistics of the observed raw and bias corrected data for the calibration and verification periods are presented in tables 8 and 9 a few scatter plots of statistics of raw and bias corrected data for calibration and verification periods are presented in fig 6 whereas empirical distribution plots of monthly seasonal and annual rainfall are presented in fig 7 as raw data comes from a model which is calibrated using the observed data there is a good match between means and standard deviations of observed and simulated raw data for calibration and verification time periods tables 8a and 9a and fig 6 and empirical distributions fig 7 however as expected auto and cross dependence attributes are not simulated well by the univariate rainfall generation model the bias correction model improves the representation of these observed attributes in the bias corrected time series 5 conclusion the majority of existing bias correction approaches focus on a single variable and consider corrections only over a single time scale of interest for example daily or monthly to address this gap open source software in r statistical computing environment has been developed to provide simple access to multivariate and multi timescale bias correction alternatives the software includes the option of running multivariate recursive nbc and two multivariate and timescale nested distribution function based approaches the package also allows the user to run these approaches as univariate alternatives with varying degree of complexities depending upon the requirement applications of the software along with information about the capabilities of the software are demonstrated using three sample datasets it is anticipated that the ease of running the software and the flexibility of exercising a wide variety of options will make it popular for practitioners carrying out impact assessments and researchers investigating downscaling methods software availability name of software package mbc developers raj mehrotra wrc civil and env engg unsw sydney e mail address raj mehrotra unsw edu au fiona johnson wrc civil and env engg unsw sydney e mail address f johnson unsw edu au ashish sharma wrc civil and env engg unsw sydney e mail address a sharma unsw edu au year first available 2018 hardware required standard pc for windows software required rgui or r studio availability and cost available free of charge software along with sample data and help file can be downloaded from the following website http www hydrology unsw edu au download software programme language written in r and fortran appendix asupplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 02 010 
26397,simulations from climate models require bias correction prior to use in impact assessments or for statistical or dynamic downscaling to finer scales there are a number of different approaches to bias correction although most of these focus on a single variable for a particular location another limitation is that often corrections are only applied for one time scale of interest for example daily or monthly aggregated simulations despite evidence of different bias structures existing at different time scales recent works have sought to address each of these limitations and have led to the development of the multivariate recursive nesting bias correction mrnbc and multivariate recursive quantile matching nested bias correction mrqnbc methods an open source software toolkit in the r statistical computing environment has been developed to provide access to these methods several applications of the software are demonstrated in this paper along with information about the capabilities of the software keywords bias correction r package persistence multivariate distribution based quantile matching nested bias correction 1 introduction general circulation models gcms are becoming increasingly sophisticated with improvements in resolution and the range of processes that are represented as a result in many cases gcms are now more accurately referred to as earth system models esms because of the number of processes that can be simulated despite these improvements and overall confidence in the representation of large scale responses such as the global temperature sensitivity there remain a number of biases in gcm simulations particularly with respect to the hydrological cycle dynamic downscaling using regional climate models rcms can improve some of these biases because their finer resolutions allow topography to be more accurately represented and at the finest resolutions these models are now considered convection permitting however in many cases significant biases can persist either from the driving gcm or the rcm itself when gcm or rcm simulations are used in statistical downscaling approaches or directly for impact assessments bias correction of the variables of interest is required mehrotra and sharma 2006 2010 there is also an increasing interest in the need to correct gcm biases in the lateral boundary conditions used to downscale to finer resolutions using appropriately chosen rcms rocheta et al 2017 traditionally bias correction has focussed on correcting the representation of individual variables over a single time scale of interest e g daily or monthly data the underlying idea behind any bias correction approach is to identify the bias in a statistic or quantile for the current climate and correct the future climate under the assumption that the bias does not change over time daily or monthly standardization forms the most basic bias correction and is used to correct for systematic biases in the mean and variances of gcm simulations wilby et al 2004 nonparametric bias correction approaches include quantile matching correction factors and transfer functions based approaches e g arnell and reynard 1996 chen et al 2013 chiew and mcmahon 2002 teutschbein and seibert 2013 mpelasoka and chiew 2009 ines and hansen 2006 li et al 2010 piani et al 2010 wood et al 2004 these approaches address biases in the overall distribution of gcm simulations e g cayan et al 2008 li et al 2010 teutschbein and seibert 2013 maurer and hidalgo 2008 a variation of quantile matching named equidistant quantile matching eqm has been proposed by li et al 2010 analogous approaches have also been proposed to correct biases in the frequency spectrum of variables of interest nguyen et al 2016 2017 commonly used bias correction approaches generally consider a single time scale e g day month or year and do not consider the biases in persistence attributes when the bias corrected variables are aggregated averaged to longer time scales for example daily to monthly seasonal or annual observed and bias corrected statistics can be quite different johnson and sharma 2012 proposed the idea of nesting multiple time scales including a persistence correction in the standard bias correction procedure this was named nested bias correction nbc as the nesting was found to create artifacts in some of the statistics of the bias corrected series mehrotra and sharma 2012 proposed multiple repeats of the nested bias correction procedure to minimise the biases at all time scales this modification was termed recursive nested bias correction rnbc one of the criticisms of bias correction is that it is generally applied to each variable separately mehrotra and sharma 2015 2016 vrac and friederichs 2015 li et al 2014 as a result although it improves the statistics of each variable the physical dependencies between different variables are overlooked colette et al 2012 maraun 2013 for water resources impact assessments bias corrected time series of a number of different variables is often needed in catchment modelling for example precipitation and temperature potential evapotranspiration etc and statistical downscaling requires a number of bias corrected upper air variables a related problem can arise with poor representation of spatial correlations if variables are corrected separately for different locations hnilica et al 2016 hanel et al 2017 to address these problems multivariate bias correction approaches have been proposed piani and haerter 2012 proposed a bias correction approach to simultaneously correct temperature and precipitation this was achieved by correcting one time series e g precipitation conditionally to the bias corrected values of the other variable s time series e g temperature copula based methods have also been proposed to consider the joint dependence between variables or the spatial dependence across grids mao et al 2015 vrac and friederichs 2015 mehrotra and sharma 2015 proposed a parametric multivariate extension whilst a multivariate and multi timescale extension of quantile matching based nonparametric bias correction alternatives was suggested by mehrotra and sharma 2016 the latter approach corrects biases in probability space as well as the more routine distribution corrections the bias corrected simulations are shown to have the correct dependence between variables or locations as well as improved persistence structures and distributions over multiple time scales the mathematical relationships used in bias correction are developed based on historical and current climate observations and are applied in a future climate under the assumption of stationarity over time salvi et al 2016 the stationary bias assumption is questionable nahar et al 2017 buser et al 2009 ehret et al 2012 but efforts to improve on the assumption still need further development different researchers have recognised this issue and have suggested possible solutions grillakis et al 2016 provide a review of a few of these approaches in the context of bias correction while multivariate bias correction approach is attractive the multivariate setup requires estimation of additional parameters extremely large matrices and complex mathematical formulations making it inaccessible to practitioners wishing to use such methods for climate change impact assessments keeping in view these aspects a multivariate bias correction mbc software package has been developed in the r statistical computing environment the package includes both multivariate recursive nesting bias correction mrnbc and multivariate quantile matching recursive nesting bias correction mrqnbc approaches mehrotra and sharma 2015 2016 and makes it simple to implement both these approaches in a fairly simple manner this paper describes the software package and provides simple examples of its applications 2 multivariate bias correction the multivariate modelling of mehrotra and sharma 2015 2016 corrects the raw gcm simulations at pre defined time scales to match the observed distributional and persistence attributes at each of these time scales while we do not claim that the proposed multivariate modelling will keep the physical relationship among the climate variable intact it is certainly a better choice than the univariate bias correction option especially when dependence biases between the multiple variables of interest are present future gcm simulations have the same corrections applied which allows for changes in the statistical properties over time but corrects for biases assuming that the biases are stationary and smaller than the magnitude of changes that are projected chen et al 2015 the approach first applies a univariate bias correction at each time scale to match the observed statistical distributional attributes these univariate bias corrected time series are subsequently adjusted to reproduce the observed auto and cross dependence attributes at each time scale more details on the structure of the multivariate bias correction models are discussed in salas 1980 and mehrotra and sharma 2015 2016 and only a few key points related to multivariate and multi timescale aspects of mrnbc and mrqnbc approaches are discussed here in mbc we describe the main statistical attributes by mean and standard deviation or distribution and the dependence attributes by the lag 0 and lag 1 auto and cross correlations at four selected bias correction time scales daily monthly quarterly and annual the statistical attributes and time scales selected are arbitrary and the approach presented here could accommodate more generic representations of statistical attributes as well as time scales johnson and sharma 2012 mehrotra and sharma 2012 2015 the bias correction approach works in stages from univariate to multivariate and from one time scale to another at each time step it first corrects for the biases in statistics distribution of the individual variables once all variables are corrected for the distributional biases these are further corrected for the time and across variables dependence biases using a multivariate autoregressive model bias corrected time series is aggregated averaged to the next time scale and same procedure is repeated the multivariate component includes two auto regressive models the first has constant parameters over time and is used to represent the daily and annual time series whilst the second model uses periodic parameters to represent the monthly and seasonal characteristics salas 1980 3 multivariate bias correction package the multivariate bias correction mbc package for the r statistical software includes both multivariate bias correction approaches namely mrnbc and mrqnbc this section provides the general details of the implementation of the bias correction methods in the r package data requirements and form of the outputs from the package 3 1 general modelling philosophy mbc provides bias corrected climate model simulations which match observed statistics and then uses the correction factors for future simulations to demonstrate the fidelity of any bias correction method it is optimal to test the method using a split sample approach with data from the historical period used to estimate the bias properties and then test the bias corrections on a second sample of historical data borrowing from hydrological literature these two periods are referred to as calibration and verification here the general idea then is to divide the historical data into two or more periods to test and compare bias correction method performance once the best bias correction approach has been determined then the full historical record can be used to estimate the bias properties which are then applied to future simulations the verification stage can be thought as of pseudo future data and thus in the following section future is used as a generic term to refer to the simulations that are being corrected using bias statistics from another period of time there is often a mismatch between the spatial scale of climate model simulations and traditional meteorological observations e g rain gauge or temperature measurements therefore it is generally recommended that gridded data products are used to calculate the bias in climate model simulations alternatively reanalysis data may also be taken as the observation data set this is particularly relevant when upper air variables require bias correction prior to use in a downscaling scheme in what follows observations thus refer to a gridded data product derived either from station data or reanalysis raw simulations are those taken directly from a climate model and the corrected simulations are the products outputs of the bias correction although in the previous discussions the mrnbc and mrqnbc were motivated by the requirements for multiple climate variables the cross dependence that is corrected in both methods can also refer to spatial correlations of a single climate variable or some combination of both multi variate and spatial dependence the mathematical formulations are the same in either case so the methods are sufficiently flexible to address the important dependence structures for any particular problem for the treatment of zero values in the observed and modelled time series a very small value uniform random values between 0 and one multiplied by a small value 0 0001 and the value itself is added to the time series before the implementation of mbc cannon et al 2015 vrac et al 2016 cannon 2017 this procedure while practically has no effect on the actual values overcomes the problem of zeros in the time series 3 2 bias correction framework this section describes the general process that is required for bias correction using the mrnbc and mrqnbc approaches full details and relevant equations are available in mehrotra and sharma 2012 2015 and 2016 the univariate corrections for both methods are applied first with the multivariate corrections applied as the second step there are some differences in the univariate corrections for the mrnbc and mrqnbc due to the differences in the underlying correction philosophies parametric vs non parametric respectively fig 1 shows the correction flow chart 3 2 1 step 1 calculate observed and model statistics the required bias corrections statistics are calculated for the observed and gcm current and future climates for all variables and all locations using the daily time series this is done using the data falling within a moving window of pre specified width for example 31 days centred on the current day of interest rajagopalan and lall 1999 sharma and lall 1999 the required statistics are daily mean and standard deviation as well as the lag 0 and lag 1 auto and cross correlations matrices across the variables 3 2 2 step 2 correct current and future climate model statistics for individual variables for the mrnbc approach the biases in the raw current and future gcm simulations are corrected first for the mean by subtracting the current climate gcm mean and adding the observed mean this time series is then centred and the standard deviation of the residuals is corrected by dividing by the current climate gcm standard deviation and multiplying by the observed standard deviation the time series is then rescaled by adding back the mean which was removed when the time series was centred for the mrqnbc approach the commonly used quantile matching method is implemented empirical cumulative distribution functions cdfs are calculated for the observed data as well as the current and future gcm simulations for a given value in the future climate gcm simulations its cumulative probability is found from the cdf the difference in the values from the observed cdf and gcm current climate cdf for this cumulative probability is also calculated this difference is used to correct the future gcm value the process is repeated for the full future time series 3 2 3 step 3 correcting for auto and cross dependence the corrected time series from step 2 are standardised this residual time series is then bias corrected for a day t lag 1 and lag 0 auto and cross correlations the correction is based on a standard multivariate autoregressive model as discussed in mehrotra and sharma 2015 2016 the corrected residual time series is then rescaled by the mean and standard deviation 3 2 4 step 4 aggregate and correct longer time scales after correction at the daily time scale the time series is aggregated to longer time scales and steps 1 to 3 are repeated at each time scale note that for monthly and seasonal time scales the parameter estimation procedure is slightly different from what is used at daily and annual time scales for some variables the transformation to longer time scales is a simple averaging process whilst for other variables for example precipitation and evapotranspiration aggregation to a longer time scale involves summation 3 2 5 step 5 final bias correction steps a weighting factor can be derived to summarise the correction required at each time scale the raw gcm daily time series is multiplied by the weighting factor from each time scale to obtain the final bias corrected time series if the recursive scheme of mehrotra and sharma 2012 is required then the bias corrected time series is again treated as a raw gcm input and the process from step 1 to step 5 is repeated multiple times 3 3 mbc details mbc is implemented in a r shell and allows variants of mrnbc and mrqnbc bias correction approaches to be applied in a fairly simple manner 3 3 1 input data the package requires all general information on the modelling choices to be provided in the basic dat file in addition four data files need to be prepared these include observed and raw data files for calibration as well as verification period it is not necessary to have equal length of data or start date for raw and observed file either for calibration or verification periods the package also allows having different number of days in a month for example gcm simulations can have 28 days in february while observed data follows a leap year format as discussed above spatial dependence across multiple locations can be corrected instead of the cross dependence of multiple climate variables it is also fairly straightforward to use the package with three files observed and gcm rcm current and future climates raw data files which is the usual case with gcm rcm output in this case the observed verification period file will be same as observed calibration period file in this set up the observations can be used to compare the change in each variable in the future verification period compared to the historical climate i e by comparing observations with bias corrected future simulations the user is first required to pick either the mrnbc or mrqnbc correction options the user then has a choice of which statistics and time scales should be corrected choices for the bias statistics include mean mean and sd or full distribution for mrqnbc lag1 auto correlation lag0 and lag1 cross correlation the options for time nesting include daily monthly seasonal annual and tri annual the package also allows flexibility of applying bias correction either to daily or to monthly time series users are allowed to define their own seasons in addition to the names of the four data files the basic dat file also requires information about the number of years of data number of variables width of the moving window used to correct the daily data the number of repeats in the recursive procedure physical lower and upper limits on the variables whether data consider leap years or not and the split of calendar months across the seasons being modelled all the information is provided in a free format separated by spaces at present the package allows for a maximum of 150 years of daily data 30 variables 12 seasons and 31 day moving window 3 3 2 package outputs upon successful completion of the program 6 output files are generated two files provide the bias corrected time series for the current and future periods there are four summary results files which provide relevant statistics on the observed raw and bias corrected data for the current and future climates containing important statistics of 1 observed and raw data for calibration 2 observed and raw data for verification 3 observed and bias corrected data for calibration and 4 observed and bias corrected data for verification time periods as mentioned above for gcm rcm future climate data corrections the observed verification file would be same as the observed calibration file summary statistics include the means standard deviations skewness lag1 and lag2 auto correlations when multiple variables or locations are corrected then auto and lag1 cross correlations are also computed the package allows the users to look at raw and bias corrected statistics either in the form of a table or as plots at multiple time scales of interest finally the package also provides plots of the empirical cumulative probability distributions of the observed and raw and observed and bias corrected time series 4 presentation of results three sample data sets have been included with the package to provide guidance to users on the different options in the package the first dataset has synthetically generated daily time series for 7 variables that could represent typical atmospheric variables used in downscaling it also includes daily rainfall as one of the variable in order to show the capability of the packages in reproducing the number of wet dry days in the application demonstrated here the mrnbc bias correction approach has been used this example also demonstrates the use of unequal lengths of time series for calibration and verification periods the second dataset demonstrates an application with equal lengths of observed and gcm data for calibration current and verification time periods it uses 7 atmospheric variables and mrnbc as the bias correction approach the third datasets uses observed and ar1 model simulated monthly rainfall at 15 locations over sydney region for this final example the mrqnbc approach is used to correct spatio temporal dependence in the rainfall simulations 4 1 first dataset the first dataset consists of 7 synthetic daily time series that are representative of reanalysis data and raw gcm simulations these include geopotential heights at 925 and 700 hpa temperature depression at 500 hpa u wind at 850 hpa north south gradient of mean sea level pressure thickness of equivalent potential temperature at 500 850 hpa and precipitation the important feature of the 7th variable of the dataset precipitation is that it demonstrates the features of the bias correction for a time series that is highly skewed with many zero values the time series have been divided into two parts with unequal data lengths and different data lengths have been used to represent the availability of reanalysis and gcm simulations the dates for the years are arbitrary and used for illustration purposes and to demonstrate the ability of the software to handle leap years or fixed number of days in a month 66 years of daily data from 1881 to 1946 is used for model calibration whereas another subset of 70 years from 1947 to 2016 is used for model verification likewise a subset of 63 years of raw gcm data from 1891 to 1953 is used for model calibration and of 61 years from 1954 to 2014 is used for model verification the nested multivariate bias correction model has been used with the bias correction applied for daily monthly seasonal and annual time scales for all atmospheric variables average while for rainfall summation option at aggregated time scales is selected three seasons in a year have been chosen as shown in the information provided in the basic dat input file table 1 the number of seasons and their definition is arbitrary in this example and used for illustration purpose only for this example the lag0 cross and lag1 auto dependence options are selected table 1 presents the details of basic dat file used for this dataset the statistics for the calibration and verification periods are presented in tables 2 and 3 the scatter plots of statistics and distribution plots of time series of raw and bias corrected data for calibration and verification periods are presented in figs 2 and 3 respectively the bias correction approach performs well in reproducing the statistics of the reanalysis data in the gcm simulations at all time scales during calibration period table 2 and fig 2 it also reproduces the time distribution of variable at all selected time scales fig 3 some biases in the statistics during verification period are noted although lag1 cross correlations and skewness are not modelled explicitly the bias correction improves their representation in the corrected time series table 2 and fig 3 the observed rainfall time series exhibits very different number of wet days 34 as against the raw time series 76 for both calibration and verification time periods after bias correction these are matched with the observations 4 2 second dataset the second dataset includes four files of equal lengths with daily records of 7 atmospheric variables averaged over sydney australia obtained from the national center for environmental prediction ncep reanalysis2 data provided by the noaa cires climate diagnostics center boulder colorado usa from their web site at http www cdc noaa gov these variables include geopotential height at 925 hpa temperature depression at 700 and 500 hpa equivalent potential temperature at 500 hpa u and v winds at 500 hpa and north south gradient of mean sea level pressure likewise daily output of csiro s mk3 0 a2 gcm for these variables for the same time period is obtained from the atmospheric research division of the csiro australia a subset of 30 years of data from 1950 to 1979 is considered for model calibration while the remaining 30 years from 1980 to 2009 is used for the model verification the gcm data has fixed 28 days in february for all years whilst the reanalysis data follows the usual leap year format the basic information about the data start and end years number of years of data file names number of variables and type of bias correction model are given in basic dat file in a simple text format the bias correction model selected is a multivariate recursive nested bias correction mrnbc model with the option of bias correction in mean standard deviation lag1 auto and lag0 cross correlations at daily monthly and annual time scales four seasons in a year are considered more details on the information included in the basic dat file are provided in table 4 upon successful completion of the bias correction procedure four result files containing a few important statistics of the raw and bias corrected data are created tables 5 and 6 provide the snapshots of a part of these files for raw and bias corrected data for mean standard deviation and auto correlation statistics for calibration and verification periods respectively raw data tables 5a and 6a exhibits some biases in these statistics the bias correction model provides a near perfect fit for the calibration period and a reasonably good fit for the verification period similarly fig 4 provides scatter plots of scaled means standard deviations lag1 autocorrelation lag0 cross correlations and lag1 cross correlations of raw and bias corrected time series for these two periods for a good match all points should lie close to diagonal the model does a good job in reproducing these statistics during the verification period albeit with some scatter for some variables fig 5 presents empirical distribution plots of daily monthly seasonal and annual time series of reanalysis and raw and bias corrected gcm data for calibration and verification time periods for a selected variable specifically temperature depression at 700 hpa temperature depression is the difference of dewpoint and air temperature at that particular pressure level here again the model performs well at all time scales during calibration however exhibits some biases at longer time scales during verification the biases noted during verification period are a function of the differences in the behaviour of the observed and raw time series during calibration and verification time periods mrnbc like any other bias correction model works on the assumption that the biases are stationary and corrects the verification time series for the biases observed in the calibration time period as seen in these results the stationary bias assumption is questionable nahar et al 2017 buser et al 2009 ehret et al 2012 but efforts to improve on the assumption still need further development 4 3 third dataset the third dataset consists of observed and model simulated monthly rainfall time series 70 years of observed rainfall records from 1921 to 1990 at 15 locations around sydney is used to generate synthetic rainfall time series using an ar1 model this dataset does not directly relate to climate model simulations but is provided to demonstrate the capability of the bias correction model to correct for biases in any model data set as the generated rainfall comes from a univariate model with order one temporal dependence it is not expected to reproduce the observed spatio temporal dependence in the simulations two sample realisations of monthly rainfall each 70 years in length are generated these synthetic rainfall sequences are then corrected using the mrqnbc model with one realisation used to calibration of the bias correction model compared to the observed rainfall data the second synthetic series is then corrected in the verification time period the observed rainfall is used both for calibration as well as to assess the skill of the bias correction over the verification period bias correction is applied at monthly seasonal and annual time scale two seasons in a year are considered and since the variable being considered is rainfall the time aggregation option is also activated the structure of basic dat file used in this example is presented in table 7 while a few basic statistics of the observed raw and bias corrected data for the calibration and verification periods are presented in tables 8 and 9 a few scatter plots of statistics of raw and bias corrected data for calibration and verification periods are presented in fig 6 whereas empirical distribution plots of monthly seasonal and annual rainfall are presented in fig 7 as raw data comes from a model which is calibrated using the observed data there is a good match between means and standard deviations of observed and simulated raw data for calibration and verification time periods tables 8a and 9a and fig 6 and empirical distributions fig 7 however as expected auto and cross dependence attributes are not simulated well by the univariate rainfall generation model the bias correction model improves the representation of these observed attributes in the bias corrected time series 5 conclusion the majority of existing bias correction approaches focus on a single variable and consider corrections only over a single time scale of interest for example daily or monthly to address this gap open source software in r statistical computing environment has been developed to provide simple access to multivariate and multi timescale bias correction alternatives the software includes the option of running multivariate recursive nbc and two multivariate and timescale nested distribution function based approaches the package also allows the user to run these approaches as univariate alternatives with varying degree of complexities depending upon the requirement applications of the software along with information about the capabilities of the software are demonstrated using three sample datasets it is anticipated that the ease of running the software and the flexibility of exercising a wide variety of options will make it popular for practitioners carrying out impact assessments and researchers investigating downscaling methods software availability name of software package mbc developers raj mehrotra wrc civil and env engg unsw sydney e mail address raj mehrotra unsw edu au fiona johnson wrc civil and env engg unsw sydney e mail address f johnson unsw edu au ashish sharma wrc civil and env engg unsw sydney e mail address a sharma unsw edu au year first available 2018 hardware required standard pc for windows software required rgui or r studio availability and cost available free of charge software along with sample data and help file can be downloaded from the following website http www hydrology unsw edu au download software programme language written in r and fortran appendix asupplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 02 010 
26398,effective infiltration ei is the amount of precipitation infiltrating into the soil and recharging the aquifers ei is estimated using direct or indirect methods or using water balance models direct and indirect methods lead to biased ei estimates since based on simplified schemas of groundwater bodies and of their recharge mechanisms water balance models include different processes and variables but they are seldom applied due to the limited availability of the input data particularly at regional scales we propose a method for ei estimation over large areas based on a monthly water balance model exploiting open source software and free open data the model integrates procedures to estimate ei and other water balance components accounting for the uncertainty of input data the model is calibrated in the central apennines italy where ei reference values are available from the literature and later applied in the alps where regional ei estimates are missing keywords effective infiltration hydrogeological balance free open data open software alps central apennines hydro bm list of variables used in the model variable description units aet actual evapotranspiration mm awc sub subsoil water capacity mm m 1 awc top topsoil water capacity mm m 1 coef inf effective infiltration coefficient dimensionless d number of days in a month d mean monthly hours of daylight units of 12 h deepstor amount of water stored in depth used as a proxy of the effective infiltration ei mm deepstor m deepstor in the considered month m mm deepstor m 1 deepstor in the previous month m 1 mm dr depth to rock m dro direct runoff mm drofact fraction of direct runoff for a given precipitation returned directly by the hydrological system dimensionless elev elevation m a s l meltmax factor controlling the dependence of snow melt fraction sm with respect to elevation dimensionless meltmax 0 meltmax for elevation of 0 m dimensionless meltmax 1000 meltmax for elevation of 1000 m dimensionless p mean monthly precipitation mm p aet precipitation available for the evapotranspiration mm pet potential evapotranspiration mm p max maximum mean monthly precipitation mm p min minimum mean monthly precipitation mm pmh hydrogeological type of parental material dimensionless pmpe if positive it is the water that can potentially infiltrate into the soil if negative it is the water potentially withdrawable by evapotranspiration from the soil mm prestor soil moisture storage for the previous month mm pmhfact fraction of water infiltrating in a month into depth dimensionless p rain mean monthly precipitation in the form of rainfall mm p snow mean monthly precipitation in the form of snowfall mm remain water remaining as surface soil water storage in a month this is transferred to the successive month for water balance calculations mm remain m remain in the considered month m mm remain m 1 remain in the successive month m 1 rfact fraction of water becoming runoff in a month after infiltration and evapotranspiration withdrawal dimensionless ro runoff generated from the soil water storage mm rototal monthly total runoff mm sm snow melt fraction mm snowstor snow storage mm st soil moisture storage mm stw soil moisture storage withdrawal mm sur excess water that cannot either infiltrate or evapotranspirate mm t mean monthly temperature c text srf dom dominant surface textural class dimensionless t max maximum mean monthly temperature c t min minimum mean monthly temperature c t rain above this temperature threshold all precipitation is rain c t snow below this temperature threshold all precipitation is snow c t snow0 temperature threshold for elevation of 0 m below this threshold all precipitation is snow c t snow1000 temperature threshold for elevation over 1000 m below this threshold all precipitation is snow c up max maximum precipitation uncertainty mm up min minimum precipitation uncertainty mm ut max maximum temperature uncertainty c ut min minimum temperature uncertainty c whc water holding capacity mm wt saturated water vapour density term g m3 year prec total precipitation in a year mm 1 introduction effective infiltration ei is the amount of meteoric water per unit surface that yearly infiltrates into the soil and recharges aquifers ei reflects the capability of a hydrogeological complex to absorb meteoric water estimating this parameter in a given area requires a detailed knowledge of its lithological morphological and climatic setting mastrorillo et al 2009 ei can be estimated using direct or indirect methods or using water balance models the direct method consists of estimating ei by dividing the average volume of water discharged by the springs in one year by their recharge area pioneering analyses of the infiltration process for aquifer recharge using direct methods were done at yearly aronis et al 1961 burdon and papakis 1961 burdon 1965 and sub yearly kessler 1957 1965 scales starting from the 50s boni and bono 1982 and boni et al 1986 applied the direct method to estimate the ei of the main aquifers in central italy the method has been successively applied by mastrorillo et al 2009 and mastrorillo and petitta 2010 for carbonate aquifers in the umbria marche region central italy mastrorillo et al 2009 suggest that the application of ei direct estimation is difficult and that this may lead to biased estimates mainly because hydrogeological bodies may be characterized by deep groundwater circulation with unknown springs and unknown hydraulic connections with other groundwater bodies the direct method also suffers some limitations due to the limited accuracy when defining the boundaries of aquifer recharge areas and due to the scarce availability of the spring discharge data as highlighted by mastrorillo et al 2009 in these conditions the effective infiltration can be deduced indirectly as a percentage of precipitation i e indirect method this indirect method is based on the use of an empirical coefficient named the effective infiltration coefficient eic this dimensionless coefficient ranges between 0 and 1 and varies for different lithology the eic is defined as the percentage of precipitation that infiltrates at depth and it is estimated at the catchment scale by dividing the volume of spring discharge by the volume of total rainfall drogue 1971 bonacci 2001 civita 2005 mastrorillo et al 2009 allocca et al 2014 such indirect effective infiltration estimation is biased since it is based on a rough parameterization of the permeability of the different lithologies outcropping in the recharge areas moreover values of eic are in general inferred from the literature and do not consider the real lithological setting of the investigated recharge areas the suitability of this indirect effective infiltration estimation method depends on the reliability of the precipitation values for which long and complete time series are rare or partial particularly in the mountainous areas where a significant part of the aquifer recharge process occurs where lacking these data are obtained through elevation precipitation correlation methods that albeit extremely rigorous may fail to estimate the actual precipitation values and their seasonal distributions the other methods proposed in the literature are based on the use of infiltration models at different spatial and temporal scales e g balance models hydrologic models etc these methods are more demanding in terms of input data e g boni et al 1982 guowei and yifeng 1991 kostka and holko 1994 jain 2012 that are rare in particular when modelling infiltration over large areas the main scope of this work is to present a method for estimating ei and the other water balance components over large areas based on a monthly water balance model following the thornthwaite mather approach thornthwaite 1948 mather 1978 1979 the method is implemented as a model named hydro bm it is coded in r an open source software environment for statistical computing and graphics r core team 2013 and it is applied over large areas in europe using free open data http opendefinition org the model was first applied in the central apennine range where ei and eic values were available from the literature boni et al 1986 in order to calibrate validate the model next the proposed approach was applied to the alpine region where ei and eic data are rare and often incomplete this work is focused on the use of free open data and open source software which is a fundamental step for the replication of any scientific result depending on computation to allow reproducibility ince et al 2012 of the proposed method but also to facilitate its application in different areas in the following we describe the free open data used as input to the model 2 we illustrate the model and its implementation 3 we describe the study areas and their main characteristics 4 we show and discuss the results of the model calibration and application in the study areas 5 and finally we present the study s conclusions 6 2 materials free open data the reproducibility of the analysis and the spatial and temporal homogeneity of the input data are fundamental issues as highlighted by turner et al 1989 understanding hydrological systems and the effects of spatial heterogeneity on groundwater requires the integration of different data types to characterize spatial units different in size shape and arrangement e g soil systems surface watersheds groundwater networks in this sense remote sensing and geographic information systems gis can provide the data and tools appropriate for the analysis singh and fiorentino 2013 moreover large amounts of free open geospatial data mainly released by public administrations and useful for hydrological systems characterization are now available to the scientific community and to the public in this work we attempted to use exclusively such kinds of free open data in input to the proposed modelling framework in addition we largely used open source and geographic free and open source software to allow the full reproducibility of the analyses in particular we used grass neteler and mitasova 2008 qgis quatum gis development team 2017 postgresql https www postgresql org and its spatial extension postgis http www postgis org to manage prepare and process the free open input geographical data raster and shapefiles and we used the r language r core team 2013 to code the water balance model as stated by baltussen et al 2013 open data can be accessed distributed and reused by everyone even for commercial purposes without the need to explicitly ask the data owner for permission open data are provided with adequate standard metadata and they are subjected to known and well documented quality assessment procedures many semi governmental organizations already openly publish parts of their data open data is an increasingly popular form of publishing information on the web as new modes of discovery collaboration and knowledge creation edson 2010 that is also high on the digital agenda in europe niggemann et al 2011 european commission 2006 for modelling purposes we exploited entirely free open data from different providers we used three different types of data with different spatial resolutions i surface elevation data 1 arc second grid resolution ii climatic data 0 5 grid resolution and iii soil environmental data 10 km grid resolution such information was processed in order to produce the input data to use in the water balance model the model input variables were aggregated to the grid resolution of the climatic data 0 5 grid resolution 55 km providing estimates of the minimum maximum and average values such an approach was applied to consider the spatial variability subjectivity and uncertainty related to the estimation of the input data fig 1 shows a matrix of the input variables column wise and the processing aggregation methods row wise used in the hydrological balance modelling framework 2 1 surface elevation data elevation data were derived for the two study areas red and blue polygons in fig 2 from the world digital elevation model aster gdem version 2 nasa land processes distributed active archive center 2013 the original data with horizontal resolution of 1 arc second 30 m fig 2 were aggregated at the 0 5 grid resolution 55 km of the climatic data to be comparable with the climatic data to account for the spatial variability of elevation we calculated for each 0 5 grid cell the minimum maximum and average elevation values 2 2 climatic data the climatic data used in this study consist of mean monthly precipitation p and temperature t values fig 3 estimated for each month in the period from 1961 to 1990 at 0 5 grid resolution such data were taken from the global historical climatology network monthly ghcn m data base version 2 distributed with a public domain license by the noaa s national climatic data center ncdc peterson and vose 1997 adopted also in several international climate assessments including in the annual reports of the intergovernmental panel on climate change ipcc to account for the possible uncertainty in the climatic data together with the average values p t we consider maximum and minimum p min p max t min t max the precipitation values are assumed to vary from p min to p max given respectively by eq 1 and 2 1 p min p up min p 2 p max p up max p where p is the ghcn m grid precipitation value up min 0 75 and up max 1 according to the variability of the monthly precipitation data observed by in different rain gauges in the italian alps and northern central apennines this conservative precipitation variability range was estimate analysing precipitation data from the italian rain gauge network managed by the italian national civil protection department dpc the temperature values are assumed to vary from t min to t max given respectively by eq 3 and 4 3 t min t ut min 4 t max t ut max where t is the ghcn m grid temperature value ut min 5 c and ut max 5 c since the proposed modelling approach is sensitive to climatic data we decided to run the model at the ghcn m grid resolution and to aggregate the other model input variables to the same spatial resolution in the analysis we use the abovementioned minimum maximum and average precipitation and temperature values calculated for each ghcn m grid cell fig 1 to simulate different climatic scenarios influencing the estimation of ei and of the other hydrogeological parameters 2 3 soil and lithology data the data for the characterization of the soil and the lithological model parameters were derived from different layers of the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 we used soil data relative to the 1 topsoil water capacity awc top 2 subsoil water capacity awc sub 3 depth to rock dr 4 dominant surface textural class text srf dom and 5 hydrogeological type of parental material pmh fig 4 all the aforementioned soil variables originally available in raster format at 10 10 km pixel resolution were aggregated at the ghcn m 0 5 grid resolution to be comparable with the climatic data similarly to the elevation data for each ghcn m grid cell we estimated the minimum maximum and average values for each soil lithological model parameters to account for the spatial heterogeneity of soil conditions inside each ghcn m cell fig 1 awc sub and awc top fig 4a b are measured in mm m 1 and are estimated from the textural class and from the soil packing density van liedekerke and panagos 2003 daroussin et al 1994 for the subsoil and topsoil respectively in the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 awc sub and awc top are expressed through categorical values given in the columns description in tables 1 and 2 starting from the categorical values to account for the subjectivity and the uncertainty related to these we attributed to the soil parameters a minimum and maximum value reported in the columns range values in tables 1 and 2 to choose the correct values for the analysis reported in the columns calibrated values in tables 1 and 2 we calibrated the model comparing the ei and eic outputs with the corresponding values available on the central apennines from the literature boni et al 1986 the calibration procedure was based on a trial and error approach and it is described in detail in section 5 dr fig 4c is the boundary in depth between unconsolidated soil and hard continuous coherent and scarcely weathered material according to van liedekerke and panagos 2003 daroussin et al 1994 dr may correspond either to the depth to the top of the r layer soil layer few affected by pedogenic processes soil science division staff 2017 or to the top of the c layer soil layer constituted by from strongly cemented to indurated bedrock soil science division staff 2017 similarly to the case of awc sub and awc top from the categorical and range values characterizing dr in the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 we derive numerical estimates of this parameter expressed in cm table 3 the awc sub awc top and dr layers were then used for estimation of the water holding capacity whc a basic input for the model which is the maximum amount of water that can be stored and withdrawn from the soil water storage for the whc computation we assumed that all the dr layer thickness is able to store water i e the awc values of the soil expressed in mm m 1 must be multiplied for the soil thickness and we also assumed that the upper half of the soil is the topsoil and the remaining lower half is the subsoil following these assumptions whc was computed following equation eq 5 5 whc awc sub dr 2 awc top dr 2 starting from the minimum maximum and average values of the awc sub awc top and dr parameters aggregated within each ghcn m grid cell simulating the spatial heterogeneity of the soil conditions in each grid cell we computed the minimum maximum and average values of whc fig 5 text srf dom fig 4d corresponds to the main surface textural class van liedekerke and panagos 2003 we use this parameter to characterize the direct and monthly runoff response for a given precipitation from the categorical values characterizing text srf dom in the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 we derived numerical estimates of the proportions of sand silt and clay according to the usda united states department of agriculture soil textural triangle soil science division staff 2017 from these textural characterizations we estimated the values of drofact and rfact which in the model 3 control respectively the direct runoff for a given precipitation returned directly by the hydrological system and the runoff returned in the following month table 4 for the previous soil parameters to account for the subjectivity and uncertainty related to these estimates we attributed to drofact and rfact minimum maximum and calibrated values range and calibrated values in table 4 pmh fig 4e is expressed in terms of storage capacity and permeability from the categorical values characterizing pmh in the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 we derive numerical estimates of pmhfact ranging from 0 to 1 table 5 this parameter is introduced into the hydrological balance model to control the fraction of water infiltrating in a month into depth 3 again to account for the subjectivity and the uncertainty related to this numerical estimate we attributed to pmhfact minimum maximum and calibrated values range and calibrated values in table 5 3 methods in this paper we propose a method based on a monthly water balance for the estimation of ei and other water balance components over large areas the calculation was implemented as an open source software based on the public domain tool usgs thornthwaite water balance model version 1 1 0 written in java by mccabe and markstrom 2007 and released without a specific licence at https wwwbrr cr usgs gov projects sw mows thornthwaite html the code performs a monthly water balance analysis based on the methods proposed by thornthwaite 1948 and mather 1978 1979 the original java software was recoded in r r core team 2016 integrating in input the different free and open data relative to precipitation temperature elevation and soil parameters see 2 the original code was modified i to be spatially distributed ii to consider deep infiltration and iii to account for the spatial input data variability uncertainty and to propagate this to the final ei and eic estimation results the new code which modifies the original water balance modelling schema was first calibrated and tested in the central apennine range using ei and eic values available from the literature boni et al 1986 successively the calibrated model was applied in the alpine region to estimate ei and eic values since in the area such estimates are rare and often incomplete the main input data used in the water balance analyses are the minimum maximum and average values of precipitation p temperature t elevation elev water holding capacity whc drofact rfact and pmhfact fig 6 shows the modelling schema adopted in the code that considers the different main time variant components variables i e representing different physical processes related to the water cycle balance i precipitation p in the form of rainfall p rain and snow p snow ii snow storage snowstor iii potential pet and actual aet evapotranspiration iv shallow infiltration st and deep infiltration deepstor storages and v runoff that includes the direct monthly runoff dro and runoff ro derived from the water balance in the previous month the balance also considers the following additional time variant auxiliary variables useful mainly to transfer water from one balance component to another a excess of water that cannot either infiltrate or evapotranspirate sur b water remaining as surface soil water storage in a month remain c precipitation available for evapotranspiration p aet and d snow melt sm the water transferred to the different balance components is modulated by some controlling factors 1 the fraction of water in the soil that infiltrates at depth pmhfact 2 the fraction of precipitation that contributes directly to the runoff in a month drofact and 3 the fraction of water that becomes runoff in a month after infiltration and evapotranspiration withdrawal rfact the temperature influences different balance components in time as described in the following the elevation soil and lithological data being time invariant modulate the different balance components only spatially the model basically calculates the water repartition among the different balance components on a monthly basis conceptually the model considers different storage elements where input and output are modulated by different processes for a given time i e for a given month the model first computes the amount of precipitation p that falls is the form of rain p rain and snow p snow expressed in millimetres when t is below a specified threshold t snow all precipitation is considered to be snow if temperature is greater than an additional threshold t rain then all precipitation is considered to be rain as referred to by mccabe and markstrom 2007 within the range defined by t snow and t rain the amount of precipitation that is snow decreases linearly from 100 to 0 percent of the total precipitation this relation is expressed as 6 p snow p t rain t t rain t snow p rain then is computed as 7 p rain p p snow t rain may vary for different location altitude in this work we assumed a constant value of t rain equal to 3 3 c based on the analysis of water balance results for different test sites carried out by mccabe and wolock 1999 according to mccabe and markstrom 2007 t snow 10 c for elevation below 1000 m and t snow 1 c for elevations above 1000 m we maintain that this variation schema is too basic and for this reason in the proposed model instead of having two discrete values we assumed a linear increase of t snow with elevation in the range of t snow 10 c for elevation of 0 m t snow0 and t snow 1 c for elevations of 1000 m t snow1000 for this purpose between 0 and 1000 m t snow is computed as 8 t snow t snow0 t snow1000 t snow0 elev 1000 another component of the water balance model is the snow melt fraction sm that is the fraction of snow storage snowstor that melts in a month and it is calculated mccabe and markstrom 2007 as 9 sm t t snow t rain t snow meltmax according to mccabe and markstrom 2007 meltmax is the maximum melt rate in a month and it is assumed to be meltmax 0 5 regardless of the elevation we maintain that this a rather conservative assumption and that meltmax should be more reasonably dependent from the elevation for such reason in the proposed model we assumed a linear variation of meltmax with the elevation considering meltmax 1 for elevation of 0 m meltmax 0 meltmax 0 5 for elevation of 1000 m meltmax 1000 for elevations between 0 and 1000 m meltmax is given by 10 meltmax meltmax 0 meltmax 1000 meltmax 0 elev 1000 meltmax basically controls the dependence of the snow melt fraction sm with respect to elevation evapotranspiration is another fundamental component of the water balance in the model the actual evapotranspiration aet is derived following mccabe and markstrom 2007 from total precipitation available for evapotranspiration p aet potential evapotranspiration pet soil moisture storage st soil moisture storage withdrawal stw p aet accounts for the rain precipitation p rain the snow melt fraction sm and eventually the water remaining as surface soil stored from the previous month remain but it does not consider direct runoff dro the last two being defined later in the text see eq 20 and 17 coherently with the methods proposed by thornthwaite 1948 and mather 1978 1979 for the estimation of evapotranspiration the vegetation effect is not considered p aet is given by the following equation 11 p aet p rain dro sm remain pet is calculated using the hamon equation hamon 1961 12 pet 13 97 d d 2 wt where pet is expressed in millimetres per month d is the number of days in a month d is the mean monthly hours of daylight in units of 12 h and wt is a saturated water vapour density term in grams per cubic metre calculated by 13 wt 4 95 e0 062 t 100 where t is the mean monthly temperature in celsius degrees hamon 1961 when p aet in a month is less than pet then aet is equal to p aet plus the amount of soil moisture that can be withdrawn from the soil moisture storage st soil moisture storage withdrawal stw decreases linearly with decreasing st such that as the soil becomes drier water becomes more difficult to remove from the soil and less is available for aet mccabe and markstrom 2007 according to mccabe and markstrom 2007 14 stw prestor pmpe prestor whc where prestor is the soil moisture storage for the previous month and pmpe is given by the equation eq 15 15 pmpe p aet pet pmpe has a double meaning i if positive it is the water that can potentially infiltrate into the soil ii if negative it is the water that can potentially be withdrawn by evapotranspiration from the soil if the sum of p aet and stw is less than pet then an evapotranspiration deficit is calculated as pet aet if p aet exceeds pet then aet is equal to pet and the water in excess of pet replenishes st when st is greater than whc the excess water becomes surplus sur and is eventually available for runoff mc cabe and markstrom 2007 the total runoff ro total is given by the sum of the direct runoff in the month dro and the runoff generated in the month ro after the infiltration and evapotranspiration withdrawals occurred 16 ro total dro ro dro is the fraction of rainfall p rain that becomes runoff in a month mccabe and markstrom 2007 and is calculated according to the following equation 17 dro p rain drofrac where drofrac varies from 0 to 1 according to the dominant surface textural class text srf dom as explained in 2 3 dro contributes directly to the total monthly runoff ro total and is not accounted for in the actual evapotranspiration aet evaluation thus in the aet calculation 18 p rain p rain dro the runoff ro is estimated after the infiltration and the evapotranspiration processes have taken place eventually ro takes into account the water remaining as surface soil water storage in the previous month remain 19 ro sur remain rfact where rfact is the fraction of water becoming runoff in a month after infiltration and evapotranspiration withdrawal rfact varies from 0 to 1 2 3 according to the dominant surface textural class text srf dom and remain i e a measure of surface soil water storage is given by 20 remain m 1 sur remain m ro where m is the considered month and m 1 is the successive month remain m 1 is an estimate of the water stored on the surface in a month and made available in the successive month to saturate the soil in the water balance calculations whether after infiltration an excess of remain m 1 is still available remain m 0 can be either accounted for in the p aet calculation option 1 or directly used in the ro estimation option 2 this balance component basically accounts for the exchange of water between surface soil moisture storage shallow aquifers i e characterized by short residence times on the order of one month or less and the surface hydrological network in this work the new parameter deepstor was introduced to account for the amount of water stored at depth each month the deep infiltration transfer is proportional to pmhfact that can be computed using two different methods according to the following equations 21 pmhfact whc pmh option steady 22 pmhfact st pmh option not steady where pmh values range from 0 to 1 and are derived from the layer of the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 pmh see 2 3 for a given location using eq 21 pmhfact remains constant whereas when using eq 22 pmhfact varies in time decreasing when the soil becomes drier i e water becomes more difficult to remove from the soil and less water is available for infiltrating at depth deepstor is estimated monthly using the following equation 23 deepstor m st pmhfact deepstor m 1 where deepstor m 1 is the deep water storage of the previous month m 1 and st pmhfact is the water stored in the shallow soil portion that infiltrates at depth in the considered month m the water contained in deepstor which is assumed as an infinite aquifer is no longer considered available in the monthly water balance deepstor can be fed by downward water soil surface movements i e coming from st but it does not contribute to the surface water movements basically this balance component accounts for the water stored in aquifers with long residence times no longer available for the monthly water exchange with the hydrological network therefore in the model the possible contribution of deepstor to the surface water system is considered to be minor this also applies for the deepstor contribution to springs the modelling of such contributions would require i a different time integration e g water accumulated at depth in previous years should be considered in the balance and ii a detailed knowledge of the deep water bodies resources and of the relative circulation exchange processes the effective infiltration coefficient coef inf is estimated yearly and could be calculated including eq 24 or excluding eq 25 the soil moisture storage withdrawal stw 24 coef inf deepstor stw year prec with stw contribution 25 coef inf deepstor year prec without stw contribution in the equations year prec is the cumulated precipitation calculated in one year starting from the ghcn m data 4 study areas we applied the model in two study areas i the central apennines and ii the alps with the main characteristics described in sections 4 1 and 4 2 respectively 4 1 central apennines the apennine range fig 2 extends longitudinally along the italian peninsula from the po basin pob to the calabria peloritani arch cal pel arch uplands are alternated by valleys and depressions while in the east and west coastal plains bound the mountain chain the central apennines roughly correspond to the mountain range section located in the umbria marche lazio abruzzo and molise italian regions fig 7 a shows schematically the hydrogeological setting of the central apennines important aquifers are hosted in the carbonate rocks that often have karstic features and are well replenished by rainfall since carbonates efficiently drain the rainwater the west side of the central apennines is characterized by the presence of volcanic aquifers that feed many large springs the permeability of these aquifers varies according to the extent of the weathering and to the different phases of eruption valleys and depressions as well as the coastal margins are infilled with permeable coarse grained quaternary deposits that host multi layered aquifers less relevant if compared to the carbonate and volcanic aquifers boni and bono 1982 crampon et al 1996 a first qualitative evaluation of the ei for the carbonate complexes in the central apennines was performed by boni and bono 1982 that recognized approximately 10 hydrogeological complexes for each complex the authors estimated the average discharge of the main springs that were used for the ei estimation and divided the average discharge values by the extent of the recharge areas the results show that in the carbonate platform complexes ei varies between 800 and 900 mm year with a corresponding eic value of approximately 0 7 in the transition facies complexes mainly composed of carbonate rocks ei varies between 700 and 900 mm year with eic in the range of 0 6 0 65 in the calcareous siliceous marly basin complexes ei varies between 500 and 650 mm year and eic ranges between 0 45 and 0 60 successively boni et al 1986 identified 22 main hydrogeological complexes in central italy grouping the main outcropping formations with similar geo lithological features for each hydrogeological complex they estimated ei by applying the method previously used in boni and bono 1982 fig 7b shows the hydrogeological complexes recognized by boni et al 1986 and table 6 shows the associated ei values in this study additionally we estimated eic fig 7c table 6 by dividing ei values by the mean annual precipitation pyear reported in table 6 the authors grouped the aquifers into five domains i the shelf domain s1 s2 u1 u2 u3 g2 s3 and s4 ii the pelagic domain g5 and s9 iii the marly calcarenitic complex u4 u5 u6 and u7 iv the volcanic domain g6 s10 s7 and u8 and v the structures made by rocks belonging to more than one domain s8 g4 s5 and s6 table 6 shows a wide variation on the surface of the recognized hydrogeological complexes that varies from 29 km2 to 35 550 km2 with an average of 892 km2 the highest values of ei were recorded in the shelf domain where ei varied between 706 and 983 mm year average value 847 75 mm year while the lowest values were recorded in the marly calcarenitic aquifers where ei ranges between 183 and 281 mm year average value 235 mm year the pelagic and volcanic aquifers show intermediate ei values the pelagic domain is composed of only two aquifers with ei 533 and 637 mm year while the volcanic domain is composed of four aquifers with ei values ranging between 280 and 778 mm year with an average value of 428 mm year medium to high values are recorded in the aquifers made by rocks belonging to more than one domain with ei ranging between 568 and 888 mm year average value 743 25 mm year coherently with ei the values of eic estimated for the shelf domain ranging between 0 88 and 0 57 with an average value of 0 70 are the highest within the recognized domains while the lowest eic values are recorded for the marly calcarenitic complex where eic ranges between 0 19 and 0 25 average value 0 22 intermediate eic values are recorded in the pelagic domain composed of two aquifers with eic of 0 48 and 0 60 and in the volcanic domain composed of four aquifers with eic values between 0 28 and 0 60 with an average value of 0 36 medium to high values are recorded in the structures made by rocks belonging to more than one domain with eic ranging from 0 44 to 0 75 average value 0 62 these values are confirmed by more recent studies mastrorillo et al 2009 mastrorillo and petitta 2010 where ei estimated for different hydrogeological carbonate complexes in central apennines varies between 800 and 150 mm year decreasing with the increase of their marly content the estimated values of eic agree with the values reported by civita 2005 ranging between 0 15 and 0 35 in plutonic rocks 0 2 and 0 5 in metamorphic rocks 0 3 and 0 5 in marl limestone and 0 7 and 1 in carbonate rocks the literature values estimated for european calcareous karst aquifers range from 0 5 to 0 76 with an average value of 0 51 burdon 1965 kessler 1965 vilimonovic 1965 drogue 1971 soulios 1984 bonacci 2001 4 2 alps the alps figs 2 and 8 are an arch shape mountain chain partially continuous with the north west boundary of the apennine chain ap and in the east to the dinarides di the pannonian basin pab bounds the alps to the east while the molasse basin m a cenozoic foredeep basin is located north of the alps the cenozoic rhône graben basin rg and the po basin pob are the west and south bounds of the chain respectively dal piaz et al 2003 the hydrogeology of the alps is highly diversified due to its extremely complicated geological setting crampon et al 1996 donnini et al 2016 roughly the following different geological domains can be identified fig 8 i austroalpine crystalline rocks in the eastern alps ea ii carbonate rocks in the north western alps predominantly in the jura mountains j between france and switzerland in the northern calcareous alps nca in west austria and in the south eastern eoalpine calcareous alps sa in italy slovenia and partially in austria and iii the western alps we with elvetic calcareous units mixed with crystalline massifs and penninic metamorphic ophiolitic units outside of the alpine chain north of the alps is located the molasse basin m filled by tertiary successions having several kilometres of thickness and south of the alps is located the po basin pob mainly consisting of alluvial deposits the crystalline metamorphic rocks are on average not highly permeable except for the highly tectonized areas characterized by extensive fracture systems the main and more productive springs in the chain are located in the calcareous complexes the molasse basin m contains numerous important aquifers while the po basin pob is characterized by subsidence and infilling forming a single complex aquifer locally affected by deep saltwater intrusion crampon et al 1996 the water in the alpine region is stored in lakes aquifers and glaciers which in turn feed many basins in europe including those of the four major rivers rhine danube po and rhône weingartner et al 2007 in particular glaciers cover an area of approximately 2050 km2 paul et al 2011 representing 1 of the total alpine area as estimated by donnini et al 2016 in the winter season precipitation can be temporarily stored as snow and ice and released in following months through their fusion european environmental agency 2010 given the above commonly the alps are defined as the water tower for europe european environmental agency 2009 gobiet et al 2014 5 model calibration and application to test and calibrate the water balance model we performed a computation in the central apennines fig 7a where the ei and eic values estimated for the main aquifers fig 7b were known from the literature boni et al 1986 and reported in table 6 such data were compared with the deep infiltration storage deepstor and the effective infiltration coefficient coef inf values both calculated by the water balance model in the analysis deepstor and coef inf were considered respectively as proxies of ei and of eic such data were considered as reference values even if these are estimates themselves to the best of knowledge of the authors even if old and rough these are the best ei and eic estimates available in the calibration study area i e the central apennines range specific studies carried out by mastrorillo et al 2009 and by mastrorillo and petitta 2010 in the same area confirm such estimates the test calibration procedure consists in a trial and error approach and allowed the choice of the optimal values the calibrated values reported in tables 1 5 of awc sub awc top dr drofact rfact these last two being derived from the different text srf dom grid categories and pmhfact derived from pmh to use in the balance model successively using the calibrated values we performed a computation in the alpine region fig 8 where ei and eic estimates are missing in all the computations the infiltration of remain m 1 was accounted for the p aet calculation option 1 in 3 pmhfact was calculated considering the option steady eq 21 and coef inf was computed without stw contribution eq 25 given the simple equations integrated in the water balance model it was easy to determine how ei and eic values are sensitive to the changes of each model input variables such sensitivity and in particular the positive or negative influence of each input variables i e t p whc elev drofact rfact and pmhfact change to the estimation of ei and eic are given in table 7 accounting such positive or negative possible influences we identified three modelling scenarios corresponding to i s1 minimum where all the model input variables are changed to contribute jointly to minimize the estimation of ei and eic ii s2 average where all the model input variables take the mean values to obtain an averaged estimation of ei and eic iii s3 maximum where all the model input variables are changed to contribute jointly to maximize the estimation of ei and eic such scenarios account for the spatial variability of the different input variables within each ghdn m grid cell table 7 summarizes how i e positive negative or no variation the model input variables have been set to simulate the three different ei scenarios in the analyses we calculated the mean values of all the model balance components after a simulation of an averaged hydrological year using the mean monthly temperature and precipitation data in the period 1961 1991 see section 2 2 for each scenario the main model outputs for all the ghcn m grid cells consist of estimates and plots of the multiple hydrological balance components at the mean monthly scale all the different estimates are summarized in the attribute table of a shapefile while the plots for each grid cell are stored in pdf format and saved in a specific folder in the file system in this study among the different model outputs we focused only on the mean ei and eic values expressed respectively in mm y 1 and in dimensionless form from 0 to 1 estimated at the end of the hydrological year since generally ghcn m grid cells intersect more than one hydrogeological complex see fig 7b c we compared the minimum the average and the maximum ei and eic values estimated by boni et al 1986 within each ghcn m grid cell with the corresponding values estimated by the model figs 9 and 10 show respectively for ei and for eic the compared values the linear correlation equations assuming an intercept value equal to 0 the determination coefficients r2 and the prediction intervals the comparison of the angular coefficients α and of the corresponding r2 values table 8 shows that s2 is the best performing scenario both in terms of estimated ei and eic values considering the average model results compared to the average values estimated by boni et al 1986 in terms of ei fig 9e and eic fig 10 e we observe that the model may underestimate the ei α 1 and overestimate the eic α 1 values given by boni et al 1986 modifying model parameters to have higher ei values would result in lower eic values thus it would be difficult to compensate the differences between the coefficients ei and the eic linear coefficient are positively correlated one possible explanation of such coefficient differences could rely upon the fact that the model uses rainfall data gridded over a large mesh possibly leading to the underestimation of the real precipitation hence to correct this bias an additional precipitation quantity should be considered in the calculation with the main effect of reducing eic and then increasing the corresponding α values this could be a possible explanation of the difference of the eic modelled values with those estimated by boni et al 1986 clearly also the ei modelled values deepstor numerator in eic equations eq 24 and 25 should increase considering this additional precipitation quantity but such a change would be less significant compared to the total precipitation increase year prec denominator in the eic equations implying a general tendency for reducing eic with the increase of precipitation fig 11 shows the minimum a average b and maximum c values of deepstor and the minimum d average e and maximum f values of coef inf for the whole study area alps and apennines corresponding respectively to the s1 s2 and s3 scenarios the analysis of fig 11 together with the comparison summarized in table 8 further confirm that s2 is the best performing scenario additionally the results highlight how the modelling approach proposed in this paper was able to efficiently simulate the hydrological water balance and particularly the effective infiltration ei expressed in mm y 1 processes at the monthly scale following the thornthwaite mather approach thornthwaite 1948 mather 1978 1979 being able to reproduce the corresponding values estimated independently by boni et al 1986 using a completely different approach we further note that different possible calibration procedures can be performed considering surface discharge and effective infiltration reference values together where these are jointly available after that the calibration was performed in the apennine range in order to further test the model we applied it in the alps such application allowed to test the model transferability and to estimate roughly the ei and eic values in the alpine region where such information is not available in fig 11 to account for the possible transferability inaccuracies together with ei and eic mean values s2 the estimates of their possible uncertainty are given s1 and s3 we maintain that the ei and eic values estimated in this paper can be helpful for a more correct and reliable estimate of the total amount of the hydrogeological resources accumulated yearly in the study areas we additionally note that similarly to what done in the alpine region the model could be applied in other areas where similar open input data are available in particular this is possible for most of the european territory and for part of the eurasian continent where the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 is already available 6 conclusions the estimation of ei i e the estimation of the amount of meteoric water that yearly infiltrates at depth for recharging the aquifers is a complex task requiring information regarding lithology morphology and climate such estimation is even more complex for large areas where free or open coherent and homogeneous data are rare in this paper we proposed a new method for the ei estimation over large areas considering a modified version of the thornthwaite mather monthly water balance model thornthwaite 1948 mather 1978 1979 and using open data http opendefinition org available for almost the entire european territory for the purpose we developed and used the hydro bm tool coded in r an open source software environment for statistical computing and graphics r core team 2013 which is able to estimate the main components of the water balance model since the input variables had different spatial resolution and since in this study we focused on the estimate of the water balance model dynamic components i e which largely depend on the climatic data we aggregated the soil input variables to the ghcn m 0 5 grid resolution of the temperature and precipitation data this procedure helped to provide estimates of the minimum maximum and average values of the model components starting from the spatial variability of the input data the model was first successfully tested calibrated in the central apennines where ei and eic estimates are available in the literature boni et al 1986 allowing the selection of the best input parameters combination the calibrated optimal model was then applied in the alpine region allowing the estimation of ei and eic values not available in the literature ei and eic values estimated in the two study areas may be helpful for a more correct and reliable estimate of the total yearly accumulated amount of hydrogeological resources despite some differences still exist between the modelled and reference data we maintain that the modelling approach proposed in this paper can be useful for the estimation of ei eic and or other water balance components at regional spatial scales and at monthly yearly temporal scales this is possible in areas where similar open input data are available and in particular for most of the european territory and part of the eurasian continent where the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 is already available software availability the hydro bm source code is still under development and it will be made available to the users upon specific request to the developers contacting one of the following e mail addresses mauro rossi irpi cnr it and marco donnini irpi cnr it acknowledgements and credits the paper partly uses data and analyses carried out by m donnini during his phd at the università degli studi di perugia funded by cnr irpi the analysis on the precipitation variability was done by m rossi using data from the italian rain gauge network managed by the italian national civil protection department dpc and funded within the agreements between dpc and cnr irpi intese operative dpc n 619 672 1015 1181 all the analyses performed in the paper were realized using software with different open source licences the hydro bm water balance model was coded in r version 3 2 x r core team 2016 using the following packages ncdf pierce 2011 raster hijmans 2014 and rgdal bivand et al 2014 additionally the r packages ggplot2 dragulescu 2013 and xlsx wickham 2009 were used for the analysis of the model results hydro bm was based on the public domain tool usgs thornthwaite water balance model version 1 1 0 written in java by mccabe and markstrom 2007 and released without a specific licence at https wwwbrr cr usgs gov projects sw mows thornthwaite html qgis version 2 18 x quatum gis development team 2017 grass version 6 4 neteler and mitasova 2008 postgresql release 9 3 x https www postgresql org and its spatial extension postgis version 2 2 x http www postgis org were used for the preparation storing and elaboration of geographical data used in the manuscript appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 005 
26398,effective infiltration ei is the amount of precipitation infiltrating into the soil and recharging the aquifers ei is estimated using direct or indirect methods or using water balance models direct and indirect methods lead to biased ei estimates since based on simplified schemas of groundwater bodies and of their recharge mechanisms water balance models include different processes and variables but they are seldom applied due to the limited availability of the input data particularly at regional scales we propose a method for ei estimation over large areas based on a monthly water balance model exploiting open source software and free open data the model integrates procedures to estimate ei and other water balance components accounting for the uncertainty of input data the model is calibrated in the central apennines italy where ei reference values are available from the literature and later applied in the alps where regional ei estimates are missing keywords effective infiltration hydrogeological balance free open data open software alps central apennines hydro bm list of variables used in the model variable description units aet actual evapotranspiration mm awc sub subsoil water capacity mm m 1 awc top topsoil water capacity mm m 1 coef inf effective infiltration coefficient dimensionless d number of days in a month d mean monthly hours of daylight units of 12 h deepstor amount of water stored in depth used as a proxy of the effective infiltration ei mm deepstor m deepstor in the considered month m mm deepstor m 1 deepstor in the previous month m 1 mm dr depth to rock m dro direct runoff mm drofact fraction of direct runoff for a given precipitation returned directly by the hydrological system dimensionless elev elevation m a s l meltmax factor controlling the dependence of snow melt fraction sm with respect to elevation dimensionless meltmax 0 meltmax for elevation of 0 m dimensionless meltmax 1000 meltmax for elevation of 1000 m dimensionless p mean monthly precipitation mm p aet precipitation available for the evapotranspiration mm pet potential evapotranspiration mm p max maximum mean monthly precipitation mm p min minimum mean monthly precipitation mm pmh hydrogeological type of parental material dimensionless pmpe if positive it is the water that can potentially infiltrate into the soil if negative it is the water potentially withdrawable by evapotranspiration from the soil mm prestor soil moisture storage for the previous month mm pmhfact fraction of water infiltrating in a month into depth dimensionless p rain mean monthly precipitation in the form of rainfall mm p snow mean monthly precipitation in the form of snowfall mm remain water remaining as surface soil water storage in a month this is transferred to the successive month for water balance calculations mm remain m remain in the considered month m mm remain m 1 remain in the successive month m 1 rfact fraction of water becoming runoff in a month after infiltration and evapotranspiration withdrawal dimensionless ro runoff generated from the soil water storage mm rototal monthly total runoff mm sm snow melt fraction mm snowstor snow storage mm st soil moisture storage mm stw soil moisture storage withdrawal mm sur excess water that cannot either infiltrate or evapotranspirate mm t mean monthly temperature c text srf dom dominant surface textural class dimensionless t max maximum mean monthly temperature c t min minimum mean monthly temperature c t rain above this temperature threshold all precipitation is rain c t snow below this temperature threshold all precipitation is snow c t snow0 temperature threshold for elevation of 0 m below this threshold all precipitation is snow c t snow1000 temperature threshold for elevation over 1000 m below this threshold all precipitation is snow c up max maximum precipitation uncertainty mm up min minimum precipitation uncertainty mm ut max maximum temperature uncertainty c ut min minimum temperature uncertainty c whc water holding capacity mm wt saturated water vapour density term g m3 year prec total precipitation in a year mm 1 introduction effective infiltration ei is the amount of meteoric water per unit surface that yearly infiltrates into the soil and recharges aquifers ei reflects the capability of a hydrogeological complex to absorb meteoric water estimating this parameter in a given area requires a detailed knowledge of its lithological morphological and climatic setting mastrorillo et al 2009 ei can be estimated using direct or indirect methods or using water balance models the direct method consists of estimating ei by dividing the average volume of water discharged by the springs in one year by their recharge area pioneering analyses of the infiltration process for aquifer recharge using direct methods were done at yearly aronis et al 1961 burdon and papakis 1961 burdon 1965 and sub yearly kessler 1957 1965 scales starting from the 50s boni and bono 1982 and boni et al 1986 applied the direct method to estimate the ei of the main aquifers in central italy the method has been successively applied by mastrorillo et al 2009 and mastrorillo and petitta 2010 for carbonate aquifers in the umbria marche region central italy mastrorillo et al 2009 suggest that the application of ei direct estimation is difficult and that this may lead to biased estimates mainly because hydrogeological bodies may be characterized by deep groundwater circulation with unknown springs and unknown hydraulic connections with other groundwater bodies the direct method also suffers some limitations due to the limited accuracy when defining the boundaries of aquifer recharge areas and due to the scarce availability of the spring discharge data as highlighted by mastrorillo et al 2009 in these conditions the effective infiltration can be deduced indirectly as a percentage of precipitation i e indirect method this indirect method is based on the use of an empirical coefficient named the effective infiltration coefficient eic this dimensionless coefficient ranges between 0 and 1 and varies for different lithology the eic is defined as the percentage of precipitation that infiltrates at depth and it is estimated at the catchment scale by dividing the volume of spring discharge by the volume of total rainfall drogue 1971 bonacci 2001 civita 2005 mastrorillo et al 2009 allocca et al 2014 such indirect effective infiltration estimation is biased since it is based on a rough parameterization of the permeability of the different lithologies outcropping in the recharge areas moreover values of eic are in general inferred from the literature and do not consider the real lithological setting of the investigated recharge areas the suitability of this indirect effective infiltration estimation method depends on the reliability of the precipitation values for which long and complete time series are rare or partial particularly in the mountainous areas where a significant part of the aquifer recharge process occurs where lacking these data are obtained through elevation precipitation correlation methods that albeit extremely rigorous may fail to estimate the actual precipitation values and their seasonal distributions the other methods proposed in the literature are based on the use of infiltration models at different spatial and temporal scales e g balance models hydrologic models etc these methods are more demanding in terms of input data e g boni et al 1982 guowei and yifeng 1991 kostka and holko 1994 jain 2012 that are rare in particular when modelling infiltration over large areas the main scope of this work is to present a method for estimating ei and the other water balance components over large areas based on a monthly water balance model following the thornthwaite mather approach thornthwaite 1948 mather 1978 1979 the method is implemented as a model named hydro bm it is coded in r an open source software environment for statistical computing and graphics r core team 2013 and it is applied over large areas in europe using free open data http opendefinition org the model was first applied in the central apennine range where ei and eic values were available from the literature boni et al 1986 in order to calibrate validate the model next the proposed approach was applied to the alpine region where ei and eic data are rare and often incomplete this work is focused on the use of free open data and open source software which is a fundamental step for the replication of any scientific result depending on computation to allow reproducibility ince et al 2012 of the proposed method but also to facilitate its application in different areas in the following we describe the free open data used as input to the model 2 we illustrate the model and its implementation 3 we describe the study areas and their main characteristics 4 we show and discuss the results of the model calibration and application in the study areas 5 and finally we present the study s conclusions 6 2 materials free open data the reproducibility of the analysis and the spatial and temporal homogeneity of the input data are fundamental issues as highlighted by turner et al 1989 understanding hydrological systems and the effects of spatial heterogeneity on groundwater requires the integration of different data types to characterize spatial units different in size shape and arrangement e g soil systems surface watersheds groundwater networks in this sense remote sensing and geographic information systems gis can provide the data and tools appropriate for the analysis singh and fiorentino 2013 moreover large amounts of free open geospatial data mainly released by public administrations and useful for hydrological systems characterization are now available to the scientific community and to the public in this work we attempted to use exclusively such kinds of free open data in input to the proposed modelling framework in addition we largely used open source and geographic free and open source software to allow the full reproducibility of the analyses in particular we used grass neteler and mitasova 2008 qgis quatum gis development team 2017 postgresql https www postgresql org and its spatial extension postgis http www postgis org to manage prepare and process the free open input geographical data raster and shapefiles and we used the r language r core team 2013 to code the water balance model as stated by baltussen et al 2013 open data can be accessed distributed and reused by everyone even for commercial purposes without the need to explicitly ask the data owner for permission open data are provided with adequate standard metadata and they are subjected to known and well documented quality assessment procedures many semi governmental organizations already openly publish parts of their data open data is an increasingly popular form of publishing information on the web as new modes of discovery collaboration and knowledge creation edson 2010 that is also high on the digital agenda in europe niggemann et al 2011 european commission 2006 for modelling purposes we exploited entirely free open data from different providers we used three different types of data with different spatial resolutions i surface elevation data 1 arc second grid resolution ii climatic data 0 5 grid resolution and iii soil environmental data 10 km grid resolution such information was processed in order to produce the input data to use in the water balance model the model input variables were aggregated to the grid resolution of the climatic data 0 5 grid resolution 55 km providing estimates of the minimum maximum and average values such an approach was applied to consider the spatial variability subjectivity and uncertainty related to the estimation of the input data fig 1 shows a matrix of the input variables column wise and the processing aggregation methods row wise used in the hydrological balance modelling framework 2 1 surface elevation data elevation data were derived for the two study areas red and blue polygons in fig 2 from the world digital elevation model aster gdem version 2 nasa land processes distributed active archive center 2013 the original data with horizontal resolution of 1 arc second 30 m fig 2 were aggregated at the 0 5 grid resolution 55 km of the climatic data to be comparable with the climatic data to account for the spatial variability of elevation we calculated for each 0 5 grid cell the minimum maximum and average elevation values 2 2 climatic data the climatic data used in this study consist of mean monthly precipitation p and temperature t values fig 3 estimated for each month in the period from 1961 to 1990 at 0 5 grid resolution such data were taken from the global historical climatology network monthly ghcn m data base version 2 distributed with a public domain license by the noaa s national climatic data center ncdc peterson and vose 1997 adopted also in several international climate assessments including in the annual reports of the intergovernmental panel on climate change ipcc to account for the possible uncertainty in the climatic data together with the average values p t we consider maximum and minimum p min p max t min t max the precipitation values are assumed to vary from p min to p max given respectively by eq 1 and 2 1 p min p up min p 2 p max p up max p where p is the ghcn m grid precipitation value up min 0 75 and up max 1 according to the variability of the monthly precipitation data observed by in different rain gauges in the italian alps and northern central apennines this conservative precipitation variability range was estimate analysing precipitation data from the italian rain gauge network managed by the italian national civil protection department dpc the temperature values are assumed to vary from t min to t max given respectively by eq 3 and 4 3 t min t ut min 4 t max t ut max where t is the ghcn m grid temperature value ut min 5 c and ut max 5 c since the proposed modelling approach is sensitive to climatic data we decided to run the model at the ghcn m grid resolution and to aggregate the other model input variables to the same spatial resolution in the analysis we use the abovementioned minimum maximum and average precipitation and temperature values calculated for each ghcn m grid cell fig 1 to simulate different climatic scenarios influencing the estimation of ei and of the other hydrogeological parameters 2 3 soil and lithology data the data for the characterization of the soil and the lithological model parameters were derived from different layers of the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 we used soil data relative to the 1 topsoil water capacity awc top 2 subsoil water capacity awc sub 3 depth to rock dr 4 dominant surface textural class text srf dom and 5 hydrogeological type of parental material pmh fig 4 all the aforementioned soil variables originally available in raster format at 10 10 km pixel resolution were aggregated at the ghcn m 0 5 grid resolution to be comparable with the climatic data similarly to the elevation data for each ghcn m grid cell we estimated the minimum maximum and average values for each soil lithological model parameters to account for the spatial heterogeneity of soil conditions inside each ghcn m cell fig 1 awc sub and awc top fig 4a b are measured in mm m 1 and are estimated from the textural class and from the soil packing density van liedekerke and panagos 2003 daroussin et al 1994 for the subsoil and topsoil respectively in the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 awc sub and awc top are expressed through categorical values given in the columns description in tables 1 and 2 starting from the categorical values to account for the subjectivity and the uncertainty related to these we attributed to the soil parameters a minimum and maximum value reported in the columns range values in tables 1 and 2 to choose the correct values for the analysis reported in the columns calibrated values in tables 1 and 2 we calibrated the model comparing the ei and eic outputs with the corresponding values available on the central apennines from the literature boni et al 1986 the calibration procedure was based on a trial and error approach and it is described in detail in section 5 dr fig 4c is the boundary in depth between unconsolidated soil and hard continuous coherent and scarcely weathered material according to van liedekerke and panagos 2003 daroussin et al 1994 dr may correspond either to the depth to the top of the r layer soil layer few affected by pedogenic processes soil science division staff 2017 or to the top of the c layer soil layer constituted by from strongly cemented to indurated bedrock soil science division staff 2017 similarly to the case of awc sub and awc top from the categorical and range values characterizing dr in the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 we derive numerical estimates of this parameter expressed in cm table 3 the awc sub awc top and dr layers were then used for estimation of the water holding capacity whc a basic input for the model which is the maximum amount of water that can be stored and withdrawn from the soil water storage for the whc computation we assumed that all the dr layer thickness is able to store water i e the awc values of the soil expressed in mm m 1 must be multiplied for the soil thickness and we also assumed that the upper half of the soil is the topsoil and the remaining lower half is the subsoil following these assumptions whc was computed following equation eq 5 5 whc awc sub dr 2 awc top dr 2 starting from the minimum maximum and average values of the awc sub awc top and dr parameters aggregated within each ghcn m grid cell simulating the spatial heterogeneity of the soil conditions in each grid cell we computed the minimum maximum and average values of whc fig 5 text srf dom fig 4d corresponds to the main surface textural class van liedekerke and panagos 2003 we use this parameter to characterize the direct and monthly runoff response for a given precipitation from the categorical values characterizing text srf dom in the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 we derived numerical estimates of the proportions of sand silt and clay according to the usda united states department of agriculture soil textural triangle soil science division staff 2017 from these textural characterizations we estimated the values of drofact and rfact which in the model 3 control respectively the direct runoff for a given precipitation returned directly by the hydrological system and the runoff returned in the following month table 4 for the previous soil parameters to account for the subjectivity and uncertainty related to these estimates we attributed to drofact and rfact minimum maximum and calibrated values range and calibrated values in table 4 pmh fig 4e is expressed in terms of storage capacity and permeability from the categorical values characterizing pmh in the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 we derive numerical estimates of pmhfact ranging from 0 to 1 table 5 this parameter is introduced into the hydrological balance model to control the fraction of water infiltrating in a month into depth 3 again to account for the subjectivity and the uncertainty related to this numerical estimate we attributed to pmhfact minimum maximum and calibrated values range and calibrated values in table 5 3 methods in this paper we propose a method based on a monthly water balance for the estimation of ei and other water balance components over large areas the calculation was implemented as an open source software based on the public domain tool usgs thornthwaite water balance model version 1 1 0 written in java by mccabe and markstrom 2007 and released without a specific licence at https wwwbrr cr usgs gov projects sw mows thornthwaite html the code performs a monthly water balance analysis based on the methods proposed by thornthwaite 1948 and mather 1978 1979 the original java software was recoded in r r core team 2016 integrating in input the different free and open data relative to precipitation temperature elevation and soil parameters see 2 the original code was modified i to be spatially distributed ii to consider deep infiltration and iii to account for the spatial input data variability uncertainty and to propagate this to the final ei and eic estimation results the new code which modifies the original water balance modelling schema was first calibrated and tested in the central apennine range using ei and eic values available from the literature boni et al 1986 successively the calibrated model was applied in the alpine region to estimate ei and eic values since in the area such estimates are rare and often incomplete the main input data used in the water balance analyses are the minimum maximum and average values of precipitation p temperature t elevation elev water holding capacity whc drofact rfact and pmhfact fig 6 shows the modelling schema adopted in the code that considers the different main time variant components variables i e representing different physical processes related to the water cycle balance i precipitation p in the form of rainfall p rain and snow p snow ii snow storage snowstor iii potential pet and actual aet evapotranspiration iv shallow infiltration st and deep infiltration deepstor storages and v runoff that includes the direct monthly runoff dro and runoff ro derived from the water balance in the previous month the balance also considers the following additional time variant auxiliary variables useful mainly to transfer water from one balance component to another a excess of water that cannot either infiltrate or evapotranspirate sur b water remaining as surface soil water storage in a month remain c precipitation available for evapotranspiration p aet and d snow melt sm the water transferred to the different balance components is modulated by some controlling factors 1 the fraction of water in the soil that infiltrates at depth pmhfact 2 the fraction of precipitation that contributes directly to the runoff in a month drofact and 3 the fraction of water that becomes runoff in a month after infiltration and evapotranspiration withdrawal rfact the temperature influences different balance components in time as described in the following the elevation soil and lithological data being time invariant modulate the different balance components only spatially the model basically calculates the water repartition among the different balance components on a monthly basis conceptually the model considers different storage elements where input and output are modulated by different processes for a given time i e for a given month the model first computes the amount of precipitation p that falls is the form of rain p rain and snow p snow expressed in millimetres when t is below a specified threshold t snow all precipitation is considered to be snow if temperature is greater than an additional threshold t rain then all precipitation is considered to be rain as referred to by mccabe and markstrom 2007 within the range defined by t snow and t rain the amount of precipitation that is snow decreases linearly from 100 to 0 percent of the total precipitation this relation is expressed as 6 p snow p t rain t t rain t snow p rain then is computed as 7 p rain p p snow t rain may vary for different location altitude in this work we assumed a constant value of t rain equal to 3 3 c based on the analysis of water balance results for different test sites carried out by mccabe and wolock 1999 according to mccabe and markstrom 2007 t snow 10 c for elevation below 1000 m and t snow 1 c for elevations above 1000 m we maintain that this variation schema is too basic and for this reason in the proposed model instead of having two discrete values we assumed a linear increase of t snow with elevation in the range of t snow 10 c for elevation of 0 m t snow0 and t snow 1 c for elevations of 1000 m t snow1000 for this purpose between 0 and 1000 m t snow is computed as 8 t snow t snow0 t snow1000 t snow0 elev 1000 another component of the water balance model is the snow melt fraction sm that is the fraction of snow storage snowstor that melts in a month and it is calculated mccabe and markstrom 2007 as 9 sm t t snow t rain t snow meltmax according to mccabe and markstrom 2007 meltmax is the maximum melt rate in a month and it is assumed to be meltmax 0 5 regardless of the elevation we maintain that this a rather conservative assumption and that meltmax should be more reasonably dependent from the elevation for such reason in the proposed model we assumed a linear variation of meltmax with the elevation considering meltmax 1 for elevation of 0 m meltmax 0 meltmax 0 5 for elevation of 1000 m meltmax 1000 for elevations between 0 and 1000 m meltmax is given by 10 meltmax meltmax 0 meltmax 1000 meltmax 0 elev 1000 meltmax basically controls the dependence of the snow melt fraction sm with respect to elevation evapotranspiration is another fundamental component of the water balance in the model the actual evapotranspiration aet is derived following mccabe and markstrom 2007 from total precipitation available for evapotranspiration p aet potential evapotranspiration pet soil moisture storage st soil moisture storage withdrawal stw p aet accounts for the rain precipitation p rain the snow melt fraction sm and eventually the water remaining as surface soil stored from the previous month remain but it does not consider direct runoff dro the last two being defined later in the text see eq 20 and 17 coherently with the methods proposed by thornthwaite 1948 and mather 1978 1979 for the estimation of evapotranspiration the vegetation effect is not considered p aet is given by the following equation 11 p aet p rain dro sm remain pet is calculated using the hamon equation hamon 1961 12 pet 13 97 d d 2 wt where pet is expressed in millimetres per month d is the number of days in a month d is the mean monthly hours of daylight in units of 12 h and wt is a saturated water vapour density term in grams per cubic metre calculated by 13 wt 4 95 e0 062 t 100 where t is the mean monthly temperature in celsius degrees hamon 1961 when p aet in a month is less than pet then aet is equal to p aet plus the amount of soil moisture that can be withdrawn from the soil moisture storage st soil moisture storage withdrawal stw decreases linearly with decreasing st such that as the soil becomes drier water becomes more difficult to remove from the soil and less is available for aet mccabe and markstrom 2007 according to mccabe and markstrom 2007 14 stw prestor pmpe prestor whc where prestor is the soil moisture storage for the previous month and pmpe is given by the equation eq 15 15 pmpe p aet pet pmpe has a double meaning i if positive it is the water that can potentially infiltrate into the soil ii if negative it is the water that can potentially be withdrawn by evapotranspiration from the soil if the sum of p aet and stw is less than pet then an evapotranspiration deficit is calculated as pet aet if p aet exceeds pet then aet is equal to pet and the water in excess of pet replenishes st when st is greater than whc the excess water becomes surplus sur and is eventually available for runoff mc cabe and markstrom 2007 the total runoff ro total is given by the sum of the direct runoff in the month dro and the runoff generated in the month ro after the infiltration and evapotranspiration withdrawals occurred 16 ro total dro ro dro is the fraction of rainfall p rain that becomes runoff in a month mccabe and markstrom 2007 and is calculated according to the following equation 17 dro p rain drofrac where drofrac varies from 0 to 1 according to the dominant surface textural class text srf dom as explained in 2 3 dro contributes directly to the total monthly runoff ro total and is not accounted for in the actual evapotranspiration aet evaluation thus in the aet calculation 18 p rain p rain dro the runoff ro is estimated after the infiltration and the evapotranspiration processes have taken place eventually ro takes into account the water remaining as surface soil water storage in the previous month remain 19 ro sur remain rfact where rfact is the fraction of water becoming runoff in a month after infiltration and evapotranspiration withdrawal rfact varies from 0 to 1 2 3 according to the dominant surface textural class text srf dom and remain i e a measure of surface soil water storage is given by 20 remain m 1 sur remain m ro where m is the considered month and m 1 is the successive month remain m 1 is an estimate of the water stored on the surface in a month and made available in the successive month to saturate the soil in the water balance calculations whether after infiltration an excess of remain m 1 is still available remain m 0 can be either accounted for in the p aet calculation option 1 or directly used in the ro estimation option 2 this balance component basically accounts for the exchange of water between surface soil moisture storage shallow aquifers i e characterized by short residence times on the order of one month or less and the surface hydrological network in this work the new parameter deepstor was introduced to account for the amount of water stored at depth each month the deep infiltration transfer is proportional to pmhfact that can be computed using two different methods according to the following equations 21 pmhfact whc pmh option steady 22 pmhfact st pmh option not steady where pmh values range from 0 to 1 and are derived from the layer of the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 pmh see 2 3 for a given location using eq 21 pmhfact remains constant whereas when using eq 22 pmhfact varies in time decreasing when the soil becomes drier i e water becomes more difficult to remove from the soil and less water is available for infiltrating at depth deepstor is estimated monthly using the following equation 23 deepstor m st pmhfact deepstor m 1 where deepstor m 1 is the deep water storage of the previous month m 1 and st pmhfact is the water stored in the shallow soil portion that infiltrates at depth in the considered month m the water contained in deepstor which is assumed as an infinite aquifer is no longer considered available in the monthly water balance deepstor can be fed by downward water soil surface movements i e coming from st but it does not contribute to the surface water movements basically this balance component accounts for the water stored in aquifers with long residence times no longer available for the monthly water exchange with the hydrological network therefore in the model the possible contribution of deepstor to the surface water system is considered to be minor this also applies for the deepstor contribution to springs the modelling of such contributions would require i a different time integration e g water accumulated at depth in previous years should be considered in the balance and ii a detailed knowledge of the deep water bodies resources and of the relative circulation exchange processes the effective infiltration coefficient coef inf is estimated yearly and could be calculated including eq 24 or excluding eq 25 the soil moisture storage withdrawal stw 24 coef inf deepstor stw year prec with stw contribution 25 coef inf deepstor year prec without stw contribution in the equations year prec is the cumulated precipitation calculated in one year starting from the ghcn m data 4 study areas we applied the model in two study areas i the central apennines and ii the alps with the main characteristics described in sections 4 1 and 4 2 respectively 4 1 central apennines the apennine range fig 2 extends longitudinally along the italian peninsula from the po basin pob to the calabria peloritani arch cal pel arch uplands are alternated by valleys and depressions while in the east and west coastal plains bound the mountain chain the central apennines roughly correspond to the mountain range section located in the umbria marche lazio abruzzo and molise italian regions fig 7 a shows schematically the hydrogeological setting of the central apennines important aquifers are hosted in the carbonate rocks that often have karstic features and are well replenished by rainfall since carbonates efficiently drain the rainwater the west side of the central apennines is characterized by the presence of volcanic aquifers that feed many large springs the permeability of these aquifers varies according to the extent of the weathering and to the different phases of eruption valleys and depressions as well as the coastal margins are infilled with permeable coarse grained quaternary deposits that host multi layered aquifers less relevant if compared to the carbonate and volcanic aquifers boni and bono 1982 crampon et al 1996 a first qualitative evaluation of the ei for the carbonate complexes in the central apennines was performed by boni and bono 1982 that recognized approximately 10 hydrogeological complexes for each complex the authors estimated the average discharge of the main springs that were used for the ei estimation and divided the average discharge values by the extent of the recharge areas the results show that in the carbonate platform complexes ei varies between 800 and 900 mm year with a corresponding eic value of approximately 0 7 in the transition facies complexes mainly composed of carbonate rocks ei varies between 700 and 900 mm year with eic in the range of 0 6 0 65 in the calcareous siliceous marly basin complexes ei varies between 500 and 650 mm year and eic ranges between 0 45 and 0 60 successively boni et al 1986 identified 22 main hydrogeological complexes in central italy grouping the main outcropping formations with similar geo lithological features for each hydrogeological complex they estimated ei by applying the method previously used in boni and bono 1982 fig 7b shows the hydrogeological complexes recognized by boni et al 1986 and table 6 shows the associated ei values in this study additionally we estimated eic fig 7c table 6 by dividing ei values by the mean annual precipitation pyear reported in table 6 the authors grouped the aquifers into five domains i the shelf domain s1 s2 u1 u2 u3 g2 s3 and s4 ii the pelagic domain g5 and s9 iii the marly calcarenitic complex u4 u5 u6 and u7 iv the volcanic domain g6 s10 s7 and u8 and v the structures made by rocks belonging to more than one domain s8 g4 s5 and s6 table 6 shows a wide variation on the surface of the recognized hydrogeological complexes that varies from 29 km2 to 35 550 km2 with an average of 892 km2 the highest values of ei were recorded in the shelf domain where ei varied between 706 and 983 mm year average value 847 75 mm year while the lowest values were recorded in the marly calcarenitic aquifers where ei ranges between 183 and 281 mm year average value 235 mm year the pelagic and volcanic aquifers show intermediate ei values the pelagic domain is composed of only two aquifers with ei 533 and 637 mm year while the volcanic domain is composed of four aquifers with ei values ranging between 280 and 778 mm year with an average value of 428 mm year medium to high values are recorded in the aquifers made by rocks belonging to more than one domain with ei ranging between 568 and 888 mm year average value 743 25 mm year coherently with ei the values of eic estimated for the shelf domain ranging between 0 88 and 0 57 with an average value of 0 70 are the highest within the recognized domains while the lowest eic values are recorded for the marly calcarenitic complex where eic ranges between 0 19 and 0 25 average value 0 22 intermediate eic values are recorded in the pelagic domain composed of two aquifers with eic of 0 48 and 0 60 and in the volcanic domain composed of four aquifers with eic values between 0 28 and 0 60 with an average value of 0 36 medium to high values are recorded in the structures made by rocks belonging to more than one domain with eic ranging from 0 44 to 0 75 average value 0 62 these values are confirmed by more recent studies mastrorillo et al 2009 mastrorillo and petitta 2010 where ei estimated for different hydrogeological carbonate complexes in central apennines varies between 800 and 150 mm year decreasing with the increase of their marly content the estimated values of eic agree with the values reported by civita 2005 ranging between 0 15 and 0 35 in plutonic rocks 0 2 and 0 5 in metamorphic rocks 0 3 and 0 5 in marl limestone and 0 7 and 1 in carbonate rocks the literature values estimated for european calcareous karst aquifers range from 0 5 to 0 76 with an average value of 0 51 burdon 1965 kessler 1965 vilimonovic 1965 drogue 1971 soulios 1984 bonacci 2001 4 2 alps the alps figs 2 and 8 are an arch shape mountain chain partially continuous with the north west boundary of the apennine chain ap and in the east to the dinarides di the pannonian basin pab bounds the alps to the east while the molasse basin m a cenozoic foredeep basin is located north of the alps the cenozoic rhône graben basin rg and the po basin pob are the west and south bounds of the chain respectively dal piaz et al 2003 the hydrogeology of the alps is highly diversified due to its extremely complicated geological setting crampon et al 1996 donnini et al 2016 roughly the following different geological domains can be identified fig 8 i austroalpine crystalline rocks in the eastern alps ea ii carbonate rocks in the north western alps predominantly in the jura mountains j between france and switzerland in the northern calcareous alps nca in west austria and in the south eastern eoalpine calcareous alps sa in italy slovenia and partially in austria and iii the western alps we with elvetic calcareous units mixed with crystalline massifs and penninic metamorphic ophiolitic units outside of the alpine chain north of the alps is located the molasse basin m filled by tertiary successions having several kilometres of thickness and south of the alps is located the po basin pob mainly consisting of alluvial deposits the crystalline metamorphic rocks are on average not highly permeable except for the highly tectonized areas characterized by extensive fracture systems the main and more productive springs in the chain are located in the calcareous complexes the molasse basin m contains numerous important aquifers while the po basin pob is characterized by subsidence and infilling forming a single complex aquifer locally affected by deep saltwater intrusion crampon et al 1996 the water in the alpine region is stored in lakes aquifers and glaciers which in turn feed many basins in europe including those of the four major rivers rhine danube po and rhône weingartner et al 2007 in particular glaciers cover an area of approximately 2050 km2 paul et al 2011 representing 1 of the total alpine area as estimated by donnini et al 2016 in the winter season precipitation can be temporarily stored as snow and ice and released in following months through their fusion european environmental agency 2010 given the above commonly the alps are defined as the water tower for europe european environmental agency 2009 gobiet et al 2014 5 model calibration and application to test and calibrate the water balance model we performed a computation in the central apennines fig 7a where the ei and eic values estimated for the main aquifers fig 7b were known from the literature boni et al 1986 and reported in table 6 such data were compared with the deep infiltration storage deepstor and the effective infiltration coefficient coef inf values both calculated by the water balance model in the analysis deepstor and coef inf were considered respectively as proxies of ei and of eic such data were considered as reference values even if these are estimates themselves to the best of knowledge of the authors even if old and rough these are the best ei and eic estimates available in the calibration study area i e the central apennines range specific studies carried out by mastrorillo et al 2009 and by mastrorillo and petitta 2010 in the same area confirm such estimates the test calibration procedure consists in a trial and error approach and allowed the choice of the optimal values the calibrated values reported in tables 1 5 of awc sub awc top dr drofact rfact these last two being derived from the different text srf dom grid categories and pmhfact derived from pmh to use in the balance model successively using the calibrated values we performed a computation in the alpine region fig 8 where ei and eic estimates are missing in all the computations the infiltration of remain m 1 was accounted for the p aet calculation option 1 in 3 pmhfact was calculated considering the option steady eq 21 and coef inf was computed without stw contribution eq 25 given the simple equations integrated in the water balance model it was easy to determine how ei and eic values are sensitive to the changes of each model input variables such sensitivity and in particular the positive or negative influence of each input variables i e t p whc elev drofact rfact and pmhfact change to the estimation of ei and eic are given in table 7 accounting such positive or negative possible influences we identified three modelling scenarios corresponding to i s1 minimum where all the model input variables are changed to contribute jointly to minimize the estimation of ei and eic ii s2 average where all the model input variables take the mean values to obtain an averaged estimation of ei and eic iii s3 maximum where all the model input variables are changed to contribute jointly to maximize the estimation of ei and eic such scenarios account for the spatial variability of the different input variables within each ghdn m grid cell table 7 summarizes how i e positive negative or no variation the model input variables have been set to simulate the three different ei scenarios in the analyses we calculated the mean values of all the model balance components after a simulation of an averaged hydrological year using the mean monthly temperature and precipitation data in the period 1961 1991 see section 2 2 for each scenario the main model outputs for all the ghcn m grid cells consist of estimates and plots of the multiple hydrological balance components at the mean monthly scale all the different estimates are summarized in the attribute table of a shapefile while the plots for each grid cell are stored in pdf format and saved in a specific folder in the file system in this study among the different model outputs we focused only on the mean ei and eic values expressed respectively in mm y 1 and in dimensionless form from 0 to 1 estimated at the end of the hydrological year since generally ghcn m grid cells intersect more than one hydrogeological complex see fig 7b c we compared the minimum the average and the maximum ei and eic values estimated by boni et al 1986 within each ghcn m grid cell with the corresponding values estimated by the model figs 9 and 10 show respectively for ei and for eic the compared values the linear correlation equations assuming an intercept value equal to 0 the determination coefficients r2 and the prediction intervals the comparison of the angular coefficients α and of the corresponding r2 values table 8 shows that s2 is the best performing scenario both in terms of estimated ei and eic values considering the average model results compared to the average values estimated by boni et al 1986 in terms of ei fig 9e and eic fig 10 e we observe that the model may underestimate the ei α 1 and overestimate the eic α 1 values given by boni et al 1986 modifying model parameters to have higher ei values would result in lower eic values thus it would be difficult to compensate the differences between the coefficients ei and the eic linear coefficient are positively correlated one possible explanation of such coefficient differences could rely upon the fact that the model uses rainfall data gridded over a large mesh possibly leading to the underestimation of the real precipitation hence to correct this bias an additional precipitation quantity should be considered in the calculation with the main effect of reducing eic and then increasing the corresponding α values this could be a possible explanation of the difference of the eic modelled values with those estimated by boni et al 1986 clearly also the ei modelled values deepstor numerator in eic equations eq 24 and 25 should increase considering this additional precipitation quantity but such a change would be less significant compared to the total precipitation increase year prec denominator in the eic equations implying a general tendency for reducing eic with the increase of precipitation fig 11 shows the minimum a average b and maximum c values of deepstor and the minimum d average e and maximum f values of coef inf for the whole study area alps and apennines corresponding respectively to the s1 s2 and s3 scenarios the analysis of fig 11 together with the comparison summarized in table 8 further confirm that s2 is the best performing scenario additionally the results highlight how the modelling approach proposed in this paper was able to efficiently simulate the hydrological water balance and particularly the effective infiltration ei expressed in mm y 1 processes at the monthly scale following the thornthwaite mather approach thornthwaite 1948 mather 1978 1979 being able to reproduce the corresponding values estimated independently by boni et al 1986 using a completely different approach we further note that different possible calibration procedures can be performed considering surface discharge and effective infiltration reference values together where these are jointly available after that the calibration was performed in the apennine range in order to further test the model we applied it in the alps such application allowed to test the model transferability and to estimate roughly the ei and eic values in the alpine region where such information is not available in fig 11 to account for the possible transferability inaccuracies together with ei and eic mean values s2 the estimates of their possible uncertainty are given s1 and s3 we maintain that the ei and eic values estimated in this paper can be helpful for a more correct and reliable estimate of the total amount of the hydrogeological resources accumulated yearly in the study areas we additionally note that similarly to what done in the alpine region the model could be applied in other areas where similar open input data are available in particular this is possible for most of the european territory and for part of the eurasian continent where the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 is already available 6 conclusions the estimation of ei i e the estimation of the amount of meteoric water that yearly infiltrates at depth for recharging the aquifers is a complex task requiring information regarding lithology morphology and climate such estimation is even more complex for large areas where free or open coherent and homogeneous data are rare in this paper we proposed a new method for the ei estimation over large areas considering a modified version of the thornthwaite mather monthly water balance model thornthwaite 1948 mather 1978 1979 and using open data http opendefinition org available for almost the entire european territory for the purpose we developed and used the hydro bm tool coded in r an open source software environment for statistical computing and graphics r core team 2013 which is able to estimate the main components of the water balance model since the input variables had different spatial resolution and since in this study we focused on the estimate of the water balance model dynamic components i e which largely depend on the climatic data we aggregated the soil input variables to the ghcn m 0 5 grid resolution of the temperature and precipitation data this procedure helped to provide estimates of the minimum maximum and average values of the model components starting from the spatial variability of the input data the model was first successfully tested calibrated in the central apennines where ei and eic estimates are available in the literature boni et al 1986 allowing the selection of the best input parameters combination the calibrated optimal model was then applied in the alpine region allowing the estimation of ei and eic values not available in the literature ei and eic values estimated in the two study areas may be helpful for a more correct and reliable estimate of the total yearly accumulated amount of hydrogeological resources despite some differences still exist between the modelled and reference data we maintain that the modelling approach proposed in this paper can be useful for the estimation of ei eic and or other water balance components at regional spatial scales and at monthly yearly temporal scales this is possible in areas where similar open input data are available and in particular for most of the european territory and part of the eurasian continent where the european soil database version 2 0 van liedekerke and panagos 2003 daroussin et al 1994 is already available software availability the hydro bm source code is still under development and it will be made available to the users upon specific request to the developers contacting one of the following e mail addresses mauro rossi irpi cnr it and marco donnini irpi cnr it acknowledgements and credits the paper partly uses data and analyses carried out by m donnini during his phd at the università degli studi di perugia funded by cnr irpi the analysis on the precipitation variability was done by m rossi using data from the italian rain gauge network managed by the italian national civil protection department dpc and funded within the agreements between dpc and cnr irpi intese operative dpc n 619 672 1015 1181 all the analyses performed in the paper were realized using software with different open source licences the hydro bm water balance model was coded in r version 3 2 x r core team 2016 using the following packages ncdf pierce 2011 raster hijmans 2014 and rgdal bivand et al 2014 additionally the r packages ggplot2 dragulescu 2013 and xlsx wickham 2009 were used for the analysis of the model results hydro bm was based on the public domain tool usgs thornthwaite water balance model version 1 1 0 written in java by mccabe and markstrom 2007 and released without a specific licence at https wwwbrr cr usgs gov projects sw mows thornthwaite html qgis version 2 18 x quatum gis development team 2017 grass version 6 4 neteler and mitasova 2008 postgresql release 9 3 x https www postgresql org and its spatial extension postgis version 2 2 x http www postgis org were used for the preparation storing and elaboration of geographical data used in the manuscript appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 005 
26399,simplicity and flexibility of meta heuristic optimization algorithms have attracted lots of attention in the field of optimization different optimization methods however hold algorithm specific strengths and limitations and selecting the best performing algorithm for a specific problem is a tedious task we introduce a new hybrid optimization framework entitled shuffled complex self adaptive hybrid evolution sc sahel which combines the strengths of different evolutionary algorithms eas in a parallel computing scheme sc sahel explores performance of different eas such as the capability to escape local attractions speed convergence etc during population evolution as each individual ea suits differently to various response surfaces the sc sahel algorithm is benchmarked over 29 conceptual test functions and a real world hydropower reservoir model case study results show that the hybrid sc sahel algorithm is rigorous and effective in finding global optimum for a majority of test cases and that it is computationally efficient in comparison to algorithms with individual ea keywords shuffled complex evolution sce hybrid optimization evolutionary algorithm ea reservoir operation hydropower software availability name of software sc sahel developer matin rahnamay naeini contact address rahnamam uci edu program language matlab year first available 2018 availability freely available to public at http chrs web uci edu resources php and mathworks website software requirements matlab 9 0 1 introduction meta heuristic optimization algorithms have gained a great deal of attention in science and engineering blum and roli 2003 boussaïd et al 2013 lee and geem 2005 maier et al 2014 nicklow et al 2010 reed et al 2013 simplicity and flexibility of these algorithms along with their robustness make them attractive tools for solving optimization problems coello et al 2007 lee and geem 2005 many of the meta heuristic algorithms are inspired by a physical phenomenon such as animals social and foraging behavior and natural selection for example simulated annealing kirkpatrick et al 1983 big bang big crunch erol and eksin 2006 gravitational search algorithm rashedi et al 2009 charged system search kaveh and talatahari 2010 are inspired by various physical phenomena ant colony optimization dorigo et al 1996 particle swarm optimization kennedy 2010 bat inspired algorithm yang 2010 firefly algorithm yang 2009 dolphin echolocation kaveh and farhoudi 2013 grey wolf optimizer mirjalili et al 2014 bacterial foraging passino 2002 genetic algorithm golberg 1989 holland 1992 and differential evolution storn and price 1997 are examples of algorithms inspired by animal s social and foraging behavior and the natural selection mechanism of darwin s evolution theorem according to the no free lunch nfl wolpert and macready 1997 theorem none of these algorithms are consistently superior to others over a variety of problems although some of them may outperform others on a certain type of optimization problem the nfl theorem has been a source of motivation for developing optimization algorithms mirjalili et al 2014 woodruff et al 2013 it has encouraged scientists and researchers to combine the strengths of different algorithms and devise more robust and efficient optimization algorithms that suit a broad class of problems qin and suganthan 2005 vrugt and robinson 2007 vrugt et al 2009 hadka and reed 2013 sadegh et al 2017 these efforts led to emergence of multi method and self adaptive optimization algorithms such as self adaptive de algorithm sade qin and suganthan 2005 a multialgorithm genetically adaptive method for single objective optimization amalgam so vrugt and robinson 2007 vrugt et al 2009 and borg hadka and reed 2013 they all regularly update the search mechanism during the course of optimization according to the information obtained from the response surface here we propose a new self adaptive hybrid optimization framework entitled shuffled complex self adaptive hybrid evolution sc sahel the sc sahel framework employs multiple evolutionary algorithms eas as search cores and enables competition among different algorithms as optimization run progresses the proposed framework differs from other multi method algorithms as it grants independent evolution of population by each ea in this framework population is partitioned into equally sized groups so called complexes each assigned to different eas number of complexes assigned to each ea is regularly updated according to their performance in general the newly developed framework has two main characteristics first all the eas evolve population in a parallel structure second each participating ea works independent of other eas the architecture of sc sahel is inspired by the concept of the shuffled complex evolution algorithm university of arizona sce ua duan et al 1992 the sce ua algorithm is a population evolution based algorithm madsen 2003 which evolves individuals by partitioning population into different complexes the complexes are evolved for a specific number of iterations independent of other complexes and then are forced to shuffle the sce ua framework employs nelder mead simplex nelder and mead 1965 technique along with the concept of controlled random search price 1987 clustering kan and timmer 1987 competitive evolution holland 1975 and complex shuffling duan et al 1993 to offer a global optimization strategy by employing these techniques the sce ua algorithm provides a robust optimization framework and has shown numerically to be competitive and efficient comparing to other algorithms such as ga for calibrating rainfall runoff models beven 2011 gan and biftu 1996 wagener et al 2004 wang et al 2010 the sce ua algorithm has been widely used in water resources management barati et al 2014 eckhardt and arnold 2001 k ajami et al 2004 lin et al 2006 liong and atiquzzaman 2004 madsen 2000 sorooshian et al 1993 toth et al 2000 yang et al 2015 yapo et al 1996 as well as other fields of study such as pyrolysis modeling ding et al 2016 hasalová et al 2016 and artificial intelligence yang et al 2017 application of the sce ua is not limited to solving single objective optimization problems the multi objective complex evolution university of arizona mocom ua is an extension of the sce ua for solving multi objective problems boyle et al 2000 yapo et al 1998 besides the sce ua architecture has been used to develop markov chain monte carlo mcmc sampling named shuffled complex evolution metropolis algorithm scem ua and the multi objective shuffled complex evolution metropolis moscem to infer posterior parameter distributions of hydrologic models vrugt et al 2003a 2003b the metropolis scheme is used as the search kernel in the scem ua and moscem ua chu et al 2010 vrugt et al 2003a 2003b there is also an enhanced version of sce ua which is developed by chu et al 2011 entitled the shuffled complex strategy with principle component analysis developed at the university of california irvine sp uci chu et al 2011 found that the sce ua algorithm may not converge to the best solution on high dimensional problems due to population degeneration phenomenon the population degeneration refers to the situation when the search particles span a lower dimension space than the original search space chu et al 2010 which causes the search algorithm to fail in finding the global optimum to address this issue the sp uci algorithm employs principle component analysis pca in order to find and restore the missing dimensions during the course of search chu et al 2011 both sce ua and sp uci start the evolution process by generating a population within the feasible parameters space then population is partitioned into different complexes and each complex is evolved independently each member of the complex has the potential to contribute to offspring in the evolution process in each evolution step more than two parents may contribute to generating offspring to make the evolution process competitive a triangular probability function is used to select parents as a result the fittest individuals will have a higher chance of being selected each complex is evolved for a specific number of iterations and then complexes are shuffled to globally share the information attained by individuals during the search the competitive complex evolution cce and modified competitive complex evolution mcce are the search cores of the sce ua and sp uci algorithm respectively the cce and mcce evolutionary processes are developed based on nelder mead nelder and mead 1965 method with some modification the evolution process in the sce ua is not limited to these algorithms in fact several studies have incorporated different eas into the structure of the sce ua algorithm for example the frog leaping fl is developed by adapting particle swarm optimization pso algorithm to the sce ua structure for solving discrete problems eusuff et al 2006 eusuff and lansey 2003 mariani et al 2011 proposed an sce ua algorithm which employs de for evolving the complexes these studies revealed the flexibility of the sce ua in combination with other types of eas however the potential of combining different algorithms into a hybrid shuffled complex scheme has not been investigated the unique structure of the sce ua algorithm along with the flexibility of the algorithm for using different eas motivated us to use the sce ua as the cornerstone of the sc sahel framework the sc sahel algorithm employs multiple eas for evolving the population in a similar structure as that of the sce ua with the goal of selecting the most suitable search algorithm at each optimization step on the one hand some eas are more capable of visiting the new regions of the search space and exploring the problem space and hence are particularly suitable at the beginning of the optimization olorunda and engelbrecht 2008 on the other hand some eas are more capable of searching within the visited regions of the search space and hence boosting the convergence process after finding the region of interest mirjalili and hashim 2010 balancing between these two steps which are referred to as exploration and exploitation moeini and afshar 2009 is a challenging task in stochastic optimization methods črepinšek et al 2013 the sc sahel algorithm maintains a balance between exploration and exploitation phases by evaluating the performance of participating eas at each optimization step eas contribute to the population evolution according to their performance in previous steps the algorithms performance is evaluated by comparing the evolved complexes before and after evolution in this process the most suitable algorithm for the problem space become the dominant search core in this study four different eas are used as search cores in the proposed sc sahel framework including modified competitive complex evolution mcce used in the sp uci algorithm modified frog leaping mfl modified grey wolf optimizer mgwo and differential evolution de to better illustrate the performance of the hybrid sc sahel algorithm the framework is benchmarked over 29 test functions and compared to sc sahel with single ea among the 29 employed test functions there are 23 classic test functions xin et al 1999 and 6 composite test functions liang et al 2005 which are commonly used as benchmarks in comparing optimization algorithms furthermore the sc sahel framework is tested for a conceptual hydropower model which is built for the folsom reservoir located in the northern california usa the objective is to maximize the hydropower generation by finding the optimum discharge from the reservoir the study period covers run off season in california from april to june in which reservoirs have the highest annual storage volume field and lund 2006 using the proposed framework we compared different eas capability of finding a near optimum solution for dry wet and below normal scenarios the results support that the proposed algorithm is not only competitive in terms of increasing power generation but also is able to reveal the advantages and disadvantages of participating eas the rest of the paper is organized as follow in section 2 structure of the sc sahel algorithm and details of four eas are presented section 3 presents the test functions settings of the experiments and results obtained for each test function section 4 introduces the reservoir model and the optimization results for the case study finally in section 5 we draw conclusion summarize some limitations about the newly introduced framework and suggest some directions for future work 2 methodology the sc sahel algorithm is a parallel optimization framework which is built based on the original sce ua architecture sc sahel however differs from the original sce ua algorithm by using multiple search mechanisms instead of only employing the nelder mead simplex downhill method in this section we first introduce the main structure of sc sahel then we present four different eas which are employed as search cores in the sc sahel framework these algorithms are selected for illustrative purpose only and can be replaced by other evolutionary algorithms some modifications are made to the original form of these algorithms to allow fair competition between eas these modifications are detailed in appendix a d 2 1 the sc sahel framework the proposed sc sahel optimization strategy starts with generating a population with a pre defined sampling method within feasible parameters range the framework supports user defined sampling methods besides built in uniform random sampling urs and latin hypercube sampling lhs the population is then partitioned into different complexes the partitioning process warrants maintaining diversity of population in each complex in doing so population is first sorted according to objective function values then sorted population is divided into ngs equally sized groups ngs being the number of complexes ensuring that members of each group have similar objective function values each complex subsequently will randomly select a member from each of these groups this procedure maintains diversity of the population within each complex the complexes are then assigned to eas and evolved in contrast to the original concept of the sce ua the complexes are evolved with different eas rather than single search mechanism at the beginning of the search an equal number of complexes is assigned to each evolutionary method for instance if population is partitioned into 8 complexes and 4 different eas are used each algorithm will evolve 2 complexes independently 2 2 2 2 after evolving the complexes for pre specified number of steps the evolutionary method performance emp metric eq 1 will be calculated for each ea 1 emp mean f mean f n mean f in which f and f n are objective function values of individuals in each complex before and after evolution respectively the emp metric measures change in the mean objective function value of individuals in each complex in comparison to their previous state a higher emp value indicates a larger reduction in the mean objective function value obtained by the individuals in the complex the performance of each evolutionary algorithm is then evaluated based on the mean value of emp calculated for each evolved complex eas are then ranked according to the emp values ranks are in turn used to assign number of complexes to each evolutionary method for the next iteration the highest ranked algorithm will be assigned an additional complex to evolve in the next shuffling step while the lowest ranked evolutionary algorithm will lose one complex for the next step for instance if all the eas have 2 complexes to evolve 2 2 2 2 case the number of complexes assigned to each ea can be updated to 3 2 2 1 in other words this logic is an award and punishment process in which the algorithm with best performances will be awarded with an additional complex to evolve in the next iteration while the worst performing algorithm will be punished by losing one complex it is worth mentioning that as some of the algorithms may have poor performance in the exploration phase they might lose all their complexes during the adaptation process this might be troublesome as these algorithms may be superior in the exploitation phase if such algorithms are terminated in the exploration phase they cannot be selected during the convergence steps hence eas termination is avoided to fully utilize the potential of eas in all the optimization steps and balance the exploration and exploitation phases the minimum number of complexes assigned to each evolutionary method is restricted to at least 1 complex in this case if the lowest ranked ea has only 1 complex to evolve it won t lose its last complex if an algorithm outperforms others throughout the evolution of complexes the number of complexes assigned to the superior ea will be equal to the total number of complexes minus the number of eas plus one in this case all other algorithms are evolving one complex only as all algorithms are evolving at least one complex they have the chance to outperform other eas and gain more complexes during the optimization process and to potentially become the dominant search method as the search continues toward exploitation phase fig 1 briefly shows the flowchart of the sc sahel algorithm pseudo code of which is as follows step 0 initialization select ngs 1 and nps suggested nps 2n 1 where n is dimension of the problem where ngs is the number of complexes and nps is the number of individuals in the complexes ngs should be proportional to the number of evolutionary algorithms so that all the participating eas have an equal number of complexes at the beginning of the search step 1 sample npt points in the feasible parameter space using a user defined sampling method where npt equals to ngs nps compute objective function value for each point step 2 rank and sort all individuals in the order of increasing objective function value step 3 partition the entire population into complexes assign complexes to the participating eas step 4 monitor and restore population dimensionality using pca algorithm optional step 5 evolve each complex using the corresponding ea step 6 after evolving the complexes for a pre defined number of iterations calculate the mean emp for each ea step 7 rank the participating eas according to the mean emp value of each evolutionary method the highest ranked method will get additional complex in the next iteration while the worst evolutionary method will lose one step 8 shuffle complexes and form a new population step 9 check whether the convergence criteria are satisfied otherwise go to step 3 sc sahel allows for different settings that can influence the performance of the algorithm careful consideration should be devoted to the selection of these settings including number of complexes number of individuals within each complex number of evolution steps before each shuffling and stopping criteria thresholds some of these settings are adopted from the suggested settings for the sce ua for instance the number of individuals within each complex is set to 2 d 1 where d is dimension of the problem however some of the suggested settings cannot be applied to the sc sahel framework due to use of different eas these settings can be changed according to the complexity of the problem and the eas employed within the framework for instance the number of complexes the number of points within each complex and the number of evolution steps before each shuffling are problem dependent the sc sahel framework employs three different stopping criteria which are adopted from sce ua and sp uci these stopping criteria include number of function evaluations range of samples that span the search space and improvement in the objective function value in the last m shuffling steps these criteria are compared to pre defined thresholds which can in turn be tuned according to the complexity of the problem improper selection of these thresholds may lead to early or delayed convergence 2 2 evolutionary algorithms employed within sc sahel in this paper we employ four different eas to illustrate the flexibility of the sc sahel framework in adopting various eas and show the algorithms competition these algorithms are briefly presented here the pseudo code and details of these algorithms can be found in appendix a d 2 2 1 modified competitive complex evolution mcce the mcce algorithm is an enhanced version of cce algorithm used in the sce ua framework which provides a robust efficient and effective ea for exploring and exploiting the search space the mcce algorithm is developed based on the nelder mead algorithm however chu et al 2011 found that the shrink concept in the nelder mead algorithm can cause premature convergence to a local optimum interested readers can refer to chu et al 2010 2011 for further details on mcce algorithm the pseudo code of the mcce algorithm is detailed in appendix a sc sahel has similar performance to sp uci when the mcce algorithm is used as the only search mechanism and pca and resampling settings of sp uci are enabled for simplification and comparison sc sahel with the mcce algorithm as search core is referred as sp uci hereafter 2 2 2 modified frog leaping mfl the frog leaping fl algorithm uses adapted pso algorithm as a local search mechanism within the sce ua framework eusuff and lansey 2003 fl has shown to be an efficient search algorithm for discrete optimization problems and can find optimum solution much faster as compared to the ga algorithm eusuff et al 2006 in order to adapt the fl algorithm to the sc sahel parallel framework we introduce a slightly modified version of fl algorithm entitled mfl further details and pseudo code of the mfl can be found in appendix b the original fl algorithm and the mfl have four main differences first the original fl is designed for discrete optimization problems however the mfl is modified for continuous domain second the modified fl uses the best point in the subcomplex for generating new points however in the original fl framework new points are generated using the best point in the complex and the entire population the reason for this modification is to avoid using any external information by participating eas in other words the amount of information given to each eas is limited to the complex assigned to the eas third as the mfl algorithm only uses the best point within the complex for generating the new generation two different jump rates are used the reason for different jump rates is to allow mfl to have a better exploration and exploitation ability during optimization process these jump rates are selected by trial and error and may need further investigation to achieve a better performance by mfl algorithm fourth when the generated offspring is not better than the parents a new point is randomly selected within the range of individuals in the subcomplex this process which is referred to as censorship step in the fl algorithm eusuff et al 2006 is different from the original algorithm the mfl algorithm uses the range of points in the complex rather than the whole feasible parameters range resampling within the whole parameter space can decrease the convergence speed of the fl algorithm hence the resampling process is carried out only within the range of points in the complex hereafter the sc sahel with mfl algorithm as the only search core is referred as sc mfl 2 2 3 modified grey wolf optimizer mgwo the grey wolf optimizer is a meta heuristic algorithm inspired by the social hierarchy and hunting behavior of grey wolves mirjalili et al 2014 2016 grey wolves hunting strategy has three main steps first chasing and approaching the prey second encircling and pursuing the prey and finally attacking the prey mirjalili et al 2014 the gwo process resembles the hunting strategy of the grey wolves in this algorithm the top three fittest individuals are selected and contribute to the evolution of population hence the individuals in the population are navigated toward the best solution the gwo algorithm has shown to be effective and efficient in many test functions and engineering problems furthermore performance of gwo is comparable to other popular optimization algorithms such as ga and pso mirjalili et al 2014 gwo follows an adaptive process to update the jump rates to maintain balance between exploration and exploitation phases the adaptive jump rate of the gwo is removed here and 3 different jump rates are used instead the reason for this modification is that the information given to each ea is limited to its assigned complex similar to mfl algorithm the modified gwo mgwo algorithm uses the range of parameters to resample individuals when the generated offspring are not superior to their parents details and pseudo code of the mgwo algorithm can be found in appendix c hereafter the sc sahel with mgwo algorithm as the only search core is referred as sc mgwo 2 2 4 differential evolution de the de algorithm is a powerful but simple heuristic population based optimization algorithm qin and suganthan 2005 sadegh and vrugt 2014 proposed by storn and price 1997 in 2011 mariani et al 2011 integrated the de algorithm into sce ua framework and showed that the new framework is able to provide more robust solutions for some optimization problems in comparison to the sce ua similar to the work by mariani et al 2011 we use a slightly modified de algorithm based on the concepts from qin and suganthan 2005 in order to integrate the de algorithm into the sc sahel framework as the de algorithm has slower performance in comparison to other eas used here we have added multiple steps to the de here the de algorithm uses three different mutation rates in three attempts in the first attempt the algorithm uses a larger mutation rate this helps exploring the search space with larger jump rates in the second attempt the algorithm reduces the mutation rate to a quarter of the first attempt this will enhance the exploitation capability of the ea if none of these mutation rates could generate a better offspring than the parents in the next attempt the mutation rate is set to half of the first attempt lastly if none of these attempts generate a better offspring in comparison to the parents a new point is randomly selected within the range of individuals in the complex the pseudo code of the modified de algorithm is detailed in appendix d the sc sahel algorithm is referred to as sc de when the de algorithm is used as the only search algorithm 3 conceptual test functions and results 3 1 test functions the sc sahel framework is benchmarked over 29 mathematical test functions using single method and multi method search mechanisms this includes 23 classic test functions obtained from xin et al 1999 the name and formulation of these functions along with their dimensionality and range of parameters are listed in table 1 we selected these test functions as they are standard and popular benchmarks for evaluating new optimization algorithms mirjalili et al 2014 the remaining 6 are composite test functions c f 1 6 liang et al 2005 which represent complex optimization problems details of the composite test functions can be found in the work of liang et al 2005 and mirjalili et al 2014 classic test functions have dimensions in the range of 2 30 and all the composite test functions are 10 dimensional figs 2 and 3 show response surface of the test functions which can be shown in 2 dimension form the sc sahel settings used for optimizing these test functions are listed in table 2 for each test function number of points in each complex and number of evolution steps for each complex are set to 2d 1 and max d 1 10 respectively where d is the dimension of the problem the number of evolution steps is set to max d 1 10 to guarantee that eas evolve the complexes for enough number of steps before evaluating the eas in the high dimension problems the maximum number of function evaluation should be selected with careful consideration several experiments were conducted to find an optimal set of parameters for the sc sahel setting these experiments revealed that a low number of evolutionary steps before shuffling the complexes may not show the potential of the eas on the other hand using a large value for the number of evolution steps may shrink the complex to a small space which cannot span the whole search space duan et al 1994 maximum number of function evaluation is determined according to the complexity of the problem and is different for each of the test cases in addition to the maximum number of function evaluation the range of the parameters in the population and the improvement in the objective function values are used as convergence criteria the optimization run is terminated if the population range is smaller than 10 7 of the feasible range or the improvement in objective function value is smaller than 0 1 of the mean objective function value in the last 50 shuffling steps the lhs mechanism is used as the sampling algorithm of sc sahel for generating the initial population the framework provides multiple settings for boundary handling which can be selected by user sc sahel uses reflection as the default boundary handling method other initial sampling and boundary handling methods are also implemented in the sc sahel framework sensitivity of the initial sampling and boundary handling on the performance of the sc sahel algorithm is not studied in this paper the aforementioned settings can be applied to a wide range of problems 3 2 results and discussion table 3 illustrates the statistics of the final function values at 30 independent runs on 29 test functions using the hybrid sc sahel and individual eas with the goal to minimize the function values the best mean function value obtained for each test function is expressed in bold in table 3 results show that the hybrid sc sahel achieved the lowest function values in 15 out of 29 test functions compared to the mean function values achieved by all individual algorithms it is noteworthy that in 20 out of 29 test functions the hybrid sc sahel was among the top two optimization methods in finding the minimum function value a two sample t test with 5 significance level also showed that the results generated with the sc sahel algorithm is generally similar to the best performing algorithms comparing among single method algorithms in general the statistics obtained by sp uci are superior to other participating eas in 12 out of 29 test functions the sp uci algorithm achieved the lowest function value sc mfl sc mgwo and sc de were superior to other algorithms in 6 10 and 11 out of 29 test functions respectively in test functions f 6 f 16 f 17 f 18 f 19 f 20 and f 23 the single method and multi method algorithms achieved same function values on average in most cases in these cases according to the statistics shown in table 3 the sp uci and sc sahel algorithms offer lower standard deviation values and show more consistent results as compared to other eas the low standard deviation values obtained by sp uci and sc sahel indicate the robustness and consistency of these two algorithms in comparison to other algorithms in the test functions that the hybrid sc sahel algorithm was not able to produce the best mean function value the achieved mean function values deviation from that of the best performing algorithms are marginal for instance on the test functions f 2 f 4 f 10 and f 22 the statistics of the values obtained by sc sahel are similar to that achieved by the best performing methods which are sp uci and sc mgwo in general the hybrid sc sahel algorithm is superior to algorithms with individual ea on most of the test functions although on some test functions the sc sahel algorithm is slightly inferior to the best performing algorithm with only marginal differences the performance of the sc sahel in these test functions can be attributed to two main reasons first in the hybrid algorithm all the eas are involved in the evolution of the population hence if one of the algorithms have poor performance in comparison to other eas it still evolves a portion of the population as the complexes are evolved independently the poor performing eas may devastate a part of the information in the evolving complex on the other hand when the algorithms are used individually in the sc sahel framework the ea utilizes the information in all the complexes and the whole population in this case better result will be achieved in comparison to the hybrid sc sahel if the ea is the fittest algorithm for the problem space second some of the eas are faster and more efficient in a specific optimization phase exploration exploitation than others however they might not be as effective as other eas for other optimization phases hence dominance of these algorithm during the exploration or exploitation phases can mislead other eas and cause early and premature convergence engagement of other algorithms in the evolution process may prevent early convergence in these cases generally the performance criteria emp is responsible for selecting the most suitable algorithm in each optimization step however the criteria used in the sc sahel is not guaranteed to perform well in all problem spaces the performance criteria are problem dependent and need further investigations based on the problem space and eas however the emp metric seems to be a suitable metric for a wide range of problems to further evaluate the performance of the hybrid sc sahel algorithm we present the success rate of the algorithms in fig 4 the success rate is defined by setting target values for the function value for each test function when the function value is smaller than the target value the goal of optimization is reached and therefore the algorithm is considered successful a higher success rate resembles a better performance we use same target value for all algorithms in order to have a fair comparison according to fig 4 in 16 out of 29 test functions the hybrid algorithm achieved 100 success rate in other cases the success rates achieved by the proposed hybrid algorithm are comparable to the best performing algorithm with single ea for instance on the test function f 9 the sc mgwo sc de and sc mfl are not successful in finding the optimum solution success rates are 0 0 and 10 respectively however the hybrid sc sahel algorithm has similar performance 80 success rate to sp uci 97 success rate on the test function f 21 the success rate of the hybrid sc sahel algorithm 87 is close to the sc mgwo 93 which is the most successful algorithm the hybrid sc sahel algorithm also achieved a higher success rate than sp uci algorithm 33 in this test function according to fig 4 the average success rate of sc sahel is about 80 over all 29 test functions and it is the highest compared to the average success rate of other eas i e 73 58 58 and 54 for sp uci sc de sc mgwo and sc mfl algorithm respectively in some situations the poor performing eas may mislead other eas and cause early and premature convergence for instance on the test function c f 5 the hybrid algorithm achieved 57 success rate which is still better success rate than sp uci sc mfl and sc mgwo which are 0 10 and 50 respectively on this test function c f 5 the performance of the hybrid sc sahel is less affected by the most successful algorithm de this may be due to the low evolution speed of the de algorithm as the sc sahel algorithm maintains both convergence speed and efficiency during the entire search the hybrid sc sahel presents promising performance on the test functions c f 2 and c f 3 on test functions c f 2 and c f 3 the success rate of hybrid sc sahel is significantly higher than other eas most of which have 0 success rates for test function c f 2 the sc de algorithm achieved the lowest objective function value and the highest success rate 37 among single method algorithms however when eas are combined in the hybrid form the objective function value and the success rate are significantly improved this shows that sc sahel has the capability of solving complex problems by utilizing the potentials and advantages of all participating algorithms and improving the search success rate in table 4 we present the mean and standard deviation of the number of function evaluation which indicates the speed of each algorithm the lowest mean number of function evaluation is expressed in bold in table 4 as one of the stopping criteria in sc sahel framework is the maximum number of function evaluation some algorithms may terminate before they show their full potential for instance the sc de and the sc mfl usually reach the maximum number of function evaluations while other algorithms satisfy other convergence criteria in much less number of function evaluations in this case the objective function value doesn t represent the potential of the slow algorithms to give a better insight into this matter the mean and standard deviation std of the number of function evaluations are compared in table 4 the goal is to compare the speed of the individual eas and the hybrid optimization algorithm according to table 4 in most of the test cases the sp uci algorithm has the least number of function evaluations regardless of the objective function value achieved by the eas comparing the success rate and the number of function evaluation for different eas shows that sp uci achieved 100 success rate with the lowest number of function evaluation in 15 out of 29 test functions the sc mgwo algorithm only achieved 100 success rate with the lowest number of function evaluation in one test function although the hybrid sc sahel algorithm is not the fastest algorithm its speed is usually close to the fastest algorithm this is due to the contribution of different eas in the evolution process and the eas behavior on different problem spaces for instance de algorithm is slower in comparison to mcce sp uci algorithm in most of the test functions hence when the algorithms are working in a hybrid form the hybrid algorithm will be slower than the situation when the mcce sp uci algorithm is used individually figs 5 7 compare the average number of complexes assigned to each ea for the 29 employed test functions during the course of the search the variation of the number of complexes assigned to each ea indicates the dominance of each ea during the course of the search hence the performance of eas at each optimization step can be monitored in many test cases mcce sp uci algorithm has a relatively higher number of complexes than other eas during the search this shows that mcce is a dominant search algorithm on most of the test functions however in some other cases mcce is only dominant in a certain period of the search while other eas have demonstrated better efficiency during the entire search for example on test functions f 7 and f 20 mcce algorithm appears to be dominant only during the beginning of the search in the test function f 7 the exploration process starts with the dominance of the mcce and shifts between mgwo and mfl after the first 20 shuffling steps in some of the test functions such as f 7 a more random fluctuation is observed in the number of complexes assigned to each ea the reason for this behavior is the close competition of eas in these shuffling steps due to the noisy response surface of the test function f 7 most of the eas cannot significantly improve the objective function values during the exploitation phase on test functions f 8 and f 18 the mfl and de algorithms are the dominant search methods respectively during the beginning of the run while mcce algorithm becomes dominant only when the algorithm is in exploitation phase lastly on test functions f 9 f 22 c f 1 and c f 4 the variations of the number of complexes and the precedence of different eas as the most dominant search algorithm are observed it is worth mentioning that figs 5 7 show the number of complexes assigned to each ea for a single optimization run our observation of each individual run results not shown herein shows variation of the number of complexes among different runs is similar to each other for most test cases the observed variation for individual runs follows a specific pattern and is not random the similarity of the eas dominance pattern indicates that the selection of the eas by the sc sahel framework only depends on the characteristics of the problem space and the eas employed this also indicates that different eas have pros and cons on different optimization problems as a summary of our experiments on the conceptual test functions tables 3 and 4 and figs 4 7 the main advantage of the sc sahel algorithm over other optimization methods is its capability of revealing the trade off among different eas and illustrating the competition of participating eas different optimization problems have different complexity which introduces various challenges for each ea by incorporating different types of eas in a parallel computing framework and implementing an award and punishment logic the newly developed sc sahel framework not only provides an effective tool for global optimization but also gives the user insights about advantages and disadvantages of participating eas on individual optimization tasks this shows the potential of the sc sahel framework for solving different class of problems with different level of complexity besides the hybrid sc sahel algorithm is superior to shuffled complex based methods with single search mechanism such as sp uci in an absolute majority of the test functions 4 example application and results in this section we demonstrate an example application of the newly developed sc sahel algorithm a conceptual reservoir model is developed with the goal of maximizing hydropower generation on a daily basis operation the model is applied to the folsom reservoir in northern california 4 1 reservoir model a conceptual model is set up based on the relationship between the hydropower generation storage water head and bathymetry of the folsom reservoir daily releases from the reservoir in the study period are treated as the parameters of the model which in turn determines the problem dimensionality the model objective is to maximize the hydropower generation for a specific period the total hydropower production is a function of the water head difference between forebay and tailwater and the turbine flow rate the driving equation of the model is based on mass balance water budget which is formulated as 2 s t s t 1 i t r t m t where s t is storage at time step t i t and r t signify total inflow and release from the reservoir at time t respectively m t is total outflow inflow error which is derived by setting up mass balance for daily observed data the objective function employed here is 3 of t 1 n 1 p t p c where p c is total power plant capacity in mw and p t is total power generated in day t in mw for each day p t is derived as follow 4 p t η ρ g q t h t where η signifies turbine efficiency ρ is water density kg m3 g is gravity 9 81 m s2 and q t is discharge m3 s at time step t h t is hydraulic head m at time step t which is defined as 5 h t h f h t w where h f and h t w are water elevation in forebay and tailwater respectively h f and h t w are derived by fitting a polynomial to reservoir bathymetry data in the reservoir model coined above multiple constraints are considered for better representation of the real behavior of the system these constraints include power generation capacity storage level spill capacity and changes in the daily hydropower discharge total daily power generation is compared to maximum capacity of the hydropower plant also rule curve is used to control reservoir storage level during the operation period besides final simulated reservoir storage is constrained to 0 9 1 1 of the observed storage in another word 10 variation from the observation data is allowed for the final simulated storage level this constraint adds information from real reservoir operation into the optimization process this constraint can be replaced by other operation rules for simulation purposes the spill capacity of dam is calculated according to the water level in the forebay and compared to simulated spilled water a quadratic function is fitted to the water level and spill capacity data to derive the spill capacity at each time step the change in daily hydropower release is also constrained to better represent actual hydropower discharge and avoid large variation in a daily release the reservoir model used here is non linear and continuous the constraints of the model render finding the feasible solution a challenging task for all the eas the sc sahel framework is used to maximize the hydropower generation by minimizing the objective function value the settings used for the sc sahel is similar to the settings used for the mathematical test functions however the maximum number of function evaluations is set to 106 lower bound of the parameters range varies monthly due to the operational rules however upper bound is determined according to the hydraulic structure of the dam 4 2 study basin folsom reservoir is located on the american river in northern california and near sacramento california folsom dam was built by us army corps of engineers during 1948 1956 and is a multi purpose facility the main functions of the facility are flood control water supply for irrigation hydropower generation maintaining environmental flow water quality purposes and providing recreational area the reservoir has a capacity of 1 203 878 290 m3 and the power plant has a total capacity of 198 7 mw three different periods are considered here the first study period is april 1st 2010 to june 30th 2010 the year 2010 is categorized as below normal period according to california department of water resources the same period is selected in 2011 and 2015 as former is categorized by california department of water resources as wet and latter is classified as critical dry year the input and output from the reservoir are obtained from california data exchange center http cdec water ca gov note that demand is not included in the model because demand data was not available from a public data source 4 3 results and discussion the boxplot of the objective function values is shown in fig 8 for the folsom reservoir during the runoff season in 2015 2010 and 2011 which are dry below normal and wet years respectively the presented results are based on 30 independent optimization runs however infeasible objective function values are removed the feasibility of the solution is evaluated according to the objective function values due to the large values returned by the penalty function considered for infeasible solutions such solutions can be distinguished from the feasible solutions for wet year 2011 case sc mgwo and sc de didn t find a feasible solution in 2 and 4 runs out of 30 independent runs respectively the hybrid sc sahel found feasible solutions in all the cases however some of these solutions are not global optima on average the hybrid sc sahel algorithm is able to achieve the lowest objective function value as compared to other algorithms during dry and below normal period during dry and below normal periods sc sahel sp uci and sc de show similar performance in the wet period the sp uci algorithm achieved the lowest objective function value the sc sahel algorithm ranked second comparing the mean objective function values in this period the results achieved by the sc de is also comparable to sc sahel and sp uci the results show that overall the hybrid sc sahel algorithm has similar or superior performance in comparison to the single method algorithms also the results achieved by sc sahel and sp uci algorithms has less variability in comparison to other algorithms which show the robustness of these algorithms the worst performing algorithm is the sc mgwo which achieved the least mean objective function value in all the study periods in fig 9 boxplot of the number of function evaluations is presented for successful runs from the 30 independent runs during dry below normal and wet period years although the sc mgwo algorithm satisfied convergence criteria in the least number of function evaluation the sc mgwo was not successful in achieving the optimum solution in many cases the sp uci algorithm is the second fastest method among all the algorithms the hybrid sc sahel sc mfl and sc de are the slowest algorithm for satisfying the convergence criteria in almost all cases the slow performance of the hybrid sc sahel is due to the fact that 2 out of 4 de and mfl participating eas have very slow performance over the response surface fig 10 demonstrates the number of complexes assigned to each ea during the search which indicates the dominance of the participating algorithms and the award and punishment logic in the reservoir model as seen in fig 10 the mgwo algorithm is dominant in the beginning of the search although it is not capable of finding the optimum solution in most cases the reason for the dominance of the mgwo is the speed of the algorithm in exploring the search space mgwo is superior to other eas in the beginning of the search however after a few iterations the mcce algorithm took the precedence and become the dominant algorithm over other eas mgwo and de are less involved in the rest of the optimization process after the initial steps however competition between mcce and mfl continues although contribution of mgwo and de are at minimum in the rest of the optimization process they are utilizing a part of information within the population this can affect the speed and performance of the sc sahel algorithm in both the wet and below normal cases the hybrid sc sahel algorithm is mostly terminated by reaching the maximum number of function evolution however the mean objective function value obtained by the hybrid sc sahel is still superior to most of the algorithms the performance of the sc sahel can be affected by the settings of the algorithm different settings have been tested and evaluated for the reservoir model the results show that the number of evolution steps before shuffling can influence the performance of the hybrid sc sahel algorithm in the current setting the number of evolution steps within each complex is set to d 1 d is dimension of the problem although this setting seems to provide acceptable performance for a wide range of problems it may not be the optimum setting for all the problems spaces and eas in the reservoir model as the study period has 91 days the model evolves each complex for 92 steps this number of evolution steps allows the algorithms to navigate the complexes toward local solutions and increase the total number of function evaluations without specific gain decreasing the number of evolution steps allows the algorithms to communicate more frequently so they can use the information obtained by other eas here for demonstrative purposes the same setting has been applied to all the problems however better performance is observed for the hybrid sc sahel algorithm when the number of evolution steps are set to a value smaller than 92 the algorithm is less sensitive to other settings for the reservoir model however they can still affect the performance of the algorithm in fig 11 we present the simulated storage for different study periods achieved by different eas during the dry period not only the sc sahel algorithm achieved the lowest objective function value but also the storage level is higher than the observed storage level in most of the period this is due to the fact that power generation is a function of water height as well as discharge rate during below normal period sc sahel sp uci and sc de algorithms show a similar behavior in terms of the storage level during wet period storage level simulated by sp uci and sc sahel algorithm is lower than all other algorithms it is worth noting that during wet period sc sahel and sp uci algorithms are able to find optimum solution which objective function value is 0 in some of the runs however the simulated storage by these algorithms show some level of uncertainties fig 11 this shows equifinality in simulation which means that same hydropower generation can be achieved by different sets of parameters feng et al 2017 this equifinality can be due to deficiencies in the model structure or the boundary conditions freer et al 1996 the wet period seems to offer a more complex response surface for the reservoir model during the wet period some algorithms such as sc de are not capable of finding a feasible solution in some of the runs in this period the large input volume and the rule curve added more complexity to the optimization problem the results of the real world application show the potential of the newly developed sc sahel framework for solving high dimension problems in general the hybrid algorithm was more successful in finding a feasible solution in comparison to single method algorithms in some cases the hybrid sc sahel was terminated due to the large number of function evaluations however the performance of the hybrid sc sahel is always comparable to the best performing method this shows the potential of the sc sahel for solving a broad class of optimization problems besides the framework provides insight into the performance of the algorithms at different steps of the optimization process this feature of the sc sahel algorithm can aid user to select the best setting and ea for the problem 5 conclusions and remarks we developed a hybrid optimization framework named shuffled complex self adaptive hybrid evolution sc sahel which uses an award and punishment logic in junction with various types of evolutionary algorithms eas and selects the best ea that fits well to different optimization problems the framework provides an arsenal of tools for testing evaluating and developing optimization algorithms we compared the performance of the hybrid sc sahel with single method algorithms on 29 test functions the results showed that the sc sahel algorithm is superior to most of single method optimization algorithms and in general offers a more robust and efficient algorithm for optimizing various problems furthermore the proposed algorithm is able to reveal the characteristics of different eas during entire search period the algorithm is also designed to work in a parallel framework which can take the advantage of available computation resources the newly developed sc sahel offers different advantages over conventional optimization tools some of the sc sahel characteristics are intelligent evolutionary method adaptation during the optimization process flexibility of the algorithm for using different evolutionary methods flexibility of the algorithm for using initial sampling and boundary handling method independent parallel evolution of complexes population degeneration avoidance using pca algorithm robust and fast optimization process evolutionary algorithms comparison for different types of problems although the presented results support advantage of the hybrid sc sahel to algorithms with individual eas there are multiple directions for further improvement of the framework for example eas performance metric for evaluating the search mechanism in the current algorithm the complex allocation to different ea is carried out by ranking the algorithm according to the emp metric the performance criteria can change the allocation process and affect the performance of the algorithm depending on the application a more comprehensive performance criterion may be necessary for achieving the best performance however the current emp criterion does not affect the conclusion and comparison of different eas in addition the current sc sahel framework is designed to solve single objective optimization problems a multi objective version can be developed to extend the scope of the application this paper serves as an introduction to the newly developed sc sahel algorithm we hope that more investigation on the interaction among different eas boundary handling schemes and response surface in different case studies and optimization problems reveal the advantages and limitations of sc sahel acknowledgments and data this work is supported by u s department of energy doe prime award de ia0000018 california energy commission cec award 300 15 005 nsf cybersees project award ccf 1331915 noaa nesdis ncdc prime award na09nes4400006 and ncsu cics and subaward 2009 1380 01 and the u s army research office award w911nf 11 1 0422 the folsom reservoir bathymetry information used here is provided by dr erfan goharian from uc davis who also helped us for setting up the reservoir model the authors would like to thank the comments of the editors and four anonymous reviewers which significantly improved the quality of this manuscript appendix a modified competitive complex evolution mcce mcce algorithm pseudo code is as follow step 0 initialize i 1 and get maximum number of iteration allowed i step 1 sort individuals in order of increasing objective function value assign individuals a triangular probability except for the fittest point according to a1 p 2 nps 1 n nps nps 1 where nps is the number of individuals in the complex and n is the rank of the sorted individuals step 2 select d 1 individuals d is problem dimension from the complex including the fittest individual in the complex step 3 the selected individuals are then stored in s forming a simplex generate offspring according to following steps i sort individuals in s according to their objective function value find centroid c of the first d individuals ii reflection reflect the worst individual in s w across the centroid to generate a new point r according to following equation a2 r 2 c w evaluate objective function for the new point f r if f 1 f r f d set offspring o r and go to vii iii expansion if f r f 1 reflect c across r and generate e a3 e 2 r c evaluate objective function for the new point f e if f e f r set o e and go to vii otherwise o r and go to vii iv outside contraction if f d f r f w calculate the outside contraction point a4 o c c 0 5 r c evaluate the outside contraction point f o c if f o c f r set o o c and go to vii otherwise o r and go to vii v inside contraction if f w f r calculate inside contraction point a5 i c c 0 5 w c evaluate inside contraction point f i c if f i c f r set o i c and go to vii otherwise continue to vi vi multinormal sampling if the steps above did not generate a better offspring an individual will be drawn with a multinormal distribution defined by simplex and replace the worst individual in the simplex regardless of objective function value the multinormal sampling is as follow a calculate the covariance matrix r for the simplex and store diagonal of matrix in d b modify d as follow a6 d m 2 d m e a n d c generate a new covariance matrix r with d m as diagonal and zeroes everywhere else d sample a point with multinormal distribution with mean of c and covariance of r and store in o vii replace the worst individual in the complex with o let i i 1 if i i go to step 1 otherwise sort the points in the complex and return the evolved complex appendix b modified frog leaping mfl modified fl mfl algorithm is as follow step 0 initialize i 1 and get maximum number of iteration allowed i step 1 sort individuals in order of increasing objective function assign individuals a triangular probability using following equation b1 p 2 nps 1 n nps nps 1 where nps is the number of individuals in the complex and n is the rank of the sorted individuals step 2 select d 1 individuals d is problem dimension from the complex step 3 the selected individuals are stored in s forming a subcomplex generate offspring according to following steps i generate a new point with the worst point in s w and best point b in the subcomplex as follow b2 n b w 0 5 r 1 5 b w where r is a random number in the range of 0 1 evaluate objective function for the new point and get f b if f b f w set o n b and go to iv ii if f w f b generate a new point with the worst point in s w and best point b in the subcomplex as follow b3 n b w 0 5 r b w evaluate objective function for the new point and get f b if f b f w set the offspring set o n b and go to iv iii censorship step if f w f b randomly generate the offspring o by sampling within the range of individuals in the subcomplex iv replace the worst individual in the complex with the offspring o let i i 1 if i i go to step 1 otherwise sort the points in the complex and return the evolved complex appendix c modified grey wolf optimizer gwo modified grey wolf optimizer is as follow step 0 initialize i 1 and get maximum number of iteration allowed i step 1 sort the individuals in the order of increasing objective function value assign individuals a triangular probability except for the fittest point using following equation c1 p 2 nps 1 n nps nps 1 where nps is the number of individuals in the complex and n is the rank of the sorted individuals step 2 select d 1 individuals d is problem dimension from the complex with triangular probability including the fittest point in the complex and store them in s step 3 select the best three points in the s and store them in α β and γ respectively the worst point in the s is stored in w step 4 for each of α β and γ evolve individuals according to the following procedure i derive a and c as follow for α β and γ c2 a 4 r 1 2 c3 c 2 r 2 where r 1 r 2 are two independent random vectors which have d dimensions and values in range of 0 1 ii derive d for α β and γ as follow c4 d α c α x α w d β c β x β w d γ c γ x γ w iii derive z for α β and γ as follow c5 z α x α a α d α z β x β a β d β z γ x γ a γ d γ iv generate new point by finding the centroid of z α z β and z γ c6 c z α z β z γ 3 v calculate and store objective function value for the new point f c if the new point is better than the worst point among the selected points f c f w set o c go to step 7 step 5 if f c f w go to step 4 and use a smaller range for a in this step a is calculated as follow c7 a 2 r 1 1 step 6 if the newly generated individual is worse than the worst individuals in subcomplex generate a new point with uniform random sampling within the range of individuals in the complex store the new point in o step 7 replace the worst individual among selected points in the complex with the offspring o let i i 1 if i i go to step 1 otherwise sort the points in the complex and return the evolved complex appendix d modified differential evolution de modified differential evolution algorithm is as follow step 0 initialize i 1 and get maximum number of iteration allowed i step 1 sort the individuals in the order of increasing objective function value assign individuals a triangular probability using following equation d1 p 2 nps 1 n nps nps 1 where nps is the number of individuals in the complex and n is the rank of the sorted individuals step 2 select d 1 points d is problem dimension from the complex with the assigned probability and store them along with the fittest point in the complex in s step 3 the selected individuals are sorted and stored in s forming a subcomplex generate offspring according to following steps i generate a new point with the worst point in s w and using the top three individuals in the subcomplex d2 v 1 w 2 f s 1 w 2 f s 2 s 3 where w is the worst point in the s s 1 s 2 and s 3 are three selected individuals then mutation and crossover operator is applied to the w and v 1 to generate v n 1 the objective function value for the new point is calculated and stored in f n 1 if f n 1 f w set o v n 1 and go to v ii if f w f n 1 generate a new point with the worst point in s w and using the top three points in the subcomplex as follow d3 v 2 w 0 5 f s 1 w 0 5 f s 2 s 3 after mutation crossover operator is applied to the w and v 2 to generate v n 2 then the objective function for the new point is derived and stored in f n 2 if f n 2 f w set o v n 2 and go to v iii if f w f n 2 generate a new point with the worst point in s w and using the top three points in the subcomplex as follow d4 v 3 w f s 1 w f s 2 s 3 after mutation crossover operator is applied to the w and v 3 to generate v n 3 the objective function value is calculated and stored in f n 3 if f n 3 f w set o v n 3 and go to v iv if the newly generated point is worse than the worst point in subcomplex generate a new point from uniform random distribution within the range of points in the complex store the new point in o v replace the worst point in the complex with the offspring o let i i 1 if i i go to step 1 otherwise sort the points in the complex and return the evolved complex abbreviations amalgam so a multialgorithm genetically adaptive method for single objective optimization cce competitive complex evolution de differential evolution ea evolutionary algorithm emp evolutionary methods performance fl frog leaping gwo grey wolf optimizer lhs latin hypercube sampling mcce modified competitive complex evolution mcmc markov chain monte carlo mfl modified frog leaping mgwo modified grey wolf optimizer mocom ua multi objective complex evolution university of arizona moscem multi objective shuffled complex evolution metropolis nfl no free lunch pca principal component analysis pso particle swarm optimization sade self adaptive differential evolution sce ua shuffle complex evolution developed at university of arizona scem ua shuffled complex evolution metropolis algorithm developed at university of arizona sc sahel shuffle complex self adaptive hybrid evolution sp uci shuffled complex strategy with principal component analysis developed at university of california irvine urs uniform random sampling 
26399,simplicity and flexibility of meta heuristic optimization algorithms have attracted lots of attention in the field of optimization different optimization methods however hold algorithm specific strengths and limitations and selecting the best performing algorithm for a specific problem is a tedious task we introduce a new hybrid optimization framework entitled shuffled complex self adaptive hybrid evolution sc sahel which combines the strengths of different evolutionary algorithms eas in a parallel computing scheme sc sahel explores performance of different eas such as the capability to escape local attractions speed convergence etc during population evolution as each individual ea suits differently to various response surfaces the sc sahel algorithm is benchmarked over 29 conceptual test functions and a real world hydropower reservoir model case study results show that the hybrid sc sahel algorithm is rigorous and effective in finding global optimum for a majority of test cases and that it is computationally efficient in comparison to algorithms with individual ea keywords shuffled complex evolution sce hybrid optimization evolutionary algorithm ea reservoir operation hydropower software availability name of software sc sahel developer matin rahnamay naeini contact address rahnamam uci edu program language matlab year first available 2018 availability freely available to public at http chrs web uci edu resources php and mathworks website software requirements matlab 9 0 1 introduction meta heuristic optimization algorithms have gained a great deal of attention in science and engineering blum and roli 2003 boussaïd et al 2013 lee and geem 2005 maier et al 2014 nicklow et al 2010 reed et al 2013 simplicity and flexibility of these algorithms along with their robustness make them attractive tools for solving optimization problems coello et al 2007 lee and geem 2005 many of the meta heuristic algorithms are inspired by a physical phenomenon such as animals social and foraging behavior and natural selection for example simulated annealing kirkpatrick et al 1983 big bang big crunch erol and eksin 2006 gravitational search algorithm rashedi et al 2009 charged system search kaveh and talatahari 2010 are inspired by various physical phenomena ant colony optimization dorigo et al 1996 particle swarm optimization kennedy 2010 bat inspired algorithm yang 2010 firefly algorithm yang 2009 dolphin echolocation kaveh and farhoudi 2013 grey wolf optimizer mirjalili et al 2014 bacterial foraging passino 2002 genetic algorithm golberg 1989 holland 1992 and differential evolution storn and price 1997 are examples of algorithms inspired by animal s social and foraging behavior and the natural selection mechanism of darwin s evolution theorem according to the no free lunch nfl wolpert and macready 1997 theorem none of these algorithms are consistently superior to others over a variety of problems although some of them may outperform others on a certain type of optimization problem the nfl theorem has been a source of motivation for developing optimization algorithms mirjalili et al 2014 woodruff et al 2013 it has encouraged scientists and researchers to combine the strengths of different algorithms and devise more robust and efficient optimization algorithms that suit a broad class of problems qin and suganthan 2005 vrugt and robinson 2007 vrugt et al 2009 hadka and reed 2013 sadegh et al 2017 these efforts led to emergence of multi method and self adaptive optimization algorithms such as self adaptive de algorithm sade qin and suganthan 2005 a multialgorithm genetically adaptive method for single objective optimization amalgam so vrugt and robinson 2007 vrugt et al 2009 and borg hadka and reed 2013 they all regularly update the search mechanism during the course of optimization according to the information obtained from the response surface here we propose a new self adaptive hybrid optimization framework entitled shuffled complex self adaptive hybrid evolution sc sahel the sc sahel framework employs multiple evolutionary algorithms eas as search cores and enables competition among different algorithms as optimization run progresses the proposed framework differs from other multi method algorithms as it grants independent evolution of population by each ea in this framework population is partitioned into equally sized groups so called complexes each assigned to different eas number of complexes assigned to each ea is regularly updated according to their performance in general the newly developed framework has two main characteristics first all the eas evolve population in a parallel structure second each participating ea works independent of other eas the architecture of sc sahel is inspired by the concept of the shuffled complex evolution algorithm university of arizona sce ua duan et al 1992 the sce ua algorithm is a population evolution based algorithm madsen 2003 which evolves individuals by partitioning population into different complexes the complexes are evolved for a specific number of iterations independent of other complexes and then are forced to shuffle the sce ua framework employs nelder mead simplex nelder and mead 1965 technique along with the concept of controlled random search price 1987 clustering kan and timmer 1987 competitive evolution holland 1975 and complex shuffling duan et al 1993 to offer a global optimization strategy by employing these techniques the sce ua algorithm provides a robust optimization framework and has shown numerically to be competitive and efficient comparing to other algorithms such as ga for calibrating rainfall runoff models beven 2011 gan and biftu 1996 wagener et al 2004 wang et al 2010 the sce ua algorithm has been widely used in water resources management barati et al 2014 eckhardt and arnold 2001 k ajami et al 2004 lin et al 2006 liong and atiquzzaman 2004 madsen 2000 sorooshian et al 1993 toth et al 2000 yang et al 2015 yapo et al 1996 as well as other fields of study such as pyrolysis modeling ding et al 2016 hasalová et al 2016 and artificial intelligence yang et al 2017 application of the sce ua is not limited to solving single objective optimization problems the multi objective complex evolution university of arizona mocom ua is an extension of the sce ua for solving multi objective problems boyle et al 2000 yapo et al 1998 besides the sce ua architecture has been used to develop markov chain monte carlo mcmc sampling named shuffled complex evolution metropolis algorithm scem ua and the multi objective shuffled complex evolution metropolis moscem to infer posterior parameter distributions of hydrologic models vrugt et al 2003a 2003b the metropolis scheme is used as the search kernel in the scem ua and moscem ua chu et al 2010 vrugt et al 2003a 2003b there is also an enhanced version of sce ua which is developed by chu et al 2011 entitled the shuffled complex strategy with principle component analysis developed at the university of california irvine sp uci chu et al 2011 found that the sce ua algorithm may not converge to the best solution on high dimensional problems due to population degeneration phenomenon the population degeneration refers to the situation when the search particles span a lower dimension space than the original search space chu et al 2010 which causes the search algorithm to fail in finding the global optimum to address this issue the sp uci algorithm employs principle component analysis pca in order to find and restore the missing dimensions during the course of search chu et al 2011 both sce ua and sp uci start the evolution process by generating a population within the feasible parameters space then population is partitioned into different complexes and each complex is evolved independently each member of the complex has the potential to contribute to offspring in the evolution process in each evolution step more than two parents may contribute to generating offspring to make the evolution process competitive a triangular probability function is used to select parents as a result the fittest individuals will have a higher chance of being selected each complex is evolved for a specific number of iterations and then complexes are shuffled to globally share the information attained by individuals during the search the competitive complex evolution cce and modified competitive complex evolution mcce are the search cores of the sce ua and sp uci algorithm respectively the cce and mcce evolutionary processes are developed based on nelder mead nelder and mead 1965 method with some modification the evolution process in the sce ua is not limited to these algorithms in fact several studies have incorporated different eas into the structure of the sce ua algorithm for example the frog leaping fl is developed by adapting particle swarm optimization pso algorithm to the sce ua structure for solving discrete problems eusuff et al 2006 eusuff and lansey 2003 mariani et al 2011 proposed an sce ua algorithm which employs de for evolving the complexes these studies revealed the flexibility of the sce ua in combination with other types of eas however the potential of combining different algorithms into a hybrid shuffled complex scheme has not been investigated the unique structure of the sce ua algorithm along with the flexibility of the algorithm for using different eas motivated us to use the sce ua as the cornerstone of the sc sahel framework the sc sahel algorithm employs multiple eas for evolving the population in a similar structure as that of the sce ua with the goal of selecting the most suitable search algorithm at each optimization step on the one hand some eas are more capable of visiting the new regions of the search space and exploring the problem space and hence are particularly suitable at the beginning of the optimization olorunda and engelbrecht 2008 on the other hand some eas are more capable of searching within the visited regions of the search space and hence boosting the convergence process after finding the region of interest mirjalili and hashim 2010 balancing between these two steps which are referred to as exploration and exploitation moeini and afshar 2009 is a challenging task in stochastic optimization methods črepinšek et al 2013 the sc sahel algorithm maintains a balance between exploration and exploitation phases by evaluating the performance of participating eas at each optimization step eas contribute to the population evolution according to their performance in previous steps the algorithms performance is evaluated by comparing the evolved complexes before and after evolution in this process the most suitable algorithm for the problem space become the dominant search core in this study four different eas are used as search cores in the proposed sc sahel framework including modified competitive complex evolution mcce used in the sp uci algorithm modified frog leaping mfl modified grey wolf optimizer mgwo and differential evolution de to better illustrate the performance of the hybrid sc sahel algorithm the framework is benchmarked over 29 test functions and compared to sc sahel with single ea among the 29 employed test functions there are 23 classic test functions xin et al 1999 and 6 composite test functions liang et al 2005 which are commonly used as benchmarks in comparing optimization algorithms furthermore the sc sahel framework is tested for a conceptual hydropower model which is built for the folsom reservoir located in the northern california usa the objective is to maximize the hydropower generation by finding the optimum discharge from the reservoir the study period covers run off season in california from april to june in which reservoirs have the highest annual storage volume field and lund 2006 using the proposed framework we compared different eas capability of finding a near optimum solution for dry wet and below normal scenarios the results support that the proposed algorithm is not only competitive in terms of increasing power generation but also is able to reveal the advantages and disadvantages of participating eas the rest of the paper is organized as follow in section 2 structure of the sc sahel algorithm and details of four eas are presented section 3 presents the test functions settings of the experiments and results obtained for each test function section 4 introduces the reservoir model and the optimization results for the case study finally in section 5 we draw conclusion summarize some limitations about the newly introduced framework and suggest some directions for future work 2 methodology the sc sahel algorithm is a parallel optimization framework which is built based on the original sce ua architecture sc sahel however differs from the original sce ua algorithm by using multiple search mechanisms instead of only employing the nelder mead simplex downhill method in this section we first introduce the main structure of sc sahel then we present four different eas which are employed as search cores in the sc sahel framework these algorithms are selected for illustrative purpose only and can be replaced by other evolutionary algorithms some modifications are made to the original form of these algorithms to allow fair competition between eas these modifications are detailed in appendix a d 2 1 the sc sahel framework the proposed sc sahel optimization strategy starts with generating a population with a pre defined sampling method within feasible parameters range the framework supports user defined sampling methods besides built in uniform random sampling urs and latin hypercube sampling lhs the population is then partitioned into different complexes the partitioning process warrants maintaining diversity of population in each complex in doing so population is first sorted according to objective function values then sorted population is divided into ngs equally sized groups ngs being the number of complexes ensuring that members of each group have similar objective function values each complex subsequently will randomly select a member from each of these groups this procedure maintains diversity of the population within each complex the complexes are then assigned to eas and evolved in contrast to the original concept of the sce ua the complexes are evolved with different eas rather than single search mechanism at the beginning of the search an equal number of complexes is assigned to each evolutionary method for instance if population is partitioned into 8 complexes and 4 different eas are used each algorithm will evolve 2 complexes independently 2 2 2 2 after evolving the complexes for pre specified number of steps the evolutionary method performance emp metric eq 1 will be calculated for each ea 1 emp mean f mean f n mean f in which f and f n are objective function values of individuals in each complex before and after evolution respectively the emp metric measures change in the mean objective function value of individuals in each complex in comparison to their previous state a higher emp value indicates a larger reduction in the mean objective function value obtained by the individuals in the complex the performance of each evolutionary algorithm is then evaluated based on the mean value of emp calculated for each evolved complex eas are then ranked according to the emp values ranks are in turn used to assign number of complexes to each evolutionary method for the next iteration the highest ranked algorithm will be assigned an additional complex to evolve in the next shuffling step while the lowest ranked evolutionary algorithm will lose one complex for the next step for instance if all the eas have 2 complexes to evolve 2 2 2 2 case the number of complexes assigned to each ea can be updated to 3 2 2 1 in other words this logic is an award and punishment process in which the algorithm with best performances will be awarded with an additional complex to evolve in the next iteration while the worst performing algorithm will be punished by losing one complex it is worth mentioning that as some of the algorithms may have poor performance in the exploration phase they might lose all their complexes during the adaptation process this might be troublesome as these algorithms may be superior in the exploitation phase if such algorithms are terminated in the exploration phase they cannot be selected during the convergence steps hence eas termination is avoided to fully utilize the potential of eas in all the optimization steps and balance the exploration and exploitation phases the minimum number of complexes assigned to each evolutionary method is restricted to at least 1 complex in this case if the lowest ranked ea has only 1 complex to evolve it won t lose its last complex if an algorithm outperforms others throughout the evolution of complexes the number of complexes assigned to the superior ea will be equal to the total number of complexes minus the number of eas plus one in this case all other algorithms are evolving one complex only as all algorithms are evolving at least one complex they have the chance to outperform other eas and gain more complexes during the optimization process and to potentially become the dominant search method as the search continues toward exploitation phase fig 1 briefly shows the flowchart of the sc sahel algorithm pseudo code of which is as follows step 0 initialization select ngs 1 and nps suggested nps 2n 1 where n is dimension of the problem where ngs is the number of complexes and nps is the number of individuals in the complexes ngs should be proportional to the number of evolutionary algorithms so that all the participating eas have an equal number of complexes at the beginning of the search step 1 sample npt points in the feasible parameter space using a user defined sampling method where npt equals to ngs nps compute objective function value for each point step 2 rank and sort all individuals in the order of increasing objective function value step 3 partition the entire population into complexes assign complexes to the participating eas step 4 monitor and restore population dimensionality using pca algorithm optional step 5 evolve each complex using the corresponding ea step 6 after evolving the complexes for a pre defined number of iterations calculate the mean emp for each ea step 7 rank the participating eas according to the mean emp value of each evolutionary method the highest ranked method will get additional complex in the next iteration while the worst evolutionary method will lose one step 8 shuffle complexes and form a new population step 9 check whether the convergence criteria are satisfied otherwise go to step 3 sc sahel allows for different settings that can influence the performance of the algorithm careful consideration should be devoted to the selection of these settings including number of complexes number of individuals within each complex number of evolution steps before each shuffling and stopping criteria thresholds some of these settings are adopted from the suggested settings for the sce ua for instance the number of individuals within each complex is set to 2 d 1 where d is dimension of the problem however some of the suggested settings cannot be applied to the sc sahel framework due to use of different eas these settings can be changed according to the complexity of the problem and the eas employed within the framework for instance the number of complexes the number of points within each complex and the number of evolution steps before each shuffling are problem dependent the sc sahel framework employs three different stopping criteria which are adopted from sce ua and sp uci these stopping criteria include number of function evaluations range of samples that span the search space and improvement in the objective function value in the last m shuffling steps these criteria are compared to pre defined thresholds which can in turn be tuned according to the complexity of the problem improper selection of these thresholds may lead to early or delayed convergence 2 2 evolutionary algorithms employed within sc sahel in this paper we employ four different eas to illustrate the flexibility of the sc sahel framework in adopting various eas and show the algorithms competition these algorithms are briefly presented here the pseudo code and details of these algorithms can be found in appendix a d 2 2 1 modified competitive complex evolution mcce the mcce algorithm is an enhanced version of cce algorithm used in the sce ua framework which provides a robust efficient and effective ea for exploring and exploiting the search space the mcce algorithm is developed based on the nelder mead algorithm however chu et al 2011 found that the shrink concept in the nelder mead algorithm can cause premature convergence to a local optimum interested readers can refer to chu et al 2010 2011 for further details on mcce algorithm the pseudo code of the mcce algorithm is detailed in appendix a sc sahel has similar performance to sp uci when the mcce algorithm is used as the only search mechanism and pca and resampling settings of sp uci are enabled for simplification and comparison sc sahel with the mcce algorithm as search core is referred as sp uci hereafter 2 2 2 modified frog leaping mfl the frog leaping fl algorithm uses adapted pso algorithm as a local search mechanism within the sce ua framework eusuff and lansey 2003 fl has shown to be an efficient search algorithm for discrete optimization problems and can find optimum solution much faster as compared to the ga algorithm eusuff et al 2006 in order to adapt the fl algorithm to the sc sahel parallel framework we introduce a slightly modified version of fl algorithm entitled mfl further details and pseudo code of the mfl can be found in appendix b the original fl algorithm and the mfl have four main differences first the original fl is designed for discrete optimization problems however the mfl is modified for continuous domain second the modified fl uses the best point in the subcomplex for generating new points however in the original fl framework new points are generated using the best point in the complex and the entire population the reason for this modification is to avoid using any external information by participating eas in other words the amount of information given to each eas is limited to the complex assigned to the eas third as the mfl algorithm only uses the best point within the complex for generating the new generation two different jump rates are used the reason for different jump rates is to allow mfl to have a better exploration and exploitation ability during optimization process these jump rates are selected by trial and error and may need further investigation to achieve a better performance by mfl algorithm fourth when the generated offspring is not better than the parents a new point is randomly selected within the range of individuals in the subcomplex this process which is referred to as censorship step in the fl algorithm eusuff et al 2006 is different from the original algorithm the mfl algorithm uses the range of points in the complex rather than the whole feasible parameters range resampling within the whole parameter space can decrease the convergence speed of the fl algorithm hence the resampling process is carried out only within the range of points in the complex hereafter the sc sahel with mfl algorithm as the only search core is referred as sc mfl 2 2 3 modified grey wolf optimizer mgwo the grey wolf optimizer is a meta heuristic algorithm inspired by the social hierarchy and hunting behavior of grey wolves mirjalili et al 2014 2016 grey wolves hunting strategy has three main steps first chasing and approaching the prey second encircling and pursuing the prey and finally attacking the prey mirjalili et al 2014 the gwo process resembles the hunting strategy of the grey wolves in this algorithm the top three fittest individuals are selected and contribute to the evolution of population hence the individuals in the population are navigated toward the best solution the gwo algorithm has shown to be effective and efficient in many test functions and engineering problems furthermore performance of gwo is comparable to other popular optimization algorithms such as ga and pso mirjalili et al 2014 gwo follows an adaptive process to update the jump rates to maintain balance between exploration and exploitation phases the adaptive jump rate of the gwo is removed here and 3 different jump rates are used instead the reason for this modification is that the information given to each ea is limited to its assigned complex similar to mfl algorithm the modified gwo mgwo algorithm uses the range of parameters to resample individuals when the generated offspring are not superior to their parents details and pseudo code of the mgwo algorithm can be found in appendix c hereafter the sc sahel with mgwo algorithm as the only search core is referred as sc mgwo 2 2 4 differential evolution de the de algorithm is a powerful but simple heuristic population based optimization algorithm qin and suganthan 2005 sadegh and vrugt 2014 proposed by storn and price 1997 in 2011 mariani et al 2011 integrated the de algorithm into sce ua framework and showed that the new framework is able to provide more robust solutions for some optimization problems in comparison to the sce ua similar to the work by mariani et al 2011 we use a slightly modified de algorithm based on the concepts from qin and suganthan 2005 in order to integrate the de algorithm into the sc sahel framework as the de algorithm has slower performance in comparison to other eas used here we have added multiple steps to the de here the de algorithm uses three different mutation rates in three attempts in the first attempt the algorithm uses a larger mutation rate this helps exploring the search space with larger jump rates in the second attempt the algorithm reduces the mutation rate to a quarter of the first attempt this will enhance the exploitation capability of the ea if none of these mutation rates could generate a better offspring than the parents in the next attempt the mutation rate is set to half of the first attempt lastly if none of these attempts generate a better offspring in comparison to the parents a new point is randomly selected within the range of individuals in the complex the pseudo code of the modified de algorithm is detailed in appendix d the sc sahel algorithm is referred to as sc de when the de algorithm is used as the only search algorithm 3 conceptual test functions and results 3 1 test functions the sc sahel framework is benchmarked over 29 mathematical test functions using single method and multi method search mechanisms this includes 23 classic test functions obtained from xin et al 1999 the name and formulation of these functions along with their dimensionality and range of parameters are listed in table 1 we selected these test functions as they are standard and popular benchmarks for evaluating new optimization algorithms mirjalili et al 2014 the remaining 6 are composite test functions c f 1 6 liang et al 2005 which represent complex optimization problems details of the composite test functions can be found in the work of liang et al 2005 and mirjalili et al 2014 classic test functions have dimensions in the range of 2 30 and all the composite test functions are 10 dimensional figs 2 and 3 show response surface of the test functions which can be shown in 2 dimension form the sc sahel settings used for optimizing these test functions are listed in table 2 for each test function number of points in each complex and number of evolution steps for each complex are set to 2d 1 and max d 1 10 respectively where d is the dimension of the problem the number of evolution steps is set to max d 1 10 to guarantee that eas evolve the complexes for enough number of steps before evaluating the eas in the high dimension problems the maximum number of function evaluation should be selected with careful consideration several experiments were conducted to find an optimal set of parameters for the sc sahel setting these experiments revealed that a low number of evolutionary steps before shuffling the complexes may not show the potential of the eas on the other hand using a large value for the number of evolution steps may shrink the complex to a small space which cannot span the whole search space duan et al 1994 maximum number of function evaluation is determined according to the complexity of the problem and is different for each of the test cases in addition to the maximum number of function evaluation the range of the parameters in the population and the improvement in the objective function values are used as convergence criteria the optimization run is terminated if the population range is smaller than 10 7 of the feasible range or the improvement in objective function value is smaller than 0 1 of the mean objective function value in the last 50 shuffling steps the lhs mechanism is used as the sampling algorithm of sc sahel for generating the initial population the framework provides multiple settings for boundary handling which can be selected by user sc sahel uses reflection as the default boundary handling method other initial sampling and boundary handling methods are also implemented in the sc sahel framework sensitivity of the initial sampling and boundary handling on the performance of the sc sahel algorithm is not studied in this paper the aforementioned settings can be applied to a wide range of problems 3 2 results and discussion table 3 illustrates the statistics of the final function values at 30 independent runs on 29 test functions using the hybrid sc sahel and individual eas with the goal to minimize the function values the best mean function value obtained for each test function is expressed in bold in table 3 results show that the hybrid sc sahel achieved the lowest function values in 15 out of 29 test functions compared to the mean function values achieved by all individual algorithms it is noteworthy that in 20 out of 29 test functions the hybrid sc sahel was among the top two optimization methods in finding the minimum function value a two sample t test with 5 significance level also showed that the results generated with the sc sahel algorithm is generally similar to the best performing algorithms comparing among single method algorithms in general the statistics obtained by sp uci are superior to other participating eas in 12 out of 29 test functions the sp uci algorithm achieved the lowest function value sc mfl sc mgwo and sc de were superior to other algorithms in 6 10 and 11 out of 29 test functions respectively in test functions f 6 f 16 f 17 f 18 f 19 f 20 and f 23 the single method and multi method algorithms achieved same function values on average in most cases in these cases according to the statistics shown in table 3 the sp uci and sc sahel algorithms offer lower standard deviation values and show more consistent results as compared to other eas the low standard deviation values obtained by sp uci and sc sahel indicate the robustness and consistency of these two algorithms in comparison to other algorithms in the test functions that the hybrid sc sahel algorithm was not able to produce the best mean function value the achieved mean function values deviation from that of the best performing algorithms are marginal for instance on the test functions f 2 f 4 f 10 and f 22 the statistics of the values obtained by sc sahel are similar to that achieved by the best performing methods which are sp uci and sc mgwo in general the hybrid sc sahel algorithm is superior to algorithms with individual ea on most of the test functions although on some test functions the sc sahel algorithm is slightly inferior to the best performing algorithm with only marginal differences the performance of the sc sahel in these test functions can be attributed to two main reasons first in the hybrid algorithm all the eas are involved in the evolution of the population hence if one of the algorithms have poor performance in comparison to other eas it still evolves a portion of the population as the complexes are evolved independently the poor performing eas may devastate a part of the information in the evolving complex on the other hand when the algorithms are used individually in the sc sahel framework the ea utilizes the information in all the complexes and the whole population in this case better result will be achieved in comparison to the hybrid sc sahel if the ea is the fittest algorithm for the problem space second some of the eas are faster and more efficient in a specific optimization phase exploration exploitation than others however they might not be as effective as other eas for other optimization phases hence dominance of these algorithm during the exploration or exploitation phases can mislead other eas and cause early and premature convergence engagement of other algorithms in the evolution process may prevent early convergence in these cases generally the performance criteria emp is responsible for selecting the most suitable algorithm in each optimization step however the criteria used in the sc sahel is not guaranteed to perform well in all problem spaces the performance criteria are problem dependent and need further investigations based on the problem space and eas however the emp metric seems to be a suitable metric for a wide range of problems to further evaluate the performance of the hybrid sc sahel algorithm we present the success rate of the algorithms in fig 4 the success rate is defined by setting target values for the function value for each test function when the function value is smaller than the target value the goal of optimization is reached and therefore the algorithm is considered successful a higher success rate resembles a better performance we use same target value for all algorithms in order to have a fair comparison according to fig 4 in 16 out of 29 test functions the hybrid algorithm achieved 100 success rate in other cases the success rates achieved by the proposed hybrid algorithm are comparable to the best performing algorithm with single ea for instance on the test function f 9 the sc mgwo sc de and sc mfl are not successful in finding the optimum solution success rates are 0 0 and 10 respectively however the hybrid sc sahel algorithm has similar performance 80 success rate to sp uci 97 success rate on the test function f 21 the success rate of the hybrid sc sahel algorithm 87 is close to the sc mgwo 93 which is the most successful algorithm the hybrid sc sahel algorithm also achieved a higher success rate than sp uci algorithm 33 in this test function according to fig 4 the average success rate of sc sahel is about 80 over all 29 test functions and it is the highest compared to the average success rate of other eas i e 73 58 58 and 54 for sp uci sc de sc mgwo and sc mfl algorithm respectively in some situations the poor performing eas may mislead other eas and cause early and premature convergence for instance on the test function c f 5 the hybrid algorithm achieved 57 success rate which is still better success rate than sp uci sc mfl and sc mgwo which are 0 10 and 50 respectively on this test function c f 5 the performance of the hybrid sc sahel is less affected by the most successful algorithm de this may be due to the low evolution speed of the de algorithm as the sc sahel algorithm maintains both convergence speed and efficiency during the entire search the hybrid sc sahel presents promising performance on the test functions c f 2 and c f 3 on test functions c f 2 and c f 3 the success rate of hybrid sc sahel is significantly higher than other eas most of which have 0 success rates for test function c f 2 the sc de algorithm achieved the lowest objective function value and the highest success rate 37 among single method algorithms however when eas are combined in the hybrid form the objective function value and the success rate are significantly improved this shows that sc sahel has the capability of solving complex problems by utilizing the potentials and advantages of all participating algorithms and improving the search success rate in table 4 we present the mean and standard deviation of the number of function evaluation which indicates the speed of each algorithm the lowest mean number of function evaluation is expressed in bold in table 4 as one of the stopping criteria in sc sahel framework is the maximum number of function evaluation some algorithms may terminate before they show their full potential for instance the sc de and the sc mfl usually reach the maximum number of function evaluations while other algorithms satisfy other convergence criteria in much less number of function evaluations in this case the objective function value doesn t represent the potential of the slow algorithms to give a better insight into this matter the mean and standard deviation std of the number of function evaluations are compared in table 4 the goal is to compare the speed of the individual eas and the hybrid optimization algorithm according to table 4 in most of the test cases the sp uci algorithm has the least number of function evaluations regardless of the objective function value achieved by the eas comparing the success rate and the number of function evaluation for different eas shows that sp uci achieved 100 success rate with the lowest number of function evaluation in 15 out of 29 test functions the sc mgwo algorithm only achieved 100 success rate with the lowest number of function evaluation in one test function although the hybrid sc sahel algorithm is not the fastest algorithm its speed is usually close to the fastest algorithm this is due to the contribution of different eas in the evolution process and the eas behavior on different problem spaces for instance de algorithm is slower in comparison to mcce sp uci algorithm in most of the test functions hence when the algorithms are working in a hybrid form the hybrid algorithm will be slower than the situation when the mcce sp uci algorithm is used individually figs 5 7 compare the average number of complexes assigned to each ea for the 29 employed test functions during the course of the search the variation of the number of complexes assigned to each ea indicates the dominance of each ea during the course of the search hence the performance of eas at each optimization step can be monitored in many test cases mcce sp uci algorithm has a relatively higher number of complexes than other eas during the search this shows that mcce is a dominant search algorithm on most of the test functions however in some other cases mcce is only dominant in a certain period of the search while other eas have demonstrated better efficiency during the entire search for example on test functions f 7 and f 20 mcce algorithm appears to be dominant only during the beginning of the search in the test function f 7 the exploration process starts with the dominance of the mcce and shifts between mgwo and mfl after the first 20 shuffling steps in some of the test functions such as f 7 a more random fluctuation is observed in the number of complexes assigned to each ea the reason for this behavior is the close competition of eas in these shuffling steps due to the noisy response surface of the test function f 7 most of the eas cannot significantly improve the objective function values during the exploitation phase on test functions f 8 and f 18 the mfl and de algorithms are the dominant search methods respectively during the beginning of the run while mcce algorithm becomes dominant only when the algorithm is in exploitation phase lastly on test functions f 9 f 22 c f 1 and c f 4 the variations of the number of complexes and the precedence of different eas as the most dominant search algorithm are observed it is worth mentioning that figs 5 7 show the number of complexes assigned to each ea for a single optimization run our observation of each individual run results not shown herein shows variation of the number of complexes among different runs is similar to each other for most test cases the observed variation for individual runs follows a specific pattern and is not random the similarity of the eas dominance pattern indicates that the selection of the eas by the sc sahel framework only depends on the characteristics of the problem space and the eas employed this also indicates that different eas have pros and cons on different optimization problems as a summary of our experiments on the conceptual test functions tables 3 and 4 and figs 4 7 the main advantage of the sc sahel algorithm over other optimization methods is its capability of revealing the trade off among different eas and illustrating the competition of participating eas different optimization problems have different complexity which introduces various challenges for each ea by incorporating different types of eas in a parallel computing framework and implementing an award and punishment logic the newly developed sc sahel framework not only provides an effective tool for global optimization but also gives the user insights about advantages and disadvantages of participating eas on individual optimization tasks this shows the potential of the sc sahel framework for solving different class of problems with different level of complexity besides the hybrid sc sahel algorithm is superior to shuffled complex based methods with single search mechanism such as sp uci in an absolute majority of the test functions 4 example application and results in this section we demonstrate an example application of the newly developed sc sahel algorithm a conceptual reservoir model is developed with the goal of maximizing hydropower generation on a daily basis operation the model is applied to the folsom reservoir in northern california 4 1 reservoir model a conceptual model is set up based on the relationship between the hydropower generation storage water head and bathymetry of the folsom reservoir daily releases from the reservoir in the study period are treated as the parameters of the model which in turn determines the problem dimensionality the model objective is to maximize the hydropower generation for a specific period the total hydropower production is a function of the water head difference between forebay and tailwater and the turbine flow rate the driving equation of the model is based on mass balance water budget which is formulated as 2 s t s t 1 i t r t m t where s t is storage at time step t i t and r t signify total inflow and release from the reservoir at time t respectively m t is total outflow inflow error which is derived by setting up mass balance for daily observed data the objective function employed here is 3 of t 1 n 1 p t p c where p c is total power plant capacity in mw and p t is total power generated in day t in mw for each day p t is derived as follow 4 p t η ρ g q t h t where η signifies turbine efficiency ρ is water density kg m3 g is gravity 9 81 m s2 and q t is discharge m3 s at time step t h t is hydraulic head m at time step t which is defined as 5 h t h f h t w where h f and h t w are water elevation in forebay and tailwater respectively h f and h t w are derived by fitting a polynomial to reservoir bathymetry data in the reservoir model coined above multiple constraints are considered for better representation of the real behavior of the system these constraints include power generation capacity storage level spill capacity and changes in the daily hydropower discharge total daily power generation is compared to maximum capacity of the hydropower plant also rule curve is used to control reservoir storage level during the operation period besides final simulated reservoir storage is constrained to 0 9 1 1 of the observed storage in another word 10 variation from the observation data is allowed for the final simulated storage level this constraint adds information from real reservoir operation into the optimization process this constraint can be replaced by other operation rules for simulation purposes the spill capacity of dam is calculated according to the water level in the forebay and compared to simulated spilled water a quadratic function is fitted to the water level and spill capacity data to derive the spill capacity at each time step the change in daily hydropower release is also constrained to better represent actual hydropower discharge and avoid large variation in a daily release the reservoir model used here is non linear and continuous the constraints of the model render finding the feasible solution a challenging task for all the eas the sc sahel framework is used to maximize the hydropower generation by minimizing the objective function value the settings used for the sc sahel is similar to the settings used for the mathematical test functions however the maximum number of function evaluations is set to 106 lower bound of the parameters range varies monthly due to the operational rules however upper bound is determined according to the hydraulic structure of the dam 4 2 study basin folsom reservoir is located on the american river in northern california and near sacramento california folsom dam was built by us army corps of engineers during 1948 1956 and is a multi purpose facility the main functions of the facility are flood control water supply for irrigation hydropower generation maintaining environmental flow water quality purposes and providing recreational area the reservoir has a capacity of 1 203 878 290 m3 and the power plant has a total capacity of 198 7 mw three different periods are considered here the first study period is april 1st 2010 to june 30th 2010 the year 2010 is categorized as below normal period according to california department of water resources the same period is selected in 2011 and 2015 as former is categorized by california department of water resources as wet and latter is classified as critical dry year the input and output from the reservoir are obtained from california data exchange center http cdec water ca gov note that demand is not included in the model because demand data was not available from a public data source 4 3 results and discussion the boxplot of the objective function values is shown in fig 8 for the folsom reservoir during the runoff season in 2015 2010 and 2011 which are dry below normal and wet years respectively the presented results are based on 30 independent optimization runs however infeasible objective function values are removed the feasibility of the solution is evaluated according to the objective function values due to the large values returned by the penalty function considered for infeasible solutions such solutions can be distinguished from the feasible solutions for wet year 2011 case sc mgwo and sc de didn t find a feasible solution in 2 and 4 runs out of 30 independent runs respectively the hybrid sc sahel found feasible solutions in all the cases however some of these solutions are not global optima on average the hybrid sc sahel algorithm is able to achieve the lowest objective function value as compared to other algorithms during dry and below normal period during dry and below normal periods sc sahel sp uci and sc de show similar performance in the wet period the sp uci algorithm achieved the lowest objective function value the sc sahel algorithm ranked second comparing the mean objective function values in this period the results achieved by the sc de is also comparable to sc sahel and sp uci the results show that overall the hybrid sc sahel algorithm has similar or superior performance in comparison to the single method algorithms also the results achieved by sc sahel and sp uci algorithms has less variability in comparison to other algorithms which show the robustness of these algorithms the worst performing algorithm is the sc mgwo which achieved the least mean objective function value in all the study periods in fig 9 boxplot of the number of function evaluations is presented for successful runs from the 30 independent runs during dry below normal and wet period years although the sc mgwo algorithm satisfied convergence criteria in the least number of function evaluation the sc mgwo was not successful in achieving the optimum solution in many cases the sp uci algorithm is the second fastest method among all the algorithms the hybrid sc sahel sc mfl and sc de are the slowest algorithm for satisfying the convergence criteria in almost all cases the slow performance of the hybrid sc sahel is due to the fact that 2 out of 4 de and mfl participating eas have very slow performance over the response surface fig 10 demonstrates the number of complexes assigned to each ea during the search which indicates the dominance of the participating algorithms and the award and punishment logic in the reservoir model as seen in fig 10 the mgwo algorithm is dominant in the beginning of the search although it is not capable of finding the optimum solution in most cases the reason for the dominance of the mgwo is the speed of the algorithm in exploring the search space mgwo is superior to other eas in the beginning of the search however after a few iterations the mcce algorithm took the precedence and become the dominant algorithm over other eas mgwo and de are less involved in the rest of the optimization process after the initial steps however competition between mcce and mfl continues although contribution of mgwo and de are at minimum in the rest of the optimization process they are utilizing a part of information within the population this can affect the speed and performance of the sc sahel algorithm in both the wet and below normal cases the hybrid sc sahel algorithm is mostly terminated by reaching the maximum number of function evolution however the mean objective function value obtained by the hybrid sc sahel is still superior to most of the algorithms the performance of the sc sahel can be affected by the settings of the algorithm different settings have been tested and evaluated for the reservoir model the results show that the number of evolution steps before shuffling can influence the performance of the hybrid sc sahel algorithm in the current setting the number of evolution steps within each complex is set to d 1 d is dimension of the problem although this setting seems to provide acceptable performance for a wide range of problems it may not be the optimum setting for all the problems spaces and eas in the reservoir model as the study period has 91 days the model evolves each complex for 92 steps this number of evolution steps allows the algorithms to navigate the complexes toward local solutions and increase the total number of function evaluations without specific gain decreasing the number of evolution steps allows the algorithms to communicate more frequently so they can use the information obtained by other eas here for demonstrative purposes the same setting has been applied to all the problems however better performance is observed for the hybrid sc sahel algorithm when the number of evolution steps are set to a value smaller than 92 the algorithm is less sensitive to other settings for the reservoir model however they can still affect the performance of the algorithm in fig 11 we present the simulated storage for different study periods achieved by different eas during the dry period not only the sc sahel algorithm achieved the lowest objective function value but also the storage level is higher than the observed storage level in most of the period this is due to the fact that power generation is a function of water height as well as discharge rate during below normal period sc sahel sp uci and sc de algorithms show a similar behavior in terms of the storage level during wet period storage level simulated by sp uci and sc sahel algorithm is lower than all other algorithms it is worth noting that during wet period sc sahel and sp uci algorithms are able to find optimum solution which objective function value is 0 in some of the runs however the simulated storage by these algorithms show some level of uncertainties fig 11 this shows equifinality in simulation which means that same hydropower generation can be achieved by different sets of parameters feng et al 2017 this equifinality can be due to deficiencies in the model structure or the boundary conditions freer et al 1996 the wet period seems to offer a more complex response surface for the reservoir model during the wet period some algorithms such as sc de are not capable of finding a feasible solution in some of the runs in this period the large input volume and the rule curve added more complexity to the optimization problem the results of the real world application show the potential of the newly developed sc sahel framework for solving high dimension problems in general the hybrid algorithm was more successful in finding a feasible solution in comparison to single method algorithms in some cases the hybrid sc sahel was terminated due to the large number of function evaluations however the performance of the hybrid sc sahel is always comparable to the best performing method this shows the potential of the sc sahel for solving a broad class of optimization problems besides the framework provides insight into the performance of the algorithms at different steps of the optimization process this feature of the sc sahel algorithm can aid user to select the best setting and ea for the problem 5 conclusions and remarks we developed a hybrid optimization framework named shuffled complex self adaptive hybrid evolution sc sahel which uses an award and punishment logic in junction with various types of evolutionary algorithms eas and selects the best ea that fits well to different optimization problems the framework provides an arsenal of tools for testing evaluating and developing optimization algorithms we compared the performance of the hybrid sc sahel with single method algorithms on 29 test functions the results showed that the sc sahel algorithm is superior to most of single method optimization algorithms and in general offers a more robust and efficient algorithm for optimizing various problems furthermore the proposed algorithm is able to reveal the characteristics of different eas during entire search period the algorithm is also designed to work in a parallel framework which can take the advantage of available computation resources the newly developed sc sahel offers different advantages over conventional optimization tools some of the sc sahel characteristics are intelligent evolutionary method adaptation during the optimization process flexibility of the algorithm for using different evolutionary methods flexibility of the algorithm for using initial sampling and boundary handling method independent parallel evolution of complexes population degeneration avoidance using pca algorithm robust and fast optimization process evolutionary algorithms comparison for different types of problems although the presented results support advantage of the hybrid sc sahel to algorithms with individual eas there are multiple directions for further improvement of the framework for example eas performance metric for evaluating the search mechanism in the current algorithm the complex allocation to different ea is carried out by ranking the algorithm according to the emp metric the performance criteria can change the allocation process and affect the performance of the algorithm depending on the application a more comprehensive performance criterion may be necessary for achieving the best performance however the current emp criterion does not affect the conclusion and comparison of different eas in addition the current sc sahel framework is designed to solve single objective optimization problems a multi objective version can be developed to extend the scope of the application this paper serves as an introduction to the newly developed sc sahel algorithm we hope that more investigation on the interaction among different eas boundary handling schemes and response surface in different case studies and optimization problems reveal the advantages and limitations of sc sahel acknowledgments and data this work is supported by u s department of energy doe prime award de ia0000018 california energy commission cec award 300 15 005 nsf cybersees project award ccf 1331915 noaa nesdis ncdc prime award na09nes4400006 and ncsu cics and subaward 2009 1380 01 and the u s army research office award w911nf 11 1 0422 the folsom reservoir bathymetry information used here is provided by dr erfan goharian from uc davis who also helped us for setting up the reservoir model the authors would like to thank the comments of the editors and four anonymous reviewers which significantly improved the quality of this manuscript appendix a modified competitive complex evolution mcce mcce algorithm pseudo code is as follow step 0 initialize i 1 and get maximum number of iteration allowed i step 1 sort individuals in order of increasing objective function value assign individuals a triangular probability except for the fittest point according to a1 p 2 nps 1 n nps nps 1 where nps is the number of individuals in the complex and n is the rank of the sorted individuals step 2 select d 1 individuals d is problem dimension from the complex including the fittest individual in the complex step 3 the selected individuals are then stored in s forming a simplex generate offspring according to following steps i sort individuals in s according to their objective function value find centroid c of the first d individuals ii reflection reflect the worst individual in s w across the centroid to generate a new point r according to following equation a2 r 2 c w evaluate objective function for the new point f r if f 1 f r f d set offspring o r and go to vii iii expansion if f r f 1 reflect c across r and generate e a3 e 2 r c evaluate objective function for the new point f e if f e f r set o e and go to vii otherwise o r and go to vii iv outside contraction if f d f r f w calculate the outside contraction point a4 o c c 0 5 r c evaluate the outside contraction point f o c if f o c f r set o o c and go to vii otherwise o r and go to vii v inside contraction if f w f r calculate inside contraction point a5 i c c 0 5 w c evaluate inside contraction point f i c if f i c f r set o i c and go to vii otherwise continue to vi vi multinormal sampling if the steps above did not generate a better offspring an individual will be drawn with a multinormal distribution defined by simplex and replace the worst individual in the simplex regardless of objective function value the multinormal sampling is as follow a calculate the covariance matrix r for the simplex and store diagonal of matrix in d b modify d as follow a6 d m 2 d m e a n d c generate a new covariance matrix r with d m as diagonal and zeroes everywhere else d sample a point with multinormal distribution with mean of c and covariance of r and store in o vii replace the worst individual in the complex with o let i i 1 if i i go to step 1 otherwise sort the points in the complex and return the evolved complex appendix b modified frog leaping mfl modified fl mfl algorithm is as follow step 0 initialize i 1 and get maximum number of iteration allowed i step 1 sort individuals in order of increasing objective function assign individuals a triangular probability using following equation b1 p 2 nps 1 n nps nps 1 where nps is the number of individuals in the complex and n is the rank of the sorted individuals step 2 select d 1 individuals d is problem dimension from the complex step 3 the selected individuals are stored in s forming a subcomplex generate offspring according to following steps i generate a new point with the worst point in s w and best point b in the subcomplex as follow b2 n b w 0 5 r 1 5 b w where r is a random number in the range of 0 1 evaluate objective function for the new point and get f b if f b f w set o n b and go to iv ii if f w f b generate a new point with the worst point in s w and best point b in the subcomplex as follow b3 n b w 0 5 r b w evaluate objective function for the new point and get f b if f b f w set the offspring set o n b and go to iv iii censorship step if f w f b randomly generate the offspring o by sampling within the range of individuals in the subcomplex iv replace the worst individual in the complex with the offspring o let i i 1 if i i go to step 1 otherwise sort the points in the complex and return the evolved complex appendix c modified grey wolf optimizer gwo modified grey wolf optimizer is as follow step 0 initialize i 1 and get maximum number of iteration allowed i step 1 sort the individuals in the order of increasing objective function value assign individuals a triangular probability except for the fittest point using following equation c1 p 2 nps 1 n nps nps 1 where nps is the number of individuals in the complex and n is the rank of the sorted individuals step 2 select d 1 individuals d is problem dimension from the complex with triangular probability including the fittest point in the complex and store them in s step 3 select the best three points in the s and store them in α β and γ respectively the worst point in the s is stored in w step 4 for each of α β and γ evolve individuals according to the following procedure i derive a and c as follow for α β and γ c2 a 4 r 1 2 c3 c 2 r 2 where r 1 r 2 are two independent random vectors which have d dimensions and values in range of 0 1 ii derive d for α β and γ as follow c4 d α c α x α w d β c β x β w d γ c γ x γ w iii derive z for α β and γ as follow c5 z α x α a α d α z β x β a β d β z γ x γ a γ d γ iv generate new point by finding the centroid of z α z β and z γ c6 c z α z β z γ 3 v calculate and store objective function value for the new point f c if the new point is better than the worst point among the selected points f c f w set o c go to step 7 step 5 if f c f w go to step 4 and use a smaller range for a in this step a is calculated as follow c7 a 2 r 1 1 step 6 if the newly generated individual is worse than the worst individuals in subcomplex generate a new point with uniform random sampling within the range of individuals in the complex store the new point in o step 7 replace the worst individual among selected points in the complex with the offspring o let i i 1 if i i go to step 1 otherwise sort the points in the complex and return the evolved complex appendix d modified differential evolution de modified differential evolution algorithm is as follow step 0 initialize i 1 and get maximum number of iteration allowed i step 1 sort the individuals in the order of increasing objective function value assign individuals a triangular probability using following equation d1 p 2 nps 1 n nps nps 1 where nps is the number of individuals in the complex and n is the rank of the sorted individuals step 2 select d 1 points d is problem dimension from the complex with the assigned probability and store them along with the fittest point in the complex in s step 3 the selected individuals are sorted and stored in s forming a subcomplex generate offspring according to following steps i generate a new point with the worst point in s w and using the top three individuals in the subcomplex d2 v 1 w 2 f s 1 w 2 f s 2 s 3 where w is the worst point in the s s 1 s 2 and s 3 are three selected individuals then mutation and crossover operator is applied to the w and v 1 to generate v n 1 the objective function value for the new point is calculated and stored in f n 1 if f n 1 f w set o v n 1 and go to v ii if f w f n 1 generate a new point with the worst point in s w and using the top three points in the subcomplex as follow d3 v 2 w 0 5 f s 1 w 0 5 f s 2 s 3 after mutation crossover operator is applied to the w and v 2 to generate v n 2 then the objective function for the new point is derived and stored in f n 2 if f n 2 f w set o v n 2 and go to v iii if f w f n 2 generate a new point with the worst point in s w and using the top three points in the subcomplex as follow d4 v 3 w f s 1 w f s 2 s 3 after mutation crossover operator is applied to the w and v 3 to generate v n 3 the objective function value is calculated and stored in f n 3 if f n 3 f w set o v n 3 and go to v iv if the newly generated point is worse than the worst point in subcomplex generate a new point from uniform random distribution within the range of points in the complex store the new point in o v replace the worst point in the complex with the offspring o let i i 1 if i i go to step 1 otherwise sort the points in the complex and return the evolved complex abbreviations amalgam so a multialgorithm genetically adaptive method for single objective optimization cce competitive complex evolution de differential evolution ea evolutionary algorithm emp evolutionary methods performance fl frog leaping gwo grey wolf optimizer lhs latin hypercube sampling mcce modified competitive complex evolution mcmc markov chain monte carlo mfl modified frog leaping mgwo modified grey wolf optimizer mocom ua multi objective complex evolution university of arizona moscem multi objective shuffled complex evolution metropolis nfl no free lunch pca principal component analysis pso particle swarm optimization sade self adaptive differential evolution sce ua shuffle complex evolution developed at university of arizona scem ua shuffled complex evolution metropolis algorithm developed at university of arizona sc sahel shuffle complex self adaptive hybrid evolution sp uci shuffled complex strategy with principal component analysis developed at university of california irvine urs uniform random sampling 
